<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005768</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01392</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Chemical compounds</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Organic chemistry</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Hormones</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Neostriatum</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Neostriatum</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title>
<alt-title alt-title-type="running-head">The successor representation as a mechanism for model-based behavior</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2929-6912</contrib-id>
<name name-style="western">
<surname>Russek</surname>
<given-names>Evan M.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Momennejad</surname>
<given-names>Ida</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Botvinick</surname>
<given-names>Matthew M.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6546-3298</contrib-id>
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5029-1430</contrib-id>
<name name-style="western">
<surname>Daw</surname>
<given-names>Nathaniel D.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Center for Neural Science, New York University, New York, NY, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Princeton Neuroscience Institute and Department of Psychology, Princeton University, Princeton, NJ, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>DeepMind, London, United Kingdom and Gatsby Computational Neuroscience Unit, University College London, United Kingdom</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Psychology and Center for Brain Science, Harvard University, Cambridge, MA, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Daunizeau</surname>
<given-names>Jean</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Brain and Spine Institute (ICM), FRANCE</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">emr443@nyu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>25</day>
<month>9</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>9</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>9</issue>
<elocation-id>e1005768</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>8</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>9</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Russek et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005768"/>
<abstract>
<p>Humans and animals are capable of evaluating actions by considering their long-run future rewards through a process described using model-based reinforcement learning (RL) algorithms. The mechanisms by which neural circuits perform the computations prescribed by model-based RL remain largely unknown; however, multiple lines of evidence suggest that neural circuits supporting model-based behavior are structurally homologous to and overlapping with those thought to carry out model-free temporal difference (TD) learning. Here, we lay out a family of approaches by which model-based computation may be built upon a core of TD learning. The foundation of this framework is the successor representation, a predictive state representation that, when combined with TD learning of value predictions, can produce a subset of the behaviors associated with model-based learning, while requiring less decision-time computation than dynamic programming. Using simulations, we delineate the precise behavioral capabilities enabled by evaluating actions using this approach, and compare them to those demonstrated by biological organisms. We then introduce two new algorithms that build upon the successor representation while progressively mitigating its limitations. Because this framework can account for the full range of observed putatively model-based behaviors while still utilizing a core TD framework, we suggest that it represents a neurally plausible family of mechanisms for model-based evaluation.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>According to standard models, when confronted with a choice, animals and humans rely on two separate, distinct processes to come to a decision. One process deliberatively evaluates the consequences of each candidate action and is thought to underlie the ability to flexibly come up with novel plans. The other process gradually increases the propensity to perform behaviors that were previously successful and is thought to underlie automatically executed, habitual reflexes. Although computational principles and animal behavior support this dichotomy, at the neural level, there is little evidence supporting a clean segregation. For instance, although dopamine—famously implicated in drug addiction and Parkinson’s disease—currently only has a well-defined role in the automatic process, evidence suggests that it also plays a role in the deliberative process. In this work, we present a computational framework for resolving this mismatch. We show that the types of behaviors associated with either process could result from a common learning mechanism applied to different strategies for how populations of neurons could represent candidate actions. In addition to demonstrating that this account can produce the full range of flexible behavior observed in the empirical literature, we suggest experiments that could detect the various approaches within this framework.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>National Institute of Mental Health (US)</institution>
</funding-source>
<award-id>T32 MH019524</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2929-6912</contrib-id>
<name name-style="western">
<surname>Russek</surname>
<given-names>Evan M.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>5R01MH109177-02</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2929-6912</contrib-id>
<name name-style="western">
<surname>Russek</surname>
<given-names>Evan M.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000925</institution-id>
<institution>John Templeton Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Momennejad</surname>
<given-names>Ida</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported in part by the National Institute of Mental Health (<ext-link ext-link-type="uri" xlink:href="https://www.nimh.nih.gov/index.shtml" xlink:type="simple">https://www.nimh.nih.gov/index.shtml</ext-link>) of the National Institutes of Health under a Training Grant, Award Number T32 MH019524, to ER, a R.L. Kirchstein National Research Service Award 1F13MH110111-01 to ER, as well as under Award Number 1R01MH109177 to MMB, SJG and NDD. IM and MMB were also supported by the John Templeton Foundation (<ext-link ext-link-type="uri" xlink:href="https://www.templeton.org/" xlink:type="simple">https://www.templeton.org/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="35"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-10-05</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The code for this manuscript is now online and available at <ext-link ext-link-type="uri" xlink:href="https://github.com/evanrussek/Predictive-Representations-PLOS-CB-2017" xlink:type="simple">https://github.com/evanrussek/Predictive-Representations-PLOS-CB-2017</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A key question in both neuroscience and psychology is how the brain evaluates candidate actions in complex, sequential decision tasks. In principle, computing an action’s expected long-run cumulative future reward (or <italic>value</italic>) requires averaging rewards over the many future state trajectories that might follow the action. In practice, the exact computation of such expectations by dynamic programming or tree search methods may be prohibitively expensive, and it is widely believed that the brain simplifies the computations occurring at decision time, in part by relying on “cached” (pre-computed) long-run value estimates [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>].</p>
<p>Such caching of values is the hallmark of prominent temporal difference (TD) learning theories, according to which prediction errors reported by phasic dopamine responses update these cached variables [<xref ref-type="bibr" rid="pcbi.1005768.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref004">4</xref>]. This, in turn, provides a neuro-computational account of inflexible stimulus-response habits, due to the fact that TD learning cannot update cached values in response to distal changes in reward (e.g., following reward devaluation). The computationally cheap but inflexible “model-free” nature of TD learning, which relies only on trial-and-error interactions with the environment, contrasts with the flexible but computationally expensive “model-based” nature of dynamic programming and tree search methods, which rely on an explicit internal model of the environment. Due to the complementary properties of model-free and model-based value computation, it has been suggested that the brain makes use of both in the form of parallel RL systems that compete for control of behavior [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>].</p>
<p>Although flexible choice behavior seems to demonstrate that humans and animals may use model-based evaluation in some circumstances, very little is known about how this is actually implemented in the brain, or indeed to what extent the behavioral phenomena that have been taken to suggest model-based learning might arise from some simpler approximation. In this article, we explore a family of algorithms that capture a range of such approximations and, we argue, provide a promising set of candidates for the neural foundations supporting such learning.</p>
<p>Our proposals are motivated by multiple suggestive, but also somewhat counterintuitive, lines of evidence, which suggest that the dopamine system and its key targets are implicated not just in the model-free behaviors that theory endows them with, but also in the more flexible choice adjustments that seem to reflect model-based learning [<xref ref-type="bibr" rid="pcbi.1005768.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref011">11</xref>]. This is perplexing for the standard account because typical model-based algorithms such as value iteration do not make use of a TD prediction error for long-run reward of the sort associated with dopamine. Instead, they store internal variables (specifically, predictions about immediate “one-step” rather than long-run consequences of actions), which require different update rules and error signals [<xref ref-type="bibr" rid="pcbi.1005768.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005768.ref013">13</xref>]. Additionally, at choice time, such algorithms require computations that are structurally different than those typically prescribed to the dopaminergic-striatal circuitry [<xref ref-type="bibr" rid="pcbi.1005768.ref014">14</xref>].</p>
<p>In this article, we revisit and extend the <italic>successor representation</italic> (SR) [<xref ref-type="bibr" rid="pcbi.1005768.ref015">15</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref016">16</xref>],(see also [<xref ref-type="bibr" rid="pcbi.1005768.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref022">22</xref>]), a predictive state representation that can endow TD learning with some aspects of model-based value computation, such as flexible adjustment following reward devaluation. That the SR, when combined with TD learning, can produce such flexible behavior makes it a promising candidate for a neural foundation for model-based learning, which (because it is built on top of a TD foundation) can also explain dopaminergic involvement in model-based learning. However, this approach, in its original form, results in behavioral inflexibilities that could serve as empirical signatures of it, but also make it inadequate for fully explaining organisms’ planning capacities. In particular, the SR simplifies planning by caching a set of intermediate quantities, expectations about cumulative future state occupancies. For this reason, unlike model-based learning, it is incapable of solving many tasks that require adjusting to changes in contingencies between actions and their long-run consequences (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref024">24</xref>]).</p>
<p>In this article, we explore a family of algorithms based around the SR, and introduce two new variants that mitigate its limitations. In particular, we examine algorithms in which the SR is updated either by computation at choice time or off-line by replayed experience, both of which help to ameliorate its problems with caching. These approaches can each account for human and animal behavior in a wide range of planning tasks, suggest connections with other models of learning and dopamine, and make empirically testable predictions. Overall, these approaches represent a family of plausible and computationally efficient hypothetical mechanisms for the full range of flexible behaviors associated with model-based learning, and could provide a clear theoretical foundation for future experimental work.</p>
<p>The article is organized as follows. In the remainder of this introduction, we review the formalism of reinforcement learning in a Markov decision process (MDP) and use this framework to delineate the problem of model-based flexibility arising from model-free circuitry and elucidate how the SR offers a potential solution to this problem. In the results section, we use simulations to demonstrate the precise behavioral limitations induced by computing values using the SR, as originally described, and evaluate these limitations with respect to the behavioral literature. We then introduce two new versions of the SR that progressively mitigate these limitations, and again simulate their expected consequences in terms of flexible or inflexible choice behavior.</p>
<sec id="sec002">
<title>Formalism: Model-based and model-free reinforcement learning</title>
<p>Here, we briefly review the formalism of reinforcement learning in a Markov decision process (MDP), which provides the foundation for our simulations (see [<xref ref-type="bibr" rid="pcbi.1005768.ref025">25</xref>] or [<xref ref-type="bibr" rid="pcbi.1005768.ref026">26</xref>] for a fuller presentation).</p>
<p>An MDP is defined by a set of states, a set of actions, a reward function <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) over state/action pairs, and a state transition distribution, <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>), where <italic>a</italic> denotes the chosen action. States and rewards occur sequentially according to these one-step functions, driven by a series of actions; the goal is to learn to choose a probabilistic policy over actions, denoted by <italic>π</italic>, that maximizes the value function, <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>), defined as the expected cumulative discounted reward:
<disp-formula id="pcbi.1005768.e001">
<alternatives>
<graphic id="pcbi.1005768.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mspace width="0.25em"/><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>]</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Here, <italic>γ</italic> is a parameter controlling temporal discounting. The value function can also be defined recursively as the sum of the immediate reward of the action chosen in that state, <italic>R</italic>(<italic>s</italic>,<italic>a</italic>), and the value of its successor state <italic>s</italic>’, averaged over possible actions, <italic>a</italic>, and transitions that would occur if the agent chose according to <italic>π</italic>:
<disp-formula id="pcbi.1005768.e002">
<alternatives>
<graphic id="pcbi.1005768.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>The value function under the optimal policy is given by:
<disp-formula id="pcbi.1005768.e003">
<alternatives>
<graphic id="pcbi.1005768.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.25em"/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>Knowledge of the value function can help to guide choices. For instance, we can define the state-action value function as the value of choosing action <italic>a</italic> and following <italic>π</italic> thereafter:
<disp-formula id="pcbi.1005768.e004">
<alternatives>
<graphic id="pcbi.1005768.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.5em"/><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula></p>
<p>Then at any state one could choose the action that maximizes <italic>Q</italic><sup><italic>π</italic></sup>(<italic>s</italic>,<italic>a</italic>). (Formally this defines a new policy, which is as good or better than the baseline policy <italic>π</italic>; analogously, <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref> can be used to define the optimal state-action value function, the maximization of which selects optimal actions.) Note that it is possible to write a recursive definition for <italic>Q</italic> in the same manner as <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref>, and work directly with the state-action values, rather than deriving them indirectly from <italic>V</italic>.</p>
<p>For expositional simplicity, in this article, we work instead with <italic>V</italic> wherever possible (mainly because this is easier to depict visually, and simplifies the notation), and accordingly we assume in our simulations that the agent derives <italic>Q</italic> using <xref ref-type="disp-formula" rid="pcbi.1005768.e004">Eq 3</xref> for guiding choices. To be concrete, in a spatial “gridworld” task of the sort we simulate, this amounts to computing a value function <italic>V</italic> over locations <italic>s</italic>, and using it to derive <italic>Q</italic> (the value of actions <italic>a</italic> heading in each of the four cardinal directions) by examining <italic>V</italic> for each adjacent state. Although this simplifies bookkeeping for this class of tasks, <italic>this is not intended as a substantive claim</italic>. Indeed, the last algorithm we propose will work directly with <italic>Q</italic> values, and the others can easily be re-expressed in this form.</p>
<p>The problem of reinforcement learning is then reduced to learning to predict the value function <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>) or <italic>V</italic><sup>*</sup>(<italic>s</italic>). There are two main families of approaches. Model-based algorithms learn to estimate the one-step transition and reward functions, <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) and <italic>R</italic>(<italic>s</italic>,<italic>a</italic>), from which it is possible to compute <italic>V</italic><sup>*</sup> (or <italic>Q</italic><sup>*</sup>) using <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref>. This typically involves unrolling the recursion in <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref> into a series of nested sums, an algorithm known as value iteration. The alternative, model-free, approach exemplified by TD learning bypasses estimating the one-step model. Instead, it directly updates a cached estimate of the value function itself. In particular, following a transition <italic>s</italic> → <italic>s</italic>′ initiated by action <italic>a</italic>, a reward prediction error, <italic>δ</italic>, is calculated and used to update <italic>V</italic>(<italic>s</italic>):
<disp-formula id="pcbi.1005768.e005">
<alternatives>
<graphic id="pcbi.1005768.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
<disp-formula id="pcbi.1005768.e006">
<alternatives>
<graphic id="pcbi.1005768.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>V</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi>
</mml:math>
</alternatives>
</disp-formula>
where <italic>α</italic><sub><italic>TD</italic></sub> is a learning rate parameter.</p>
<p>The TD update rule is derived from the recursion in <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref>: each step iteratively pushes the left hand side of the equation, <italic>V</italic>(<italic>s</italic>), closer to <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) + <italic>γV</italic>(<italic>s</italic>′), which is a one-sample estimate of the right hand side.</p>
<p>Finally, analogous sample-based updates may also be conducted offline (e.g., between steps of actual, “online” experience). This is a key insight of Sutton’s Dyna architecture [<xref ref-type="bibr" rid="pcbi.1005768.ref027">27</xref>] (see also [<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>]). The approach, like TD, caches estimates of <italic>V</italic>(<italic>s</italic>). Here TD learning is supplemented by additional offline updates. Specifically, samples consisting of a transition and reward triggered by a state-action (<italic>s</italic>,<italic>a</italic>,<italic>r</italic>,<italic>s</italic>’) are generated either from a learned one-step model’s probability distributions <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) and <italic>R</italic>(<italic>s</italic>,<italic>a</italic>), or instead simply replayed, model-free, from stored episodes of previously experienced transitions. For each sample, <italic>V</italic>(<italic>s</italic>) is then updated according to Eq (<xref ref-type="disp-formula" rid="pcbi.1005768.e005">4</xref>). Given sufficient iterations of sampling between steps of real experience, this approach can substitute for explicit value iteration and produce estimates at each step comparable to model-based approaches that more directly solve Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e002">1</xref> or <xref ref-type="disp-formula" rid="pcbi.1005768.e003">2</xref>.</p>
<p>A further distinction, which will become important later, is that between <italic>on-policy</italic> methods, based on <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref>, and <italic>off-policy</italic> methods, based on <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref>. On-policy methods estimate a policy-dependent value function <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>), whereas off-policy methods directly estimate the optimal value function <italic>V</italic><sup>*</sup>(<italic>s</italic>). Typically, model-based methods are off-policy (since having learned a one-step model it is possible to use <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref> to directly compute the optimal policy); whereas different TD learning variants can be either on- or off-policy.</p>
</sec>
<sec id="sec003">
<title>Model-free learning in the brain</title>
<p>Due to the similarity between the phasic responses of midbrain dopamine neurons, and the TD prediction error <italic>δ</italic> (<xref ref-type="disp-formula" rid="pcbi.1005768.e005">Eq 4</xref>), it has long been suggested that this system implements TD learning [<xref ref-type="bibr" rid="pcbi.1005768.ref002">2</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref003">3</xref>]. More specifically (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref004">4</xref>]; <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1A</xref>) it has been suggested that values <italic>V</italic> or <italic>Q</italic> are associated with the firing of medium spiny neurons in striatum [<xref ref-type="bibr" rid="pcbi.1005768.ref029">29</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref030">30</xref>], as a function of an input state (or state-action) representation carried by their afferent neurons in frontal cortex, and that learning of the value function is driven by dopamine-mediated adjustment of the cortico-striatal synapses connecting these neurons. Selection among these striatal value representations would then drive action choice. Although not entirely uncontroversial, a great deal of evidence about dopamine and its targets supports this hypothesis (see [<xref ref-type="bibr" rid="pcbi.1005768.ref026">26</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref031">31</xref>] for fuller reviews).</p>
<fig id="pcbi.1005768.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Cortico-striatal loops and reinforcement learning.</title>
<p>a) Canonical circuit for TD learning. A dopaminergic prediction error, signaled in substantia nigra pars compacta and ventral tegmental area, updates the value of cortically represented states and actions by modifying cortico-striatal synapses. Depending on their value, represented in striatal medium spiny neurons (MSN), actions are passed through to basal-ganglia action systems. b) Results of rodent lesion studies. Lesions to a cortico-striatal loop passing through dorsomedial (DM) striatum prevent flexibly adjusting behavior following reward devaluation. This area receives input from ventromedial prefrontal cortex and projects, via globus pallidus, to dorsomedial nucleus of the thalamus. This loop is generally thought to implement model-based learning [<xref ref-type="bibr" rid="pcbi.1005768.ref032">32</xref>]. Lesions to cortico-striatal loop passing through dorsolateral (DL) striatum cause animals to maintain ability to flexibly adjust behavior following devaluation, despite over-training. This area receives input from sensory and motor areas of cortex and projects, via globus pallidus, to posterior nucleus of the thalamus. This loop is generally thought to implement model-free learning [<xref ref-type="bibr" rid="pcbi.1005768.ref032">32</xref>]. In addition to receiving similar dopaminergic innervation from substantia nigra pars compacta (SnC), such loops are famously thought to be homologous to one another [<xref ref-type="bibr" rid="pcbi.1005768.ref033">33</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g001" xlink:type="simple"/>
</fig>
<p>Such theories provide a neural implementation of Thorndike’s early law of effect, the reinforcement principle according to which rewarded actions (here, those paired with positive prediction error) tend to be repeated [<xref ref-type="bibr" rid="pcbi.1005768.ref034">34</xref>]. However, the hypothesis that animals or humans rely exclusively on this principle to make decisions has long been known to be false, as demonstrated by a line of learning experiments whose basic logic traces to rodent spatial navigation experiments by Tolman [<xref ref-type="bibr" rid="pcbi.1005768.ref023">23</xref>] (for modern variants, see [<xref ref-type="bibr" rid="pcbi.1005768.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref035">35</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref037">37</xref>]).</p>
<p>To facilitate simulation and analysis, we here frame the logic of these experiments in terms of “grid-world” spatial MDPs. When viewed as MDPs, Tolman’s experiments can be divided into two categories, which require subjects to adjust to either of two different sorts of local changes in the underlying MDP. Experience with these changes is staged so as to reveal whether they are relying on cached values or recomputing them from a representation of the full MDP.</p>
<p>Accordingly, revaluation tasks, such as latent learning, reward devaluation, and sensory preconditioning, examine whether animals appropriately adjust behavior following changes in <italic>R</italic>(<italic>s</italic>,<italic>a</italic>), such as a newly introduced reward (<xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2A</xref>). Analogously, contingency change (e.g., detour or contingency degradation) tasks examine whether animals appropriately adjust behavior following changes in <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>), such as a blocked passageway (<xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2B</xref>). Model-free RL is insensitive to these manipulations because it caches cumulative expected rewards and requires additional learning to update the stored <italic>V</italic>. Conversely, model-based RL, which uses the one-step model directly to compute <italic>V</italic> at decision time, reacts immediately and flexibly to any experience that affects it. Note that the difference in behavior on these types of tasks predicted by the algorithms is categorical, and not a question of degree or learning speed. In particular, because of the representations they learn and update, model-based algorithms can make the correct choice following these manipulations without any further retraining (i.e. so long as they learn locally about the new contingency or value, they can immediately make appropriate choices in distal parts of the state space), whereas model-free algorithms cannot (in general, they must first experience trajectories starting from the test state and leading to the state with the changed value or transition contingency). Animals sometimes fail to correctly update behavior following revaluations, consistent with inflexible, model-free caching schemes [<xref ref-type="bibr" rid="pcbi.1005768.ref038">38</xref>]. However, findings that in other circumstances animals can indeed flexibly adapt their behavior following such manipulations (without any further retraining–e.g. tested on the very first trial, or without feedback) has long been interpreted as evidence for their use of internal models, as in model-based RL or similar methods [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref023">23</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref039">39</xref>]. A key goal of this article is to interrogate this assumption, and to consider neural mechanisms that, despite falling short of full model-based RL, might support such behavioral flexibility.</p>
<fig id="pcbi.1005768.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Grid-world representation of Tolman’s tasks.</title>
<p>Dark grey positions represent maze boundaries. Light grey positions represent maze hallways. a) Latent learning: following a period of randomly exploring the maze (starting from S) the agent is notified that reward has been placed in position R. We examine whether the agent’s policy immediately updates to reflect the shortest path from S to R. b) Detour: after the agent learns to use the shortest path to reach a reward state R from state S, a barrier is placed in state B. After the agent is notified that state B is no longer accessible from its neighboring state, we examine whether its policy immediately updates to reflect the new shortest path to R from S.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>The puzzle of model-based learning and its neural substrates</title>
<p>A further set of rodent lesion studies have used reward devaluation tasks to suggest that apparently model-based and model-free behaviors (i.e., behavior that is either flexible or insensitive following reward devaluation) depend on dissociable sets of brain areas (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref040">40</xref>]; <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1B</xref>). This led to the hypothesis (e.g., [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref032">32</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref041">41</xref>]) that these two forms of reinforcement learning depend on competing systems in the brain—the dopaminergic TD system previously described, plus a second–less clearly understood–circuit supporting model-based behavior. But how is this latter computation carried out in the brain?</p>
<p>A number of fairly abstract theories have been based around explicit computation of the state-action value based on some form of <xref ref-type="disp-formula" rid="pcbi.1005768.e004">Eq 3</xref>, e.g. by learning an estimate of the one-step transition function, <italic>P</italic>(<italic>s</italic>’|<italic>s</italic>,<italic>a</italic>) and the immediate reward function <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) and using them iteratively to compute the future value by tree search, value iteration, or Bayesian inference [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref043">43</xref>]. These theories have not spoken in detail about the neural implementation of these computations, but an accompanying presumption has been that the model-based system does not rely on a dopaminergic prediction error signal. This is because the TD prediction error of <xref ref-type="disp-formula" rid="pcbi.1005768.e005">Eq 4</xref> (for <italic>γ</italic> &gt; 0, which is the parameter regime needed to explain phasic dopamine’s signature responses to the anticipation as well as receipt of reward [<xref ref-type="bibr" rid="pcbi.1005768.ref044">44</xref>]) is specifically useful for directly learning long-run cumulative values <italic>V</italic>. In contrast, the idea of model-based learning is to <italic>derive</italic> these values iteratively by stringing together short-term predictions from a learned one-step model [<xref ref-type="bibr" rid="pcbi.1005768.ref012">12</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref045">45</xref>]. Note that the prediction error normally thought to be reported by dopamine neurons is not appropriate here: the prediction error signal for updating the <italic>immediate</italic> reward model <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) is like <xref ref-type="disp-formula" rid="pcbi.1005768.e005">Eq 4</xref> but with <italic>γ</italic> = 0, which is not consistent with anticipatory phasic dopamine responses. (However, correlates of prediction errors for <italic>γ</italic> = 0 have been observed using human fMRI [<xref ref-type="bibr" rid="pcbi.1005768.ref046">46</xref>]). Furthermore, the hypothesized process of adding these rewards up over anticipated trajectories at choice time, such as by value iteration or tree search, has no counterpart in model-free choice. Instead, learning from anticipatory TD errors stores complete long-run values (e.g., in corticostriatal synapses), requiring no further computation at choice time.</p>
<p>However, neither the rodent lesion data nor another body of work studying the neural correlates of model-based learning in humans suggests such a clean differentiation between the dopaminergic-striatal circuit (supposed to support TD) and some other presumably non-dopaminergic substrate for model-based learning. Instead, lesions suggest each type of learning is supported by a different subregion of striatum, together with connected regions of cortex (<xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1B</xref>) and basal ganglia. This suggests that putatively model-based and model-free systems may map onto adjacent but structurally parallel cortico-basal ganglionic loops [<xref ref-type="bibr" rid="pcbi.1005768.ref033">33</xref>], thus perhaps involving analogous (striatal) computations operating over distinct (cortical) input representations [<xref ref-type="bibr" rid="pcbi.1005768.ref047">47</xref>].</p>
<p>Also contrary to a strict division between systems, both dorsomedial and dorsolateral striatal territories have similar interrelationships with dopamine [<xref ref-type="bibr" rid="pcbi.1005768.ref048">48</xref>], though the involvement of their differential dopaminergic input in devaluation sensitivity has not been completely assessed [<xref ref-type="bibr" rid="pcbi.1005768.ref049">49</xref>]. Research on humans’ learning in a two-step MDP (which has similar logic to devaluation studies) supports the causal involvement of dopamine in model-based learning [<xref ref-type="bibr" rid="pcbi.1005768.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref010">10</xref>]. Furthermore, dopaminergic recordings in rodents [<xref ref-type="bibr" rid="pcbi.1005768.ref011">11</xref>] (though see [<xref ref-type="bibr" rid="pcbi.1005768.ref037">37</xref>]), and neuroimaging of prediction error signals in human striatum [<xref ref-type="bibr" rid="pcbi.1005768.ref006">6</xref>] suggest that these signals integrate model-based evaluations.</p>
<p>Altogether, the research reviewed here supports the idea that model-based evaluations are at least partly supported by the same sort of dopaminergic-striatal circuit thought to support TD learning, though perhaps operating in separate cortico-striatal loops. This suggestion, if true, provides strong hints about the neural basis of model-based behavior. However, for the reasons discussed above, this also seems puzzlingly inconsistent with the abstract, textbook [<xref ref-type="bibr" rid="pcbi.1005768.ref050">50</xref>] picture of model-based learning by <xref ref-type="disp-formula" rid="pcbi.1005768.e004">Eq 3</xref>.</p>
<p>Several more neurally explicit theories of some aspects of model-based computation have been advanced, which go some way toward resolving this tension by incorporating a dopaminergic component. Doya [<xref ref-type="bibr" rid="pcbi.1005768.ref051">51</xref>] introduced a circuit by which projections via the cerebellum perform one step of forward state prediction, which activates a dopaminergic prediction error for the anticipated state. The candidate action can then be accepted or rejected by thresholding this anticipated prediction error against some aspiration level. It is unclear, however, how this one-step, serial approach can be generalized to tasks involving stochastic state transitions, direct comparison between multiple competing actions, or rewards accumulated over multiple steps (as in tasks like [<xref ref-type="bibr" rid="pcbi.1005768.ref052">52</xref>]).</p>
<p>A similar idea has arisen from recordings in spatial tasks, where the firing of place cells along trajectories ahead of the animal suggests a hippocampal basis for a similar (though multi-step) state anticipation process, potentially driving evaluation of these candidate states using learned reward values in ventral striatum [<xref ref-type="bibr" rid="pcbi.1005768.ref053">53</xref>]. It is, however, unclear how this activity fits into a larger circuit for accumulation of these evaluations and comparison between options.</p>
<p>Finally, another candidate approach is based on the Dyna framework discussed above. In this case, model-generated experience can be played back “off-line,” e.g. between trials or during rest. These ersatz experiences can, in turn, drive dopaminergic prediction errors and updating of striatal <italic>Q</italic> values using the same mechanisms as real experience. As noted above, given sufficient off-line replay, this can achieve the same effect as model-based planning; in particular, it can update <italic>Q</italic> values following revaluation and other manipulations [<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref054">54</xref>]. However, without a more traditional “on-line” planning component, this approach degrades (to that of basic, model-free <italic>Q</italic> learning) when there is insufficient time or resources for off-line replay, and when truly novel situations are encountered [<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>].</p>
<p>Here we propose and analyze a different family of approaches to these problems, which relate to the above proposals in that they incorporate elements of upstream predictive input to ventral striatum, and also of a different and more-flexible approach to offline updates. The proposed approach, based on the SR, builds even more directly on the standard TD learning model of dopaminergic-striatal circuitry.</p>
</sec>
<sec id="sec005">
<title>The successor representation</title>
<p>The research reviewed above suggests that flexible, seemingly model-based choices may be accomplished using computations that are homologous to those used in model-free RL. How can this be? In fact, it is known that evaluations with some features of model-based learning can result from TD learning over a different input representation. As shown by Dayan [<xref ref-type="bibr" rid="pcbi.1005768.ref015">15</xref>], <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref> can reformulated as:
<disp-formula id="pcbi.1005768.e007">
<alternatives>
<graphic id="pcbi.1005768.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>Here, <italic>M</italic><sup><italic>π</italic></sup> is a matrix of expected cumulative discounted <italic>future state occupancies</italic>, measuring the cumulative time expected to be spent in each future state <italic>s</italic>′, if one were to start in some state <italic>s</italic> and follow policy <italic>π</italic> (<xref ref-type="fig" rid="pcbi.1005768.g003">Fig 3</xref>):
<disp-formula id="pcbi.1005768.e008">
<alternatives>
<graphic id="pcbi.1005768.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mspace width="0.25em"/><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <inline-formula id="pcbi.1005768.e009"><alternatives><graphic id="pcbi.1005768.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula> if its argument is true and 0 otherwise. Thus, this form rearranges the expectation over future trajectories in <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref> by first computing expected occupancies for each state, then summing rewards obtained, via actions, in each state over these.</p>
<fig id="pcbi.1005768.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Example state representations.</title>
<p>a) Agent position (rodent image) in a maze whose hallways are indicated by grey. b) Punctate representation of the agent’s current state. Model-free behavior results from TD computation applied to this representation c,d) Possible successor representations of agent’s state. Model-based behavior may result from TD applied to this type of representation. The successor representation depends on the action selection policy the agent is expected to follow in future states. The figures show the representation of the current state under a random policy (c) versus a policy favoring rightward moves (d).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g003" xlink:type="simple"/>
</fig>
<p><italic>M</italic><sup><italic>π</italic></sup> can also be used as a set of basis functions for TD learning of values. Specifically, we represent each state using a vector of features given by the corresponding row of <italic>M</italic> (<xref ref-type="fig" rid="pcbi.1005768.g003">Fig 3</xref>), i.e. by the future occupancies expected for each state <italic>s</italic>′. Then we approximate <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>) by some weighted combination of these features:
<disp-formula id="pcbi.1005768.e010">
<alternatives>
<graphic id="pcbi.1005768.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>Comparing Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e007">5</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e010">7</xref> demonstrates this approximation will be correct when the weight <italic>w</italic>(<italic>s</italic>′) for each successor state corresponds to its one-step reward, averaged over actions in <italic>s</italic>′, ∑<sub><italic>a</italic></sub><italic>π</italic>(<italic>a</italic>|<italic>s</italic>′)<italic>R</italic>(<italic>s</italic>′,<italic>a</italic>). One way to learn these weights is using standard TD learning (adapted for linear function approximation rather than the special case of a punctate state representation). In particular, following a transition <italic>s</italic> → <italic>s</italic>′, each index <italic>i</italic> of <italic>w</italic> is updated:
<disp-formula id="pcbi.1005768.e011">
<alternatives>
<graphic id="pcbi.1005768.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>Here, <italic>δ</italic> is defined as in <xref ref-type="disp-formula" rid="pcbi.1005768.e005">Eq 4</xref>. Note that in the algorithms discussed below, the agent must estimate the successor matrix <italic>M</italic><sup><italic>π</italic></sup> from experience. If the feature matrix <italic>M</italic><sup><italic>π</italic></sup> were known and static, a simpler alternative to Eq (<xref ref-type="disp-formula" rid="pcbi.1005768.e011">8</xref>) for <italic>w</italic> would be to learn the one-step rewards by a delta rule on the immediate reward <italic>R</italic>. Since the successor representation is just a particular case of a linear feature vector for TD learning, the advantage of learning weights by the TD rule of <xref ref-type="disp-formula" rid="pcbi.1005768.e011">Eq 8</xref> is that weights learned this way will estimate value <italic>V</italic><sup><italic>π</italic></sup> for any feature matrix <italic>M</italic>, such as estimates of the successor matrix <italic>M</italic><sup><italic>π</italic></sup> prior to convergence (<xref ref-type="supplementary-material" rid="pcbi.1005768.s001">S1 Fig</xref>).</p>
<p>Altogether, this algorithm suggests a strategy for providing different inputs into a common dopaminergic/TD learning stage to produce different sorts of value predictions (see also [<xref ref-type="bibr" rid="pcbi.1005768.ref016">16</xref>]). In particular, whereas model-free valuation may arise from TD mapping of a punctate representation of the current state (<xref ref-type="fig" rid="pcbi.1005768.g003">Fig 3B</xref>) in sensory and motor cortex to values in dorsolateral striatum (<xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1B</xref>), at least some aspects of model-based valuation may arise by analogous TD mapping of the successor representation (<xref ref-type="fig" rid="pcbi.1005768.g003">Fig 3C and 3D</xref>) in prefrontal cortex or hippocampus to values in dorsomedial striatum (<xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1B</xref>). This is possible because the successor matrix <italic>M</italic> has a predictive aspect reflecting knowledge of the state transitions <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>), at least in terms of aggregate occupancy, separate from the state/action rewards <italic>R</italic>(<italic>s</italic>,<italic>a</italic>).</p>
<p>This approach may thus offer a solution to how flexible, seemingly model-based choices can be implemented, and indeed can arise from the same dopaminergic-striatal circuitry that carries out model-free TD learning. What remains to be shown is whether algorithms based on this strategy–applying the SR as input to TD learning–can produce the full range of model-based behaviors. In the remainder of this paper, we simulate the behavior of such algorithms to explore this question.</p>
<p>To simulate learning using the SR, we need to also simulate how the successor matrix <italic>M</italic><sup><italic>π</italic></sup> is itself produced from experience. <italic>M</italic><sup><italic>π</italic></sup> can be defined through a recursive equation that is directly analogous to Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e002">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e003">2</xref>:
<disp-formula id="pcbi.1005768.e012">
<alternatives>
<graphic id="pcbi.1005768.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where <bold>1</bold><sub><italic>s</italic></sub> is the vector of all zeros except for a 1 in the sth position and <italic>T</italic><sup><italic>π</italic></sup> is the one-step state transition matrix that is dependent on <italic>π</italic>, <italic>T</italic><sup><italic>π</italic></sup>(<italic>s</italic>,<italic>s</italic>′) = ∑<sub><italic>a</italic></sub><italic>π</italic>(<italic>a</italic>|<italic>s</italic>) <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>).</p>
<p>Similar to how approaches to estimating <italic>V</italic> are derived from Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e002">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e003">2</xref>, one could derive analogous approaches to estimating <italic>M</italic><sup><italic>π</italic></sup> from <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>. Specifically, one could utilize a “model-based” approach that would learn <italic>T</italic><sup><italic>π</italic></sup> and use it iteratively to derive a solution for <italic>M</italic><sup><italic>π</italic></sup>. Alternatively, a TD learning approach could be taken to learn <italic>M</italic><sup><italic>π</italic></sup> directly, without use of a one-step model <italic>T</italic><sup><italic>π</italic></sup>. (This approach is analogous to model-free TD methods for learning <italic>V</italic>, though it is arguably not really model-free since <italic>M</italic><sup><italic>π</italic></sup> is itself a sort of long-run transition model.) This TD learning approach would cache rows of M and update them after transitioning from their corresponding states, by moving the cached row closer to a one-sample estimate of the right hand side of <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>. Lastly, such TD updates could also occur offline, using simulated or previously experienced samples. This approach for learning <italic>M</italic><sup><italic>π</italic></sup> would be comparable to the Dyna approach for learning <italic>V</italic>. The three models we consider below correspond to these three different possibilities.</p>
<p>Finally, note that SR-based algorithms have favorable computational properties; in particular, at choice time, given <italic>M</italic><sup><italic>π</italic></sup> (e.g. if it is learned and cached rather than computed from a one-step model), SR can compute values <italic>V</italic><sup><italic>π</italic></sup> with a single dot product (e.g., a single layer of a linear neural network, <xref ref-type="disp-formula" rid="pcbi.1005768.e010">Eq 7</xref>), analogous to model-free TD algorithms. This is in contrast to the multiple steps of iterative computation required at choice time for computing value via <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref> in standard model-based approaches. This comes at the cost of storing the successor matrix <italic>M</italic><sup><italic>π</italic></sup>: if S is the number of states in the task, the SR matrix has a number of entries equal to <italic>S</italic><sup>2</sup>. Such entries of <italic>M</italic><sup><italic>π</italic></sup> can be stored as the (all-to-all) set of weights from a single layer of a neural network mapping input states to their successor representation.</p>
</sec>
</sec>
<sec id="sec006" sec-type="results">
<title>Results</title>
<p>In the following sections, we explore the behavioral consequences of each of these strategies. We structure the results as follows. For each learning method, we first present the algorithm. Then we present the results of simulations using that algorithm. The purpose of simulations is to verify our qualitative reasoning about the behavior of the algorithm and illustrate how the algorithm’s behavior compares to that of model-based dynamic programming methods. These simulations also suggest experiments that could be used to identify whether an animal or human were planning using such a strategy. Each task that we simulate is designed to be a categorical test of the algorithm. Following some change in the task to which the agent must respond, some of the algorithms can arrive at the correct decision without additional experience, but other algorithms cannot. Such failures are due to the computational properties of the algorithms themselves and are thus parameter-independent. To ensure that this is the case, for each simulation presented in the results, we have verified that the qualitative result can be observed robustly under a wide range of parameter settings. In general, there are parameter settings under which models, which are demonstrated below to succeed in a given task, can be made to fail it. However, there are <italic>no parameter settings</italic> under which a model that is shown below to fail a given task will pass it (<xref ref-type="supplementary-material" rid="pcbi.1005768.s003">S1 Table</xref>).</p>
<p>For each algorithm, we discuss its biological plausibility as well as how that algorithm’s performance lines up with that of animals.</p>
<sec id="sec007">
<title>Algorithm 1: The original successor representation (SR-TD)</title>
<p>The original SR [<xref ref-type="bibr" rid="pcbi.1005768.ref015">15</xref>] (which we call SR-TD) constructs the future state occupancy predictions <italic>M</italic><sup><italic>π</italic></sup> using a TD learning approach. This approach caches rows of <italic>M</italic><sup><italic>π</italic></sup> and incrementally updates them after transitioning from their corresponding states. Specifically, following each state transition <italic>s</italic> → <italic>s</italic>′ each element of row <italic>s</italic> is updated as follows:
<disp-formula id="pcbi.1005768.e013">
<alternatives>
<graphic id="pcbi.1005768.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <bold>1</bold><sub><italic>s</italic></sub> is the vector of all zeros except for a 1 in the sth position. <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:) is used as input to another TD learning stage, this time to learn the weights <italic>w</italic> for predicting expected future value from the state occupancy vector. To simulate SR-TD, we have the agent learn <italic>M</italic><sup><italic>π</italic></sup> and <bold><italic>w</italic></bold> in parallel, updating each (according to Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e013">10</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e011">8</xref>, respectively) at each transition; and sample actions according to an <italic>ϵ</italic>-greedy policy (see <xref ref-type="sec" rid="sec036">Methods</xref>).</p>
</sec>
<sec id="sec008">
<title>Simulation results</title>
<sec id="sec009">
<title>SR-TD can solve some reward revaluation tasks</title>
<p>SR-TD is able to produce behavior analogous to model-based learning in some reward revaluation tasks that categorically defeat simple TD learning. To demonstrate this, we simulated the behavior of SR-TD in a grid-world version of Tolman’s <italic>latent learning</italic> task (<xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2A</xref>). The agent first explores the grid-world randomly, during which it learns the successor matrix <italic>M</italic><sup><italic>π</italic></sup> corresponding to a random policy. Next, it learns that reward is available at position R (importantly, by being placed repeatedly at R and receiving reward, but not experiencing trajectories leading there from any other location). This experience induces prediction errors that cause the agent to update its weight vector, <bold><italic>w</italic></bold> in the position corresponding to the rewarded state. Finally, in a test probe, we allow the agent to form values by multiplying its current version of <italic>M</italic><sup><italic>π</italic></sup> with <bold><italic>w</italic></bold>, and measure whether (immediately on the first test trial following reward training) those values would produce a policy reflective of the shortest path from position S to R.</p>
<p><xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4B</xref> shows SR-TD performance on a latent learning task: SR-TD can, without further learning, produce a new policy reflecting the shortest path to the rewarded location (<xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2B</xref>). As a comparison to SR-TD’s performance, we also simulated the behavior of a simpler foil algorithm that represents model-free (or, actually, limited model-based) performance. This algorithm applied TD learning updates to non-predictive, punctate, state representations to estimate <italic>V</italic>. As with the SR, we permitted this algorithm to convert state values to state-action values by using a single-step of model-based look-ahead. Although this algorithm’s performance is representative of the failure of fully model-free algorithms at solving these revaluation tasks, we designed it to go beyond a vanilla model-free TD algorithm by allowing a single-step of model-based lookahead. This is analogous to a limited sort of model-based learning that has been suggested previously to be implemented by the basal ganglia and cerebellum [<xref ref-type="bibr" rid="pcbi.1005768.ref051">51</xref>]. <xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4A</xref> shows that this algorithm cannot solve latent learning problems: it learns nothing about paths around the maze from the reward training, and would have to discover the path to the new reward from scratch by additional exploration.</p>
<fig id="pcbi.1005768.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Behavior of SR-TD.</title>
<p>a) One-step of model-based lookahead combined with TD learning applied to punctate representations cannot solve the latent learning task. Median value function (grayscale) and implied policy (arrows) are shown immediately after the agent learns about reward in latent learning task. b) SR-TD can solve the latent learning task. Median value function (grayscale) and implied policy (arrows) are shown immediately after the agent learns about reward in latent learning task. c) SR-TD can only update predicted future state occupancies following direct experience with states and their multi-step successors. For instance, if SR-TD were to learn that s” no longer follows s’, it would not be able to infer that state s” no longer follows state s. Whether animals make this sort of inference is tested in the detour task. d) SR-TD cannot solve detour problems. Median value function (grayscale) and implied policy (arrows) are shown after SR-TD encounters barrier in detour task. SR-TD fails to update decision policy to reflect the new shortest path.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>SR-TD cannot solve transition revaluation tasks</title>
<p>However, SR-TD is limited in its ability to react correctly to other seemingly similar manipulations. Because <italic>M</italic><sup><italic>π</italic></sup> reflects long-run cumulative state occupancies, rather than the individual one-step transition distribution, <italic>P</italic>(<italic>s</italic>’|<italic>s</italic>,<italic>a</italic>), SR-TD cannot adjust its valuations to local changes in the transitions without first updating <italic>M</italic><sup><italic>π</italic></sup> at different locations. This inflexibility prevents SR-TD from flexibly adjusting value estimates after learning about changes in transition structure (“transition revaluation”; <xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4B</xref>). Consider a grid-world version of Tolman’s detour task (Figs <xref ref-type="fig" rid="pcbi.1005768.g002">2B</xref> and <xref ref-type="fig" rid="pcbi.1005768.g004">4D</xref>). Here, following an exploration period during which the agent is able to form an estimate of <italic>M</italic><sup><italic>π</italic></sup> under a random policy, the agent is trained to seek reward at R, starting from S. Later, a blockade is introduced at B. Again, the agent is allowed to experience this change only <italic>locally</italic>, by repeatedly being dropped in the state next to the barrier, attempting the action that leads to it and learning that the states to the right are no longer accessible from this state. This experience causes the agent to update <italic>M</italic><sup><italic>π</italic></sup> for the state immediately next to the barrier. However, despite this update, the rows of <italic>M</italic><sup><italic>π</italic></sup> corresponding to the states that lie along a path between the start state and the state next to the barrier remain unchanged. From <xref ref-type="disp-formula" rid="pcbi.1005768.e013">Eq 10</xref>, it can be seen that these updates can only occur from direct experience, i.e., a series of new trajectories starting at these states that encounter the barricade. SR-TD fails to reduce the value of these states (<xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4D</xref>), and thus would approach the barricade rather than taking a detour on the first visit back to S. As shown in supplemental materials, the depth-limited model-free algorithm also fails this test (<xref ref-type="supplementary-material" rid="pcbi.1005768.s003">S1 Table</xref>). A fully model-based algorithm (not shown) does make the correct choice in this case.</p>
</sec>
</sec>
<sec id="sec011">
<title>Interim discussion</title>
<sec id="sec012">
<title>Biological plausibility</title>
<p>The reward learning stage of this rule (learning weights <bold><italic>w</italic></bold> to map <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:) to <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>)) is the standard dopaminergic TD rule, Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e005">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e011">8</xref>, operating over a new input. The update rule for that input, <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:), is also based on a TD learning rule, but here applied to learning to predict cumulative future state occupancies. This uses a vector-valued error signal to update an entire row of <italic>M</italic><sup><italic>π</italic></sup> at each step. Crucially, despite the functional similarity between this rule and the TD update prescribed to dopamine, we do not suggest that dopamine carries this second error signal. Neurally, this sort of learning might, instead, be implemented using Hebbian associative learning between adjacent consecutive states [<xref ref-type="bibr" rid="pcbi.1005768.ref055">55</xref>], with decaying eligibility traces (like TD(1)) to capture longer-run dependencies. Lastly, although we have defined the successor representation over tabular representations of states, is also possible to combine the SR with function approximation and distributed representations in order to reduce its dimensionality [<xref ref-type="bibr" rid="pcbi.1005768.ref021">21</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref056">56</xref>].</p>
</sec>
<sec id="sec013">
<title>Behavioral adequacy</title>
<p>SR-TD is capable of solving some reward revaluation experiments. For similar reasons, SR-TD can solve sensory preconditioning (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref037">37</xref>]) and reward devaluation tasks (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref035">35</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref036">36</xref>]), both of which turn on an analogous ability to update behavior when state transition probabilities are held constant but reward values are changed. Evidence for model-based behavior in animals and humans has typically come from these types of tasks, suggesting that SR-TD could underlie a good proportion of behavior considered to be model-based. However, SR-TD is incapable of solving seemingly analogous tasks that require replanning under a transition rather than a reward change. Because there is at least some evidence from the early literature [<xref ref-type="bibr" rid="pcbi.1005768.ref057">57</xref>] that animals can adapt correctly to such detour situations, we suggest that this inflexibility prevents SR-TD, on its own, from being a plausible mechanism for the full repertoire of model-based behavior.</p>
</sec>
</sec>
<sec id="sec014">
<title>Algorithm 2: Dynamic recomputation of the successor representation (SR-MB)</title>
<p>Here, we explore a novel “model-based” approach, SR-MB, for constructing the expected state occupancy vector <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:). SR-MB learns a one-step transition model, <italic>T</italic><sup><italic>π</italic></sup> and uses it, at decision time, to derive a solution to <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>. One key constraint on a model-based implementation suggested by the data is that the computation should be staged in a way consistent with the architecture suggested by <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1A</xref>. Specifically, the TD architecture in <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1A</xref> suggests that, because the states are represented in cortex (or hippocampus) and weights (which capture information about rewards) and value are represented in downstream cortico-striatal synapses and medium spiny striatal neurons, information about <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) and <italic>V</italic>(<italic>s</italic>) should not be used in the online construction of states. For the SR approach, this implies that <italic>M</italic> be constructed without using direct knowledge of <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) or <italic>V</italic>(<italic>s</italic>). As we see below, this serial architecture–a cortical state-prediction stage providing input for a subcortical reward-prediction stage–if true, would impose interesting limitations on the resulting behavior.</p>
<p>To construct <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:), SR-MB first learns the one-step state transition matrix <italic>T</italic><sup><italic>π</italic></sup>, implemented in our simulations through separate learning of <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>,<italic>a</italic>) as well as <italic>π</italic>(<italic>a</italic>|<italic>s</italic>), the agent’s previously expressed decision policy (see <xref ref-type="sec" rid="sec036">Methods</xref>). Prior to each decision, <italic>T</italic><sup><italic>π</italic></sup> is used to compute a solution to <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>. This solution can be expressed in either of two forms. A given row, <italic>s</italic>, of <italic>M</italic> can be computed individually as the sum of n-step transition probabilities starting from state <italic>s</italic>:
<disp-formula id="pcbi.1005768.e014">
<alternatives>
<graphic id="pcbi.1005768.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>…</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>Alternatively, matrix inversion can be used to solve for the entire successor matrix at once:
<disp-formula id="pcbi.1005768.e015">
<alternatives>
<graphic id="pcbi.1005768.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:msup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p>To implement SR-MB, we use <xref ref-type="disp-formula" rid="pcbi.1005768.e015">Eq 12</xref>. However, this is not a mechanistic commitment of the model, since <xref ref-type="disp-formula" rid="pcbi.1005768.e014">Eq 11</xref> is equivalent.</p>
<p>Given <italic>M</italic><sup><italic>π</italic></sup>, SR-MB learns the reward prediction weights <bold><italic>w</italic></bold> and forms <italic>V</italic> and <italic>Q</italic> values in the same way as SR-TD.</p>
<p>Note finally that this scheme is similar to solving <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref> for on-policy values <italic>V</italic><sup><italic>π</italic></sup> by value iteration, except that the sums are rearranged to put state prediction upstream of reward prediction, as per <xref ref-type="disp-formula" rid="pcbi.1005768.e010">Eq 7</xref> and in line with the neural architecture of <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1A</xref>. The max operator in <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref> prevents a similar rearrangement that would allow this scheme to be used for off-policy optimal values <italic>V</italic><sup>*</sup> (<xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref>), as discussed below. The restriction to on-policy values <italic>V</italic><sup><italic>π</italic></sup> is the major empirical signature of this version of the model.</p>
</sec>
<sec id="sec015">
<title>Simulation results</title>
<sec id="sec016">
<title>SR-MB can solve transition revaluation tasks</title>
<p>Using an updated <italic>T</italic><sup><italic>π</italic></sup> to recompute <italic>M</italic><sup><italic>π</italic></sup> at decision time ensures that behavior is sensitive to changes in the transition structure. We demonstrate this by showing that unlike SR-TD, SR-MB successfully solves Tolman’s detour task in addition to latent learning. In the detour task, after being dropped in the state next to the barrier, SR-MB updates its estimate of <italic>P</italic>(<italic>s</italic>’|<italic>sa</italic>) for the <italic>sa</italic> leading into the barrier. This new <italic>P</italic>(<italic>s</italic>’|<italic>sa</italic>) is then combined with the estimate of <italic>π</italic>(<italic>s</italic>,<italic>a</italic>) (learned through prior experience including initial random exploration of the maze and then trials starting in S and ending in R) to compute <italic>T</italic><sup><italic>π</italic></sup>. The row of <italic>T</italic><sup><italic>π</italic></sup> corresponding to the state next to the barrier at this point now no longer predicts transitioning into the barrier state. When the agent then recomputes <italic>M</italic><sup><italic>π</italic></sup> by <xref ref-type="disp-formula" rid="pcbi.1005768.e015">Eq 12</xref>, using the updated <italic>T</italic><sup><italic>π</italic></sup>, rows of <italic>M</italic><sup><italic>π</italic></sup> corresponding to the path between the start state and the barrier no longer predict future occupancy of states on the other side of the barrier. When <italic>M</italic><sup><italic>π</italic></sup> is then used to compute <italic>V</italic><sup><italic>π</italic></sup>, the values immediately (on the first test trial after the barrier is encountered) result in a policy reflective of the new shortest path (<xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5A</xref>.)</p>
<fig id="pcbi.1005768.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Behavior of SR-MB.</title>
<p>a) SR-MB can solve the detour task. Median value function (grayscale) and implied policy (arrows) after SR-MB encounters barrier. b) SR-MB determines successor states relative to a cached policy. If SR-MB learned from previous behavior that it will always select action a1, the value of s would become insensitive to changes in reward at s2’. C) Novel “policy” revaluation task. After a phase of random exploration, we place a reward in location R1. The agent completes a series of trials that alternatively start from locations S1 and S2 and end when R1 is reached. We then place a larger reward in location R2 and record the agent’s value function and implied policy upon encountering it. d) SR-MB cannot solve the novel “policy” revaluation task. Median value function and implied policy recorded immediately after SR-MB learns about reward placed in location R2. Notice that if the agent were to start from location S1, its policy would suboptimally lead it to the smaller reward at R1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec017">
<title>SR-MB is limited by policy dependence</title>
<p>SR-MB is able to solve both tasks that have been used as examples of planning in animals and humans. We thus sought to determine whether there were any tasks, perhaps not yet explored in the empirical literature, that could differentiate it from approaches that utilize “full” model-based value iteration. A key feature of SR-MB, as well as SR-TD, is that it computes <italic>M</italic><sup><italic>π</italic></sup> with respect to a policy <italic>π</italic>. For SR-MB, <italic>T</italic><sup><italic>π</italic></sup> is computed using <italic>π</italic>(<italic>a</italic>|<italic>s</italic>), which is learned through observation of previous actions. Because <italic>M</italic><sup><italic>π</italic></sup> is policy dependent, so are the value estimates that it produces. SR-TD and SR-MB are thus “on-policy” methods–their estimates of <italic>V</italic><sup><italic>π</italic></sup> can be compared to the estimates of a traditional model-based approach used to solve <xref ref-type="disp-formula" rid="pcbi.1005768.e002">Eq 1</xref>. A key limitation of “on-policy” methods is that their value estimates are made with respect to the policy under which learning occurred. This is an important limitation, as we will see below, because new learning about parts of the MDP can moot the previously learned policy <italic>π</italic> (and hence invalidate the associated successor matrix and values).</p>
<p>For the successor representation strategy, this limitation could be bypassed if we could compute <italic>M</italic><sup>*</sup>–an “off-policy” successor matrix based on the successor states expected under the <italic>optimal</italic> policy–which would, in turn provide the state input for solving for <italic>V</italic><sup>*</sup>, the optimal long-run values. However, given the architectural constraints we have suggested, computing <italic>M</italic><sup>*</sup> is not straightforward. In particular, defining <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref> with respect to the optimal policy would require replacing <italic>T</italic><sup><italic>π</italic></sup> with <italic>T*</italic>: the one-step transition matrix corresponding to the optimal policy, <italic>T</italic><sup>*</sup>(<italic>s</italic>,<italic>s</italic>’) = <italic>P</italic>(<italic>s</italic>’|<italic>s</italic>,<italic>a</italic><sup>*</sup>), where <italic>a</italic><sup>*</sup> is the action in state <italic>s</italic> that maximizes future rewards. Computing <italic>a</italic><sup>*</sup> online, however, would require access to <italic>R</italic>(<italic>s</italic>,<italic>a</italic>), which would violate the suggested serial staging of the computation, that <italic>M</italic> be constructed without using reward information. Policy dependence of value predictions thus defines both the major limitation as well as the empirical signature of SR-MB.</p>
</sec>
<sec id="sec018">
<title>SR-MB cannot solve novel policy revaluation tasks</title>
<p>What are the behavioral implications of SR-MB estimating values using the policy that was expressed during learning? Consider a situation where state <italic>s</italic>’ can be reached from state <italic>s</italic> using action <italic>a</italic>, but SR-MB learned from past behavior that <italic>π</italic>(<italic>a</italic>|<italic>s</italic>) is near 0 (<xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5B</xref>). Then it will not include rewards at <italic>s</italic>’ in the value of <italic>s</italic>, even if separately learning (say, by visiting <italic>s</italic>’ but not in a trajectory starting from <italic>s</italic>) that <italic>s</italic>’ is the most rewarding state reachable from <italic>s</italic>. In other words, caching of the policy at <italic>s</italic> blinds the agent (without further exploration and relearning) to changes in the reward function that should change that policy. Value iteration based on <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref> does not have this limitation because the max operation would look ahead to the reward at <italic>s</italic>’ to determine whether it should be included.</p>
<p>These considerations suggest a novel revaluation task (<xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5C</xref>). Here, SR-MB first performs many trials where R1 is rewarded, starting from both S1 as well as a new start position, S2. This experience causes the agent to learn a policy <italic>π</italic>(<italic>s</italic>,<italic>a</italic>) that reflects moves away from the bottom right corner of the maze. Next, a larger reward is introduced at R2. Despite having learned about this reward (by starting at R2 and updating <bold><italic>w</italic></bold> at the state corresponding to this location), because computation of <italic>M</italic><sup><italic>π</italic></sup> utilizes <italic>T</italic><sup><italic>π</italic></sup>, which reflects moves away from the bottom right corner of the maze, <italic>M</italic><sup><italic>π</italic></sup> does not predict the future occupancy of state R2 from any state along a path starting at S1. Because of this, the values of these states do not update to include the newly updated parts of the weight vector corresponding to position R2. Thus, despite the higher reward in R2, the agent would choose to head toward R1 from S1, due to caching of the incorrect policy in <italic>T</italic><sup><italic>π</italic></sup> (<xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5D</xref>). Thus, this task defeats SR-MB (as well as, shown in supplemental materials, the depth limited model-free planners and SR-TD; see also [<xref ref-type="bibr" rid="pcbi.1005768.ref022">22</xref>]), though it can be solved by standard model-based learning using <xref ref-type="disp-formula" rid="pcbi.1005768.e003">Eq 2</xref>.</p>
</sec>
</sec>
<sec id="sec019">
<title>Interim discussion</title>
<sec id="sec020">
<title>Biological plausibility</title>
<p>SR-MB requires the brain to compute <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:) from <italic>T</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:) for a particular state <italic>s</italic> under evaluation. Although in our simulations, we used <xref ref-type="disp-formula" rid="pcbi.1005768.e015">Eq 12</xref> to compute the entire successor matrix at once, this is not a mechanistic commitment of the model. For instance, recurrent neural networks offer a simple way to compute <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,:) based on spreading activation implementing <xref ref-type="disp-formula" rid="pcbi.1005768.e014">Eq 11</xref>. Consider a network with one node for each state and the weight between node <italic>s</italic> and node <italic>s</italic>’ set to <italic>γT</italic><sup><italic>π</italic></sup>(<italic>s</italic>,<italic>s</italic>′). If node <italic>s</italic> is activated, then at each successive time step, the network activity will represent each successive component of the sum. Indeed, this model arose in the early connectionist literature [<xref ref-type="bibr" rid="pcbi.1005768.ref017">17</xref>].</p>
<p>Alternatively, it is also well established that recurrent neural networks can perform matrix inversion by relaxing to an attractor [<xref ref-type="bibr" rid="pcbi.1005768.ref058">58</xref>], making a computation based on <xref ref-type="disp-formula" rid="pcbi.1005768.e015">Eq 12</xref> plausible as well.</p>
<p>A final mechanism for computing <italic>M</italic> from <italic>T</italic> would involve sampling transitions from <italic>T</italic> offline and using them to iteratively update <italic>M</italic> according to <xref ref-type="disp-formula" rid="pcbi.1005768.e013">Eq 10</xref>, the SR-TD update. In the following section, we explore how an approach based on this idea, when carried out over state-actions, can be used to solve the “off-policy” planning problem as well.</p>
</sec>
<sec id="sec021">
<title>Behavioral adequacy</title>
<p>SR-MB produces the two behaviors that are considered signatures of planning in the empirical literature: immediate adjustment of decision policy following learned changes in either reward structure (latent learning) or transition structure (detour problem). The novel policy revaluation task demonstrates that SR-MB still produces errors that could in principle be behaviorally detectable, but have not been exercised by standard experimental tasks [<xref ref-type="bibr" rid="pcbi.1005768.ref059">59</xref>].</p>
</sec>
</sec>
<sec id="sec022">
<title>Algorithm 3: Off-policy experience resampling (SR-Dyna)</title>
<p>Here we introduce a third approach towards solving <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>, SR-Dyna, which can be compared to Sutton’s Dyna approach [<xref ref-type="bibr" rid="pcbi.1005768.ref027">27</xref>] for solving Eqs <xref ref-type="disp-formula" rid="pcbi.1005768.e002">1</xref> and <xref ref-type="disp-formula" rid="pcbi.1005768.e003">2</xref>. Akin to how Dyna replays experienced transitions offline to update estimates of <italic>V</italic>(<italic>s</italic>), SR-Dyna replays experienced transitions to update the successor matrix. When this approach is combined with an ‘off-policy’ update rule, similar to Q learning, to update the successor matrix offline, it is capable of solving the off-policy planning problem. Utilizing this type of update, however, requires us to work with a state-action version of the successor representation, <italic>H</italic>, which can be used directly to form <italic>Q</italic> values [<xref ref-type="bibr" rid="pcbi.1005768.ref060">60</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref061">61</xref>]. The key idea here is to define future occupancy not over states but over state/action pairs, <italic>sa</italic>. Analogous to <xref ref-type="disp-formula" rid="pcbi.1005768.e007">Eq 5</xref>, <italic>Q</italic><sup><italic>π</italic></sup> can then be expressed:
<disp-formula id="pcbi.1005768.e016">
<alternatives>
<graphic id="pcbi.1005768.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>a</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p><italic>H</italic> is a matrix of expected cumulative discounted future state-action visitations, i.e. given that you are starting with state <italic>s</italic> and action <italic>a</italic>, the cumulative (discounted) expected number of times you will encounter each other state/action pair:
<disp-formula id="pcbi.1005768.e017">
<alternatives>
<graphic id="pcbi.1005768.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>]</mml:mo><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula></p>
<p><italic>H</italic> can then be used as a linear basis for learning <italic>Q</italic>(<italic>s</italic>,<italic>a</italic>), using the SARSA TD algorithm to learn a weight for each column of <italic>H</italic>. In particular, when state-action <italic>s</italic>’<italic>a</italic>’ is performed after state action <italic>sa</italic>, a prediction error is calculated and used to update <bold><italic>w</italic></bold>:
<disp-formula id="pcbi.1005768.e018">
<alternatives>
<graphic id="pcbi.1005768.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi>′</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<disp-formula id="pcbi.1005768.e019">
<alternatives>
<graphic id="pcbi.1005768.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Like <italic>M</italic>, <italic>H</italic> can be defined recursively:
<disp-formula id="pcbi.1005768.e020">
<alternatives>
<graphic id="pcbi.1005768.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="0.5em"/><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
where <italic>T</italic><sup><italic>π</italic></sup> is the one-step state-action transition matrix, <inline-formula id="pcbi.1005768.e021"><alternatives><graphic id="pcbi.1005768.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. As with SR-TD, this recursion can be used to derive a TD-like update rule by which an estimate of <italic>H</italic> can be iteratively updated:
<disp-formula id="pcbi.1005768.e022">
<alternatives>
<graphic id="pcbi.1005768.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e022" xlink:type="simple"/>
<mml:math display="block" id="M22">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula></p>
<p>As with SR-MB, it is also possible to derive <italic>H</italic> from <italic>T</italic><sup><italic>π</italic></sup>(<italic>sa</italic>,<italic>s</italic>′<italic>a</italic>′) using an explicit “model-based” solution analogous to <xref ref-type="disp-formula" rid="pcbi.1005768.e012">Eq 9</xref>. However, here, we investigate the approach of updating <italic>H</italic> off-line (e.g., between trials or during rest periods) using replay of experienced trajectories (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref062">62</xref>]). The key assumption we make is that this off-line replay can sequentially activate both the state and reward (cortical and basal ganglia) stages of <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1A</xref>, giving rise to an off-policy update of <italic>H</italic> with respect to the policy <italic>π</italic><sup>*</sup> that is optimal given the current rewards. By comparison, as articulated above, we assumed such policy maximization was not possible when computing the successor representation <italic>M</italic> on-line for SR-MB, since this entire computation was supposed to happen in cortex at decision time, upstream of the striatal reward learning stage.</p>
<p>Following each transition, SR-Dyna stores the sample (<italic>s</italic>,<italic>a</italic>,<italic>s</italic>’). Then in between decisions, SR-Dyna randomly selects (with a recency weighted bias) <italic>k</italic> samples (with replacement). For each sample, it updates H as follows:
<disp-formula id="pcbi.1005768.e023">
<alternatives>
<graphic id="pcbi.1005768.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e023" xlink:type="simple"/>
<mml:math display="block" id="M23">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where
<disp-formula id="pcbi.1005768.e024">
<alternatives>
<graphic id="pcbi.1005768.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi>a</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>That is, when <italic>H</italic> updates from previously experienced samples, it performs an off-policy update using the best action it could have chosen, rather than the one it actually chose.</p>
</sec>
<sec id="sec023">
<title>Simulation results</title>
<sec id="sec024">
<title>SR-Dyna can solve policy revaluation tasks</title>
<p>Given sufficient sampling (large enough <italic>k</italic>), this off-policy updating not only permits SR-Dyna to solve the detour task, but also to solve the novel policy revaluation task (<xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6A</xref>). In the policy revaluation task, after the agent is introduced to the new reward in R2, and updates <bold><italic>w</italic></bold>, we permit it to draw 10,000 random samples from memory and perform an update for each. With each sample drawn, SR-Dyna partially adjusts the predictions of future state occupancies from a given state action <italic>sa</italic>, so that they become closer to the predictions of future state occupancies from <italic>s</italic>’<italic>a</italic><sup>*</sup>, where <italic>a</italic><sup>*</sup> is chosen with consideration of the newly updated <bold><italic>w</italic></bold>. Such updates thus allow SR-Dyna to re-learn a new version of <italic>H</italic><sup><italic>π</italic></sup>, corresponding to the policy that would result from repeated choices under the updated <bold><italic>w</italic></bold>. Once updated, <italic>H</italic><sup><italic>π</italic></sup> comes to reflect prediction of future states reflective of a policy that moves towards the new highest reward in the bottom right of the maze. When the updated <italic>H</italic><sup><italic>π</italic></sup> is then used to compute new values (on the first test trial following the policy retraining), those values result in a policy that would bring the agent from S to the new highest reward in R2 along the shortest path. This simulation demonstrates that SR-Dyna can thus produce behavior identical to “full” model-based value iteration in this task (as well as the other revaluation tasks previously simulated, as shown in <xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6</xref>). However, it has the potential advantage that updating can take place fully off-line and thus offload computation to situations that may be ecologically convenient such as sleep or wakeful rest.</p>
<fig id="pcbi.1005768.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005768.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison of SR-Dyna and Dyna-Q.</title>
<p>Median value function (grayscale) and implied policy after each algorithm (row) learns about relevant change in each of the 3 tasks (column). Both SR-Dyna (a) and Dyna-Q (b) can solve all 3 tasks when a sufficient number of samples backed up. c) Without a sufficient number of samples, SR-Dyna can still solve the latent learning task. d) Without a sufficient number of samples, Dyna-Q cannot solve any of the 3 tasks.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec025">
<title>Time constraints can distinguish SR-Dyna from Dyna-Q</title>
<p>SR-Dyna is capable of behaving equivalently to dynamic programming and tree-search in that it can solve transition and policy revaluation tasks. We were thus interested in how it could be differentiated experimentally. Sutton’s original Dyna algorithm (Dyna-Q) differs from value iteration and tree search in that its ability to pass revaluation experiments is dependent on having enough time offline to perform sufficient number of sample backups. Given enough time between decisions, sufficient replay can occur and Dyna-Q can pass any type of revaluation task (<xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6B</xref>). In contrast, without sufficient offline replay, Dyna-Q degrades to a model-free agent and it cannot pass any revaluation task (<xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6D</xref>). SR-Dyna is similarly differentiated from tree-search and value iteration in that its flexibility depends on completing a sufficient number of sample backups offline. As demonstrated above, given a sufficient number of backups, SR-Dyna can pass any type of revaluation (<xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6A</xref>). Without sufficient replay, its performance degrades to that of SR-TD–it can pass reward revaluation but fails transition and policy revaluation (<xref ref-type="fig" rid="pcbi.1005768.g006">Fig 6C</xref>). SR-Dyna is thus differentiated from Dyna-Q in that, unlike Dyna-Q, without sufficient replay, it can still pass reward revaluation; that is, it retains a certain degree of “model-based” flexibility even in the degraded case. These predictions could be tested with experimental designs aimed at preventing replay by manipulating the length of rest periods or the difficulty of distractor tasks [<xref ref-type="bibr" rid="pcbi.1005768.ref027">27</xref>].</p>
<p>Time constraints or distractor tasks at decision time can also disambiguate the different algorithms. Tree-search and value iteration take time and effort at decision time, whereas SR-Dyna can support rapid action selection by inspecting its lookup table.</p>
</sec>
</sec>
<sec id="sec026">
<title>Interim discussion</title>
<sec id="sec027">
<title>Biological plausibility</title>
<p>Following real experience, SR-Dyna uses a similar update rule as SR-TD, yet uses it to operate over state-actions rather than states. This is plausible given that the same Hebbian learning principles could operate over cortical or hippocampal representations of state/action conjunctions just as well as they could over states.</p>
<p>As with Dyna-Q, SR-Dyna dovetails nicely with neural evidence about memory replay. Specifically, the widely demonstrated phenomenon of reactivation during rest or sleep of sequences of hippocampal activity seen during prior experiences [<xref ref-type="bibr" rid="pcbi.1005768.ref063">63</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref064">64</xref>], seems well suited to support the sort of off-line updates imagined by both Dyna approaches. (Although we have not simulated realistic hippocampal replay dynamics here, the Dyna approaches can learn from experiences replayed in arbitrary order.) The successor matrix updated by SR-Dyna might itself exist in the recurrent connections of hippocampal neurons [<xref ref-type="bibr" rid="pcbi.1005768.ref018">18</xref>], though another intriguing possibility is that it is instead stored in prefrontal cortex (as in <xref ref-type="fig" rid="pcbi.1005768.g001">Fig 1B</xref>). This second possibility lines up neatly with complementary system theories in the memory literature, according to which such hippocampal replay plays a role in post-encoding consolidation of memories by restructuring how information is represented across neocortical networks [<xref ref-type="bibr" rid="pcbi.1005768.ref065">65</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref066">66</xref>]. Such a connection should be explored in future research.</p>
</sec>
<sec id="sec028">
<title>Behavioral adequacy</title>
<p>Given sufficient replay, SR-Dyna is capable of producing behavior as flexible as that of full model-based value iteration. As with Dyna-Q, practical applications of SR-Dyna in larger environments will require developing sophisticated methods for selecting which samples to replay (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref067">67</xref>]). We intend to develop such methods in future work.</p>
</sec>
</sec>
</sec>
<sec id="sec029" sec-type="conclusions">
<title>Discussion</title>
<p>Despite evidence that animals engage in flexible behaviors suggestive of model-based planning, we have little knowledge of how these computations are actually performed in the brain. Indeed, what evidence we have–particularly concerning the involvement of dopamine in these computations–seems difficult to reconcile with the standard abstract computational picture of planning by tree search using a learned model. We have here proposed variants of the SR that can address this question, serving as empirically consistent mechanisms for some or indeed all of the behaviors associated with model-based learning. Moreover, these are each built as utilizing a common TD learning stage for reward expectancies, allowing them to fit within the systems-level picture suggested by rodent lesion studies, and also explaining the involvement of dopamine in model-based valuation. In particular, they each envision how model-based learning could arise from the same dopaminergic TD learning associated with simple model-free learning, operating over a different and more elaborated cortical input representation.</p>
<sec id="sec030">
<title>Accounting for TD circuitry in apparently model-based behavior</title>
<p>More specifically, our motivation to develop this approach was based on three related sets of findings in the empirical literature. The first are that lesions to dorsomedial striatum prevent animals from adjusting preferences following reward revaluation [<xref ref-type="bibr" rid="pcbi.1005768.ref005">5</xref>]. In contrast, lesions to neighboring dorsolateral striatum cause rats to maintain devaluation sensitivity, even following overtraining [<xref ref-type="bibr" rid="pcbi.1005768.ref040">40</xref>]. In the framework presented here, neurons in dorsomedial striatum could represent values derived by applying TD learning to the successor representation and neurons in dorsolateral striatum could represent values derived by applying TD to tabular representations. Lesions to dorsomedial striatum would thus force the animal to work with values in dorsolateral striatum, derived from tabular representations and thus not sensitive to devaluation. In contrast, lesions to dorsolateral striatum would cause the brain to work with values derived from the SR, which are devaluation-sensitive.</p>
<p>The second set of findings include several reports that the phasic DA response (or analogous prediction error related BOLD signals in humans) tracks apparently model-based information [<xref ref-type="bibr" rid="pcbi.1005768.ref006">6</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref011">11</xref>]. We have focused our simulations on choice behavior, and have not presented our theories' analogous predictions about the responses of neurons, such as DA cells, thought to signal decision variables. However, whenever the SR algorithms' expectations about action values incorporate "model-based" information (such as latent learning, <xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4A</xref>) neural signals related to those predictions and to prediction errors would be similarly informed. Thus the theories predict systematic expectancy-related effects in the modeled dopamine response, tracking the differences in choice preference relative to the standard “model-free” accounts, which are blind to reward contingencies in these tasks.</p>
<p>A third distinct set of findings also speaks to a relationship between dopamine and model-based learning. These are reports that several measures of dopaminergic efficiency (both causal and correlational) track the degree to which human subjects engage in model-based decision strategies in both multistep reward revaluation tasks and multiplayer games [<xref ref-type="bibr" rid="pcbi.1005768.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref010">10</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref068">68</xref>]. One possibility is that these effects reflect strengthened vs. weakened phasic dopaminergic signaling, which in our model controls reward learning for SR-based “model-based” estimates in dorsomedial striatum. However, this account does not explain the specificity of these effects to measures of putative model-based (vs. model-free) learning. These effects may instead be related to functions of dopamine other than prediction error signaling, such as <italic>tonic</italic> dopamine’s involvement supporting working memory [<xref ref-type="bibr" rid="pcbi.1005768.ref069">69</xref>] or its hypothesized role controlling the allocation of cognitive effort [<xref ref-type="bibr" rid="pcbi.1005768.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref070">70</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref071">71</xref>].</p>
</sec>
<sec id="sec031">
<title>Other potential explanations</title>
<p>The framework outlined in this paper is not the only direction toward a neurobiologically explicit theory of putatively model-based behavior, nor even the only suggestion explaining the involvement of dopamine. As discussed above and pointed out in [<xref ref-type="bibr" rid="pcbi.1005768.ref028">28</xref>], Sutton’s original Dyna algorithm–in which experience replayed offline is used to update action values <italic>V</italic> or <italic>Q</italic> directly–offers another avenue by which seemingly model-based flexibility can be built on the foundation of the standard prediction error model of dopamine. This is a promising piece of the puzzle, but exclusive reliance on replay to underpin all behavioral flexibility seems unrealistic. Among our innovations here is to suggest that replay can also be used to learn and update a successor representation, which then confers many of the other advantages of model-based learning (such as flexibility in the face of reward devaluation) without the dependence on further replay to replan. Furthermore, the addition of SR to the Dyna framework explains a number of phenomena that replay, on its own, does not. For instance, given that Dyna-Q works with a single set of cached Q values, updated through both experience and replay, it is not clear how it could, on its own, explain the apparent segregation of revaluation sensitive and insensitive value estimates in dorsomedial and dorsolateral striatum [<xref ref-type="bibr" rid="pcbi.1005768.ref005">5</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref040">40</xref>].</p>
<p>Another potential solution to some of the puzzles motivating this work is that dopamine could have a role in action selection, as part of a circuit for partial model-based action evaluation [<xref ref-type="bibr" rid="pcbi.1005768.ref047">47</xref>]. According to this idea, dopamine neurons could compute a prediction error measuring the difference between the value of the current state and the future value of a predicted successor state, caused by a given candidate action. The size of this prediction error could then determine whether the action is performed. This mechanism would endow the brain with a single step of model-based prediction. However, it is not straightforward how this sort of approach could underlie model-based learning in tasks requiring more than a single step of prediction, and accordingly our simulations (see <xref ref-type="fig" rid="pcbi.1005768.g004">Fig 4A</xref> and supplemental materials) show that it cannot solve any of the revaluation tasks considered here, which all probe for deeper search through the state space. A recent study provided convincing behavioral evidence that humans sometimes simplify model-based action selection by combining just one step of state prediction with cached successor state values [<xref ref-type="bibr" rid="pcbi.1005768.ref072">72</xref>]. Yet this same study along with others [<xref ref-type="bibr" rid="pcbi.1005768.ref052">52</xref>] also provided evidence that humans can plan through more than one step and thus are not confined to this approximation. It is also not straightforward how this sort of mechanism could endow model-based predictions in cases where stochasticity requires consideration of “trees” of possible future states.</p>
<p>Nevertheless, by elucidating a more general framework in which a predictive state representation may feed into downstream dopaminergic reward learning, we view our framework as fleshing out the spirit of this suggestion while also addressing these issues. We similarly realize other conceptual suggestions in the literature suggesting that more flexible model-based like behavior may arise not through tree-search like planning, but rather by applying model-free RL to more sophisticated state representations [<xref ref-type="bibr" rid="pcbi.1005768.ref073">73</xref>]. A more specific application of this idea, [<xref ref-type="bibr" rid="pcbi.1005768.ref074">74</xref>] demonstrated that a sophisticated representation that includes reward history can produce model-based like behavior in the two-step reward revaluation task. The successor representation adds to this work by clarifying for any task’s transition structure, the precise representation that can be used to generate model-based behavior.</p>
<p>Relatedly, because it places model-based state prediction in the input to a standard TD learning circuit, our framework could easily be extended to include several modules with inputs corresponding to several different types or granularities of models: for instance, varying degrees of temporal abstraction corresponding to different time discount factors in <xref ref-type="disp-formula" rid="pcbi.1005768.e008">Eq 6</xref> [<xref ref-type="bibr" rid="pcbi.1005768.ref046">46</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref075">75</xref>]. This would parallel a number of other recent suggestions that different striatal loops model the world at different levels of hierarchical abstraction [<xref ref-type="bibr" rid="pcbi.1005768.ref047">47</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref076">76</xref>], while also harmonizing the somewhat underspecified model-based evaluation process these theories assume with the predominant temporal difference account of striatal learning.</p>
</sec>
<sec id="sec032">
<title>Multiplicity and arbitration</title>
<p>Although our presentation culminated with proposing an algorithm (SR-Dyna) that can in principle perform equivalently to full model-based learning using value iteration, this need not be the only goal and there need not be only a single answer. The behaviors associated with model-based learning may not have unitary sources in the brain but may instead be multiply determined. All of the algorithms we have considered are viable candidate pieces of a larger set of decision systems. Notably, the experiments we have highlighted as suggesting striatal or dopaminergic involvement in “model-based” learning and inspiring the present work all use extremely shallow planning problems (e.g. operant lever pressing, two-stimulus Pavlovian sequences, or two-step MDPs) together with reward revaluation designs. Even SR-TD is sufficient to explain these. It may well be that planning in other tasks, like chess, or in spatial mazes, is supported by entirely different circuits that really do implement something like tree search; or that they differentially require replay, like SR-Dyna. Also, although replay-based approaches go a long way, value computation at choice time using more traditional model-based approaches is likely needed at the very least to explain the ability to evaluate truly novel options (like the value of “tea jelly”; [<xref ref-type="bibr" rid="pcbi.1005768.ref077">77</xref>]) using semantic knowledge. Some evidence that rodents may use more than just replay to compute values, even in spatial tasks, comes from findings that the prevalence of sharp-wave-ripples, a putative sign of replay, is inversely related to the prevalence of vicarious trial and error behaviors, a process thought to be involved in decision-time value computation, potentially by standard MB dynamic programming or alternatively SR-MB [<xref ref-type="bibr" rid="pcbi.1005768.ref078">78</xref>].</p>
<p>Relatedly, if the brain might cache both endpoint decision variables like <italic>Q</italic>, or their precursors like <italic>M</italic>, update either or both with off-line replay, and optionally engage in further model-based recomputation at choice time, then the arbitration or control question of how the brain prioritizes all this computation to harvest rewards most effectively and efficiently becomes substantially more complicated than previously considered. The prioritization of replay–which memories to replay when–becomes particularly important. The particular ordering and dynamics of replay are also outside our modeling here: in order to focus our investigation on the simplest behavioral predictions of SR-Dyna, we chose the simplest, naive sampling scheme in which the agent replays a single state-action-state transition with uniformly random probability. This sampling strategy is not a mechanistic commitment (nor of course does it reflect the dynamics of realistic hippocampal replay trajectories), and we expect that like Dyna-Q, SR-Dyna would work even more efficiently given more sophisticated replay prioritization schemes. In this regard, we expect that the prioritization of replay, like the arbitration between model-based vs model-free tradeoffs [<xref ref-type="bibr" rid="pcbi.1005768.ref001">1</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref041">41</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref079">79</xref>], might operate according to the principles of efficient cost-benefit management. We expect that in addition to more typical observed patterns of replay, such a scheme may be able to explain cases where the replayed sequences are not a simple reflection of the animal’s current policy [<xref ref-type="bibr" rid="pcbi.1005768.ref080">80</xref>]. The current model is robust to differences in replay but would need to be extended with a more principled and detailed replay model to address these questions.</p>
</sec>
<sec id="sec033">
<title>Future experimental work</title>
<p>With simulations, we have presented experiments that could be used to elicit recognizable behavior form the different algorithms proposed here. Although we ruled out the simplest approach, SR-TD, due to its inflexibility, it is worth more carefully considering the evidence against it. The main counterexamples to SR-TD are transition revaluation and detour tasks. Apart from the classic work of Tolman and Honzik [<xref ref-type="bibr" rid="pcbi.1005768.ref057">57</xref>], the original results of which are actually quite mixed (see [<xref ref-type="bibr" rid="pcbi.1005768.ref081">81</xref>]), there is surprisingly little evidence to go on. A number of different studies have shown that healthy animals will normally choose the shortest alternative route after learning about a blockade preventing a previously preferred route (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref082">82</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref084">84</xref>]). However, in these studies, the animal learns about the blockade after starting from the maze starting location. Thus, unlike in our simulations in which the animal learns about the blockade in isolation, animals in these tasks would have the opportunity to learn from direct experience that maze locations leading up to the blockade are no longer followed by maze locations further along the previously preferred path. Such tasks could thus potentially be solved by SR-TD. Studies that show that animals will take a shortcut to a goal that is discovered along a preferred path present a somewhat cleaner test for SR-TD [<xref ref-type="bibr" rid="pcbi.1005768.ref085">85</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref086">86</xref>]; however it is often difficult to interpret a potential role of exploration or visual (rather than cognitive map) guidance in the resulting behavior. Work in humans, however, seems to more clearly suggest an ability to solve detour tasks without re-learning [<xref ref-type="bibr" rid="pcbi.1005768.ref087">87</xref>]. Simon and Daw [<xref ref-type="bibr" rid="pcbi.1005768.ref024">24</xref>] for instance directly assessed SR-TD’s fit to human subjects’ choice adjustments in a changing spatial maze, and found it fit poorly relative to traditional model-based learning.</p>
<p>Overall, additional careful work that measures how animals respond to transition changes, learned in isolation, is needed. Whereas Tolman’s other early reward revaluation experiments (latent learning) have been conceptually replicated in many modern, non-spatial tasks like instrumental reward devaluation and sensory preconditioning, the same is not true of detours. Indeed, the modern operant task that is often presented as analogous to detours, so-called instrumental contingency degradation (e.g., [<xref ref-type="bibr" rid="pcbi.1005768.ref088">88</xref>]), is not functionally equivalent. In such tasks, the association between an action and its outcome is degraded through introduction of background rewards. However, because the information about the changed contingency is not presented separately from the rest of the experience about actions and their rewards, unlike all the other tests discussed here, contingency degradation as it has been studied in instrumental conditioning can actually be solved by a simple model-free learner that re-learns the new action values. The puzzle here is actually not how animals can solve the task, but why they should ever fail to solve it. This has thus led to a critique not of model-based but of model-free learning theories [<xref ref-type="bibr" rid="pcbi.1005768.ref047">47</xref>].</p>
<p>In any case, the modeling considerations proposed here suggest that more careful laboratory work on “transition revaluation” type changes to detect use of SR-TD, is warranted. Similarly, “policy revaluations” along the lines of that in <xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5</xref> would be useful to detect to what extent planning along the lines of SR-MB is contributing. Finally, although SR-Dyna in principle can perform model-based value computation, this depends on sufficient replay. The SR-Dyna hypothesis suggests the testable prediction that behavior should degrade to SR-TD under conditions when replay can contribute less. A number of experiments in the rodent literature have explored the behavioral deficits that result from interrupting sharp-wave ripples (events in which hippocampal replay is known to occur). Such manipulations have been shown to produce behavioral deficits that are consistent with the SR-Dyna hypothesis, yet not exclusively predicted by it. For instance, two studies found that suppression of hippocampal sharp-wave ripples during rest slows down acquisition of the correct behavioral policy in spatial learning tasks in which the task environment is static [<xref ref-type="bibr" rid="pcbi.1005768.ref089">89</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref090">90</xref>]. These results are consistent with the notion that the purpose of replay is to provide additional experience, which is used to update some representation relevant to learning. However, these results are not specifically diagnostic of the successor matrix. For instance, preventing replay would slow down policy acquisition for Dyna-Q as well as SR-Dyna (<xref ref-type="supplementary-material" rid="pcbi.1005768.s002">S2 Fig</xref>).</p>
<p>Another study found a more specific effect of suppressing hippocampal sharp wave ripples during performance of a task. Here, the manipulation caused learning deficits selective for a subset of trials in which animals faced a hidden-state problem. In particular, these were trials in which animals had to choose which direction to turn depending on the events of the previous trial [<xref ref-type="bibr" rid="pcbi.1005768.ref091">91</xref>]. Such tasks constitute hidden-state problems, in that the “state” required to make the correct choice cannot be deduced entirely from an animal’s immediate sensory experience. In RL terms, to solve these problems, the animals must construct an augmented internal state, distinct from a simple representation of the immediate sensory situation [<xref ref-type="bibr" rid="pcbi.1005768.ref092">92</xref>]. One interpretation of the experimental result is that blocking replay interfered with this internal state construction process.</p>
<p>This result resonates with our SR-Dyna proposal, which also posits that replay is involved in constructing the mapping between the sensory state and a different, augmented internal representation of it: the SR. However, our model as currently specified augments the state space with predictive features to support model-based flexibility, and does not currently address other sorts of elaborations of the state input that have been used in other work to facilitate learning in situations with hidden state or other uncertain sensory input [<xref ref-type="bibr" rid="pcbi.1005768.ref093">93</xref>–<xref ref-type="bibr" rid="pcbi.1005768.ref095">95</xref>]. Fully understanding these results therefore requires augmenting our model to address hidden state as well as state prediction. In fact, these two functions may be closely related: a number of approaches to the hidden state problem in the computational RL literature address it using predictive representations that are related to the SR [<xref ref-type="bibr" rid="pcbi.1005768.ref096">96</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref097">97</xref>].</p>
<p>In human studies, factors like the duration of off-task rest periods and presence of distractor tasks during such periods have been manipulated to extend, limit, or interfere with replay. Some evidence suggests that distractor tasks at decision time have no effect on reward revaluation [<xref ref-type="bibr" rid="pcbi.1005768.ref027">27</xref>], consistent with SR-Dyna. Other recent work has demonstrated that humans benefit from additional pre-decision time in revaluation tasks that closely resemble “policy revaluation” [<xref ref-type="bibr" rid="pcbi.1005768.ref098">98</xref>] and that this benefit recruits a network including the prefrontal cortex and basal ganglia. Such work is consistent with the predictions of both Dyna-Q as well as SR-Dyna accounts of value updating presented here.</p>
<p>Overall, future work will need to combine such manipulations of replay, with the three revaluation tasks imagined in this paper and demonstrate differences in the effects of manipulations on reward versus transition and policy revaluations. We have recently demonstrated, though without attempting to manipulate replay, that humans are worse at adjusting behavior following transition and policy revaluations compared to reward revaluations, suggesting that they may at least partially use either an SR-TD or SR-Dyna (with limited sample backups) strategy for evaluation [<xref ref-type="bibr" rid="pcbi.1005768.ref059">59</xref>].</p>
</sec>
<sec id="sec034">
<title>Neural substrate of the successor representation</title>
<p>We have suggested, on the basis of rodent lesion studies, that the SR may be encoded in parts of prefrontal cortex that project to dorosomedial striatum. However, we should note that recent work has also implicated the hippocampus as a potential site of the SR. Specifically, a state-state version of the SR can explain some properties of hippocampal place cells [<xref ref-type="bibr" rid="pcbi.1005768.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref099">99</xref>] as well as fMRI measures of the representation of visual stimuli in tasks where such stimuli are presented sequentially [<xref ref-type="bibr" rid="pcbi.1005768.ref100">100</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref101">101</xref>]. This work has largely built on ideas of the hippocampus in general as a site of cognitive map [<xref ref-type="bibr" rid="pcbi.1005768.ref102">102</xref>] as well as prior suggestions that hippocampal place cells may in fact encode the transition structure of the environment [<xref ref-type="bibr" rid="pcbi.1005768.ref103">103</xref>] and that such transition information may make them ideal basis functions for TD learning [<xref ref-type="bibr" rid="pcbi.1005768.ref104">104</xref>] If a state version of the SR exists in the hippocampus, we think it is reasonable that value weights would be leaned by neurons connecting the hippocampus to ventral striatum, in the same TD manner discussed in this paper.</p>
<p>However, we also think a case can be made for the prefrontal cortex as another candidate basis for an SR. In addition to the rodent lesion evidence reviewed in the introduction of this paper, the prefrontal cortex shares many cognitive-map properties observed in the hippocampus [<xref ref-type="bibr" rid="pcbi.1005768.ref105">105</xref>] and has been suggested to be the basis of state representations for reinforcement learning [<xref ref-type="bibr" rid="pcbi.1005768.ref093">93</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref106">106</xref>]. A number of human studies have demonstrated the PFC’s role in the representation of prospective goals [<xref ref-type="bibr" rid="pcbi.1005768.ref107">107</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref108">108</xref>]. Furthermore, unlike the hippocampus, parts of the PFC appear to be involved in action representation in addition to state representation [<xref ref-type="bibr" rid="pcbi.1005768.ref109">109</xref>], thus making it a candidate to hold a potential state-action version of the successor matrix. Importantly, these proposals are in no way mutually exclusive. For instance, recent work has demonstrated that hippocampal output is necessary for preserving PFC representations of task structure [<xref ref-type="bibr" rid="pcbi.1005768.ref110">110</xref>]. Overall, further experimental work will be required to determine whether either or indeed both these areas serves as the basis for the successor representation, and what specific roles they play in learning and representation.</p>
</sec>
<sec id="sec035">
<title>Connection to other cognitive processes</title>
<p>Finally, the SR may contribute to a number of other cognitive processes. Above we noted that there is evidence that areas of medial temporal lobe seem to encode predictive representations. In line with this, it has been noted that there is a close correspondence between the update rule used by SR-TD and update rules in the temporal context model of memory [<xref ref-type="bibr" rid="pcbi.1005768.ref019">19</xref>]. Also, recent approaches to reinforcement learning in the brain have advocated for a hierarchical approach in which punctate actions are supplemented by temporally abstract policies [<xref ref-type="bibr" rid="pcbi.1005768.ref111">111</xref>]. In this context, it has been suggested that the SR may be useful for discovering useful temporal abstractions by identifying bottlenecks in the state space that can then be used to organize states and action into a hierarchy [<xref ref-type="bibr" rid="pcbi.1005768.ref018">18</xref>,<xref ref-type="bibr" rid="pcbi.1005768.ref112">112</xref>]. The efficacy of the SR for model-based RL opens the possibility that the brain accomplishes planning, action chunking, and grouping episodic memories using a common mechanism.</p>
<p>Overall, this article has laid out a family of candidate mechanistic hypotheses for explaining the full range of behaviors typically associated with model-based learning, while connecting them with the circuitry for model-free learning as currently understood. In addition to the transition and policy revaluation behavioral experiments suggested above, future neuroimaging work could seek evidence for these hypotheses. Specifically, failures to flexibly update decision policies that are caused by caching of either the successor representation (as in SR-TD or SR-Dyna with insufficient replay) or a decision policy (as in SR-MB) should be accompanied by neural markers of non-updated future state occupancy predictions. Such neural markers could be identified using representational similarity analysis (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref113">113</xref>]), cross-stimulus suppression (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref114">114</xref>]) or through use of category specific, decodable, visual stimuli (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref115">115</xref>]). Similar work in experimental animals such as rodents (e.g. [<xref ref-type="bibr" rid="pcbi.1005768.ref116">116</xref>]) could use the full range of invasive tools to trace the inputs to dorsomedial vs. dorsolateral striatum, so as to examine the information represented there and how it changes following the various sorts of revaluation manipulations discussed here. As has been the case for model-free learning, the emergence of an increasingly clear and quantitative taxonomy of different candidate algorithms is likely to guide this work and help to elucidate the neural basis of model-based learning.</p>
</sec>
</sec>
<sec id="sec036" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec037">
<title>General simulation methods</title>
<p>All simulations were carried out in 10x10 (N = 100 states) grid-worlds in which the agent could move in any of the four cardinal directions, unless a wall blocked such a movement. States with rewards contained a single action. Upon selecting that action, the agent received the reward and was taken to a terminal state. Each task was simulated with each algorithm 500 times. For each simulation, we recorded the agent’s value function at certain points. For SR-Dyna, which worked with action values rather than state values, the state value function was computed as the max action value available in that state. Figures display the median value, for each state, over the 500 runs. To determine the implied policy for the median value function, we computed, for each state, which accessible successor state had the maximum median value.</p>
</sec>
<sec id="sec038">
<title>Specific task procedures</title>
<sec id="sec039">
<title>Latent learning task</title>
<p>The latent learning task was simulated in the grid-world environment shown in <xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2A</xref>. Starting from position S, the agent first took 25000 steps exploring the maze. After exploration, the reward in position R1 was raised to 10. To learn about the reward, the agent completed a single step, starting from position R1, 20 times. We then recorded the state value function.</p>
</sec>
<sec id="sec040">
<title>Detour task</title>
<p>The detour task was simulated using the grid-world environment shown in <xref ref-type="fig" rid="pcbi.1005768.g002">Fig 2B</xref>. Starting from position S, the agent first took 10000 steps exploring the maze. The reward in position R was then increased to 10. The agent then completed 5 trials, starting from position S that ended when the reward was reached. A wall was then added to in position B. To learn about the wall, the agent completed a single step, starting from the position immediately left of the wall, 40 times. We then recorded the state value function.</p>
</sec>
<sec id="sec041">
<title>Novel revaluation task</title>
<p>The novel revaluation task was simulated using the environment in <xref ref-type="fig" rid="pcbi.1005768.g005">Fig 5C</xref>. The agent first completed the entire latent learning task. After successfully reaching position R1 from position S, the agent then completed 20 trials. Each trial alternately started at S1 or S2 and ended when the agent reached position R1. We then set the reward in position R2 to 20. To learn about the new reward, the agent completed one step, starting from position R2, 20 times. We then recorded the state value function.</p>
</sec>
</sec>
<sec id="sec042">
<title>Additional details on algorithms</title>
<sec id="sec043">
<title>One-step look-ahead</title>
<p>The one-step look-ahead model stored an estimate of state-value function <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>). At the beginning of each simulation <italic>V</italic><sup><italic>π</italic></sup> was initilzled to <bold>0</bold>. Following each transition <italic>V</italic><sup><italic>π</italic></sup> was updated according to <xref ref-type="disp-formula" rid="pcbi.1005768.e005">Eq 4</xref>. Prior to each choice, Q-values for each action <italic>a</italic> in state <italic>s</italic> were then computed as <italic>Q</italic><sup><italic>π</italic></sup>(<italic>s</italic>,<italic>a</italic>) = <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>’) where <italic>s</italic>’ is the state that deterministically follows action a in state s. Note that leaving <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) out of this equation works because rewards are paired exclusively with actions in terminal states (and thus <italic>R</italic>(<italic>s</italic>,<italic>a</italic>) for non-terminal actions is 0).</p>
</sec>
<sec id="sec044">
<title>Original successor representation (SR-TD)</title>
<p>SR-TD computed <italic>V</italic><sup><italic>π</italic></sup>(<italic>s</italic>) using two structures: the successor matrix, <italic>M</italic><sup><italic>π</italic></sup>(<italic>s</italic>,<italic>s</italic>’) and a weight vector, <italic>w</italic>(<italic>s</italic>). At the beginning of each simulation, <italic>M</italic><sup><italic>π</italic></sup> was initialized as an identity matrix; however, rows corresponding to terminal states were set to 0. The weight vector was initialized as <bold><italic>w</italic></bold> = <bold>0</bold>. Following each transition, <italic>M</italic> and <bold><italic>w</italic></bold> were updated using Eqs (<xref ref-type="disp-formula" rid="pcbi.1005768.e011">8</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005768.e013">10</xref>). In implementing the update in <xref ref-type="disp-formula" rid="pcbi.1005768.e011">Eq 8</xref>, each element of the feature vector, <italic>M</italic>(<italic>s</italic>,:), was scaled by <italic>M</italic>(<italic>s</italic>,:) * <italic>M</italic>(<italic>s</italic>,:)<sup><italic>T</italic></sup>. This scaling permits the weight learning rate parameter to maintain a consistent interpretation as proportional step-size. Prior to each choice, <italic>V</italic> was computed using Eq (<xref ref-type="disp-formula" rid="pcbi.1005768.e010">7</xref>). Q-values for each action were computed the same as for the one-step look-ahead model.</p>
</sec>
<sec id="sec045">
<title>Recomputation of the successor matrix (SR-MB)</title>
<p>This algorithm starts each task with a basic knowledge of the ‘physics’ of grid-world: which successor state, <italic>s</italic>’, would follow each action <italic>sa</italic> in a situation in which <italic>sa</italic> is available (e.g. not blocked by a wall). It also stores and updates, for each state <italic>s</italic>, <italic>A</italic><sub><italic>s</italic></sub>, the set of actions currently available in state <italic>s</italic> as well as a policy <italic>π</italic>(<italic>a</italic>|<italic>s</italic>), which stores the probability of selecting action <italic>a</italic> in state <italic>s</italic> (as learned from the agent’s own previous choices). <italic>A</italic><sub><italic>s</italic></sub> was initialized to reflect all four cardinal actions being available in each state. Each row <italic>π</italic> were initialized as a uniform distribution over state-actions, <italic>π</italic>(<italic>a</italic>|<italic>s</italic>) = 0.25.</p>
<p>After performing action <italic>a</italic> in state <italic>s</italic> and transitioning to state <italic>s</italic>’, <italic>A</italic><sub><italic>s’</italic></sub> was updated to reflect which actions are available in state <italic>s</italic>’ and <italic>π</italic> is updated using a delta rule:
<disp-formula id="pcbi.1005768.e025">
<alternatives>
<graphic id="pcbi.1005768.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>:</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>:</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>α</italic><sub><italic>π</italic></sub> is a free parameter.</p>
<p>Prior to each choice, the model computed each row, s, of one-step transition matrix <italic>T</italic><sup><italic>π</italic></sup> as follows:
<disp-formula id="pcbi.1005768.e026">
<alternatives>
<graphic id="pcbi.1005768.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>π</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
where <bold>1</bold><sub><italic>s</italic>′</sub> is a vector zeros of length <italic>S</italic> with a 1 in position corresponding to state <italic>s</italic>’ and <italic>s</italic>’ is the state to which action <italic>a</italic> in state <italic>s</italic> deterministically leads. <italic>T</italic><sup><italic>π</italic></sup> was then used to compute <italic>M</italic> using <xref ref-type="disp-formula" rid="pcbi.1005768.e015">Eq 12</xref>. Computation of <italic>V</italic> and <italic>Q</italic> was then the same as in SR-TD.</p>
</sec>
<sec id="sec046">
<title>Episodic replay algorithm (SR-Dyna)</title>
<p>This algorithm computed <italic>Q</italic>(<italic>sa</italic>) using two structures: a state-action successor matrix, <italic>H</italic>(<italic>sa</italic>,<italic>s</italic>’<italic>a</italic>’) and weight vector <italic>w</italic>(<italic>sa</italic>). At the beginning of each simulation, the successor matrix <italic>H</italic> was initialized to an identity matrix; however rows corresponding to terminal states were set to <bold>0</bold>. The weight vector was initialized to <bold><italic>w</italic></bold> = <bold>0</bold>. The algorithm also stored every sample (<italic>s</italic>,<italic>a</italic>,<italic>s</italic>’). After performing action <italic>a</italic> in state <italic>s</italic> and transitioning to state <italic>s</italic>’ the sample (<italic>s</italic>’,<italic>a</italic>’,<italic>s</italic>’) was stored, and <italic>H</italic> and <italic>w</italic> were updated according to Eqs (<xref ref-type="disp-formula" rid="pcbi.1005768.e018">15</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005768.e022">17</xref>). Following each step, we also selected 10 one-step samples (according to recency weighted probabilities with replacement) from the stored history, and replayed each to update H according to <xref ref-type="disp-formula" rid="pcbi.1005768.e023">Eq 18</xref>. Following transitions in which a learned change occurred to either the reward function or available actions, <italic>k</italic> one-step samples were selected and used to update the model, where k was set to 10 in the insufficient replay condition and to 10000 in the sufficient replay condition. Samples were drawn by first selecting a state-action, <italic>sa</italic>, from a uniform distribution. A sample then drawn from the set of experienced samples, initiated in <italic>sa</italic>, according to an in, initiated from sa was then selected according to an exponential distribution with <italic>λ</italic> = 1/5.</p>
<p><bold>Dyna-Q</bold>: This algorithm stored <italic>Q</italic>(<italic>sa</italic>). At the beginning of each simulation, <italic>Q</italic> was initialized to <bold>0</bold>. The algorithm also stored every experienced sample (<italic>s</italic>,<italic>a</italic>,<italic>r</italic>,<italic>s</italic>’). After performing action <italic>a</italic> in state <italic>s</italic>, experiencing reward r, and transitioning to state <italic>s</italic>’ the sample (<italic>s</italic>’,<italic>a</italic>’,<italic>r</italic>,<italic>s</italic>’) was stored and Q was updated according to the Q-learning prediction error:
<disp-formula id="pcbi.1005768.e027">
<alternatives>
<graphic id="pcbi.1005768.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e027" xlink:type="simple"/>
<mml:math display="block" id="M27">
<mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi mathvariant="normal">′</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pcbi.1005768.e028">
<alternatives>
<graphic id="pcbi.1005768.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005768.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:mi>Q</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Following each step, as with SR-Dya, we also selected 10 one-step samples (according to recency weighted probabilities with replacement) from the stored history, and replayed each to update <italic>Q</italic> according to equation the update above. Following transitions in which a learned change occurred to either the reward function or available actions, <italic>k</italic> one-step samples were selected and used to update the model, where k was set to 10 in the insufficient replay condition and to 10000 in the sufficient replay condition. Samples were drawn the same way as in SR-Dyna.</p>
</sec>
<sec id="sec047">
<title>Parameters</title>
<p>All algorithms converted Q-values to actions using an <italic>ϵ</italic>-greedy policy which selects the highest-valued action with probability 1 – <italic>ϵ</italic>, and chooses randomly with probability <italic>ϵ</italic>. Parameters for all models and each simulation were varied and we observed that the qualitative results can be observed under a wide range of parameter settings (<xref ref-type="supplementary-material" rid="pcbi.1005768.s003">S1 Table</xref>). For the figures in the results section, the following parameters were used. For all models, <italic>ϵ</italic> was set to 0.1. In addition, all models used a discount parameter <italic>γ</italic> = 0.95. The three SR models used a weight learning rate parameter <italic>α</italic><sub><italic>w</italic></sub> = 0.3. Model Dyna-Q used a learning rate <italic>α</italic><sub><italic>Q</italic></sub> = 0.3. In addition to these parameters, SR-TD and SR-Dyna used a successor-matrix learning rate of <italic>α</italic><sub><italic>sr</italic></sub> = 0.3 and SR-MB used a policy learning rate of <italic>α</italic><sub><italic>π</italic></sub> = 0.1.</p>
</sec>
</sec>
</sec>
<sec id="sec048">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005768.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Advantage of TD learning over direct reward learning of weights.</title>
<p>a) Task environment. On each trial, the agent was placed in state S. Trials ended when the agent reached state R, which contained a reward value of 10. Unlike the latent learning task in the main text, this task did not contain an exploratory period enabling the agent to learn the successor matrix prior to the introduction of reward. b) Number of steps on each trial for an agent learning weights using TD learning and an agent learning weights applying delta rule to the reward function. Plotted lines show average over 500 runs. 95% confidence intervals are contained within line thickness. Parameters for each of the two algorithms were set to those that minimized the average number of total steps over 80 trials. Such parameters were found by grid search over <italic>α</italic><sub><italic>sr</italic></sub> ∈ [.1,.3,.5,.7,.9], <italic>ϵ</italic> ∈ [0.1,0.3,0.5] and <italic>α</italic><sub><italic>w</italic></sub> ∈ [.1,.3,.5,.7,.9]. Both algorithms learned the SR using the SR-TD update. The “TD Learning” algorithm updated weights using TD learning. The “Reward Learning” algorithm updated weights by delta-rule learning on the immediate reward function. Specifically, after performing action <italic>a</italic> in state <italic>s</italic> and receiving reward <italic>r</italic>, the following update was performed: <italic>w</italic>(<italic>s</italic>) ← <italic>w</italic>(<italic>s</italic>) + <italic>α</italic><sub><italic>w</italic></sub>(<italic>r</italic> − <italic>w</italic>(<italic>s</italic>)).”</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005768.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Preventing replay slows acquisition for both SR-Dyna and Dyna-Q.</title>
<p>Both algorithms under the two sampling settings were simulated on the task displayed in <xref ref-type="supplementary-material" rid="pcbi.1005768.s001">S1 Fig</xref>. a) Results of simulations with SR-Dyna. b) Results of simulations with Dyna-Q. Both a) and b) show number of steps on each trial for agent permitted to replay 20 samples between each decision and an agent not permitted to replay any samples. Plotted lines show average over 500 runs. 95% confidence intervals are contained within shaded region around lines. For each algorithm and sample setting, we chose parameters that minimized average total number of steps over 80 trials by a grid search in the following range: <italic>α</italic><sub><italic>sr</italic></sub> ∈ [.1,.3,.5,.7,.9], <italic>ϵ</italic> ∈ [0.1,0.3,0.5], <italic>α</italic><sub><italic>w</italic></sub> ∈ [.1,.3,.5,.7,.9], and <italic>α</italic><sub><italic>Q</italic></sub> ∈ [.1,.3,.5,.7,.9].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005768.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005768.s003" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Robustness of simulation results to varying parameters.</title>
<p>Here, we display the results of simulating each task, using each algorithm under a wide variety of parameter settings. Each table below corresponds to a particular algorithm simulating a particular task. For a given parameter setting, the algorithm was simulated 500 times. A check indicates that the 500 run median value function produced by that parameter setting results in the optimal policy for the task. A cross indicates that it does not result in the optimal policy.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005768.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source>. <year>2005</year>;<volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1560" xlink:type="simple">10.1038/nn1560</ext-link></comment> <object-id pub-id-type="pmid">16286932</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Barto a</surname> <given-names>C</given-names></name>. <article-title>A model of how the basal ganglia generates and uses neural signals that predict reinforcement</article-title>. <source>Model Inf Process Basal Ganglia</source>. <year>1995</year>; <fpage>249</fpage>–<lpage>270</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>J Neurosci</source>. <year>1996</year>;<volume>16</volume>: <fpage>1936</fpage>–<lpage>1947</lpage>. <object-id pub-id-type="pmid">8774460</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Seeberger</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism</article-title>. <source>Science</source> (80-). <year>2004</year>;<volume>306</volume>: <fpage>1940</fpage>–<lpage>1943</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1102941" xlink:type="simple">10.1126/science.1102941</ext-link></comment> <object-id pub-id-type="pmid">15528409</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Ostlund</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>The role of the dorsomedial striatum in instrumental conditioning</article-title>. <source>Eur J Neurosci</source>. <year>2005</year>;<volume>22</volume>: <fpage>513</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2005.04218.x" xlink:type="simple">10.1111/j.1460-9568.2005.04218.x</ext-link></comment> <object-id pub-id-type="pmid">16045504</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Raymond</surname> <given-names>J</given-names></name>. <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <year>2011</year>;<volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Smittenaar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dopamine Enhances Model-Based over Model-Free Choice Behavior</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>75</volume>: <fpage>418</fpage>–<lpage>424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.03.042" xlink:type="simple">10.1016/j.neuron.2012.03.042</ext-link></comment> <object-id pub-id-type="pmid">22884326</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doll</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Bath</surname> <given-names>KG</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Variability in Dopamine Genes Dissociates Model-Based and Model-Free Reinforcement Learning</article-title>. <source>J Neurosci</source>. <year>2016</year>;<volume>36</volume>: <fpage>1211</fpage>–<lpage>1222</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1901-15.2016" xlink:type="simple">10.1523/JNEUROSCI.1901-15.2016</ext-link></comment> <object-id pub-id-type="pmid">26818509</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deserno</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Boehme</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buchert</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heinze</surname> <given-names>H-J</given-names></name>, <name name-style="western"><surname>Grace</surname> <given-names>AA</given-names></name>, <etal>et al</etal>. <article-title>Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2015</year>;<volume>112</volume>: <fpage>1595</fpage>–<lpage>600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1417219112" xlink:type="simple">10.1073/pnas.1417219112</ext-link></comment> <object-id pub-id-type="pmid">25605941</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharp</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Foerde</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Shohamy</surname> <given-names>D</given-names></name>. <article-title>Dopamine selectively remediates “model-based” reward learning: A computational approach</article-title>. <source>Brain</source>. <year>2015</year>;<volume>139</volume>: <fpage>355</fpage>–<lpage>364</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/awv347" xlink:type="simple">10.1093/brain/awv347</ext-link></comment> <object-id pub-id-type="pmid">26685155</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sadacca</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Midbrain dopamine neurons compute inferred and cached value prediction errors in a common framework</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.13665" xlink:type="simple">10.7554/eLife.13665</ext-link></comment> <object-id pub-id-type="pmid">26949249</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glascher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>States versus rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source>. Elsevier Ltd; <year>2010</year>;<volume>66</volume>: <fpage>585</fpage>–<lpage>595</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.04.016" xlink:type="simple">10.1016/j.neuron.2010.04.016</ext-link></comment> <object-id pub-id-type="pmid">20510862</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Multiple Forms of Value Learning and the Function of Dopamine BT—Neuroeconomics: Decision Making and the Brain</article-title>. <source>Neuroeconomics Decision Making and the Brain</source>. <year>2008</year>. pp. <fpage>367</fpage>–<lpage>387</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://books.google.com/books?hl=en&amp;lr=&amp;id=g0QPLzBXDEMC&amp;oi=fnd&amp;pg=PA367&amp;dq=balleine+neuroeconomics&amp;ots=i9afuLQDYl&amp;sig=usxp3lfOydDCxVhoXJXa_IFCPLU" xlink:type="simple">http://books.google.com/books?hl=en&amp;lr=&amp;id=g0QPLzBXDEMC&amp;oi=fnd&amp;pg=PA367&amp;dq=balleine+neuroeconomics&amp;ots=i9afuLQDYl&amp;sig=usxp3lfOydDCxVhoXJXa_IFCPLU</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>The algorithmic anatomy of model-based evaluation</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130478</fpage>–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0478" xlink:type="simple">10.1098/rstb.2013.0478</ext-link></comment> <object-id pub-id-type="pmid">25267820</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Improving Generalisation for Temporal Difference Learning: The Successor Representation</article-title>. <source>Neural Comput</source>. <year>1993</year>;<volume>5</volume>: <fpage>613</fpage>–<lpage>624</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Motivated Reinforcement Learning</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2002</year>;</mixed-citation></ref>
<ref id="pcbi.1005768.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Pinette</surname> <given-names>B</given-names></name>. <article-title>The learning of world models by connectionist networks</article-title>. <source>Proceedings of the Seventh Annual Conference of the Cognitive Science Society</source>. <year>1985</year>. pp. <fpage>54</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stachenfeld</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>Design Principles of the Hippocampal Cognitive Map</article-title>. <source>Adv Neural Inf Process Syst</source> <day>27</day>. <year>2014</year>; <fpage>1</fpage>–<lpage>9</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf%5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map" xlink:type="simple">http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf%5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>K a.</given-names></name>, <name name-style="western"><surname>Sederberg</surname> <given-names>PB</given-names></name>. <article-title>The Successor Representation and Temporal Context</article-title>. <source>Neural Comput</source>. <year>2012</year>;<volume>24</volume>: <fpage>1553</fpage>–<lpage>1568</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00282" xlink:type="simple">10.1162/NECO_a_00282</ext-link></comment> <object-id pub-id-type="pmid">22364500</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Suri</surname> <given-names>RE</given-names></name>. <article-title>Anticipatory responses of dopamine neurons and cortical neurons reproduced by internal model</article-title>. <source>Exp Brain Res</source>. <year>2001</year>;<volume>140</volume>: <fpage>234</fpage>–<lpage>240</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s002210100814" xlink:type="simple">10.1007/s002210100814</ext-link></comment> <object-id pub-id-type="pmid">11521155</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref021"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Barreto A, Munos R, Schaul T, Silver D. Successor Features for Transfer in Reinforcement Learning. arXiv Prepr. 2016;1606.</mixed-citation></ref>
<ref id="pcbi.1005768.ref022"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Lehnert L, Tellex S, Littman ML. Advantages and Limitations of using Successor Features for Transfer in Reinforcement Learning. arXiv. 2017; Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/pdf/1708.00102.pdf" xlink:type="simple">https://arxiv.org/pdf/1708.00102.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tolman</surname> <given-names>EC</given-names></name>. <article-title>Cognitive maps in rats and men</article-title>. <source>Psychol Rev</source>. <year>1948</year>;<volume>55</volume>: <fpage>189</fpage>–<lpage>208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0061626" xlink:type="simple">10.1037/h0061626</ext-link></comment> <object-id pub-id-type="pmid">18870876</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simon</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Neural correlates of forward planning in a spatial decision task in humans</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>5526</fpage>–<lpage>5539</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4647-10.2011" xlink:type="simple">10.1523/JNEUROSCI.4647-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21471389</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <article-title>Reinforcement Learning: An Introduction</article-title>. <year>2012</year>; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNN.1998.712192" xlink:type="simple">10.1109/TNN.1998.712192</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref026"><label>26</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>. <chapter-title>Value Learning through Reinforcement</chapter-title>. In: <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Fehr</surname> <given-names>E</given-names></name>, editors. <source>Neuroeconomics</source>. <edition>2nd ed.</edition> <publisher-loc>London</publisher-loc>: <publisher-name>Elsevier</publisher-name>; <year>2014</year>. pp. <fpage>283</fpage>–<lpage>298</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/B978-0-12-416008-8.00015–2" xlink:type="simple">10.1016/B978-0-12-416008-8.00015–2</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>. <article-title>Dyna, an integrated architecture for learning, planning, and reacting</article-title>. <source>ACM SIGART Bull</source>. <year>1991</year>;<volume>2</volume>: <fpage>160</fpage>–<lpage>163</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/122344.122377" xlink:type="simple">10.1145/122344.122377</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Markman</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Otto</surname> <given-names>AR</given-names></name>. <article-title>Retrospective revaluation in sequential decision making: a tale of two systems</article-title>. <source>J Exp Psychol Gen</source>. <year>2014</year>;<volume>143</volume>: <fpage>182</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0030844" xlink:type="simple">10.1037/a0030844</ext-link></comment> <object-id pub-id-type="pmid">23230992</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>. <article-title>Representation of Action-Specific Reward Values in the Striatum</article-title>. <source>Science</source> (80-). <year>2005</year>;<volume>310</volume>. Available: <ext-link ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/310/5752/1337" xlink:type="simple">http://science.sciencemag.org/content/310/5752/1337</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Value Representations in the Primate Striatum during Matching Behavior</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>58</volume>: <fpage>451</fpage>–<lpage>463</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.02.021" xlink:type="simple">10.1016/j.neuron.2008.02.021</ext-link></comment> <object-id pub-id-type="pmid">18466754</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2011</year>;<volume>108</volume> <issue>Suppl</issue>: <fpage>15647</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1014269108" xlink:type="simple">10.1073/pnas.1014269108</ext-link></comment> <object-id pub-id-type="pmid">21389268</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action</article-title>. <source>Neuropsychopharmacology</source>. <year>2010</year>;<volume>35</volume>: <fpage>48</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/npp.2009.131" xlink:type="simple">10.1038/npp.2009.131</ext-link></comment> <object-id pub-id-type="pmid">19776734</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alexander</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Crutchner</surname> <given-names>MD</given-names></name>. <article-title>Functional architecture of basal ganglia circuits: neural substrated of parallel processing</article-title>. <source>Trends Neurosci</source>. <year>1990</year>;<volume>13</volume>: <fpage>266</fpage>–<lpage>271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0166-2236(90)90107-L" xlink:type="simple">10.1016/0166-2236(90)90107-L</ext-link></comment> <object-id pub-id-type="pmid">1695401</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorndike</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Jelliffe</surname></name>. <article-title>Animal Intelligence. Experimental Studies</article-title>. <source>The Journal of Nervous and Mental Disease</source>. Transaction Publishers; <year>1912</year>. p. <fpage>357</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/00005053-191205000-00016" xlink:type="simple">10.1097/00005053-191205000-00016</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camerer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ho</surname> <given-names>T-H</given-names></name>. <article-title>Experience-Weighted Atttraction in Normal Form Games</article-title>. <source>Econometrica</source>. <year>1999</year>;<volume>67</volume>: <fpage>827</fpage>–<lpage>874</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <chapter-title>The role of learning in the operation of motivational systems</chapter-title>. In: <name name-style="western"><surname>Gallistel</surname> <given-names>CR</given-names></name>, editor. <source>Steven’s handbook of experimental psychology: Learning, motivation and emotion</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2002</year>. pp. <fpage>497</fpage>–<lpage>534</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/0471214426.pas0312" xlink:type="simple">10.1002/0471214426.pas0312</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wimmer</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Shohamy</surname> <given-names>D</given-names></name>. <article-title>Preference by association: how memory mechanisms in the hippocampus bias decisions</article-title>. <source>Science</source> (80-). American Association for the Advancement of Science; <year>2012</year>;<volume>338</volume>: <fpage>270</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1223252" xlink:type="simple">10.1126/science.1223252</ext-link></comment> <object-id pub-id-type="pmid">23066083</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>. <article-title>Actions and Habits: The Development of Behavioural Autonomy</article-title> [Internet]. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>. <year>1985</year>. pp. <fpage>67</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.1985.0010" xlink:type="simple">10.1098/rstb.1985.0010</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dickinson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>B</given-names></name>. <article-title>Motivational control of goal-directed action.</article-title> <source>Anim Learn Behav</source>. <year>1994</year>;<volume>22</volume>: <fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03199951" xlink:type="simple">10.3758/BF03199951</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title>. <source>Eur J Neurosci</source>. <year>2004</year>;<volume>19</volume>: <fpage>181</fpage>–<lpage>189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2004.03095.x" xlink:type="simple">10.1111/j.1460-9568.2004.03095.x</ext-link></comment> <object-id pub-id-type="pmid">14750976</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>. <article-title>Speed/accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002055" xlink:type="simple">10.1371/journal.pcbi.1002055</ext-link></comment> <object-id pub-id-type="pmid">21637741</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pezzulo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rigoli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Chersi</surname> <given-names>F</given-names></name>. <article-title>The mixed instrumental controller: Using value of information to combine habitual choice and mental simulation</article-title>. <source>Front Psychol</source>. <year>2013</year>;<volume>4</volume>: <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00092" xlink:type="simple">10.3389/fpsyg.2013.00092</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solway</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title>. <source>Psychol Rev</source>. NIH Public Access; <year>2012</year>;<volume>119</volume>: <fpage>120</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0026435" xlink:type="simple">10.1037/a0026435</ext-link></comment> <object-id pub-id-type="pmid">22229491</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source> (80-). <year>1997</year>;<volume>275</volume>: <fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Neuroeconomics</article-title>. <source>Neuroeconomics</source>. 1st ed. <year>2009</year>. pp. <fpage>367</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/B978-0-12-374176-9.00024–5" xlink:type="simple">10.1016/B978-0-12-374176-9.00024–5</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamawaki</surname> <given-names>S</given-names></name>. <article-title>Prediction of Immediate and Future Rewards Differentially Recruits Cortico-Basal Ganglia Loops</article-title>. <source>Nature Neuroscience. Tokyo</source>; <year>2004</year>. pp. <fpage>887</fpage>–<lpage>893</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-4-431-55402-8_22" xlink:type="simple">10.1007/978-4-431-55402-8_22</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Habits, action sequences and reinforcement learning</article-title>. <source>Eur J Neurosci</source>. <year>2012</year>;<volume>35</volume>: <fpage>1036</fpage>–<lpage>1051</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2012.08050.x" xlink:type="simple">10.1111/j.1460-9568.2012.08050.x</ext-link></comment> <object-id pub-id-type="pmid">22487034</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haber</surname> <given-names>SN</given-names></name>. <article-title>The primate basal ganglia: Parallel and integrative networks</article-title>. <source>J Chem Neuroanat</source>. <year>2003</year>;<volume>26</volume>: <fpage>317</fpage>–<lpage>330</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jchemneu.2003.10.003" xlink:type="simple">10.1016/j.jchemneu.2003.10.003</ext-link></comment> <object-id pub-id-type="pmid">14729134</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faure</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Haberland</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Massioui</surname> <given-names>N El</given-names></name>. <article-title>Lesion to the Nigrostriatal Dopamine System Disrupts Stimulus–Response Habit Formation</article-title>. <source>J Neurosci</source>. <year>2005</year>;<volume>25</volume>: <fpage>2771</fpage>–<lpage>2780</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3894-04.2005" xlink:type="simple">10.1523/JNEUROSCI.3894-04.2005</ext-link></comment> <object-id pub-id-type="pmid">15772337</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref050"><label>50</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <chapter-title>Reinforcement learning: an introduction</chapter-title>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>What are the Computations of the Cerebellum, the Basal Gangila, and the Cerebral Cortex?</article-title> <source>Sci Technol</source>. <year>1999</year>;<volume>12</volume>: <fpage>1</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(99)00046-5" xlink:type="simple">10.1016/S0893-6080(99)00046-5</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Lally</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Faulkner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Seifritz</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Interplay of approximate planning strategies</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2015</year>;<volume>112</volume>: <fpage>3098</fpage>–<lpage>103</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1414219112" xlink:type="simple">10.1073/pnas.1414219112</ext-link></comment> <object-id pub-id-type="pmid">25675480</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Expectancies in decision making, reinforcement learning, and ventral striatum</article-title>. <source>Front Neurosci</source>. Frontiers; <year>2010</year>;<volume>3</volume>: <fpage>6</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.01.006.2010" xlink:type="simple">10.3389/neuro.01.006.2010</ext-link></comment> <object-id pub-id-type="pmid">21221409</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Ludvig EA, Mirian MS, Kehoe EJ, Sutton RS. Associative learning from replayed experience. bioRxiv. 2017; <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/100800" xlink:type="simple">https://doi.org/10.1101/100800</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Spike-timing-dependent Hebbian plasticity as temporal difference learning</article-title>. <source>Neural Comput</source>. <year>2001</year>;<volume>13</volume>: <fpage>2221</fpage>–<lpage>2237</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976601750541787" xlink:type="simple">10.1162/089976601750541787</ext-link></comment> <object-id pub-id-type="pmid">11570997</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gehring</surname> <given-names>CA</given-names></name>. <article-title>Approximate Linear Successor Representation</article-title>. <source>Reinforcement Learning Decision Making</source>. <year>2015</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf" xlink:type="simple">http://people.csail.mit.edu/gehring/publications/clement-gehring-rldm-2015.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref057"><label>57</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tolman</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Honzik</surname> <given-names>CH</given-names></name>. <chapter-title>Introduction and removal of reward, and maze performance in rats</chapter-title>. <publisher-name>Univ Calif Publ Psychol</publisher-name>. <year>1930</year>;</mixed-citation></ref>
<ref id="pcbi.1005768.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shin</surname> <given-names>S</given-names></name>. <article-title>An optimization network for matrix inversion</article-title>. <source>Neural Inf Process Syst</source>. <year>1988</year>; <fpage>397</fpage>–<lpage>401</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Russek</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Cheong</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The successor representation in human reinforcement learning</article-title>. <source>Nat Hum Behav</source>. Springer US; <year>2017</year>;<volume>1</volume>: <fpage>680</fpage>–<lpage>692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/083824" xlink:type="simple">10.1101/083824</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref060"><label>60</label><mixed-citation publication-type="other" xlink:type="simple">Wang T, Bowlingm M, Schuurmans D. Dual representations for dynamic programming and reinforcement learning. Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning, ADPRL 2007. 2007. pp. 44–51. 10.1109/ADPRL.2007.368168</mixed-citation></ref>
<ref id="pcbi.1005768.ref061"><label>61</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>White</surname> <given-names>LM</given-names></name>. <source>Temporal Difference Learning: Eligibility Traces and the Successor Representation for Actions</source> [Internet]. <publisher-name>University of Toronto</publisher-name>. <year>1995</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.4525&amp;rep=rep1&amp;type=pdf" xlink:type="simple">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.4525&amp;rep=rep1&amp;type=pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref062"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Blundell C, Uria B, Pritzel A, Li Y, Ruderman A, Leibo JZ, et al. Model-Free Episodic Control. arXiv:160604460v1 [statML]. 2016; 1–12.</mixed-citation></ref>
<ref id="pcbi.1005768.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>B</given-names></name>. <article-title>Reactivation of hippocampal ensemble memories during sleep</article-title>. <source>Science</source> (80-). <year>1994</year>;<volume>265</volume>.</mixed-citation></ref>
<ref id="pcbi.1005768.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kudrimoti</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Barnes</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>. <article-title>Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title>. <source>J Neurosci</source>. <year>1999</year>;<volume>19</volume>: <fpage>4090</fpage>–<lpage>101</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10234037" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10234037</ext-link> <object-id pub-id-type="pmid">10234037</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source>Psychol Rev</source>. <year>1995</year>;<volume>102</volume>: <fpage>419</fpage>–<lpage>457</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.102.3.419" xlink:type="simple">10.1037/0033-295X.102.3.419</ext-link></comment> <object-id pub-id-type="pmid">7624455</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Two-stage model of memory trace formation: A role for “noisy” brain states</article-title>. <source>Neuroscience</source>. <year>1989</year>;<volume>31</volume>: <fpage>551</fpage>–<lpage>570</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0306-4522(89)90423-5" xlink:type="simple">10.1016/0306-4522(89)90423-5</ext-link></comment> <object-id pub-id-type="pmid">2687720</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moore</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Atkeson</surname> <given-names>CG</given-names></name>. <article-title>Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time</article-title>. <source>Mach Learn</source>. <year>1993</year>;<volume>13</volume>: <fpage>103</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1022635613229" xlink:type="simple">10.1023/A:1022635613229</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Set</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Saez</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Houser</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Myung</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Zhong</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Dissociable contribution of prefrontal and striatal dopaminergic genes to learning in economic games</article-title>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1316259111" xlink:type="simple">10.1073/pnas.1316259111</ext-link></comment> <object-id pub-id-type="pmid">24979760</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Neurocomputational models of working memory</article-title>. <source>Nat Neurosci</source>. Nature Publishing Group; <year>2000</year>;<volume>3</volume>: <fpage>1184</fpage>–<lpage>1191</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/81460" xlink:type="simple">10.1038/81460</ext-link></comment> <object-id pub-id-type="pmid">11127836</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Tonic dopamine: Opportunity costs and the control of response vigor</article-title>. <source>Psychopharmacology (Berl)</source>. <year>2007</year>;<volume>191</volume>: <fpage>507</fpage>–<lpage>520</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00213-006-0502-4" xlink:type="simple">10.1007/s00213-006-0502-4</ext-link></comment> <object-id pub-id-type="pmid">17031711</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boureau</surname> <given-names>YL</given-names></name>, <name name-style="western"><surname>Sokol-Hessner</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Deciding How To Decide: Self-Control and Meta-Decision Making</article-title>. <source>Trends Cogn Sci</source>. Elsevier Ltd; <year>2015</year>;<volume>19</volume>: <fpage>700</fpage>–<lpage>710</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2015.08.013" xlink:type="simple">10.1016/j.tics.2015.08.013</ext-link></comment> <object-id pub-id-type="pmid">26483151</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Smittenaar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Adaptive integration of habits into depth-limited planning defines a habitual-goal-directed spectrum</article-title>. <source>Proc Natl Acad Sci U S A</source>. National Academy of Sciences; <year>2016</year>;<volume>113</volume>: <fpage>12868</fpage>–<lpage>12873</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1609094113" xlink:type="simple">10.1073/pnas.1609094113</ext-link></comment> <object-id pub-id-type="pmid">27791110</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hiroyuki</surname> <given-names>N</given-names></name>. <article-title>Multiplexing signals in reinforcement learning with internal models and dopamine</article-title>. <source>Curr Opin Neurobiol</source>. Elsevier Ltd; <year>2014</year>;<volume>25</volume>: <fpage>123</fpage>–<lpage>129</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2014.01.001" xlink:type="simple">10.1016/j.conb.2014.01.001</ext-link></comment> <object-id pub-id-type="pmid">24463329</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akam</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Costa</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>: <fpage>1</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004648" xlink:type="simple">10.1371/journal.pcbi.1004648</ext-link></comment> <object-id pub-id-type="pmid">26657806</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>. <article-title>TD Models: Modeling the world at a mixture of time scales</article-title>. <source>Proceedings of the 12th Int Conf on Machine Learning</source>. <year>1995</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/CBO9781107415324.004" xlink:type="simple">10.1017/CBO9781107415324.004</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Hierarchical control of goal-directed action in the cortical–basal ganglia network</article-title>. <source>Curr Opin Behav Sci</source>. Elsevier Ltd; <year>2015</year>;<volume>5</volume>: <fpage>1</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cobeha.2015.06.001" xlink:type="simple">10.1016/j.cobeha.2015.06.001</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barron</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>. <article-title>Online evaluation of novel choices by simultaneous representation of multiple memories</article-title>. <source>Nat Neurosci</source>. Nature Publishing Group; <year>2013</year>;<volume>16</volume>: <fpage>1492</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3515" xlink:type="simple">10.1038/nn.3515</ext-link></comment> <object-id pub-id-type="pmid">24013592</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Papale</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Zielinski</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Jadhav</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Papale</surname> <given-names>AE</given-names></name>, <etal>et al</etal>. <article-title>Interplay between Hippocampal Sharp-Wave-Ripple Events and Vicarious Trial and Error Behaviors in Report Interplay between Hippocampal Sharp-Wave-Ripple Events and Vicarious Trial and Error Behaviors in Decision Making</article-title>. <source>Neuron</source>. Elsevier Inc.; <year>2016</year>;<volume>92</volume>: <fpage>975</fpage>–<lpage>982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.10.028" xlink:type="simple">10.1016/j.neuron.2016.10.028</ext-link></comment> <object-id pub-id-type="pmid">27866796</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>. <article-title>Neural Computations Underlying Arbitration between Model-Based and Model-free Learning</article-title>. <source>Neuron</source>. Elsevier Inc.; <year>2014</year>;<volume>81</volume>: <fpage>687</fpage>–<lpage>699</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.11.028" xlink:type="simple">10.1016/j.neuron.2013.11.028</ext-link></comment> <object-id pub-id-type="pmid">24507199</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Touretzky</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Hippocampal Replay Is Not a Simple Function of Experience</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>: <fpage>695</fpage>–<lpage>705</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.01.034" xlink:type="simple">10.1016/j.neuron.2010.01.034</ext-link></comment> <object-id pub-id-type="pmid">20223204</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ciancia</surname> <given-names>F</given-names></name>. <article-title>Tolman and Honzik (1930) revisited: or The mazes of psychology (1930–1980)</article-title>. <source>Psychol Rec</source>. <year>1991</year>;<volume>41</volume>: <fpage>461</fpage>–<lpage>472</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tolman+and+honzik+(1930)+revisited+or+the+mazes+of+psychology+(1930-1980)#0" xlink:type="simple">http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tolman+and+honzik+(1930)+revisited+or+the+mazes+of+psychology+(1930-1980)#0</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Thinus-Blanc</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chapuis</surname> <given-names>N</given-names></name>. <article-title>Route planning in cats, in relation to the visibility of the goal</article-title>. <source>Anim Behav</source>. <year>1983</year>;<volume>31</volume>: <fpage>594</fpage>–<lpage>599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0003-3472(83)80083-9" xlink:type="simple">10.1016/S0003-3472(83)80083-9</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Winocur</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Moscovitch</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rosenbaum</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Sekeres</surname> <given-names>M</given-names></name>. <article-title>An investigation of the effects of hippocampal lesions in rats on pre- and postoperatively acquired spatial memory in a complex environment</article-title>. <source>Hippocampus</source>. <year>2010</year>;<volume>20</volume>: <fpage>1350</fpage>–<lpage>1365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20721" xlink:type="simple">10.1002/hipo.20721</ext-link></comment> <object-id pub-id-type="pmid">20054817</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jovalekic</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hayman</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Becares</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Reid</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Horizontal biases in rats’ use of three-dimensional space</article-title>. <source>Behav Brain Res</source>. <year>2011</year>;<volume>222</volume>: <fpage>279</fpage>–<lpage>288</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bbr.2011.02.035" xlink:type="simple">10.1016/j.bbr.2011.02.035</ext-link></comment> <object-id pub-id-type="pmid">21419172</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chapuis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Durup</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Thinus-Blanc</surname> <given-names>C</given-names></name>. <article-title>The role of exploratory experience in a shortcut task by golden hamsters (&amp;lt;i&amp;gt;Mesocricetus auratus&amp;lt;/i&amp;gt;)</article-title>. <source>Learn Behav</source>. <year>1987</year>;<volume>15</volume>: <fpage>174</fpage>–<lpage>178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03204960" xlink:type="simple">10.3758/BF03204960</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alvernhe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Van Cauter</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Save</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Poucet</surname> <given-names>B</given-names></name>. <article-title>Different CA1 and CA3 representations of novel routes in a shortcut situation</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>7324</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1909-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1909-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18632936</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spiers</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Gilbert</surname> <given-names>SJ</given-names></name>. <article-title>Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions</article-title>. <source>Front Hum Neurosci</source>. <year>2015</year>;<volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2015.00125" xlink:type="simple">10.3389/fnhum.2015.00125</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corbit</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Ostlund</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Sensitivity to instrumental contingency degradation is mediated by the entorhinal cortex and its efferents via the dorsal hippocampus</article-title>. <source>J Neurosci</source>. <year>2002</year>;<volume>22</volume>: <fpage>10976</fpage>–<lpage>84</lpage>. 22/24/10976 [pii] <object-id pub-id-type="pmid">12486193</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Girardeau</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Benchenane</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wiener</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Zugaro</surname> <given-names>MB</given-names></name>. <article-title>Selective suppression of hippocampal ripples impairs spatial memory</article-title>. <source>Nat Neurosci</source>. Nature Publishing Group; <year>2009</year>;<volume>12</volume>: <fpage>1222</fpage>–<lpage>1223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2384" xlink:type="simple">10.1038/nn.2384</ext-link></comment> <object-id pub-id-type="pmid">19749750</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ego-Stengel</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>. <article-title>Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat</article-title>. <source>Hippocampus</source>. <year>2010</year>;<volume>20</volume>: <fpage>1</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20707" xlink:type="simple">10.1002/hipo.20707</ext-link></comment> <object-id pub-id-type="pmid">19816984</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jadhav</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Kemere</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>German</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>LM</given-names></name>. <article-title>Awake Hippocampal Sharp-Wave Ripples Support Spatial Memory</article-title>. <source>Science</source> (80-). <year>2012</year>;<volume>336</volume>. Available: <ext-link ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/336/6087/1454.long" xlink:type="simple">http://science.sciencemag.org/content/336/6087/1454.long</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>. <article-title>Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies</article-title>. <source>Front Behav Neurosci</source>. <year>2012</year>;<volume>6</volume>: <fpage>1</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2012.00079" xlink:type="simple">10.3389/fnbeh.2012.00079</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Takahashi</surname> <given-names>YK</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Orbitofrontal Cortex as a Cognitive Map of Task Space</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>: <fpage>267</fpage>–<lpage>279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.11.005" xlink:type="simple">10.1016/j.neuron.2013.11.005</ext-link></comment> <object-id pub-id-type="pmid">24462094</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nomoto</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sakagami</surname> <given-names>M</given-names></name>. <article-title>Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>: <fpage>10692</fpage>–<lpage>10702</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4828-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4828-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20702700</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Decision theory, reinforcement learning, and the brain</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2008</year>;<volume>8</volume>: <fpage>429</fpage>–<lpage>453</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/CABN.8.4.429" xlink:type="simple">10.3758/CABN.8.4.429</ext-link></comment> <object-id pub-id-type="pmid">19033240</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref096"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>. <article-title>Predictive Representations of State</article-title>. <source>Neural Inf Process Syst</source>. <year>2001</year>;<volume>14</volume>: <fpage>1555</fpage>–<lpage>1561</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.2493&amp;rep=rep1&amp;type=pdf" xlink:type="simple">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.2493&amp;rep=rep1&amp;type=pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref097"><label>97</label><mixed-citation publication-type="other" xlink:type="simple">Schlegel M, White A, White M. Stable predictive representations with general value functions for continual learning. Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference. 2017. Available: <ext-link ext-link-type="uri" xlink:href="https://sites.ualberta.ca/~amw8/cldl.pdf" xlink:type="simple">https://sites.ualberta.ca/~amw8/cldl.pdf</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fermin</surname> <given-names>ASR</given-names></name>, <name name-style="western"><surname>Yoshida</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Yoshimoto</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Model-based action planning involves cortico-cerebellar and basal ganglia networks</article-title>. <source>Sci Rep</source>. Nature Publishing Group; <year>2016</year>;<volume>6</volume>: <fpage>31378</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep31378" xlink:type="simple">10.1038/srep31378</ext-link></comment> <object-id pub-id-type="pmid">27539554</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref099"><label>99</label><mixed-citation publication-type="other" xlink:type="simple">Stachenfeld KL, Botvinick MM, Gershman SJ. The hippocampus as a predictive map. biorRxiv. 2017; <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/097170" xlink:type="simple">http://dx.doi.org/10.1101/097170</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1005768.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schapiro</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Rogers</surname> <given-names>TT</given-names></name>, <name name-style="western"><surname>Cordova</surname> <given-names>NI</given-names></name>, <name name-style="western"><surname>Turk-Browne</surname> <given-names>NB</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Neural representations of events arise from temporal community structure</article-title>. <source>Nat Neurosci</source>. Nature Research; <year>2013</year>;<volume>16</volume>: <fpage>486</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3331" xlink:type="simple">10.1038/nn.3331</ext-link></comment> <object-id pub-id-type="pmid">23416451</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garvert</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>A map of abstract relational knowledge in the human hippocampal–entorhinal cortex</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>: <fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.17086" xlink:type="simple">10.7554/eLife.17086</ext-link></comment> <object-id pub-id-type="pmid">28448253</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref102"><label>102</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>O’Keefe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nadel</surname> <given-names>L</given-names></name>. <source>The hippocampus as a cognitive map</source> [Internet]. <publisher-name>Clarendon Press</publisher-name>; <year>1978</year>. Available: <ext-link ext-link-type="uri" xlink:href="http://arizona.openrepository.com/arizona/handle/10150/620894" xlink:type="simple">http://arizona.openrepository.com/arizona/handle/10150/620894</ext-link></mixed-citation></ref>
<ref id="pcbi.1005768.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaussier</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Revel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Banquet</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Babeau</surname> <given-names>V</given-names></name>. <article-title>From view cells and place cells to cognitive map learning: processing stages of the hippocampal system</article-title>. <source>Biol Cybern</source>. Springer-Verlag; <year>2002</year>;<volume>86</volume>: <fpage>15</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s004220100269" xlink:type="simple">10.1007/s004220100269</ext-link></comment> <object-id pub-id-type="pmid">11918209</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gustafson</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Neymotin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Olypher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vayntrub</surname> <given-names>Y</given-names></name>. <article-title>Grid Cells, Place Cells, and Geodesic Generalization for Spatial Reinforcement Learning</article-title>. <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>, editor. <source>PLoS Comput Biol</source>. The MIT Press; <year>2011</year>;<volume>7</volume>: <fpage>e1002235</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002235" xlink:type="simple">10.1371/journal.pcbi.1002235</ext-link></comment> <object-id pub-id-type="pmid">22046115</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wikenheiser</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Over the river, through the woods: cognitive maps in the hippocampus and orbitofrontal cortex</article-title>. <source>Nat Rev Neurosci</source>. Nature Research; <year>2016</year>;<volume>17</volume>: <fpage>513</fpage>–<lpage>523</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn.2016.56" xlink:type="simple">10.1038/nrn.2016.56</ext-link></comment> <object-id pub-id-type="pmid">27256552</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schuck</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Correspondence</surname> <given-names>YN</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Human Orbitofrontal Cortex Represents a Cognitive Map of State Space</article-title>. <source>Neuron</source>. Elsevier Inc; <year>2016</year>;<volume>91</volume>: <fpage>1402</fpage>–<lpage>1412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.08.019" xlink:type="simple">10.1016/j.neuron.2016.08.019</ext-link></comment> <object-id pub-id-type="pmid">27657452</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Haynes</surname> <given-names>J-D</given-names></name>. <article-title>Human anterior prefrontal cortex encodes the “what” and “when” of future intentions</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>61</volume>: <fpage>139</fpage>–<lpage>148</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.02.079" xlink:type="simple">10.1016/j.neuroimage.2012.02.079</ext-link></comment> <object-id pub-id-type="pmid">22418393</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Haynes</surname> <given-names>J-D</given-names></name>. <article-title>Encoding of Prospective Tasks in the Human Prefrontal Cortex under Varying Task Loads</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>17342</fpage>–<lpage>17349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0492-13.2013" xlink:type="simple">10.1523/JNEUROSCI.0492-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24174667</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref109"><label>109</label><mixed-citation publication-type="other" xlink:type="simple">Miller EK, Cohen JD. A N I NTEGRATIVE T HEORY OF P REFRONTAL C ORTEX F UNCTION. 2001; 167–202.</mixed-citation></ref>
<ref id="pcbi.1005768.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wikenheiser</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Marrero-Garcia</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Suppression of Ventral Hippocampal Output Impairs Integrated Orbitofrontal Encoding of Task Structure</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>95</volume>: <fpage>1197</fpage>–<lpage>1207</lpage>.e3. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.08.003" xlink:type="simple">10.1016/j.neuron.2017.08.003</ext-link></comment> <object-id pub-id-type="pmid">28823726</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref111"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AC</given-names></name>. <article-title>Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective</article-title>. <source>Cognition</source>. <year>2009</year>;<volume>113</volume>: <fpage>262</fpage>–<lpage>280</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2008.08.011" xlink:type="simple">10.1016/j.cognition.2008.08.011</ext-link></comment> <object-id pub-id-type="pmid">18926527</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weinstein</surname> <given-names>A</given-names></name>. <article-title>Model-based hierarchical reinforcement learning and human action control</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>;<volume>369</volume>: <fpage>20130480</fpage>–. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0480" xlink:type="simple">10.1098/rstb.2013.0480</ext-link></comment> <object-id pub-id-type="pmid">25267822</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schapiro</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Rogers</surname> <given-names>TT</given-names></name>, <name name-style="western"><surname>Cordova</surname> <given-names>NI</given-names></name>, <name name-style="western"><surname>Turk-Browne</surname> <given-names>NB</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>. <article-title>Neural representations of events arise from temporal community structure</article-title>. <source>Nat Publ Gr</source>. <year>2013</year>;<volume>16</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3331" xlink:type="simple">10.1038/nn.3331</ext-link></comment> <object-id pub-id-type="pmid">23416451</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boorman</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Rajendran</surname> <given-names>VG</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>JX</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>. <article-title>Two Anatomically and Computationally Distinct Learning Signals Predict Changes to Stimulus-Outcome Associations in Hippocampus</article-title>. <source>Neuron</source>. The Authors; <year>2016</year>;<volume>89</volume>: <fpage>1343</fpage>–<lpage>1354</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.02.014" xlink:type="simple">10.1016/j.neuron.2016.02.014</ext-link></comment> <object-id pub-id-type="pmid">26948895</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref115"><label>115</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doll</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Duncan</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Shohamy</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Model-based choices involve prospective neural activity</article-title>. <source>Nat Neurosci</source>. <year>2015</year>;<volume>18</volume>: <fpage>767</fpage>–<lpage>772</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3981" xlink:type="simple">10.1038/nn.3981</ext-link></comment> <object-id pub-id-type="pmid">25799041</object-id></mixed-citation></ref>
<ref id="pcbi.1005768.ref116"><label>116</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>NF</given-names></name>, <name name-style="western"><surname>Cameron</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Taliaferro</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <etal>et al</etal>. <article-title>Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4287" xlink:type="simple">10.1038/nn.4287</ext-link></comment> <object-id pub-id-type="pmid">27110917</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>