<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
   <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
         <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
         <publisher-name>Public Library of Science</publisher-name>
         <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
   </journal-meta>
   <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00824</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1003037</article-id>
      <article-categories>
         <subj-group subj-group-type="heading">
            <subject>Research Article</subject>
         </subj-group>
<subj-group subj-group-type="Discipline-v2"><subject>Biology</subject>
<subj-group>
<subject>Computational biology</subject>
<subj-group>
<subject>Computational neuroscience</subject>
<subj-group>
<subject>Circuit models</subject>
</subj-group>
</subj-group>
</subj-group>
<subj-group>
<subject>Neuroscience</subject>
<subj-group>
<subject>Developmental neuroscience</subject>
<subj-group>
<subject>Synaptic plasticity</subject>
</subj-group>
</subj-group>
</subj-group>
</subj-group>
      </article-categories>
      <title-group>
         <article-title>Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</article-title>
         <alt-title alt-title-type="running-head">Emergence of Bayesian Computation through STDP</alt-title>
      </title-group>
      <contrib-group>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Nessler</surname><given-names>Bernhard</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Pfeiffer</surname>
<given-names>Michael</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Buesing</surname>
<given-names>Lars</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Maass</surname>
<given-names>Wolfgang</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
      </contrib-group>
      <aff id="aff1"><label>1</label><addr-line>Institute for Theoretical Computer Science, Graz University of Technology, Graz, Austria</addr-line></aff>
      <aff id="aff2"><label>2</label><addr-line>Institute of Neuroinformatics, University of Zürich and ETH Zürich, Zürich, Switzerland</addr-line></aff>
      <contrib-group>
         <contrib contrib-type="editor" xlink:type="simple">
            <name name-style="western"><surname>Sporns</surname>
<given-names>Olaf</given-names>
            </name><role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
         </contrib>
      </contrib-group>
      <aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
      <author-notes>
         <corresp id="cor1">* E-mail: <email xlink:type="simple">nessler@igi.tugraz.at</email></corresp>
         <fn fn-type="conflict">
            <p>The authors have declared that no competing interests exist.</p>
         </fn>
         <fn fn-type="con">
            <p>Conceived and designed the experiments: BN MP WM. Performed the experiments: BN MP. Wrote the paper: BN MP LB WM.</p>
         </fn>
      </author-notes>
      <pub-date pub-type="collection">
         <month>4</month>
         <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
         <day>25</day>
         <month>4</month>
         <year>2013</year>
      </pub-date>
      <volume>9</volume>
      <issue>4</issue>
      <elocation-id>e1003037</elocation-id>
      <history>
         <date date-type="received">
            <day>19</day>
            <month>5</month>
            <year>2012</year>
         </date>
         <date date-type="accepted">
            <day>4</day>
            <month>3</month>
            <year>2013</year>
         </date>
      </history>
      <permissions>
         <copyright-year>2013</copyright-year>
         <copyright-holder>Nessler et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
      <related-article id="RA1" related-article-type="companion" ext-link-type="uri" page="e1002294" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002294"> <article-title>Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons</article-title></related-article>
      <related-article id="RA2" related-article-type="companion" ext-link-type="uri" page="e1002211" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002211"> <article-title>Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons</article-title></related-article>
      <abstract>
         <p>The principles by which networks of neurons compute, and how spike-timing dependent plasticity (STDP) of synaptic weights generates and maintains their computational function, are unknown. Preceding work has shown that soft winner-take-all (WTA) circuits, where pyramidal neurons inhibit each other via interneurons, are a common motif of cortical microcircuits. We show through theoretical analysis and computer simulations that Bayesian computation is induced in these network motifs through STDP in combination with activity-dependent changes in the excitability of neurons. The fundamental components of this emergent Bayesian computation are priors that result from adaptation of neuronal excitability and implicit generative models for hidden causes that are created in the synaptic weights through STDP. In fact, a surprising result is that STDP is able to approximate a powerful principle for fitting such implicit generative models to high-dimensional spike inputs: Expectation Maximization. Our results suggest that the experimentally observed spontaneous activity and trial-to-trial variability of cortical neurons are essential features of their information processing capability, since their functional role is to represent probability distributions rather than static neural codes. Furthermore it suggests networks of Bayesian computation modules as a new model for distributed information processing in the cortex.</p>
      </abstract>
      <abstract abstract-type="summary">
         <title>Author Summary</title>
         <p>How do neurons learn to extract information from their inputs, and perform meaningful computations? Neurons receive inputs as continuous streams of action potentials or “spikes” that arrive at thousands of synapses. The strength of these synapses - the synaptic weight - undergoes constant modification. It has been demonstrated in numerous experiments that this modification depends on the temporal order of spikes in the pre- and postsynaptic neuron, a rule known as STDP, but it has remained unclear, how this contributes to higher level functions in neural network architectures. In this paper we show that STDP induces in a commonly found connectivity motif in the cortex - a winner-take-all (WTA) network - autonomous, self-organized learning of probabilistic models of the input. The resulting function of the neural circuit is Bayesian computation on the input spike trains. Such unsupervised learning has previously been studied extensively on an abstract, algorithmical level. We show that STDP approximates one of the most powerful learning methods in machine learning, Expectation-Maximization (EM). In a series of computer simulations we demonstrate that this enables STDP in WTA circuits to solve complex learning tasks, reaching a performance level that surpasses previous uses of spiking neural networks.</p>
      </abstract>
      <funding-group>
         <funding-statement>This work was written under partial support by project #FP7-216593 (SECO), project #FP7-506778 (PASCAL2), project #FP7-243914 (BRAIN-I-NETS) and project #FP7-269921 (BrainScaleS) of the European Union. MP has been supported by the Samsung Advanced Institute of Technology and a Forschungskredit grant of the University of Zurich. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
<counts>
<page-count count="30"/>
</counts>
</article-meta>
</front>
<body>
   <sec id="s1">
      <title>Introduction</title>
      <p>Numerous experimental data show that the brain applies principles of Bayesian inference for analyzing sensory stimuli, for reasoning and for producing adequate motor outputs <xref ref-type="bibr" rid="pcbi.1003037-Rao1">1</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Krding1">5</xref>. Bayesian inference has been suggested as a mechanism for the important task of probabilistic perception <xref ref-type="bibr" rid="pcbi.1003037-Fiser1">[6]</xref>, in which hidden causes (e.g. the categories of objects) that explain noisy and potentially ambiguous sensory inputs have to be inferred. This process requires the combination of prior beliefs about the availability of causes in the environment, and probabilistic generative models of likely sensory observations that result from any given cause. By Bayes Theorem, the result of the inference process yields a <italic>posterior</italic> probability distribution over hidden causes that is computed by multiplying the <italic>prior</italic> probability with the <italic>likelihood</italic> of the sensory evidence for all possible causes. In this article we refer to the computation of posterior probabilities through a combination of probabilistic prior and likelihood models as Bayesian computation. It has previously been shown that priors and models that encode likelihoods of external stimuli for a given cause can be represented in the parameters of neural network models <xref ref-type="bibr" rid="pcbi.1003037-Fiser1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Ma1">[7]</xref>. However, in spite of the existing evidence that Bayesian computation is a primary information processing step in the brain, it has remained open how networks of neurons can acquire these priors and likelihood models, and how they combine them to arrive at posterior distributions of hidden causes.</p>
      <p>The fundamental computational units of the brain, neurons and synapses, are well characterized. The synaptic connections are subject to various forms of plasticity, and recent experimental results have emphasized the role of STDP, which constantly modifies synaptic strengths (weights) in dependence of the difference between the firing times of the pre- and postsynaptic neurons (see <xref ref-type="bibr" rid="pcbi.1003037-Dan1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Feldman1">[9]</xref> for reviews). Functional consequences of STDP can resemble those of rate-based Hebbian models <xref ref-type="bibr" rid="pcbi.1003037-Song1">[10]</xref>, but may also lead to the emergence of temporal coding <xref ref-type="bibr" rid="pcbi.1003037-Kempter1">[11]</xref> and rate-normalization <xref ref-type="bibr" rid="pcbi.1003037-Kempter2">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Abbott1">[13]</xref>. In addition, the excitability of neurons is modified through their firing activity <xref ref-type="bibr" rid="pcbi.1003037-Daoudal1">[14]</xref>. Some hints about the organization of local computations in stereotypical columns or so-called cortical microcircuits <xref ref-type="bibr" rid="pcbi.1003037-Grillner1">[15]</xref> arises from data about the anatomical structure of these hypothesized basis computational modules of the brain. In particular, it has been observed that local ensembles of pyramidal neurons on layers 2/3 and layers 5/6 typically inhibit each other, via indirect synaptic connections involving inhibitory neurons <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>. These ubiquitous network motifs were called soft winner-take-all (WTA) circuits, and have been suggested as neural network models for implementing functions like non-linear selection <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Hahnloser1">[17]</xref>, normalization <xref ref-type="bibr" rid="pcbi.1003037-Carandini1">[18]</xref>, selective attention <xref ref-type="bibr" rid="pcbi.1003037-Itti1">[19]</xref>, decision making <xref ref-type="bibr" rid="pcbi.1003037-Nessler1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pfeiffer1">[21]</xref>, or as primitives for general purpose computation <xref ref-type="bibr" rid="pcbi.1003037-Maass1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Rutishauser1">[23]</xref>.</p>
      <p>A comprehensive theory that explains the emergence of computational function in WTA networks of spiking neurons through STDP has so far been lacking. We show in this article that STDP and adaptations of neural excitability are likely to provide the fundamental components of Bayesian computation in soft WTA circuits, yielding representations of posterior distributions for hidden causes of high-dimensional spike inputs through the firing probabilities of pyramidal neurons. This is shown in detail for a simple, but very relevant feed-forward model of Bayesian inference, in which the distribution for a single hidden cause is inferred from the afferent spike trains. Our new theory thus describes how modules of soft WTA circuits can acquire and perform Bayesian computations to solve one of the fundamental tasks in perception, namely approximately inferring the category of an object from feed-forward input. Neural network models that can handle Bayesian inference in general graphical models, including bi-directional inference over arbitrary sets of random variables, explaining away effects, different statistical dependency models, or inference over time require more complex network architectures <xref ref-type="bibr" rid="pcbi.1003037-Buesing1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pecevski1">[25]</xref>, and are the topic of ongoing research. Such networks can be composed out of interconnected soft WTA circuits, which has been shown to be a powerful principle for designing neural networks that can solve arbitrary deterministic or stochastic computations <xref ref-type="bibr" rid="pcbi.1003037-Maass1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Rutishauser1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pecevski1">[25]</xref>. Our theory can thus be seen as a first step towards learning the desired functionality of individual modules.</p>
      <p>At the heart of this link between Bayesian computation and network motifs of cortical microcircuits lies a new theoretical insight on the micro-scale: If the STDP-induced changes in synaptic strength depend in a particular way on the current synaptic strength, STDP approximates for each synapse exponentially fast the conditional probability that the presynaptic neuron has fired just before the postsynaptic neuron (given that the postsynaptic neuron fires). This principle suggests that synaptic weights can be understood as conditional probabilities, and the ensemble of all weights of a neuron as a generative model for high-dimensional inputs that - after learning - causes it to fire with a probability that depends on how well its current input agrees with this generative model. The concept of a generative model is well known in theoretical neuroscience <xref ref-type="bibr" rid="pcbi.1003037-Hinton1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Hinton2">[27]</xref>, but it has so far primarily been applied in the context of an abstract non-spiking neural circuit architecture. In the Bayesian computations that we consider in this article, internal generative models are represented implicitly through the learned values of bottom-up weights in spiking soft-WTA circuits, and inference is carried out by neurons that integrate such synaptic inputs and compete for firing in a WTA circuit. In contrast to previous rate-based models for probabilistic inference <xref ref-type="bibr" rid="pcbi.1003037-Keck1">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Sato2">[30]</xref> every spike in our model has a clear semantic interpretation: one spike indicates the instantaneous assignment of a certain value to an abstract variable represented by the firing neuron. In a Bayesian inference context, every input spike provides evidence for an observed variable, whereas every output spike represents one stochastic sample from the posterior distribution over hidden causes encoded in the circuit.</p>
      <p>We show that STDP is able to approximate the arguably most powerful known learning principle for creating these implicit generative models in the synaptic weights: Expectation Maximization (EM). The fact that STDP approximates EM is remarkable, since it is known from machine learning that EM can solve a fundamental chicken-and-egg problem of unsupervised learning systems <xref ref-type="bibr" rid="pcbi.1003037-Dempster1">[31]</xref>: To detect - without a teacher - hidden causes for complex input data, and to induce separate learning agents to specialize each on one of the hidden causes. The problem is that as long as the hidden causes are unknown to the learning system, it cannot tell the hidden units what to specialize on. EM is an iterative process, where initial guesses of hidden causes are applied to the current input (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e001" xlink:type="simple"/></inline-formula>-step) and successively improved (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e002" xlink:type="simple"/></inline-formula>-step), until a local maximum in the log-likelihood of the input data is reached. In fact, the basic idea of EM is so widely applicable and powerful that most state-of-the art machine learning approaches for discovering salient patterns or structures in real-world data without a human supervisor rely on some form of EM <xref ref-type="bibr" rid="pcbi.1003037-Bishop1">[32]</xref>. We show that in our spiking soft-WTA circuit each output spike can be viewed as an application of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e003" xlink:type="simple"/></inline-formula>-step of EM. The subsequent modification of the synaptic weights between the presynaptic input neurons and the very neuron that has fired the postsynaptic spike according to STDP can be viewed as a move in the direction of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e004" xlink:type="simple"/></inline-formula>-step of a stochastic online EM procedure. This procedure strives to create optimal internal models for high-dimensional spike inputs by maximizing their <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e005" xlink:type="simple"/></inline-formula>-likelihood. We refer to this interpretation of the functional role of STDP in the context of spiking WTA circuits as <bold>s</bold>pike-based <bold>E</bold>xpectation <bold>M</bold>aximization (SEM).</p>
      <p>This analysis gives rise to a new perspective of the computational role of local WTA circuits as parts of cortical microcircuits, and the role of STDP in such circuits: The fundamental computational operations of Bayesian computation (Bayes Theorem) for the inference of hidden causes from bottom-up input emerge in these local circuits through plasticity. The pyramidal neurons in the WTA circuit encode in their spikes samples from a posterior distribution over hidden causes for high-dimensional spike inputs. Inhibition in the WTA accounts for normalization <xref ref-type="bibr" rid="pcbi.1003037-Carandini1">[18]</xref>, and in addition controls the rate at which samples are generated. The necessary multiplication of likelihoods (given by implicit generative models that are learned and encoded in their synaptic weights) with simultaneously learned priors for hidden causes (in our model encoded in the neuronal excitability), does not require any extra computational machinery. Instead, it is automatically carried out (on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e006" xlink:type="simple"/></inline-formula> scale) through linear features of standard neuron models. We demonstrate the emergent computational capability of these self-organizing modules for Bayesian computation through computer simulations. In fact, it turns out that a resulting configuration of networks of spiking neurons can solve demanding computational tasks, such as the discovery of prototypes for handwritten digits without any supervision. We also show that these emergent Bayesian computation modules are able to discover, and communicate through a sparse output spike code, repeating spatio-temporal patterns of input spikes. Since such self-adaptive computing and discrimination capability on high-dimensional spatio-temporal spike patterns is not only essential for early sensory processing, but could represent a generic information processing step also in higher cortical areas, our analysis suggests to consider networks of self-organizing modules for spike-based Bayesian computation as a new model for distributed real-time information processing in the brain.</p>
      <p>Preliminary ideas for a spike-based implementation of EM were already presented in the extended abstract <xref ref-type="bibr" rid="pcbi.1003037-Nessler1">[20]</xref>, where we analyzed the relationship of a simple STDP rule to a Hebbian learning rule, and sketched a proof for stochastic online EM. In the present work we provide a rigorous mathematical analysis of the learning procedure, a proof of convergence, expand the framework towards learning spatio-temporal spike patterns, and discuss in detail the relationship of our STDP rule to experimental results, as well as the interpretation of spikes as samples from instantaneous posterior probability distributions in the context of EM.</p>
   </sec>
   <sec id="s2">
      <title>Results</title>
      <p>In this section we define a simple model circuit and show that every spiking event of the circuit can be described as one independent sample of a discrete probability distribution, which itself evolves over time in response to the spiking input. Within this network we analyze a variant of a STDP rule, in which the strength of potentiation depends on the current weight value. This local learning rule, which is supported by experimental data, and at intermediate spike frequencies closely resembles typical STDP rules from the literature, drives every synaptic weight to converge stochastically to the log of the probability that the presynaptic input neuron fired a spike within a short time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e007" xlink:type="simple"/></inline-formula>, before the postsynaptic neuron spikes at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e008" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e009"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e009" xlink:type="simple"/><label>(1)</label></disp-formula>We then show that the network model can be viewed as performing Bayesian computation, meaning that every spike can be understood as a sample from a posterior distribution over hidden causes in a generative probabilistic model, which combines prior probabilities and evidence from current input spike trains.</p>
      <p>This understanding of spikes as samples of hidden causes leads to the central result of this paper. We show that STDP implements a stochastic version of Expectation Maximization for the unsupervised learning of the generative model and present convergence results for SEM. Importantly, this implementation of EM is based on spike events, rather than spike rates.</p>
      <p>Finally we discuss how our model can be implemented with biologically realistic mechanisms. In particular this provides a link between mechanisms for lateral inhibition in WTA circuits and learning of probabilistic models. We finally demonstrate in several computer experiments that SEM can solve very demanding tasks, such as detecting and learning repeatedly occurring spike patterns, and learning models for images of handwritten digits without any supervision.</p>
      <sec id="s2a">
         <title>Definition of the network model</title>
         <p>Our model consists of a network of spiking neurons, arranged in a WTA circuit, which is one of the most frequently studied connectivity patterns (or network motifs) of cortical microcircuits <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>. The input of the circuit is represented by the excitatory neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e010" xlink:type="simple"/></inline-formula>. This input projects to a population of excitatory neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e011" xlink:type="simple"/></inline-formula> that are arranged in a WTA circuit (see <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1</xref>). We model the effect of lateral inhibition, which is the competition mechanism of a WTA circuit <xref ref-type="bibr" rid="pcbi.1003037-Oster1">[33]</xref>, by a common inhibitory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e012" xlink:type="simple"/></inline-formula> that is fed to all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e013" xlink:type="simple"/></inline-formula> neurons and in turn depends on the activity of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e014" xlink:type="simple"/></inline-formula> neurons. Evidence for such common local inhibitory signals for nearby neurons arises from numerous experimental results, see e.g. <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Okun1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Fino1">[36]</xref>. We do not a priori impose a specific functional relationship between the common inhibition signal and the excitatory activity. Instead we will later derive necessary conditions for this relationship, and propose a mechanism that we use for the experiments.</p>
         <fig id="pcbi-1003037-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g001</object-id><label>Figure 1</label>
            <caption>
               <title>The network model and its probabilistic interpretation.</title>
               <p><bold>A</bold> Circuit architecture. External input variables are encoded by populations of spiking neurons, which feed into a Winner-take-all (WTA) circuit. Neurons within the WTA circuit compete via lateral inhibition and have their input weights updated through STDP. Spikes from the WTA circuit constitute the output of the system. <bold>B</bold> Generative probabilistic model for a multinomial mixture: A vector of external input variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e015" xlink:type="simple"/></inline-formula> is dependent on a hidden cause, which is represented by the discrete random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e016" xlink:type="simple"/></inline-formula>. In this model it is assumed that the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e017" xlink:type="simple"/></inline-formula>'s are conditionally independent of each other, given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e018" xlink:type="simple"/></inline-formula>. The inference task is to infer the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e019" xlink:type="simple"/></inline-formula>, given the observations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e020" xlink:type="simple"/></inline-formula>. Our neuronal network model encodes the conditional probabilities of the graphical model into the weight vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e021" xlink:type="simple"/></inline-formula>, such that the activity of the network can be understood as execution of this inference task.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g001" position="float" xlink:type="simple"/></fig>
         <p>The individual units <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e022" xlink:type="simple"/></inline-formula> are modeled by a simplified Spike Response Model <xref ref-type="bibr" rid="pcbi.1003037-Gerstner1">[37]</xref> in which the membrane potential is computed as the difference between the excitatory input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e023" xlink:type="simple"/></inline-formula> and the common inhibition term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e024" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e025" xlink:type="simple"/></inline-formula> sums up the excitatory inputs from neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e026" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003037.e027"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e027" xlink:type="simple"/><label>(2)</label></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e028" xlink:type="simple"/></inline-formula> models the EPSPs evoked by spikes of the presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e029" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e030" xlink:type="simple"/></inline-formula> models the intrinsic excitability of the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e031" xlink:type="simple"/></inline-formula>. In order to simplify our analysis we assume that the EPSP can be modeled as a step function with amplitude <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e032" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e033" xlink:type="simple"/></inline-formula> it takes on the value 1 in a finite time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e034" xlink:type="simple"/></inline-formula> after a spike and is zero before and afterwards. Further spikes within this time window do not contribute additively to the EPSP, but only extend the time window during which the EPSP is in the high state. We will later show how to extend our results to the case of realistically shaped and additive EPSPs.</p>
         <p>We use a stochastic firing model for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e035" xlink:type="simple"/></inline-formula>, in which the firing probability depends exponentially on the membrane potential, i.e.,<disp-formula id="pcbi.1003037.e036"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e036" xlink:type="simple"/><label>(3)</label></disp-formula>which is in good agreement with most experimental data <xref ref-type="bibr" rid="pcbi.1003037-Jolivet1">[38]</xref>. We can thus model the firing behavior of every neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e037" xlink:type="simple"/></inline-formula> in the WTA as an independent inhomogeneous Poisson process whose instantaneous firing rate is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e038" xlink:type="simple"/></inline-formula>.</p>
         <p>In order to understand how this network model generates samples from a probability distribution, we first observe that the combined firing activity of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e039" xlink:type="simple"/></inline-formula> in the WTA circuit is simply the sum of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e040" xlink:type="simple"/></inline-formula> independent Poisson processes, and can thus again be modeled as an inhomogeneous Poisson process with rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e041" xlink:type="simple"/></inline-formula>. Furthermore, in any infinitesimally small time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e042" xlink:type="simple"/></inline-formula>, the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e043" xlink:type="simple"/></inline-formula> spikes with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e044" xlink:type="simple"/></inline-formula>. Thus, if we know that at some point in time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e045" xlink:type="simple"/></inline-formula>, i.e. within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e046" xlink:type="simple"/></inline-formula>, <italic>one</italic> of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e047" xlink:type="simple"/></inline-formula> produces an output spike, the conditional probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e048" xlink:type="simple"/></inline-formula> that this spike originated from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e049" xlink:type="simple"/></inline-formula> can be expressed as<disp-formula id="pcbi.1003037.e050"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e050" xlink:type="simple"/><label>(4)</label></disp-formula>Every single spike from the WTA circuit can thus be seen as an independent sample from the instantaneous distribution in <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> at the time of the spike. Although the instantaneous firing rate of every neuron directly depends on the value of the inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e051" xlink:type="simple"/></inline-formula>, the relative proportion of the rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e052" xlink:type="simple"/></inline-formula> to the total WTA firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e053" xlink:type="simple"/></inline-formula> is independent of the inhibition, because all neurons receive the same inhibition signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e054" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e055" xlink:type="simple"/></inline-formula> determines only the value of the sample at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e056" xlink:type="simple"/></inline-formula>, but not the time point at which a sample is created. The temporal structure of the sampling process depends only on the overall firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e057" xlink:type="simple"/></inline-formula>.</p>
         <p>This implementation of a stochastic WTA circuit does not constrain in any way the kind of spike patterns that can be produced. Every neuron fires independently according to a Poisson process, so it is perfectly possible (and sometimes desirable) that there are two or more neurons that fire (quasi) simultaneously. This is no contradiction to the above theoretical argument of single spikes as samples. There we assumed that there was only one spike at a time inside a time window, but since we assumed these windows to be infinitesimally small, the probability of two spikes occurring exactly at the same point in continuous time is zero.</p>
         <sec id="s2a1">
            <title>Synaptic and intrinsic plasticity</title>
            <p>We can now establish a link between biologically plausible forms of spike-based learning in the above network model and learning via EM in probabilistic graphical models. The synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e058" xlink:type="simple"/></inline-formula> of excitatory connections between input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e059" xlink:type="simple"/></inline-formula> and neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e060" xlink:type="simple"/></inline-formula> in the WTA circuit change due to STDP. Many different versions of STDP rules have emerged from experimental data <xref ref-type="bibr" rid="pcbi.1003037-Dan1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Caporale1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>. For synaptic connections between excitatory neurons, most of them yield a long term potentiation (LTP) when the presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e061" xlink:type="simple"/></inline-formula> fires before the postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e062" xlink:type="simple"/></inline-formula>, otherwise a long term depression (LTD). In our model we use a STDP rule in which the shape of the positive update follows the shape of EPSPs at the synapses, and in which the amplitude of the update <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e063" xlink:type="simple"/></inline-formula> depends on the value of the synaptic weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e064" xlink:type="simple"/></inline-formula> before the update as in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>. Specifically, we propose a rule in which the ratio of LTP and LTD amplitudes is inversely exponentially dependent on the current synaptic weight. LTP curves that mirror the EPSP shape are in accordance with previous studies, which analyzed optimal shapes of STDP curves under different mathematical criteria <xref ref-type="bibr" rid="pcbi.1003037-Toyoizumi1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pfister1">[42]</xref>. The depression part of the rule in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> is a flat offset that contrasts the potentiation. We will show later that this form of LTD occurs in our simulations only at very low repetition frequencies, and instead at natural frequencies our model gives rise to a form of STDP with spike-timing dependent LTD that is very similar to plasticity curves observed in biology <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Bi1">[43]</xref>. We will also analyze the relationship between this rule and a biologically more realistic STDP rule with an explicit time-decaying LTD part.</p>
            <fig id="pcbi-1003037-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g002</object-id><label>Figure 2</label>
               <caption>
                  <title>Learning curves for STDP.</title>
                  <p>Under the simple STDP model (red curve), potentiation occurs only if the postsynaptic spike falls within a time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e065" xlink:type="simple"/></inline-formula> (typically <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e066" xlink:type="simple"/></inline-formula>ms) after the presynaptic spike. The convergence properties of this simpler version in conjunction with rectangular non-additive EPSPs are easier to analyze. In our simulations we use the more complex version (blue dashed curve) in combination with EPSPs that are modeled as biologically realistic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e067" xlink:type="simple"/></inline-formula>-kernels (with plausible time-constants for rise and decay of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e068" xlink:type="simple"/></inline-formula> respectively <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e069" xlink:type="simple"/></inline-formula> ms).</p>
               </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g002" position="float" xlink:type="simple"/></fig>
            <p>We can formulate this STDP-rule as a Hebbian learning rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e070" xlink:type="simple"/></inline-formula> - with learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e071" xlink:type="simple"/></inline-formula> - which is triggered by a spike of the postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e072" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e073" xlink:type="simple"/></inline-formula>. The dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e074" xlink:type="simple"/></inline-formula> on the synaptic activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e075" xlink:type="simple"/></inline-formula> and the current value of the synaptic weight is given by<disp-formula id="pcbi.1003037.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e076" xlink:type="simple"/><label>(5)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e077" xlink:type="simple"/></inline-formula> reflects the previously defined step function shape of the EPSP, this update rule is exactly equivalent to the simple STDP rule (solid red curve) in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> for the case of the pairing of one pre- and one postsynaptic spike. The dependence on the presynaptic activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e078" xlink:type="simple"/></inline-formula> is reflected directly by the time difference <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e079" xlink:type="simple"/></inline-formula> between the pre- and the postsynaptic spikes. According to this rule positive updates are only performed if the presynaptic neuron fired in a time window of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e080" xlink:type="simple"/></inline-formula> ms before the postsynaptic spike. This learning rule therefore respects the causality principle of LTP that is implied in Hebb's original formulation <xref ref-type="bibr" rid="pcbi.1003037-Hebb1">[44]</xref>, rather than looking only at correlations of firing rates.</p>
            <p>We can interpret the learning behavior of this simple STDP rule from a probabilistic perspective. Defining a stationary joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e081" xlink:type="simple"/></inline-formula> over the binary input activations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e082" xlink:type="simple"/></inline-formula> at the times of the postsynaptic spikes, and the binary vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e083" xlink:type="simple"/></inline-formula>, which indicates the source of the postsynaptic spike by setting one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e084" xlink:type="simple"/></inline-formula>, we show in <xref ref-type="sec" rid="s4">Methods</xref> that the equilibrium condition of the expected update <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e085" xlink:type="simple"/></inline-formula> leads to the single solution<disp-formula id="pcbi.1003037.e086"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e086" xlink:type="simple"/><label>(6)</label></disp-formula>This stochastic convergence to the log-probability of the presynaptic neuron being active right before the postsynaptic neuron fires is due to the exponential dependence of the potentiation term on the current weight value. Log-probabilities are necessarily negative values, whereas for biological neural networks we typically expect excitatory, i.e. positive weights from the excitatory input neurons. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e087" xlink:type="simple"/></inline-formula> shifts the range of the values for the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e088" xlink:type="simple"/></inline-formula> into the positive regime for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e089" xlink:type="simple"/></inline-formula>. For the sake of simplicity we assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e090" xlink:type="simple"/></inline-formula> for the following theoretical analysis and we show in <xref ref-type="sec" rid="s4">Methods</xref> that all results remain true for any positive value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e091" xlink:type="simple"/></inline-formula>.</p>
            <p>In analogy to the plasticity of the synaptic weights we also explore a form of intrinsic plasticity of the neurons. We interpret <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e092" xlink:type="simple"/></inline-formula> as an indicator for the excitability of the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e093" xlink:type="simple"/></inline-formula> and apply a circuit-spike triggered update rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e094" xlink:type="simple"/></inline-formula> with<disp-formula id="pcbi.1003037.e095"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e095" xlink:type="simple"/><label>(7)</label></disp-formula>Whenever a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e096" xlink:type="simple"/></inline-formula> fires, the excitability is increased and the amount of increase is inversely exponentially dependent on the current excitability. Otherwise the excitability is decreased by a constant. Such positive feedback through use-dependent changes in the excitability of neurons were found in numerous experimental studies (see e.g. <xref ref-type="bibr" rid="pcbi.1003037-Daoudal1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Cudmore1">[45]</xref>). This concrete model of intrinsic plasticity drives the excitability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e097" xlink:type="simple"/></inline-formula> towards the only equilibrium point of the update rule, which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e098" xlink:type="simple"/></inline-formula>. In <xref ref-type="sec" rid="s4">Methods</xref> (see ‘Weight offsets and positive weights’) we show that the depression of the excitability can be modeled either as an effect of lateral inhibition from firing of neighboring neurons, or as a constant decay, independent of the instantaneous circuit activity. Both methods lead to different values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e099" xlink:type="simple"/></inline-formula>, it is true, but encode identical instantaneous distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e100" xlink:type="simple"/></inline-formula>.</p>
            <p>Note, however, that also negative feedback effects on the excitability through homeostatic mechanisms were observed in experiments <xref ref-type="bibr" rid="pcbi.1003037-Abbott1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Turrigiano1">[46]</xref>. In a forthcoming article <xref ref-type="bibr" rid="pcbi.1003037-Habenschuss1">[47]</xref> we show that the use of such homeostatic mechanisms instead of <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> in an, otherwise unchanged, network model may be interpreted as a posterior constraint in the context of EM.</p>
         </sec>
         <sec id="s2a2">
            <title>Generative probabilistic model</title>
            <p>The instantaneous spike distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e101" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> can be understood as the result of Bayesian inference in an underlying generative probabilistic model for the abstract multinomial observed variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e102" xlink:type="simple"/></inline-formula> and a hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e103" xlink:type="simple"/></inline-formula>. We define the probability distribution of the variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e105" xlink:type="simple"/></inline-formula>, as shown by the graphical model in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref>, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e106" xlink:type="simple"/></inline-formula>. The parametrization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e107" xlink:type="simple"/></inline-formula> of the graphical model consists of a prior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e108" xlink:type="simple"/></inline-formula>on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e109" xlink:type="simple"/></inline-formula>, and conditional probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e110" xlink:type="simple"/></inline-formula> for every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e111" xlink:type="simple"/></inline-formula>.</p>
            <p>The probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e112" xlink:type="simple"/></inline-formula> is a generative model and therefore serves two purposes: On the one hand, it can be used to generate samples of the hidden variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e113" xlink:type="simple"/></inline-formula> and the observable variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e114" xlink:type="simple"/></inline-formula>. This is done by sampling <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e115" xlink:type="simple"/></inline-formula> from the prior distribution, and then sampling the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e116" xlink:type="simple"/></inline-formula>'s, which depend on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e117" xlink:type="simple"/></inline-formula> and can be generated according to the conditional probability tables. The resulting marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e118" xlink:type="simple"/></inline-formula> is a special case of a multinomial mixture distribution.</p>
            <p>On the other hand, for any given observation of the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e119" xlink:type="simple"/></inline-formula>, one can infer the value of the hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e120" xlink:type="simple"/></inline-formula> that led to the generation of this value for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e121" xlink:type="simple"/></inline-formula>. By application of Bayes' rule one can infer the posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e122" xlink:type="simple"/></inline-formula> over all possible values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e123" xlink:type="simple"/></inline-formula>, which is proportional to the product of the prior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e124" xlink:type="simple"/></inline-formula> and the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e125" xlink:type="simple"/></inline-formula>.</p>
            <p>We define population codes to represent the external observable variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e126" xlink:type="simple"/></inline-formula> by the input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e127" xlink:type="simple"/></inline-formula>, and the hidden variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e128" xlink:type="simple"/></inline-formula> by the circuit neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e129" xlink:type="simple"/></inline-formula>: For every variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e130" xlink:type="simple"/></inline-formula> and every possible (discrete) value that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e131" xlink:type="simple"/></inline-formula> can adopt, there is exactly one neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e132" xlink:type="simple"/></inline-formula> which represents this combination. We call <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e133" xlink:type="simple"/></inline-formula> the set of the indices of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e134" xlink:type="simple"/></inline-formula>'s that represent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e135" xlink:type="simple"/></inline-formula>, and we call <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e136" xlink:type="simple"/></inline-formula> the possible value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e137" xlink:type="simple"/></inline-formula> that is represented by neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e138" xlink:type="simple"/></inline-formula>. Thus we can define an interpretation for the spikes from the input neurons by<disp-formula id="pcbi.1003037.e139"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e139" xlink:type="simple"/><label>(8)</label></disp-formula>A spike from the group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e140" xlink:type="simple"/></inline-formula> represents an instantaneous evidence about the observable variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e141" xlink:type="simple"/></inline-formula> at the time of the spike. In the same way every neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e142" xlink:type="simple"/></inline-formula> represents one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e143" xlink:type="simple"/></inline-formula> possible values for the hidden variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e144" xlink:type="simple"/></inline-formula>, and every single spike conveys an instantaneous value for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e145" xlink:type="simple"/></inline-formula>. We can safely assume that all neurons - including the input neurons - fire according to their individual local stochastic processes or at least exhibit some local stochastic jitter. For the theoretical analysis one can regard a spike as an instantaneous event at a single point in time. Thus in a continuous time no two events from such local stochastic processes can happen at exactly the same point in time. Thus, there is never more than one spike at any single point in time within a group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e146" xlink:type="simple"/></inline-formula>, and every spike can be treated as a proper sample from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e147" xlink:type="simple"/></inline-formula>. However, the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e148" xlink:type="simple"/></inline-formula> coding for hidden causes need to integrate evidence from multiple inputs, and thus need a mechanism to retain the instantaneous evidence from a single spike over time, in order to learn from spatial and temporal correlations in the input.</p>
            <p>In our framework this is modeled by postsynaptic potentials on the side of the receiving neurons that are generated in response to input spikes, and, by their shape, represent evidence over time. In the simple case of the non-additive step-function model of the EPSP in <xref ref-type="disp-formula" rid="pcbi.1003037.e027">Eq. (2)</xref>, every spike indicates new evidence for the encoded variable that remains valid during a time window of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e149" xlink:type="simple"/></inline-formula>, after which the evidence is cleared. In the case that there is no spike from one group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e150" xlink:type="simple"/></inline-formula> within a time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e151" xlink:type="simple"/></inline-formula>, this is interpreted as missing evidence (or missing value) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e152" xlink:type="simple"/></inline-formula> in a subsequent inference. In practice it may also occur that EPSPs within a group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e153" xlink:type="simple"/></inline-formula> of input neurons overlap, which would indicate contradicting evidence for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e154" xlink:type="simple"/></inline-formula>. For the theoretical analysis we will first assume that spikes from different input neurons within the same group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e155" xlink:type="simple"/></inline-formula> are not closer in time than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e156" xlink:type="simple"/></inline-formula>, in order to avoid such conflicts. We will later drop this restriction in the extension to more realistically shaped additive EPSPs by slightly enhancing the probabilistic model.</p>
            <p>In our experiments with static input patterns we typically use the following basis scheme to encode the external input variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e157" xlink:type="simple"/></inline-formula> by populations of stochastic spiking neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e158" xlink:type="simple"/></inline-formula>: at every point in time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e159" xlink:type="simple"/></inline-formula> there is exactly one neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e160" xlink:type="simple"/></inline-formula> in every group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e161" xlink:type="simple"/></inline-formula> that represents the instantaneous value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e162" xlink:type="simple"/></inline-formula>. We call this neuron the active neuron of the group, whereas all other neurons of the group are inactive. During the time where a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e163" xlink:type="simple"/></inline-formula> is active it fires stochastically according to a Poisson processes with a certain constant or oscillating rate. The inactive neurons, however, remain silent, i.e. they fire with a rate near 0. Although not explicitly modeled here, such an effect can result from strong lateral inhibition in the input populations. This scheme certainly fulfills the definition in <xref ref-type="disp-formula" rid="pcbi.1003037.e139">Eq. (8)</xref>.</p>
            <p>Here and in the following we will write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e164" xlink:type="simple"/></inline-formula> to denote the input activation through the EPSPs of the network model, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e165" xlink:type="simple"/></inline-formula> to denote a variable in the probabilistic model, which models the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e166" xlink:type="simple"/></inline-formula> over all time points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e167" xlink:type="simple"/></inline-formula>. We will also use notations like <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e168" xlink:type="simple"/></inline-formula>, which refers to the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e169" xlink:type="simple"/></inline-formula> in the probabilistic model taking on the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e170" xlink:type="simple"/></inline-formula>. We can then reformulate the abstract probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e171" xlink:type="simple"/></inline-formula> using the above population codes that define the binary variable vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e172" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e173" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e174" xlink:type="simple"/></inline-formula> s.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e175" xlink:type="simple"/></inline-formula> as:<disp-formula id="pcbi.1003037.e176"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e176" xlink:type="simple"/><label>(9)</label></disp-formula>Under the normalization conditions<disp-formula id="pcbi.1003037.e177"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e177" xlink:type="simple"/><label>(10)</label></disp-formula>the normalization constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e178" xlink:type="simple"/></inline-formula> vanishes and the parametrization of the distribution simplifies to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e180" xlink:type="simple"/></inline-formula>. Even for non-normalized weights, the definition in <xref ref-type="disp-formula" rid="pcbi.1003037.e176">Eq. (9)</xref> still represents the same type of distribution, although there is no more one-to-one mapping between the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e181" xlink:type="simple"/></inline-formula> and the parameters of the graphical model (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Note also that such log-probabilities are exactly (up to additive constants) the local equilibrium points in <xref ref-type="disp-formula" rid="pcbi.1003037.e086">Eq. (6)</xref> of the STDP rule in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>. In the section “STDP approximates Expectation Maximization” we will discuss in detail how this leads to unsupervised learning of a generative model of the input data in a WTA circuit.</p>
         </sec>
         <sec id="s2a3">
            <title>Spike-based Bayesian computation</title>
            <p>We can now formulate an exact link between the above generative probabilistic model and our neural network model of a simplified spike-based WTA circuit. We show that at any point in time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e182" xlink:type="simple"/></inline-formula> at which the network generates an output spike, the relative firing probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e183" xlink:type="simple"/></inline-formula> of the output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e184" xlink:type="simple"/></inline-formula> as in <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref>, are equal to the posterior distribution of the hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e185" xlink:type="simple"/></inline-formula>, given the current evidences encoded in the input activations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e186" xlink:type="simple"/></inline-formula>. For a given input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e187" xlink:type="simple"/></inline-formula> we use Bayes' rule to calculate the posterior probability of cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e188" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e189" xlink:type="simple"/></inline-formula>. We can identify the prior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e190" xlink:type="simple"/></inline-formula> with the excitabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e191" xlink:type="simple"/></inline-formula> of the neurons. The log-likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e192" xlink:type="simple"/></inline-formula> of the current evidences given the cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e193" xlink:type="simple"/></inline-formula> corresponds to the sum of excitatory EPSPs, which depend on the synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e194" xlink:type="simple"/></inline-formula>. This leads to the calculation<disp-formula id="pcbi.1003037.e195"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e195" xlink:type="simple"/><label>(11)</label></disp-formula>This shows that at all times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e196" xlink:type="simple"/></inline-formula> every spike from the WTA circuit represents one sample of the instantaneous posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e197" xlink:type="simple"/></inline-formula>.</p>
            <p>The crucial observation, however, is that this relation is valid at any point in time, independently of the inhibitory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e198" xlink:type="simple"/></inline-formula>. It is only the ratio between the quantities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e199" xlink:type="simple"/></inline-formula> that determines the relative firing probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e200" xlink:type="simple"/></inline-formula> of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e201" xlink:type="simple"/></inline-formula>.</p>
         </sec>
         <sec id="s2a4">
            <title>Background oscillations and learning with missing values</title>
            <p>We will now show that for the case of a low average input firing rate, a modulation of the firing rate can be beneficial, as it can synchronize firing of pre- and post-synaptic neurons. Each active neuron then fires according to an inhomogeneous Poisson process, and we assume for simplicity that the time course of the spike rate for all neurons follows the same oscillatory (sinusoidal) pattern around a common average firing rate. Nevertheless the spikes for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e202" xlink:type="simple"/></inline-formula> are drawn as samples from independent processes. In addition, let the common inhibition signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e203" xlink:type="simple"/></inline-formula> be modulated by an additional oscillatory current <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e204" xlink:type="simple"/></inline-formula> with amplitude <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e205" xlink:type="simple"/></inline-formula>, oscillation frequency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e206" xlink:type="simple"/></inline-formula> (same as for the input oscillation), and phase shift <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e207" xlink:type="simple"/></inline-formula>. Due to the increased number of input neurons firing simultaneously, and the additional background current, pre- and post-synaptic firing of active neurons will synchronize. The frequency of the background oscillation can be chosen in principle arbitrarily, as long as the number of periods per input example is constant. Otherwise the network will weight different input examples by the number of peaks during presentation, which might lead to learning of a different generative model.</p>
            <p>The effect of a synchronization of pre- and post-synaptic firing can be very beneficial, since at low input firing rates it might happen that none of the input neurons in a population of neurons encoding an external variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e208" xlink:type="simple"/></inline-formula> fires within the integration time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e209" xlink:type="simple"/></inline-formula> of output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e210" xlink:type="simple"/></inline-formula>. This corresponds to learning with missing attribute values for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e211" xlink:type="simple"/></inline-formula>, which is known to impair learning performance in graphical models <xref ref-type="bibr" rid="pcbi.1003037-Ghahramani1">[48]</xref>. Our novel interpretation is therefore that background oscillations can reduce the percentage of missing values by synchronizing presynaptic firing rates. This agrees with previous studies, which have shown that it is easier for single detector neurons learning with phenomenological STDP rules to detect spike patterns embedded in a high-dimensional input stream, if the patterns are encoded relative to a background oscillation <xref ref-type="bibr" rid="pcbi.1003037-Masquelier1">[49]</xref>, or the patterns consist of dense and narrow bursts of synchronous activity <xref ref-type="bibr" rid="pcbi.1003037-Gilson1">[50]</xref>. These results still hold if only a small part of the afferents participates in the pattern, or spikes from the pattern are missing, since the increased synchrony facilitates the identification of the pattern. Although we show in experiments that this increased synchronization can improve the learning performance of spike-based probabilistic learners in practice, it is important to note that background oscillations are not necessary for the theory of spike-based Expectation Maximization to hold. Also, brain oscillations have previously been associated with various fundamental cognitive functions like e.g. attention, memory, consciousness, or neural binding. In contrast, our suggested role for oscillations as a mechanism for improving learning and inference with missing values is very specific within our framework, and although some aspects are compatible with higher-level theories, we do not attempt here to provide alternative explanations for these phenomena.</p>
            <p>Our particular model of oscillatory input firing rates leaves the average firing rates unchanged, hence the effect of oscillations does not simply arise due to a larger number of input or output spikes. It is the increased synchrony of input and output spikes by which background oscillations can facilitate learning for tasks in which inputs have little redundancy, and missing values during learning thus would have a strong impact. We demonstrate this in the following experiment, where a common background oscillation for the input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e212" xlink:type="simple"/></inline-formula> and the output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e213" xlink:type="simple"/></inline-formula> significantly speeds up and improves the learning performance. In other naturally occurring input distributions with more structured inputs, oscillations might not improve the performance.</p>
         </sec>
      </sec>
      <sec id="s2b">
         <title>Example 1: Learning of probabilistic models with STDP</title>
         <p><xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref> demonstrates the emergence of Bayesian computation in the generic network motif of <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref> in a simple example. Spike inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e214" xlink:type="simple"/></inline-formula> (top row of <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3D</xref>) are generated through four different hidden processes (associated with four different colors). Each of them is defined by a Gauss distribution over a 2D pixel array with a different center, which defines the probability of every pixel to be on. Spike trains encode the current value of a pixel by a firing rate of 25 Hz or 0 Hz for 40 ms. Each pixel was encoded by two input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e215" xlink:type="simple"/></inline-formula> via population coding, exactly one of them had a firing rate of 25 Hz for each input image. A 10 ms period without firing separates two images in order to avoid overlap of EPSPs for input spikes belonging to different input images.</p>
         <fig id="pcbi-1003037-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g003</object-id><label>Figure 3</label>
            <caption>
               <title>Example for the emergence of Bayesian computation through STDP and adaptation of neural excitability.</title>
               <p><bold>A, B</bold>: Visualization of hidden structure in the spike inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e216" xlink:type="simple"/></inline-formula> shown in D, E: Each row in panels A and B shows two results of drawing pixels from the same Gauss distribution over a 28×28 pixel array. Four different Gauss distributions were used in the four rows, and the location of their center represents the latent variable behind the structure of the input spike train. <bold>C</bold>: Transformation of the four 2D images in B into four linear arrays, resulting from random projections from 2D locations to 1D indices. Black lines indicate active pixels, and pixels that were active in less than 4<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e217" xlink:type="simple"/></inline-formula> of all images were removed before the transformation (these pixels are white in panel H). By the random projection, both the 2D structure of the underlying pixel array and the value of the latent variable are hidden when the binary 1D vector is encoded through population coding into the spike trains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e218" xlink:type="simple"/></inline-formula> that the neural circuit receives. <bold>D</bold>: Top row: Spike trains from 832 input neurons that result from the four linear patterns shown in panel C (color of spikes indicates which of the four hidden processes had generated the underlying 2D pattern, after 50 ms another 2D pattern is encoded). The middle and bottom row show the spike output of the four output neurons at the beginning and after 500 s of unsupervised learning with continuous spike inputs (every 50 ms another 2D pattern was randomly drawn from one of the 4 different Gauss distributions, with different prior probabilities of 0.1, 0.2, 0.3, and 0.4.). Color of spikes indicates the emergent specialization of the four output neurons on the four hidden processes for input generation. Black spikes indicate incorrect guesses of hidden cause. <bold>E</bold>: Same as D, but with a superimposed 20 Hz oscillation on the firing rates of input neurons and membrane potentials of the output neurons. Fewer error spikes occur in the output, and output spikes are more precisely timed. <bold>F</bold>: Internal models (weight vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e219" xlink:type="simple"/></inline-formula>) of output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e220" xlink:type="simple"/></inline-formula> after learning (pixel array). <bold>G</bold>: Autonomous learning of priors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e221" xlink:type="simple"/></inline-formula>, that takes place simultaneously with the learning of internal models. <bold>H</bold>: Average “winner” among the four output neurons for a test example (generated with equal probability by any of the 4 Gaussians) when a particular pixel was drawn in this test example, indicating the impact of the learned priors on the output response. <bold>I</bold>: Emergent discrimination capability of the output neurons during learning (red curve). The dashed blue curve shows that a background oscillation as in E speeds up discrimination learning. Curves in G and I represent averages over 20 repetitions of the learning experiment.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g003" position="float" xlink:type="simple"/></fig>
         <p>After unsupervised learning with STDP for 500 s (applied to continuous streams of spikes as in panel D of <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>) the weight vectors shown in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3F</xref> (projected back into the virtual 2D input space) emerged for the four output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e222" xlink:type="simple"/></inline-formula>, demonstrating that these neurons had acquired internal models for the four different processes that were used to generate inputs. The four different processes for generating the underlying 2D input patterns had been used with different prior probabilities (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e223" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3G</xref> shows that this imbalance resulted in four different priors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e224" xlink:type="simple"/></inline-formula> encoded in the biases <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e225" xlink:type="simple"/></inline-formula> of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e226" xlink:type="simple"/></inline-formula>. When one compares the unequal sizes of the colored areas in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3H</xref> with the completely symmetric internal models (or likelihoods) of the four neurons shown in panel F, one sees that their firing probability approximates a posterior over hidden causes that results from multiplying their learned likelihoods with their learned priors. As a result, the spike output becomes sparser, and almost all neurons only fire when the current input spikes are generated by that one of the four hidden processes on which they have specialized (<xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3D</xref>, bottom row). In <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3I</xref> the performance of the network is quantified over time by the normalized conditional entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e227" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e228" xlink:type="simple"/></inline-formula> is the correct hidden cause of each input image <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e229" xlink:type="simple"/></inline-formula> in the training set, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e230" xlink:type="simple"/></inline-formula> denotes the discrete random variable defined by the firing probabilities of output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e231" xlink:type="simple"/></inline-formula> for each image under the currently learned model. Low conditional entropy indicates that each neuron learns to fire predominantly for inputs from one class. <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3E</xref> as well as the dashed blue line in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3I</xref> show that the learning process is improved when a common background oscillation at 20 Hz is superimposed on the firing rate of input neurons and the membrane potential of the output neurons, while keeping the average input and output firing rates constant. The reason is that in general it may occur that an output neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e232" xlink:type="simple"/></inline-formula> receives during its integration time window (40 ms in this example) no information about the value of a pixel (because neither the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e233" xlink:type="simple"/></inline-formula> that has a high firing rate for 40 ms if this pixel is black, nor the associated neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e234" xlink:type="simple"/></inline-formula> that has a high firing rate if this pixel is white fire during this time window). A background oscillation reduces the percentage of such missing values by driving presynaptic firing times together (see top row of <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3E</xref>). Note that through these oscillations the overall output firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e235" xlink:type="simple"/></inline-formula> fluctuates strongly, but since the same oscillation is used consistently for all four types of patterns, the circuit still learns the correct distribution of inputs.</p>
         <p>This task had been chosen to become very fast unsolvable if many pixel values are missing. Many naturally occurring input distributions, like the ones addressed in the subsequent computer experiments, tend to have more redundancy, and background oscillations did not improve the learning performance for those.</p>
      </sec>
      <sec id="s2c">
         <title>STDP approximates Expectation Maximization</title>
         <p>In this section we will develop the link between the unsupervised learning of the generative probabilistic model in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref> and the learning effect of STDP as defined in our spiking network model in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref>. Starting from a learning framework derived from the concept of Expectation Maximization <xref ref-type="bibr" rid="pcbi.1003037-Dempster1">[31]</xref>, we show that the biologically plausible STDP rule from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> can naturally approximate a stochastic, online version of this optimization algorithm. We call this principle SEM (spike-based EM).</p>
         <p>SEM can be viewed as a bootstrapping procedure. The relation between the firing probabilities of the neurons within the WTA circuit and the continuous updates of the synaptic weights with our STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> drive the initially random firing of the circuit in response to an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e236" xlink:type="simple"/></inline-formula> towards learning the correct generative model of the input distribution. Whenever a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e237" xlink:type="simple"/></inline-formula> fires in response to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e238" xlink:type="simple"/></inline-formula>, the STDP rule increases the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e239" xlink:type="simple"/></inline-formula> of synapses from those presynaptic neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e240" xlink:type="simple"/></inline-formula> that had fired shortly before <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e241" xlink:type="simple"/></inline-formula>. In absence of a recent presynaptic spike from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e242" xlink:type="simple"/></inline-formula> the weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e243" xlink:type="simple"/></inline-formula> is decreased. As a consequence, when next a pattern similar to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e244" xlink:type="simple"/></inline-formula> is presented, the probability for the same <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e245" xlink:type="simple"/></inline-formula> to fire and further adapt its weights, is increased. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e246" xlink:type="simple"/></inline-formula> becomes more of an “expert” for one subclass of input patterns, it actually becomes less likely to fire for non-matching patterns. The competition in the WTA circuit ensures that other <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e247" xlink:type="simple"/></inline-formula>-neurons learn to specialize for these different input categories.</p>
         <p>In the framework of Expectation Maximization, the generation of a spike in a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e248" xlink:type="simple"/></inline-formula>-neuron creates a sample from the currently encoded posterior distribution of hidden variables, and can therefore be viewed as the stochastic Expectation, or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e249" xlink:type="simple"/></inline-formula>-step. The subsequent application of STDP to the synapses of this neuron can be understood as an approximation of the Maximization, or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e250" xlink:type="simple"/></inline-formula>-step. The online learning behavior of the network can be understood as a stochastic online EM algorithm.</p>
         <sec id="s2c1">
            <title>Learning the parameters of the probability model by EM</title>
            <p>The goal of learning the parametrized generative probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e251" xlink:type="simple"/></inline-formula> is to find parameter values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e252" xlink:type="simple"/></inline-formula>, such that the marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e253" xlink:type="simple"/></inline-formula> of the model distribution approximates the actual stationary distribution of spike inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e254" xlink:type="simple"/></inline-formula> as closely as possible. We define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e255" xlink:type="simple"/></inline-formula> as the probability to observe the activation vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e256" xlink:type="simple"/></inline-formula> at some point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e257" xlink:type="simple"/></inline-formula> in time (see <xref ref-type="disp-formula" rid="pcbi.1003037.e892">Eq. (72)</xref> in <xref ref-type="sec" rid="s4">Methods</xref> for a precise mathematical definition). The learning task can thus be formalized as the minimization of the Kullback-Leibler divergence between the two distributions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e258" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e259" xlink:type="simple"/></inline-formula>. A mathematically equivalent formulation is the maximization of the expected likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e260" xlink:type="simple"/></inline-formula> of the inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e261" xlink:type="simple"/></inline-formula>, drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e262" xlink:type="simple"/></inline-formula>. The parametrization of the generative probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e263" xlink:type="simple"/></inline-formula> is highly redundant, i.e. for every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e264" xlink:type="simple"/></inline-formula> there is a continuous manifold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e265" xlink:type="simple"/></inline-formula>, that all define identical generative distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e266" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e554">Eq. (24)</xref>. There is, however, exactly one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e267" xlink:type="simple"/></inline-formula> in this sub-manifold of the weight space that fulfills the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>. By imposing the normalization conditions as constraints to the maximization problem, we can thus find unique local maxima (see “Details to Learning the parameters of the probability model by EM” in <xref ref-type="sec" rid="s4">Methods</xref>).</p>
            <p>The most common way to solve such unsupervised learning problems with hidden variables is the mathematical framework of Expectation Maximization (EM). In its standard form, the EM algorithm is a batch learning mechanism, in which a fixed, finite set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e268" xlink:type="simple"/></inline-formula> instances of input vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e269" xlink:type="simple"/></inline-formula> is given, and the task is to find the parameter vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e270" xlink:type="simple"/></inline-formula> that maximizes the log-likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e271" xlink:type="simple"/></inline-formula> of these <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e272" xlink:type="simple"/></inline-formula> instances to be generated as independent samples by the model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e273" xlink:type="simple"/></inline-formula>.</p>
            <p>Starting from a random initialization for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e274" xlink:type="simple"/></inline-formula>, the algorithm iterates between E-steps and M-steps. In the E-steps, the current parameter vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e275" xlink:type="simple"/></inline-formula> is used to find the posterior distributions of the latent variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e276" xlink:type="simple"/></inline-formula>, each given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e277" xlink:type="simple"/></inline-formula>.</p>
            <p>In the M-steps a new parameter vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e278" xlink:type="simple"/></inline-formula> is computed, which maximizes the expected value of the complete-data log-likelihood function, subject to the normalization constraints in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>. The analytical solution for this M-step (compare <xref ref-type="bibr" rid="pcbi.1003037-Bishop1">[32]</xref>) is given by<disp-formula id="pcbi.1003037.e279"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e279" xlink:type="simple"/><label>(12)</label></disp-formula>The iterated application of this update procedure is guaranteed to converge to a (local) maximum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e280" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003037-Dempster1">[31]</xref>. It is obvious that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e281" xlink:type="simple"/></inline-formula> fulfills the desired normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref> after every update.</p>
            <p>Although the above deterministic algorithm requires that the same set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e282" xlink:type="simple"/></inline-formula> training examples is re-used for every EM iteration, similar results also hold valid for online learning scenarios. In an online setup new samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e283" xlink:type="simple"/></inline-formula> are drawn from the input distribution at every iteration, which is closer to realistic neural network learning settings. Instead of analytically computing the expected value of the complete-data log-likelihood function, a Monte-Carlo estimate is computed using the samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e284" xlink:type="simple"/></inline-formula>, drawn according to their posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e285" xlink:type="simple"/></inline-formula>. Even though additional stochastic fluctuations are introduced due to the stochastic sampling process, this stochastic EM algorithm will also converge to a stable result in the limit of infinite iterations, if the number of samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e286" xlink:type="simple"/></inline-formula> is increased with every iteration <xref ref-type="bibr" rid="pcbi.1003037-Jank1">[51]</xref>.</p>
            <p>In order to simplify the further notation we introduce the augmented input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e287" xlink:type="simple"/></inline-formula> from which we can sample pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e288" xlink:type="simple"/></inline-formula> and define<disp-formula id="pcbi.1003037.e289"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e289" xlink:type="simple"/><label>(13)</label></disp-formula></p>
            <p>Sampling pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e290" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e291" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e292" xlink:type="simple"/></inline-formula> corresponds to online sampling of inputs, combined with a stochastic E-step. The subsequent M-step<disp-formula id="pcbi.1003037.e293"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e293" xlink:type="simple"/><label>(14)</label></disp-formula>essentially computes averages over all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e294" xlink:type="simple"/></inline-formula> samples: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e295" xlink:type="simple"/></inline-formula> is the average of the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e296" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e297" xlink:type="simple"/></inline-formula> is a conditional average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e298" xlink:type="simple"/></inline-formula> taken over those instances in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e299" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e300" xlink:type="simple"/></inline-formula>.</p>
            <p>The expected value of the new weight vector after one iteration, i.e., the sampling E-step and the averaging M-step, can be expressed in a very compact form based on the augmented input distribution as<disp-formula id="pcbi.1003037.e301"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e301" xlink:type="simple"/><label>(15)</label></disp-formula></p>
            <p>A necessary condition for a point convergence of the iterative algorithm is a stable equilibrium point, i.e. a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e302" xlink:type="simple"/></inline-formula> at which the expectation of the next update <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e303" xlink:type="simple"/></inline-formula> is identical to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e304" xlink:type="simple"/></inline-formula>. Thus we arrive at the following necessary implicit condition for potential convergence points of this stochastic algorithm.<disp-formula id="pcbi.1003037.e305"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e305" xlink:type="simple"/><label>(16)</label></disp-formula>This very intuitive implicit “solution” is the motivation for relating the function of the simple STDP learning rule (solid red line in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>) in the neural circuit shown in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref> to the framework of EM.</p>
         </sec>
         <sec id="s2c2">
            <title>Spike-based Expectation Maximization</title>
            <p>In order to establish a mathematically rigorous link between the STDP rule in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> in the spike-based WTA circuit and stochastic online EM we identify the functionality of both the E- and the M-steps with the learning behavior of the spiking WTA-circuit with STDP.</p>
            <p>In a biologically plausible neural network setup, one cannot assume that observations are stored and computations necessary for learning are deferred until a suitable sample size has been reached. Instead, we relate STDP learning to online learning algorithms in the spirit of Robbins-Monro stochastic approximations, in which updates are performed after every observed input.</p>
            <p>At an arbitrary point in time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e306" xlink:type="simple"/></inline-formula> at which any one neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e307" xlink:type="simple"/></inline-formula> of the WTA circuit fires, the posterior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e308" xlink:type="simple"/></inline-formula> according to <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> gives the probability that the spike at this time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e309" xlink:type="simple"/></inline-formula> has originated from the neuron with index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e310" xlink:type="simple"/></inline-formula>. The pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e311" xlink:type="simple"/></inline-formula> can therefore be seen as a sample from the augmented input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e312" xlink:type="simple"/></inline-formula>. Hence, we can conclude that the generation of a spike by the WTA circuit corresponds to the generation of samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e313" xlink:type="simple"/></inline-formula> during the E-step. There are additional conditions on the inhibition signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e314" xlink:type="simple"/></inline-formula> that have to be met in order to generate unbiased samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e315" xlink:type="simple"/></inline-formula> from the input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e316" xlink:type="simple"/></inline-formula>. These are discussed in depth in the section “Role of the Inhibition”, but for now let us assume that these conditions are fulfilled.</p>
            <p>The generation of a spike in the postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e317" xlink:type="simple"/></inline-formula> triggers an STDP update according to <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> in all synapses from incoming presynaptic neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e318" xlink:type="simple"/></inline-formula>, represented by weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e319" xlink:type="simple"/></inline-formula>. We next show that the biologically plausible STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> (see also <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>) together with the rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> can be derived as approximating the M-step in stochastic online EM.</p>
            <p>The update in <xref ref-type="disp-formula" rid="pcbi.1003037.e293">Eq. (14)</xref> suggests that every synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e320" xlink:type="simple"/></inline-formula> collects the activation statistics of its input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e321" xlink:type="simple"/></inline-formula> (the presynaptic neuron), given that its output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e322" xlink:type="simple"/></inline-formula> (the postsynaptic neuron) fires. These statistics can be gathered online from samples of the augmented input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e323" xlink:type="simple"/></inline-formula>.</p>
            <p>From this statistical perspective each weight can be interpreted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e324" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e325" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e326" xlink:type="simple"/></inline-formula> are two local virtual counters in each synapse. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e327" xlink:type="simple"/></inline-formula> represents the number of the events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e328" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e329" xlink:type="simple"/></inline-formula> represents the number of the events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e330" xlink:type="simple"/></inline-formula>, i.e. the postsynaptic spikes. Even though all virtual counters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e331" xlink:type="simple"/></inline-formula> within one neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e332" xlink:type="simple"/></inline-formula> count the same postsynaptic spikes, it is easier to think of one individual such counter for every synapse. If we interpret the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e333" xlink:type="simple"/></inline-formula> as a local learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e334" xlink:type="simple"/></inline-formula>, we can derive <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> (see <xref ref-type="sec" rid="s4">Methods</xref>) as the spike-event triggered stochastic online learning rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e335" xlink:type="simple"/></inline-formula> that approximates in the synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e336" xlink:type="simple"/></inline-formula> the log of the running average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e337" xlink:type="simple"/></inline-formula> at the spiking times of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e338" xlink:type="simple"/></inline-formula>. The update formula shows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e339" xlink:type="simple"/></inline-formula> is only changed, if the postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e340" xlink:type="simple"/></inline-formula> fires, whereas spike events of other neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e341" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e342" xlink:type="simple"/></inline-formula> are irrelevant for the statistics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e343" xlink:type="simple"/></inline-formula>. Thus the learning rule is purely local for every synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e344" xlink:type="simple"/></inline-formula>; it only has to observe its own pre- and postsynaptic signals. Additionally we show in the <xref ref-type="sec" rid="s4">Methods</xref> section “Adaptive learning rates with Variance tracking” a very efficient heuristic how the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e345" xlink:type="simple"/></inline-formula> can be estimated locally.</p>
            <p>Analogously we can derive the working mechanism of the update rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> as updates of the log of a fraction at the respective points in time.</p>
            <p>The simple STDP rules in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> thus approximate the M-step in a formal generative probabilistic model with local, biologically plausible computations. It remains to be shown that these STDP rules actually drive the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e346" xlink:type="simple"/></inline-formula> to converge to the target points in <xref ref-type="disp-formula" rid="pcbi.1003037.e305">Eq. (16)</xref> of the stochastic EM algorithm.</p>
            <p>We can conclude from the equilibrium conditions of the STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e086">Eq. (6)</xref> that convergence can only occur at the desired local maxima of the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e347" xlink:type="simple"/></inline-formula> subject to the normalization constraints. However, it remains to be shown that the update algorithm converges at all and that there are no limit cycles.</p>
         </sec>
         <sec id="s2c3">
            <title>Proof of convergence</title>
            <p>Even though we successfully identified the learning behavior of the simple STDP rule (<xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>) in the circuit model with the E- and the M-steps of the EM algorithm, this is not yet sufficient for a complete proof of convergence for the whole learning system. Not only are the single updates just approximations to the M-step, these approximations, in addition, violate the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>. Although the system - as we will show - converges towards normalized solutions, there is always a stochastic fluctuation around the normalization conditions. One can therefore not simply argue that <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> implements a stochastic version of the generalized EM algorithm; instead, we have to resort to the theory of stochastic approximation algorithms as presented in <xref ref-type="bibr" rid="pcbi.1003037-Kushner1">[52]</xref>. Under some technical assumptions (see <xref ref-type="sec" rid="s4">Methods</xref>) we can state</p>
            <p>Theorem 1: <italic>The algorithm in </italic><xref ref-type="disp-formula" rid="pcbi.1003037.e076"><italic>Eq. (5</italic></xref>,<xref ref-type="disp-formula" rid="pcbi.1003037.e095"><italic>7)</italic></xref><italic> updates </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e348" xlink:type="simple"/></inline-formula><italic> in a way that it converges with probability 1 to the set of local maxima of the likelihood function </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e349" xlink:type="simple"/></inline-formula><italic>, subject to the normalization constraints in </italic><xref ref-type="disp-formula" rid="pcbi.1003037.e177"><italic>Eq. (10)</italic></xref><italic>.</italic></p>
            <p>The detailed proof, which is presented in <xref ref-type="sec" rid="s4">Methods</xref>, shows that the expected trajectory of the weight vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e350" xlink:type="simple"/></inline-formula> is determined by two driving forces. The first one is a normalization force which drives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e351" xlink:type="simple"/></inline-formula> from every arbitrary point towards the regime where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e352" xlink:type="simple"/></inline-formula> is normalized. The second force is the real learning force that drives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e353" xlink:type="simple"/></inline-formula> to a desired maximum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e354" xlink:type="simple"/></inline-formula>. However, this interpretation of the learning force is valid only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e355" xlink:type="simple"/></inline-formula> is sufficiently close to normalized.</p>
         </sec>
      </sec>
      <sec id="s2d">
         <title>The role of the inhibition</title>
         <p>We have previously shown that the output spikes of the WTA circuit represent samples from the posterior distribution in <xref ref-type="disp-formula" rid="pcbi.1003037.e195">Eq. (11)</xref>, which only depends on the ratios between the membrane potentials <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e356" xlink:type="simple"/></inline-formula>. The rate at which these samples are produced is the overall firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e357" xlink:type="simple"/></inline-formula> of the WTA circuit and can be controlled by modifying the common inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e358" xlink:type="simple"/></inline-formula> of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e359" xlink:type="simple"/></inline-formula>.</p>
         <p>Although any time-varying output firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e360" xlink:type="simple"/></inline-formula> produces correct samples from the posterior distribution in <xref ref-type="disp-formula" rid="pcbi.1003037.e195">Eq. (11)</xref> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e361" xlink:type="simple"/></inline-formula>, for learning we also require that the input patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e362" xlink:type="simple"/></inline-formula> observed at the spike times are unbiased samples from the true input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e363" xlink:type="simple"/></inline-formula>. If this is violated, some patterns coincide with a higher <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e364" xlink:type="simple"/></inline-formula>, and thus have a stronger influence on the learned synaptic weights. In <xref ref-type="sec" rid="s4">Methods</xref> we formally show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e365" xlink:type="simple"/></inline-formula> acts as a multiplicative weighting of the current input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e366" xlink:type="simple"/></inline-formula>, and so the generative model will learn a slightly distorted input distribution.</p>
         <p>An unbiased set of samples can be obtained if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e367" xlink:type="simple"/></inline-formula> is independent of the current input activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e368" xlink:type="simple"/></inline-formula>, e.g. if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e369" xlink:type="simple"/></inline-formula> is constant. This could in theory be achieved if we let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e370" xlink:type="simple"/></inline-formula> depend on the current values of the membrane potentials <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e371" xlink:type="simple"/></inline-formula>, and set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e372" xlink:type="simple"/></inline-formula>. Such an immediate inhibition is commonly assumed in rate-based soft-WTA models, but it seems implausible to compute this in a spiking neuronal network, where only spikes can be observed, but not the presynaptic membrane potentials.</p>
         <p>However, our results show that a perfectly constant firing rate is not a prerequisite for convergence to the right probabilistic model. Indeed we can show that it is sufficient that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e373" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e374" xlink:type="simple"/></inline-formula> are stochastically independent, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e375" xlink:type="simple"/></inline-formula> is not correlated to the appearance of any specific value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e376" xlink:type="simple"/></inline-formula>. Still this might be difficult to achieve since the firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e377" xlink:type="simple"/></inline-formula> is functionally linked to the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e378" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e379" xlink:type="simple"/></inline-formula>, but it clarifies the role of the inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e380" xlink:type="simple"/></inline-formula> as de-correlating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e381" xlink:type="simple"/></inline-formula> from the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e382" xlink:type="simple"/></inline-formula>, at least in the long run.</p>
         <p>One possible biologically plausible mechanism for such a decorrelation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e383" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e384" xlink:type="simple"/></inline-formula> is an inhibitory feedback from a population of neurons that is itself excited by the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e385" xlink:type="simple"/></inline-formula>. Such WTA competition through lateral inhibition has been studied extensively in the literature <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Oster1">[33]</xref>. In the implementation used for the experiments in this paper every spike from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e386" xlink:type="simple"/></inline-formula>-neurons causes an immediate very strong inhibition signal that lasts longer than the refractory period of the spiking neuron. This strong inhibition decays exponentially and is overlaid by a noise signal with high variability that follows an Ornstein-Uhlenbeck process (see “Inhibition Model in Computer Simulations” in <xref ref-type="sec" rid="s4">Methods</xref>). This will render the time of the next spike of the system almost independent of the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e387" xlink:type="simple"/></inline-formula>.</p>
         <p>It should also be mentioned that a slight correlation between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e388" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e389" xlink:type="simple"/></inline-formula> may be desirable, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e390" xlink:type="simple"/></inline-formula> might also be externally modulated (for example through attention, or neuromodulators such as Acetylcholin), as an instrument of selective input learning. This might lead e.g. to slightly higher firing rates for well-known inputs (high <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e391" xlink:type="simple"/></inline-formula>), or salient inputs, as opposed to reduced rates for unknown arbitrary inputs. In general, however, combining online learning with a sampling rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e392" xlink:type="simple"/></inline-formula> that is correlated to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e393" xlink:type="simple"/></inline-formula> may lead to strange artifacts and might even prohibit the convergence of the system due to positive feedback effects. A thorough analysis of such effects and of possible learning mechanisms that cope with positive feedback effects is the topic of future research.</p>
         <p>Our theoretical analysis sheds new light on the requirements for inhibition in spiking WTA-like circuits to support learning and Bayesian computation. Inhibition does not only cause competition between the excitatory neurons, but also regulates the overall firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e394" xlink:type="simple"/></inline-formula> of the WTA circuit. Variability in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e395" xlink:type="simple"/></inline-formula> does not influence the performance of the circuit, as long as there is no systematic dependence between the input and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e396" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2e">
         <title>Continuous-time interpretation with realistically shaped EPSPs</title>
         <p>In our previous analysis we have assumed a simplified non-additive step-function model for the EPSP. This allowed us to describe all input evidence within the last time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e397" xlink:type="simple"/></inline-formula> by one binary vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e398" xlink:type="simple"/></inline-formula>, but required us to assume that no two neurons within the same group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e399" xlink:type="simple"/></inline-formula> fired within that period. We will now give an intuitive explanation to show that this restriction can be dropped and present an interpretation for additive biologically plausibly shaped EPSPs as inference in a generative model.</p>
         <p>The postsynaptic activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e400" xlink:type="simple"/></inline-formula> under an additive EPSPs is given by the convolution<disp-formula id="pcbi.1003037.e401"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e401" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e402" xlink:type="simple"/></inline-formula> describes an arbitrarily shaped kernel, e.g. an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e403" xlink:type="simple"/></inline-formula>-shaped EPSP function which is the difference of two exponential functions (see <xref ref-type="bibr" rid="pcbi.1003037-Gerstner1">[37]</xref>) with different time constants. We use 1 ms for the rise and 15 ms for the decay in our simulations. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e404" xlink:type="simple"/></inline-formula> replaces <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e405" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e027">Eq. (2)</xref> in the computation of the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e406" xlink:type="simple"/></inline-formula> of our model neurons. We can still understand the firing of neurons in the WTA circuit according to the relative firing probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e407" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> as Bayesian inference. To see this, we imagine an extension of the generative probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e408" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref>, which contains multiple instances of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e409" xlink:type="simple"/></inline-formula>, exactly one for every input spike from all input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e410" xlink:type="simple"/></inline-formula>. For a fixed common hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e411" xlink:type="simple"/></inline-formula>, all instances of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e412" xlink:type="simple"/></inline-formula> are conditionally independent of each other, and have the same conditional distributions for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e413" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> for the full derivation of the extended probabilistic model). According to the definition in <xref ref-type="disp-formula" rid="pcbi.1003037.e139">Eq. (8)</xref> of the population code every input spike represents evidence that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e414" xlink:type="simple"/></inline-formula> in an instance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e415" xlink:type="simple"/></inline-formula> should take on a certain value. Since every spike contributes only to one instance, any finite input spike pattern can be interpreted as valid evidence for multiple instances of inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e416" xlink:type="simple"/></inline-formula>.</p>
         <p>The inference of a single hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e417" xlink:type="simple"/></inline-formula> in such extended graphical model from multiple instances of evidence is relatively straightforward: due to the conditional independence of different instances, we can compute the input likelihood for any hidden cause simply as the product of likelihoods for every single evidence. Inference thus reduces to counting how often every possible evidence occurred in all instances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e418" xlink:type="simple"/></inline-formula>, which means counting the number of spikes of every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e419" xlink:type="simple"/></inline-formula>. Since single likelihoods are implicitly encoded in the synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e420" xlink:type="simple"/></inline-formula> by the relationship <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e421" xlink:type="simple"/></inline-formula>, we can thus compute the complete input likelihood by adding up step-function like EPSPs with amplitudes corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e422" xlink:type="simple"/></inline-formula>. This yields correct results, even if one input neuron spikes multiple times.</p>
         <p>In the above model, the timing of spikes does not play a role. If we want to assign more weight to recent evidence, we can define a heuristic modification of the extended graphical model, in which contributions from spikes to the complete input log-likelihood are linearly interpolated in time, and multiple pieces of evidence simply accumulate. This is exactly what is computed in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e423" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref>, where the shape of the kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e424" xlink:type="simple"/></inline-formula> defines how the contribution of an input spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e425" xlink:type="simple"/></inline-formula> evolves over time. Defining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e426" xlink:type="simple"/></inline-formula> as the weight for the evidence of the assignment of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e427" xlink:type="simple"/></inline-formula> to value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e428" xlink:type="simple"/></inline-formula>, it is easy to see (and shown in detail in <xref ref-type="sec" rid="s4">Methods</xref>) that the instantaneous output distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e429" xlink:type="simple"/></inline-formula> represents the result of inference over causes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e430" xlink:type="simple"/></inline-formula>, given the time-weighted evidences of all previous input spikes, where the weighting is done by the EPSP-function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e431" xlink:type="simple"/></inline-formula>. Note that this evidence weighting mechanism is not equivalent to the much more complex mechanism for inference in presence of uncertain evidence, which would require more elaborate architectures than our feed-forward WTA-circuit. In our case, past evidence does not become uncertain, but just less important for the inference of the instantaneous hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e432" xlink:type="simple"/></inline-formula>.</p>
         <p>We can analogously generalize the spike-triggered learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> for continuous-valued input activations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e433" xlink:type="simple"/></inline-formula> according to <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref>:<disp-formula id="pcbi.1003037.e434"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e434" xlink:type="simple"/><label>(18)</label></disp-formula>The update of every weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e435" xlink:type="simple"/></inline-formula> is triggered when neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e436" xlink:type="simple"/></inline-formula>, i.e. the postsynaptic neuron, fires a spike. The shape of the LTP part of the STDP curve is determined by the shape of the EPSP, defined by the kernel function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e437" xlink:type="simple"/></inline-formula>. The positive part of the update in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref> is weighted by the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e438" xlink:type="simple"/></inline-formula> at the time of firing the postsynaptic spike. Negative updates are performed if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e439" xlink:type="simple"/></inline-formula> is close to zero, which indicates that no presynaptic spikes were observed recently. The complex version of the STDP curve (blue dashed curve in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref>), which resembles more closely to the experimentally found STDP curves, results from the use of biologically plausible <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e440" xlink:type="simple"/></inline-formula>-shaped EPSPs. In this case, the LTP window of the weight update decays with time, following the shape of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e441" xlink:type="simple"/></inline-formula>-function. This form of synaptic plasticity was used in all our experiments. If EPSPs accumulate due to high input stimulation frequencies, the resulting shape of the STDP curve becomes even more similar to previously observed experimental data, which is investigated in detail in the following section.</p>
         <p>The question remains, how this extension of the model and the heuristics for time-dependent weighting of spike contributions affect the previously derived theoretical properties. Although the convergence proof does not hold anymore under such general conditions we can expect (and show in our Experiments) that the network will still show the principal behavior of EM under fairly general assumptions on the input: we have to assume that the instantaneous spike rate of every input group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e442" xlink:type="simple"/></inline-formula> is not dependent on the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e443" xlink:type="simple"/></inline-formula> that it currently encodes, which means that the total input spike rate must not depend on the hidden cause <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e444" xlink:type="simple"/></inline-formula>. Note that this assumption on every input group is identical to the desired output behavior of the WTA circuit according to the conditions on the inhibition as derived earlier. This opens up the possibility of building networks of recursively or hierarchically connected WTA circuits. Note also that the grouping of inputs into different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e445" xlink:type="simple"/></inline-formula> is only a notational convenience. The neurons in the WTA circuit do not have to know which inputs are from the same group, neither for inference nor for learning, and can thus treat all input neurons equally.</p>
      </sec>
      <sec id="s2f">
         <title>Relationship to experimental data on synaptic plasticity</title>
         <p>In biological STDP experiments that induce pairs of pre- and post-synaptic spikes at different time delays, it has been observed that the shape of the plasticity curve changes as a function of the repetition frequency for those spike pairs <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>. The observed effect is that at very low frequencies no change or only LTD occurs, a “classical” STDP window with timing-dependent LTD and LTP is observed at intermediate frequencies around 20 Hz, and at high frequencies of 40 Hz or above only LTP is observed, independently of which spikes comes first.</p>
         <p>Although our theoretical model does not explicitly include a stimulation-frequency dependent term like other STDP models (e.g. <xref ref-type="bibr" rid="pcbi.1003037-Gjorgjieva1">[53]</xref>), we can study empirically the effect of a modification of the frequency of spike-pairing. We simulate this for a single synapse, at which we force pre- and post-synaptic spikes with varying time differences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e446" xlink:type="simple"/></inline-formula>, and at fixed stimulation frequencies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e447" xlink:type="simple"/></inline-formula> of either 1 Hz, 20 Hz, or 40 Hz. Modeling EPSPs as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e448" xlink:type="simple"/></inline-formula>-kernels with time constants of 1 ms for the rise and 15 ms for the decay, we obtain the low-pass filtered signals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e449" xlink:type="simple"/></inline-formula> as in <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref>, which grow as EPSPs start to overlap at higher stimulation frequencies. At the time of a post-synaptic spike we compute the synaptic update according to the rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref>, but keep both the weight and the learning rate fixed (at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e450" xlink:type="simple"/></inline-formula>) to distinguish timing-dependent from weight-dependent effects.</p>
         <p>In <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4A</xref> we observe that, as expected, at low stimulation frequencies (1 Hz) the standard shape of the complex STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref> from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> is recovered, since there is no influence from previous spikes. The shift towards pure LTD that is observed in biology <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref> would require an additional term that depends on postsynaptic firing rates like in <xref ref-type="bibr" rid="pcbi.1003037-Gjorgjieva1">[53]</xref>, and is a topic of future research. However, note that in biology this shift to LTD was observed only in paired recordings, neglecting the cooperative effect of other synapses, and other studies have also reported LTP at low stimulation frequencies <xref ref-type="bibr" rid="pcbi.1003037-Bi1">[43]</xref>. At higher stimulation frequencies (20 Hz in <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4B</xref>) the EPSPs from different pre-synaptic spikes start to overlap, which results in larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e451" xlink:type="simple"/></inline-formula> compared with isolated pre-synaptic spikes. We also see that the LTD part of the STDP window becomes timing-dependent (due to overlapping EPSPs), and thus the shape of the STDP curve becomes similar to standard models of STDP and observed biological data <xref ref-type="bibr" rid="pcbi.1003037-Bi1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Song2">[54]</xref>. For even higher stimulation frequencies the STDP window shifts more and more towards LTP (see <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4B and C</xref>). This is in good accordance with observations in biology <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>. Also in agreement with biological data, the minimum of the update occurs around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e452" xlink:type="simple"/></inline-formula>, because there the new <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e453" xlink:type="simple"/></inline-formula>-kernel EPSP is not yet effective, and the activation due to previous spikes has decayed maximally.</p>
         <fig id="pcbi-1003037-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g004</object-id><label>Figure 4</label>
            <caption>
               <title>Relationship between the continuous-time SEM model and experimental data on synaptic plasticity.</title>
               <p><bold>A–C</bold>: The effect of the continuous-time plasticity rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref> at a single synapse for different stimulation frequencies and different time-differences between pre- and post-synaptic spike pairs. Only time-intervals without overlapping pairs are shown. <bold>A</bold>: For very low stimulation frequencies (1 Hz) the standard shape of the complex learning rule from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> is recovered. <bold>B</bold>: At a stimulation frequency of 20 Hz the plasticity curve shifts more towards LTP, and depression is no longer time independent, due to overlapping EPSPs. <bold>C</bold>: At high stimulation frequencies of 40 Hz or above, the STDP curve shifts towards only LTP, and thus becomes similar to a rate-based Hebbian learning rule. <bold>D</bold>: Cumulative effect of pre- and post-synaptic burst stimulation (50 Hz bursts of 5 pre-synaptic and 4 post-synaptic spikes) with different onset delays of -120, -60, 10, 20, 30, 80 and 140 ms (time difference between the onsets of the post- and pre-synaptic bursts). As in <xref ref-type="bibr" rid="pcbi.1003037-Kobayashi1">[55]</xref>, the amount of overlap between bursts determines the magnitude of LTP, rather than the exact temporal order of spikes.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g004" position="float" xlink:type="simple"/></fig>
         <p>Another effect that is observed in hippocampal synapses when two neurons are stimulated with bursts, is that the magnitude of LTP is determined mostly by the amount of overlap between the pre- and post-synaptic bursts, rather than the exact timing of spikes <xref ref-type="bibr" rid="pcbi.1003037-Kobayashi1">[55]</xref>. In <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4D</xref> we simulated this protocol with our continuous-time SEM rule for different onset time-differences of the bursts, and accumulated the synaptic weight updates in response to 50 Hz bursts of 5 pre-synaptic and 4 post-synaptic spikes. We performed this experiment for the same onset time differences used in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref> of <xref ref-type="bibr" rid="pcbi.1003037-Kobayashi1">[55]</xref>, and found qualitatively similar results. For long time-differences, when EPSPs have mostly decayed, we observed an LTD effect, which was not observed in biology, but can be attributed to differences in synaptic time constants between biology and simulation.</p>
         <p>These results suggest that our STDP rule derived from theoretical principles exhibits several of the key properties of synaptic plasticity observed in nature, depending on the encoding of inputs. This is quite remarkable, since these properties are not explicitly part of our learning rule, but rather emerge from a simpler rule with strong theoretical guarantees. Other phenomenological <xref ref-type="bibr" rid="pcbi.1003037-Morrison1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Clopath1">[57]</xref> or mechanistic models of STDP <xref ref-type="bibr" rid="pcbi.1003037-Graupner1">[58]</xref> also show some of these characteristics, but come without such theoretical properties. The functional consequence of reproducing such key biological characteristics of STDP is that our new learning rule also exhibits most of the key functional properties of STDP, like e.g. strengthening synapses of inputs that are causally involved in firing the postsynaptic neuron, while pruning the connections that do not causally contribute to postsynaptic firing <xref ref-type="bibr" rid="pcbi.1003037-Song1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Abbott1">[13]</xref>. At low and intermediate firing rates our rule also shifts the onset of postsynaptic firing towards the start of repeated spike patterns <xref ref-type="bibr" rid="pcbi.1003037-Masquelier1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Gilson1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Masquelier2">[59]</xref>, while depressing synapses that only become active for a pattern following the one for which the post-synaptic neuron is responsive. If patterns change quickly, then the stronger depression for presynaptic spikes with small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e454" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4B</xref> enhances the capability of the WTA to discriminate such patterns. With simultaneous high frequency stimulation (<xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4C and D</xref>) we observe that only LTP occurs, which is due to the decay of EPSPs not being fast enough to allow depression. In this scenario, the learning rule is less sensitive to timing, and rather becomes a classical Hebbian measure of correlations between pre- and post-synaptic firing rates. However, since inputs are encoded in a population code we can assume that the same neuron is not continuously active throughout, and so even at high firing rates for active input neurons, the synapses that are inactive during postsynaptic firing will still be depressed, which means that convergence to an equilibrium value is still possible for all synapses.</p>
         <p>It is a topic of future research which effects observed in biology can be reproduced with more complex variations of the spike-based EM rule that are also dependent on postsynaptic firing rates, or whether existing phenomenological models of STDP can be interpreted in the probabilistic EM framework. In fact, initial experiments have shown that several variations of the spike-based EM rule can lead to qualitatively similar empirical results for the learned models in tasks where the input spike trains are Poisson at average or high rates over an extended time window (such as in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>). These variations include weight-dependent STDP rules that are inversed in time, symmetrical in time, or have both spike timing-dependent LTD and LTP. Such rules can converge towards the same equilibrium values as the typical causal STDP rule. However, they will behave differently if inputs are encoded through spatio-temporal spike patterns (as in Example 4: Detection of Spatio-Temporal Spike Patterns). Further variations can include short-term plasticity effects for pre-synaptic spikes, as observed and modeled in <xref ref-type="bibr" rid="pcbi.1003037-Froemke1">[60]</xref>, which induce a stimulation-frequency dependent reduction of the learning rate, and could thus serve as a stabilization mechanism.</p>
      </sec>
      <sec id="s2g">
         <title>Spike-timing dependent LTD</title>
         <p>Current models of STDP typically assume a “double-exponential” decaying shape of the STDP curve, which was first used in <xref ref-type="bibr" rid="pcbi.1003037-Song2">[54]</xref> to fit experimental data. This is functionally different from the shape of the complex STDP curve in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>, where the LTD part is realized by a constant timing-independent offset.</p>
         <p>Although not explicitly covered by the previously presented theory of SEM, the same analytical tools can be used to explain functional consequences of timing-dependent LTD in our framework. Analogous to our approach for the standard SEM learning rule, we develop (in <xref ref-type="sec" rid="s4">Methods</xref>) an extension of the simple step-function STDP rule from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> with timing-dependent LTD, which is easier to analyze. We then generalize these results towards arbitrarily shaped STDP curves. The crucial result is that as long as the spike-timing dependent LTD rule retains the characteristic inversely-exponential weight-dependent relationship between the strengths of LTP and LTD that was introduced for standard SEM in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>, an equilibrium property similar to <xref ref-type="disp-formula" rid="pcbi.1003037.e086">Eq. (6)</xref> still holds (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Precisely speaking, the new equilibrium will be at the difference between the logarithms of the average presynaptic spiking probabilities <italic>before</italic> and <italic>after</italic> the postsynaptic spike. This shows that spike-timing dependent LTD also yields synaptic weights that can be interpreted in terms of log-probabilities, which can thus be used for inference.</p>
         <p>The new rule emphasizes contrasts between the current input pattern and the immediately following activity. Still, the results of the new learning rule and the original rule from <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> in our experiments are qualitatively similar. This can be explained from a stochastic learning perspective: at any point in time the relative spiking probabilities of excitatory neurons in the WTA circuit in <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> depend causally on the weighted sums of preceding presynaptic activities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e455" xlink:type="simple"/></inline-formula>. However, they clearly do not depend on future presynaptic activity. Thus, the postsynaptic neuron will learn through SEM to fire for increasingly similar stochastic realizations of presynaptic input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e456" xlink:type="simple"/></inline-formula>, whereas the presynaptic activity pattern following a postsynaptic spike will become more variable. In the extreme case where patterns are short and separated by noise, there will be no big difference between input patterns following firing of any of the WTA neurons, and so their relevance for the competition will become negligible.</p>
         <p>Experimental evidence shows that the time constants of the LTP learning window are usually smaller than the time constants of the LTD window (<xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Froemke1">[60]</xref>), which will further enhance the specificity of the LTP learning as opposed to the LTD part that computes the average over a longer window.</p>
         <p>Note that the exponential weight dependence of the learning rule implies a certain robustness towards linearly scaling LTP or LTD strengths, which only leads to a constant offset of the weights. Assuming that the offset is the same for all synapses, this does not affect firing probabilities of neurons in a WTA circuit (see <xref ref-type="sec" rid="s4">Methods</xref> “Weight offsets and positive weights”).</p>
      </sec>
      <sec id="s2h">
         <title>Example 2: Learning of probabilistic models for orientation selectivity</title>
         <p>We demonstrated in this computer experiment the emergence of orientation selective cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e457" xlink:type="simple"/></inline-formula> through STDP in the WTA circuit of <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref> when the spike inputs encode isolated bars in arbitrary orientations. Input images were generated by the following process: Orientations were sampled from a uniform distribution, and lines of 7 pixels width were drawn in a 28×28 pixel array. We added noise to the stimuli by flipping every pixel with a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e458" xlink:type="simple"/></inline-formula> chance, see <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5A</xref>. Finally, a circular mask was applied to the images to avoid artifacts from image corners. Spikes trains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e459" xlink:type="simple"/></inline-formula> were encoded according to the same population coding principle described in the previous example <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>, in this case using a Poisson firing rate of 20 Hz for active units.</p>
         <fig id="pcbi-1003037-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g005</object-id><label>Figure 5</label>
            <caption>
               <title>Emergence of orientation selective cells for visual input consisting of oriented bars with random orientations.</title>
               <p><bold>A</bold> Examples of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e460" xlink:type="simple"/></inline-formula>-pixel input images with oriented bars and additional background noise. <bold>B</bold> Internal models (weight vectors of output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e461" xlink:type="simple"/></inline-formula>) that are learned through STDP after the presentation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e462" xlink:type="simple"/></inline-formula> input images (each encoded by spike trains for 50 ms, as in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>). <bold>C, D</bold> Plot of the most active neuron for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e463" xlink:type="simple"/></inline-formula> images of bars with orientations from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e464" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e465" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e466" xlink:type="simple"/></inline-formula> steps. Colors correspond to the colors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e467" xlink:type="simple"/></inline-formula> neurons in B. Before training (<bold>C</bold>), the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e468" xlink:type="simple"/></inline-formula> output neurons fire without any apparent pattern. After training (<bold>D</bold>) they specialize on different orientations and cover the range of possible angles approximately uniformly. <bold>E</bold>: Spike train encoding of the 10 samples in A. <bold>F,G</bold>: Spike trains produced by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e469" xlink:type="simple"/></inline-formula> output neurons in response to these samples before and after learning with STDP for 200 s. Colors of the spikes indicate the identity of the output neuron, according to the color code in B.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g005" position="float" xlink:type="simple"/></fig>
         <p>After training with STDP for 200 s, presenting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e470" xlink:type="simple"/></inline-formula> different images, the projection of the learned weight vectors back into the 2D input space (<xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5B</xref>) shows the emergence of 10 models with different orientations, which cover the possible range of orientations almost uniformly. When we plot the strongest responding neuron as a function of orientation (<xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5C, D</xref>), measured by the activity in response to 360 noise-free images of oriented bars in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e471" xlink:type="simple"/></inline-formula> steps, we can see no structure in the response before learning (<xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5C</xref>). However, after unsupervised learning, panel D clearly shows the emergence of continuous, uniformly spaced regions in which one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e472" xlink:type="simple"/></inline-formula> neurons fires predominantly. This can also be seen in the firing behavior in response to the input spike trains in <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5E</xref>, which result from the example images in panel A. <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5F</xref> shows that the output neurons initially fire randomly in response to the input, and many different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e473" xlink:type="simple"/></inline-formula> neurons are active for one image. In contrast, the responses after learning in panel G are much sparser, and only occasionally multiple neurons are active for one input image, which is the case when the angle of the input image is in between the preferred angles of two output neurons, and therefore multiple models have a non-zero probability of firing.</p>
         <p>In our experiment the visual input consisted of noisy images of isolated bars, which illustrates learning of a probabilistic model in which a continuous hidden cause (the orientation angle) is represented by a population of neurons, and also provides a simple model for the development of orientation selectivity. It has previously been demonstrated that similar Gabor-like receptive field structures can be learned with a sparse-coding approach using patches of natural images as inputs <xref ref-type="bibr" rid="pcbi.1003037-Olshausen1">[61]</xref>. The scenario considered here is thus substantially simplified, since we do not present natural but isolated stimuli. However, it is worth noting that experimental studies have shown that (in mice and ferret) orientation selectivity, but not e.g. direction selectivity, exists in V1 neurons even before eye opening <xref ref-type="bibr" rid="pcbi.1003037-Li1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Espinosa1">[63]</xref>. This initial orientation selectivity develops from innate mechanisms and from internally generated inputs during this phase <xref ref-type="bibr" rid="pcbi.1003037-Espinosa1">[63]</xref>, e.g. retinal waves, which have different, and very likely simpler statistics than natural stimuli. Our model shows that a WTA circuit could learn orientation selectivity from such simple bar-like inputs, but does not provide an alternative explanation to the results of studies like <xref ref-type="bibr" rid="pcbi.1003037-Olshausen1">[61]</xref> using natural image stimuli. Although beyond the scope of this paper, we expect that later shaping of selectivity through exposure to natural visual experience would not alter the receptive fields by much, since the neurons have been primed to spike (and thereby trigger plasticity) only in response to a restricted class of local features.</p>
      </sec>
      <sec id="s2i">
         <title>Example 3: Emergent discrimination of handwritten digits through STDP</title>
         <p>Spike-based EM is a quite powerful learning principle, as we demonstrate in <xref ref-type="fig" rid="pcbi-1003037-g006">Fig. 6</xref> through an application to a computational task that is substantially more difficult than previously considered tasks for networks of spiking neurons: We show that a simple network of spiking neurons can learn without any supervision to discriminate handwritten digits from the MNIST benchmark dataset <xref ref-type="bibr" rid="pcbi.1003037-LeCun1">[64]</xref> consisting of 70,000 samples (30 are shown in <xref ref-type="fig" rid="pcbi-1003037-g006">Fig. 6A</xref>). This is one of the most frequently used benchmark tasks in machine learning. It has mostly been used to evaluate supervised or semi-supervised machine learning algorithms <xref ref-type="bibr" rid="pcbi.1003037-Hinton2">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Ciresan1">[65]</xref>, or to evaluate unsupervised feature learning approaches <xref ref-type="bibr" rid="pcbi.1003037-Hinton3">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Ranzato1">[67]</xref>. Although the MNIST dataset contains labels (the intended digit) for each sample of a handwritten digit, we deleted these labels when presenting the dataset to the neural circuit of <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref>, thereby forcing the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e474" xlink:type="simple"/></inline-formula> neurons on the output layer to self-organize in a completely unsupervised fashion. Each sample of a handwritten digit was encoded by 708 spike trains over 40 ms (and 10 ms periods without firing between digits to avoid overlap of EPSPs between images), similarly as for the task of <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>. Each pixel was represented by two input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e475" xlink:type="simple"/></inline-formula>, one of which produced a Poisson spike train at 40 Hz during these 40 ms. This yielded usually at most one or two spikes during this time window, demonstrating that the network learns and computes with information that is encoded through spikes, rather than firing rates. After 500 s of unsupervised learning by STDP almost all of the output neurons fired more sparsely, and primarily for handwritten samples of just one of the digits (see <xref ref-type="fig" rid="pcbi-1003037-g006">Fig. 6E</xref>).</p>
         <fig id="pcbi-1003037-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g006</object-id><label>Figure 6</label>
            <caption>
               <title>Emergent discrimination of handwritten digits through STDP.</title>
               <p><bold>A</bold>: Examples of digits from the MNIST dataset. The third and fourth row contain test examples that had not been shown during learning via STDP. <bold>B</bold>: Spike train encoding of the first 5 samples in the third row of A. Colors illustrate the different classes of digits. <bold>C, D</bold>: Spike trains produced by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e476" xlink:type="simple"/></inline-formula> output neurons before and after learning with STDP for 500 s. Colored spikes indicate that the class of the input and the class for which the neuron is mostly selective (based on human classification of its generative model shown in F) agree, otherwise spikes are black. <bold>E</bold>: Temporal evolution of the self-organization process of the 100 output neurons (for the complex version of STDP-curve shown in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref>), measured by the conditional entropy of digit labels under the learned models at different time points. <bold>F</bold>: Internal models generated by STDP for the 100 output neurons after 500 s. The network had not received any information about the number of different digits that exist and the colors for different ways of writing the first 5 digits were assigned by the human supervisor. On the basis of this assignment the test samples in row 3 of panel A had been recognized correctly.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g006" position="float" xlink:type="simple"/></fig>
         <p>The application to the MNIST dataset had been chosen to illustrate the power of SEM in complex tasks. MNIST is one of the most popular benchmarks in machine learning, and state-of-the-art methods achieve classification error rates well below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e477" xlink:type="simple"/></inline-formula>. The model learned by SEM can in principle also be used for classification, by assigning each neuron to the class for which it fires most strongly. However, since this is an unsupervised method, not optimized for classification but for learning a generative model, the performance is necessarily worse. We achieve an error rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e478" xlink:type="simple"/></inline-formula> on the 10-digit task on a previously unseen test set. This compares favorably to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e479" xlink:type="simple"/></inline-formula> error that we obtained with a standard machine learning approach that directly learned the mixture-of-multinomials graphical model in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref> with a batch EM algorithm. This control experiment was not constrained by a neural network architecture or biologically plausible learning, but instead mathematically optimized the parameters of the model in up to 200 iterations over the whole training set. The batch method achieves a final conditional entropy of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e480" xlink:type="simple"/></inline-formula>, which is slightly better than the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e481" xlink:type="simple"/></inline-formula> final result of the SEM approach, and shows that better performance on the classification task does not necessarily mean better unsupervised model learning.</p>
      </sec>
      <sec id="s2j">
         <title>Example 4: Detection of Spatio-Temporal Spike Patterns</title>
         <p>Our final application demonstrates that the modules for Bayesian computation that emerge in WTA circuits through STDP can not only explain the emergence of feature maps in primary sensory cortices like in <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5</xref>, but could also be viewed as generic computational units in generic microcircuits throughout the cortex. Such generic microcircuit receives spike inputs from many sources, and it would provide a very useful computational operation on these if it could autonomously detect repeatedly occurring spatio-temporal patterns within this high-dimensional input stream, and report their occurrence through a self-organizing sparse coding scheme to other microcircuits. We have created such input streams with occasionally repeated embedded spike patterns for the computer experiment reported in <xref ref-type="fig" rid="pcbi-1003037-g007">Fig. 7</xref>. <xref ref-type="fig" rid="pcbi-1003037-g007">Fig. 7D</xref> demonstrates that sparse output codes for the 5 embedded spike patterns emerge after applying STDP in a WTA circuit for 200 s to such input stream. Furthermore, we show in the Supplement that these sparse output codes generalize (even without any further training) to time-warped versions of these spike patterns.</p>
         <fig id="pcbi-1003037-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g007</object-id><label>Figure 7</label>
            <caption>
               <title>Output neurons self-organize via STDP to detect and represent spatio-temporal spike patterns.</title>
               <p><bold>A</bold>: Sample of the Poisson input spike trains at 20 Hz (only 100 of the 500 input channels are shown). Dashed vertical lines mark time segments of 50 ms length where spatio-temporal spike patterns are embedded into noise. <bold>B</bold>: Same spike input as in A, but spikes belonging to five repeating spatio-temporal patterns (frozen Poisson spike patterns at 15 Hz) are marked in five different colors. These spike patterns are superimposed by noise (Poisson spike trains at 5 Hz), and interrupted by segments of pure noise of the same statistics (Poisson spike trains at 20 Hz) for intervals of randomly varying time lengths. <bold>C, D</bold>: Firing probabilities and spike outputs of 6 output neurons (z-neurons in <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref>) for the spike input shown in A, after applying STDP for 200 s to continuous spike trains of the same structure (without any supervision or reward). These 6 output neurons have self-organized so that 5 of them specialize on one of the 5 spatio-temporal patterns. One of the 6 output neurons (firing probability and spikes marked in black) only responds to the noise between these patterns. The spike trains in A represent test inputs, that had never been shown during learning.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g007" position="float" xlink:type="simple"/></fig>
         <p>Even though our underlying probabilistic generative model (<xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref>) does not include time-dependent terms, the circuit in this example performs inference over time. The reason for this is that synapses that were active when a neuron fired become reinforced by STDP, and therefore make the neuron more likely to fire again when a similar spatial pattern is observed. Since we use EPSPs that smoothly decay over time, one neuron still sees a trace of previous input spikes as it fires again, and thus different spatial patterns within one reoccurring spatio-temporal pattern are recognized by the same neuron. The maximum length for such patterns is determined by the time constants of EPSPs. With our parameters (1 ms rise, 15 ms decay time constant) we were able to recognize spike patterns up to 50–100 ms. For longer spatio-temporal patterns, different neurons become responsive to different parts of the pattern. The neuron that responds mostly to noise in <xref ref-type="fig" rid="pcbi-1003037-g007">Figs. 7D</xref> did not learn a specific spatial pattern, and therefore wins by default when none of the specialized neurons responds. Similar effects have previously been described <xref ref-type="bibr" rid="pcbi.1003037-Masquelier2">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Masquelier3">[68]</xref>, but for different neuron models, classical STDP curves, and not in the context of probabilistic inference.</p>
         <p>For this kind of task, where also the exact timing of spikes in the patterns matters (which is not necessarily the case in the examples in <xref ref-type="fig" rid="pcbi-1003037-g003">Figs. 3</xref>, <xref ref-type="fig" rid="pcbi-1003037-g005">5</xref>, and <xref ref-type="fig" rid="pcbi-1003037-g006">6</xref>, where input neurons generate Poisson spike trains with different rates), we found that the shape of the STDP kernel plays a larger role. For example, a time-inverted version of the SEM rule, where pre-before-post firing causes LTD instead of LTP, cannot learn this kind of task, because once a neuron has learned to fire for a sub-pattern of the input, its firing onset is shifted back in time, rather than forward in time, which happens with standard SEM, but also with classical STDP <xref ref-type="bibr" rid="pcbi.1003037-Gilson1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Masquelier2">[59]</xref>. Instead, with a time-inverted SEM rule, different neurons would learn to fire stronger for the offsets of different patterns.</p>
         <p>Such emergent compression of high-dimensional spike inputs into sparse low-dimensional spike outputs could be used to merge information from multiple sensory modalities, as well as from internal sources (memory, predictions, expectations, etc.), and to report the co-occurrence of salient events to multiple other brain areas. This operation would be useful from the computational perspective no matter in which cortical area it is carried out. Furthermore, the computational modules that we have analyzed can easily be connected to form networks of such modules, since their outputs are encoded in the same way as their inputs: through probabilistic spiking populations that encode for abstract multinomial variables. Hence the principles for the emergence of Bayesian computation in local microcircuits that we have exhibited could potentially also explain the self-organization of distributed computations in large networks of such microcircuits.</p>
      </sec>
   </sec>
   <sec id="s3">
      <title>Discussion</title>
      <p>We have shown that STDP induces a powerful unsupervised learning principle in networks of spiking neurons with lateral inhibition: spike-based Expectation Maximization. Each application of STDP can be seen as a move in the direction of the M-step in a stochastic online EM algorithm that strives to maximize the log-likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e482" xlink:type="simple"/></inline-formula> of the spike input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e483" xlink:type="simple"/></inline-formula>. This is equivalent to the minimization of the Kullback-Leibler divergence between the true distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e484" xlink:type="simple"/></inline-formula> of spike inputs, and the generative model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e485" xlink:type="simple"/></inline-formula> that is implicitly represented by the WTA circuit from the Bayesian perspective. This theoretically founded principle guarantees that iterative applications of STDP to different spike inputs do not induce a meaningless meandering of the synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e486" xlink:type="simple"/></inline-formula> through weight space, but rather convergence to at least a local optimum in the fitting of the model to the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e487" xlink:type="simple"/></inline-formula> of high-dimensional spike inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e488" xlink:type="simple"/></inline-formula>. This generation of an internal model through STDP provides the primary component for the self-organization of Bayesian computation. We have shown that the other component, the prior, results from a simple rule for use-dependent adaptation of neuronal excitability. As a consequence, the firing of a neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e489" xlink:type="simple"/></inline-formula> in a stochastic WTA circuit (<xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref>) can be viewed as sampling from the posterior distribution of hidden causes for high-dimensional spike inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e490" xlink:type="simple"/></inline-formula> (and simultaneously as the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e491" xlink:type="simple"/></inline-formula>-step in the context of online EM): A prior (encoded by the thresholds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e492" xlink:type="simple"/></inline-formula> of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e493" xlink:type="simple"/></inline-formula>) is multiplied with a likelihood (encoded through an implicit generative distribution defined by the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e494" xlink:type="simple"/></inline-formula> of these neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e495" xlink:type="simple"/></inline-formula>), to yield through the firing probabilities of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e496" xlink:type="simple"/></inline-formula> a representation of the posterior distribution of hidden causes for the current spike input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e497" xlink:type="simple"/></inline-formula>. The multiplications and the divisive normalization that are necessary for this model are carried out by the linear neurons in the log-scale. This result is then transformed into an instantaneous firing rate, assuming an exponential relationship between rate and the membrane potential <xref ref-type="bibr" rid="pcbi.1003037-Jolivet1">[38]</xref>. It is important that the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e498" xlink:type="simple"/></inline-formula> fire stochastically, i.e., that there exists substantial trial-to trial variability, since otherwise they could not represent a probability distribution. Altogether our models supports the view that probability distributions, rather than deterministic neural codes, are the primary units of information in the brain, and that computational operations are carried out on probabilities, rather than on deterministic bits of information.</p>
      <p>Following the “probabilistic turn” in cognitive science <xref ref-type="bibr" rid="pcbi.1003037-Griffiths1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Griffiths2">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Oaksford1">[69]</xref> and related hypotheses in computational neuroscience <xref ref-type="bibr" rid="pcbi.1003037-Rao1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Doya1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Krding1">[5]</xref>, probabilistic inference has become very successful in explaining behavioral data on human reasoning and other brain functions. Yet, it has remained an important open problem how networks of spiking neurons can learn to implement those probabilistic inference operations and probabilistic data structures. The soft WTA model presented in this article provides an answer for the case of Bayesian inference and learning in a simple graphical model, where a single hidden cause has to be inferred from bottom-up input. Although this is not yet a mechanism for learning to perform general Bayesian inference in arbitrary graphical models, it clearly is a first step into that direction. Importantly, the encoding of posterior distributions through spiking activity of the neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e499" xlink:type="simple"/></inline-formula> in a WTA circuit is perfectly compatible with the assumed input encoding from external variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e500" xlink:type="simple"/></inline-formula> into spiking activity in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e501" xlink:type="simple"/></inline-formula>. Thus, the interpretation of spikes from output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e502" xlink:type="simple"/></inline-formula> as samples of the posterior distributions over hidden variables in principle allows for using these spikes as input for performing further probabilistic inference.</p>
      <p>This compatibility of input and output codes means that SEM modules could potentially be hierarchically and/or recurrently coupled in order to serve as inputs of one another, although it remains to be shown how this coupling affects the dynamics of learning and inference. Future research will therefore address the important questions whether interconnected networks of modules for Bayesian computation that emerge through STDP can provide the primitive building blocks for probabilistic models of cortical computation. Previous studies <xref ref-type="bibr" rid="pcbi.1003037-Rutishauser1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pecevski1">[25]</xref> have shown that interconnected networks of WTA modules are indeed computationally very powerful. In particular, <xref ref-type="bibr" rid="pcbi.1003037-Buesing1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Pecevski1">[25]</xref> have recently shown how recurrently connected neurons can be designed to perform neural sampling, an approach in which time-independent probability distributions can be represented through spiking activity in recurrent neural networks. The question how salient random variables come to be represented by the firing activity of neurons has remained open. This paper shows that such representations may emerge autonomously through STDP.</p>
      <p>A prediction for networks of hierarchically coupled SEM modules would be that more and more abstract hidden causes can be learned in higher layers such as it has been demonstrated in machine learning approaches using Deep Belief Networks <xref ref-type="bibr" rid="pcbi.1003037-Hinton3">[66]</xref> and more recently in Deep Boltzmann Machines (DBM) <xref ref-type="bibr" rid="pcbi.1003037-Salakhutdinov1">[70]</xref>. This effect would correspond to the emergence of abstract feature selectivity in higher visual areas of primates (e.g. face-selective cells in IT, <xref ref-type="bibr" rid="pcbi.1003037-Desimone1">[71]</xref>). The hierarchical structure, however, that would result from such deeply organized SEM-modules is more reminiscent of a Deep Sum-Product Network <xref ref-type="bibr" rid="pcbi.1003037-Poon1">[72]</xref>, a recently presented new architecture, which has a much simpler learning dynamics but arguably a similar expressive power as DBM. In addition, with a consistent input encoding, associations between different sensory modalities could be formed by connecting inputs from different low-level or high-level sources to a single SEM.</p>
      <p>Importantly, while the discussion above focused only on the representation of complex stimuli by neurons encoding abstract hidden causes, SEM can also be an important mechanism for fast and reliable reinforcement learning or decision making under uncertainty. Preprocessing via single or multiple SEM circuits provides an abstraction of the state of the organism, which is much lower-dimensional than the complete stream of individual sensory signals. Learning a behavioral strategy by reading out such behaviorally relevant high-level state signals and mapping them into actions could therefore speed up learning by reducing the state space. In previous studies <xref ref-type="bibr" rid="pcbi.1003037-Pfeiffer1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Nessler2">[73]</xref> we have shown how optimal strategies can be learned very fast by simple local learning rules for reinforcement learning or categorization, if a preprocessing of input signals based on probabilistic dependencies is performed. SEM would be a suitable unsupervised mechanism for learning such preprocessing networks for decision making.</p>
      <p>We also have shown that SEM is a very powerful principle that endows networks of spiking neurons to solve complex tasks of practical relevance (see e.g. <xref ref-type="fig" rid="pcbi-1003037-g006">Fig. 6</xref>), and as we have shown, their unsupervised learning performance is within the range of conventional machine learning approaches. Furthermore, this could be demonstrated for computations on spike inputs with an input dimension of about 1000 presynaptic neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e503" xlink:type="simple"/></inline-formula>, a number that approaches the typical dimension of the spike input that a cortical neuron receives. A very satisfactory aspect is that this high computational performance can be achieved by networks of spiking neurons that learn completely autonomously by STDP, without any postulated teacher or other guidance. This could benefit the field of neuromorphic engineering <xref ref-type="bibr" rid="pcbi.1003037-Indiveri1">[74]</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Jin1">[76]</xref>, which develops dedicated massively parallel and very efficient hardware for emulating spiking neural networks and suitable plasticity rules. The link between spiking neuron models and plasticity rules and established machine learning concepts provides a novel way of installing well-understood Bayesian inference and learning mechanisms on neuromorphic hardware. First steps towards implementing SEM-like rules in different types of neuromorphic hardware have been taken.</p>
      <sec id="s3a">
         <title>Prior related work</title>
         <p>A first model for competitive Hebbian learning paradigm in non-spiking networks of neurons had been introduced in <xref ref-type="bibr" rid="pcbi.1003037-Rumelhart1">[77]</xref>. They analyzed a Hebbian learning rule in a hard WTA network and showed that there may exist equilibrium states, in which the average change of all weight values vanishes for a given set of input patterns. They showed that in these cases the weights adopt values that are proportional to the conditional probability of the presynaptic neuron being active given that the postsynaptic unit wins (rather than the log of this conditional probability, as in our framework). <xref ref-type="bibr" rid="pcbi.1003037-Nowlan1">[78]</xref> showed that the use of a soft competition instead of a hard winner assignment and corresponding average weight updates lead to an exact gradient ascent on the log-likelihood function of a generative model of a mixture of Gaussians. However, these learning rules had not yet been analyzed in the context of EM.</p>
         <p>Stochastic approximation algorithms for expectation maximization <xref ref-type="bibr" rid="pcbi.1003037-Dempster1">[31]</xref> were first considered in <xref ref-type="bibr" rid="pcbi.1003037-Celeux1">[79]</xref>, incremental and on-line EM algorithms with soft-max competition in <xref ref-type="bibr" rid="pcbi.1003037-Nowlan2">[80]</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Neal1">[82]</xref>. A proof of the stochastic approximation convergence for on-line EM in exponential family models with hidden variables was shown in <xref ref-type="bibr" rid="pcbi.1003037-Sato1">[29]</xref>. They developed a sophisticated schedule for the learning rate in this much more general model, but did not yet consider individual learning rates for different weights.</p>
         <p><xref ref-type="bibr" rid="pcbi.1003037-Song2">[54]</xref> initiated the investigation of STDP in the context of unsupervised competitive Hebbian learning and demonstrated that correlations of input spike trains can be learned in this way. They also showed that this leads to a competition between the synapses for the control of the timing of the postsynaptic action potential. A similar competition can also be observed during learning in our model, since our learning rule automatically drives the weights towards satisfying the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>.</p>
         <p><xref ref-type="bibr" rid="pcbi.1003037-Savin1">[83]</xref> present a network and learning model that is designed to perform Independent Component Analysis (ICA) with spiking neurons through STDP and intrinsic plasticity. The mixture model of independent components can also be formulated as a generative model, and the goal of ICA is to find the optimal parameters of the mixing matrix. It has been shown that also this problem can be solved by a variant of Expectation Maximization <xref ref-type="bibr" rid="pcbi.1003037-Dayan1">[84]</xref>, so there is some similarity to the identification of hidden causes in our model.</p>
         <p>Recently, computer experiments in <xref ref-type="bibr" rid="pcbi.1003037-Gupta1">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Gupta2">[86]</xref> have used STDP in the context of WTA circuits to achieve a clustering of input patterns. Their STDP rules implements linear updates, independent of the current weight values, mixed with a homeostasis rule to keep the sum of all weights constant and every weight between 0 and 1. This leads to weights that are roughly proportional to the probability of the presynaptic neuron's firing given that the post-synaptic neuron fires afterwards. The competition between the output neurons is carried out as hard-max. In <xref ref-type="bibr" rid="pcbi.1003037-Gupta1">[85]</xref> the 4 output neurons learn to differentiate the 4 presented patterns and smoothly interpolate new rotated input patterns, whereas in <xref ref-type="bibr" rid="pcbi.1003037-Gupta2">[86]</xref> 48 neurons learn to differentiate characters in a small pixel raster. <xref ref-type="bibr" rid="pcbi.1003037-Gupta2">[86]</xref> uses a STDP rule where both LTP and LTD are modeled as exponentially dependent on the time difference. However, the very specific experimental setting with synchronous regular firing of the input neurons makes it difficult to generalize their result to more general input spike trains. No theoretical analysis is provided in <xref ref-type="bibr" rid="pcbi.1003037-Gupta1">[85]</xref> or <xref ref-type="bibr" rid="pcbi.1003037-Gupta2">[86]</xref>, but their experimental results can be explained by our SEM approach. Instead of adding up logs of conditional probabilities and performing the competition on the exponential of the sums, they sum up the conditional probabilities directly and use this sum of probabilities for the competition. This can be seen as a linear approximation of SEM, especially under the additional normalization conditions that they impose by homeostasis rules.</p>
         <p>It has previously been shown that spike patterns embedded in noise can be detected by STDP <xref ref-type="bibr" rid="pcbi.1003037-Masquelier1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Gilson1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Masquelier2">[59]</xref>. Competitive pattern learning through STDP has recently been studied in <xref ref-type="bibr" rid="pcbi.1003037-Masquelier3">[68]</xref>. They simulate a deterministic version of a winner-take-all circuit consisting of a fixed number of neurons, all listening to the same spiking input lines and connected to each other with a strong inhibition. The STDP learning rule that they propose is additive and weight-independent. Just like our results, they also observe that different neurons specialize on different fixed repeated input pattern, even though the repeated patterns are embedded in spiking noise such that the mean activity of all inputs remains the same throughout the learning phase. Additionally they show that within each pattern the responsible neuron tries to detect the start of the pattern. In contrast to our approach they do not give any analysis of convergence guarantees, nor does their model try to build a generative probabilistic model of the input distribution.</p>
         <p><xref ref-type="bibr" rid="pcbi.1003037-Rao2">[87]</xref>–<xref ref-type="bibr" rid="pcbi.1003037-Zemel1">[89]</xref> investigated the possibility to carry out Bayesian probabilistic computations in recurrent networks of spiking neurons, both using probabilistic population codes. They showed that the ongoing dynamics of belief propagation in temporal Bayesian models can be represented and inferred by such networks, but they do not exhibit any neuronal plausible learning mechanism. <xref ref-type="bibr" rid="pcbi.1003037-Ma1">[7]</xref> presented another approach to Bayesian inference using probabilistic population codes, also without any learning result.</p>
         <p>An interesting complementary approach is presented in <xref ref-type="bibr" rid="pcbi.1003037-Deneve1">[90]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Deneve2">[91]</xref>, where a single neuron is modeled as hidden Markov model with two possible states. This approach has the advantage, that the instantaneous synaptic input does not immediately decide the output state, but only incrementally influences the probability for switching the state. The weights and the temporal behavior can be learned online using local statistics. The downside of this approach is that this hidden Markov model can have only two states. In contrast, the SEM approach can be applied to networks with any number of output neurons.</p>
         <p>In <xref ref-type="bibr" rid="pcbi.1003037-Gtig1">[92]</xref> it was shown that a suitable rule for supervised spike-based learning (the Tempotron learning rule) can be used to train a network to recognize spatio-temporal spike patterns. This discriminative learning scheme enables the recognizing neuron to focus on the most discriminative segment of the pattern. In contrast, our generative unsupervised learning scheme drives the recognizing neuron to generalize and spike many times during the whole pattern, and thus learns the spatial average activity pattern. The conductance based approach of <xref ref-type="bibr" rid="pcbi.1003037-Gtig1">[92]</xref> differs drastically from our method (and the results shown in the Supplement) insofar as here only STDP was used (focusing on average spatial patterns), no supervision was involved, and the time-warped input pattern had never been shown during training.</p>
         <p>An alternative approach to implement the learning of generative probabilistic models in spiking neuronal networks is given in <xref ref-type="bibr" rid="pcbi.1003037-Rezende1">[93]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Brea1">[94]</xref>. Both approaches are based on the idea to model a sequence of spikes in a Hidden-Markov-Model-like probabilistic model and learn the model parameters through different variants of EM, in which a sequence of spikes represents one single sample of the model's distribution. Due to the explicit incorporation of inference over time, these models are more powerful than ours and thus require non-trivial, non-local learning mechanisms.</p>
      </sec>
      <sec id="s3b">
         <title>Experimentally testable predictions of the proposed model</title>
         <p>Our analysis has shown, that STDP supports the creation of internal models and implements spike-based EM if changes of synaptic weights depend in a particular way on the current value of the weight: Weight potentiation depends in an inversely exponential manner on the current weight (see <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>). This rule for weight potentiation (see <xref ref-type="fig" rid="pcbi-1003037-g008">Fig. 8A</xref>) is consistent with all published data on this dependence: <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5</xref> in <xref ref-type="bibr" rid="pcbi.1003037-Bi1">[43]</xref> and <xref ref-type="fig" rid="pcbi-1003037-g005">Fig. 5C</xref> in <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref> for STDP, as well as Fig. 10 in <xref ref-type="bibr" rid="pcbi.1003037-Liao1">[95]</xref> and <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1</xref> in <xref ref-type="bibr" rid="pcbi.1003037-Montgomery1">[96]</xref> for other protocols for LTP induction. One needs to say, however, that these data exhibit a large trial-to-trial variability, so that it is hard to infer precise quantitative laws from them. On the other hand, the applications of STDP that we have examined in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>–<xref ref-type="fig" rid="pcbi-1003037-g007">7</xref> work almost equally well if the actual weight increase varies by up to 100% from the weight increase proposed by our STDP rule (see open circles in <xref ref-type="fig" rid="pcbi-1003037-g008">Fig. 8A</xref>). The resulting distribution of weight increases matches qualitatively the above mentioned experimental data quite well.</p>
         <fig id="pcbi-1003037-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g008</object-id><label>Figure 8</label>
            <caption>
               <title>Ideal dependence of weight potentiation under STDP on the initial value of the weight (solid lines).</title>
               <p>Open circles represent results of samples from this ideal curve with 100% noise, that can be used in the previously discussed computer experiments with almost no loss in performance. <bold>A</bold>: Dependence of weight potentiation on initial weight according to the STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>. <bold>B</bold>: Same with an additional factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e504" xlink:type="simple"/></inline-formula>.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g008" position="float" xlink:type="simple"/></fig>
         <p>The prediction of our model for the dependence of the amount of weight depression on the current weight is drastically different: Even though we make the strong simplification that the depression part of the STDP rule is independent of the time difference between pre- and postsynaptic spike, the formulation in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> makes the assumption, that the amount of the depression should be independent of the current weight value. It is this contrast between an exponential dependency for LTP and a constant LTD which makes the weight converge to the logarithm of the conditional presynaptic firing probability in <xref ref-type="disp-formula" rid="pcbi.1003037.e086">Eq. (6)</xref>. In experiments this dependency has been investigated in-vitro <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>. There it has been found that the <italic>percentage</italic> of weight depression under STDP is independent of the current weight, which implies that the amount of depression is linear in the current weight value. This seems to contradict the presented learning rule. However, the key property that is needed for the desired equilibrium condition is the ratio between LTP and LTD. So the equilibrium proof in <xref ref-type="disp-formula" rid="pcbi.1003037.e572">Eq. (28)</xref> remains unchanged if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e505" xlink:type="simple"/></inline-formula> is multiplied (for potentiation and depression) by some arbitrary function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e506" xlink:type="simple"/></inline-formula> of the current weight value. Choosing for example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e507" xlink:type="simple"/></inline-formula> yields a depression whose percentage is independent of the initial value, which would be consistent with the above mentioned in-vitro data <xref ref-type="bibr" rid="pcbi.1003037-Sjstrm1">[40]</xref>. The resulting dependence for potentiation is plotted in <xref ref-type="fig" rid="pcbi-1003037-g008">Fig. 8B</xref>. Since this curve is very similar to that of <xref ref-type="fig" rid="pcbi-1003037-g008">Fig. 8A</xref>, the above mentioned experimental data for potentiation are too noisy to provide a clear vote for one of these two curves. Thus more experimental data are needed for determining the dependence of weight potentiation on the initial weight. Whereas the relevance of this dependency had previously not been noted, our analysis suggests that such a contrast it is in fact essential for the capability of STDP to create internal models for high-dimensional spike inputs.</p>
         <p>Our analysis has shown, that if the excitability of neurons is also adaptive, with a rule as in <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> that is somewhat analogous to that for synaptic plasticity, then neurons can also learn appropriate priors for Bayesian computation. Several experimental studies have already confirmed, that the intrinsic excitability of neurons does in fact increase when they are more frequently activated <xref ref-type="bibr" rid="pcbi.1003037-Cudmore1">[45]</xref>, see <xref ref-type="bibr" rid="pcbi.1003037-Debanne1">[97]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Daoudal1">[14]</xref> and <xref ref-type="bibr" rid="pcbi.1003037-Caporale1">[39]</xref> for reviews. But a quantitative study, which relates the resulting change in intrinsic excitability to its initial value, is missing.</p>
         <p>Our model proposes that pyramidal neurons in cortical microcircuits are organized into stochastic WTA circuits, that together represent a probability distribution. This organization is achieved by a suitably regulated common inhibitory signal, where the inhibition follows the excitation very closely. Such instantaneous balance between excitation and inhibition was described by <xref ref-type="bibr" rid="pcbi.1003037-Okun1">[34]</xref>. A resulting prediction of the WTA structure is that the firing activity of these neurons is highly de-correlated due to the inhibitory competition. In contrast to previous experimental results, that reported higher correlations, it has recently been confirmed in <xref ref-type="bibr" rid="pcbi.1003037-Ecker1">[35]</xref> for the visual cortex of awake monkey that nearby neurons, even though they share common input show extremely low correlations.</p>
         <p>Another prediction is that neural firing activity especially for awake animals subject to natural stimuli is quite sparse, since only those neurons fire whose internal model matches their spike input. A number of experimental studies confirm this predictions (see <xref ref-type="bibr" rid="pcbi.1003037-Olshausen2">[98]</xref> for a review). Our model also predicts, that the neural firing response to stimuli exhibits a fairly high trial-to-trial variability, as is typical for drawing repeated samples from a posterior distribution (unless the posterior probability is close to 0 or 1). A fairly high trial-to-trial variability is a common feature of most recordings of neuronal responses (see e.g. <xref ref-type="bibr" rid="pcbi.1003037-Kerr1">[99]</xref>, <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref> in <xref ref-type="bibr" rid="pcbi.1003037-Nikolic1">[100]</xref>; a review is provided in <xref ref-type="bibr" rid="pcbi.1003037-Faisal1">[101]</xref>). In addition, our model predicts that this trial-to-trial variability decreases for repeatedly occurring natural stimuli (especially if this occurs during attention) and discrimination capability improves for these stimuli, since the internal models of neurons are becoming better fitted to their spike input during these repetitions (“sharpening of tuning”), yielding posterior probabilities closer to 1 or 0 for these stimuli. These predictions are consistent with a number of experimental data related to perceptual learning <xref ref-type="bibr" rid="pcbi.1003037-Gilbert1">[102]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Gilbert2">[103]</xref>, and with the evolution of neuronal responses to natural scenes that were shown repeatedly in conjunction with nucleus basalis stimulation <xref ref-type="bibr" rid="pcbi.1003037-Goard1">[104]</xref>.</p>
         <p>In addition our model predicts that if the distribution of sensory inputs changes, the organization of codes for such sensory inputs also changes. More frequently occurring sensory stimuli will be encoded with a finer resolution (see <xref ref-type="bibr" rid="pcbi.1003037-deVillersSidani1">[105]</xref> for a review of related experimental data). Furthermore in the case of sensory deprivation (see <xref ref-type="bibr" rid="pcbi.1003037-Merabet1">[106]</xref>) our model predicts that neurons that used to encode stimuli which no longer occur will start to participate in the encoding of other stimuli.</p>
         <p>We have shown in <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref> that an underlying background oscillation on neurons that provide input to a WTA circuit speeds up the learning process, and produces more precise responses after learning. This result predicts that cortical areas that collaborate on a common computational task, especially under attention, exhibit some coherence in their LFP. This has already been shown for neurons in close proximity <xref ref-type="bibr" rid="pcbi.1003037-Maldonado1">[107]</xref> but also for neurons in different cortical areas <xref ref-type="bibr" rid="pcbi.1003037-Uhlhaas1">[108]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Uhlhaas2">[109]</xref>.</p>
         <p>If one views the modules for Bayesian computation that we have analyzed in this article as building blocks for larger cortical networks, these networks exhibit a fundamental difference to networks of neurons: Whereas a neuron needs a sufficiently strong excitatory drive in order to reach its firing threshold, the output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e508" xlink:type="simple"/></inline-formula> of a stochastic WTA circuit according to our model in <xref ref-type="disp-formula" rid="pcbi.1003037.e036">Eq. (3)</xref> are firing already on their own - even without any excitatory drive from the input neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e509" xlink:type="simple"/></inline-formula> (due to assumed background synaptic inputs; modeled in our simulations by an Ornstein-Uhlenbeck process, as suggested by in-vivo data <xref ref-type="bibr" rid="pcbi.1003037-Destexhe1">[110]</xref>). Rather, the role of the input from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e510" xlink:type="simple"/></inline-formula>-neurons is to modulate which of the neurons in the WTA circuit fire. One consequence of this characteristic feature is that even relatively few presynaptic neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e511" xlink:type="simple"/></inline-formula> can have a strong impact on the firing of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e512" xlink:type="simple"/></inline-formula>-neurons, provided the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e513" xlink:type="simple"/></inline-formula>-neurons have learned (via STDP) that these <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e514" xlink:type="simple"/></inline-formula>-neurons provide salient information about the hidden cause for the total input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e515" xlink:type="simple"/></inline-formula> from all presynaptic neurons. This consequence is consistent with the surprisingly weak input from the LGN to area V1 <xref ref-type="bibr" rid="pcbi.1003037-Douglas1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Binzegger1">[111]</xref>, <xref ref-type="bibr" rid="pcbi.1003037-Markov1">[112]</xref>. It is also consistent with the recently found exponential distance rule for the connection strength between cortical areas <xref ref-type="bibr" rid="pcbi.1003037-Markov1">[112]</xref>. This rule implies that the connection strength between distal cortical areas, say between primary visual cortex and PFC, is surprisingly weak. Our model suggests that these weak connections can nevertheless support coherent brain computation and memory traces that are spread out over many, also distal, cortical areas.</p>
         <p>Apart from these predictions regarding aspects of brain computation on the microscale and macroscale, a primary prediction of our model is that complex computations in cortical networks of neurons - including very efficient and near optimal processing of uncertain information - are established and maintained through STDP, on the basis of genetically encoded stereotypical connection patterns (WTA circuits) in cortical microcircuits.</p>
      </sec>
   </sec>
   <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <p>According to our input model, every external multinomial variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e516" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e517" xlink:type="simple"/></inline-formula> is encoded through a group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e518" xlink:type="simple"/></inline-formula> of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e519" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e520" xlink:type="simple"/></inline-formula>. The generative model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e521" xlink:type="simple"/></inline-formula> from <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1B</xref> is implicitly encoded in the WTA circuit of <xref ref-type="fig" rid="pcbi-1003037-g001">Fig. 1A</xref> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e522" xlink:type="simple"/></inline-formula> excitatory neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e523" xlink:type="simple"/></inline-formula> by:<disp-formula id="pcbi.1003037.e524"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e524" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e525" xlink:type="simple"/></inline-formula> is the binary indicator function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e526" xlink:type="simple"/></inline-formula> taking on value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e527" xlink:type="simple"/></inline-formula>. In the generative model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e528" xlink:type="simple"/></inline-formula> we define the binary variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e529" xlink:type="simple"/></inline-formula> and set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e530" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e531" xlink:type="simple"/></inline-formula> represents the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e532" xlink:type="simple"/></inline-formula> of the multinomial variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e533" xlink:type="simple"/></inline-formula> (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e534" xlink:type="simple"/></inline-formula> s.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e535" xlink:type="simple"/></inline-formula>) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e536" xlink:type="simple"/></inline-formula>, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e537" xlink:type="simple"/></inline-formula>. The sets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e538" xlink:type="simple"/></inline-formula> represent a partition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e539" xlink:type="simple"/></inline-formula>, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e540" xlink:type="simple"/></inline-formula> and the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e541" xlink:type="simple"/></inline-formula> used in <xref ref-type="disp-formula" rid="pcbi.1003037.e176">Eq. (9)</xref> are equivalent expressions. The value of the normalization constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e542" xlink:type="simple"/></inline-formula> can be calculated explicitly as<disp-formula id="pcbi.1003037.e543"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e543" xlink:type="simple"/><label>(20)</label></disp-formula>This generative model can be rewritten as a mixture distribution with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e544" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e545" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e546"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e546" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula id="pcbi.1003037.e547"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e547" xlink:type="simple"/><label>(22)</label></disp-formula><disp-formula id="pcbi.1003037.e548"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e548" xlink:type="simple"/><label>(23)</label></disp-formula>In order to show how the constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e549" xlink:type="simple"/></inline-formula> cancel out we write the full joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e550" xlink:type="simple"/></inline-formula> and the “hidden cause” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e551" xlink:type="simple"/></inline-formula> as the product of the prior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e552" xlink:type="simple"/></inline-formula> and the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e553" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e554"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e554" xlink:type="simple"/><label>(24)</label></disp-formula><disp-formula id="pcbi.1003037.e555"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e555" xlink:type="simple"/><label>(25)</label></disp-formula><disp-formula id="pcbi.1003037.e556"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e556" xlink:type="simple"/><label>(26)</label></disp-formula><disp-formula id="pcbi.1003037.e557"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e557" xlink:type="simple"/><label>(27)</label></disp-formula></p>
      <p>Under the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref> the parameters of the mixture distribution simplify to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e558" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e559" xlink:type="simple"/></inline-formula>, since all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e560" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e561" xlink:type="simple"/></inline-formula>.</p>
      <p>The generative model in <xref ref-type="disp-formula" rid="pcbi.1003037.e554">Eq. (24)</xref> is well defined only for vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e562" xlink:type="simple"/></inline-formula>, such that there is exactly one “1” entry per group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e563" xlink:type="simple"/></inline-formula>. However, in the network model with rectangular, renewable EPSPs, there are time intervals where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e564" xlink:type="simple"/></inline-formula> may violate this condition, if the interval between two input spikes is longer that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e565" xlink:type="simple"/></inline-formula>. It is obvious from <xref ref-type="disp-formula" rid="pcbi.1003037.e554">Eq. (24)</xref> that this has the effect of dropping all factors representing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e566" xlink:type="simple"/></inline-formula>, since this results in an exponent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e567" xlink:type="simple"/></inline-formula>. Under proper normalization conditions (or at least if all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e568" xlink:type="simple"/></inline-formula> have identical values), this drop of an entire input group in the calculation of the posterior in <xref ref-type="disp-formula" rid="pcbi.1003037.e195">Eq. (11)</xref> is identical to performing inference with unknown <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e569" xlink:type="simple"/></inline-formula> (see ‘Impact of missing input values’). <xref ref-type="disp-formula" rid="pcbi.1003037.e195">Eq. (11)</xref> holds aslong as there are no two input spikes from different neurons within the same group closer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e570" xlink:type="simple"/></inline-formula>, which we have assumed for the simple input model with rectangular, renewable EPSPs.</p>
      <sec id="s4a">
         <title>Equilibrium condition</title>
         <p>We will now show that all equilibria of the stochastic update rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref>, i.e., all points where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e571" xlink:type="simple"/></inline-formula>, exactly match the implicit solution conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e642">Eq. (46)</xref>, and vice versa:<disp-formula id="pcbi.1003037.e572"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e572" xlink:type="simple"/><label>(28)</label></disp-formula>Analogously, one can show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e573" xlink:type="simple"/></inline-formula>. Note that this result implies that the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> has no equilibrium points outside the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>, since all equilibrium points fulfill the implicit solutions condition in <xref ref-type="disp-formula" rid="pcbi.1003037.e642">Eq. (46)</xref> and these in turn fulfill the normalization conditions.</p>
      </sec>
      <sec id="s4b">
         <title>Details to <italic>Learning the parameters of the probability model by EM</italic></title>
         <p>In this section we will analyze the theoretical basis for learning the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e574" xlink:type="simple"/></inline-formula> of the generative probability model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e575" xlink:type="simple"/></inline-formula> given in <xref ref-type="disp-formula" rid="pcbi.1003037.e176">Eq. (9)</xref> from a machine learning perspective. In contrast to the intuitive explanation of the Results section which was based on Expectation Maximization we will now derive an implicit analytical solution for a (locally) optimal weight vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e576" xlink:type="simple"/></inline-formula>, and rewrite this solution in terms of log probabilities. We will later use this derivation in order to show that the stochastic online learning rule provably converges towards this solution.</p>
         <p>For an exact definition of the learning problem, we assume that the input is given by a stream of vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e577" xlink:type="simple"/></inline-formula>, in which every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e578" xlink:type="simple"/></inline-formula> is drawn independently from the input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e579" xlink:type="simple"/></inline-formula>. In principle, this stream of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e580" xlink:type="simple"/></inline-formula>'s corresponds to the samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e581" xlink:type="simple"/></inline-formula> that are observed at the spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e582" xlink:type="simple"/></inline-formula> of the circuit. However, in order to simplify the proofs in this and subsequent sections, we will neglect any possible temporal correlation between successive samples.</p>
         <p>The learning task is to find parameter values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e583" xlink:type="simple"/></inline-formula>, such that the marginal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e584" xlink:type="simple"/></inline-formula> of the model distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e585" xlink:type="simple"/></inline-formula> approximates the actual input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e586" xlink:type="simple"/></inline-formula> as accurately as possible. This is equivalent to minimizing the Kullback-Leibler divergence between the two distributions:<disp-formula id="pcbi.1003037.e587"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e587" xlink:type="simple"/><label>(29)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e588" xlink:type="simple"/></inline-formula> is the (constant) entropy of the input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e589" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e590" xlink:type="simple"/></inline-formula> denotes the expectation over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e591" xlink:type="simple"/></inline-formula>, according to the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e592" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e593" xlink:type="simple"/></inline-formula> is constant, minimizing the right hand side of <xref ref-type="disp-formula" rid="pcbi.1003037.e587">Eq. (29)</xref> is equivalent to maximizing the expected log likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e594" xlink:type="simple"/></inline-formula>.</p>
         <p>There are many different parametrizations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e595" xlink:type="simple"/></inline-formula> that define identical generative distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e596" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e554">Eq. (24)</xref>. There is, however, exactly one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e597" xlink:type="simple"/></inline-formula> in this sub-manifold of the weight space that fulfills the normalization conditions in <xref ref-type="disp-formula" rid="pcbi.1003037.e177">Eq. (10)</xref>.</p>
         <p>We thus redefine the goal of learning more precisely as the constrained maximization problem<disp-formula id="pcbi.1003037.e598"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e598" xlink:type="simple"/><label>(30)</label></disp-formula><disp-formula id="pcbi.1003037.e599"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e599" xlink:type="simple"/><label>(31)</label></disp-formula></p>
         <p>This maximization problem never has a unique solution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e600" xlink:type="simple"/></inline-formula>, because any permutation of the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e601" xlink:type="simple"/></inline-formula> and their corresponding weights leads to different joint distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e602" xlink:type="simple"/></inline-formula>, all of them having identical marginals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e603" xlink:type="simple"/></inline-formula>. The local maxima of <xref ref-type="disp-formula" rid="pcbi.1003037.e598">Eq. (30)</xref> can be found using the Lagrange multiplier method.</p>
         <p>Note that we do at no time enforce normalization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e604" xlink:type="simple"/></inline-formula> during the learning process, nor do we require normalized initialization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e605" xlink:type="simple"/></inline-formula>. Instead, we will show that the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5</xref>,<xref ref-type="disp-formula" rid="pcbi.1003037.e095">7)</xref> automatically drives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e606" xlink:type="simple"/></inline-formula> towards a local maximum, in which the normalization conditions are fulfilled.</p>
         <p>Under the constraints in <xref ref-type="disp-formula" rid="pcbi.1003037.e599">Eq. (31)</xref> the normalization constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e607" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e546">Eq. (21)</xref> equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e608" xlink:type="simple"/></inline-formula>, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e609" xlink:type="simple"/></inline-formula> simplifies to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e610" xlink:type="simple"/></inline-formula> - with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e611" xlink:type="simple"/></inline-formula> - and we can define a Lagrangian function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e612" xlink:type="simple"/></inline-formula> for the maximization problem in Eq. (30,31) by<disp-formula id="pcbi.1003037.e613"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e613" xlink:type="simple"/><label>(32)</label></disp-formula>Setting the derivatives to zero we arrive at the following set of equations in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e614" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e615" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e616"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e616" xlink:type="simple"/><label>(33)</label></disp-formula><disp-formula id="pcbi.1003037.e617"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e617" xlink:type="simple"/><label>(34)</label></disp-formula>Summing over those equations that have the same multiplier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e618" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e619" xlink:type="simple"/></inline-formula>, resp., leads to<disp-formula id="pcbi.1003037.e620"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e620" xlink:type="simple"/><label>(35)</label></disp-formula><disp-formula id="pcbi.1003037.e621"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e621" xlink:type="simple"/><label>(36)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e622" xlink:type="simple"/></inline-formula> is the shorthand notation for the equivalent expression <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e623" xlink:type="simple"/></inline-formula>. The identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e624" xlink:type="simple"/></inline-formula>, the identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e625" xlink:type="simple"/></inline-formula> the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e626" xlink:type="simple"/></inline-formula>, which follows from the definition of population encoding, and the constraints in <xref ref-type="disp-formula" rid="pcbi.1003037.e599">Eq. (31)</xref> are used in order to derive the explicit solution for the Lagrange multipliers<disp-formula id="pcbi.1003037.e627"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e627" xlink:type="simple"/><label>(37)</label></disp-formula>in dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e628" xlink:type="simple"/></inline-formula>. We insert this solution for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e629" xlink:type="simple"/></inline-formula> into the gradient Eq. (33,34) and get<disp-formula id="pcbi.1003037.e630"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e630" xlink:type="simple"/><label>(38)</label></disp-formula><disp-formula id="pcbi.1003037.e631"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e631" xlink:type="simple"/></disp-formula>from which we derive an implicit solution for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e632" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e633"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e633" xlink:type="simple"/><label>(39)</label></disp-formula><disp-formula id="pcbi.1003037.e634"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e634" xlink:type="simple"/></disp-formula>It is easily verified that all fixed points of this implicit solution satisfy the normalization constraints:<disp-formula id="pcbi.1003037.e635"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e635" xlink:type="simple"/><label>(40)</label></disp-formula><disp-formula id="pcbi.1003037.e636"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e636" xlink:type="simple"/><label>(41)</label></disp-formula></p>
         <p>Finally, in order to simplify the notation we use the augmented input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e637" xlink:type="simple"/></inline-formula>. The expectations in <xref ref-type="disp-formula" rid="pcbi.1003037.e633">Eq. (39)</xref> nicely evaluate to<disp-formula id="pcbi.1003037.e638"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e638" xlink:type="simple"/><label>(42)</label></disp-formula><disp-formula id="pcbi.1003037.e639"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e639" xlink:type="simple"/><label>(43)</label></disp-formula><disp-formula id="pcbi.1003037.e640"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e640" xlink:type="simple"/><label>(44)</label></disp-formula><disp-formula id="pcbi.1003037.e641"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e641" xlink:type="simple"/><label>(45)</label></disp-formula>which allows us to rewrite the implicit solution in a very intuitive form as:<disp-formula id="pcbi.1003037.e642"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e642" xlink:type="simple"/><label>(46)</label></disp-formula>Any weight vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e643" xlink:type="simple"/></inline-formula> that fulfills <xref ref-type="disp-formula" rid="pcbi.1003037.e642">Eq. (46)</xref> is either a (local) maximum, a saddle point or a (local) minimum of the log likelihood function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e644" xlink:type="simple"/></inline-formula> under the normalization constraints.</p>
         <p>An obvious numerical approach to solve this fixed point equation is the repeated application of <xref ref-type="disp-formula" rid="pcbi.1003037.e633">Eq. (39)</xref>. According to the derivations in the Results section this corresponds exactly to the Expectation Maximization algorithm. But every single iteration asks for the evaluation of expectations with respect to the input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e645" xlink:type="simple"/></inline-formula>, which theoretically requires infinite time in an online learning setup.</p>
      </sec>
      <sec id="s4c">
         <title>Details to <italic>Spike-based Expectation Maximization</italic></title>
         <p>We derive the update rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> from the statistical perspective that each weight can be interpreted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e646" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e647" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e648" xlink:type="simple"/></inline-formula> correspond to counters of the events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e649" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e650" xlink:type="simple"/></inline-formula>. Every new event <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e651" xlink:type="simple"/></inline-formula> leads to a weight update<disp-formula id="pcbi.1003037.e652"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e652" xlink:type="simple"/><label>(47)</label></disp-formula><disp-formula id="pcbi.1003037.e653"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e653" xlink:type="simple"/><label>(48)</label></disp-formula><disp-formula id="pcbi.1003037.e654"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e654" xlink:type="simple"/><label>(49)</label></disp-formula><disp-formula id="pcbi.1003037.e655"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e655" xlink:type="simple"/><label>(50)</label></disp-formula>where the log-function is linearly approximated around 1 as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e656" xlink:type="simple"/></inline-formula>. The factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e657" xlink:type="simple"/></inline-formula> is understood as learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e658" xlink:type="simple"/></inline-formula> in the additive update rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e659" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e660" xlink:type="simple"/></inline-formula>, i.e. if there is no postsynaptic spike, the update <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e661" xlink:type="simple"/></inline-formula>. In the case of a postsynaptic spike, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e662" xlink:type="simple"/></inline-formula>, the update <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e663" xlink:type="simple"/></inline-formula> decomposes in the two cases <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e664" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e665" xlink:type="simple"/></inline-formula> as it is stated explicit in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>.</p>
         <p>As a side note, we observe that by viewing our STDP rule as an approximation to counting statistics, the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e666" xlink:type="simple"/></inline-formula> can be understood as the inverse of the equivalent sample size from which the statistics was gathered. If the above rule is used with a small constant learning rate we will get a close approximation to an exponentially decaying average. If the learning rate decays like <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e667" xlink:type="simple"/></inline-formula> we will get an approximation to an online updated average, where all samples are equally weighted. We will come back to a regulation mechanism for the learning rate in the section ‘Variance Tracking’.</p>
      </sec>
      <sec id="s4d">
         <title>Details to <italic>Proof of convergence</italic></title>
         <p>In this section we give the proof of Theorem 1. Formally, we define the sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e668" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e669" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e670" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e671" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e672" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e673" xlink:type="simple"/></inline-formula>: For all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e674" xlink:type="simple"/></inline-formula> we assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e675" xlink:type="simple"/></inline-formula> is drawn independently from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e676" xlink:type="simple"/></inline-formula>. The value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e677" xlink:type="simple"/></inline-formula> is drawn from the posterior distribution of the model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e678" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003037.e195">Eq. (11)</xref>), given the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e679" xlink:type="simple"/></inline-formula> and the current model parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e680" xlink:type="simple"/></inline-formula>. The weight updates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e681" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e682" xlink:type="simple"/></inline-formula>, are calculated according to <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e095">(7)</xref> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e683" xlink:type="simple"/></inline-formula>. The sequence of weight vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e684" xlink:type="simple"/></inline-formula> is determined by the randomly initialized vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e685" xlink:type="simple"/></inline-formula>, and by the iteration equation<disp-formula id="pcbi.1003037.e686"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e686" xlink:type="simple"/><label>(51)</label></disp-formula>The projection function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e687" xlink:type="simple"/></inline-formula> represents a coordinate-wise clipping of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e688" xlink:type="simple"/></inline-formula> to a hyper-rectangle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e689" xlink:type="simple"/></inline-formula> such that <disp-formula id="pcbi.1003037.e690"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e690" xlink:type="simple"/><label>(52)</label></disp-formula>The bound <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e691" xlink:type="simple"/></inline-formula> is assumed to be chosen so that all (finite) maxima of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e692" xlink:type="simple"/></inline-formula> are inside of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e693" xlink:type="simple"/></inline-formula>. For the sequence of learning rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e694" xlink:type="simple"/></inline-formula> we assume that<disp-formula id="pcbi.1003037.e695"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e695" xlink:type="simple"/><label>(53)</label></disp-formula></p>
         <p>Under these assumptions we can now restate the theorem formally:</p>
         <p>Theorem 1: <italic>The sequence </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e696" xlink:type="simple"/></inline-formula><italic> converges with probability 1 to the set </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e697" xlink:type="simple"/></inline-formula><italic> of all points within the hyper-rectangle </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e698" xlink:type="simple"/></inline-formula><italic> that fulfill the equilibrium conditions in </italic><xref ref-type="disp-formula" rid="pcbi.1003037.e086"><italic>Eq. (6)</italic></xref><italic>. The stable convergence points among </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e699" xlink:type="simple"/></inline-formula><italic> are the (local) maxima of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e700" xlink:type="simple"/></inline-formula><italic>, subject to the normalization constraints in </italic><xref ref-type="disp-formula" rid="pcbi.1003037.e177"><italic>Eq. (10)</italic></xref>.</p>
         <p>The iterative application of the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e076">(7)</xref> is indeed a stochastic approximation algorithm for learning a (locally) optimal parameter vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e701" xlink:type="simple"/></inline-formula>. We resort to the theory of stochastic approximation algorithms as presented in <xref ref-type="bibr" rid="pcbi.1003037-Kushner1">[52]</xref> and use the method of the “mean limit” ordinary differential equation (ODE). The goal is to show that the sequence of the weight vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e702" xlink:type="simple"/></inline-formula> under the stochastic learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e076">(7)</xref> converges to one of the local maxima of <xref ref-type="disp-formula" rid="pcbi.1003037.e598">Eq. (30)</xref> with probability one, i.e., the probability to observe a non-converging realization of this sequence is zero. The location of the local maximum to which a single sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e703" xlink:type="simple"/></inline-formula> converges depends on the starting point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e704" xlink:type="simple"/></inline-formula> as well as on the concrete realization of the stochastic noise sequence. We will not discuss the effect of this stochasticity in more detail, except for stating that a stochastic approximation algorithm is usually less prone to get stuck in small local maxima than its deterministic version. The stochastic noise introduces perturbations that decrease slowly over time, which has an effect that is comparable to simulated annealing.</p>
         <p>We will use the basic convergence theorem of <xref ref-type="bibr" rid="pcbi.1003037-Kushner1">[52]</xref> to establish the convergence of the sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e705" xlink:type="simple"/></inline-formula> to the limit set of the mean limit ODE. Then it remains to show that this limit set is identical to the desired set of all equilibrium points and thus, particularly, does not contain limit cycles.</p>
         <p><bold>Proof:</bold> In the notation of <xref ref-type="bibr" rid="pcbi.1003037-Kushner1">[52]</xref>, the mean update of the stochastic algorithm in <xref ref-type="disp-formula" rid="pcbi.1003037.e686">Eq. (51)</xref> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e706" xlink:type="simple"/></inline-formula>. The bounds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e707" xlink:type="simple"/></inline-formula> imply that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e708" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e709" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e710" xlink:type="simple"/></inline-formula>.</p>
         <p>For any set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e711" xlink:type="simple"/></inline-formula> we define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e712" xlink:type="simple"/></inline-formula> as the positive limit set of the mean limit ODE <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e713" xlink:type="simple"/></inline-formula> for all initial conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e714" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e715"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e715" xlink:type="simple"/><label>(54)</label></disp-formula></p>
         <p>According to Theorem 3.1 in Chapter 5 of <xref ref-type="bibr" rid="pcbi.1003037-Kushner1">[52]</xref>, the sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e716" xlink:type="simple"/></inline-formula> under the algorithm in <xref ref-type="disp-formula" rid="pcbi.1003037.e686">Eq. (51)</xref> converges for all start conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e717" xlink:type="simple"/></inline-formula> to the limit set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e718" xlink:type="simple"/></inline-formula> with probability one in the sense that<disp-formula id="pcbi.1003037.e719"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e719" xlink:type="simple"/><label>(55)</label></disp-formula></p>
         <p>We will now show that the limit set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e720" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e721" xlink:type="simple"/></inline-formula> is identical to the set of stationary points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e722" xlink:type="simple"/></inline-formula> and does not contain limit cycles. It is obvious that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e723" xlink:type="simple"/></inline-formula> is a subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e724" xlink:type="simple"/></inline-formula> since for all initial conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e725" xlink:type="simple"/></inline-formula> the trajectory of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e726" xlink:type="simple"/></inline-formula> fulfills <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e727" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e728" xlink:type="simple"/></inline-formula>. Thus it remains to be shown that there are no other points in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e729" xlink:type="simple"/></inline-formula> (like e.g. limit cycles).</p>
         <p>We split the argument into two parts. In the first part we will show that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e730" xlink:type="simple"/></inline-formula> all trajectories of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e731" xlink:type="simple"/></inline-formula> converge asymptotically to the manifold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e732" xlink:type="simple"/></inline-formula> defined by the normalization constraints 31. This leads to the conclusion that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e733" xlink:type="simple"/></inline-formula>. In the second part we will show that all trajectories within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e734" xlink:type="simple"/></inline-formula> converge to the stationary points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e735" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e736" xlink:type="simple"/></inline-formula>. Both parts together yield the desired result that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e737" xlink:type="simple"/></inline-formula> are the only limit points of the ODE <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e738" xlink:type="simple"/></inline-formula>.</p>
         <p>The first part we start by defining the set of functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e739" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e740" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e741" xlink:type="simple"/></inline-formula> to represent the deviation of the current <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e742" xlink:type="simple"/></inline-formula> from each of the normalization constraints 31, i.e.,<disp-formula id="pcbi.1003037.e743"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e743" xlink:type="simple"/><label>(56)</label></disp-formula>The manifold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e744" xlink:type="simple"/></inline-formula> is the set of all points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e745" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e746" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e747" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e748" xlink:type="simple"/></inline-formula>. Furthermore, we calculate the gradient vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e749" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e750" xlink:type="simple"/></inline-formula> for each of these functions with respect to the argument <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e751" xlink:type="simple"/></inline-formula>. Note that many entries of these gradient vectors are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e752" xlink:type="simple"/></inline-formula>, since every single function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e753" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e754" xlink:type="simple"/></inline-formula> only depends on a few entries of its argument <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e755" xlink:type="simple"/></inline-formula>. The nonzero entries of these gradients are<disp-formula id="pcbi.1003037.e756"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e756" xlink:type="simple"/><label>(57)</label></disp-formula>We can now show that the trajectory of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e757" xlink:type="simple"/></inline-formula> in any point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e758" xlink:type="simple"/></inline-formula> always points in direction of decreasing absolute values for all deviations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e759" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e760" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e761"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e761" xlink:type="simple"/><label>(58)</label></disp-formula><disp-formula id="pcbi.1003037.e762"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e762" xlink:type="simple"/><label>(59)</label></disp-formula><disp-formula id="pcbi.1003037.e763"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e763" xlink:type="simple"/><label>(60)</label></disp-formula><disp-formula id="pcbi.1003037.e764"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e764" xlink:type="simple"/><label>(61)</label></disp-formula>This shows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e765" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e766" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e767" xlink:type="simple"/></inline-formula>. This implies that the limit set of all trajectories with initial conditions outside <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e768" xlink:type="simple"/></inline-formula> is contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e769" xlink:type="simple"/></inline-formula>, or more formally <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e770" xlink:type="simple"/></inline-formula>. Note that the continuity and the boundedness of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e771" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e772" xlink:type="simple"/></inline-formula> implies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e773" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e774" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e775" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e776" xlink:type="simple"/></inline-formula>. Therefore we can now conclude as the result of the first part<disp-formula id="pcbi.1003037.e777"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e777" xlink:type="simple"/><label>(62)</label></disp-formula>i.e. the limit set of all trajectories starting outside the manifold of normalized weights is contained in the limit set of all trajectories starting within the normalization constraints. The <xref ref-type="disp-formula" rid="pcbi.1003037.e764">equations (61)</xref> also prove that any trajectory with initial condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e778" xlink:type="simple"/></inline-formula> stays within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e779" xlink:type="simple"/></inline-formula>, since all components of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e780" xlink:type="simple"/></inline-formula> with directions orthogonal to the tangent space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e781" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e782" xlink:type="simple"/></inline-formula> are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e783" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e784" xlink:type="simple"/></inline-formula>, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e785" xlink:type="simple"/></inline-formula> is in the tangent space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e786" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e787" xlink:type="simple"/></inline-formula>.</p>
         <p>This immediately leads to the second part of the proof, which is based on the gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e788" xlink:type="simple"/></inline-formula> of the Lagrangian <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e789" xlink:type="simple"/></inline-formula> as given in <xref ref-type="disp-formula" rid="pcbi.1003037.e616">Eq. (33</xref>, <xref ref-type="disp-formula" rid="pcbi.1003037.e617">34</xref>). For any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e790" xlink:type="simple"/></inline-formula> let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e791" xlink:type="simple"/></inline-formula> be the linear projection matrix that orthogonally projects any vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e792" xlink:type="simple"/></inline-formula> into the tangent space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e793" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e794" xlink:type="simple"/></inline-formula>. The projection <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e795" xlink:type="simple"/></inline-formula> of the gradient of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e796" xlink:type="simple"/></inline-formula> at any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e797" xlink:type="simple"/></inline-formula> points towards the strongest increase of the value of the objective function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e798" xlink:type="simple"/></inline-formula> under the constraints of the normalization conditions. Thus, the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e799" xlink:type="simple"/></inline-formula> increases in the direction of any vector within the tangent space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e800" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e801" xlink:type="simple"/></inline-formula> that has a positive scalar product with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e802" xlink:type="simple"/></inline-formula>. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e803" xlink:type="simple"/></inline-formula> is a tangent vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e804" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e805" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e806" xlink:type="simple"/></inline-formula>, the orthogonal component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e807" xlink:type="simple"/></inline-formula> of the gradient is orthogonal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e808" xlink:type="simple"/></inline-formula>. Thus, the value of the scalar product with the projected gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e809" xlink:type="simple"/></inline-formula> is identical to the value of the scalar product with the gradient itself <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e810" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e811"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e811" xlink:type="simple"/><label>(63)</label></disp-formula><disp-formula id="pcbi.1003037.e812"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e812" xlink:type="simple"/></disp-formula>with equality if and only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e813" xlink:type="simple"/></inline-formula>, which is equivalent to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e814" xlink:type="simple"/></inline-formula>. This shows that all trajectories with initial condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e815" xlink:type="simple"/></inline-formula> stay within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e816" xlink:type="simple"/></inline-formula> forever and converge to the set of stationary points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e817" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e818" xlink:type="simple"/></inline-formula>. Combining the results of both parts as<disp-formula id="pcbi.1003037.e819"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e819" xlink:type="simple"/><label>(64)</label></disp-formula>establishes the stochastic convergences of any sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e820" xlink:type="simple"/></inline-formula> to the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e821" xlink:type="simple"/></inline-formula> with probability one.</p>
         <sec id="s4d1">
            <title>Weight offsets and positive weights</title>
            <p>All weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e822" xlink:type="simple"/></inline-formula> in the theoretical model are logs of probabilities and therefore always have negative values. Through a simple transformation we can shift all weights into the positive range in order to be able to use positive weights only, which is the common assumption for excitatory connections in biologically inspired neural network models. We will now show that setting the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e823" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> different from 1 leads to a linear shift of the resulting weight values by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e824" xlink:type="simple"/></inline-formula>, without changing the functionality of the Spike-based EM algorithm.</p>
            <p>Firstly, we observe that the application of the update rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e825" xlink:type="simple"/></inline-formula> on a shifted weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e826" xlink:type="simple"/></inline-formula> is identical to the application of the update rule with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e827" xlink:type="simple"/></inline-formula> on the original weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e828" xlink:type="simple"/></inline-formula>, since<disp-formula id="pcbi.1003037.e829"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e829" xlink:type="simple"/><label>(65)</label></disp-formula>Secondly, we see that the relative firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e830" xlink:type="simple"/></inline-formula> of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e831" xlink:type="simple"/></inline-formula> remains unchanged if all weights are subject to the same offset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e832" xlink:type="simple"/></inline-formula>, since<disp-formula id="pcbi.1003037.e833"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e833" xlink:type="simple"/><label>(66)</label></disp-formula><disp-formula id="pcbi.1003037.e834"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e834" xlink:type="simple"/><label>(67)</label></disp-formula><disp-formula id="pcbi.1003037.e835"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e835" xlink:type="simple"/><label>(68)</label></disp-formula>In contrast, the overall firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e836" xlink:type="simple"/></inline-formula> increases by the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e837" xlink:type="simple"/></inline-formula>. By our definition of the population coding for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e838" xlink:type="simple"/></inline-formula>, this factor equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e839" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e840" xlink:type="simple"/></inline-formula> is the number of original input variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e841" xlink:type="simple"/></inline-formula>. An increase of the inhibitory signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e842" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e843" xlink:type="simple"/></inline-formula> can therefore compensate the increase of overall firing rate. Using this shifted representation, a single excitatory synapse can take on values in the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e844" xlink:type="simple"/></inline-formula>, corresponding to probabilities in the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e845" xlink:type="simple"/></inline-formula>.</p>
            <p>Similarly the consideration holds valid that it is mathematically equivalent whether the depression of the excitability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e846" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e095">Eq. (7)</xref> is modeled either as an effect of lateral spiking activity or as a constant decay, independent of the circuit activity. In the first case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e847" xlink:type="simple"/></inline-formula> converges to the relative spiking probability of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e848" xlink:type="simple"/></inline-formula> neuron such that the sum of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e849" xlink:type="simple"/></inline-formula> is indeed 1 as described by our theory. In the second case, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e850" xlink:type="simple"/></inline-formula> really describe absolute firing rates in some time scale defined by the decay constant. In the logarithmic scale of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e851" xlink:type="simple"/></inline-formula> this is nothing else than a constant offset and thus cancels down in <xref ref-type="disp-formula" rid="pcbi.1003037.e835">Eq. (68)</xref>.</p>
         </sec>
         <sec id="s4d2">
            <title>Impact of missing input values</title>
            <p>The proof of theorem 1 assumes that every sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e852" xlink:type="simple"/></inline-formula> gathered online is a binary vector which contains exactly one entry with value 1 in every group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e853" xlink:type="simple"/></inline-formula>. This value indicates the value of the abstract variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e854" xlink:type="simple"/></inline-formula> that is encoded by this group. As long as the spikes from the input neurons are closely enough in time, this condition will be fulfilled for every activation vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e855" xlink:type="simple"/></inline-formula>. For the cases in which the value of the abstract variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e856" xlink:type="simple"/></inline-formula> changes, the first spike from group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e857" xlink:type="simple"/></inline-formula> has to appear exactly at that point in time at which the rectangular EPSP for the previous value vanishes, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e858" xlink:type="simple"/></inline-formula> ms after the last preceding spike.</p>
            <p>We will now break up this strong restriction of the provable theory and analyze the results that are to be expected, if we allow for interspike intervals longer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e859" xlink:type="simple"/></inline-formula>. We interpret the resulting “gaps” in the information about the value of an input group as missing value in the sense of Bayesian inference.</p>
            <p>We had already addressed the issue of such missing values, resulting from presynaptic neurons that do not spike within the integration time window of an output neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e860" xlink:type="simple"/></inline-formula>, in the discussion of <xref ref-type="fig" rid="pcbi-1003037-g003">Fig. 3</xref>.</p>
            <p>A profound analysis of the correct handling of missing data in EM can be found in <xref ref-type="bibr" rid="pcbi.1003037-Ghahramani1">[48]</xref>. Their analysis implies that the correct learning action would be to leave all weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e861" xlink:type="simple"/></inline-formula> in the group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e862" xlink:type="simple"/></inline-formula> unchanged, if the value of the external variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e863" xlink:type="simple"/></inline-formula> is missing, i.e., if all corresponding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e864" xlink:type="simple"/></inline-formula>'s are 0. However, in this case the STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> reduces these weights by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e865" xlink:type="simple"/></inline-formula>. This leads to a modification of the analysis of the equilibrium condition (28):<disp-formula id="pcbi.1003037.e866"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e866" xlink:type="simple"/><label>(69)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e867" xlink:type="simple"/></inline-formula> is the probability that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e868" xlink:type="simple"/></inline-formula> belongs to a group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e869" xlink:type="simple"/></inline-formula> in which the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e870" xlink:type="simple"/></inline-formula> is unknown. We assume that the probability for such a missing value event is independent of the (true) value of the abstract variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e871" xlink:type="simple"/></inline-formula> and we assume further that the probability of such missing value events is the same for all groups <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e872" xlink:type="simple"/></inline-formula> and thus conclude that this offset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e873" xlink:type="simple"/></inline-formula> is expected to be the same for all weights. It can easily be verified, that such an offset does not change the resulting probabilities of the competition in the inference according to <xref ref-type="disp-formula" rid="pcbi.1003037.e835">Eq. (68)</xref>.</p>
         </sec>
         <sec id="s4d3">
            <title>Adaptive learning rates with Variance Tracking</title>
            <p>In our experiments we used an adaptation of the variance tracking heuristic from <xref ref-type="bibr" rid="pcbi.1003037-Nessler2">[73]</xref> for an adaptive control of learning rates. If we assume that the consecutive values of the weights represent independent samples of their true stochastic distribution at the current learning rate, then this observed distribution is the log of a beta-distribution defined by the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e874" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e875" xlink:type="simple"/></inline-formula> that were used in <xref ref-type="disp-formula" rid="pcbi.1003037.e655">Eq. (50)</xref> to define the update of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e876" xlink:type="simple"/></inline-formula> from sufficient statistics. Analytically (see supplement) this distribution has the first and second moments<disp-formula id="pcbi.1003037.e877"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e877" xlink:type="simple"/><label>(70)</label></disp-formula>From the first equation we estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e878" xlink:type="simple"/></inline-formula>. This leads to a heuristic estimate for the (inverse of the) current sample size based on the empirically observed variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e879" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e880"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e880" xlink:type="simple"/><label>(71)</label></disp-formula>The empirical estimates of these first two moments can be gathered online by exponentially decaying averages using the same learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e881" xlink:type="simple"/></inline-formula>. Even though the assumption of independent samples for the estimates of the moments is not met, one can argue about two cases: In case of a stationary evolution of the weight, the strong dependence of consecutive samples typically leads to an underestimation of the variance. This in turn leads to a decrease of the learning rate which is the desired effect of a stationary evolution. In case of a directed evolution of the weight the variance will at least indicate the amount of the current gradient of the evolution despite the strong dependence and thus keep the learning rate high enough to support fast convergence towards the asymptote of the gradient.</p>
            <p>An adaptive learning rate such as in <xref ref-type="disp-formula" rid="pcbi.1003037.e880">Eq. (71)</xref> facilitates a spontaneous reorganization of the internal models encoded by the weight vectors of the output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e882" xlink:type="simple"/></inline-formula> in case that the input distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e883" xlink:type="simple"/></inline-formula> changes (see Fig. S1 in <xref ref-type="supplementary-material" rid="pcbi.1003037.s001">Text S1</xref>).</p>
         </sec>
      </sec>
      <sec id="s4e">
         <title>Details to <italic>Role of the Inhibition</italic></title>
         <sec id="s4e1">
            <title>Biased sampling problem</title>
            <p>In this section we analyze the influence of the instantaneous output firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e884" xlink:type="simple"/></inline-formula> of the learning circuit and derive the analytical result that the output rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e885" xlink:type="simple"/></inline-formula> plays the role of a multiplicative weighting of samples during learning. We show how a theoretically optimal inhibition signal can compensate this effect and describe how this compensation is approximated in our experiments.</p>
            <p>We start with the assumption that the input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e886" xlink:type="simple"/></inline-formula> can be described by some stationary stochastic process. An empirical estimate of its stationary distribution can be obtained by measuring the relative duration of presentation of every different discrete value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e887" xlink:type="simple"/></inline-formula> in a time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e888" xlink:type="simple"/></inline-formula>. The accuracy of this empirical estimate of the input distribution can be increased by using a longer time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e889" xlink:type="simple"/></inline-formula>, such that in the limit of an infinitely large time window the estimate will converge to the true stationary input distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e890" xlink:type="simple"/></inline-formula>, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e891" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e892"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e892" xlink:type="simple"/><label>(72)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e893" xlink:type="simple"/></inline-formula> is a vectorized version of the Kronecker Delta with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e894" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e895" xlink:type="simple"/></inline-formula>, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e896" xlink:type="simple"/></inline-formula>.</p>
            <p>However, even though the WTA-circuit receives this time-continuous input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e897" xlink:type="simple"/></inline-formula>, the spike-triggered STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e076">(7)</xref> updates the model parameters - i.e. the synaptic weights - only at those time points where one of the output neurons spikes. We denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e898" xlink:type="simple"/></inline-formula> the (empirical) distribution that is obtained from the observations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e899" xlink:type="simple"/></inline-formula> at the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e900" xlink:type="simple"/></inline-formula> spike events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e901" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003037.e902"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e902" xlink:type="simple"/><label>(73)</label></disp-formula>The distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e903" xlink:type="simple"/></inline-formula> that is seen by the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> depends not only on the time-continuous input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e904" xlink:type="simple"/></inline-formula>, but also on the concrete spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e905" xlink:type="simple"/></inline-formula> of the circuit. The output spikes thus serve as trigger events at which the continuous input signal is <italic>sampled</italic>.</p>
            <p>The spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e906" xlink:type="simple"/></inline-formula> and the total number of spikes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e907" xlink:type="simple"/></inline-formula> of the whole circuit within a time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e908" xlink:type="simple"/></inline-formula> are distributed according to an inhomogeneous Poisson process with the instantaneous rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e909" xlink:type="simple"/></inline-formula>. For any stochastic realization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e910" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e911" xlink:type="simple"/></inline-formula> in the time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e912" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e913" xlink:type="simple"/></inline-formula>, we can derive the expectation of the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e914" xlink:type="simple"/></inline-formula> by taking the limit for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e915" xlink:type="simple"/></inline-formula> and call this the expected empirical distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e916" xlink:type="simple"/></inline-formula>. Thus<disp-formula id="pcbi.1003037.e917"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e917" xlink:type="simple"/><label>(74)</label></disp-formula><disp-formula id="pcbi.1003037.e918"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e918" xlink:type="simple"/><label>(75)</label></disp-formula>where we divided the expectation into two parts. Firstly we take the expectation over the total number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e919" xlink:type="simple"/></inline-formula> of spikes, secondly we take the expectation over the spike times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e920" xlink:type="simple"/></inline-formula>, given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e921" xlink:type="simple"/></inline-formula>. We now make use of the fact that for any inhomogeneous Poisson process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e922" xlink:type="simple"/></inline-formula>, conditioned on the total number of events <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e923" xlink:type="simple"/></inline-formula> within a certain time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e924" xlink:type="simple"/></inline-formula>, the event times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e925" xlink:type="simple"/></inline-formula> are distributed as order statistics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e926" xlink:type="simple"/></inline-formula> unordered independent samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e927" xlink:type="simple"/></inline-formula> from the probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e928" xlink:type="simple"/></inline-formula>. The expectation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e929" xlink:type="simple"/></inline-formula> over an arbitrary function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e930" xlink:type="simple"/></inline-formula> is the integral <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e931" xlink:type="simple"/></inline-formula>, independent of the event number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e932" xlink:type="simple"/></inline-formula>, thus<disp-formula id="pcbi.1003037.e933"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e933" xlink:type="simple"/><label>(76)</label></disp-formula><disp-formula id="pcbi.1003037.e934"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e934" xlink:type="simple"/><label>(77)</label></disp-formula><disp-formula id="pcbi.1003037.e935"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e935" xlink:type="simple"/><label>(78)</label></disp-formula>Since the remaining term within the expectation operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e936" xlink:type="simple"/></inline-formula> is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e937" xlink:type="simple"/></inline-formula> we obtain the final result<disp-formula id="pcbi.1003037.e938"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e938" xlink:type="simple"/><label>(79)</label></disp-formula>This shows that the output rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e939" xlink:type="simple"/></inline-formula> acts as a multiplicative weighting of the contribution of the current input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e940" xlink:type="simple"/></inline-formula> to the expected empirical distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e941" xlink:type="simple"/></inline-formula>, which is learned in the limit of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e942" xlink:type="simple"/></inline-formula> by the simple STDP rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e076">(7)</xref>.</p>
            <p>It turns out that the condition of a constant rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e943" xlink:type="simple"/></inline-formula> is by far stronger than necessary. In fact, it is easy to see from a comparison of <xref ref-type="disp-formula" rid="pcbi.1003037.e892">Eq. (72)</xref> and <xref ref-type="disp-formula" rid="pcbi.1003037.e938">Eq. (79)</xref>, that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e944" xlink:type="simple"/></inline-formula> for all values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e945" xlink:type="simple"/></inline-formula> if and only if the relative weight for the input value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e946" xlink:type="simple"/></inline-formula>, which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e947" xlink:type="simple"/></inline-formula>, is independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e948" xlink:type="simple"/></inline-formula> in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e949" xlink:type="simple"/></inline-formula>. This is certainly true if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e950" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e951" xlink:type="simple"/></inline-formula> are stochastically independent, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e952" xlink:type="simple"/></inline-formula> is not correlated to the occurrence of any specific value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e953" xlink:type="simple"/></inline-formula>.</p>
         </sec>
         <sec id="s4e2">
            <title>Inhibition model in computer simulations</title>
            <p>In our computer simulation the inhibition is implemented by adding a strongly negative impulse to the membrane potential of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e954" xlink:type="simple"/></inline-formula>-neurons whenever one of them fires, which decays with a time constant of 5 ms back to its resting value. In addition, a noise term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e955" xlink:type="simple"/></inline-formula> is added to the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e956" xlink:type="simple"/></inline-formula> that models background synaptic inputs through an Ornstein-Uhlenbeck (OU) process (as proposed in <xref ref-type="bibr" rid="pcbi.1003037-Destexhe1">[110]</xref> for modeling in-vivo conditions) and causes stochastic firing. For each experiment, all parameters for the inhibition model are listed in “Simulation Parameters” in the Supplementary Material.</p>
         </sec>
      </sec>
      <sec id="s4f">
         <title>Details to <italic>Continuous-Time Interpretation with Realistically Shaped EPSPs</italic></title>
         <p>Let the external input vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e957" xlink:type="simple"/></inline-formula> consist of multiple discrete-valued functions in time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e958" xlink:type="simple"/></inline-formula>, and let us assume that for every input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e959" xlink:type="simple"/></inline-formula> there exists an independent Poisson sampling process with rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e960" xlink:type="simple"/></inline-formula> which generates spike times for the group of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e961" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e962" xlink:type="simple"/></inline-formula>. At every spike time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e963" xlink:type="simple"/></inline-formula> there is exactly one neuron in the group that fires a spike, and this is the neuron that is associated with the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e964" xlink:type="simple"/></inline-formula>. First, we analyze additive step-function EPSPs, i.e. the postsynaptic activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e965" xlink:type="simple"/></inline-formula> is given by the convolution in <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e966" xlink:type="simple"/></inline-formula> is a step-function kernel with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e967" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e968" xlink:type="simple"/></inline-formula> for a fixed EPSP-duration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e969" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e970" xlink:type="simple"/></inline-formula> otherwise. In order to understand the resulting distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e971" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> as Bayesian inference we extend our underlying generative probabilistic model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e972" xlink:type="simple"/></inline-formula> such that it contains multiple instances of the variable vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e973" xlink:type="simple"/></inline-formula>, called <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e974" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e975" xlink:type="simple"/></inline-formula> is the total number of spikes from all input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e976" xlink:type="simple"/></inline-formula> within the time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e977" xlink:type="simple"/></inline-formula>. We can see every spike as a single event in continuous time. The full probabilistic model is defined as<disp-formula id="pcbi.1003037.e978"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e978" xlink:type="simple"/><label>(80)</label></disp-formula>which defines that the multiple instances are modeled as being conditionally independent of each other, given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e979" xlink:type="simple"/></inline-formula>. Let the vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e980" xlink:type="simple"/></inline-formula> describe the corresponding spike “patterns” in which every binary vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e981" xlink:type="simple"/></inline-formula> has exactly one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e982" xlink:type="simple"/></inline-formula> entry <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e983" xlink:type="simple"/></inline-formula>. All other values are zero, thus it represents exactly one evidence for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e984" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e985" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e986" xlink:type="simple"/></inline-formula>, s.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e987" xlink:type="simple"/></inline-formula>, according to the decoding in <xref ref-type="disp-formula" rid="pcbi.1003037.e139">Eq. (8)</xref>.</p>
         <p>Due to the conditional independences in the probabilistic model every such evidence, i.e. every spike, contributes one factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e988" xlink:type="simple"/></inline-formula> to the likelihood term in the inference of the hidden node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e989" xlink:type="simple"/></inline-formula>. The inference is expressed as <disp-formula id="pcbi.1003037.e990"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e990" xlink:type="simple"/><label>(81)</label></disp-formula>The identity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e991" xlink:type="simple"/></inline-formula> reveals that the above posterior distribution is realized by the relative spike probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e992" xlink:type="simple"/></inline-formula> of the network model according to <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e993" xlink:type="simple"/></inline-formula> replaces <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e994" xlink:type="simple"/></inline-formula> in the computation of the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e995" xlink:type="simple"/></inline-formula>. Due to the step function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e996" xlink:type="simple"/></inline-formula> the result of the convolution in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e997" xlink:type="simple"/></inline-formula> equals the number of spikes within the time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e998" xlink:type="simple"/></inline-formula> from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e999" xlink:type="simple"/></inline-formula>. The factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1000" xlink:type="simple"/></inline-formula>, which has the meaning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1001" xlink:type="simple"/></inline-formula> in the network model, is multiplied <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1002" xlink:type="simple"/></inline-formula> times to the likelihood.</p>
         <p>The above discrete probabilistic model gives an interpretation only for integer values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1003" xlink:type="simple"/></inline-formula>, i.e. for functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1004" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1005" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1006" xlink:type="simple"/></inline-formula> or any positive integer at any time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1007" xlink:type="simple"/></inline-formula>. For an interpretation of arbitrarily shaped EPSPs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1008" xlink:type="simple"/></inline-formula> - especially for continuously decaying functions - in the context of our probabilistic model, we now extend this weighting mechanism from integer valued weights to real valued weights by a linear interpolation of the likelihood in the log-space.</p>
         <p>The obvious restrictions on the EPSP function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1009" xlink:type="simple"/></inline-formula> are that it is non-negative, zero for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1010" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1011" xlink:type="simple"/></inline-formula>, in order to avoid acausal or nondecaying behavior, and unboundedly growing postsynaptic potentials at constant input rates. We assume the normalization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1012" xlink:type="simple"/></inline-formula>. Let again <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1013" xlink:type="simple"/></inline-formula> be the times of the past spiking events and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1014" xlink:type="simple"/></inline-formula> be the indices of the corresponding input neurons. The output distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1015" xlink:type="simple"/></inline-formula>can be written as<disp-formula id="pcbi.1003037.e1016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1016" xlink:type="simple"/><label>(82)</label></disp-formula>which nicely illustrates that every single past spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1017" xlink:type="simple"/></inline-formula> is seen as an evidence in the inference, but that evidence is weighted with a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1018" xlink:type="simple"/></inline-formula>, which is between 0 and 1.</p>
         <p>The analogous interpolation for continuous-valued input activations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1019" xlink:type="simple"/></inline-formula> yields the learning rule in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref>, which is illustrated in <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> as the “Complex STDP rule” (blue dashed curve). The resulting shape of the LTP part of the STDP curve is determined by the EPSP shape defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1020" xlink:type="simple"/></inline-formula>. The positive part of the update in <xref ref-type="disp-formula" rid="pcbi.1003037.e434">Eq. (18)</xref> is weighted by the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1021" xlink:type="simple"/></inline-formula> at the time of firing the postsynaptic spike. Negative updates are performed if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1022" xlink:type="simple"/></inline-formula> is close to zero, which indicates that no presynaptic spikes were observed recently.</p>
         <p>The proof of stochastic convergence does not explicitly assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1023" xlink:type="simple"/></inline-formula> is a binary vector, but is valid for any (positive) random variable vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1024" xlink:type="simple"/></inline-formula> with finite variance. Further, the proof assumes the condition that in every group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1025" xlink:type="simple"/></inline-formula> the sum of the input activities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1026" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1027" xlink:type="simple"/></inline-formula> at all times or at least at those points in time at which one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1028" xlink:type="simple"/></inline-formula> neuron of the WTA-circuit fires. The condition can be relaxed such that the sum per group does not have to be equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1029" xlink:type="simple"/></inline-formula> but to any arbitrary (positive) constant if the corresponding normalization constraint is adapted accordingly. Due to the decaying character of the EPSP shape, this sum will never stay constant, even for very regular input patterns. If we only assumed a constant average activation within a group, allowing for stochastic fluctuations around the target value, it turns out that this condition alone is not enough. We need to further assume that these stochastic fluctuations in the sum of every input group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1030" xlink:type="simple"/></inline-formula> are stochastically independent of the circuit's response <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1031" xlink:type="simple"/></inline-formula>. This assumption is intricate and may depend on the data and the learning progress itself, so it will usually not be exactly fulfilled. We can, however, argue that we are close to independence if at least the sum of activity in every group <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1032" xlink:type="simple"/></inline-formula> is independent of the value of the underlying abstract variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1033" xlink:type="simple"/></inline-formula>.</p>
         <p>In our simulations we obtain the input activations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1034" xlink:type="simple"/></inline-formula> by simulating biologically realistic EPSPs at every synapse, using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1035" xlink:type="simple"/></inline-formula>-kernels with plausible time constants to model the contributions of single input spikes.</p>
      </sec>
      <sec id="s4g">
         <title>Details to <italic>Spike-timing dependent LTD</italic></title>
         <p>We formalize the presynaptic activity of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1036" xlink:type="simple"/></inline-formula> <italic>after</italic> a postsynaptic spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1037" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1038" xlink:type="simple"/></inline-formula>, s.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1039" xlink:type="simple"/></inline-formula> if there is a spike from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1040" xlink:type="simple"/></inline-formula> within the time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1041" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1042" xlink:type="simple"/></inline-formula> otherwise. This trace is used purely for mathematical analysis, and cannot be known to the postsynaptic neuron at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1043" xlink:type="simple"/></inline-formula>, since the future input activity is unknown. Mechanistically, however, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1044" xlink:type="simple"/></inline-formula> can be implemented as a trace updated by postsynaptic firing, and utilized for plasticity at the time of presynaptic firing <xref ref-type="bibr" rid="pcbi.1003037-Schmiedt1">[113]</xref>. Let us now consider the STDP rule illustrated by the red curve in <xref ref-type="fig" rid="pcbi-1003037-g009">Fig. 9</xref>, where a depression of the synapse happens only if there is a presynaptic spike within the short time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1045" xlink:type="simple"/></inline-formula> <italic>after</italic> the postsynaptic spike, i.e. if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1046" xlink:type="simple"/></inline-formula>. The application of this STDP-rule in our neuronal circuit is equivalent to the circuit-spike triggered update rule<disp-formula id="pcbi.1003037.e1047"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1047" xlink:type="simple"/><label>(83)</label></disp-formula>which replaces <xref ref-type="disp-formula" rid="pcbi.1003037.e076">Eq. (5)</xref>. In analogy to <xref ref-type="disp-formula" rid="pcbi.1003037.e086">Eq. (6)</xref> the equilibrium of this new update rule can be derived as<disp-formula id="pcbi.1003037.e1048"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1048" xlink:type="simple"/><label>(84)</label></disp-formula><disp-formula id="pcbi.1003037.e1049"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1049" xlink:type="simple"/><label>(85)</label></disp-formula>under the assumption that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1050" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1051" xlink:type="simple"/></inline-formula> are sampled from a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1052" xlink:type="simple"/></inline-formula>. This shows that the synaptic weights can be interpreted as the log-likelihood ratio of the presynaptic neuron firing before instead of after the postsynaptic neuron. In other words, the neuron's synaptic weights learn the contrast between the current input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1053" xlink:type="simple"/></inline-formula> that caused firing, and the following pattern of activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1054" xlink:type="simple"/></inline-formula>. Note that any factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1055" xlink:type="simple"/></inline-formula> (for LTP) or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1056" xlink:type="simple"/></inline-formula> (for LTD) only leads to a constant offset of the weight which - under the assumption that the offset is the same for all synapses - can be neglected due to the WTA circuit (see <xref ref-type="sec" rid="s4">Methods</xref> “Weight offsets and positive weights”).</p>
         <fig id="pcbi-1003037-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003037.g009</object-id><label>Figure 9</label>
            <caption>
               <title>STDP learning curves with time-dependent LTD.</title>
               <p>Under the simple STDP model (red curve), weight-dependent LTP occurs only if the postsynaptic spike falls within a time window of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1057" xlink:type="simple"/></inline-formula> after the presynaptic spike, and LTD occurs in a time window of the same length, but for the opposite order of spikes. This can be extended to a more complex STDP rule (blue dashed curve), in which both LTP and LTD follow <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1058" xlink:type="simple"/></inline-formula>-kernels with different time constants, typically with longer time-constants for LTD.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003037.g009" position="float" xlink:type="simple"/></fig>
         <p>Similarly to our analysis for the standard SEM rule, we can derive a continuous-time interpretation of the timing-dependent LTD rule. As we did in <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref>, we can define<disp-formula id="pcbi.1003037.e1059"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1059" xlink:type="simple"/><label>(86)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1060" xlink:type="simple"/></inline-formula> is the same convolution kernel as in <xref ref-type="disp-formula" rid="pcbi.1003037.e401">Eq. (17)</xref>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1061" xlink:type="simple"/></inline-formula> is an arbitrary but time-inversed kernel, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1062" xlink:type="simple"/></inline-formula> for positive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1063" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1064" xlink:type="simple"/></inline-formula> for negative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1065" xlink:type="simple"/></inline-formula>. The value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1066" xlink:type="simple"/></inline-formula> thus reflects a time-discounted sum of presynaptic activity immediately after the postsynaptic spike.</p>
         <p>The complex STDP rule from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref>, which models LTD as a constant time-independent depression, can be seen as an extreme case of the spike-timing dependent LTD rule. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1067" xlink:type="simple"/></inline-formula> is a step function with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1068" xlink:type="simple"/></inline-formula> in the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1069" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1070" xlink:type="simple"/></inline-formula> everywhere else, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1071" xlink:type="simple"/></inline-formula> is just the average rate of presynaptic activity in the time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1072" xlink:type="simple"/></inline-formula> following a postsynaptic spike. In the limit of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1073" xlink:type="simple"/></inline-formula> this is equivalent to the overall spiking rate of the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1074" xlink:type="simple"/></inline-formula>, which is proportional to the marginal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1075" xlink:type="simple"/></inline-formula> in the probabilistic model. Precisely, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1076" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1077" xlink:type="simple"/></inline-formula> is the base firing rate of an active input in our input encoding model. The equilibrium point of every weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1078" xlink:type="simple"/></inline-formula> becomes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1079" xlink:type="simple"/></inline-formula>, neglecting the offsets induced by the constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1080" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1081" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1082" xlink:type="simple"/></inline-formula>. It is easy to see that the probabilistic interpretation of the neuronal model from <xref ref-type="disp-formula" rid="pcbi.1003037.e050">Eq. (4)</xref> is invariant under the transformation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1083" xlink:type="simple"/></inline-formula>, since<disp-formula id="pcbi.1003037.e1084"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1084" xlink:type="simple"/><label>(87)</label></disp-formula><disp-formula id="pcbi.1003037.e1085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1085" xlink:type="simple"/><label>(88)</label></disp-formula><disp-formula id="pcbi.1003037.e1086"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003037.e1086" xlink:type="simple"/><label>(89)</label></disp-formula>which proves that in our network model the complex STDP rule from <xref ref-type="fig" rid="pcbi-1003037-g002">Fig. 2</xref> is equivalent to an offset-free STDP rule in the limit of an arbitrarily long window for LTD. In practice, of course, we can assume that the times between pre- and post-synaptic spikes are finite, and we have shown in <xref ref-type="fig" rid="pcbi-1003037-g004">Fig. 4</xref> that as a result, very realistic shapes of STDP curves emerge at intermediate stimulation frequencies.</p>
      </sec>
   </sec>
   <sec id="s5">
      <title>Supporting Information</title>
<supplementary-material id="pcbi.1003037.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003037.s001" position="float" xlink:type="simple">
<label>Text S1</label>
<caption><p><bold>Supplement.</bold> Derivation of Variance tracking, Adaptation to changing input distributions, Invariance to Time-Warping, Simulation Parameters.</p>
      <p>(PDF)</p>
</caption></supplementary-material>
   </sec>
</body>
<back>
   <ack>
      <p>We would like to thank Wulfram Gerstner for critical comments to an earlier version of this paper. MP would like to thank Rodney J. Douglas and Tobi Delbruck for their generous advice and support.</p>
   </ack>
   <ref-list>
      <title>References</title>
      <ref id="pcbi.1003037-Rao1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Rao RPN, Olshausen BA, Lewicki MS (2002) Probabilistic Models of the Brain. MIT Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Doya1"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Doya K, Ishii S, Pouget A, Rao RPN (2007) Bayesian Brain: Probabilistic Approaches to Neural Coding. MIT-Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Griffiths1"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name> (<year>2006</year>) <article-title>Optimal predictions in everyday cognition</article-title>. <source>Psychological Science</source> <volume>17</volume>: <fpage>767</fpage>–<lpage>773</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Griffiths2"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">Griffiths TL, Kemp C, Tenenbaum JB (2008) Bayesian models of cognition. In: Sun R, editor. Handbook of Computational Cognitive Modeling. Cambridge Univ. Press. chapter 3. p. 59100.</mixed-citation></ref>
      <ref id="pcbi.1003037-Krding1"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2004</year>) <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source> <volume>427</volume>: <fpage>244</fpage>–<lpage>247</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Fiser1"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Statistically optimal perception and learning: from behavior to neural representation</article-title>. <source>Trends in Cogn Sciences</source> <volume>14</volume>: <fpage>119</fpage>–<lpage>130</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Ma1"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nature Neuroscience</source> <volume>9</volume>: <fpage>1432</fpage>–<lpage>1438</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Dan1"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Spike timing-dependent plasticity of neural circuits</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>23</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Feldman1"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldman</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>The spike-timing dependence of plasticity</article-title>. <source>Neuron</source> <volume>75</volume>: <fpage>556</fpage>–<lpage>571</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Song1"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2001</year>) <article-title>Cortical developing and remapping through spike timing-dependent plasticity</article-title>. <source>Neuron</source> <volume>32</volume>: <fpage>339</fpage>–<lpage>350</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Kempter1"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name> (<year>1999</year>) <article-title>Hebbian learning and spiking neurons</article-title>. <source>Phys Rev E</source> <volume>59</volume>: <fpage>4498</fpage>–<lpage>4514</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Kempter2"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name> (<year>2001</year>) <article-title>Intrinsic stabilization of output rates by spikebased Hebbian learning</article-title>. <source>Neural Computation</source> <volume>13</volume>: <fpage>2709</fpage>–<lpage>2741</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Abbott1"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2000</year>) <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1178</fpage>–<lpage>1183</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Daoudal1"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daoudal</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Long-term plasticity of intrinsic excitability: learning rules and mechanisms</article-title>. <source>Learn Mem</source> <volume>10</volume>: <fpage>456</fpage>–<lpage>465</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Grillner1"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Grillner S, Graybiel A (2006) Microcircuits: The Interface between Neurons and Global Brain Function. MIT-Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Douglas1"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name> (<year>2004</year>) <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annual Review of Neuroscience</source> <volume>27</volume>: <fpage>419</fpage>–<lpage>451</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Hahnloser1"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name>, <name name-style="western"><surname>Sarpeshkar</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mahowald</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2000</year>) <article-title>Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</article-title>. <source>Nature</source> <volume>405</volume>: <fpage>947</fpage>–<lpage>951</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Carandini1"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Heeger</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature Reviews Neuroscience</source> <volume>13</volume>: <fpage>51</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Itti1"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name> (<year>1998</year>) <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source> <volume>20</volume>: <fpage>1254</fpage>–<lpage>1259</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Nessler1"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>STDP enables spiking neurons to detect hidden causes of their inputs</article-title>. <source>Proceedings of NIPS Advances in Neural Information Processing Systems</source> <volume>22</volume>: <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Pfeiffer1"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfeiffer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Reward-modulated Hebbian Learning of Decision Making</article-title>. <source>Neural Computation</source> <volume>22</volume>: <fpage>1399</fpage>–<lpage>1444</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Maass1"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2000</year>) <article-title>On the computational power of winner-take-all</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>2519</fpage>–<lpage>2535</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Rutishauser1"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rutishauser</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>State-dependent computation using coupled recurrent networks</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>478</fpage>–<lpage>509</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Buesing1"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name> (<year>2011</year>) <collab xlink:type="simple">MaassW</collab> (<year>2011</year>) <article-title>Neural dynamics as sampling: A model for stochastic computation in recurrent networks of spiking neurons</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002211</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Pecevski1"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons</article-title>. <source>PLoS Comput Biol</source> <volume>7(12)</volume>: <fpage>e1002294</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Hinton1"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name> (<year>1997</year>) <article-title>Generative models for discovering sparse distributed representations</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>352</volume>: <fpage>1177</fpage>–<lpage>1190</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Hinton2"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Osindero</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>YW</given-names></name> (<year>2006</year>) <article-title>A Fast Learning Algorithm for Deep Belief Nets</article-title>. <source>Neural Computation</source> <volume>18</volume>: <fpage>1527</fpage>–<lpage>1554</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Keck1"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keck</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Savin</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Lücke</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Feedforward Inhibition and Synaptic Scaling - Two Sides of the Same Coin?</article-title> <source>PLoS Computational Biology</source> <volume>8</volume>: <fpage>e1002432</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Sato1"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Sato M (1999) Fast learning of on-line EM algorithm. Technical report, ATR Human Information Processing Research Laboratories, Kyoto, Japan.</mixed-citation></ref>
      <ref id="pcbi.1003037-Sato2"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name> (<year>2000</year>) <article-title>On-line EM Algorithm for the Normalized Gaussian Network</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>407</fpage>–<lpage>432</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Dempster1"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name> (<year>1977</year>) <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source> <volume>39</volume>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Bishop1"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Bishop CM (2006) Pattern Recognition and Machine Learning. New York: Springer.</mixed-citation></ref>
      <ref id="pcbi.1003037-Oster1"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oster</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Computation with spikes in a winner-take-all network</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>2437</fpage>–<lpage>2465</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Okun1"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lampl</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nature Neuroscience</source> <volume>11</volume>: <fpage>535</fpage>–<lpage>537</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Ecker1"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ecker</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Keliris</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Decorrelated neuronal firing in cortical microcircuits</article-title>. <source>Science</source> <volume>327</volume>: <fpage>584</fpage>–<lpage>587</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Fino1"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fino</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Yuste</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Dense inhibitory connectivity in neocortex</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>1188</fpage>–<lpage>1203</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gerstner1"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Gerstner W, Kistler WM (2002) Spiking Neuron Models. Cambridge: Cambridge University Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Jolivet1"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolivet</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rauch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title>. <source>Journal of Computational Neuroscience</source> <volume>21</volume>: <fpage>35</fpage>–<lpage>49</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Caporale1"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caporale</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2008</year>) <article-title>Spike timing-dependent plasticity: a Hebbian learning rule</article-title>. <source>Annual Review of Neuroscience</source> <volume>31</volume>: <fpage>25</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Sjstrm1"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name> (<year>2001</year>) <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source> <volume>32</volume>: <fpage>1149</fpage>–<lpage>1164</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Toyoizumi1"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2005</year>) <article-title>Generalized Bienenstock-Cooper-Munro rule for spiking neurons that maximizes information transmission</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>102</volume>: <fpage>5239</fpage>–<lpage>5244</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Pfister1"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Barber</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning</article-title>. <source>Neural Computation</source> <volume>18</volume>: <fpage>1318</fpage>–<lpage>1348</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Bi1"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>Journal of Neuroscience</source> <volume>18</volume>: <fpage>10464</fpage>–<lpage>10472</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Hebb1"><label>44</label><mixed-citation publication-type="other" xlink:type="simple">Hebb DO (1949) The Organization of Behavior. New York: Wiley.</mixed-citation></ref>
      <ref id="pcbi.1003037-Cudmore1"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cudmore</surname><given-names>RH</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name> (<year>2004</year>) <article-title>Long-term potentiation of intrinsic excitability in LV visual cortical neurons</article-title>. <source>Journal of Neurophysiology</source> <volume>92</volume>: <fpage>341</fpage>–<lpage>348</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Turrigiano1"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turrigiano</surname><given-names>G</given-names></name> (<year>2011</year>) <article-title>Too many cooks? intrinsic and synaptic homeostatic mechanisms in cortical circuit refinement</article-title>. <source>Annual Rreview of Neuroscience</source> <volume>34</volume>: <fpage>89</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Habenschuss1"><label>47</label><mixed-citation publication-type="other" xlink:type="simple">Habenschuss S, Bill J, Nessler B (2012) Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints. In: Advances in Neural Information Processing Systems 25. pp. 782–790.</mixed-citation></ref>
      <ref id="pcbi.1003037-Ghahramani1"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Ghahramani Z, Michael MIJ (1997) Mixture models for learning from incomplete data. In: Computational Learning Theory and Natural Learning Systems: Volume IV: Making Learning Systems Practical. MA, USA: MIT Press Cambridge. pp. 67–85.</mixed-citation></ref>
      <ref id="pcbi.1003037-Masquelier1"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Masquelier</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hugues</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Deco</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Oscillations, Phase-of-Firing Coding, and Spike Timing-Dependent Plasticity:An Efficient Learning Scheme</article-title>. <source>Journal of Neuroscience</source> <volume>29</volume>: <fpage>13484</fpage>–<lpage>13493</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gilson1"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Masquelier</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hugues</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>STDP Allows Fast Rate-Modulated Coding with Poisson-Like Spike Trains</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002231</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Jank1"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jank</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>The EM algorithm, its randomized implementation and global optimization: Some challenges and opportunities for operations research</article-title>. <source>Perspectives in Operations Research</source> <fpage>367</fpage>–<lpage>392</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Kushner1"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">Kushner H, Yin G (2003) Stochastic approximation and recursive algorithms and applications, volume 35. Springer Verlag.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gjorgjieva1"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gjorgjieva</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Audet</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name> (<year>2011</year>) <article-title>A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-order spatiotemporal correlations</article-title>. <source>PNAS</source> <volume>108</volume>: <fpage>19383</fpage>–<lpage>19388</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Song2"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2000</year>) <article-title>Competitive Hebbian learning through spike-timing dependent synaptic plasticity</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Kobayashi1"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kobayashi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Spike Train Timing-Dependent Associative Modification of Hippocampal CA3 Recurrent Synapses by Mossy Fibers</article-title>. <source>Neuron</source> <volume>41</volume>: <fpage>445</fpage>–<lpage>454</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Morrison1"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>Phenomenological models of synaptic plasticity based on spike timing</article-title>. <source>Biological Cybernetics</source> <volume>98</volume>: <fpage>459</fpage>–<lpage>478</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Clopath1"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title>. <source>Nature Neuroscience</source> <volume>13</volume>: <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Graupner1"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graupner</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location</article-title>. <source>PNAS</source> <volume>109</volume>: <fpage>3991</fpage>–<lpage>3996</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Masquelier2"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Masquelier</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Guyonneau</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Spike timing dependent plasticity finds the start of repeating patterns in continuous spike trains</article-title>. <source>PLoS ONE</source> <volume>3</volume>: <fpage>e1377</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Froemke1"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2002</year>) <article-title>Spike-timing-dependent synaptic modification induced by natural spike trains</article-title>. <source>Nature</source> <volume>415</volume>: <fpage>433</fpage>–<lpage>438</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Olshausen1"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1996</year>) <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Li1"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Fitzpatrick</surname><given-names>D</given-names></name>, <name name-style="western"><surname>White</surname><given-names>L</given-names></name> (<year>2006</year>) <article-title>The development of direction selectivity in ferret visual cortex requires early visual experience</article-title>. <source>Nature neuroscience</source> <volume>9</volume>: <fpage>676</fpage>–<lpage>681</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Espinosa1"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Espinosa</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Stryker</surname><given-names>MP</given-names></name> (<year>2012</year>) <article-title>Development and Plasticity of the Primary Visual Cortex</article-title>. <source>Neuron</source> <volume>75</volume>: <fpage>230</fpage>–<lpage>249.s</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-LeCun1"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname><given-names>P</given-names></name> (<year>1998</year>) <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source> <volume>86</volume>: <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Ciresan1"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ciresan</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Meier</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Gambardella</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Deep, big, simple neural nets for handwritten digit recognition</article-title>. <source>Neural computation</source> <volume>22</volume>: <fpage>3207</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Hinton3"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname><given-names>RR</given-names></name> (<year>2006</year>) <article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source> <volume>313</volume>: <fpage>504</fpage>–<lpage>507</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Ranzato1"><label>67</label><mixed-citation publication-type="other" xlink:type="simple">Ranzato M, Huang F, Boureau YL, Lecun Y (2007) Unsupervised learning of invariant feature hierarchies with applications to object recognition. In: Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR'07). pp. 1–8.</mixed-citation></ref>
      <ref id="pcbi.1003037-Masquelier3"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Masquelier</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Guyonneau</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Competitive STDP-based spike pattern learning</article-title>. <source>Neural Computation</source> <volume>21</volume>: <fpage>1259</fpage>–<lpage>1276</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Oaksford1"><label>69</label><mixed-citation publication-type="other" xlink:type="simple">Oaksford M, Chater N (2007) Bayesian Rationality: The Probabilistic Approach to Human Reasoning. Oxford University Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Salakhutdinov1"><label>70</label><mixed-citation publication-type="other" xlink:type="simple">Salakhutdinov R, Hinton G (2009) Deep boltzmann machines. In: Proceedings of the international conference on artificial intelligence and statistics. Cambridge, MA: MIT Press, volume 5. pp. 448–455.</mixed-citation></ref>
      <ref id="pcbi.1003037-Desimone1"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name> (<year>1991</year>) <article-title>Face-selective cells in the temporal cortex of monkeys</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>3</volume>: <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Poon1"><label>72</label><mixed-citation publication-type="other" xlink:type="simple">Poon H, Domingos P (2011) Sum-product networks: A new deep architecture. In: Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on. IEEE, pp. 689–690.</mixed-citation></ref>
      <ref id="pcbi.1003037-Nessler2"><label>73</label><mixed-citation publication-type="other" xlink:type="simple">Nessler B, Pfeiffer M, Maass W (2009) Hebbian learning of Bayes optimal decisions. In Proc of NIPS 2008: Advances in Neural Information Processing Systems 21.</mixed-citation></ref>
      <ref id="pcbi.1003037-Indiveri1"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Indiveri</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Linares-Barranco</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hamilton</surname><given-names>T</given-names></name>, <name name-style="western"><surname>van Schaik</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Etienne-Cummings</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Neuromorphic silicon neuron circuits</article-title>. <source>Frontiers in Neuroscience</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Schemmel1"><label>75</label><mixed-citation publication-type="other" xlink:type="simple">Schemmel J, Brüderle D, Meier K, Ostendorf B (2007) Modeling synaptic plasticity within networks of highly accelerated I&amp;F neurons. In: International Symposium on Circuits and Systems, ISCAS 2007. IEEE, pp. 3367–3370.</mixed-citation></ref>
      <ref id="pcbi.1003037-Jin1"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jin</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Lujan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Plana</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Davies</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Temple</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Modeling spiking neural networks on SpiNNaker</article-title>. <source>Computing in Science &amp; Engineering</source> <volume>12</volume>: <fpage>91</fpage>–<lpage>97</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Rumelhart1"><label>77</label><mixed-citation publication-type="other" xlink:type="simple">Rumelhart DE, Zipser D (1988) Feature discovery by competitive learning. In: Waltz D, Feldman JA, editors, Connectionist Models and Their Implications: Readings from Cognitive Science, Ablex Publishing Corporation. pp. 205–242.</mixed-citation></ref>
      <ref id="pcbi.1003037-Nowlan1"><label>78</label><mixed-citation publication-type="other" xlink:type="simple">Nowlan SJ (1990) Maximum likelihood competitive learning. In: Touretzky D, editor, Advances in Neural Information Processing Systems (NIPS), San Mateo, California: Morgan Kaufmann, volume 2. pp. 574–582.</mixed-citation></ref>
      <ref id="pcbi.1003037-Celeux1"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Celeux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Diebolt</surname><given-names>J</given-names></name> (<year>1985</year>) <article-title>The SEM algorithm: A probabilistic teacher algorithm derived from the EM algorithm for the mixture problem</article-title>. <source>Comput Statist Quater</source> <volume>2</volume>: <fpage>73</fpage>–<lpage>82</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Nowlan2"><label>80</label><mixed-citation publication-type="other" xlink:type="simple">Nowlan SJ (1991) Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures. Technical Report CS-91-126, Carnegie Mellon University, Pittsburgh.</mixed-citation></ref>
      <ref id="pcbi.1003037-Jordan1"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Jacobs</surname><given-names>RA</given-names></name> (<year>1994</year>) <article-title>Hierarchical mixtures of experts and the algorithm</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>181</fpage>–<lpage>214</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Neal1"><label>82</label><mixed-citation publication-type="other" xlink:type="simple">Neal RM, Hinton GE (1998) A view of the EM algorithm that justifies incremental, sparse, and other variants. In: Jordan MI, editor, Learning in Graphical Models, Kluwer Academic Publishers, volume 89. pp. 355–370.</mixed-citation></ref>
      <ref id="pcbi.1003037-Savin1"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Savin</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Joshi</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Independent Component Analysis in Spiking Neurons</article-title>. <source>PLoS Computational Biology</source> <volume>6</volume>: <fpage>e1000757</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Dayan1"><label>84</label><mixed-citation publication-type="other" xlink:type="simple">Dayan P, Abbott LF (2001) Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. Cambridge, MA: MIT Press.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gupta1"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>LN</given-names></name> (<year>2009</year>) <article-title>Hebbian learning with winner take all for spiking neural networks</article-title>. <source>IEEE International Joint Conference on Neural Networks</source> <fpage>1189</fpage>–<lpage>1195</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gupta2"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>LN</given-names></name> (<year>2007</year>) <article-title>Character recognition using spiking neural networks</article-title>. <source>IJCNN</source> <fpage>53</fpage>–<lpage>58</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Rao2"><label>87</label><mixed-citation publication-type="other" xlink:type="simple">Rao RP (2005) Hierarchical Bayesian inference in networks of spiking neurons. In: Advances in Neural Information Processing Systems. MIT Press, volume 17, pp. 1113–1120.</mixed-citation></ref>
      <ref id="pcbi.1003037-Rao3"><label>88</label><mixed-citation publication-type="other" xlink:type="simple">Rao RPN (2007) Neural models of Bayesian belief propagation. In: Doya K, Ishii S, Pouget A, Rao RPN, editors, Bayesian Brain., Cambridge, MA: MIT-Press. pp. 239–267.</mixed-citation></ref>
      <ref id="pcbi.1003037-Zemel1"><label>89</label><mixed-citation publication-type="other" xlink:type="simple">Zemel R, Huys QJM, Natarajan R, Dayan P (2005) Probabilistic computation in spiking populations. In: Advances in Neural Information Processing Systems 17: Proceedings of the 2004 Conference. volume 17, pp. 1609–1616.</mixed-citation></ref>
      <ref id="pcbi.1003037-Deneve1"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian spiking neurons I: Inference</article-title>. <source>Neural Computation</source> <volume>20</volume>: <fpage>91</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Deneve2"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deneve</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian spiking neurons II: Learning</article-title>. <source>Neural Computation</source> <volume>20</volume>: <fpage>118</fpage>–<lpage>145</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gtig1"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gütig</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Time-warp invariant neuronal processing</article-title>. <source>PLoS Biology</source> <volume>7</volume>: <fpage>e1000141</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Rezende1"><label>93</label><mixed-citation publication-type="other" xlink:type="simple">Rezende DJ, Wierstra D, Gerstner W (2011) Variational learning for recurrent spiking networks. In: Shawe-Taylor J, Zemel R, Bartlett P, Pereira F, Weinberger K, editors, Advances in Neural Information Processing Systems 24. pp. 136–144.</mixed-citation></ref>
      <ref id="pcbi.1003037-Brea1"><label>94</label><mixed-citation publication-type="other" xlink:type="simple">Brea J, SennW, Pfister JP (2011) Sequence learning with hidden units in spiking neural networks. In: Shawe-Taylor J, Zemel R, Bartlett P, Pereira F, Weinberger K, editors, Advances in Neural Information Processing Systems 24. pp. 1422–1430.</mixed-citation></ref>
      <ref id="pcbi.1003037-Liao1"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liao</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Malinow</surname><given-names>R</given-names></name> (<year>1992</year>) <article-title>Direct measurement of quantal changes underlying long-term potentiation in CA1 hippocampus</article-title>. <source>Neuron</source> <volume>9</volume>: <fpage>1089</fpage>–<lpage>1097</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Montgomery1"><label>96</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montgomery</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Pavlidis</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Madison</surname><given-names>DV</given-names></name> (<year>2001</year>) <article-title>Pair recordings reveal all-silent synaptic connections and the postsynaptic expression of long-term potentiation</article-title>. <source>Neuron</source> <volume>29</volume>: <fpage>691</fpage>–<lpage>701</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Debanne1"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name> (<year>2010</year>) <article-title>Spike-timing dependent plasticity beyond synapse – pre- and postsynaptic plasticity of intrinsic neuronal excitability</article-title>. <source>Front Syn Neurosci</source> <volume>2</volume>: <fpage>21</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Olshausen2"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>2005</year>) <article-title>How close are we to understanding V1?</article-title> <source>Neural Computation</source> <volume>17</volume>: <fpage>1665</fpage>–<lpage>1699</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Kerr1"><label>99</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerr</surname><given-names>JND</given-names></name>, <name name-style="western"><surname>de Kock</surname><given-names>CPJ</given-names></name>, <name name-style="western"><surname>Greenberg</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Bruno</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Spatial organization of neuronal population responses in layer 2/3 of rat barrel cortex</article-title>. <source>Journal of Neuroscience</source> <volume>27</volume>: <fpage>13316</fpage>–<lpage>13328</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Nikolic1"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nikolic</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Haeusler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Distributed fading memory for stimulus properties in the primary visual cortex</article-title>. <source>PLoS Biology</source> <volume>7</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Faisal1"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faisal</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Selen</surname><given-names>LPJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2008</year>) <article-title>Noise in the nervous system</article-title>. <source>Nature Reviews Neuroscience</source> <volume>9</volume>: <fpage>292</fpage>–<lpage>303</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gilbert1"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Sigman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Crist</surname><given-names>RE</given-names></name> (<year>2001</year>) <article-title>The neural basis of perceptual learning</article-title>. <source>Neuron</source> <volume>31</volume>: <fpage>681</fpage>–<lpage>697</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Gilbert2"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Piech</surname><given-names>V</given-names></name> (<year>2009</year>) <article-title>Perceptual learning and adult cortical plasticity</article-title>. <source>Journal of Physiology</source> <volume>387</volume>: <fpage>2743</fpage>–<lpage>2751</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Goard1"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goard</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Basal forebrain activation enhances cortical coding of natural scenes</article-title>. <source>Nature Neuroscience</source> <volume>12</volume>: <fpage>1444</fpage>–<lpage>1449</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-deVillersSidani1"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Villers-Sidani</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name> (<year>2011</year>) <article-title>Lifelong plasticity in the rat auditory cortex: basic mechanisms and role of sensory experience</article-title>. <source>Prog Brain Res</source> <volume>191</volume>: <fpage>119</fpage>–<lpage>131</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Merabet1"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merabet</surname><given-names>LB</given-names></name>, <name name-style="western"><surname>Pascual-Leone</surname><given-names>A</given-names></name> (<year>2010</year>) <article-title>Neural reorganization following sensory loss: the opportunity of change</article-title>. <source>Nature Reviews Neuroscience</source> <volume>11</volume>: <fpage>44</fpage>–<lpage>52</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Maldonado1"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maldonado</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Babul</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Rodriguez</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Synchronization of neuronal responses in primary visual cortex of monkeys viewing natural images</article-title>. <source>Journal of Neurophysiology</source> <volume>100</volume>: <fpage>1523</fpage>–<lpage>1532</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Uhlhaas1"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uhlhaas</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Pipa</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lima</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Melloni</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Neuenschwander</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Neural synchrony in cortical networks: history, concept and current status</article-title>. <source>Front Integr Neurosci</source> <volume>3</volume>: <fpage>17</fpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Uhlhaas2"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uhlhaas</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Roux</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Rodriguez</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Rotarska-Jagiela</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Neural synchrony and the development of cortical networks</article-title>. <source>Trends in Cogn Sciences</source> <volume>14</volume>: <fpage>72</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Destexhe1"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rudolph</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fellous</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2001</year>) <article-title>Fluctuating synaptic conductances recreate in vivo-like activity in neocortical neurons</article-title>. <source>Neuroscience</source> <volume>107</volume>: <fpage>13</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Binzegger1"><label>111</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Binzegger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name> (<year>2007</year>) <article-title>Stereotypical bouton clustering of individual neurons in cat primary visual cortex</article-title>. <source>Journal of Neuroscience</source> <volume>27</volume>: <fpage>12242</fpage>–<lpage>12254</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Markov1"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markov</surname><given-names>NT</given-names></name>, <name name-style="western"><surname>Misery</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Falchier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lamy</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vezoli</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Weight Consistency Specifies Regularities of Macaque Cortical Networks</article-title>. <source>Cerebral Cortex</source> <volume>21</volume>: <fpage>1254</fpage>–<lpage>1272</lpage>.</mixed-citation></ref>
      <ref id="pcbi.1003037-Schmiedt1"><label>113</label><mixed-citation publication-type="other" xlink:type="simple">Schmiedt J, Albers C, Pawelzik K (2010) Spike timing-dependent plasticity as dynamic filter. In: Advances in Neural Information Processing Systems 23. pp. 2110–2118.</mixed-citation></ref>
   </ref-list>
</back>
</article>