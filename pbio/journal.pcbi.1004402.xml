<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01297</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004402</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Dynamic Integration of Value Information into a Common Probability Currency as a Theory for Flexible Decision Making</article-title>
<alt-title alt-title-type="running-head">Dynamic Integration of Value Information</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Christopoulos</surname> <given-names>Vassilios</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schrater</surname> <given-names>Paul R.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Psychology, University of Minnesota, Minneapolis, Minnesota, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Computer Science &amp; Engineering, University of Minnesota, Minneapolis, Minnesota, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>O’Reilly</surname> <given-names>Jill X</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Oxford University, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: VC PRS. Performed the experiments: VC. Analyzed the data: VC PRS. Contributed reagents/materials/analysis tools: VC. Wrote the paper: VC PRS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">vchristo@caltech.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>9</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>9</issue>
<elocation-id>e1004402</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>7</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>6</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Christopoulos, Schrater</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004402" xlink:type="simple"/>
<abstract>
<p>Decisions involve two fundamental problems, selecting goals and generating actions to pursue those goals. While simple decisions involve choosing a goal and pursuing it, humans evolved to survive in hostile dynamic environments where goal availability and value can change with time and previous actions, entangling goal decisions with action selection. Recent studies suggest the brain generates concurrent action-plans for competing goals, using online information to bias the competition until a single goal is pursued. This creates a challenging problem of integrating information across diverse types, including both the dynamic value of the goal and the costs of action. We model the computations underlying dynamic decision-making with disparate value types, using the probability of getting the highest pay-off with the least effort as a common currency that supports goal competition. This framework predicts many aspects of decision behavior that have eluded a common explanation.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Choosing between alternative options requires assigning and integrating values along a multitude of dimensions. For instance, when buying a car, different cars may vary for their price, quality, fuel economy and more. Solving this problem requires finding a common currency to allow integration of disparate value dimensions. In dynamic decisions, in which the environment changes continuously, this multi-dimensional integration must be updated over time. Despite many years of research, it is still unclear how the brain integrates value information and makes decisions in the presence of competing alternatives. In the current study, we propose a probabilistic theory that allows dynamically integrating value information into a common currency. It builds on successful models in motor control and decision-making. It is comprised of a series of control schemes with each of them attached to an individual goal, generating an optimal action-plan to achieve that goal starting from the current state. The key novelty is the relative desirability computation that integrates good- and action- values to a single dynamic variable that weighs the individual action-plans as a function of state and time. By dynamically integrating value information, our theory models many key results in movement decisions that have previously eluded a common explanation.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by ONR: N000140710937. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="26"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A soccer player moves the ball down the field, looking for an open teammate or a chance to score a goal. Abstractly, the soccer player faces a ubiquitous but challenging decision problem. He/she must select between many competing goals while acting, whose costs and benefits can change dynamically during ongoing actions. In this game scenario, the attacker has options to pass the ball to one of his/her teammates. An undefended player is preferred, but this opportunity will soon be lost if the ball is not quickly passed. If all teammates are marked by opposing players, other alternatives like holding the ball and delaying the decision may be better. Critically, the best option is not immediately evident before acting. To decide which strategy to follow at a given moment requires <italic>dynamically integrating</italic> value information from disparate sources. This information is diverse relating to both the dynamic value of the goal (i.e., relative reward of the goal, probability that reward is available for that goal) and the dynamic action cost (i.e., cost of actions to pursue that goal, precision required), creating a challenging problem in integrating information across these diverse types in real time. Despite intense research in decision neuroscience, dynamic value integration into a common currency remains poorly understood.</p>
<p>Previous explanations fall into two categories. The <italic>goods-based</italic> theory [<xref ref-type="bibr" rid="pcbi.1004402.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref007">7</xref>] proposes that all the decision factors associated with an option are integrated into a subjective economic value independently computed for each alternative. This view is consistent with evidence suggesting convergence of value information in the prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1004402.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref007">7</xref>]. Critically, action planning starts only after a decision is made. While this view is sufficient for decisions like buying or renting a house, modifications are needed for decisions while acting. Alternatively, an <italic>action-based</italic> theory proposes that options have associated action-plans. According to this theory, when the brain is faced with multiple potential goals, it generates concurrent action-plans that compete for selection and uses value information to bias this competition until a single option is selected [<xref ref-type="bibr" rid="pcbi.1004402.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref014">14</xref>]. This theory has been received apparent support from neurophysiological [<xref ref-type="bibr" rid="pcbi.1004402.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref019">19</xref>] and behavioral [<xref ref-type="bibr" rid="pcbi.1004402.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref023">23</xref>] studies. Although the action-based theory explains competition, it leaves mysterious how action cost is integrated with good value (also referred as stimulus value in some decision-making studies [<xref ref-type="bibr" rid="pcbi.1004402.ref013">13</xref>]) that have different currencies and how goods-based decisions that do not involve action competition are made. To solve complex decision problems, the brain must dynamically integrate all the factors that influence the desirability of engaging in an action-plan directed towards a goal.</p>
<p>We propose a theory of dynamic value integration that subsumes both goods-based and action-based theories. We provide a simple, computationally feasible way to integrate online information about the cost of actions and the value of goods into an evolving assessment of the <italic>desirability</italic> of each goal. By integrating value information into a common currency, our approach models many key results in decision tasks with competing goals that have eluded a common explanation, including trajectory averaging in rapid reaching tasks with multiple potential goals, a common explanation for errors due to competition including the global-effect paradigm in express saccadic movements [<xref ref-type="bibr" rid="pcbi.1004402.ref024">24</xref>], and a unified explanation for the pattern of errors due to competition in sequential decisions [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>].</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>This section describes analytically the computational theory developed in this study to model decisions in tasks with competing goals. We used a reaching task as a paradigm. Full details of the architecture and stochastic optimal control methodology that underlies the control schemes of our theory is in <xref ref-type="supplementary-material" rid="pcbi.1004402.s002">S1 Text</xref> for reaching and <xref ref-type="supplementary-material" rid="pcbi.1004402.s003">S2 Text</xref> for saccade models.</p>
<sec id="sec003">
<title>Action selection in reaching tasks with competing goals</title>
<p>Stochastic optimal control has proven a powerful tool at modeling goal-directed movements, such as reaching [<xref ref-type="bibr" rid="pcbi.1004402.ref026">26</xref>], grasping [<xref ref-type="bibr" rid="pcbi.1004402.ref027">27</xref>] and walking [<xref ref-type="bibr" rid="pcbi.1004402.ref028">28</xref>] (for review see [<xref ref-type="bibr" rid="pcbi.1004402.ref029">29</xref>]). It involves solving for a policy <italic>π</italic> that maps states into actions <bold>u</bold><sub><italic>t</italic></sub> = <italic>π</italic>(<bold>x</bold><sub><italic>t</italic></sub>) by minimizing a cost function penalizing actions and deviations from a goal. Despite the growing popularity of optimal control models, most of them are limited to tasks with single goals, because policies are easily defined towards a single goal. On the other hand, it is unclear how to define policies in the presence of multiple goals, each of which may provide different reward and may require different effort. The core difficulty is to develop a single policy that selects actions that pursue many targets but ultimately arrives at only one.</p>
<p>One of the simplest solutions is to carefully construct a composite cost function that incorporates all targets. However, naive applications of this approach can produce quite poor results. For instance, an additive mixture of quadratic cost functions is a new cost function with a minimum that does not lie at any of the competing targets. The difficulty is that quadratic cost functions do not capture the winner-take-all implicit reward structure, since mixtures of quadratics reward best for terminal positions in between targets. Even when such a cost function can be constructed, it can be very difficult to solve the policy, since these types of decision problems are P-SPACE complete—a class of problems more intractable than NP-complete. Any dynamic change in targets configuration requires a full re-computation, which makes the approach difficult to implement as a real-time control strategy [<xref ref-type="bibr" rid="pcbi.1004402.ref030">30</xref>].</p>
<p>To preserve simplicity, we propose to decompose the problem into policy solutions for the individual targets. The overall solution should involve following the best policy at each moment, given incoming information. We can construct a simple cost function that has this property using indicator variables <italic>ν</italic>(<bold>x</bold><sub><italic>t</italic></sub>). The indicator variables encode the policy that has the lowest future expected value from each state—in other words, it categorizes the state space into regions where following one of the policies to a goal <italic>i</italic> is the best option. In essence, a goal <italic>i</italic> “owns” these regions of the state space. We can write the cost function that describes this problem as a <italic>ν</italic>-weighted mixture of individual cost functions <inline-formula id="pcbi.1004402.e001"><alternatives><graphic id="pcbi.1004402.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e001"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>J</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mi>s</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1004402.e002"><alternatives><graphic id="pcbi.1004402.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>J</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>J</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mi>J</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:msub><mml:mi>T</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>-</mml:mo> <mml:mi>S</mml:mi> <mml:msub><mml:mi mathvariant="bold">p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mi>Q</mml:mi> <mml:msub><mml:mi>T</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:msub><mml:mi>T</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>-</mml:mo> <mml:mi>S</mml:mi> <mml:msub><mml:mi mathvariant="bold">p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>T</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mi>R</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msub><mml:mrow/><mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:munder> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>N</italic> is the total number of targets and <italic>ν</italic><sub><italic>j</italic></sub> is the indicator variable associated with the target <italic>j</italic>. The cost function <italic>J</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>, <italic>π</italic><sub><italic>j</italic></sub>) describes the individual goal for reaching the target <italic>j</italic> starting from the current state <bold>x</bold><sub><italic>t</italic></sub> and following the policy <italic>π</italic><sub><italic>j</italic></sub> for time instances <italic>t</italic> = [<italic>t</italic><sub>1</sub>, ⋯, <italic>t</italic><sub><italic>T</italic><sub><italic>j</italic></sub></sub>]. <italic>T</italic><sub><italic>j</italic></sub> is the time-to-contact that target <italic>j</italic> and <italic>S</italic> is a matrix that picks out the hand and target positions from the state vector. The first term of the cost <italic>J</italic><sub><italic>j</italic></sub> is the accuracy cost that penalizes actions that drive the end-point of the reaching trajectory away from the target position <bold>p</bold><sub><italic>j</italic></sub>. The second term is the motor command cost that penalizes the effort required to reach the target. Both the accuracy cost and the motor command cost characterize the “action cost” <italic>V</italic><sub><italic>π</italic><sub><italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>) for implementing the policy <italic>π</italic><sub><italic>j</italic></sub> at the state <bold>x</bold><sub><italic>t</italic></sub>. Matrices <italic>Q</italic><sub><italic>T</italic><sub><italic>j</italic></sub></sub> and <italic>R</italic> define the precision- and the control- dependent costs, respectively (see <xref ref-type="supplementary-material" rid="pcbi.1004402.s002">S1 Text</xref> for more details).</p>
<p>When there is no uncertainty as to which policy to implement at a given time and state (e.g., actual target location is known), the <italic>ν</italic>-weighted cost function in <xref ref-type="disp-formula" rid="pcbi.1004402.e002">Eq (1)</xref> is equivalent to the classical optimal control problem. The best policy is given by the minimization of the cost function in <xref ref-type="disp-formula" rid="pcbi.1004402.e002">Eq (1)</xref> with <italic>ν</italic><sub><italic>j</italic></sub> = 1 for the actual target <italic>j</italic> and <italic>ν</italic><sub><italic>i</italic> ≠ <italic>j</italic></sub> = 0 for the rest of the non-targets. However, when there is more than one competing target in the field, there is uncertainty about which policy to follow at each time and state. In this case, the best policy is given by minimizing the expected cost function with expectation across the probability distribution of the indicator variable <italic>ν</italic>. This minimization can be approximated by the weighted average of the minimization of the expected individual cost functions, <xref ref-type="disp-formula" rid="pcbi.1004402.e003">Eq (2)</xref>.
<disp-formula id="pcbi.1004402.e003"><alternatives><graphic id="pcbi.1004402.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>ν</mml:mi></mml:msub> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mo form="prefix">arg</mml:mo> <mml:mspace width="3.33333pt"/><mml:munder><mml:mo form="prefix">min</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:munder> <mml:msub><mml:mi>J</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>ν</mml:mi></mml:msub> <mml:msubsup><mml:mi>π</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where ⟨.⟩<sub><italic>ν</italic></sub> is the expected value across the probability distribution of the indicator variable <italic>ν</italic>, and <inline-formula id="pcbi.1004402.e004"><alternatives><graphic id="pcbi.1004402.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e004"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the optimal policy to reach goal <italic>j</italic> starting from the current state <bold>x</bold><sub><italic>t</italic></sub>. For notational simplicity, we omit the * sign from the policy <italic>π</italic>, and from now on <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>) will indicate the <italic>optimal</italic> policy to achieve the goal <italic>j</italic> at state <bold>x</bold><sub><italic>t</italic></sub>.</p>
</sec>
<sec id="sec004">
<title>Computing policy desirability</title>
<p>The first problem is to compute the weighting factor ⟨<italic>ν</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>)⟩<sub><italic>ν</italic></sub>, which determines the contribution of each individual policy <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>) to the weighted average <italic>π</italic><sub><italic>mix</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>). Let’s consider for now that all the alternative targets have the same good values and hence the behavior is determined solely by the action costs. Recall that <italic>V</italic><sub><italic>π</italic><sub><italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>) represents the value function—i.e., cost that is expected to accumulate from the current state <bold>x</bold><sub><italic>t</italic></sub> to target <italic>j</italic> including the accuracy penalty at the end of the movement, under the policy <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>). This cost partially characterizes the probability of achieving at least <italic>V</italic><sub><italic>π</italic><sub><italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>) starting from state <bold>x</bold>(<italic>t</italic>) at time <italic>t</italic> and adopting the policy <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>) to reach the target <italic>j</italic>, <xref ref-type="disp-formula" rid="pcbi.1004402.e005">Eq (3)</xref>:
<disp-formula id="pcbi.1004402.e005"><alternatives><graphic id="pcbi.1004402.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>Δ</mml:mi> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>λ</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>λ</mml:mi></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>λ</italic> is the free “inverse temperature” parameter (<xref ref-type="supplementary-material" rid="pcbi.1004402.s004">S3 Text</xref>). This assumption can be taken as is, or justified from the path integral approach in [<xref ref-type="bibr" rid="pcbi.1004402.ref031">31</xref>] and [<xref ref-type="bibr" rid="pcbi.1004402.ref032">32</xref>]. The probability that the value function of the policy <italic>π</italic><sub><italic>j</italic></sub> at the current state <bold>x</bold><sub><italic>t</italic></sub> is lower than the rest of the alternatives <italic>P</italic>(<italic>V</italic><sub><italic>π</italic><sub><italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>) &lt; <italic>V</italic><sub><italic>π</italic><sub><italic>i</italic> ≠ <italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>)) can be approximated by the softmax-type equation in <xref ref-type="disp-formula" rid="pcbi.1004402.e006">Eq (4)</xref>, which gives an estimate of the probability of <italic>ν</italic><sub><italic>j</italic></sub> at <bold>x</bold><sub><italic>t</italic></sub>:
<disp-formula id="pcbi.1004402.e006"><alternatives><graphic id="pcbi.1004402.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ν</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>λ</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>λ</mml:mi></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mi>λ</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>λ</mml:mi></mml:mfrac> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>N</italic> is the total number of targets (i.e., and total number of policies) that are available at the current state.</p>
<p>Given that all targets have the same good values, the probability <italic>P</italic>(<italic>ν</italic><sub><italic>j</italic></sub>|<bold>x</bold><sub><italic>t</italic></sub>) characterizes the “relative desirability” <italic>rD</italic>(<italic>π<sub>j</sub></italic>(<bold>x</bold><sub><italic>t</italic></sub>)) of the policy <italic>π</italic><sub><italic>j</italic></sub> to pursue the goal <italic>j</italic> at a given state <bold>x</bold><sub><italic>t</italic></sub>. It reflects how desirable is to follow the policy <italic>π</italic><sub><italic>j</italic></sub> at that state with respect to the alternatives. Therefore, we can write that:
<disp-formula id="pcbi.1004402.e007"><alternatives><graphic id="pcbi.1004402.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>However, in a natural environment the alternative goals are usually attached with different values that we should take into account before making a decision. We integrate the good values into the relative desirability by computing the probability that pursing the goal <italic>j</italic> will result in overall higher pay-off <italic>r</italic><sub><italic>j</italic></sub> than the alternatives, <italic>P</italic>(<italic>r</italic><sub><italic>j</italic></sub> &gt; <italic>r</italic><sub><italic>i</italic> ≠ <italic>j</italic></sub>):
<disp-formula id="pcbi.1004402.e008"><alternatives><graphic id="pcbi.1004402.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:msub><mml:mi>π</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>To integrate the goods-related component on the relative desirability, we consider two cases:
<list list-type="order">
<list-item><p>The reward magnitude is fixed and equal for all targets, but the receipt of reward is probabilistic. In this case, the probability that the value of the target <italic>j</italic> is higher than the rest of the alternatives is given by the reward probability of this target <italic>P</italic>(<italic>target</italic> = <italic>j</italic>|<italic>x</italic><sub><italic>t</italic></sub>) = <italic>p</italic><sub><italic>j</italic></sub>:
<disp-formula id="pcbi.1004402.e009"><alternatives><graphic id="pcbi.1004402.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p></list-item>
<list-item><p>The target provides a reward with probability <italic>p</italic><sub><italic>j</italic></sub>, but the reward magnitude is not fixed. Instead, we assume that it follows a distribution <inline-formula id="pcbi.1004402.e010"><alternatives><graphic id="pcbi.1004402.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>δ</italic>(<italic>r</italic><sub><italic>j</italic></sub>) is the Delta dirac function, and <italic>μ</italic><sub><italic>j</italic></sub> and <italic>σ</italic><sub><italic>j</italic></sub> are the mean and the standard deviation of the reward attached to the target <italic>j</italic>. For simplicity reasons, we focus on the case with two potential targets, in which the goal is to achieve the highest pay-off after <italic>N</italic> trials. In this case, the goods-related component of the desirability function is <inline-formula id="pcbi.1004402.e011"><alternatives><graphic id="pcbi.1004402.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004402.e012"><alternatives><graphic id="pcbi.1004402.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e012"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> is the net reward attached to the target <italic>j</italic>—i.e., the average reward received from the target <italic>j</italic> across <italic>N</italic> trials.</p></list-item>
</list></p>
<p>To compute the probability <inline-formula id="pcbi.1004402.e013"><alternatives><graphic id="pcbi.1004402.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, we need the probability distribution of <inline-formula id="pcbi.1004402.e014"><alternatives><graphic id="pcbi.1004402.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Given <italic>p</italic>(<italic>n</italic>) = <italic>Binomial</italic>(<italic>n</italic>, <italic>p</italic><sub><italic>j</italic></sub>, <italic>N</italic>) is the probability of receiving <italic>n</italic>-times reward after <italic>N</italic> trials, the probability distribution of <inline-formula id="pcbi.1004402.e015"><alternatives><graphic id="pcbi.1004402.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is:
<disp-formula id="pcbi.1004402.e016"><alternatives><graphic id="pcbi.1004402.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
We can show that a mean based on <italic>n</italic> samples has a Normal distribution <inline-formula id="pcbi.1004402.e017"><alternatives><graphic id="pcbi.1004402.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mi>n</mml:mi> <mml:mi>N</mml:mi></mml:mfrac> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mi>n</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, the distribution of <inline-formula id="pcbi.1004402.e018"><alternatives><graphic id="pcbi.1004402.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> can be written as:
<disp-formula id="pcbi.1004402.e019"><alternatives><graphic id="pcbi.1004402.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e019"/><mml:math id="M19" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mfrac><mml:mi>n</mml:mi> <mml:mi>N</mml:mi></mml:mfrac> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mi>n</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>For a large number of trials <italic>N</italic> &gt; &gt; 0, <italic>p</italic>(<italic>n</italic>) is concentrated around <italic>n</italic> = <italic>p</italic><sub><italic>j</italic></sub> <italic>N</italic> and <inline-formula id="pcbi.1004402.e020"><alternatives><graphic id="pcbi.1004402.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e020"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. To compute <inline-formula id="pcbi.1004402.e021"><alternatives><graphic id="pcbi.1004402.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we define a new random variable, <inline-formula id="pcbi.1004402.e022"><alternatives><graphic id="pcbi.1004402.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e022"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which has Normal distribution with mean <italic>p</italic><sub>1</sub> <italic>μ</italic><sub>1</sub> − <italic>p</italic><sub>2</sub> <italic>μ</italic><sub>2</sub> and variance <inline-formula id="pcbi.1004402.e023"><alternatives><graphic id="pcbi.1004402.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e023"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. We can show that <inline-formula id="pcbi.1004402.e024"><alternatives><graphic id="pcbi.1004402.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e024"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is given as:
<disp-formula id="pcbi.1004402.e025"><alternatives><graphic id="pcbi.1004402.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:mi>f</mml:mi> <mml:mi>c</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mo>(</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>erfc</italic> is the complementary error function. Using that <italic>erfc</italic>(<italic>x</italic>) = 1 − <italic>erf</italic>(<italic>x</italic>), where <italic>erf</italic> is the error function, we can write that:
<disp-formula id="pcbi.1004402.e026"><alternatives><graphic id="pcbi.1004402.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e026"/><mml:math id="M26" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mo>(</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>C</mml:mi> <mml:mi>u</mml:mi> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mi>o</mml:mi> <mml:mi>r</mml:mi> <mml:mi>m</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
This result is consistent with the common practice of modeling choice probabilities as a softmax function between options. For example, the cumulative normal distribution can be approximated by the following logistic function [<xref ref-type="bibr" rid="pcbi.1004402.ref033">33</xref>]:
<disp-formula id="pcbi.1004402.e027"><alternatives><graphic id="pcbi.1004402.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>&gt;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="italic">l</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msqrt><mml:mfrac><mml:mi>S</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1004402.e028"><alternatives><graphic id="pcbi.1004402.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e028"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec005">
<title>Target probability encodes the order of policies in sequential movement tasks</title>
<p>In the preceding sections we developed a theory for the case that targets are presented simultaneously and the expected reward depends only on successfully reaching the target—i.e. reward availability is not state- and time- dependent. However, decisions are not limited only to this case but often involve goals with time-dependent values. In this section, we extend our approach to model visuomotor tasks with sequential goals, focusing on a pentagon copying task.</p>
<p>The theory precedes as before, with a set of control schemes that instantiate policies <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>)—where (<italic>j</italic> = 1, ⋯ 5)—that drive the hand from the current state to the vertex <italic>j</italic>. However, to draw the shape in a proper spatial order, we cannot use the same policy mixing as with simultaneously presented goals. Instead, we have to take into account the sequential constraints that induce a temporal order across the vertices. We can conceive the vertices as potential goals that provide the same amount of reward, but with different probabilities (i.e., similar to scenario 2 in the reaching task) with the exception that we design the target probability to be time- and state- dependent, so that it encodes the order of policies for copying the pentagon. The target probability <italic>P</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>) describes the probability that the vertex <italic>j</italic> is the current goal of the task at the state <bold>x</bold><sub><italic>t</italic></sub> after departing from the vertex <italic>j</italic> − 1, or in other words, it describes the probability that we copy the segment defined by the two successive vertices <italic>j</italic> − 1 and <italic>j</italic>.</p>
<p>We define an indicator function <italic>e</italic><sub><italic>j</italic></sub> that is 1 if we arrive at vertex <italic>j</italic> and 0 otherwise.
<disp-formula id="pcbi.1004402.e029"><alternatives><graphic id="pcbi.1004402.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>e</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi>e</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>e</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>e</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
<disp-formula id="pcbi.1004402.e030"><alternatives><graphic id="pcbi.1004402.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where <inline-formula id="pcbi.1004402.e031"><alternatives><graphic id="pcbi.1004402.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e031"/><mml:math id="M31" display="inline" overflow="scroll"><mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the time to arrive at vertex <italic>j</italic> -i.e, time to complete drawing the segment defined by the vertices <italic>j</italic> − 1 and <italic>j</italic>.</p>
<p>Let’s assume that we are copying the shape counterclockwise starting from the purple vertex (see right inset in <xref ref-type="fig" rid="pcbi.1004402.g001">Fig 1A</xref>), at the initial state at time <italic>t</italic> = 0. The probability distribution of time to arrive at vertex <italic>j</italic>, <inline-formula id="pcbi.1004402.e032"><alternatives><graphic id="pcbi.1004402.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e032"/><mml:math id="M32" display="inline" overflow="scroll"><mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, is given by <xref ref-type="disp-formula" rid="pcbi.1004402.e033">Eq (15)</xref>.
<disp-formula id="pcbi.1004402.e033"><alternatives><graphic id="pcbi.1004402.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e033"/><mml:math id="M33" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>j</mml:mi></mml:munderover> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where <inline-formula id="pcbi.1004402.e034"><alternatives><graphic id="pcbi.1004402.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e034"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the probability distribution of time to arrive at vertex <italic>k</italic> given that we started from vertex <italic>k</italic> − 1. We generated 100 trajectories between two successive vertices and found that <inline-formula id="pcbi.1004402.e035"><alternatives><graphic id="pcbi.1004402.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e035"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be approximated by a Normal distribution <inline-formula id="pcbi.1004402.e036"><alternatives><graphic id="pcbi.1004402.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e036"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Using <xref ref-type="disp-formula" rid="pcbi.1004402.e033">Eq (15)</xref>, we show that <inline-formula id="pcbi.1004402.e037"><alternatives><graphic id="pcbi.1004402.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e037"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is also Gaussian distribution, but with <italic>j</italic> times the mean and variance—<inline-formula id="pcbi.1004402.e038"><alternatives><graphic id="pcbi.1004402.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e038"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:msub><mml:mi>μ</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:msubsup><mml:mi>σ</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as shown in <xref ref-type="fig" rid="pcbi.1004402.g001">Fig 1A</xref>. Considering that, we estimate that target probability <italic>P</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>), <xref ref-type="fig" rid="pcbi.1004402.g001">Fig 1B</xref>. Each time that we arrive at a vertex, we condition on completion, and <italic>P</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>) is re-evaluated for the next vertices.</p>
<fig id="pcbi.1004402.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Encoding the order of policies in sequential movements.</title>
<p><bold>A</bold>: Probability distribution of time to arrive at vertex <italic>j</italic> starting from the original state at time <italic>t</italic> = 0 and visiting all the precedent vertices. Each color codes the segments and the vertices of the pentagon as shown in the right inset. The pentagon is copied counterclockwise (as indicated by the arrow) starting from the purple vertex at <italic>t</italic> = 0. The gray trajectories illustrate examples from the 100 reaches generated to estimate the probability distribution of time to arrive at vertex <italic>k</italic> given that we started from vertex <italic>k</italic> − 1, <inline-formula id="pcbi.1004402.e039"><alternatives><graphic id="pcbi.1004402.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e039"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi></mml:mrow> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <bold>B</bold>: Probability distribution <italic>P</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>), which describes the probability to copy the segment defined by the two successive vertices <italic>j</italic> − 1 and <italic>j</italic> at state <bold>x</bold><sub><italic>t</italic></sub>. This probability distribution is estimated at time <italic>t</italic> = 0 and when arriving at the next vertex, we condition on completion, and <italic>P</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>) is re-evaluated for the next vertices.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g001"/>
</fig>
</sec>
</sec>
<sec id="sec006" sec-type="results">
<title>Results</title>
<sec id="sec007">
<title>Model architecture</title>
<p>The basic architecture of the model is a set of control schemes, associated with individual goals, <xref ref-type="fig" rid="pcbi.1004402.g002">Fig 2</xref>. Each scheme is a stochastic optimal control system that generates both a goal-specific <italic>policy</italic> <italic>π</italic><sub><italic>j</italic></sub>, which is a mapping between states and best-actions, and an action-cost function that computes the expected control costs to achieve the goal <italic>j</italic> from any state (see <xref ref-type="supplementary-material" rid="pcbi.1004402.s002">S1 Text</xref> for more details). It is important to note that a policy is not particular a sequence of actions—rather it is a controller that tells you what action-plan <bold>u</bold><sub><italic>j</italic></sub> (i.e., sequence of actions <italic>u</italic><sub><italic>i</italic></sub>) to take from a state <bold>x</bold><sub><italic>t</italic></sub> to the goal (i.e., <italic>π</italic><sub><italic>j</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>) = <bold>u</bold><sub><italic>j</italic></sub> = [<italic>u</italic><sub><italic>t</italic></sub>, <italic>u</italic><sub><italic>t</italic>+1</sub>, ⋯ <italic>u</italic><sub><italic>t</italic><sub><italic>end</italic></sub></sub>]). In addition, the action-cost function is a map <italic>cost</italic>(<italic>j</italic>) = <italic>V</italic><sub><italic>π</italic><sub><italic>j</italic></sub></sub>(<bold>x</bold><sub><italic>t</italic></sub>) that gives the expected action cost from each state to the goal.</p>
<fig id="pcbi.1004402.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The architectural organization of the theory.</title>
<p>It consists of multiple stochastic optimal control schemes where each of them is attached to a particular goal presented currently in the field. We illustrate the architecture of the theory using the hypothetical scenario of the soccer game, in which the player who is possessing the ball is presented with 3 alternative options—i.e., 3 teammates—located at different distances from the current state <bold>x</bold><sub><italic>t</italic></sub>. In such a situation, the control schemes related to these options are triggered and generate 3 action plans (<bold>u</bold><sub>1</sub> = <italic>π</italic><sub>1</sub>(<bold>x</bold><sub><italic>t</italic></sub>), <bold>u</bold><sub>2</sub> = <italic>π</italic><sub>2</sub>(<bold>x</bold><sub><italic>t</italic></sub>) and <bold>u</bold><sub>3</sub> = <italic>π</italic><sub>3</sub>(<bold>x</bold><sub><italic>t</italic></sub>)) to pursue each of the individual options. At each time <italic>t</italic>, desirabilities of the each policy in terms of action cost and good value are computed separately, then combined into an overall desirability. The action cost of each policy is the cost-to-go of the remaining actions that would occur if the policy were followed from the current state <bold>x</bold><sub><italic>t</italic></sub> to the target. These action costs are converted into a relative desirability that characterizes the probability that implementing this policy will have the lowest cost relative to the alternative policies. Similarly, the good value attached to each policy is evaluated in the goods-space and is converted into a relative desirability that characterizes the probability that implementing that policy (i.e., select the goal <italic>i</italic>) will result in highest reward compare to the alternative options, from the current state <bold>x</bold><sub><italic>t</italic></sub>. These two desirabilities are combined to give what we call “relative-desirability” value, which reflects the degree to which the individual policy <italic>π</italic><sub><italic>i</italic></sub> is desirable to follow, at the given time and state, with respect to the other available policies. The overall policy that the player follows is a time-varying weighted mixture of the individual policies using the desirability value as weighted factor. Because relative desirability is time- and state- dependent, the weighted mixture of policies produces a range of behavior from “winner-take-all” (i.e., pass the ball) to “spatial averaging” (i.e., keep the ball and delay your decision).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g002"/>
</fig>
<p>Let’s reconsider the soccer game scenario and assume a situation in which the player has 3 alternative options to pass the ball (i.e., 3 unmarked teammates) at different distances from the current state <bold>x</bold><sub><italic>t</italic></sub>. In such a situation, the control schemes related to these options become active and suggest 3 action-plans (<bold>u</bold><sub>1</sub> = <italic>π</italic><sub>1</sub>(<bold>x</bold><sub><italic>t</italic></sub>), <bold>u</bold><sub>2</sub> = <italic>π</italic><sub>2</sub>(<bold>x</bold><sub><italic>t</italic></sub>) and <bold>u</bold><sub>3</sub> = <italic>π</italic><sub>3</sub>(<bold>x</bold><sub><italic>t</italic></sub>)) to pursue the individual options. Each of the alternative action-plans is assigned with value related to the option itself (e.g., teammates’ performance, distance of the teammates to the goalie) and with cost required to implement this plan (e.g., effort). For instance, it requires less effort to pass the ball to the nearby teammate No.1, but the distant teammate No.2 is considered a better option, because he/she is closer to the opponent goalie. While the game progresses, the cost of the action-plans and the estimates of the values of the alternative options change continuously. To make a correct choice, the player should integrate the incoming information online and while acting. However, the value of the options and the cost of the actions have different “currencies”, making the value integration a challenging procedure. The proposed theory uses a probabilistic approach to dynamically integrate value information from disparate sources into a common currency that we call the <italic>relative desirability</italic> function <italic>w</italic>(<bold>x</bold><sub><italic>t</italic></sub>). While common currency usually refers to integration in value space, relative desirability combines in the space of policy weights. Using relative desirability, integration of disparate values is accomplished by combining each different type of value in its own space, then computing the relative impact of that value on the set of available policies.</p>
</sec>
<sec id="sec008">
<title>Relative desirability function</title>
<p>The crux of our approach is that to make a decision, we only need to know what is the current best option and whether we can achieve it. This changes the complex problem of converting action costs to good values into a simple problem of maximizing the chances of getting the best of the alternatives that are currently available. To integrate value information with different “currencies”, we compute the probability of achieving the most rewarding option from a given time and state. This probability has both action-related and goods-related components with an intuitive interpretation: the probability of getting the highest reward with the least effort. We call this value relative desirability (rD) because it quantifies the attractiveness of the policy <italic>π</italic> for each goal <italic>i</italic> from state <bold>x</bold><sub><italic>t</italic></sub> relative to the alternative options:
<disp-formula id="pcbi.1004402.e040"><alternatives><graphic id="pcbi.1004402.e040g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e040"/><mml:math id="M40" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
The first term is the “action-related” component of the relative desirability and describes the probability that pursuing the goal <italic>i</italic> has lowest cost relative to alternatives, at the given state <bold>x</bold><sub><italic>t</italic></sub>. The second term refers to the “goods-related” component and describes the probability that selecting the goal <italic>i</italic> will result in highest reward compared to the alternatives, at the current state <bold>x</bold><sub><italic>t</italic></sub>. Note that the relative desirability values of the alternative options are normalized so that they all sum to 1.</p>
<p>To illustrate the relative desirability function, consider a reaching task with two potential targets presented in left (target <italic>L</italic>) and right (target <italic>R</italic>) visual fields (gray circles in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3A</xref>). For any state <bold>x</bold><sub><italic>t</italic></sub> where the policy to the right target is more “desirable” than to the left target, we have the following inequality:
<disp-formula id="pcbi.1004402.e041"><alternatives><graphic id="pcbi.1004402.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e041"/><mml:math id="M41" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula></p>
<fig id="pcbi.1004402.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Relative desirability function in reaching movements with multiple potential targets.</title>
<p><bold>A</bold>: A method followed to visualize the relative desirability function of two competing reaching policies (see <xref ref-type="sec" rid="sec006">results</xref> section for more details). <bold>B</bold>: Heat map of the log-transformed action cost for reaching the left target (gray circle) starting from different states. Red and blue regions correspond to high and low cost states, respectively. The black arrows describe the average hand velocity at a given state. <bold>C</bold>: Similar to panel <italic>B</italic> but for the right target. <bold>D</bold>: Heat map of the relative desirability function at different states to reach to the right target, when both targets provide the same amount of reward with equal probability. <bold>E</bold>: Similar to <italic>D</italic>, but for a scenario in which the right target provides the same amount of reward with the left one, but with 4 times higher probability. <bold>F</bold>: Similar to <italic>D</italic>, but for a scenario in which the mean reward provided by right target is 4 times higher than then one provided by the left target.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g003"/>
</fig>
<p>This inequality predicts two extreme reaching behaviors—a direct movement to the target <italic>R</italic> (i.e., winner-take-all) when <italic>rD</italic>(<italic>π<sub>R</sub></italic>(<bold>x</bold><sub><italic>t</italic></sub>)) &gt; &gt; <italic>rD</italic> (<italic>π<sub>L</sub></italic>(<bold>x</bold><sub><italic>t</italic></sub>)), and a spatial averaging movement towards an intermediate position between the two targets when <italic>rD</italic>(<italic>π<sub>R</sub></italic>(<bold>x</bold><sub><italic>t</italic></sub>)) ≈ <italic>rD</italic>(<italic>π<sub>L</sub></italic>(<bold>x</bold><sub><italic>t</italic></sub>)). Rearranging this equation, using <italic>P</italic>(<italic>reward</italic>(<italic>L</italic>) &gt; <italic>reward</italic>(<italic>R</italic>)) = 1 − <italic>P</italic>(<italic>reward</italic>(<italic>R</italic>) &gt; <italic>reward</italic>(<italic>L</italic>)) we see that the relative desirability to pursue the target <italic>R</italic> increases with the odds that target <italic>L</italic> has more reward and lower cost:
<disp-formula id="pcbi.1004402.e042"><alternatives><graphic id="pcbi.1004402.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e042"/><mml:math id="M42" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>D</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>→</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&lt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>To gain more insight on how action cost and good value influence the reaching behavior, we visualize the relative desirability to reach the right target in 3 scenarios (the desirability related to the left target is a mirror image of the right one):</p>
<list list-type="bullet">
<list-item><p><bold>Scenario 1: Both targets provide the same reward magnitude with equal probability</bold>.</p></list-item>
</list>
<p specific-use="continuation">For this case,</p>
<p specific-use="continuation"><disp-formula id="pcbi.1004402.e043"><alternatives><graphic id="pcbi.1004402.e043g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e043"/><mml:math id="M43" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p specific-use="continuation">which means target R is more desirable when</p>
<p specific-use="continuation"><disp-formula id="pcbi.1004402.e044"><alternatives><graphic id="pcbi.1004402.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e044"/><mml:math id="M44" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p specific-use="continuation">Now the action cost (and hence relative desirability) is a function of the hand-state, making them difficult to illustrate. For a point-mass hand in 2D, the hand state is captured by the 4D position-velocity. To visualize this 4D relative desirability map in two dimensions, we “slice” through the 4D position-velocity space by making velocity a function of position in the following way. All trajectories are constrained to start at position (0, 0) with zero velocity. We then allow the trajectory to arrive at one of a set of spatial positions (100 total) around a circle of radius 85% of the distance between the start point and the midpoint (black star) of the two potential targets. For each of these points, we constrain the hand velocity to have direction (red arrows in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3A</xref>) in line with the start point (gray square in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3A</xref>) and the hand position on the circle (black dots in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3A</xref>). We set the magnitude of the velocities to match the speed of the optimal reaching movement at 85% of completion (blue trace for left target in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3A</xref>). From each position-velocity pair on the circle, we sample 100 optimal movements to each of the two targets (solid and discontinuous traces are illustrated examples for reaching the left and the right target, respectively). We discretize the space and compute the action cost to reach the targets from each state—the expected cost from each state to the goal following the policy for that goal, including an accuracy penalty at the end of the movement. <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3B and 3C</xref> depict these action costs, where blue indicates low cost and red indicates high cost, respectively. <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3D</xref> illustrates the action costs converted into relative desirability values to reach the right target (indicted by a solid gray circle), where blue and red regions correspond to states with low and high desirability, respectively. Notice desirability increases rapidly as the reach approaches a target, resulting in winner-take-all selection of an action-plan once moving definitely towards a target. However, when the hand position is about the same distance from both targets (greenish areas) there is no dominant policy, leading to strong competition and spatial averaging of the competing policies.</p>
<list list-type="bullet">
<list-item><p><bold>Scenario 2: Both targets provide the same reward magnitude but with different probabilities</bold>.</p></list-item>
</list>
<p specific-use="continuation">In this case, desirability also depends on the probability of reward. Since both targets provide the same amount of reward, but with different probabilities, the goods-related term simplifies:</p>
<p specific-use="continuation"><disp-formula id="pcbi.1004402.e045"><alternatives><graphic id="pcbi.1004402.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e045"/><mml:math id="M45" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>g</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p specific-use="continuation">where <italic>p</italic><sub><italic>R</italic></sub> describes the probability of earning reward by pursuing the right target. Hence, the target <italic>R</italic> is more desirable in a state <bold>x</bold><sub><italic>t</italic></sub> when</p>
<p specific-use="continuation"><disp-formula id="pcbi.1004402.e046"><alternatives><graphic id="pcbi.1004402.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p specific-use="continuation">The relative desirability function for the right target is illustrated in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3E</xref>, when <italic>p</italic><sub><italic>R</italic></sub> is 4 times higher than the probability of the left target (<italic>p</italic><sub><italic>R</italic></sub> = 0.8, <italic>p</italic><sub><italic>L</italic></sub> = 0.2). The right target is more desirable for most states (reddish areas), unless the hand position is already nearby the left target (blue areas), predicting frequent winner-take-all behavior -i.e., direct reaches to the right target.</p>
<list list-type="bullet">
<list-item><p><bold>Scenario 3: Probability and reward magnitude differ between the two targets</bold>.</p></list-item>
</list>
<p specific-use="continuation">More generally, the reward magnitude attached to each target is not fixed, but both the reward magnitude and reward probability vary. We assume that target <italic>j</italic> provides a reward with probability <italic>p</italic><sub><italic>j</italic></sub>, and that the magnitude follows a Normal distribution with mean <italic>μ</italic><sub><italic>j</italic></sub> and standard deviation <italic>σ</italic><sub><italic>j</italic></sub>. Hence, the distribution of the rewards attached to the left target (L) and right target (R) is a mixture of distributions:</p>
<p specific-use="continuation"><disp-formula id="pcbi.1004402.e047"><alternatives><graphic id="pcbi.1004402.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>L</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
<disp-formula id="pcbi.1004402.e048"><alternatives><graphic id="pcbi.1004402.e048g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e048"/><mml:math id="M48" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>R</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula></p>
<p specific-use="continuation">where <italic>δ</italic> is the Dirac function.</p>
<p>In visuomotor decision tasks, the ultimate goal is usually to achieve the highest reward after <italic>N</italic> trials. In this case, the probability that the right target provides overall higher reward than the left one over <italic>N</italic> trials can be approximated by a logistic function <italic>l</italic> with argument <italic>p</italic><sub><italic>R</italic></sub> <italic>μ</italic><sub><italic>R</italic></sub> − <italic>p</italic><sub><italic>L</italic></sub> <italic>μ</italic><sub><italic>L</italic></sub> (see <xref ref-type="sec" rid="sec002">Materials and Methods</xref> section for more details). When the reward values are precisely encoded, this simplifies to:
<disp-formula id="pcbi.1004402.e049"><alternatives><graphic id="pcbi.1004402.e049g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e049"/><mml:math id="M49" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>≈</mml:mo> <mml:mi mathvariant="italic">l</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
Hence, pursuing the target <italic>R</italic> is more desirable in a state <bold>x</bold><sub><italic>t</italic></sub> when
<disp-formula id="pcbi.1004402.e050"><alternatives><graphic id="pcbi.1004402.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e050"/><mml:math id="M50" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>&lt;</mml:mo> <mml:mi mathvariant="italic">l</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula></p>
<p><xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3F</xref> illustrates the heat map of the relative desirability values at different states of the policy to reach the right target (solid gray circle), when both targets have the same reward probability <italic>p</italic><sub><italic>L</italic></sub> = <italic>p</italic><sub><italic>R</italic></sub> = 0.5, but <italic>μ</italic><sub><italic>R</italic></sub> = 4<italic>μ</italic><sub><italic>L</italic></sub>, i.e. <italic>reward</italic>(<italic>R</italic>) ∼ 0.5<italic>δ</italic>(<italic>reward</italic>(<italic>R</italic>)) + 0.5<italic>N</italic>(2, 1) and <italic>reward</italic>(<italic>L</italic>) ∼ 0.5<italic>δ</italic>(<italic>reward</italic>(<italic>L</italic>)) + 0.5<italic>N</italic>(0.5, 1). Similar to the previous scenario, reaching behavior is dominated mostly by the goods-related component and consequently reaching the right target is more desirable than reaching the left target for most states (reddish areas), leading frequently to “winner-take-all” behavior.</p>
</sec>
<sec id="sec009">
<title>Desirability predicts reaching behavior in decision tasks with multiple potential goals</title>
<p>Several studies have shown that reaching decisions made while acting follow a “delay-and-mix” policy, with the mixing affected by target configuration and task properties [<xref ref-type="bibr" rid="pcbi.1004402.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref023">23</xref>]. Subjects were trained to perform rapid reaching movements either to a single target or to two equidistant, equiprobable targets (i.e., actual target location is unknown prior to movement onset in two-target trials). Black and green traces in <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4A</xref> show single-target trials, characterized by trajectories straight to the target location. Red and blue traces show the delay-and-mix policy for reaches in two-target trials—an initial reaching movement towards an intermediate position between the two stimuli followed by corrective movements after the target was revealed. Relative desirability predicts this behavior (<xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3D</xref>), for equiprobable reward (<italic>scenario 1</italic>). In this case, the relative desirability is determined solely by the distance from the current hand position to the targets. Since targets are equidistant, the reaching costs are comparable and hence the two competing policies have about the same desirability values for states between the origin and the target locations (see the greenish areas in <xref ref-type="fig" rid="pcbi.1004402.g003">Fig 3D</xref>). Hence, the weighted mixture of policies produces spatial averaging trajectories (red and blue traces in <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4E</xref>). Note that each controller <italic>i</italic>, which is associated with the potential target <italic>i</italic>, generates an optimal policy <italic>π</italic><sub><italic>i</italic></sub>(<bold>x</bold><sub><italic>t</italic></sub>) to reach that target starting from the current state <bold>x</bold><sub><italic>t</italic></sub>. On single-target trials, the actual location of the target is known prior to movement onset and hence the desirability is 1 for the cued target. Consequently the simulated reaches are made directly to the actual target location (green and black traces in <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4E</xref>).</p>
<fig id="pcbi.1004402.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Rapid reaching movements in tasks with competing targets.</title>
<p>Top row illustrates experimental results in rapid reaching tasks with multiple potential targets [<xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref023">23</xref>] (images are reproduced with permission of the authors). When the target position is known prior to movement onset, reaches are made directly to that target (black and green traces in <bold>A</bold>), otherwise, reaches aim to an intermediate location, before correcting in-flight to the cued target (red and blue traces in <bold>A</bold>). The competition between the two reaching policies that results in spatial averaging movements, is biased by the spatial distribution of the targets (<bold>B</bold>), by recent trial history (<bold>C</bold>) and the number of targets presented in each visual field (<bold>D</bold>). The bottom row (<bold>E-H</bold>) illustrates the simulated reaching movements generated in tasks with multiple potential targets. Each bottom panel corresponds to the reaching condition described on the top panels.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g004"/>
</fig>
<p>The competition between policies is also modulated by spatial location of the targets [<xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>]. When one of the targets was shifted, reaching trajectories shifted towards a new intermediate position <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4B</xref>. This behavior is also captured by our framework—perturbing the spatial distribution of the potential targets, the weighted policy is also perturbed in the same direction <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4F</xref>. This finding is somehow counterintuitive, since the targets are no longer equidistant from the origin and it would be expected that the simulated reach responses would be biased towards the closer target. However, the magnitude of the perturbation is too small to change the action costs enough to significantly bias the competition. More significant are the action costs required to change direction once the target is revealed, and these costs are symmetric between targets.</p>
<p>Reaching behavior is also influenced by goods-related decision variables, like target probability. When subjects were informed that the potential targets were not equiprobable, the reach responses were biased towards the target with the highest reward probability [<xref ref-type="bibr" rid="pcbi.1004402.ref011">11</xref>]. This finding is consistent with relative desirability predictions in <italic>scenario 2</italic>—targets with higher reward probabilities are more desirable than the alternative options for most of the states. Reward probabilities learned via feedback can also be modeled in the same framework. Instead of informing subjects directly about target probabilities, the experimenters generated a block of trials in which one of the targets was consecutively cued for action [<xref ref-type="bibr" rid="pcbi.1004402.ref022">22</xref>]. Subjects showed a bias towards the cued target that accumulated across trials (<xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4C</xref>) consistent with probability learning. We modeled this paradigm by updating the reward probability using a simple reinforcement learning algorithm (see <xref ref-type="supplementary-material" rid="pcbi.1004402.s005">S4 Text</xref> for more details). In line with the experimental findings, the simulated reach responses were increasingly biased to the target location that was consecutively cued for action on the past trials, <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4G</xref>.</p>
<p>Unlike most value computation methods, our approach can make strong predictions for what happens when additional targets are introduced. A previous study showed that by varying the number of potential targets, reaching movements were biased towards the side of space that contains more targets [<xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>], <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4D</xref>. Our approach predicts this effect due to normalization across policies. When there are more targets in one hemifield than the other, there are more alternative reaching policies towards this space biasing the competition to that side, <xref ref-type="fig" rid="pcbi.1004402.g004">Fig 4H</xref>. Overall, these findings show that weighting individual policies with the relative desirability values can explain many aspects of human behavior in reaching decisions with competing goals.</p>
</sec>
<sec id="sec010">
<title>Desirability predicts errors in oculomotor decision tasks</title>
<p>A good theory should predict not only successful decisions, but also decisions that result in errors in behavior. Experimental studies provide fairly clear evidence that humans and animals follow a “delay-and-mix” behavior even when it appears pathological. A typical example is the “global effect” paradigm that occurs frequently in oculomotor decisions with competing goals. When two equally rewarded targets are placed in close proximity—less than 30° angular distance—and the subject is free to choose between them, saccade trajectories usually end on intermediate locations between targets [<xref ref-type="bibr" rid="pcbi.1004402.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref035">35</xref>]. To test whether our theory can capture this phenomenon, we modeled the saccadic movements to individual targets using optimal control theory (see <xref ref-type="supplementary-material" rid="pcbi.1004402.s003">S2 Text</xref> for more details) and ran a series of simulated oculomotor decision tasks. Consistent with the experimental findings, the simulated eye movements land primarily in a position between the two targets for 30° target separation (gray traces in <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5A</xref>), whereas they aim directly to one of them for 90° target separation (black traces in <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5A</xref>).</p>
<fig id="pcbi.1004402.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Saccadic movements in tasks with competing targets.</title>
<p><bold>A</bold>: Simulated saccadic movements for pair of targets with 30° (gray traces) and 90° (black traces) target separation. <bold>B</bold>: A method followed to visualize the relative desirability function of two competing saccadic policies (see <xref ref-type="sec" rid="sec006">results</xref> section for more details). <bold>C</bold>: Heat map of the relative desirability function at different states to saccade to the left target, at a 30° target separation. Red and blue regions corresponds to high and low desirability states, respectively. Black traces correspond to averaged trajectories in single-target trials. Notice the strong competition between the two saccadic policies (greenish areas). <bold>D</bold>: Similar to panel <italic>C</italic>, but for 90° target separation. In this case, targets are located in areas with no competition between the two policies (red and blue regions). <bold>E</bold>: Examples of saccadic movements (left column) with the corresponding time course of the relative desirability of the two policies (right column). The first two rows illustrate characteristic examples from 30° target separation, in which competition results primarily in saccade averaging (top panel) and less frequently in correct movements (middle panel). The bottom row shows a characteristic example from 90° target separation, in which the competition is resolved almost immediately after saccadic onset, producing almost no errors. <bold>F</bold>: Percentage of simulated averaging saccades for different degrees of target separation (red line)—green, blue and cyan lines describe the percentage of averaging saccades performed by 3 monkeys [<xref ref-type="bibr" rid="pcbi.1004402.ref024">24</xref>].</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g005"/>
</fig>
<p>We visualize the relative desirability of the left target (i.e., desirability to saccade to the left target) at different states, both for 30° and 90° target separation. We followed a similar procedure as for the reaching case but used an ellipse. Particularly, individual saccadic movements are constrained to start at (0, 0) and arrive at one of the sequence (100 total) of spatial positions with <italic>zero velocity</italic> around an ellipse with center intermediate between the two targets (black star), with minor axis twice the distance between the origin and the center of the ellipse, and major axis double the length of the minor axis (<xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5B</xref>). For each position on the ellipse, we generate 100 optimal saccadic movements and evaluate the relative desirability to saccade to the left target (solid gray circle) at different states. <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5C</xref> depicts the heat-map of the relative desirability for 30° target separation. The black traces represent the average trajectories for direct saccadic movements, when only a single target is presented. Notice that regions defined by the starting position (0, 0) (gray square) and the locations of the targets is characterized by states with strong competition between the two saccadic policies (greenish areas). Consequently the weighted mixture of policies results frequently in spatial averaging movements that land between the two targets. On the other hand, when the targets are placed in distance, such as the 90° case presented in <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5D</xref>, the targets are located in areas in which one of the policies clearly dominates the other, and therefore the competition is easily resolved.</p>
<p><xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5E</xref> shows examples of saccadic movements (left column) with the corresponding time course of relative desirability values to saccade to the left and the right target (right column). The first two rows show trials from the 30° target separation task, where the competition between the two saccadic policies results in global effect (upper panels) and saccadic movement to the right target (middle panels). The two policies have about the same relative desirability values at different states resulting in a strong competition. Because saccades are ballistic with little opportunity for correction during the trajectory, competition produces the global effect paradigm. However, if the competition is resolved shortly after saccade onset, the trajectory ends up to one of the targets. On the other hand, when the two targets are placed in distance, the competition is easily resolved and the mixture of the policies generates direct movements to one of the targets (lower panel).</p>
<p>These findings suggest that the competition between alternative policies depends on the geometrical configuration of the targets. We quantified the effects of the targets’ spatial distribution to eye movements by computing the percentage of averaging saccades against the target separation. The results presented in <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5F</xref> (red line) indicate that averaging saccades were more frequent for 30° target separation and fell off gradually as the distance between the targets increases (see the <xref ref-type="sec" rid="sec012">Discussion</xref> section for more details on how competition leads to errors in behavior). This finding is also in line with experimental results from an oculomotor decision study with express saccadic movements in non-human primates (green, blue and cyan lines in <xref ref-type="fig" rid="pcbi.1004402.g005">Fig 5F</xref> describe the performance of 3 monkeys [<xref ref-type="bibr" rid="pcbi.1004402.ref024">24</xref>]).</p>
</sec>
<sec id="sec011">
<title>Desirability explains the competition in sequential decision tasks</title>
<p>In previous sections we considered decisions between multiple competing goals. However, ecological decisions are not limited only to simultaneous goals, but often involve choices between goals with time-dependent values. Time-dependent values mean that some of the goals may spoil or have limited period of worth such that they must be reached within a time window or temporal order. A characteristic example is sequential decision tasks that require a chain of decisions between successive goals. Substantial evidence suggests that the production of sequential movements involves concurrent representation of individual policies associated with the sequential goals that are internally activated before the order is imposed upon them [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref039">39</xref>]. To model these tasks using our approach, the critical issue is how to mix the individual control policies. State-dependent policy mixing as described previously will dramatically fail, since the desirability values do not take into account the temporal constraints. However, it is relatively easy to incorporate the sequential constraints and time-dependence into the goods-related component of the relative desirability function. We illustrate how sequential decision tasks can be modeled using a simulated copying task used in neurophysiological [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref040">40</xref>] and brain imaging studies [<xref ref-type="bibr" rid="pcbi.1004402.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref042">42</xref>].</p>
<p>Copying geometrical shapes can be conceived as sequential decisions with goal-directed movements from one vertex (i.e., target) of the shape to another in a proper spatial order. To model this, each controller <italic>j</italic> provides a policy <italic>π</italic><sub><italic>j</italic></sub> to reach the vertex <italic>j</italic> starting from the current state. We encode the order of the policies using a time-dependent target reward probability <italic>p</italic>(<italic>vertex</italic> = <italic>j</italic>|<bold>x</bold><sub><italic>t</italic></sub>) that describes the probability that <italic>vertex</italic> <italic>j</italic> is the current goal of the task at state <bold>x</bold><sub><italic>t</italic></sub> (see <xref ref-type="sec" rid="sec002">Materials and Methods</xref> section for more details). In fact, it describes the probability to copy the segment defined by the successive vertices <italic>j</italic> − 1 and <italic>j</italic> at a given state <bold>x</bold><sub><italic>t</italic></sub>.</p>
<p>We evaluated the theory in a simulated copying task with 3 geometrical shapes (i.e., equilateral triangle, square and pentagon). Examples of movement trajectories from the pentagon task is shown in <xref ref-type="fig" rid="pcbi.1004402.g006">Fig 6A</xref>. <xref ref-type="fig" rid="pcbi.1004402.g006">Fig 6B</xref> depicts the time course of the relative desirability values of the segments from a successful trial. The desirability of each segment peaks once the model starts copying that segment and falls down gradually, whereas the desirability of the following segment starts rising while copying the current segment. Notice that the competition is stronger for middle segments than the first or the last segment in the sequence. Consequently, errors, such as rounding of corners and transposition errors (i.e., copying other segments than the current one in the sequence) are more frequent when copying the middle segments of the shape, than during the execution of the early or late segments. These simulation results are congruent with studies showing that human/animal accuracy in serial order tasks is better during early or late elements in the sequence [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref043">43</xref>]. A characteristic example is illustrated in <xref ref-type="fig" rid="pcbi.1004402.g006">Fig 6C</xref>, in which the competition between copying the “blue” and the “green” segments resulted in an error trial.</p>
<fig id="pcbi.1004402.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004402.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Sequential movements.</title>
<p><bold>A</bold>: Examples of simulated trajectories for continuously copying a pentagon. <bold>B</bold>: Time course of the relative desirability values of the 5 individual policies (i.e., 5 segments) in a successful trial for copying a pentagon. The line colors correspond to the segments of the pentagon as shown in the top panel. The shape was copied counterclockwise (as indicated by the arrow) starting from the gray vertex. Each of the horizontal discontinuous lines indicate the completion time of copying the current segment. Notice that the desirability of the current segment peaks immediately after the start of drawing that segment and falls down gradually, whereas the desirability of the following segment starts rising while copying the current segment. Because of that, the consecutive segments compete for action selection frequently producing error trials, as illustrated in panel <bold>C</bold>. Finally, the panels (<bold>D</bold>) and (<bold>E</bold>) depict examples of simulated trajectories for continuously copying an equilateral triangle and a square, respectively, counterclockwise starting from the bottom right vertex.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.g006"/>
</fig>
<p>Notice also that the temporal pattern of desirability values is congruent with populations of neural activity in prefrontal cortex during the copying task that encode each of the segments [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>]. The strength of the neuronal population corresponding to a segment predicted the serial position of the segment in the motor sequence, providing a neural basis for Lashley’s hypothesis. Interestingly, the temporal evolution of the population activities resembles the temporal evolution of the relative desirabilities of policies in our theory. This finding provides a direct neural correlate of relative desirability suggesting that the computations in our model are biologically plausible. Finally, <xref ref-type="fig" rid="pcbi.1004402.g006">Fig 6D and 6E</xref> illustrate examples of movement trajectories for copying an equilateral triangle and a square.</p>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>How the brain dynamically selects between alternatives challenges a widely used model of decisions that posit comparisons of abstract representations of “goods” [<xref ref-type="bibr" rid="pcbi.1004402.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref007">7</xref>]. According to this model, the brain integrates all the decision variables of an option into a subjective economic value and makes a decision by comparing the values of the alternative options. Most importantly, the comparison is taking place within the space of goods, independent of the sensorimotor contingencies of choice [<xref ref-type="bibr" rid="pcbi.1004402.ref005">5</xref>]. While abstract representation of values have been found in brain areas like orbitrofrontal cortex (OFC) and ventromedial prefrontal cortex (vmPFC) [<xref ref-type="bibr" rid="pcbi.1004402.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref044">44</xref>], these representations do not necessarily exclude the involvement of sensorimotor areas in decisions between actions. Recent studies provide evidence for an “action-based” theory involving competition between concurrent prepared actions associated with alternative goals [<xref ref-type="bibr" rid="pcbi.1004402.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref017">17</xref>]. The main line of evidence of this theory is recent findings from neurophysiological studies [<xref ref-type="bibr" rid="pcbi.1004402.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref046">46</xref>] and studies that involve reversible inactivation of sensorimotor regions [<xref ref-type="bibr" rid="pcbi.1004402.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref048">48</xref>]. According to these studies, sensorimotor structures, such as the lateral intraparietal area (LIP) [<xref ref-type="bibr" rid="pcbi.1004402.ref048">48</xref>], the dorsal premotor cortex (dPM) [<xref ref-type="bibr" rid="pcbi.1004402.ref016">16</xref>], the superior colliculus (SC) [<xref ref-type="bibr" rid="pcbi.1004402.ref047">47</xref>] and the parietal reach region (PRR) [<xref ref-type="bibr" rid="pcbi.1004402.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref046">46</xref>] are causally involved in decisions.</p>
<p>Despite the attractiveness of the “action-based” theory to model decisions between actions, what has been missing is a computational theory that can combine good values (e.g., money, juice reward) with action costs (e.g., amount of effort) into an integrated theory of dynamic decision-making. Previous studies have used principles from Statistical Decision Theory (SDT) to model human behavior in visuomotor decisions [<xref ref-type="bibr" rid="pcbi.1004402.ref049">49</xref>]. According to these studies, action-selection can be modeled as a decision problem that maximizes the desirableness of outcomes, where desirableness can be captured by an expected gain function. Despite the significant contribution of these studies to the understanding of the mechanisms of visuomotor decisions, they have focused mostly on static environments, in which the availability and the value of an option do not change with time and previous actions. Additionally, the expected gain functions usually involve the integration of decision values that have the same currency, such as expected monetary gains and losses—e.g., humans perform rapid reaching movements towards displays with regions that, if touched within a boundary lead to monetary reward, otherwise to monetary penalty [<xref ref-type="bibr" rid="pcbi.1004402.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref051">51</xref>]. In the current study, we propose a probabilistic model that shows how value information from disparate sources with different “currencies” can be integrated in a manner that is both online and can be updated during action execution. The model is based on stochastic optimal control theory and is consistent with the view that decision and action are merged in a parallel rather than serial order. It is comprised of a series of control schemes that each of them is attached to an individual goal and generates a policy to achieve that goal starting from the current state. The key to our model is the relative desirability value that integrates the action costs and good values to a single variable that weighs the individual control policies as a function of state and time. It has intuitive meaning of the probability of getting the highest pay-off with the least cost following a specific policy at a given time and state. Because the desirability is state- and time- dependent, the weighted mixture of policies produces a range of behavior automatically, from “winner-take-all” to “weighted averaging”. By dynamically integrating in terms of probabilities across policies, relative-desirability varies with decision context. Relative desirability’s effective exchange rate changes whenever action costs increase or decrease, the set of options change, or the value of goods increase. Moreover, relative desirability is dynamic and state-dependent, allowing for dynamic changes in the effective exchange rate between action costs and the good values. We believe these properties are critical for maintaining adaptability in a changing environment. Throughout our evolutionary history, new opportunities and dangers constantly present themselves, making a fixed exchange rate between action costs and good value maladaptive.</p>
<p>The proposed computational framework can be conceived as analogous to classical value-comparison models in decision making, such as the drift diffusion model (DDM) [<xref ref-type="bibr" rid="pcbi.1004402.ref052">52</xref>] and the leaky competing accumulator (LCA) model [<xref ref-type="bibr" rid="pcbi.1004402.ref053">53</xref>], but for decisions that require continuous evaluation of in-flowing value information during ongoing actions. In the standard version of these models, choosing between two options is described by accumulator-to-threshold mechanisms. Sensory evidence associated with each alternative is accumulated, until the integrated evidence for one of them reaches a decision threshold. Despite the success of these frameworks to model a variety of decision tasks, they are difficult to extend beyond binary choices, require a pre-defined decision threshold and are mainly applied in perceptual decisions, in which decision precedes action. Unlike these models, the proposed computational theory can model decisions between multiple alternatives that either are presented simultaneously or sequentially, does not require any pre-defined decision threshold and can handle tasks in which subjects cannot wait to accumulate evidence before making a choice. The relative desirability integrates dynamically both sensory and motor evidence associated with a particular policy and reflects the degree to which this policy is best to follow at any given time and state with respect to the alternatives.</p>
<p>We tested our theory in a series of visuomotor decision tasks that involve reaching and saccadic movements and found that it captures many aspects of human and animal behavior observed in recent decision studies with multiple potential targets [<xref ref-type="bibr" rid="pcbi.1004402.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1004402.ref023">23</xref>]. In line with these studies, the theory predicts the “delay-and-mix” behavior, when the competing goals have about the same good values and action costs and the “pre-selection” behavior, when one of the alternative goals is clearly the best option.</p>
<p>The present computational theory bears some similarities with the Hierarchical Reinforcement Learning (HRL) models used extensively in decision-making studies [<xref ref-type="bibr" rid="pcbi.1004402.ref054">54</xref>]. According to HRL theory, decision-making takes place at different level of abstractions, where the higher levels select the best current goal and the lower levels generate the optimal policy to implement the choice. Although, the HRL implements the dynamic aspects of decision-making by re-evaluating the alternative options and selecting the best one at a given time and state, there are two fundamental differences with the present theory. First, HRL always selects the best policy and typically pursues it until all the actions in the sequence have been performed. On the other hand, our computational theory generates a weighted average of the alternative policies and executes only part of it before re-evaluating the alternative option (i.e., see <xref ref-type="supplementary-material" rid="pcbi.1004402.s006">S5 Text</xref> about the “receding horizon control” theory). Second, the HRL uses a softmax transformation to evaluate the alternative options, whereas the proposed computational theory uses both the expected reward and the effort cost associated with each alternative. Additionally, other similar modular frameworks consisting of multiple control systems, such as the MOSAIC model [<xref ref-type="bibr" rid="pcbi.1004402.ref055">55</xref>] and the Q-decomposition framework [<xref ref-type="bibr" rid="pcbi.1004402.ref056">56</xref>], have been previously proposed to model tasks with multiple goals. However, these frameworks do not incorporate the idea of integrating both the good values and action costs into the action selection process. Hence, they fail to make predictions on how value information from disparate sources influences the motor competition and how this competition can lead to erroneous behavior.</p>
<sec id="sec013">
<title>Decisions between conflicting options</title>
<p>We developed our model for cases where the competing options are similar. These are also cases where the relative effort and reward desirabilities are similar. For two options, it means the relative desirabilities would be far from zero or one. Here we consider extreme situations where one option requires much more effort or supplies much less reward. For extreme cases, the relative desirability calculation appears to break down and produces an “indeterminate” form for each alternative option. Here we explain why that happens, and how the indeterminacy is avoided by adding even a tiny amount of noise in implementing the calculation.</p>
<p>To illustrate the indeterminacy, consider selecting between an “extremely hard but very rewarding” and an “extremely easy but unrewarding” option. The hard option offers significantly higher reward than the easy option <italic>reward</italic>(<italic>Hard</italic>) &gt; &gt; <italic>reward</italic>(<italic>Easy</italic>), but it requires significantly higher effort to get it than the easy one <italic>cost</italic>(<italic>Hard</italic>) &gt; &gt; <italic>cost</italic>(<italic>Easy</italic>). According to the definition of the relative desirability, the reward-related component of the desirability will approach 1 for the hard option and 0 for the easy option, since <italic>P</italic>(<italic>reward</italic>(<italic>Hard</italic>) &gt; <italic>reward</italic>(<italic>Easy</italic>)) = 1. On the other hand, the effort-related component of the desirability will be 0 for the hard option and 1 for the easy option, since <italic>P</italic>(<italic>cost</italic>(<italic>Hard</italic>) &gt; <italic>cost</italic>(<italic>Easy</italic>)) = 1. The relative reliability multiplies these values and renormalizes, leading to the indeterminate form <italic>rD</italic>(<italic>option</italic>(1)) = 0*1/(0*1+1*0) = 0/0 and <italic>rD</italic>(<italic>option</italic>(2)) = 1*0/(0*1+1*0) = 0/0. In this case the model apparently fails to make a coherent choice. As long as the probability formula for reward and effort are continuous mappings, this indeterminacy will only be experienced in the limit that one option is infinitely harder to get (inaccessible) while the accessible option is comparably worthless.</p>
<p>However, the indeterminacy is an extreme example of an important class of problems where effort and reward values for the two options are in conflict with each other. Because there is a trade-off associated with <italic>reward vs effort</italic> neither option is clearly better than the other. While none of the decisions modeled here have extreme conflict, we nevertheless believe that the indeterminacy described above will never occur in a biological decision-making system due to the effects of even tiny amounts of noise on the relative desirability computation. If we assume that desirability values are the brain’s estimate of how “desirable” one option is with respect to alternatives in terms of expected outcome and effort cost, then it is reasonable to assume these estimates are not always precise. In other words, biological estimates of desirability should manifest stochastic errors, which we model by including noise in the estimates. In the <xref ref-type="supplementary-material" rid="pcbi.1004402.s007">S6 Text</xref> we show the effect of this noise is profound. For the extreme scenario in which <italic>P</italic>(<italic>reward</italic>(<italic>Hard</italic>) &gt; <italic>reward</italic>(<italic>Easy</italic>)) = 1 and <italic>P</italic>(<italic>cost</italic>(<italic>Hard</italic>) &gt; <italic>cost</italic>(<italic>Easy</italic>)) = 1, in the presence of noise the relative desirability of each option is 0.5. Thus, indeterminacy produces a lack of preference—since the “easy” option dominates the “hard” option in terms of effort, but the “hard” option is better than the “easy” option in terms of reward. In general, cases with extreme conflict will produce lack of preference, but these cases are also unstable—small changes in factors affecting the valuation such as the internal states of the subject (e.g., hunger level, fatigue level) can produce large shifts in preference. In the <xref ref-type="supplementary-material" rid="pcbi.1004402.s007">S6 Text</xref>, we further discuss the effects of noise in decisions with multiple options.</p>
</sec>
<sec id="sec014">
<title>Does the brain play dice?</title>
<p>One of the key assumptions in our study is that the brain continuously evaluates the relative desirability—i.e., the probability that a given policy will result in the highest pay-off with the least effort—in decisions with competing options. Although this idea is novel, experimental studies provide evidence that the brain maintains an explicit representation of “probability of choice” when selecting among competing options (for a review see [<xref ref-type="bibr" rid="pcbi.1004402.ref009">9</xref>]). For binary perceptual decisions, this probability describes the likelihood of one or another operant response, whereas for value-based decisions it describes the probability that selecting a particular option will result in the highest reward. Classic experimental studies reported a smooth relationship between stimulus parameters and the probability of choice suggesting that the brain translates value information to probabilities when making decisions [<xref ref-type="bibr" rid="pcbi.1004402.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref058">58</xref>]. Additionally, neurophysiological recordings in non-human primates revealed activity related to the probability of choice in the lateral intraparietal area (LIP) both in “two-alternative force-choice eye movement decisions” and in “value-based oculomotor decisions”. In the first case, the animals performed the random-dot motion (RDM) direction discrimination task while neuronal activity was recorded from the LIP [<xref ref-type="bibr" rid="pcbi.1004402.ref059">59</xref>]. The activity of the LIP neurons reflects a general decision variable that is monotonically related to the logarithm of the likelihood ratio that the animals will select one direction of motion versus the other. In classic value-based decisions, the animals had to select between two targets presented simultaneously in both hemifields [<xref ref-type="bibr" rid="pcbi.1004402.ref015">15</xref>]. The activity of the LIP neurons is modulated by a number of decision-related variables including the expected reward and the outcome probability. These experimental findings have inspired previous computational theories to model perceptual- and value-based decisions [<xref ref-type="bibr" rid="pcbi.1004402.ref009">9</xref>]. According to these studies, when the brain is faced with competing alternatives, it implements a series of computations to transform sensory and value information into a probability of choice. The proposed idea of the relative desirability value can be conceived as an extension of these theories taking into account both the expected reward and the expected effort related to a choice.</p>
</sec>
<sec id="sec015">
<title>Action competition explains errors in behavior</title>
<p>One of the novelties of this theory is that it predicts not only successful decisions, but decisions that result in poor or incorrect actions. A typical example is the “global effect” paradigm that occurs frequently in short latency saccadic movements. When the goal elements are located in close proximity and subjects are free to choose between them, erroneous eye movements usually land at intermediate locations between the goals [<xref ref-type="bibr" rid="pcbi.1004402.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref035">35</xref>]. Although the neural mechanisms underlying the global effect paradigm have not been understood fully yet, the prevailing view suggests that it occurs due to unresolved competition between the populations of neurons that encode the movements towards the two targets. Any target in the field is represented by a population of neurons that encodes the movement direction towards its location as a vector. The strength of the population is proportional to the saliency (e.g., size, luminance) and the expected pay-off of the target. When two similar targets are placed in close proximity, the populations corresponding to them will be combined to one mean population with the direction of the vector towards an intermediate location. If one of the targets is more salient or provide more reward than the other, the vector is biased to this target location. Since subjects have to perform saccadic movements to one of the targets, the competition between the two populations has to be resolved in time by inhibiting one of them. The time to suppress the neuronal activity that encodes one of the alternatives may be insufficient for short latency saccades resulting in averaging eye movements. Our findings are consistent with this theory. The strength of the neuronal population is consistent with relative desirability of the policy that drives the effector directed to the target. When the two equally rewarded targets are placed in close proximity, the two policies generate similar actions. Given that both targets are attached with the same goods-related values, the relative desirability of the two policies are about the same at different states, resulting in a strong competition. Because saccades are ballistic with little opportunity for correction during movement, the competition produces averaging saccades. On the other hand, placing the two targets in distance, the two saccadic policies generate dissimilar actions and consequently the competition is easier to be resolved in time.</p>
<p>Competition between policies in closely aligned goals can also explain errors in sequential decision tasks that involve serial order movements as described by Lashley [<xref ref-type="bibr" rid="pcbi.1004402.ref036">36</xref>]. The key idea in Lashley’s pioneer work (1951) is that the generation of serial order behavior involves the parallel activation of sequence of actions that are internally activated before each of the actions are executed. The main line of evidence of this hypothesis was the errors that occur frequently in serial order tasks, such as speech [<xref ref-type="bibr" rid="pcbi.1004402.ref037">37</xref>], typing [<xref ref-type="bibr" rid="pcbi.1004402.ref038">38</xref>], reaching [<xref ref-type="bibr" rid="pcbi.1004402.ref039">39</xref>] and copying of geometrical shapes [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>]. For instance, a common error in typing and speaking is to swap or transpose nearby letters, even words. Lashley suggested that errors in sequential tasks would be most likely to occur when executing nearby elements within a sequence. Recent neurophysiological studies provide the neural basis of the Lashley’s hypothesis showing that the serial characteristics of a sequence of movements are represented in an orderly fashion in the prefrontal cortex, in time before the start of drawing [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004402.ref040">40</xref>]. Training monkeys to copy geometrical shapes and recording the activity of individual neurons in the prefrontal cortex, the experimenters were able to identify populations of neurons that encode each of the segments [<xref ref-type="bibr" rid="pcbi.1004402.ref025">25</xref>]. The strength of the neuronal population corresponding to a segment predicted the serial position of the segment in the motor sequence. Interestingly, the temporal evolution of the strength of the segment representation during the execution of the trajectories for copying the shapes resembles the temporal evolution of the relative desirabilities of policies in our theory. This finding suggests that the strength of the neuronal population of a particular segment may encode the relative desirability (or components of the desirability) of copying that segment at a given time with respect to the alternatives. This hypothesis is also supported by error analysis in the serial order tasks, which showed that errors more frequently occurred when executing elements with nearly equal strength of representation. In a similar manner, our theory predicts that when two policies have about equal relative desirabilities over extended periods of the movement, the competition between them may lead to errors in behavior.</p>
</sec>
<sec id="sec016">
<title>A conceptual alternative in understanding the pathophysiology of the hemispatial neglect syndrome</title>
<p>Finally, our theory provides a conceptual alternative in understanding important aspects of neurological disorders that cause deficits in choice behavior, such as the spatial extinction syndrome. This syndrome is a subtle form of hemispatial neglect that occurs frequently after brain injury. It is characterized by the inability to respond to stimuli in the contralesional hemifield, but only when a simultaneous ipsilesional stimulus is also presented [<xref ref-type="bibr" rid="pcbi.1004402.ref060">60</xref>]. Recent studies reported contralesional bias that reminiscent the extinction syndrome, in oculomotor decision tasks after reversible pharmacological inactivation of the LIP [<xref ref-type="bibr" rid="pcbi.1004402.ref048">48</xref>] and the Pulvinar [<xref ref-type="bibr" rid="pcbi.1004402.ref061">61</xref>] in monkeys. According to our theory, this effect could be related to a deficit in value integration after inactivation, rather than simply sensory attention deficit.</p>
</sec>
<sec id="sec017">
<title>Conclusion</title>
<p>In sum, decisions require integrating both good values and action costs, which are often time and state dependent such that simple approaches pre-selection of goals or fixed weighted mixture of policies cannot account for the complexities of natural behavior. By focusing on a fundamental probabilistic computation, we provide a principled way to dynamically integrate these values that can merge work on decision making with motor control.</p>
</sec>
</sec>
<sec id="sec018">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004402.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s001" mimetype="image/tiff" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Effects of noise in the relative desirability estimation.</title>
<p><bold>A</bold>: We tested the effects of noise in the relative desirability estimation in a two-choice decision making task. The model was free to choose between the two targets (<italic>g</italic><sub>1</sub>, <italic>g</italic><sub>2</sub>) presented in different distances from the current hand position (<italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>). Each of these targets offers reward that follows a Normal distribution <inline-formula id="pcbi.1004402.e051"><alternatives><graphic id="pcbi.1004402.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004402.e051"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. <bold>B</bold>: Heat map of the relative desirability value for selecting the target <italic>g</italic><sub>2</sub> as a function of the distance <italic>r</italic><sub>1</sub> and the expected reward <italic>μ</italic><sub>1</sub> of the alternative target <italic>g</italic><sub>1</sub>. <bold>C</bold>: Relative desirability value for selecting the target <italic>g</italic><sub>2</sub> as a function of the distance <italic>r</italic><sub>1</sub> for different noise level <italic>μ</italic><sub><italic>ξ</italic></sub>. <bold>D</bold>: Relative desirability value of selecting the target <italic>g</italic><sub>2</sub> in the “do-nothing vs. do-hard” decision (i.e., <italic>r</italic><sub>1</sub> = 0) as a function of the noise level <italic>μ</italic><sub><italic>ξ</italic></sub>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s002" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Stochastic optimal control theory for reaching movements.</title>
<p>A detailed description of the stochastic optimal control theory used to model reaching movements to single targets.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s003" mimetype="application/pdf" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Stochastic optimal control theory for eye movements.</title>
<p>A detailed description of the stochastic optimal control theory used to model eye movements to single targets.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s004" mimetype="application/pdf" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Inverse temperature parameter λ.</title>
<p>A description of the inverse temperature parameter λ used in the transformation of the value-function to probability value in Eqs <xref ref-type="disp-formula" rid="pcbi.1004402.e005">3</xref> and <xref ref-type="disp-formula" rid="pcbi.1004402.e006">4</xref> (see the main manuscript).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s005" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s005" mimetype="application/pdf" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>Updating the target probability based on history of trials.</title>
<p>A description of the method used to update the target probability based on the trial history in rapid reaching movements.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s006" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s006" mimetype="application/pdf" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>Receding horizon control.</title>
<p>We implemented a receding horizon control technique to handle contingencies like changing the position of the targets, perturbations and effects of noise.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004402.s007" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004402.s007" mimetype="application/pdf" xlink:type="simple">
<label>S6 Text</label>
<caption>
<title>Noise in the relative desirability estimation.</title>
<p>We provide a detailed description of the effects of noise in the relative desirability estimation.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Dr. J. Bonaiuto, Dr. Sofia Sakellaridi and Luke Urban for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004402.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Friedman</surname> <given-names>M</given-names></name> (<year>1953</year>) <source>Essays in Positive Economics</source>. <publisher-name>Chicago University Press</publisher-name>, <publisher-loc>Chicago, IL</publisher-loc>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tversky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kahneman</surname> <given-names>D</given-names></name> (<year>1981</year>) <article-title>The framing of decisions and the psychology of choice</article-title>. <source>Science</source>. <volume>211</volume>: <fpage>453</fpage>–<lpage>458</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.7455683" xlink:type="simple">10.1126/science.7455683</ext-link></comment> <object-id pub-id-type="pmid">7455683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Roesch</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Olson</surname> <given-names>C</given-names></name> (<year>2004</year>) <article-title>Neuronal activity related to reward value and motivation in primate frontal cortex</article-title>. <source>Science</source>. <volume>304</volume>: <fpage>307</fpage>–<lpage>310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1093223" xlink:type="simple">10.1126/science.1093223</ext-link></comment> <object-id pub-id-type="pmid">15073380</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>J</given-names></name> (<year>2006</year>) <article-title>Neurons in orbitofrontal cortex encode economic value</article-title>. <source>Nature</source>. <volume>44</volume>: <fpage>223</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04676" xlink:type="simple">10.1038/nature04676</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name> (<year>2011</year>) <article-title>Neurobiology of economic choice: a good-based model</article-title>. <source>Annu Rev Neurosci</source>. <volume>34</volume>: <fpage>333</fpage>–<lpage>359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-061010-113648" xlink:type="simple">10.1146/annurev-neuro-061010-113648</ext-link></comment> <object-id pub-id-type="pmid">21456961</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Freedman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>J</given-names></name> (<year>2011</year>) <article-title>A proposed common neural mechanism for categorization and perceptual decisions</article-title>. <source>Nat Neurosci</source>. <volume>14</volume>: <fpage>143</fpage>–<lpage>146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2740" xlink:type="simple">10.1038/nn.2740</ext-link></comment> <object-id pub-id-type="pmid">21270782</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cai</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name> (<year>2012</year>) <article-title>Neuronal encoding of subjective value in dorsal and ventral anterior cingulate cortex</article-title>. <source>J Neurosci</source>. <volume>32</volume>: <fpage>3791</fpage>–<lpage>3808</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3864-11.2012" xlink:type="simple">10.1523/JNEUROSCI.3864-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22423100</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Basso</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wurtz</surname> <given-names>R</given-names></name> (<year>1998</year>) <article-title>Modulation of neuronal activity in superior colliculus by changes in target probability</article-title>. <source>J Neurosci</source>. <volume>18</volume>: <fpage>7519</fpage>–<lpage>7534</lpage>. <object-id pub-id-type="pmid">9736670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sugrue</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Corrado</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name> (<year>2005</year>) <article-title>Choosing the greater of two goods: Neural currencies for valuation and decision making</article-title>. <source>Nat Rev Neurosci</source>. <volume>6</volume>: <fpage>363</fpage>–<lpage>375</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1666" xlink:type="simple">10.1038/nrn1666</ext-link></comment> <object-id pub-id-type="pmid">15832198</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name> (<year>2007</year>) <article-title>Cortical mechanisms of action selection: The affordance competition hypothesis</article-title>. <source>Nat Rev Philos Trans R Soc Lond B Biol Sci</source>. <volume>362</volume>: <fpage>1585</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2007.2054" xlink:type="simple">10.1098/rstb.2007.2054</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hudson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>M</given-names></name> (<year>2007</year>) <article-title>Movement planning with probabilistic target information</article-title>. <source>Nat Rev Philos J Neurophysiol</source>. <volume>98</volume>: <fpage>3034</fpage>–<lpage>3046</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00858.2007" xlink:type="simple">10.1152/jn.00858.2007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chapman</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gallivan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Milne</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Culham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goodale</surname> <given-names>M</given-names></name> (<year>2010</year>) <article-title>Reaching for the unknown: Multiple target encoding and real-time decision-making in a rapid reach task</article-title>. <source>Cognition</source> <volume>116</volume>: <fpage>168</fpage>–<lpage>176</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2010.04.008" xlink:type="simple">10.1016/j.cognition.2010.04.008</ext-link></comment> <object-id pub-id-type="pmid">20471007</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hare</surname> <given-names>T</given-names></name> (<year>2010</year>) <article-title>Neural computations associated with goal-directed choice</article-title>. <source>Curr Opin Neurobiol</source>. <volume>2</volume>: <fpage>262</fpage>–<lpage>270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2010.03.001" xlink:type="simple">10.1016/j.conb.2010.03.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name> (<year>2012</year>) <article-title>Making decisions through a distributed consensus</article-title>. <source>Curr Opin Neurobiol</source>. <volume>22</volume>: <fpage>1</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2012.05.007" xlink:type="simple">10.1016/j.conb.2012.05.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Platt</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>P</given-names></name> (<year>1999</year>) <article-title>Neural correlates of decision variables in parietal cortex</article-title>. <source>Nature</source> <volume>400</volume>: <fpage>233</fpage>–<lpage>238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/22268" xlink:type="simple">10.1038/22268</ext-link></comment> <object-id pub-id-type="pmid">10421364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>J</given-names></name> (<year>2005</year>) <article-title>Neural correlates of reaching decisions in dorsal premotor cortex: Specification of multiple direction choices and final selection of action</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>801</fpage>–<lpage>814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.01.027" xlink:type="simple">10.1016/j.neuron.2005.01.027</ext-link></comment> <object-id pub-id-type="pmid">15748854</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gold</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>M</given-names></name> (<year>2007</year>) <article-title>The neural basis of decision making</article-title>. <source>Annu Rev Neurosci</source>. <volume>30</volume>: <fpage>535</fpage>–<lpage>574</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.29.051605.113038" xlink:type="simple">10.1146/annurev.neuro.29.051605.113038</ext-link></comment> <object-id pub-id-type="pmid">17600525</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kable</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>P</given-names></name> (<year>2009</year>) <article-title>The neurobiology of decision: consensus and controversy</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>733</fpage>–<lpage>745</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.09.003" xlink:type="simple">10.1016/j.neuron.2009.09.003</ext-link></comment> <object-id pub-id-type="pmid">19778504</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cisek</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kalaska</surname> <given-names>J</given-names></name> (<year>2010</year>) <article-title>Neural mechanisms for interacting with a world full of action choices</article-title>. <source>Annu Rev Neurosci</source>. <volume>33</volume>: <fpage>269</fpage>–<lpage>298</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.051508.135409" xlink:type="simple">10.1146/annurev.neuro.051508.135409</ext-link></comment> <object-id pub-id-type="pmid">20345247</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Song</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nakayama</surname> <given-names>K</given-names></name> (<year>2008</year>) <article-title>Target selection in visual search as revealed by movement trajectories</article-title>. <source>Vision Res</source>. <volume>48</volume>: <fpage>853</fpage>–<lpage>861</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2007.12.015" xlink:type="simple">10.1016/j.visres.2007.12.015</ext-link></comment> <object-id pub-id-type="pmid">18262583</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Song</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nakayama</surname> <given-names>K</given-names></name> (<year>2009</year>) <article-title>Hidden cognitive states revealed in choice reaching tasks</article-title>. <source>Trends Cogn Sci</source>. <volume>13</volume>: <fpage>360</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2009.04.009" xlink:type="simple">10.1016/j.tics.2009.04.009</ext-link></comment> <object-id pub-id-type="pmid">19647475</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chapman</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gallivan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Milne</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Culham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goodale</surname> <given-names>M</given-names></name> (<year>2010</year>) <article-title>Short-term motor plasticity revealed in a visuomotor decision-making task</article-title>. <source>Behav Brain Res</source>. <volume>214</volume>: <fpage>130</fpage>–<lpage>134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bbr.2010.05.012" xlink:type="simple">10.1016/j.bbr.2010.05.012</ext-link></comment> <object-id pub-id-type="pmid">20472001</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gallivan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chapman</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Milne</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ansari</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Culham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goodale</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>One to four, and nothing more: Non-conscious parallel object individuation in action</article-title>. <source>Psychol Sci</source>. <volume>22</volume>: <fpage>803</fpage>–<lpage>811</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797611408733" xlink:type="simple">10.1177/0956797611408733</ext-link></comment> <object-id pub-id-type="pmid">21562312</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chou</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>P</given-names></name> (<year>1999</year>) <article-title>Express averaging saccades in monkeys</article-title>. <source>Vision Res</source>. <volume>39</volume>: <fpage>4200</fpage>–<lpage>4216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(99)00133-9" xlink:type="simple">10.1016/S0042-6989(99)00133-9</ext-link></comment> <object-id pub-id-type="pmid">10755158</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Averbeck</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chafee</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Crowe</surname> <given-names>D</given-names></name> (<year>2002</year>) <article-title>Parallel processing of serial movements in prefrontal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. <volume>99</volume>: <fpage>13172</fpage>–<lpage>13177</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.162485599" xlink:type="simple">10.1073/pnas.162485599</ext-link></comment> <object-id pub-id-type="pmid">12242330</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>M</given-names></name> (<year>2002</year>) <article-title>Optimal feedback control as a theory of motor coordination</article-title>. <source>Nat Neurosci</source>. <volume>5</volume>: <fpage>1226</fpage>–<lpage>1235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn963" xlink:type="simple">10.1038/nn963</ext-link></comment> <object-id pub-id-type="pmid">12404008</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Christopoulos</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Schrater</surname> <given-names>P</given-names></name> (<year>2011</year>) <article-title>An optimal feedback control framework for grasping objects with position uncertainty</article-title>. <source>Neural Comput</source>. <volume>23</volume>: <fpage>2511</fpage>–<lpage>2436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00180" xlink:type="simple">10.1162/NECO_a_00180</ext-link></comment> <object-id pub-id-type="pmid">21732861</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Popovic</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stei</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Oguztoreli</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lebiedowska</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jonic</surname> <given-names>S</given-names></name> (<year>1999</year>) <article-title>Optimal control of walking with functional electrical stimulation: a computer simulation study</article-title>. <source>IEEE Trans Rehabil Eng</source>. <volume>7</volume>: <fpage>69</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/86.750554" xlink:type="simple">10.1109/86.750554</ext-link></comment> <object-id pub-id-type="pmid">10188609</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Franklin</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name> (<year>2011</year>) <article-title>Computational mechanisms of sensorimotor control</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>425</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.10.006" xlink:type="simple">10.1016/j.neuron.2011.10.006</ext-link></comment> <object-id pub-id-type="pmid">22078503</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Papadimitriou</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Tsitsiklis</surname> <given-names>JN</given-names></name> (<year>1987</year>) <article-title>The complexity of markov decision processes</article-title>. <source>Mathematics of Operations Research</source> <volume>12</volume>: <fpage>441</fpage>–<lpage>450</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1287/moor.12.3.441" xlink:type="simple">10.1287/moor.12.3.441</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kappen</surname> <given-names>H</given-names></name> (<year>2005</year>) <article-title>Path integrals and symmetry breaking for optimal control theory</article-title>. <source>J Statistical Mechanics: Theory and Experiment</source> <volume>11</volume>: <fpage>P11011</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/1742-5468/2005/11/P11011" xlink:type="simple">10.1088/1742-5468/2005/11/P11011</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Theodorou</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Buchli</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schaal</surname> <given-names>S</given-names></name> (<year>2010</year>) <article-title>A generalized path integral control approach to reinforcement learning</article-title>. <source>J Machine Learning Research</source> <volume>11</volume>: <fpage>3137</fpage>–<lpage>3181</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bowling</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Khasawneh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kaewkuekool</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cho</surname> <given-names>B</given-names></name> (<year>2009</year>) <article-title>A logistic approximation to the cumulative normal distribution</article-title>. <source>J Industrial Engineering and Management</source> <volume>2</volume>: <fpage>114</fpage>–<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3926/jiem.2009.v2n1.p114-127" xlink:type="simple">10.3926/jiem.2009.v2n1.p114-127</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ottes</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>van Gisbergen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Eggermont</surname> <given-names>JJ</given-names></name> (<year>1984</year>) <article-title>Metrics of saccade responses to visual double stimuli: two different modes</article-title>. <source>Vision Res</source>. <volume>24</volume>: <fpage>1169</fpage>–<lpage>1179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0042-6989(84)90172-X" xlink:type="simple">10.1016/0042-6989(84)90172-X</ext-link></comment> <object-id pub-id-type="pmid">6523740</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>van der Stigchel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Meeter</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Theeuwes</surname> <given-names>J</given-names></name> (<year>2006</year>) <article-title>Eye movement trajectories and what they tell us</article-title>. <source>Neurosci Biobehav Rev</source>. <volume>30</volume>: <fpage>666</fpage>–<lpage>679</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neubiorev.2005.12.001" xlink:type="simple">10.1016/j.neubiorev.2005.12.001</ext-link></comment> <object-id pub-id-type="pmid">16497377</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Lashley</surname> <given-names>K</given-names></name> (<year>1951</year>) <chapter-title>The problem of serial order in behavior</chapter-title>. In: <source>Cerebral mechanisms in behavior: The Hixon symposium</source>, ed. <name name-style="western"><surname>Jeffress</surname> <given-names>L.A.</given-names></name>. <publisher-name>Wiley</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>MacNeilage</surname> <given-names>P</given-names></name> (<year>1964</year>) <article-title>Typing errors as clues to serial ordering mechanisms in language behaviour</article-title>. <source>Lang Speech</source>. <volume>7</volume>: <fpage>144</fpage>–<lpage>159</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rosenbaum</surname> <given-names>D</given-names></name> (<year>1991</year>) <article-title>Drawing and writing; chapter In Human motor control</article-title>. <source>Academic Press</source>. <volume>7</volume>: <fpage>254</fpage>–<lpage>275</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rosenbaum</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jax</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Weiss</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>der Wel</surname> <given-names>RV</given-names></name> (<year>2007</year>) <article-title>The problem of serial order in behavior: Lashley’s legacy</article-title>. <source>Hum Mov Sci</source>. <volume>26</volume>: <fpage>525</fpage>–<lpage>554</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.humov.2007.04.001" xlink:type="simple">10.1016/j.humov.2007.04.001</ext-link></comment> <object-id pub-id-type="pmid">17698232</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Averbeck</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chafee</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Crowe</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Georgopoulos</surname> <given-names>A</given-names></name> (<year>2003</year>) <article-title>Neural activity in prefrontal cortex during copying geometrical shapes. i. single cells encode shape, sequence and metric parameters</article-title>. <source>Exp Brain Res</source>. <volume>159</volume>: <fpage>127</fpage>–<lpage>141</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tzagarakis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Jerde</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ugurbil</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Georgopoulos</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>Cerebral cortical mechanisms of copying geometrical shapes: a multidimensional scaling analysis of fmri patterns of activation</article-title>. <source>Exp. Brain Res</source>. <volume>194</volume>: <fpage>369</fpage>–<lpage>380</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-009-1709-5" xlink:type="simple">10.1007/s00221-009-1709-5</ext-link></comment> <object-id pub-id-type="pmid">19189086</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Christopoulos</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Leuthold</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Georgopoulos</surname> <given-names>A</given-names></name> (<year>2012</year>) <article-title>Spatiotemporal neural interactions underlying continuous drawing movements as revealed by magnetoencephalography</article-title>. <source>Exp. Brain Res</source>. <volume>222</volume>: <fpage>159</fpage>–<lpage>171</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-012-3208-3" xlink:type="simple">10.1007/s00221-012-3208-3</ext-link></comment> <object-id pub-id-type="pmid">22923206</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Robinson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>M</given-names></name> (<year>1926</year>) <article-title>Effect of serial position upon memorization</article-title>. <source>Am J Psychol</source>. <volume>37</volume>: <fpage>538</fpage>–<lpage>552</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/1414914" xlink:type="simple">10.2307/1414914</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kennerley</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Decision making and reward in frontal cortex: complementary evidence from neurophysiological and neuropsychological studies</article-title>. <source>Behav Neurosci</source>. <volume>125</volume>: <fpage>297</fpage>–<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0023575" xlink:type="simple">10.1037/a0023575</ext-link></comment> <object-id pub-id-type="pmid">21534649</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cui</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>R</given-names></name> (<year>2007</year>) <article-title>Posterior parietal cortex encodes autonomously selected motor plans</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>552</fpage>–<lpage>559</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.09.031" xlink:type="simple">10.1016/j.neuron.2007.09.031</ext-link></comment> <object-id pub-id-type="pmid">17988637</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Klaes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Westendorff</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chakrabarti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gail</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>Choosing goals, not rules: deciding among rule-based action plans</article-title>. <source>Neuron</source> <volume>70</volume>: <fpage>536</fpage>–<lpage>548</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.053" xlink:type="simple">10.1016/j.neuron.2011.02.053</ext-link></comment> <object-id pub-id-type="pmid">21555078</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McPeek</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Keller</surname> <given-names>E</given-names></name> (<year>2004</year>) <article-title>Deficits in saccade target selection after inactivation of superior colliculus</article-title>. <source>Nat Neurosci</source>. <volume>7</volume>: <fpage>757</fpage>–<lpage>763</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1269" xlink:type="simple">10.1038/nn1269</ext-link></comment> <object-id pub-id-type="pmid">15195099</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kagan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>R</given-names></name> (<year>2012</year>) <article-title>Functional imaging reveals rapid reorganization of cortical activity after parietal inactivation in monkeys</article-title>. <source>Proc Natl Acad Sci U S A</source>. <volume>109</volume>: <fpage>8274</fpage>–<lpage>8279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1204789109" xlink:type="simple">10.1073/pnas.1204789109</ext-link></comment> <object-id pub-id-type="pmid">22562793</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Trommershauser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name> (<year>2008</year>) <article-title>Decision making, movement planning and statistical decision theory</article-title>. <source>Trends Cogn Sci</source>. <volume>12</volume>: <fpage>291</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2008.04.010" xlink:type="simple">10.1016/j.tics.2008.04.010</ext-link></comment> <object-id pub-id-type="pmid">18614390</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Trommershauser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name> (<year>2003</year>) <article-title>Statistical decision theory and the selection of rapid, goal-directed movements</article-title>. <source>J Opt Soc Am A Opt Image Sci Vis</source>. <volume>20</volume>: <fpage>1419</fpage>–<lpage>1433</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1364/JOSAA.20.001419" xlink:type="simple">10.1364/JOSAA.20.001419</ext-link></comment> <object-id pub-id-type="pmid">12868646</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Trommershauser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gepshtein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name> (<year>2005</year>) <article-title>Optimal compensation for changes in task-relevant movement variability</article-title>. <source>J Neurosci</source>. <volume>25</volume>: <fpage>7169</fpage>–<lpage>7178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1906-05.2005" xlink:type="simple">10.1523/JNEUROSCI.1906-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16079399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name> (<year>2002</year>) <article-title>Diffusion model account of response time and accuracy in a brightness discrimination task: fitting real data and failing to fit fake but plausible data</article-title>. <source>Psychon Bull Rev</source>. <volume>2</volume>: <fpage>278</fpage>–<lpage>291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03196283" xlink:type="simple">10.3758/BF03196283</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Usher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>J</given-names></name> (<year>2001</year>) <article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title>. <source>Psychol. Rev</source>. <volume>108</volume>: <fpage>550</fpage>–<lpage>592</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.108.3.550" xlink:type="simple">10.1037/0033-295X.108.3.550</ext-link></comment> <object-id pub-id-type="pmid">11488378</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name> (<year>2012</year>) <article-title>Hierarchical reinforcement learning and decision making</article-title>. <source>Curr Opinion Neurobio</source>. <volume>22</volume>: <fpage>956</fpage>–<lpage>962</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2012.05.008" xlink:type="simple">10.1016/j.conb.2012.05.008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Haruno</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name> (<year>2001</year>) <article-title>MOSAIC model for sensorimotor learning and control</article-title>. <source>Neural Comput</source>. <volume>13</volume>: <fpage>2201</fpage>–<lpage>2220</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976601750541778" xlink:type="simple">10.1162/089976601750541778</ext-link></comment> <object-id pub-id-type="pmid">11570996</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="other">Russell S, Zimdars A (2003) Q-decomposition for reinforcement learning agents. In proceedings of the 20<sup><italic>th</italic></sup> International Conference on Machine Learning (ICML-2003) pp 656–663, Washington DC.</mixed-citation>
</ref>
<ref id="pcbi.1004402.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name> (<year>1996</year>) <article-title>Motion perception: seeing and deciding</article-title>. <source>Proc Natl Acad Sci U S A</source>. <volume>93</volume>: <fpage>628</fpage>–<lpage>633</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.93.2.628" xlink:type="simple">10.1073/pnas.93.2.628</ext-link></comment> <object-id pub-id-type="pmid">8570606</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dorris</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name> (<year>2004</year>) <article-title>Activity in posterior parietal cortex is correlated with the relative subjective desirability of action</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>365</fpage>–<lpage>378</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2004.09.009" xlink:type="simple">10.1016/j.neuron.2004.09.009</ext-link></comment> <object-id pub-id-type="pmid">15473973</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name> (<year>2001</year>) <article-title>Neural computations that underlie decisions about sensory stimuli</article-title>. <source>Trends Cogn Sci</source>. <volume>5</volume>: <fpage>10</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(00)01567-9" xlink:type="simple">10.1016/S1364-6613(00)01567-9</ext-link></comment> <object-id pub-id-type="pmid">11164731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mesulam</surname> <given-names>M</given-names></name> (<year>1999</year>) <article-title>Spatial attention and neglect: parietal, frontal and cingulate contributions to the mental representation and attentional targeting of salient extrapersonal events</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <volume>354</volume>: <fpage>1325</fpage>–<lpage>1346</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.1999.0482" xlink:type="simple">10.1098/rstb.1999.0482</ext-link></comment> <object-id pub-id-type="pmid">10466154</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004402.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilke</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kagan</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>R</given-names></name> (<year>2013</year>) <article-title>Effects of pulvinar inactivation on spatial decision-making between equal and asymmetric reward options</article-title>. <source>J Cogn Neurosci</source>. <volume>25</volume>: <fpage>1270</fpage>–<lpage>1283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn_a_00399" xlink:type="simple">10.1162/jocn_a_00399</ext-link></comment> <object-id pub-id-type="pmid">23574581</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>