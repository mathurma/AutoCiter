<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00653</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006729</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population metrics</subject><subj-group><subject>Population density</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Computational geometry for modeling neural populations: From visualization to simulation</article-title>
<alt-title alt-title-type="running-head">Computational geometry for modeling neural populations</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7162-4425</contrib-id>
<name name-style="western">
<surname>de Kamps</surname> <given-names>Marc</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4262-5549</contrib-id>
<name name-style="western">
<surname>Lepperød</surname> <given-names>Mikkel</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5397-8753</contrib-id>
<name name-style="western">
<surname>Lai</surname> <given-names>Yi Ming</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Institute for Artificial and Biological Intelligence, University of Leeds, Leeds, West Yorkshire, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Institute of Basic Medical Sciences, and Center for Integrative Neuroplasticity, University of Oslo, Oslo, Norway</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Currently at the School of Mathematical Sciences, University of Nottingham, Nottingham, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gutkin</surname> <given-names>Boris S.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>École Normale Supérieure, College de France, CNRS, FRANCE</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">M.deKamps@leeds.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>4</day>
<month>3</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e1006729</elocation-id>
<history>
<date date-type="received">
<day>24</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>de Kamps et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006729"/>
<abstract>
<p>The importance of a mesoscopic description level of the brain has now been well established. Rate based models are widely used, but have limitations. Recently, several extremely efficient population-level methods have been proposed that go beyond the characterization of a population in terms of a single variable. Here, we present a method for simulating neural populations based on two dimensional (2D) point spiking neuron models that defines the state of the population in terms of a density function over the neural state space. Our method differs in that we do not make the diffusion approximation, nor do we reduce the state space to a single dimension (1D). We do not hard code the neural model, but read in a grid describing its state space in the relevant simulation region. Novel models can be studied without even recompiling the code. The method is highly modular: variations of the deterministic neural dynamics and the stochastic process can be investigated independently. Currently, there is a trend to reduce complex high dimensional neuron models to 2D ones as they offer a rich dynamical repertoire that is not available in 1D, such as limit cycles. We will demonstrate that our method is ideally suited to investigate noise in such systems, replicating results obtained in the diffusion limit and generalizing them to a regime of large jumps. The joint probability density function is much more informative than 1D marginals, and we will argue that the study of 2D systems subject to noise is important complementary to 1D systems.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>A group of slow, noisy and unreliable cells collectively implement our mental faculties, and how they do this is still one of the big scientific questions of our time. Mechanistic explanations of our cognitive skills, be it locomotion, object handling, language comprehension or thinking in general—whatever that may be—is still far off. A few years ago the following question was posed: Imagine that aliens would provide us with a brain-sized clump of matter, with complete freedom to sculpt realistic neuronal networks with arbitrary precision. Would we be able to build a brain? The answer appears to be no, because this technology is actually materializing, not in the form of an alien kick-start, but through steady progress in computing power, simulation methods and the emergence of databases on connectivity, neural cell types, complete with gene expression, etc. A number of groups have created brain-scale simulations, others like the Blue Brain project may not have simulated a full brain, but they included almost every single detail known about the neurons they modelled. And yet, we do not know how we reach for a glass of milk.</p>
<p>Mechanistic, large-scale models require simulations that bridge multiple scales. Here we present a method that allows the study of two dimensional dynamical systems subject to noise, with very little restrictions on the dynamical system or the nature of the noise process. Given that high dimensional realistic models of neurons have been reduced successfully to two dimensional dynamical systems, while retaining all essential dynamical features, we expect that this method will contribute to our understanding of the dynamics of larger brain networks without requiring the level of detail that make brute force large-scale simulations so unwieldy.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100007601</institution-id>
<institution>Horizon 2020</institution>
</institution-wrap>
</funding-source>
<award-id>720270</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7162-4425</contrib-id>
<name name-style="western">
<surname>de Kamps</surname> <given-names>Marc</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010661</institution-id>
<institution>Horizon 2020 Framework Programme</institution>
</institution-wrap>
</funding-source>
<award-id>785907</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7162-4425</contrib-id>
<name name-style="western">
<surname>de Kamps</surname> <given-names>Marc</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This project received funding from the European Union’s Horizon 2020 research and innovation programme under Grant Agreement No. 720270 (HBP SGA1) and Specific Grant Agreement No. 785907 (Human Brain Project SGA2) (MdK; YML). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="15"/>
<table-count count="2"/>
<page-count count="41"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-03-14</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology</italic> Methods paper.</p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The population or mesoscopic level is now recognised as a very important description level for brain dynamics. Traditionally rate based models [<xref ref-type="bibr" rid="pcbi.1006729.ref001">1</xref>] have been used: models that characterize the state of a population by a single variable. There are inherent limitations to this approach, for example a poor replication of transient dynamics that is observed in simulations of spiking neurons, and various groups have proposed a population density approach. Density methods start from individual point model neurons, consider their state space, and define a density function over this space. The density function characterizes how individual neurons of a population are distributed over state space. These methods have been used successfully for one dimensional point model neurons, i.e. models characterized by a single state variable, usually membrane potential. Such models, e.g. based on leaky- (LIF) or quadratic-integrate-and-fire (QIF), exponential-integrate-and-fire neurons, have a long-standing tradition in neuroscience [<xref ref-type="bibr" rid="pcbi.1006729.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref006">6</xref>]. Related approaches consider densities of quantities such as the time since last spike [<xref ref-type="bibr" rid="pcbi.1006729.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref008">8</xref>], but here too a single variable is considered to be too coarse grained to represent the state of a population.</p>
<p>Recently, increased computing power and more sophisticated algorithms, e.g. [<xref ref-type="bibr" rid="pcbi.1006729.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>], have made the numerical solution of time dependent density equations become tractable for one dimensional neural models. In parallel, dimensional reductions of the density have been developed, usually by expressing the density in terms of a limited set of basis functions. By studying the evolution as a time-dependent weighting of this basis the dimensionality is reduced, often resulting in sets of first order non-linear differential equations, which sometimes are interpreted as ‘rate based’ models [<xref ref-type="bibr" rid="pcbi.1006729.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref015">15</xref>].</p>
<p>The one dimensional density is very tractable: membrane potential distributions and firing rates have been shown to match spiking neuron simulations accurately, particularly in the limit of infinitely large populations, at much lower computational cost than direct spiking simulations: Cain <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref016">16</xref>] report a speedup of two orders of magnitude compared to a direct (Monte Carlo) simulation. The problem of such one dimensional models is that they leave out details that may affect the population, such as synaptic dynamics and adaptation. Mathematically, the inclusion of variables other than just the membrane potential is no problem, but this increases the dimensionality of the state space, which negates most—but not all—computational advantages that density functions have over Monte Carlo simulation. This problem has led to considerable efforts to produce effective one dimensional methods that allow the inclusion of more realistic features of neural dynamics. Cain <italic>et al</italic>. have included the effects of conductances by making synaptic effects potential dependent in an otherwise standard one dimensional paradigm. Schwalger <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref017">17</xref>] consider the distribution over the last spike times of neurons. Under a quasi-renewal approximation that the probability of a neuron firing is only dependent on the last spike time and recent population activity, they are able to model the evolution of the last spike time distribution and the population activity resulting in a system of one dimensional distributions. Both groups have modeled a large-scale spiking neuron model of a cortical column, achieving impressive agreement between Monte Carlo and density methods. Another attempt to reduce the dimensionality of the problem are moment-closure methods [<xref ref-type="bibr" rid="pcbi.1006729.ref018">18</xref>], which we will not consider here. Recently, Augustin <italic>et al</italic> have presented a method to include adaptation into a one-dimensional density approach [<xref ref-type="bibr" rid="pcbi.1006729.ref015">15</xref>].</p>
<p>There have been a number of studies of two dimensional densities [<xref ref-type="bibr" rid="pcbi.1006729.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref021">21</xref>]. They have made clear that analyzing the evolution of the joint probability density provides valuable insight in population dynamics, but they are not generic: it is not explicit that the method can be extended to other neural models without recoding the algorithm.</p>
<p>Here, we present a generic method for simulating two dimensional densities. Unlike the vast majority of studies so far, it does not start from a Fokker-Planck assumption but starts from the master equation of a point process (usually, but not necessarily) Poisson, and models the joint density distribution without dimensional reduction. We believe the method is important given the trend in theoretical neuroscience to reduce complex realistic biophysical models to effective two dimensional model neurons. Adaptive-exponential-integrate-and-fire (AdExp), Fitzhugh-Nagumo and Izhikevich neurons are examples of two dimensional model neurons that have been introduced as realistic reductions of more complex conductance based models. It is important to study these systems when subjected to noise.</p>
<p>The method is extremely flexible: upon the creation of a novel neural model (2D) we will be able to simulate a population subjected to synaptic input without writing a single line of new code. We require the user to present a visualization of the model in the form of the streamlines of its vector field, presented in a certain file format. Since these files can be exchanged, model exchange does not require recoding. As long as this vector field behaves reasonably—the qualification of what constitutes reasonable is a main topic of this paper—the method will be able to take it as input, and can be guaranteed to deliver sensible simulation results. The method is highly visual: it starts off with a user or stock provided visualization of a neural model, and uses computational geometry to calculate the transition matrices involved in modeling synaptic input, which is represented as a stochastic process. We will argue that with a visualization in hand one can often predict how noise will drive the system, and run a simulation to confirm these predictions. We will also show that the visualization gives a good overview of possible shapes of dynamics.</p>
<p>The method cannot compete in speed with effective one dimensional density methods, but holds up well compared to direct spiking neuron simulations. In particular memory use is at least an order of magnitude lower than for direct simulation. As we will show, this allows the simulation of large networks on a single machine equipped with a GPGPU. Since very few assumptions are used, it can be used to examine the influence of approximations made in other methods. For example, because no diffusion approximation is made, we are able to examine the influence of strong synapses, which can lead to a marked deviation from diffusion results [<xref ref-type="bibr" rid="pcbi.1006729.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>]. We can also model populations that are in partial synchrony.</p>
<p>This work captures most one dimensional population density techniques, as they are a special case of two dimensional models, in particular the method by Cain <italic>et al</italic>., and we also replicate results obtained in the diffusion limit as numerical solutions of Fokker-Planck equations with high precision. Although we have not tried this, theory suggests that the method should work just as well when escape noise is used [<xref ref-type="bibr" rid="pcbi.1006729.ref007">7</xref>]. With the ability to exchange neural model files, without having to recode, it is easy to check how different neural models generate dynamics in similar circuits. A software implementation of this method is available at <ext-link ext-link-type="uri" xlink:href="http://miind.sf.net" xlink:type="simple">http://miind.sf.net</ext-link> with a mirror repository on github <ext-link ext-link-type="uri" xlink:href="https://github.com/dekamps/miind" xlink:type="simple">https://github.com/dekamps/miind</ext-link>.</p>
<p>Since this is a methods paper, the <bold>Material and Methods</bold> section contains the main result, and we will present this first so that the reader may form an understanding of how the simulation results are produced. In the results section, we will show that our method works for a number of very different neural models. We will also show that strong transients, which occur in some models as a consequence of rapidly changing input, but not in others, can be understood in geometrical terms when considering the state space of the neural model.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<p>We will consider point model neurons with a two dimensional state space. In general such models are described by a vector field <inline-formula id="pcbi.1006729.e001"><alternatives><graphic id="pcbi.1006729.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which is defined on an open subset of <inline-formula id="pcbi.1006729.e002"><alternatives><graphic id="pcbi.1006729.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>. The equations of motion of an individual neuron are given by:
<disp-formula id="pcbi.1006729.e003"><alternatives><graphic id="pcbi.1006729.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>τ</italic> is the membrane time constant of the neuron. We will adapt the convention that the first coordinate of <inline-formula id="pcbi.1006729.e004"><alternatives><graphic id="pcbi.1006729.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> always represents a neuron’s membrane potential <italic>v</italic> and will refer to the second coordinate of <inline-formula id="pcbi.1006729.e005"><alternatives><graphic id="pcbi.1006729.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> as <italic>w</italic>, as is conventional for the adaptation variable in the AdExp model and the recovery variable in the Fitzhugh-Nagumo model (although not in the conductance-based model). Usually boundary conditions are imposed. When a threshold potential <italic>V</italic><sub>th</sub> is present, part of ∂<italic>M</italic>, the edge of <italic>M</italic>, overlaps with <italic>V</italic> = <italic>V</italic><sub>th</sub>. This part of ∂<italic>M</italic> is called the threshold. When a neuron state approaches the threshold from below, the state is reset, sometimes after a refractive time interval <italic>τ</italic><sub>ref</sub> during which its state is effectively undefined. The reset results in coordinate <italic>v</italic> being set to a reset potential <italic>V</italic><sub>reset</sub>, whilst the second coordinate remains unaffected if no refractive interval or period is considered. If there is a refractive period, there are variations: sometimes the second coordinate is kept constant, sometimes further evolution according to <xref ref-type="disp-formula" rid="pcbi.1006729.e003">Eq (1</xref>) for a period of <italic>τ</italic><sub>ref</sub> is considered and the reset value of the second coordinate is taken to be the resulting value of <italic>w</italic>(<italic>t</italic><sub>spike</sub> + <italic>τ</italic><sub>ref</sub>), where <italic>t</italic><sub>spike</sub> is the time when the neuron hits threshold. The neuron itself emits a spike upon hitting the threshold. This description fits many neuron models: e.g. adaptive-integrate-and-fire; conductance-based leaky-integrate-and-fire; Izhikevich [<xref ref-type="bibr" rid="pcbi.1006729.ref022">22</xref>], and many others.</p>
<p>We are interested in populations of neurons. We consider a population to be homogeneous: all neurons have the same parameters, and statistically see the same input: they are subject to input spike trains generated from the same distribution. Under those considerations one can define a density, <inline-formula id="pcbi.1006729.e006"><alternatives><graphic id="pcbi.1006729.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, over state space for a population that is sufficiently large. <inline-formula id="pcbi.1006729.e007"><alternatives><graphic id="pcbi.1006729.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is defined as the fraction of neurons in the population whose state vector is in <inline-formula id="pcbi.1006729.e008"><alternatives><graphic id="pcbi.1006729.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. For spike trains generated by a Markov process, the evolution equation of the density obeys the differential Chapman-Kolmogorov equation:
<disp-formula id="pcbi.1006729.e009"><alternatives><graphic id="pcbi.1006729.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>M</mml:mi></mml:msub> <mml:mi>d</mml:mi> <mml:msup><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>{</mml:mo> <mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>∣</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∣</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <inline-formula id="pcbi.1006729.e010"><alternatives><graphic id="pcbi.1006729.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <italic>τ</italic> are from the neuron model as stated in <xref ref-type="disp-formula" rid="pcbi.1006729.e003">Eq (1)</xref>.</p>
<p>Input spikes will cause instantaneous responses in the state space of neurons. For delta synapses, for example, an input spike will cause a transition from membrane potential <italic>V</italic> to membrane potential <italic>V</italic> + <italic>h</italic>, where <italic>h</italic> is the synaptic efficacy, which may be drawn from a probability distribution <italic>p</italic>(<italic>h</italic>). In current based models the jump may be in the input current, and in conductance based models, studied below, the jump is in conductance, rather than membrane potential. Nonetheless, in all of these cases the input spikes cause instantaneous transitions from one point in state space to another. The right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> expresses that the loss of neurons in one part of state space is balanced by their reappearance in another after the jump. As a concrete example, consider input spikes generated by a Poisson point process with delta synapses:
<disp-formula id="pcbi.1006729.e011"><alternatives><graphic id="pcbi.1006729.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∣</mml:mo> <mml:mi>v</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:mi>v</mml:mi> <mml:mo>−</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>ν</mml:mi> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mo>−</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>ν</italic> is the rate of the Poisson process and <italic>h</italic> is the synaptic efficacy, which for simplicity we will consider here as a single fixed value. <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> reduces to:
<disp-formula id="pcbi.1006729.e012"><alternatives><graphic id="pcbi.1006729.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>∂</mml:mi> <mml:mrow><mml:mi>∂</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mi>τ</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mo>−</mml:mo> <mml:mi>h</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where the <italic>v</italic><sub><italic>i</italic></sub> are the components of <inline-formula id="pcbi.1006729.e013"><alternatives><graphic id="pcbi.1006729.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>.</p>
<p>At this stage, often a Taylor expansion is made for the right-hand side of the equation up to second order, which leads to a Fokker-Planck equation. We will not pursue this approach, instead we will point out, as observed by de Kamps [<xref ref-type="bibr" rid="pcbi.1006729.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>] and Iyer <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref011">11</xref>] that the method of characteristics can be used to bring <xref ref-type="disp-formula" rid="pcbi.1006729.e012">Eq 3</xref> into a different form. Consider a line segment <italic>l</italic> in state space, and pick a point <inline-formula id="pcbi.1006729.e014"><alternatives><graphic id="pcbi.1006729.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>x</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>l</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>≡</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> at <italic>t</italic> = 0. The system of ordinary differential equations <xref ref-type="disp-formula" rid="pcbi.1006729.e003">Eq 1</xref> defines a curve that describes the evolution of point <inline-formula id="pcbi.1006729.e015"><alternatives><graphic id="pcbi.1006729.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> through state space. This curve is an integral curve of the vector field <inline-formula id="pcbi.1006729.e016"><alternatives><graphic id="pcbi.1006729.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and can be found by integration. Writing this curve as <inline-formula id="pcbi.1006729.e017"><alternatives><graphic id="pcbi.1006729.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mi>v</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we can introduce a new coordinate system:
<disp-formula id="pcbi.1006729.e018"><alternatives><graphic id="pcbi.1006729.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>v</mml:mi> <mml:mo>→</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mo>=</mml:mo> <mml:mrow><mml:mi>v</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>t</mml:mi> <mml:mo>→</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>In this new coordinate system <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> becomes:
<disp-formula id="pcbi.1006729.e019"><alternatives><graphic id="pcbi.1006729.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mn>1</mml:mn> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mn>1</mml:mn> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:msubsup><mml:mi>v</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
which has the form of a Poisson master equation. This implies that rather than solving the partial integro-differential equation <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref>, we have to solve the system of ordinary differential equations <xref ref-type="disp-formula" rid="pcbi.1006729.e019">Eq 5</xref>. This system describes mass transport from bin to bin and no longer has a dependency on the gradient of the density profile: the drift term in <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> has been transformed away. <xref ref-type="disp-formula" rid="pcbi.1006729.e019">Eq 5</xref> describes mass transport from one position to another. The distance between these positions is now immaterial and this means that arbitrarily large synaptic efficacies can be handled.</p>
<p>The observation that for a system that co-moves with the neural dynamics all mass transport is determined by the stochastic process is important. It suggests that the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e019">Eq 5</xref>—representing the master equation of a Poisson process—can be replaced by more general forms without affecting the left-hand side of the equation that allows use of the method of characteristics. Indeed, recently we have considered a generalization to spike trains generated by non-Markov processes [<xref ref-type="bibr" rid="pcbi.1006729.ref023">23</xref>]. This generalizes the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref>, but leaves the left-hand side unchanged, and in [<xref ref-type="bibr" rid="pcbi.1006729.ref023">23</xref>] we show explicitly that for one dimensional densities the method discussed here extends to non-Markov renewal processes. The generalization of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> requires a convolution over the recent history of the density, using a kernel whose shape is dependent on the renewal process.</p>
<p>Consider a two dimensional state space with coordinates <italic>v</italic> and <italic>w</italic>. The coordinate transformation just described defines a mapping from point <italic>x</italic> on a line segment of initial points to a point in state space:
<disp-formula id="pcbi.1006729.e020"><alternatives><graphic id="pcbi.1006729.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">M</mml:mi> <mml:mo>:</mml:mo> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>→</mml:mo> <mml:mo>(</mml:mo> <mml:mi>v</mml:mi> <mml:mo>,</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
This has two implications: first, the evolution of the initial line segment <italic>l</italic> over a given fixed period of time defines a region of state space. The state space relevant to a simulation may have to be built from several such regions. Second, the mapping is time-dependent: <xref ref-type="disp-formula" rid="pcbi.1006729.e018">Eq 4</xref> must be solved in a coordinate system that itself is subject to dynamics: that of the deterministic neuron. This suggests a solution consisting of two interleaved steps: one accounting for deterministic movement of neurons, and one where <xref ref-type="disp-formula" rid="pcbi.1006729.e019">Eq 5</xref> is solved numerically. We will now describe this process in detail.</p>
<sec id="sec003">
<title>State space models of neuronal populations</title>
<p>As an example, we consider a conductance based model with first order synaptic kinetics following [<xref ref-type="bibr" rid="pcbi.1006729.ref020">20</xref>]. It is given by:
<disp-formula id="pcbi.1006729.e021"><alternatives><graphic id="pcbi.1006729.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>τ</mml:mi> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>V</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>e</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mtext>e</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
<disp-formula id="pcbi.1006729.e022"><alternatives><graphic id="pcbi.1006729.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mtext>e</mml:mtext></mml:msub> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>e</mml:mtext></mml:msub></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>e</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mtext>syn</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
Numerical values are taken from [<xref ref-type="bibr" rid="pcbi.1006729.ref020">20</xref>], and given in <xref ref-type="table" rid="pcbi.1006729.t001">Table 1</xref>. <italic>I</italic><sub>syn</sub>(<italic>t</italic>) represents the influence of incoming spikes on the neurons. A conventional representation of such a model is given by a vector field, see <xref ref-type="fig" rid="pcbi.1006729.g001">Fig 1</xref>.</p>
<table-wrap id="pcbi.1006729.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.t001</object-id>
<label>Table 1</label>
<caption>
<title>Constants taken from Apfaltrer et al. (2006), Appendix B.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006729.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="left">membrane time constant</td>
<td align="center"><italic>τ</italic><sub><italic>m</italic></sub></td>
<td align="right">20 ms</td>
</tr>
<tr>
<td align="left">reset potential</td>
<td align="center"><italic>E</italic><sub>r</sub></td>
<td align="right">-65 mV</td>
</tr>
<tr>
<td align="left">reversal potential</td>
<td align="center"><italic>E</italic><sub>rev</sub></td>
<td align="right">-65 mV</td>
</tr>
<tr>
<td align="left">equilibrium potential <italic>e</italic></td>
<td align="center"><italic>E</italic><sub>e</sub></td>
<td align="right">0 V</td>
</tr>
<tr>
<td align="left">synaptic time constant <italic>e</italic></td>
<td align="center"><italic>τ</italic><sub>s</sub></td>
<td align="right">5 mV</td>
</tr>
<tr>
<td align="left">threshold potential</td>
<td align="center"><italic>V</italic><sub>th</sub></td>
<td align="right">-55 mV</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1006729.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g001</object-id>
<label>Fig 1</label>
<caption>
<title/>
<p>A: Vector field for a conductance based model, along with a few integral curves. At very low conductance, there is a drive towards equilibrium regardless of the initial point. At higher conductance values the drive is dominated by a trend towards the equilibrium potential of the excitatory synapse (0 mV). The blue integral curves demonstrate this. The red integral curve represents a neuron that hits the threshold potential (-55 mV), and subsequently undergoes a reset to the reset potential (-65 mV). This neuron will emit a spike. After reset, it will not hit threshold again and eventually asymptotes to equilibrium potential (-60 mV). B An example grid for the conductance based model. The grid is built from strips. Strip numbers are arbitrary, as long as they are unique, but it is convenient to number them in order of creation. By construction, cell numbers within a strip are ordered by the dynamics: neurons that are in cell number <italic>j</italic> of strip <italic>i</italic> at time <italic>t</italic> are in cell number <italic>j</italic> + 1 mod <italic>n<sub>j</sub></italic> of strip <italic>j</italic> at time <italic>t</italic> + Δ<italic>t</italic>, where <italic>n</italic><sub><italic>j</italic></sub> is the number of cells in that strip.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g001" xlink:type="simple"/>
</fig>
<list list-type="bullet">
<list-item>
<p>A number of initial points are taken:
<disp-formula id="pcbi.1006729.e023"><alternatives><graphic id="pcbi.1006729.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">I</mml:mi> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mi>V</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mtext>min</mml:mtext></mml:msub> <mml:mo>;</mml:mo> <mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mi>i</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>g</mml:mi> <mml:mo>∣</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>,</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
for given fixed <italic>V</italic><sub>min</sub>, <italic>n</italic><sub><italic>g</italic></sub>, Δ<italic>g</italic></p>
</list-item>
</list>
<p>Consider a two dimensional dynamical system defined by a vector field. A point in state space will be represented by a two dimensional vector <inline-formula id="pcbi.1006729.e024"><alternatives><graphic id="pcbi.1006729.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. A grid is constructed from strips. As mentioned previously, usually one dimension is a membrane potential, and we will denote coordinates in this dimension by a small letter <italic>v</italic>. The second dimension can be used to represent parameters such as adaptation, conductance, and will be represented by <italic>w</italic>. A strip is constructed by choosing two neighbouring points in state space, e.g. <inline-formula id="pcbi.1006729.e025"><alternatives><graphic id="pcbi.1006729.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and integrating the vector field for a time <italic>T</italic> that is assumed to be an integer multiple of a period of time Δ<italic>t</italic>, which we assume to be a defining characteristic of the grid. Let <italic>T</italic> = <italic>n</italic>Δ<italic>t</italic>, then two discretized neighbouring characteristics
<disp-formula id="pcbi.1006729.e026"><alternatives><graphic id="pcbi.1006729.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">S</mml:mi> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mi>n</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mi>n</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
define a strip. Within a strip, the set of points
<disp-formula id="pcbi.1006729.e027"><alternatives><graphic id="pcbi.1006729.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">C</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mi>i</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mi>i</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
defines a cell, which is quadrilateral in shape. The quadrilateral should be <italic>simple</italic>, but not necessarily <italic>convex</italic> (<xref ref-type="fig" rid="pcbi.1006729.g002">Fig 2A</xref>). We reject cells with less than a certain area. As we will see in concrete examples, boundaries in state space are approached through areas of vanishing measure. The area cut tends to remove complex cells, and we will reject them in general. An example of a grid generated by this procedure is given in <xref ref-type="fig" rid="pcbi.1006729.g003">Fig 3</xref>.</p>
<fig id="pcbi.1006729.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g002</object-id>
<label>Fig 2</label>
<caption>
<title/>
<p>A: As a result of the integration procedure simple quadrilaterals (left, middle) should emerge, which are usually convex (left), except near stationary points or limit cycles where concave quadrilaterals (middle) can be formed. Complex, i.e. self-intersecting, quadrilaterals can occur around strong non linearities, for example the crossing of nullclines. These definitions hold for any polygon. B: The problem of defining the Master equation: we can easily calculate how much mass per unit time leaves a given bin (<italic>i</italic>, <italic>j</italic>). This mass will reappear at a position <italic>h</italic> away from the original bin, where <italic>h</italic> is the synaptic efficacy. In the figure bin (13,7) is translated along vector (0, 0.1). This corresponds to neurons that have received an input spike, and therefore are experiencing a jump in conductance. Most neurons that are in bin (13,7), will end up in bin (13, 5) and (14,4), with some in bin (13,6) and (14,6). So <italic>C</italic><sub>(0.,0.1)</sub>(13, 7) = {(13, 5), (14, 5), (13, 6), (14, 6)}. C: Some events will end up outside of the grid after translation. D: Fiducial quadrilaterals can be used to test where they have gone missing, and where is the best place to reassign them to the grid.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006729.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Probability mass is maintained in a mass array.</title>
<p>In general, mass does not move, except when the mass has moved beyond the end of a strip. The relationship between the mass array and the mesh is updated with each time step, resulting in the apparent motion of probability mass through the mesh (top left and top right). At the end of each simulation step, probability mass is removed from each first bin of the strip, and added to a special quadrilateral (bottom): the reversal bin. Mass does not move from here, and only synaptic input can cause mass to leave this bin.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g003" xlink:type="simple"/>
</fig>
<p>Strip numbers are arbitrary, as long as they are unique, but it is convenient to number them in order of creation. In the remainder of the paper, we will assume that strip numbers created by the integration procedure start at 1, and are consecutive, so that the numbers <italic>i</italic>∈ {1, ⋯, <italic>N</italic><sub><italic>strip</italic></sub>} with <italic>N</italic><sub><italic>strip</italic></sub> the number of strips, each identify a unique strip. Strip no. 0 is reserved for stationary points. There may be 0 or more cells in strip 0. The number of cells in strip <italic>i</italic> is denoted by <italic>n</italic><sub><italic>cell</italic></sub>(<italic>i</italic>). We refer to the tuple (<italic>i</italic>, <italic>j</italic>), with <italic>i</italic> the strip number and <italic>j</italic> the cell number, as the <italic>coordinates</italic> of the bin in the grid. <italic>N</italic><sub><italic>cells</italic></sub> is the total number of cells in the grid.</p>
<p>For all strips <italic>i</italic> (<italic>i</italic> &gt; 0 by construction), cell numbers within a strip are ordered by the dynamics: neurons that are in cell number <italic>j</italic> of strip <italic>i</italic> at time <italic>t</italic> are in cell number <italic>j</italic> + 1 mod <italic>n<sub>j</sub></italic> of strip <italic>j</italic> at time <italic>t</italic> + Δ<italic>t</italic>, where <italic>n</italic><sub><italic>j</italic></sub> is the number of cells in that strip.</p>
<p>Neurons that are in a cell in strip no. 0 are assumed to be stationary and do not move through the strip. Examples of cells in this strip are reversal bins. The handling of stationary bins will be discussed below.</p>
</sec>
<sec id="sec004">
<title>Representing a density profile</title>
<p>A simulation progresses in multiple steps of Δ<italic>t</italic>, so the current simulation time <italic>t</italic><sub>sim</sub> is specified by an integer <italic>k</italic>, defined by:
<disp-formula id="pcbi.1006729.e028"><alternatives><graphic id="pcbi.1006729.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>t</mml:mi> <mml:mtext>sim</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
The density profile can be represented in an array <inline-formula id="pcbi.1006729.e029"><alternatives><graphic id="pcbi.1006729.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> of length <italic>N</italic><sub><italic>cells</italic></sub>. Each element of this array is associated with the grid as follows. Let <italic>c</italic><sub><italic>cell</italic></sub>(0) ≡ 0 and for 0 &lt; <italic>i</italic> ≤ <italic>N</italic><sub><italic>strip</italic></sub> let <italic>c</italic><sub><italic>cell</italic></sub>(<italic>i</italic>) ≡ <italic>c</italic><sub><italic>cell</italic></sub>(<italic>i</italic> − 1) + <italic>n</italic><sub><italic>cell</italic></sub>(<italic>i</italic> − 1), so <italic>c</italic><sub><italic>cell</italic></sub>(<italic>i</italic>) represents the total number of cells in all strips up to strip <italic>i</italic>. Now define the index function <inline-formula id="pcbi.1006729.e030"><alternatives><graphic id="pcbi.1006729.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mi mathvariant="script">I</mml:mi></mml:math></alternatives></inline-formula>:
<disp-formula id="pcbi.1006729.e031"><alternatives><graphic id="pcbi.1006729.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="2pt"/><mml:mtext>mod</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> </mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
This is a time dependent mapping: its effect is a forward motion of probability mass with each forward time step. We will refer to the updating of the mapping by incrementing <italic>k</italic> as a <italic>mass rotation</italic> as probability mass that reaches the end of a strip, will reappear at the beginning of the strip at the next time step. This effect is almost always undesirable as it would effect a jump wise displacement of probability mass. In most models this can be prevented by removing the probability mass from the beginning of each strip and setting the content of this bin to 0, and adding the removed mass to a another bin. A typical example arises in the case of integrate-and-fire models. Here, there is usually a reversal point. Such a point can be emulated by creating a small quadrilateral, and making this cell number 0 in strip number 0.</p>
<p>The procedure of mapping probability mass from the beginning of a strip to special bins in state space is called a <italic>reversal mapping</italic>. It consists of a list of coordinate pairs. The first coordinate labels the bin where probability will be removed, the second coordinate labels the bin where the probability will reappear. The concept of reversal mapping extends to other neural models—we will consider adaptive-exponential-integrate-and-fire (AdExp), Fitzhugh-Nagumo, and quadratic-integrate-and-fire neurons. All of these models need a prescription for what happens with the probability mass after reaching the end of a strip, and we will refer to this as the reversal mapping, even if the model does not really have a reversal bin, to contrast it from the <italic>threshold mapping</italic>. Although handling a threshold is similar, interaction with synaptic input means that the mapping requires extra precautions. We will discuss this in the section below.</p>
<p>The whole process of advancing probability through a grid by means of updating a relationship with a grid is illustrated in <xref ref-type="fig" rid="pcbi.1006729.g003">Fig 3</xref>. Up to this point we have only referred to probability mass. If a density representation is desired, one can calculate the density by:
<disp-formula id="pcbi.1006729.e032"><alternatives><graphic id="pcbi.1006729.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="script">M</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi mathvariant="script">A</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <inline-formula id="pcbi.1006729.e033"><alternatives><graphic id="pcbi.1006729.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi mathvariant="script">A</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the area of quadrilateral (<italic>i</italic>, <italic>j</italic>), and <inline-formula id="pcbi.1006729.e034"><alternatives><graphic id="pcbi.1006729.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mi mathvariant="script">M</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the probability mass present in the quadrilateral (<italic>i</italic>, <italic>j</italic>) at simulation time <italic>k</italic>Δ<italic>t</italic>. We note that this procedure implements a complete numerical solution for the advective part of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref>.</p>
</sec>
<sec id="sec005">
<title>Handling synaptic input</title>
<p>We will assume that individual neurons will receive Poisson spike trains with a known rate for a known synaptic distribution of the post synaptic population. Without loss of generality we will limit the exposition to a single fixed synaptic efficacy; continuous distributions can be sampled by generating several matrices, one for each synaptic efficacy, and adding them together. Adding the individual matrices, which are band matrices, and very sparse, results in another band matrix, still sparse, albeit with a slightly broader band. Overall run times are hardly affected unless really broad synaptic distributions are sampled.</p>
<p>A connection between two populations will be defined by the tuple (<italic>N</italic><sub><italic>con</italic></sub>, <italic>h</italic>, <italic>τ</italic><sub><italic>delay</italic></sub>). Here <italic>N</italic><sub><italic>con</italic></sub> is the number of connections from presynaptic neurons onto a representative neuron in the receiving population, <italic>τ</italic><sub>delay</sub> the delay in the transmission of presynaptic spikes and <italic>h</italic> the synaptic efficacy. The firing rate <italic>ν</italic> is either given, or inferred from the state of the presynaptic population, but in both cases assumed to be known. For the population these assumptions lead to a Master equation:
<disp-formula id="pcbi.1006729.e035"><alternatives><graphic id="pcbi.1006729.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∫</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>{</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>h</mml:mi></mml:msub></mml:msub> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>v</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo> <mml:mspace width="4pt"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>V</italic> is an area of state space and <italic>V</italic><sub><italic>h</italic></sub> the same area, translated by an amount <italic>h</italic> in dimension <italic>i</italic>. It is dependent on the neuronal model in which variable the jump takes place. In AdExp the jump is in membrane potential, in conductance based models it is in the conductance variable. Here, we will discuss the problem using conductance based neurons as an example, but the methodology applies to any model.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1006729.e035">Eq (11)</xref> determines the right hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq (2)</xref>, and the stage is set for numerical solution. The left hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq (2)</xref> describes the advective part, and is purely determined by the neuron model, which ultimately determines the grid. We already have described the movement of probability mass due to advection during a time step Δ<italic>t</italic>, and need to complete this by implementing a numerical solution for <xref ref-type="disp-formula" rid="pcbi.1006729.e035">Eq (11)</xref>.</p>
<p><xref ref-type="disp-formula" rid="pcbi.1006729.e035">Eq (11)</xref> describes the transfer of probability mass from one region of state space to another. We will assume that the grid we use for the model of advection is sufficiently fine, so that the density within a single bin can be considered to be constant, and choose area <italic>V</italic> in <xref ref-type="disp-formula" rid="pcbi.1006729.e035">Eq (11)</xref> to coincide with our grid bins. We approximate (<xref ref-type="disp-formula" rid="pcbi.1006729.e035">11</xref>) by:
<disp-formula id="pcbi.1006729.e036"><alternatives><graphic id="pcbi.1006729.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>{</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>h</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msub> <mml:mi mathvariant="script">M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
The bin (<italic>i</italic>, <italic>j</italic>) translated by a distance <italic>h</italic> will cover a number of other bins of the grid. Let (<italic>p</italic>, <italic>q</italic>) be a bin partly covered by the translated bin (<italic>i</italic>, <italic>j</italic>) and let <italic>α</italic><sub><italic>p</italic>,<italic>q</italic></sub> be the fraction of the surface area of the translated bin that covers bin (<italic>p</italic>, <italic>q</italic>). (By construction 0 &lt; <italic>α</italic><sub><italic>p</italic>,<italic>q</italic></sub> ≤ 1.) The set <italic>C</italic><sub><italic>h</italic></sub>(<italic>i</italic>, <italic>j</italic>) is defined as the set of tuples (<italic>p</italic>, <italic>q</italic>), for all such bins, i.e. those bins that are covered by translated bin (<italic>i</italic>, <italic>j</italic>) (and no others). We will refer to <italic>C</italic><sub><italic>h</italic></sub>(<italic>i</italic>, <italic>j</italic>) as the <italic>displacement set</italic>. Usually, the displacement is in one dimension only, where this is not the case we will write <inline-formula id="pcbi.1006729.e037"><alternatives><graphic id="pcbi.1006729.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The problem of determining <italic>C</italic><sub><italic>h</italic></sub>(<italic>i</italic>, <italic>j</italic>) is one of computational geometry that can be solved before simulation starts. It is illustrated in <xref ref-type="fig" rid="pcbi.1006729.g002">Fig 2B</xref>, where the grid of the conductance based model is shown.</p>
<p>This problem is easily stated but hard to solve efficiently. Conceptually, a Monte Carlo approach is simplest, and since the computation can be done offline—before simulation starts—this approach is preferable. It is straightforward for a given bin of the grid (<italic>i</italic>, <italic>j</italic>) to generate random points that are contained within its quadrilateral. Assume these points are translated by a vector <inline-formula id="pcbi.1006729.e038"><alternatives><graphic id="pcbi.1006729.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. It is now a matter of determining in which bin a translated point falls. In order to achieve this the grid is stored as a list of cells. Each cell, being a quadrilateral, is represented by a list of four coordinates. During construction of the grid, vertices of a cell are stored in counter clockwise order.</p>
<p>When a quadrilateral is convex, and the vertices are stored in counter clockwise order, the × operator defined by:
<disp-formula id="pcbi.1006729.e039"><alternatives><graphic id="pcbi.1006729.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>×</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>≡</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
results in an “inward” pointing normal <inline-formula id="pcbi.1006729.e040"><alternatives><graphic id="pcbi.1006729.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mover accent="true"><mml:mi>n</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. If the position vector of a point has a positive scalar product with the ‘inward’ normal of all four line segments that define the quadrilateral the point is inside, otherwise it is outside. These half line tests are cheap and easy to implement. If the quadrilateral is not convex, but simple, it can be split into two triangles which are convex.</p>
<p>We perform linear search to find a grid cell that contains the translated point, or to conclude there is no such cell. Better efficiency can be obtained with k-d trees, but we have found the generation of translation matrices not to be a bottleneck in our workflow, and linear search allows straightforward brute force parallelization. At most one cell will contain the translated point. For now, we will assume that the translated point will be inside a given bin (<italic>p</italic>, <italic>q</italic>). Later, for concrete neuron models we will discuss specific ways of handling transitions falling outside the grid. If bin (<italic>p</italic>, <italic>q</italic>) is not represented in <inline-formula id="pcbi.1006729.e041"><alternatives><graphic id="pcbi.1006729.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, an entry for it will be added to it. The process is then repeated, in total <italic>N</italic><sub><italic>point</italic></sub> times. For each cell (<italic>p</italic>, <italic>q</italic>) represented in <inline-formula id="pcbi.1006729.e042"><alternatives><graphic id="pcbi.1006729.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> a count <italic>n</italic><sub>(<italic>p</italic>,<italic>q</italic>)</sub> is maintained and <italic>α</italic><sub><italic>p</italic>,<italic>q</italic></sub> is estimated by:
<disp-formula id="pcbi.1006729.e043"><alternatives><graphic id="pcbi.1006729.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>o</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula> <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> is of the form
<disp-formula id="pcbi.1006729.e044"><alternatives><graphic id="pcbi.1006729.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi mathvariant="script">M</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">T</mml:mi> <mml:mo>·</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1006729.e045"><alternatives><graphic id="pcbi.1006729.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mi mathvariant="script">T</mml:mi></mml:math></alternatives></inline-formula> is called the transition matrix. The displacement set determines the transition matrix.</p>
<p>Here, we have described a Monte Carlo strategy that uses serial search to determine the set <inline-formula id="pcbi.1006729.e046"><alternatives><graphic id="pcbi.1006729.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and consequently the constants <italic>α</italic><sub><italic>p</italic>,<italic>q</italic></sub> for bins (<italic>p</italic>, <italic>q</italic>) in that set. With these constants determined, it is a straightforward matter to solve <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> numerically.</p>
<p>The main algorithm now consists of three steps: updating the index relationship <xref ref-type="disp-formula" rid="pcbi.1006729.e031">Eq 9</xref>, which constitutes the movement of probability mass through the grid during a time interval Δ<italic>t</italic>; implementing the reversal mapping; solving <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> during Δ<italic>t</italic>. The order of these steps matters. Implementing the reversal bin after the master equation may lead to removing probability mass from the beginning of the strip that should have been mapped to a reversal bin.</p>
</sec>
<sec id="sec006">
<title>Handling a threshold</title>
<p>Many neuron models incorporate a threshold of some sort. For example, in the original conductance based model by [<xref ref-type="bibr" rid="pcbi.1006729.ref020">20</xref>], a threshold of -55 mV is applied. This corresponds to a vertical boundary in the (<italic>V</italic>, <italic>g</italic>) plane (see <xref ref-type="fig" rid="pcbi.1006729.g001">Fig 1</xref>). Neurons that hit this threshold from lower potentials generate a spike and are taken out of the system. After a period <italic>τ</italic><sub>ref</sub>, they are reintroduced at (<italic>V</italic><sub>reset</sub>, <italic>g</italic>(<italic>t</italic><sub>spike</sub> + <italic>τ</italic><sub>ref</sub>)), where <italic>t</italic><sub>spike</sub> is the time when the neuron hits the threshold, and <italic>g</italic>(<italic>t</italic><sub>spike</sub>) is the conductance value the neuron had at the time of hitting the threshold. In this model, following [<xref ref-type="bibr" rid="pcbi.1006729.ref020">20</xref>], it is assumed that the conductance variable continues to evolve according to <xref ref-type="disp-formula" rid="pcbi.1006729.e022">Eq 8</xref>, without being affected by the spike.</p>
<p>We handle this as follows. For each strip it is determined which cells contain the threshold boundary, i.e. at least one vertex lies below the threshold potential and at least one lies on or above the threshold potential. The set of all such cells is called the <italic>threshold set</italic>. In a similar way a <italic>reset</italic> set is constructed, the set of cells that contain the reset potential. In the simplest case, for each cell in the threshold set the cell in the reset set is identified that is closest in <italic>w</italic> to that of a threshold cell. The threshold cell is then mapped to the corresponding reset cell and the set of all such mappings is called the reset mapping.</p>
<p>Sometimes, the value of <italic>w</italic> is adapted after a neuron spikes. In the AdExp model, for example, <italic>w</italic> → <italic>w</italic> + <italic>b</italic> after a spike. In this case, we translate each cell in the reset set in direction (0, <italic>w</italic>), and calculate its displacement set, just as we did for the transition matrix. The reset mapping is then not implemented between the threshold cell and the original reset cell, but to the displacement set of that reset cell. We do this for all threshold cells and thus arrive at a slightly more complex reset mapping.</p>
<p>Due to the irregularity of the grid, it may happen that some transitions of the Master equation are into cells that are above the threshold potential. This will lead to stray probability above threshold, if not corrected. We correct for this during the generation of the transition matrix. If during event generation a point ends up above threshold after translation, we look for the closest threshold cell for this point. The event is then attributed to that threshold cell, and not the stray cell above threshold. In this way transitions from below or on threshold to cells above threshold are explicitly ruled out.</p>
<p>The reset mapping must be carried out immediately after the solution of the master equation, before the next update of the index function.</p>
</sec>
<sec id="sec007">
<title>Gaps in state space</title>
<p>All grids are finite. For that reason alone the Monte Carlo procedure described above will result in translated points that cannot be attributed to any cell. Those events are lost and will lead to unbalanced transitions: mass will flow out of bins near the edge, but will not reappear anywhere else in the system and there is a possibility that mass evaporates from the system. This problem does not occur just at the edges, but also in the vicinity of stationary points. We will see that some dynamical systems display strong non linearities that will make it impossible to cover state space densely. The ability to deal with such gaps in state space is the most important technical challenge for this method.</p>
<p>In <xref ref-type="fig" rid="pcbi.1006729.g002">Fig 2</xref> we show how to handle these gaps. <xref ref-type="fig" rid="pcbi.1006729.g002">Fig 2B</xref> shows that a cell which is translated by 5 mV can fall across a small cleft not part of the grid. We cover this gap by a quadrilateral (in green): a <italic>fiducial</italic> cell. An event that is not within the grid, but inside this quadrilateral needs to assigned to a mesh cell, otherwise the transition matrix will not conserve probability mass. It is straightforward to maintain a list of grid cells that have at least one vertex in the fiducial bin. We assign the event to the grid cell that is closest along the projection in the jump direction.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006729.g002">Fig 2D</xref> shows the total number of events lost in the generation of transition matrix corresponding to a jump of 5 mV, thereby revealing gaps in state space. The orange quadrilaterals are the fiducial bins. After reassignments all events fall inside the grid and probability will be balanced.</p>
</sec>
<sec id="sec008">
<title>Marginal distributions</title>
<p>It is straightforward to calculate marginal distributions. Again, we use Monte Carlo simulation to generate points inside a given quadrilateral (<italic>p</italic>, <italic>q</italic>). We then histogram these points in <italic>v</italic> and <italic>w</italic>. For each bin <italic>i</italic> in the <italic>v</italic> histogram, we can now estimate a matrix element <italic>α</italic><sub>(<italic>p</italic>,<italic>q</italic>),<italic>i</italic></sub> by dividing the number of points in bin <italic>i</italic> by the total number of points that were generated. For a given distribution, one can now multiply the total mass in bin (<italic>p</italic>, <italic>q</italic>) by <italic>α</italic><sub>(<italic>p</italic>,<italic>q</italic>),<italic>i</italic></sub> to find how much of this mass should be allocated to bin <italic>i</italic>. If one does this for every cell (<italic>p</italic>, <italic>q</italic>) in the grid, one will find the distribution of mass over the marginal histogram, and can calculate the marginal density from this.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Results/Discussion</title>
<p>We present a succession of population simulations of four neuronal models. A neuron with a single excitatory conductance has a simple state space, and its simulation provides few problems. It is a familiar model and therefore a good one to introduce and demonstrate the formalism. We then move to one dimensional results and replicate some familiar results for LIF and QIF neurons: density profiles, transient firing rates and gain curves. This allows us to quantitatively examine some of the strengths and weaknesses of the method. We then discuss two models that show a progression of difficulty in covering state space: AdExp and Fitzhugh-Nagumo. Although a methods paper, we feel that nonetheless we can infer a number of general principles that run as a common thread through our use cases, and we present them here.</p>
<list list-type="bullet">
<list-item>
<p>We obtain a general method for simulating populations of spiking point model neurons with a one or two dimensional state space, subject to Poisson spike trains. When restricted to one dimension, the method is equivalent to that published by de Kamps (2013) and Iyer <italic>et al</italic>. (2013) and is very efficient, as the work of Cain <italic>et al</italic>. demonstrates. The method is able to replicate earlier work on 2D models, but is more general, as first, it is able to accept novel models in the form of a grid file and therefore does not require source code changes when a new model is considered, and second, does not rely on the diffusion approximation, but allows a variety of stochastic processes to be considered. The method is most efficient for synaptic efficacies and firing rates commensurate with what is found in the brain, but can be pushed to reproduce diffusion results, although dedicated numerical strategies for solving the ensuing 2D Fokker-Planck equations will be more efficient. Nonetheless, the possibility to study the diffusion limit as a special case is a useful property of the method.</p>
</list-item>
<list-item>
<p>The method is insensitive to the gradient density, and will accurately model delta synapses and handle discontinuities of the density profile, and is able to model populations that are in partial synchrony, allowing the modelling of the decorrelation process itself.</p>
</list-item>
<list-item>
<p>The neural model will be presented in a file representation of a state space diagram. For some models it is hard to cover state space completely due to singularities, for example when approaching nullclines. Such parts of state space are effectively forbidden for endogenous deterministic neural dynamics, but noise may place events there, moving neurons outside state space. We find there are two cases where this happens: first, on the approach of one of the nullclines the system approaches a stable equilibrium or a limit cycle. The system does not contain enough information in one of the two dimensions and the grid cannot be meaningfully continued. We find that the motion of probability mass inside such a region can be inferred from the dynamics around it. A limit cycle, for example can be inferred from the grid closing onto it, even when we cannot extrapolate the grid directly to the limit cycle. In a similar way we can capture the motion of mass towards a stable equilibrium: when motion has stopped in one direction, but still continues in the other, we find that placing neurons that are deposited into accessible regions by synaptic input in nearby parts of state space accurately captures the overall motion of mass around these regions. Similar considerations apply around unstable regions of state space, and because we can time invert the dynamical system when constructing the grid, we find that these problems can be handled in much the same way.</p>
</list-item>
<list-item>
<p>Transient responses can be understood in geometrical terms. If a boundary, either a reflecting or an absorbing one, is present in state space, the population will exhibit a strong oscillatory response (“ringing”) when the input is strong enough to push neurons towards the boundary and noise is too weak to disperse neurons before reaching it. The converse is also true: if despite the presence of a boundary, state space allows neurons a way around it, strong transients will be absent. Rate based models based on first order differential equations on using a gain function will model these transients incorrectly, or not at all.</p>
</list-item>
<list-item>
<p>The method can describe the version of Tsodyks-Makram synapses used by Vasilaki and Giugliano [<xref ref-type="bibr" rid="pcbi.1006729.ref024">24</xref>] in a model of network formation.</p>
</list-item>
<list-item>
<p>By far the most challenging grid to make was that of a Fitzhugh-Nagumo neuron, because the approach to the limit cycle in part also implies an approach to the nullcines of the system, leading to a loss of information in one dimension. Where the nullclines cross this problem is exacerbated. We find that we have to imply the limit cycle: we define the grid in the approach to the limit cycle and infer the deterministic dynamics in an area around the limit cycle from the surrounding grid cells.</p>
</list-item>
</list>
<sec id="sec010">
<title>Conductance based neurons</title>
<p>We consider neurons with a single excitatory synapse as given by <xref ref-type="disp-formula" rid="pcbi.1006729.e022">Eq (8)</xref>. In <xref ref-type="fig" rid="pcbi.1006729.g004">Fig 4</xref> we present first the simulation of a jump response: a group of neurons is at rest at time <italic>t</italic> = 0 and all neurons are at (<italic>V</italic> = −65 mV, <italic>g</italic> = 0). From <italic>t</italic> = 0 onward the neurons will receive Poisson distributed input spike trains with a rate of 1000 Hz. A neuron that receives an input spike will undergo an instantaneous state transition and move up in conductance space. Until it receives a further input spike it will start to move through state space under its endogenous neural dynamics: the neuron will depolarize and simultaneously reduce its conductance. The process was described in Sec. <bold>Materials and Methods: State Space Models of Neuronal Populations</bold>.</p>
<fig id="pcbi.1006729.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>A: The evolution of the joint probability density function at four different points in time (1, 5, 15, 28 ms) in (<italic>V</italic>, <italic>g</italic>) space (<italic>V</italic> membrane potential, <italic>g</italic> conductance) for synaptic efficacy <italic>J</italic> = 0.05, Poisson generated input spike train with rate <italic>ν</italic> = 1000 spikes per second. B: The resulting population firing rate, calculated from the fraction of mass crossing threshold per unit time as a solid black line. Spiking neuron simulation results shown by red markers. Onset and resulting firing rates are in agreement throughout. Unlike one dimensional neural models, conductance based models produce almost no overshoot.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g004" xlink:type="simple"/>
</fig>
<p>The density is represented as a heat plot: the maximum density is white, lower density areas are shown as cooler colours from white through yellow to red. The color scale is logarithmic, so red areas represent very low probability. <xref ref-type="fig" rid="pcbi.1006729.g004">Fig 4A</xref>) shows the evolution of the density of a population that was at equilibrium at <italic>t</italic> = 0 at four points in time <italic>t</italic> = 1, 5, 15 and 28 ms by which time steady state has been reached. We see probability mass moving mainly upwards under the influence of incoming spike trains. We will see that the mass ‘rotates’ in the direction of the threshold; and finally a steady state is realized: a state where the density profile has become stationary. We also have simulated a group of 10000 neurons and modeled incoming Poisson spike trains for each one. We keep track of their position in (<italic>V</italic>, <italic>g</italic>) space and represent their state at a given time as points in state space. The cloud of points clearly tracks the white areas of the density. The shot noise structure is clearly visible in the band structure early in the simulation where neurons are present at multiples of the synaptic efficacy, reflecting that some neurons have sustained multiple hits by incoming spike trains.</p>
<p>As neurons are moving through threshold, they themselves emit a spike and contribute to the response firing rate of the population, defined as the fraction of the population that spikes per time interval, divided by that time interval. We can therefore calculate the response firing rate from the amount of mass moving through threshold per unit time. We show the jump response of the population as a plot of populating firing rate as a function of time in <xref ref-type="fig" rid="pcbi.1006729.g004">Fig 4B</xref>. The firing rate calculated from the density matches that calculated from the Monte Carlo simulation very well. Interestingly, there is almost no overshoot in the firing rate, as also noted by Richardson (2004), who studied this system using Fokker-Planck equations. Although we study shot noise, in the absence of a fundamental scale in the <italic>g</italic> direction, the central limit theorem ensures that the marginal distribution in <italic>g</italic> is Gaussian within a few milliseconds. It is clear that the population disperse in the <italic>g</italic> direction and drifts towards the threshold relatively slowly. The absence of a barrier allows the dispersal of the population before it hits threshold, greatly reducing any overshoot in the firing rate, which is quite unlike one dimensional neural models, as we shall see in Sec. <bold>Results: One Dimension</bold>.</p>
<p>Let us contrast this with a simulation where we introduce a maximum conductance <italic>g</italic><sub>max</sub> = 0.8, which for simplicity we assume to be voltage independent. This then introduces a reflecting boundary at <italic>g</italic> = <italic>g</italic><sub>max</sub>, and therefore introduces a scale by which an efficacy can judged to be small or large. As expected, probability mass is squashed against this boundary (<xref ref-type="fig" rid="pcbi.1006729.g005">Fig 5A</xref>) and has nowhere to go but laterally, in the direction of the threshold. Interestingly, the mass has not dispersed and clear groupings of mass huddled against the boundary can be observed. The traversal of the threshold by these groupings produces clear oscillations in the firing rate: a “ringing” effect. The firing rate jump response reflects the effect of the presence of a maximum conductance in state space.</p>
<fig id="pcbi.1006729.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g005</object-id>
<label>Fig 5</label>
<caption>
<title/>
<p>A: The density at <italic>t</italic> = 15 ms. Events are reflected against a reflecting conductance boundary. B: Gain curves for different input rates and synaptic efficacies. The maximum conductance clearly affects the shape of the gain curves, although at input rate 3 kHz for <italic>J</italic> = 1 mV the effect is moderate. C: The transient looks very different in the case where a maximum conductance is present (red): the “ringing effect” is much stronger, compared to the case without maximum (black), while the overall firing rates do not differ greatly. The cause can be seen in A: neurons have not had time to disperse before they are forced across threshold; clear groupings can be seen at the maximum conductance.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g005" xlink:type="simple"/>
</fig>
<p>We run two simulations: one with and one without maximum conductance, but otherwise identical, and repeat this experiment for two different synaptic efficacies: <italic>J</italic> = 1 and 3 mV. Both simulations use an input rate of 3 kHz. In the case of no maximum conductance, probability mass can disperse in the <italic>g</italic> direction and mostly does so before arriving at the threshold. In <xref ref-type="fig" rid="pcbi.1006729.g005">Fig 5</xref> one sees that the introduction of a maximum conductance leads to a reduced response firing rate for high inputs. This can be interpreted as the population unable to respond to an increase of input once the majority of its ion channels are already open. <xref ref-type="fig" rid="pcbi.1006729.g005">Fig 5</xref> shows that the firing rates of Monte Carlo simulations and our method agree over the entire range of input.</p>
<p>Even when the effects on the response firing rate are moderate, the transient dynamics can be radically different. For an efficacy <italic>J</italic> = 1 mV and and input rate <italic>ν</italic><sub>in</sub> = 3 kHz, the firing rates for maximum conductance, compared to no maximum come out as 175 Hz vs 195 Hz. In <xref ref-type="fig" rid="pcbi.1006729.g005">Fig 5C</xref> we show the response firing rate as a function of time. The result for the unrestrained conductance is given by the red line, which despite the high output firing rate still almost produces no overshoot. When we restrict the maximum conductance we see a somewhat reduced firing rate but a pronounced transient response (“ringing”) which persists much longer than for an unrestrained conductance. It is striking to see that the reintroduction of a barrier in state space results in pronounced transients. In both cases, the calculated firing rates agree well with Monte Carlo simulation. We attribute this ringing to a geometrical effect: the introduction of a barrier in the direction of where the stochastic process is pushing neurons.</p>
</sec>
<sec id="sec011">
<title>One dimension: Leaky- and quadratic-integrate-and-fire neurons and size effects on the transition matrix</title>
<p>Although these model neurons are characterized by a single dimension—the membrane potential—they can be viewed as a two dimensional model that is realized in a single strip, and where transitions take place between one bin in potential space to another. This is completely equivalent—in implementation and concept—to the geometric binning method introduced independently by de Kamps [<xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>] and Iyer <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref011">11</xref>], with one exception: the generation of transition matrices by Monte Carlo. In one dimension it is not necessary to use Monte Carlo generation: the transition matrix elements can be calculated to an arbitrary precision because in one dimension the geometrical problem outlined in Sec. <bold>Materials and Methods: Handling Synaptic Input</bold> is much simpler and can be solved by linear search. It is clear that unlike the 2D case, it is straightforward to find the exact areas covered by translated bins, and hence no Monte Carlo generation process is required.</p>
<p>Nevertheless, it is interesting to use this method. The transition matrix generation for the 2D case is relatively expensive, and as precision scales with the square root of the number of events it is interesting to see how few we can use in practice without distorting our results. The answer is: surprisingly few. As benchmark we set up a population of LIF neurons with membrane constant <italic>τ</italic> = 50 ms, following [<xref ref-type="bibr" rid="pcbi.1006729.ref005">5</xref>], and assume that each neuron receives Poisson distributed spike trains with a rate <italic>ν</italic> = 800 Hz. We assume delta synapses, i.e. an instantaneous jump in the postsynaptic potential by a magnitude <italic>h</italic> = 0.03, with the membrane potential <italic>V</italic>∈ [−1, 1), i.e. we use a rescaled threshold potential <italic>V</italic> = 1. The grid is generated with a time step Δ<italic>t</italic> = 0.1 ms, and is shown in <xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6B</xref>.</p>
<fig id="pcbi.1006729.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g006</object-id>
<label>Fig 6</label>
<caption>
<title/>
<p>A: Characteristics for leaky- (left) and quadratic-integrate-and-fire (right) neurons. B: The resulting grids for both neuron types. C: Typical steady state densities, strongly influenced by the shape of the grid, and ultimately the neural dynamics. D: a typical jump response of the firing rate. For comparable output frequencies, QIF neurons “ring” for longer, which we attribute to a closer grouping of probability mass.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g006" xlink:type="simple"/>
</fig>
<p>The simulation results are shown in <xref ref-type="fig" rid="pcbi.1006729.g007">Fig 7</xref> and replicates earlier work [<xref ref-type="bibr" rid="pcbi.1006729.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>]. The use of a finite number of points in the Monte Carlo process used for the generation of transition matrices generates random fluctuations with respect to the true values. The effect of these fluctuations is clearly visible in the shape of the density profile, and only for <italic>N</italic><sub><italic>point</italic></sub> = 10000 the profile is as smooth as in earlier results where we calculated the transition matrix analytically. How bad is this? To put these fluctuations into perspective, we used a direct simulation of 10000 spiking neurons and histogrammed their membrane potential at a simulation time well after <italic>t</italic> = 0.3 s, so that they can be assumed to sample the steady state distribution. In the figure, they have been indicated by red markers. Comparing the results we see that the fluctuations for <italic>N</italic><sub><italic>point</italic></sub> = 10 are comparable to those of a Monte Carlo simulation using a sizeable population of 10000 neurons. Moreover, in the population firing rates the finite size effects are almost invisible. This is somewhat surprising, but a consideration of the underlying process that generates the firing rate explains this. Neurons are introduced at equilibrium and will undergo several jumps before they reach threshold. The finite size effects of the Monte Carlo process induce variations in those jumps in different regions of state space, but these fluctuations are unbiased and will average out over a number of jumps. So neurons will experience variability in the time they reach threshold, but this variability does not come in the main from fluctuations in the transition matrix elements. It should be emphasized that the transition matrices are a quenched source of randomness, because transition matrices are fixed before the simulation starts. So although ultimately caused by finite size effects, their contribution is different compared to the unquenched finite size effects that can be seen in the population of 10000 neurons.</p>
<fig id="pcbi.1006729.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g007</object-id>
<label>Fig 7</label>
<caption>
<title/>
<p>A: The steady state density as a function of membrane potential. B: The firing rate as a function of time, for transition matrices that were generated for different values of <italic>N</italic><sub><italic>point</italic></sub>, the number of events used in the Monte Carlo generation of transition matrices.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g007" xlink:type="simple"/>
</fig>
<p>It is instructive to look at some examples because it highlights strengths and weaknesses of the method in terms of familiar results. In <xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6A</xref>, the characteristics of both neural models are given. In <xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6B</xref> the state space of LIF (left) and QIF neurons (right) are shown, at lower resolution than used in simulation to elucidate the dynamics. Rather than with numbers which would be unreadable at this scale, we indicate the direction in which cell numbers increase, and therefore the direction in which neural mass will move, by arrows. One can see that the LIF neuron is comprised of two strips, and the QIF neuron of three, where the arrows indicate in which direction the cell numbers are increasing. In the LIF grid, there is one stationary bin, in the QIF there are two. They are represented as separate stationary cells, covering the space between the strips, indicated by the blue downward pointing arrows.</p>
<p>In <xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6C</xref> we consider the steady state of LIF (left) and QIF neurons (right) after being subjected to a jump response of Poisson distributed spike trains starting at <italic>t</italic> = 0 (LIF: <italic>ν</italic><sub><italic>in</italic></sub> = 800 spikes/s <italic>J</italic> = 0.03 (normalized w.r.t. threshold; QIF: <italic>J</italic> = 0.05)). The shape of the characteristics and therefore of the grid clearly reflect their influence on the steady state density distribution. The output firing rate (<xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6D</xref>) shows the clear “ringing” in the transient firing rate that is mostly absent in conductance based neurons. Again, this can be interpreted geometrically: the stochastic process pushes neurons in the direction of a threshold, but they reach it without having had the opportunity to disperse. Decorrelation only happens after most neurons have gone through threshold at least once. It is also interesting to see that for comparable firing rates the ringing is much stronger for QIF than for LIF neurons. We also interpret this as a geometrical effect: the effective threshold for QIF neurons is <italic>V</italic> = 3 (normalized units), not 10, as neurons with a membrane potential above 3 will spike. It is clear from <xref ref-type="fig" rid="pcbi.1006729.g006">Fig 6D</xref> that compared to LIF neurons, QIF neuron bulk up close to the threshold and are constrained more than their LIF counterparts, thereby making it harder to decorrelate before passing threshold.</p>
<p>For reference, in <xref ref-type="fig" rid="pcbi.1006729.g008">Fig 8</xref> we show that the method accurately reproduces results from the diffusion limit, as well as generalizes correctly beyond it. If one uses a single Poisson spike train to emulate a Gaussian white noise input, employing the relationship:
<disp-formula id="pcbi.1006729.e047"><alternatives><graphic id="pcbi.1006729.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>μ</mml:mi></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ν</mml:mi> <mml:mtext>in</mml:mtext></mml:msub> <mml:mi>J</mml:mi> <mml:mi>τ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>ν</mml:mi> <mml:mtext>in</mml:mtext></mml:msub> <mml:msup><mml:mi>J</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>τ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
one can use our method to predict the steady state firing rates as a function of <italic>J</italic>, the synaptic efficacy and <italic>ν</italic><sub>in</sub> the rate of the Poisson process for given membrane constant <italic>τ</italic>. Organizing the results in terms of <italic>μ</italic> and <italic>σ</italic>, as given by <xref ref-type="disp-formula" rid="pcbi.1006729.e047">Eq 14</xref>, one expects a close correspondence for low <italic>σ</italic>, since <xref ref-type="disp-formula" rid="pcbi.1006729.e047">Eq 14</xref> leads to small values of <italic>J</italic> compared to threshold. One expects deviations at high <italic>σ</italic>, where <italic>J</italic> does not come out small. <xref ref-type="fig" rid="pcbi.1006729.g008">Fig 8</xref> shows that this is indeed the case when firing rates are compared to analytic results obtained in the diffusion approximation. Our method produces the correct deviations from the diffusion approximation results, and agrees with Monte Carlo simulation. Elsewhere [<xref ref-type="bibr" rid="pcbi.1006729.ref012">12</xref>], we have shown that diffusion results can be accurately modeled using two Poisson rates for high <italic>σ</italic>.</p>
<fig id="pcbi.1006729.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g008</object-id>
<label>Fig 8</label>
<caption>
<title/>
<p>A: Gain curve for quadratic-integrate-and-fire neurons. Population density techniques handle deviations from the diffusion approximation correctly: when one tries to emulate Gaussian white noise with a single Poisson spike train input, deviations are expected at high values of <italic>σ</italic> as synaptic efficacies are forced to be large. The dashed lines give the diffusion approximation, black markers the prediction by our method and red bars Monte Carlo results, which agree with each other, but deviate from the diffusion prediction. B: The frequency spectrum shows the expected <inline-formula id="pcbi.1006729.e048"><alternatives><graphic id="pcbi.1006729.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>ω</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:math></alternatives></inline-formula> dependency for high frequencies. C: the delta peak of a coherently firing group of neurons in correctly represented; the decrease in partial synchrony of the population is modeled correctly over long times.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g008" xlink:type="simple"/>
</fig>
<p>In <xref ref-type="fig" rid="pcbi.1006729.g008">Fig 8B</xref> we replicate the gain spectrum for QIF neurons and show that the high frequency dependence falls off as <inline-formula id="pcbi.1006729.e049"><alternatives><graphic id="pcbi.1006729.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>ω</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:math></alternatives></inline-formula> as predicted by Fourcaud-Trocmé <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref025">25</xref>]. These results reaffirm that our method accurately predicts results within and beyond the diffusion limit, and that a substantial body of existing literature can be seen to be a special case of our method.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006729.g008">Fig 8C</xref> shows a population of QIF neurons that fire in synchrony at <italic>t</italic> = 0, undergoing a slow decorrelation by low rate Poisson input spike trains. The neurons have all been prepared in the same state, and therefore are at the same position in state space. We use <italic>F</italic>(<italic>V</italic>) = <italic>V</italic><sup>2</sup> + 1, so these neurons are bursting, as the current parameter is larger than 0, and there are no fixed points. Neurons that receive an input spike leave the peak and travel on their own through state space. This results in a very complex density profile, where the initial density peak is still visible after 1s. Such a peak would have diffused away rapidly in a diffusion limit approximation. Monte Carlo events in red markers show that the density profile is not a numerical artefact, but reflects the complexity of the density profile.</p>
</sec>
<sec id="sec012">
<title>Adaptive-exponential-integrate-and-fire neurons</title>
<p>We consider the AdExp model as presented by Brette and Gerstner [<xref ref-type="bibr" rid="pcbi.1006729.ref026">26</xref>], which describes individual neurons by the following equations:
<disp-formula id="pcbi.1006729.e050"><alternatives><graphic id="pcbi.1006729.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mtext>m</mml:mtext></mml:msub> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>V</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:msub><mml:mo>Δ</mml:mo> <mml:mtext>T</mml:mtext></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mfrac><mml:mrow><mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mtext>T</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Δ</mml:mo> <mml:mtext>T</mml:mtext></mml:msub></mml:mfrac></mml:msup> <mml:mo>−</mml:mo> <mml:mi>w</mml:mi> <mml:mo>+</mml:mo> <mml:mi>I</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mtext>w</mml:mtext></mml:msub> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>E</mml:mi> <mml:mtext>l</mml:mtext></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
Upon spiking, the neuron is reset to potential <italic>V</italic><sub>reset</sub> and increases its adaptivity value: <italic>w</italic> → <italic>w</italic> + <italic>b</italic>. Here <italic>C</italic><sub>m</sub> is the membrane capacity and <italic>g</italic><sub><italic>l</italic></sub> the passive conductance. <italic>V</italic><sub>T</sub> is the value at which a neuron starts to spike; the spike dynamics is controlled by Δ<sub>T</sub>. The numerical values of the parameters are summarized in <xref ref-type="table" rid="pcbi.1006729.t002">Table 2</xref> and are taken from [<xref ref-type="bibr" rid="pcbi.1006729.ref026">26</xref>].</p>
<table-wrap id="pcbi.1006729.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.t002</object-id>
<label>Table 2</label>
<caption>
<title>Parameters for the AdExp model as given in [<xref ref-type="bibr" rid="pcbi.1006729.ref026">26</xref>].</title>
</caption>
<alternatives>
<graphic id="pcbi.1006729.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-bottom:thick">Quantity</th>
<th align="center" style="border-bottom:thick">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><italic>C</italic><sub><italic>m</italic></sub></td>
<td align="center">281 pF</td>
</tr>
<tr>
<td align="left"><italic>g</italic><sub><italic>l</italic></sub></td>
<td align="center">30 nS</td>
</tr>
<tr>
<td align="left"><italic>E</italic><sub><italic>l</italic></sub></td>
<td align="center">-70.6 mV</td>
</tr>
<tr>
<td align="left"><italic>V</italic><sub><italic>T</italic></sub></td>
<td align="center">-50.4 mV</td>
</tr>
<tr>
<td align="left">Δ<sub><italic>T</italic></sub></td>
<td align="center">2 mV</td>
</tr>
<tr>
<td align="left"><italic>τ</italic><sub><italic>w</italic></sub></td>
<td align="center">144 ms</td>
</tr>
<tr>
<td align="left"><italic>a</italic></td>
<td align="center">4 nS</td>
</tr>
<tr>
<td align="left"><italic>b</italic></td>
<td align="center">0.0805 nA</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>An overview of the state space is given in <xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9A</xref>. At <italic>w</italic> = 0 the dynamics is as expected, a drive towards the equilibrium potential that suddenly reverses into a spike onset at higher values of <italic>V</italic>, essentially producing an exponential-integrate-and-fire neuron. At high <italic>w</italic> two effects conspire to make the neuron less excitable: the equilibrium potential is lower and the drive towards this equilibrium is stronger for a given value of <italic>V</italic>. At low <italic>w</italic> values, the opposite happens: the equilibrium value is higher, closer to threshold, and below equilibrium there is a stronger depolarizing trend making the neuron more excitable. Interestingly, at hyperpolarization the system does not only respond by driving the membrane potential back towards equilibrium potential, but also downwards.</p>
<fig id="pcbi.1006729.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g009</object-id>
<label>Fig 9</label>
<caption>
<title/>
<p>A: Overview of state space for the adaptive-exponential-integrate-and-fire neuron. B: a detail of a realistic mesh near the equilibrium potential. C-E: evolution of the probability density at <italic>t</italic> = 0.01, 0.1, 0.4s. The input (switched on at <italic>t</italic> = 0) is a Poisson distributed spike train of 3000 spikes/s, delta synapses with efficacy <italic>J</italic> = 1 mV.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g009" xlink:type="simple"/>
</fig>
<p>There are two critical points, the equilibrium point (<italic>E</italic><sub><italic>l</italic></sub>, 0) and a saddle point in the top right. They are at the crossing of two nullclines: the <italic>w</italic>-nullcline is a straight line, whereas the <italic>V</italic>-nullcline follows a strongly curved trajectory, which is close to the stable manifold of the saddle point in a substantial part of state space. Below (to the right) the stable manifold neurons spike, regardless of where they are initially, while above (to the left) of the stable manifold neurons converge to the equilibrium, but how, and how long this takes is strongly dependent on the initial conditions. This model is the first to require a judicial treatment of the grid boundaries.</p>
<p>Let us examine the the equilibrium point first. The exponential build-up of cells observed in one dimensional models occurs here as well, but here it is not a good idea to introduce a fiducial cut and cover the remaining part of state space with a cell. The inset of <xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9B</xref> shows that equilibrium is reached much faster in the <italic>V</italic> direction, than in the <italic>w</italic> direction. This is a direct consequence of the adaptation time constant <italic>τ</italic><sub><italic>w</italic></sub> being an order of magnitude larger than the membrane time constant <italic>τ</italic> ≡ <italic>C</italic><sub>m</sub>/<italic>g</italic><sub>l</sub>. For high <italic>w</italic>, mass will move downwards along the diagonal, until low values of <italic>w</italic> are reached, as is demonstrated by the left inset of <xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9</xref>. A long, but very narrow region separates different parts of the grid. What to do? First, we observe that the offending region is essentially forbidden for neurons: for most neurons starting from a random position in state space it would take a long time (of the order of 100 ms) to approach this no man’s land. At the input firing rates we will be considering, neurons will experience an input spike well before running off the strip, so essentially only noise can place neurons there. If we forbid this, by allocating events that are translated into the cleft between the two grid parts to the cells in the grid that are closest to it along the projection of the jump, we guarantee that no probability mass will leak out of the grid. Mass that reaches the end of the strips will be placed in a reversal bin, like the one dimensional case. Mass on the left of the side of the cleft will move in the same direction as that on the right side of the cleft. By using Euclidean distance projected along the jump direction, we minimize the bias due to this procedure, although we may artificially introduce a small extra source of variability.</p>
<p>On the right hand side, the stable manifold almost coincides with the <italic>V</italic> nullcline, resulting in a very narrow region of dynamics in the vertical direction. Immediately outside neurons rapidly move away laterally. This part of the grid is created by reversing the time direction, integrating towards the stable manifold. The grid strongly deforms here: cell area decreases rapidly and even small numerical inaccuracies will lead to cells that are degenerate. We use cell area as a stopping criterion. The last cells before breaking off are extremely elongated. The spike region is also created by reversing the time direction. Again, we conclude that the cleft is a forbidden area: a small fluctuation in the state variable will cause a neuron to move away rapidly. Our main concern, again, is neurons that are placed into this cleft by the noise process. Again, we move neurons to the closest cell next to the cleft in the jump direction. This is reasonable, since natural fluctuations would put them there soon anyway. Effectively we have broadened the separatrix a little bit, but we still capture the upwards (for high <italic>w</italic>—past the saddle point: downwards) movement close to the stable manifold.</p>
<p>In <xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9C–9E</xref> the evolution of a population in (<italic>V</italic>, <italic>w</italic>) space is shown at three different points in time: <italic>t</italic> = 0.05, 0.1 and 0.4 s. <xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9C</xref> shows the input spikes pushing the state towards threshold, and a small number of neurons have spiked. They re-emerge at the reset potential, but with much higher <italic>w</italic>, due to spike adaptation. This is determined by the <italic>b</italic> parameter of the AdExp model. Close to the reset potential the banded shot noise structure, due to the use of a delta-peaked synaptic efficacy, is visible. The steady state is reached after approximately 400 ms. The population stabilizes at high <italic>w</italic> values, and the bulk of the population is clearly well below threshold, due to stronger leak behavior at these values of <italic>w</italic>. In sub figure E there is a minute deformation of the density, due to the limits of the grid, and density heaps up here, but the fraction of probability mass affected is negligible. Monte Carlo events, indicated by the dots, are not restricted to the grid and some fall outside the grid.</p>
<p>The firing rate response corresponding to the population experiencing an excitatory input (<xref ref-type="fig" rid="pcbi.1006729.g009">Fig 9C–9E</xref>) is given in <xref ref-type="fig" rid="pcbi.1006729.g010">Fig 10A</xref>. Again, agreement with Monte Carlo simulation is excellent, we are able to study the relative contributions of current- and spike-based adaptation to the firing rate. We can easily simulate neurons with current- but not spike-based adaptation by not incorporating the jump in <italic>w</italic> after reset; while ignoring all forms of adaptation can be done by simply using a 1D grid and ignoring values of <italic>w</italic> ≠ 0. The vast difference between adaptive neurons and non-adaptive neurons is also reflected in the gain spectrum. <xref ref-type="fig" rid="pcbi.1006729.g010">Fig 10B</xref> shows the gain spectrum of a (non-adaptive) exponential-integrate-and-fire neuron and a neuron that has a constant rate of adaptation due to the background rate upon which the small sinusoidal modulation has been imposed. The difference between the adaptive and non-adaptive neuron is considerable. Both neurons show a <inline-formula id="pcbi.1006729.e051"><alternatives><graphic id="pcbi.1006729.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ω</mml:mi></mml:mfrac></mml:math></alternatives></inline-formula> dependence in the high frequency limit, as is expected for exponential neurons [<xref ref-type="bibr" rid="pcbi.1006729.ref025">25</xref>]. (<xref ref-type="fig" rid="pcbi.1006729.g010">Fig 10A</xref> shows that the shape of the spike, which is reflected in the large cells on the right of the grid is independent of <italic>w</italic>.) It is clear that a meaningful time-independent gain function cannot be chosen, so that it is not possible to develop linear response theory.</p>
<fig id="pcbi.1006729.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g010</object-id>
<label>Fig 10</label>
<caption>
<title/>
<p>A: compares the response for three cases: no adaptation; only current adaptation; and both current- as well as spike-based adaptation is included. B: The gain for a small sinusoidal input modulated on a background input as function of frequency, for no adaptation and AdExp with current- and spike-based adaptation. Both spectra show the <inline-formula id="pcbi.1006729.e052"><alternatives><graphic id="pcbi.1006729.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>ω</mml:mi></mml:mfrac></mml:math></alternatives></inline-formula> dependency expected of an exponential-integrate-and-fire neuron, as the spike shape, represented by the grid at high <italic>V</italic> values is independent of <italic>w</italic>. However, the numerical difference between the cases is vast.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g010" xlink:type="simple"/>
</fig>
<p>It is interesting to observe the marginal distributions—in <xref ref-type="fig" rid="pcbi.1006729.g011">Fig 11</xref> we show the marginal distributions, together with the joint distribution. The distribution in <italic>V</italic> looks remarkably like that of an LIF neuron, except near the threshold, where the spike region, which is not present for LIF, flattens the density. The <italic>w</italic> distribution suggests a much stronger overlap than the joint distribution, which shows a clear separation. It is clear that, had the three density blobs been oriented more diagonally, the marginal <italic>w</italic> distribution would have shown a single cluster.</p>
<fig id="pcbi.1006729.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Comparison of the joint distribution function of the AdExp neuron with the marginals in <italic>V</italic> and <italic>w</italic>.</title>
<p>The marginal <italic>w</italic> distribution still reveals four clusters, but the joint distribution reveals them as being far better resolved than one would judge from the marginals.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec013">
<title>Frequency-dependent short-term synaptic dynamics</title>
<p>Vasilaki and Giugliano have studied the formation of network motifs [<xref ref-type="bibr" rid="pcbi.1006729.ref024">24</xref>], using both microscopic spiking neural simulations and mean-field approximation. In their mean-field simulations they considered both spike-timing dependent long-term plasticity, and frequency-dependent short-term dynamics, where they use a version of the Tsodyks-Markram synapse [<xref ref-type="bibr" rid="pcbi.1006729.ref027">27</xref>]. The short-term dynamics is of interest because it introduces something we have not considered before: the magnitude of the jump being dependent on the position of where the jump originates. Following [<xref ref-type="bibr" rid="pcbi.1006729.ref024">24</xref>], if <italic>G</italic><sub><italic>ij</italic></sub> defines the amplitude of the postsynaptic contribution from presynaptic neuron <italic>j</italic> to postsynaptic neuron <italic>i</italic>, then this is considered to be proportional to the amount of resources used for neurontransmission <italic>u</italic><sub><italic>ij</italic></sub><italic>r</italic><sub><italic>ij</italic></sub> and to their maximal availability <italic>A</italic><sub><italic>ij</italic></sub>, so
<disp-formula id="pcbi.1006729.e053"><alternatives><graphic id="pcbi.1006729.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>r</italic> relates to the recovery and <italic>u</italic> to the facilitation of synapses, and the time constants <italic>τ</italic><sub>rec</sub> and <italic>τ</italic><sub>facil</sub> are different for facilitating and depressing synapses. They describe frequency-dependent short-term synaptic dynamics by:
<disp-formula id="pcbi.1006729.e054"><alternatives><graphic id="pcbi.1006729.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>rec</mml:mtext></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:munderover><mml:mo>∑</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mi>∞</mml:mi></mml:munderover> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:msub><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
<disp-formula id="pcbi.1006729.e055"><alternatives><graphic id="pcbi.1006729.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>facil</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mi>∞</mml:mi></mml:munderover> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:msub><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
From now on, we will drop the indices <italic>ij</italic> and just refer to a single connection. In the simulation below we will use <italic>τ</italic><sub>rec</sub> = 0.1 s and <italic>τ</italic><sub>facil</sub> = 0.9 s and study a population of facilitating synapses (Vasilaki and Giugliano used <italic>τ</italic><sub>rec</sub> = 0.9<italic>s</italic>, <italic>τ</italic><sub>facil</sub> = 0.1 s for depressing synapses.) <italic>U</italic> is a fixed constant, for facilitating (depressing) synapses <italic>U</italic> = 0.1(0.8). <xref ref-type="disp-formula" rid="pcbi.1006729.e054">Eq 17</xref> expresses that an individual synapse is subject to deterministic dynamics, and that upon the arrival of a spike at time <italic>t</italic><sub><italic>k</italic></sub> both <italic>u</italic> and <italic>r</italic> undergo a finite jump, whose magnitude is dependent on the current value of <italic>u</italic> and <italic>r</italic>. <xref ref-type="disp-formula" rid="pcbi.1006729.e009">Eq 2</xref> describes this situation, when the following transition probabilities are introduced:
<disp-formula id="pcbi.1006729.e056"><alternatives><graphic id="pcbi.1006729.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>|</mml:mo> <mml:mi>r</mml:mi> <mml:mo>,</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ν</mml:mi> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:mi>r</mml:mi> <mml:mo>+</mml:mo> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:mi>u</mml:mi> <mml:mo>−</mml:mo> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>ν</mml:mi> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>u</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
We have to modify the process of generating our transition matrices: now for each quadrilateral cell (<italic>p</italic>, <italic>q</italic>), we determine the centroid (<italic>u</italic><sub>(<italic>p</italic>,<italic>q</italic>)</sub>, <italic>r</italic><sub>(<italic>p</italic>,<italic>q</italic>)</sub>) and we determine the covering set by defining
<disp-formula id="pcbi.1006729.e057"><alternatives><graphic id="pcbi.1006729.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
and determining the cover set as before. The jump now becomes cell dependent.</p>
<p>It is easy to cover almost the entire state space. In <xref ref-type="fig" rid="pcbi.1006729.g012">Fig 12A</xref> we show the grid. In <xref ref-type="fig" rid="pcbi.1006729.g012">Fig 12B</xref>, we show the sample path of three synapses, assuming that the presynaptic firing rate <italic>ν</italic> = 5 Hz. In C-F we show the evolution of a population of synapses. The influence of the step size which increases in the <italic>r</italic> (horizontal) direction with <italic>u</italic> and <italic>r</italic>, but decreases in the <italic>u</italic> (vertical) direction with <italic>u</italic>. There is good agreement with Monte Carlo simulation throughout. With the joint distribution available, it is possible to use <xref ref-type="disp-formula" rid="pcbi.1006729.e053">Eq 16</xref> and calculate the distribution of <italic>G</italic><sub><italic>ij</italic></sub> or its expectation value.</p>
<fig id="pcbi.1006729.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Evolution of the state of a population of facilitating Tsodyks-Markram synapses.</title>
<p>A: Grid. B: Sample path of individual synapses. The state dependency of the jump size is clearly visible. C-F: evolution of density over time.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g012" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>Fitzhugh-Nagumo neurons</title>
<p>We consider the well-known Fitzhugh-Nagumo neuron model [<xref ref-type="bibr" rid="pcbi.1006729.ref028">28</xref>], which is given by:
<disp-formula id="pcbi.1006729.e058"><alternatives><graphic id="pcbi.1006729.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>V</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>V</mml:mi> <mml:mo>−</mml:mo> <mml:mfrac><mml:msup><mml:mi>V</mml:mi> <mml:mn>3</mml:mn></mml:msup> <mml:mn>3</mml:mn></mml:mfrac> <mml:mo>−</mml:mo> <mml:mi>W</mml:mi> <mml:mo>+</mml:mo> <mml:mi>I</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>W</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>08</mml:mn> <mml:mo>(</mml:mo> <mml:mi>V</mml:mi> <mml:mo>+</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>7</mml:mn> <mml:mo>−</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>8</mml:mn> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>It is an attractive neuron model as it captures many properties of the biologically realistic Hodgkin-Huxley neuron, while being much more tractable—being reduced to two dimensions aids greatly in analysis and visualization. The two variables are a nondimensionalized voltage-like variable <italic>V</italic> and a recovery variable <italic>W</italic>. Also we note a variable <italic>I</italic> representing a constant external current.</p>
<p>When <italic>I</italic> = 0, there is a stable equilibrium point at ≈ (−1.199, −0.624) corresponding to a resting state. As <italic>I</italic> increases, the system undergoes a Hopf bifurcation to a stable limit cycle around an unstable equilibrium. (Increasing <italic>I</italic> further leads to a stable fixed point at positive <italic>V</italic> and <italic>W</italic> termed “excitation block”.) In this paper, we will consider an intermediate value <italic>I</italic> = 0.5 in order to demonstrate how our method can be used on systems with limit cycles.</p>
<p>We simulate white noise by providing the system with both inhibitory and excitatory noisy input with a high rate and low synaptic efficacy, and successfully capture the diffusion of probability in a neighbourhood around the limit cycle (<xref ref-type="fig" rid="pcbi.1006729.g013">Fig 13A–13D</xref> for <italic>t</italic> = 5, 10, 50 and 1000 s, <italic>J</italic> = ±0.02, <italic>ν</italic> = 20 spikes/s). It is interesting to study a purely excitatory input with large synapses (<italic>J</italic> = 0.1, <italic>ν</italic> = 2 Hz). This leads to a deformed limit cycle, shifted towards higher <italic>V</italic>. This is expected as the net input now is <italic>I</italic> = 0.7. The band is also broader, as one would expected as higher values of synaptic efficacy imply larger variability.</p>
<fig id="pcbi.1006729.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g013</object-id>
<label>Fig 13</label>
<caption>
<title/>
<p>A-D: Evolution of the density at <italic>t</italic> = 5, 10, 50 s an steady state for a diffusive input (<italic>J</italic> = ±0.02, <italic>ν</italic> = 20 spikes/s). E: excitatory input for a weak rate, but with large synaptic efficacy (<italic>J</italic> = 0.1, <italic>ν</italic> = 2 spikes/s). F: inhibition captures most neurons at the fixed point, a weak ghost cycle of neurons that escape by fluctuation is visible, but is considerably displaced compared to the standard limit cycle.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g013" xlink:type="simple"/>
</fig>
<p>Another case we consider is noisy inhibitory input (<xref ref-type="fig" rid="pcbi.1006729.g013">Fig 13F</xref>). As we would expect, the system is effectively driven back below the bifurcation to a stable equilibrium, although we still see some variance-driven probability follow a limit cycle that differs considerable from the original limit cycle. We can understand this by converting the noisy input into zero-mean noise and a steady inhibitory current, and looking at the streamlines of the system with these parameters instead. As seen in <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14D</xref>, while all the the trajectories converge to the fixed point, those starting on the right side of phase space first increase <italic>w</italic> until they reach the right branch of the cubic nullcline, then follow a path close to the limit cycle to return to the fixed point. It is interesting to see that the method captures limit cycles that do not coincide with the limit cycle of the original grid.</p>
<fig id="pcbi.1006729.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g014</object-id>
<label>Fig 14</label>
<caption>
<title/>
<p>A: The numbered mesh provides a coarse overview of the dynamics of the system, whereas we run our simulations using the much finer mesh. B: Different trajectories in the Fitzhugh-Nagumo model (as shown by solid and dashed lines) can merge before reaching the limit cycle. C: The region of the Fitzhugh-Nagumo model tiled with stationary cells; limit cycle is shown for reference. D: Sample trajectories in the Fitzhugh-Nagumo model in the parameter regime where the system tends to a fixed point. E: Trajectories that begin close to each other have the potential to diverge rapidly. F: Attempting to build cells from these trajectories can lead to “stretched” cells that intersect other cells.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g014" xlink:type="simple"/>
</fig>
<p>Rabinovitch and Rogachevskii [<xref ref-type="bibr" rid="pcbi.1006729.ref029">29</xref>] describe the two “vertical” sections of the path to be transient attractors (T-attractors) separated by a diagonal transient repeller (T-repeller) (alternatively, a separatrix [<xref ref-type="bibr" rid="pcbi.1006729.ref030">30</xref>]) close to the central branch of the cubic nullcline. Trajectories close to each other but starting on different sides of the T-repeller separate rapidly before eventually reaching the same steady state, which creates considerable problems in creating the grid (see <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14E</xref>). The authors perform a detailed analysis of the system by extending the notion of isochrones from limit cycles to excitable systems. We note that their isochrones are similar in character to the lines in our grid perpendicular to the streamlines of the system.</p>
<p>Next, we outline some of the numerical subtleties involved in generating the computational grid for the Fitzhugh-Nagumo model. Following the procedure from Sec. <bold>Materials and Methods</bold>, one can attempt to generate a grid by starting with a set of initial conditions, and solving the differential equations of the system forwards in time to obtain a set of trajectories (or integral curves). Each pair of trajectories then has a strip between them and the individual cells are obtained by dividing the strip into equal-time bins. However, in a system with a limit cycle, if we start with initial conditions outside the limit cycle, we see that the trajectories generated from them converge onto the limit cycle. Moreover, it is impossible to obtain trajectories inside the limit cycle from outside the limit cycle, and vice versa. This means that we have to handle the limit cycle, outside, and inside, as separate sections of the plane.</p>
<p>Since the limit cycle is a one-dimensional object with zero width, we have to artificially define a small width around it. We then choose sets of initial conditions outside and inside the limit cycle and integrate the trajectories until they reach a certain small Euclidean distance from the limit cycle, and then define our limit cycle strip as the space left. In this left over space we define quadrilaterals so as to fill up this ring. This becomes a strip in its own right, representing the limit cycle. Earlier we described the reversal mapping: mass reaching the end of a strip must be removed and deposited in a cell representing a stable point. Here, we use a similar approach: mass that arrives at the end of a strip must be removed and deposited on the limit cycle. We find the cell on the limit cycle that is closest in Euclidean distance to the limit cycle. Since the machinery to do this is already in place in the form of a reversal mapping, we will also refer here to this process as a reversal mapping. The modeler presents this reversal mapping in the same file format as used previously.</p>
<p>Initially we had attempted to define our limit cycle cells as having a fixed width, and then obtain strips by integrating backwards in time from the corners of these cells. Indeed, the coarse schematic grid in <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14</xref> has trajectories generated in this way for the interior of the limit cycle. However, for the purposes of actual computation, this method leads to degenerate cells. This is due to the fact that close to the limit cycle, trajectories move almost parallel to it, in particular along the “horizontal” segments of the limit cycle, where the fast <italic>v</italic> dynamics dominate. This leads to long, thin cells being created, which become degenerate when approaching the limit cycle—adjacent trajectories overlap to the degree of accuracy of the numerical integrator, leading to self-intersecting cells or cells with zero area.</p>
<p>From the outside of the limit cycle, most of the state space can be covered by simply choosing points on the edge of the region of interest and integrating forwards in time until one reaches the limit cycle. However, care must be taken when trajectories converge before arriving at the limit cycle, as shown in <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14B</xref>. This happens particularly along the cubic nullcline. We handle this by checking for degenerate cells or cells with area close to zero. These cells are then deleted from the grid, and instead a reversal mapping is created from the previous cell onto the closest (in Euclidean terms) cell.</p>
<p>The interior of the limit cycle proves to be even more challenging. Not only is there an unstable fixed point, also there exist canard trajectories, which have been the subject of considerable mathematical interest [<xref ref-type="bibr" rid="pcbi.1006729.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref032">32</xref>]. Loosely speaking, near the central portion of the cubic nullcline, there are slow but unstable trajectories. This leads to two types of numerical issues—first, the slow dynamics cause a build-up of exponentially many very small cells. We work around this by defining a minimum value of <inline-formula id="pcbi.1006729.e059"><alternatives><graphic id="pcbi.1006729.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>—regions below this value are considered to be approximately stationary, since they will have much slower dynamics than any noisy input we consider. The region we find is shown in <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14C</xref>.</p>
<p>We use cubic splines to approximate the boundary of this region, and then use points on this boundary as initial points for trajectories on the inside of the limit cycle to generate strips. Due to the instabilities in this region of the system, trajectories can be highly curved, and trajectories with initial conditions close to each other can diverge quickly, leading to cells which may intersect with each other, as shown in <xref ref-type="fig" rid="pcbi.1006729.g014">Fig 14E</xref>. As these areas with highly curved trajectories are still locally smooth, it may be possible to increase the resolution of the grid until non-degenerate cells are obtained, as we do here. However, it may not always be possible to do so due to computational constraints—in that case it may be more practical to delete bad cells after the creation of the grid and cover any gaps with fiducial bins.</p>
<p>To sum up, regions where trajectories merge—such as the limit cycle and nullclines in this case—involve moving from the two dimensional plane onto one dimensional trajectories, and pose conceptual as well as computational difficulties. Regions with highly curved trajectories may be possible to handle with very fine resolutions, but may pose difficulties at coarser resolutions. In both cases it is possible to handle such regions using an automated procedure: cells are checked for being complex quadrilaterals or having too small an area. Those satisfying this condition are deleted, and renewal mappings from the cells before them to the nearest cells are generated. Any gaps in the grid due to this can be handled using the prescribed method for creating fiducial bins.</p>
<p>In conclusion, we have successfully extended our procedure to dynamical systems with limit cycles and complex dynamics such as canards. While we have to make some compromises in the regions which pose significant analytic difficulty, these regions are those in which neurons would not spend any significant amount of time. Hence, our method would still be suitable for studying neural circuits of such populations.</p>
</sec>
<sec id="sec015">
<title>Numerical solution and efficiency</title>
<p>We solve <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> by a forward Euler scheme. Since we interleave moving probability mass through the grid with a numerical solution of <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref>, we solve <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> over a period Δ<italic>t</italic>, which can be as short as 10<sup>−4</sup> s for some neural models. This renders sophisticated adaptive size solvers relatively inefficient. The matrices in <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> tend to be sparse band matrices, and one advantage of the forward Euler scheme is that it is embarrassingly parallel. For a single population in this we partition Δ<italic>t</italic> into <italic>n</italic><sub><italic>euler</italic></sub> time steps. For most simulations in this paper <italic>n</italic><sub><italic>euler</italic></sub> = 10 is adequate, we will discuss an exception below. In a forward Euler scheme <xref ref-type="disp-formula" rid="pcbi.1006729.e036">Eq 12</xref> is discretized and a single step is given by:
<disp-formula id="pcbi.1006729.e060"><alternatives><graphic id="pcbi.1006729.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>map</mml:mtext> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>map</mml:mtext> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>M</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with
<disp-formula id="pcbi.1006729.e061"><alternatives><graphic id="pcbi.1006729.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006729.e061" xlink:type="simple"/><mml:math display="block" id="M61"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>M</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>ν</mml:mi> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>u</mml:mi> <mml:mi>l</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:mo>{</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>h</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:msub> <mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>map</mml:mtext> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>map</mml:mtext> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
The current simulation time is <italic>t</italic><sub>sim</sub> <italic>k</italic>Δ<italic>t</italic>, where Δ<italic>t</italic> is the mesh time step. The current map, which indicates where probability mass has moved under the influence of endogenous neural dynamics is therefore labelled by map<sub><italic>k</italic></sub>(<italic>i</italic>, <italic>j</italic>), which maps cell (<italic>i</italic>, <italic>j</italic>) to a unique mass array index as per <xref ref-type="disp-formula" rid="pcbi.1006729.e031">Eq 9</xref>. Note that the mapping should not be applied to the set <italic>C</italic><sub><italic>h</italic></sub>(<italic>i</italic>, <italic>j</italic>). Simulation starts at <italic>k</italic> = 0, <italic>l</italic> = 0. Each Euler step <italic>l</italic> is increased, until <italic>l</italic> = <italic>n</italic><sub><italic>euler</italic></sub>, upon which <italic>l</italic> is reset to 0 and <italic>k</italic> is increased by 1, until the desired end time is reached. The sets <italic>C</italic><sub><italic>h</italic></sub>(<italic>i</italic>, <italic>j</italic>) and coefficients <italic>α</italic><sub><italic>p</italic>,<italic>q</italic></sub> remain constant throughout simulation.</p>
<p>Despite appearances, the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006729.e061">Eq 22</xref> is of the form of a matrix vector multiplication, where the matrix is very sparse (<xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15A</xref>). The matrix elements are numerical constants, and there is no dependence between rows, meaning that each row can be evaluated independently of the others and therefore the problem is extremely parallel: each row can be calculated in a separate thread when available.</p>
<fig id="pcbi.1006729.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006729.g015</object-id>
<label>Fig 15</label>
<caption>
<title/>
<p>A-B: Comparison of the default mesh for conductance based neurons to a reduced mesh. Although coarser cells are visible in B, the density is not visibly affected, in particular in the white areas, where the bulk of the density is present. C: Firing rates are almost identical. D: Interaction between the C++ driver and the GPU registers, Interaction between CPU and GPU in a CUDA-based simulation. During the initialization, the mass array (red), the map array (green), the matrix elements (yellow) are copied onto the GPU. During simulation only the map (green) is updated in C++ and copied to the GPU, and firing rates are calculated from the meshes on the GPU. On the CPU the firing rates are processed by the network, and delays are applied where applicable. Resulting firing rates are sent back to the GPU for processing the the next simulation step. E: Run time factors for direct simulation, CUDA and C++ implementation (default and reduced mesh). F: memory use, NEST and CUDA implementation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006729.g015" xlink:type="simple"/>
</fig>
<p>Larger networks can be simulated by vectorizing the network: vectors representing the mass of individual populations are laid out in a single array representing the mass of multiple populations. Connections from population <italic>i</italic> to population <italic>j</italic> then are represented by block matrix elements, each consisting of one or more transition matrices (one for every efficacy associated with the connection) generated by the process described above.</p>
<p>We have created both a C++ and a CUDA implementation and evaluated them on single populations, as well as networks of populations. When generating networks we consider networks of conductance based populations, each population connected to constant input (’cortical background’), with the network being sparsely connected (connectivity 5%), efficacy chosen fixed (<italic>h</italic> = 0.05), with the number of connections drawn from a uniform distribution (<italic>n</italic> = 1, …, 100). For small networks, the running times are not particularly onerous, with or without parallelization. For larger networks, in the C++ implementation we evaluated a single block matrix per thread using OpenMP. This means that in our implementation individual matrix vector calculations are not parallel, but that several matrix vector calculations are performed simultaneously. Since OpenMP offers a relatively small number of threads, this still makes efficient use of resources. The parallelization model for CUDA is different: we write a so-called kernel to evaluate <xref ref-type="disp-formula" rid="pcbi.1006729.e061">Eq 22</xref> and launch a kernel for each block matrix. CUDA’s loop unrolling automatically performs parallelization within the kernel, and by launching kernels in different streams, inter kernel concurrency can be achieved. It is then the question whether the large number of threads compensate for the inherently slower GPU hardware. In <xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15D</xref> we show how the GPU interacts with the C++ driver. During initialization the mass array is set up on the GPU, as well as the mapping, and the matrix elements. During a simulation run, the mass array mapping is updated, and firing rates are exchanged, but other than for visualization purposes, the mass array is not transferred, meaning lightweight communication between GPU and CPU.</p>
<p>We find that the number of cells in a mesh determines the performance. In <xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15</xref> we examine the conductance based neuron example again. If we use the original mesh, without considering performance, we find that the method is slower than direct simulation as performed by NEST by a factor of three (6s per population for NEST—20s per population for the CUDA implementation). If we reduce the granularity of the original mesh, we find that we can bring the size of the mesh down from 120k cells to 25k cells, with the density and firing rate predictions unaffected (<xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15A</xref> vs <xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15B</xref> for density, B being the coarser mesh which is only visible in the bigger cells on top, C for firing rate). For the reduced mesh, the performance of the C++ implementation is equal to that of NEST, where the CUDA implementation is a little bit slower (measured on a Tesla P100). Both direct (NEST) simulation and C++ implementation use parallelization with 16 threads for this comparison. The real time factors (real time second divided by wall time of one simulated second) are shown in <xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15E</xref>. A striking difference is the memory use (<xref ref-type="fig" rid="pcbi.1006729.g015">Fig 15F</xref>), which for the CUDA implementation is orders of magnitudes lower (300 MB for the largest network of 1000 populations, whereas a 100 population network with NEST already uses 10 GB). We conclude that the CUDA implementation supports the simulation of large networks on a single PC equipped with a GPGPU, whereas direct simulation requires a substantial cluster.</p>
<p>In the method we considered so far, a relatively large number of cells emerge around stationary points due to the exponential shrinkage of state space around them. In principle it is possible to group these cells together into larger ones, and to group them into strips that would run at a lower speed compared to the basic time step of the mesh. This would reduce the number of cells in the mesh considerably, while the basic granularity of the mesh will not be affected much. Such resulting merged cells are no longer quadrilateral and the method will have to be extended to be able to handle non-convex cells, which will be one of the first priorities in further work.</p>
</sec>
<sec id="sec016">
<title>General remarks</title>
<p>We have demonstrated a very general method to study noise in 2D dynamical systems and applied them to various neural models and Tsodyks-Markram synapses. The state space of the deterministic model must be represented by a grid. The requirement that a grid be made is both a strength and a weakness: the state space relevant to the simulation must be chosen judiciously before the simulation starts. But since it must be constructed beforehand, integration can be done very accurately, using time steps that are much smaller than typically used in Monte Carlo simulators. If general purpose simulators are used with a default time step, and without adaptive methods that monitor errors, they may not alert the user to problematic regions of state space. Our method requires a careful layout of state space before simulation starts. We found that the requirement of a grid forces visualization and thereby already creates an understanding of the dynamics that can be expected.</p>
<p>When the state space cannot contain the simulation, this is clearly visible, either through loss of mass, or by the accumulation of mass at the edge of the grid. This proved useful in one instance, where a well known neural simulator produced a crash (due to an instability of the particular neural model implementation, not the simulator as such). Our method is very robust and stable, once a suitable grid is available. In general, we find that grids can be taken quite coarse in state space, but that a relatively small time step must be used for completely accurate results, such as comparison to analytic results like gain curves. When numerical errors are acceptable, and only qualitative agreement is required, much coarser grids can be used that require far less simulation time.</p>
<p>Our method is not as efficient as effective 1D methods [<xref ref-type="bibr" rid="pcbi.1006729.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref017">17</xref>], but makes very few assumptions. It handles time-dependent input without any restrictions. This is useful, for example, when comparing against basis functions expansions [<xref ref-type="bibr" rid="pcbi.1006729.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref033">33</xref>]. These basis functions are typically determined for constant input, and time-dependent input must be treated as an adiabatic approximation. Our method does not require this. In short, our method may serve as benchmark for faster methods.</p>
<p>Population density techniques are part of an emerging ecology of simulation techniques, and it is important to consider their strength and weaknesses compared to related approaches. Direct spiking neuron simulations are straightforward in small to medium-sized networks, but hard to get right in large-scale simulations, where they are resource intensive (they can have large memory requirements, as well as being CPU intensive). The “missing spike” problem, and the need to keep track of spike information and its exchange between the various processors involved are just examples demonstrating that direct simulations are not straightforward. They have developed into a discipline of their own [<xref ref-type="bibr" rid="pcbi.1006729.ref034">34</xref>]. Population density techniques are conceptually simple, but unable to model pairwise correlations within a population, and the inclusion of finite-size effects is not straightforward (see [<xref ref-type="bibr" rid="pcbi.1006729.ref014">14</xref>] for an attempt). In general, population density techniques are not able to describe quenched network states, although fully connected networks are amenable to such analyses [<xref ref-type="bibr" rid="pcbi.1006729.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref036">36</xref>]. Recently, a number of studies have explored path integral approaches to calculate pairwise correlations and to suggest a functional role for such correlations (e.g. [<xref ref-type="bibr" rid="pcbi.1006729.ref036">36</xref>–<xref ref-type="bibr" rid="pcbi.1006729.ref038">38</xref>]). Often, these techniques use the diffusion approximation, or are restricted to remain close to stationary states. Two advantages of the technique described in this paper is that the latter restrictions do not apply. Theoretically, population density equations have been put on a rigorous mathematical footing [<xref ref-type="bibr" rid="pcbi.1006729.ref039">39</xref>], justifying its use for weakly connected networks where the quenched state of the network is not important. These papers also add to a substantial body of observation that even for small networks population density techniques predict the firing rate correctly (e.g. [<xref ref-type="bibr" rid="pcbi.1006729.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006729.ref040">40</xref>] and many others). So, when modeling firing rates is the main objective, and the network is such that the populations may be far from equilibrium, population density techniques are a good candidate. They are also valuable in repeat experiments on single cells, as they show what noise will do to otherwise identical neurons.</p>
<p>Modeling a complex system of real neurons probably requires a hybrid approach. Mazzucato <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006729.ref041">41</xref>] give an example of such an approach. They analyse the dimensionality of neural data recorded by multielectrode array. The dimensionality is estimated from pairwise correlations, something which would be impossible in a pure population density approach, as the pairwise correlations would vanish in an infinitely large population. However, they also produce a spiking network model, to validate their explanation and here they use population density techniques to establish the dynamical regime for their spiking network.</p>
<p>The study of 2D systems subject to noise is an important topic in its own right, given that limit cycles require at least two dimensions. The current trend in neuroscience towards 2D geometrical models reinforces this point.</p>
<p>An important prerequisite for the method to work is that the dynamical system can be represented faithfully. We found that some systems have challenging regions of state space: stationary points, whether stable or not, and limit cycles need careful handling and a full cover of state space is not possible. However, we find that we can infer motion of probability mass inside such regions from the immediate surroundings, the limit cycle of the Fitzhugh-Nagumo system as a case in point: it emerges as a region rather than as a curve from terminating the grid as it approaches the limit cycle.</p>
<p>There are interesting parallels between our method and a recently proposed method for determining missing spikes in hybrid time-driven, event-driven spiking neuron simulations [<xref ref-type="bibr" rid="pcbi.1006729.ref042">42</xref>]. Here, the authors consider the problem of missing spikes: the possibility that a neuron is below threshold at the end of a simulation step, but has crossed the threshold during the step. They solve this problem by determining whether a neuron is inside a volume in state space between the threshold and the backpropagated threshold. They find this easier than determining the actual point of crossing, and their method is reminiscent of ours when we calculate the transition matrix. They too consider a mapping like <xref ref-type="disp-formula" rid="pcbi.1006729.e020">Eq 6</xref> which they are able to calculate explicitly for current based neurons. They conclude that apart from the threshold and the backpropagated threshold, the boundary is given by the vanishing tangent space of the map, precisely the criterion we used numerically (area of cell—in the absence of analytic solutions) to define boundaries of state space.</p>
<p>It is interesting to speculate about extending the method towards even higher dimensions. At first sight, this seems unfeasible: a three dimensional grid might already require millions of bins. It is not efficient to simulate systems with a size of the order 10<sup>4</sup> particles by a larger number of bins. It would also be considerably harder to visualize the results. Nonetheless, probability tends to cluster in specific areas of state space and we find large parts of state space effectively unoccupied. A dynamical representation of the occupied part of state space would lead to a more scalable method.</p>
<p>Our simulation results have shown that we can simulate large networks consisting of hundreds or thousands of populations. To make really large networks run more efficiently, we need smaller meshes and the best way to achieve that we now believe is to lump the large number of small cells that emerge near stationary points into larger ones, as described above. This will be our main focus for the near future.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Gaute Einevoll, Michele Giugliano and Viktor Jirsa for discussions, and Martín Pérez-Guevara for pushing large-scale use of the method.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006729.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Cowan</surname> <given-names>JD</given-names></name>. <article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title>. <source>Biophysical Journal</source>. <year>1972</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3495(72)86068-5" xlink:type="simple">10.1016/S0006-3495(72)86068-5</ext-link></comment> <object-id pub-id-type="pmid">4332108</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stein</surname> <given-names>RB</given-names></name>. <article-title>A theoretical analysis of neuronal variability</article-title>. <source>Biophysical Journal</source>. <year>1965</year>;<volume>5</volume>(<issue>2</issue>):<fpage>173</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3495(65)86709-1" xlink:type="simple">10.1016/S0006-3495(65)86709-1</ext-link></comment> <object-id pub-id-type="pmid">14268952</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref003">
<label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Johannesma PIM. Stochastic neural activity: A theoretical investigation. Nijmegen: Faculteit der Wiskunde en Natuurwetenschappen; 1969.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Knight</surname> <given-names>BW</given-names></name>. <article-title>Dynamics of encoding in a population of neurons</article-title>. <source>The Journal of general physiology</source>. <year>1972</year>;<volume>59</volume>(<issue>6</issue>):<fpage>734</fpage>–<lpage>766</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1085/jgp.59.6.734" xlink:type="simple">10.1085/jgp.59.6.734</ext-link></comment> <object-id pub-id-type="pmid">5025748</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Omurtag</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Sirovich</surname> <given-names>L</given-names></name>. <article-title>On the simulation of large populations of neurons</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2000</year>;<volume>8</volume>(<issue>1</issue>):<fpage>51</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1008964915724" xlink:type="simple">10.1023/A:1008964915724</ext-link></comment> <object-id pub-id-type="pmid">10798499</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of computational neuroscience</source>. <year>2000</year>;<volume>8</volume>(<issue>3</issue>):<fpage>183</fpage>–<lpage>208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1008925309027" xlink:type="simple">10.1023/A:1008925309027</ext-link></comment> <object-id pub-id-type="pmid">10809012</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Population dynamics of spiking neurons: fast transients, asynchronous states, and locking</article-title>. <source>Neural computation</source>. <year>2000</year>;<volume>12</volume>(<issue>1</issue>):<fpage>43</fpage>–<lpage>89</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976600300015899" xlink:type="simple">10.1162/089976600300015899</ext-link></comment> <object-id pub-id-type="pmid">10636933</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ly</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tranchina</surname> <given-names>D</given-names></name>. <article-title>Spike train statistics and dynamics with synaptic input from any renewal process: a population density approach</article-title>. <source>Neural Computation</source>. <year>2009</year>;<volume>21</volume>(<issue>2</issue>):<fpage>360</fpage>–<lpage>396</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2008.03-08-743" xlink:type="simple">10.1162/neco.2008.03-08-743</ext-link></comment> <object-id pub-id-type="pmid">19431264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nykamp</surname> <given-names>DQ</given-names></name>, <name name-style="western"><surname>Tranchina</surname> <given-names>D</given-names></name>. <article-title>A population density approach that facilitates large-scale modeling of neural networks: Analysis and an application to orientation tuning</article-title>. <source>Journal of computational neuroscience</source>. <year>2000</year>;<volume>8</volume>(<issue>1</issue>):<fpage>19</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1008912914816" xlink:type="simple">10.1023/A:1008912914816</ext-link></comment> <object-id pub-id-type="pmid">10798498</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>de Kamps</surname> <given-names>M</given-names></name>. <article-title>A simple and stable numerical solution for the population density equation</article-title>. <source>Neural computation</source>. <year>2003</year>;<volume>15</volume>(<issue>9</issue>):<fpage>2129</fpage>–<lpage>2146</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603322297322" xlink:type="simple">10.1162/089976603322297322</ext-link></comment> <object-id pub-id-type="pmid">12959669</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Iyer</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Menon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Buice</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mihalas</surname> <given-names>S</given-names></name>. <article-title>The influence of synaptic weight distribution on neuronal population dynamics</article-title>. <source>PLoS computational biology</source>. <year>2013</year>;<volume>9</volume>(<issue>10</issue>):<fpage>e1003248</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003248" xlink:type="simple">10.1371/journal.pcbi.1003248</ext-link></comment> <object-id pub-id-type="pmid">24204219</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref012">
<label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">de Kamps M. A generic approach to solving jump diffusion equations with applications to neural populations. arXiv preprint arXiv:13091654. 2013.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mattia</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Del Giudice</surname> <given-names>P</given-names></name>. <article-title>Population dynamics of interacting spiking neurons</article-title>. <source>Physical Review E</source>. <year>2002</year>;<volume>66</volume>(<issue>5</issue>):<fpage>051917</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.66.051917" xlink:type="simple">10.1103/PhysRevE.66.051917</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mattia</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Del Giudice</surname> <given-names>P</given-names></name>. <article-title>Finite-size dynamics of inhibitory and excitatory interacting spiking neurons</article-title>. <source>Physical Review E</source>. <year>2004</year>;<volume>70</volume>(<issue>5</issue>):<fpage>052903</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.70.052903" xlink:type="simple">10.1103/PhysRevE.70.052903</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Augustin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ladenbauer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Baumann</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Obermayer</surname> <given-names>K</given-names></name>. <article-title>Low-dimensional spike rate models derived from networks of adaptive integrate-and-fire neurons: comparison and implementation</article-title>. <source>PLoS computational biology</source>. <year>2017</year>;<volume>13</volume>(<issue>6</issue>):<fpage>e1005545</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005545" xlink:type="simple">10.1371/journal.pcbi.1005545</ext-link></comment> <object-id pub-id-type="pmid">28644841</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cain</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Iyer</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mihalas</surname> <given-names>S</given-names></name>. <article-title>The computational properties of a simplified cortical column model</article-title>. <source>PLoS computational biology</source>. <year>2016</year>;<volume>12</volume>(<issue>9</issue>):<fpage>e1005045</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005045" xlink:type="simple">10.1371/journal.pcbi.1005045</ext-link></comment> <object-id pub-id-type="pmid">27617444</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwalger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Deger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Towards a theory of cortical columns: From spiking neurons to interacting neural populations of finite size</article-title>. <source>PLoS computational biology</source>. <year>2017</year>;<volume>13</volume>(<issue>4</issue>):<fpage>e1005507</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005507" xlink:type="simple">10.1371/journal.pcbi.1005507</ext-link></comment> <object-id pub-id-type="pmid">28422957</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ly</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tranchina</surname> <given-names>D</given-names></name>. <article-title>Critical analysis of dimension reduction by a moment closure method in a population density approach to neural network modeling</article-title>. <source>Neural computation</source>. <year>2007</year>;<volume>19</volume>(<issue>8</issue>):<fpage>2032</fpage>–<lpage>2092</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2007.19.8.2032" xlink:type="simple">10.1162/neco.2007.19.8.2032</ext-link></comment> <object-id pub-id-type="pmid">17571938</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Casti</surname> <given-names>ARR</given-names></name>, <name name-style="western"><surname>Omurtag</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sornborger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kaplan</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Victor</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>A population study of integrate-and-fire-or-burst neurons</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>5</issue>):<fpage>957</fpage>–<lpage>986</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602753633349" xlink:type="simple">10.1162/089976602753633349</ext-link></comment> <object-id pub-id-type="pmid">11972903</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Apfaltrer</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ly</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tranchina</surname> <given-names>D</given-names></name>. <article-title>Population density methods for stochastic neurons with realistic synaptic kinetics: Firing rate dynamics and fast computational methods</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2006</year>;<volume>17</volume>(<issue>4</issue>):<fpage>373</fpage>–<lpage>418</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09548980601069787" xlink:type="simple">10.1080/09548980601069787</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Richardson</surname> <given-names>MJ</given-names></name>. <article-title>Effects of synaptic conductance on the voltage distribution and firing rate of spiking neurons</article-title>. <source>Physical Review E</source>. <year>2004</year>;<volume>69</volume>(<issue>5</issue>):<fpage>051918</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.69.051918" xlink:type="simple">10.1103/PhysRevE.69.051918</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref022">
<label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name>. <source>Dynamical systems in neuroscience</source>. <publisher-name>MIT press</publisher-name>; <year>2007</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lai</surname> <given-names>YM</given-names></name>, <name name-style="western"><surname>de Kamps</surname> <given-names>M</given-names></name>. <article-title>Population density equations for stochastic processes with memory kernels</article-title>. <source>Physical Review E</source>. <year>2017</year>;<volume>95</volume>(<issue>6</issue>):<fpage>062125</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.95.062125" xlink:type="simple">10.1103/PhysRevE.95.062125</ext-link></comment> <object-id pub-id-type="pmid">28709222</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Giugliano</surname> <given-names>M</given-names></name>. <article-title>Emergence of connectivity motifs in networks of model neurons with short-and long-term plastic synapses</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>1</issue>):<fpage>e84626</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0084626" xlink:type="simple">10.1371/journal.pone.0084626</ext-link></comment> <object-id pub-id-type="pmid">24454735</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fourcaud-Trocmé</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Van Vreeswijk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>How spike generation mechanisms determine the neuronal response to fluctuating inputs</article-title>. <source>The Journal of neuroscience</source>. <year>2003</year>;<volume>23</volume>(<issue>37</issue>):<fpage>11628</fpage>–<lpage>11640</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.23-37-11628.2003" xlink:type="simple">10.1523/JNEUROSCI.23-37-11628.2003</ext-link></comment> <object-id pub-id-type="pmid">14684865</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brette</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title>. <source>Journal of neurophysiology</source>. <year>2005</year>;<volume>94</volume>(<issue>5</issue>):<fpage>3637</fpage>–<lpage>3642</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00686.2005" xlink:type="simple">10.1152/jn.00686.2005</ext-link></comment> <object-id pub-id-type="pmid">16014787</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tsodyks</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1997</year>;<volume>94</volume>(<issue>2</issue>):<fpage>719</fpage>–<lpage>723</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.94.2.719" xlink:type="simple">10.1073/pnas.94.2.719</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>FitzHugh</surname> <given-names>R</given-names></name>. <article-title>Impulses and physiological states in theoretical models of nerve membrane</article-title>. <source>Biophysical journal</source>. <year>1961</year>;<volume>1</volume>(<issue>6</issue>):<fpage>445</fpage>–<lpage>466</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3495(61)86902-6" xlink:type="simple">10.1016/S0006-3495(61)86902-6</ext-link></comment> <object-id pub-id-type="pmid">19431309</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rabinovitch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rogachevskii</surname> <given-names>I</given-names></name>. <article-title>Threshold, excitability and isochrones in the Bonhoeffer–van der Pol system</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>. <year>1999</year>;<volume>9</volume>(<issue>4</issue>):<fpage>880</fpage>–<lpage>886</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1063/1.166460" xlink:type="simple">10.1063/1.166460</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rabinovitch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Thieberger</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>M</given-names></name>. <article-title>Forced Bonhoeffer˘van der Pol oscillator in its excited mode</article-title>. <source>Phys Rev E</source>. <year>1994</year>;<volume>50</volume>:<fpage>1572</fpage>–<lpage>1578</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.50.1572" xlink:type="simple">10.1103/PhysRevE.50.1572</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Desroches</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jeffrey</surname> <given-names>MR</given-names></name>. <article-title>Canards and curvature: the ‘smallness of <italic>ϵ</italic>’ in slow–fast dynamics</article-title>. <source>Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</source>. <year>2011</year>;<volume>467</volume>(<issue>2132</issue>):<fpage>2404</fpage>–<lpage>2421</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspa.2011.0053" xlink:type="simple">10.1098/rspa.2011.0053</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rotstein</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Coombes</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gheorghe</surname> <given-names>AM</given-names></name>. <article-title>Canard-Like Explosion of Limit Cycles in Two-Dimensional Piecewise-Linear Models of FitzHugh–Nagumo Type</article-title>. <source>SIAM Journal on Applied Dynamical Systems</source>. <year>2012</year>;<volume>11</volume>(<issue>1</issue>):<fpage>135</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1137/100809866" xlink:type="simple">10.1137/100809866</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stefanescu</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Jirsa</surname> <given-names>VK</given-names></name>. <article-title>Reduced representations of heterogeneous mixed neural networks with synaptic coupling</article-title>. <source>Physical Review E</source>. <year>2011</year>;<volume>83</volume>(<issue>2</issue>):<fpage>026204</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.83.026204" xlink:type="simple">10.1103/PhysRevE.83.026204</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kunkel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Potjans</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Eppler</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Plesser</surname> <given-names>HEE</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>. <article-title>Meeting the memory challenges of brain-scale network simulation</article-title>. <source>Frontiers in Neuroinformatics</source>. <year>2012</year>;<volume>5</volume>:<fpage>35</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2011.00035" xlink:type="simple">10.3389/fninf.2011.00035</ext-link></comment> <object-id pub-id-type="pmid">22291636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Montbrió</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Pazó</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Roxin</surname> <given-names>A</given-names></name>. <article-title>Macroscopic description for networks of spiking neurons</article-title>. <source>Physical Review X</source>. <year>2015</year>;<volume>5</volume>(<issue>2</issue>):<fpage>021028</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buice</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Chow</surname> <given-names>CC</given-names></name>. <article-title>Dynamic finite size effects in spiking neural networks</article-title>. <source>PLoS computational biology</source>. <year>2013</year>;<volume>9</volume>(<issue>1</issue>):<fpage>e1002872</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002872" xlink:type="simple">10.1371/journal.pcbi.1002872</ext-link></comment> <object-id pub-id-type="pmid">23359258</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Schücker J, Goedeke S, Dahmen D, Helias M. Functional methods for disordered neural networks. arXiv preprint arXiv:160506758. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Dahmen D, Grün S, Diesmann M, Helias M. Two types of criticality in the brain. arXiv preprint arXiv:171110930. 2017.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baladron</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fasoli</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Faugeras</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Touboul</surname> <given-names>J</given-names></name>. <article-title>Mean-field description and propagation of chaos in networks of Hodgkin-Huxley and FitzHugh-Nagumo neurons</article-title>. <source>The Journal of Mathematical Neuroscience</source>. <year>2012</year>;<volume>2</volume>(<issue>1</issue>):<fpage>10</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/2190-8567-2-10" xlink:type="simple">10.1186/2190-8567-2-10</ext-link></comment> <object-id pub-id-type="pmid">22657695</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source>. <year>1997</year>;<volume>7</volume>(<issue>3</issue>):<fpage>237</fpage>–<lpage>252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006729.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mazzucato</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fontanini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>La Camera</surname> <given-names>G</given-names></name>. <article-title>Stimuli reduce the dimensionality of cortical activity</article-title>. <source>Frontiers in systems neuroscience</source>. <year>2016</year>;<volume>10</volume>:<fpage>11</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2016.00011" xlink:type="simple">10.3389/fnsys.2016.00011</ext-link></comment> <object-id pub-id-type="pmid">26924968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006729.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Krishnan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Porta Mana</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Helias</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Di Napoli</surname> <given-names>EA</given-names></name>. <article-title>Perfect detection of spikes in the linear sub-threshold dynamics of point neurons</article-title>. <source>Frontiers in neuroinformatics</source>. <year>2017</year>;<volume>11</volume>:<fpage>75</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2017.00075" xlink:type="simple">10.3389/fninf.2017.00075</ext-link></comment> <object-id pub-id-type="pmid">29379430</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>