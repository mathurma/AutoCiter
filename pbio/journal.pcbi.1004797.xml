<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01628</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004797</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Approximation methods</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Monte Carlo method</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Differential equations</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal to noise ratio</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Annealed Importance Sampling for Neural Mass Models</article-title>
<alt-title alt-title-type="running-head">Annealed Importance Sampling for Neural Mass Models</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Penny</surname> <given-names>Will</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Sengupta</surname> <given-names>Biswa</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Wellcome Trust Centre for Neuroimaging, University College London, London, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Daunizeau</surname> <given-names>Jean</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Brain and Spine Institute (ICM), FRANCE</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: WP. Performed the experiments: WP. Analyzed the data: WP. Contributed reagents/materials/analysis tools: WP BS. Wrote the paper: WP BS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">w.penny@ucl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>4</day>
<month>3</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>3</issue>
<elocation-id>e1004797</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>9</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Penny, Sengupta</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004797"/>
<abstract>
<p>Neural Mass Models provide a compact description of the dynamical activity of cell populations in neocortical regions. Moreover, models of regional activity can be connected together into networks, and inferences made about the strength of connections, using M/EEG data and Bayesian inference. To date, however, Bayesian methods have been largely restricted to the Variational Laplace (VL) algorithm which assumes that the posterior distribution is Gaussian and finds model parameters that are only locally optimal. This paper explores the use of Annealed Importance Sampling (AIS) to address these restrictions. We implement AIS using proposals derived from Langevin Monte Carlo (LMC) which uses local gradient and curvature information for efficient exploration of parameter space. In terms of the estimation of Bayes factors, VL and AIS agree about which model is best but report different degrees of belief. Additionally, AIS finds better model parameters and we find evidence of non-Gaussianity in their posterior distribution.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The activity of populations of neurons in the human brain can be described using a set of differential equations known as a neural mass model. These models can then be connected to describe activity in multiple brain regions and, by fitting them to human brain imaging data, statistical inferences can be made about changes in macroscopic connectivity among brain regions. For example, the strength of a connection from one region to another may be more strongly engaged in a particular patient population or during a specific cognitive task. Current statistical inference approaches use a Bayesian algorithm based on principles of local optimization and the assumption that uncertainty about model parameters (e.g. connectivity), having seen the data, follows a Gaussian distribution. This paper evaluates current methods against a global Bayesian optimization algorithm and finds that the two approaches (local/global) agree about which model is best, but finds that the global approach produces better parameter estimates.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by two grants from the Wellcome Trust (<ext-link ext-link-type="uri" xlink:href="http://www.wellcome.ac.uk" xlink:type="simple">www.wellcome.ac.uk</ext-link>). WP was funded by grant number 091593/Z/10/Z, and BS was funded by grant number 088130/Z/09/Z. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="4"/>
<page-count count="25"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data used in this study are generated from mathematical models which are available from <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/download/spm12_updates/" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm/download/spm12_updates/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Dynamical systems models instantiated using differential equations are a mainstay of modern neuroscience and provide mathematical descriptions of neuronal activity over multiple spatial and temporal scales [<xref ref-type="bibr" rid="pcbi.1004797.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref002">2</xref>]. In imaging neuroscience a widely adopted framework, called Dynamic Causal Modelling (DCM), has been developed for fitting such models to brain imaging data using a Bayesian approach [<xref ref-type="bibr" rid="pcbi.1004797.ref003">3</xref>]. This allows inferences to be made about changes in parameters (eg. effective connectivity) in the human brain using noninvasive imaging data. There is now a library of DCMs which differ according to their level of biological realism and the data features they explain. DCM can be applied to fMRI [<xref ref-type="bibr" rid="pcbi.1004797.ref003">3</xref>], EEG and MEG [<xref ref-type="bibr" rid="pcbi.1004797.ref004">4</xref>] and invasive electrophysiological data [<xref ref-type="bibr" rid="pcbi.1004797.ref005">5</xref>].</p>
<p>The Bayesian approach to model fitting in DCM is based on the Variational Laplace (VL) algorithm [<xref ref-type="bibr" rid="pcbi.1004797.ref006">6</xref>]. One of its core assumptions, the ‘Laplace Assumption’, is that the posterior distribution is Gaussian. This assumption is typically instantiated by finding the maximum posterior parameter vector, using numerical optimisation, and making a Taylor expansion around this value and retaining terms up to second order [<xref ref-type="bibr" rid="pcbi.1004797.ref007">7</xref>]. It has been found to be more robust than higher-order moment expansions on empirical data [<xref ref-type="bibr" rid="pcbi.1004797.ref008">8</xref>]. In VL, the posterior is assumed to factorise into a product of probability distributions, one over latent variables controlling noise variances and one over model parameters. Each distribution is multivariate Gaussian with mean and covariance that are iteratively updated to maximise an approximation to the model evidence [<xref ref-type="bibr" rid="pcbi.1004797.ref006">6</xref>].</p>
<p>The Laplace approximation is attractive because it provides a computationally simple method for both quantifying posterior uncertainty in model parameters and approximating the model evidence for Bayesian model comparison.</p>
<p>A theoretical motivation for the the Laplace approximation is that the posterior will tend to a Gaussian in the limit where the number of data points goes to infinity [<xref ref-type="bibr" rid="pcbi.1004797.ref009">9</xref>]. But as previously noted in the context of DCM [<xref ref-type="bibr" rid="pcbi.1004797.ref010">10</xref>], it is questionable as to whether posteriors are Gaussian for datasets that are encountered in practice which naturally have a finite number of data points. The VL algorithm has two potential weaknesses (i) as with any local optimisation method working in a non-convex domain [<xref ref-type="bibr" rid="pcbi.1004797.ref011">11</xref>] it may fall into a local maxima and (ii) the distribution around the maxima may be non-Gaussian.</p>
<p>In this paper we compare VL to Monte Carlo methods in the challenging context of identifying Neural Mass Models (NMMs) [<xref ref-type="bibr" rid="pcbi.1004797.ref012">12</xref>]. The advantage of Monte Carlo methods is that, provided the sampling process runs for a sufficiently long time, the samples converge in distribution to the exact posterior. This obviates the need for Gaussian assumptions but at the cost of potentially very long sampling times. To address these issues this paper uses the Annealed Importance Sampling (AIS) algorithm [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] with proposals made using a Langevin Monte Carlo (LMC) procedure [<xref ref-type="bibr" rid="pcbi.1004797.ref014">14</xref>]. The use of AIS has two benefits (i) it can accomodate multiple local maxima and (ii) it provides an estimate of the Bayesian model evidence. The use of LMC improves convergence properties because proposals are made using local gradient and curvature information [<xref ref-type="bibr" rid="pcbi.1004797.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref015">15</xref>].</p>
<p>Previously, the Metropolis-Hastings (MH) algorithm has been used to validate VL in the context of DCM for fMRI [<xref ref-type="bibr" rid="pcbi.1004797.ref016">16</xref>]. Whilst these findings are largely consistent with the Laplace assumption this study is incomplete in a number of respects (i) only results from a single Markov chain were reported thus raising the possibility that a local maxima was found, (ii) no sample-based estimate of the model evidence was provided, and (iii) the neurodynamical models used in fMRI are based on linear dynamical systems, so this finding may not hold for the nonlinear dynamical models [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>] underlying other DCMs such as those for M/EEG data.</p>
<p>This paper assesses how well the two Bayesian estimation algorithms (AIS-LMC and VL) perform inference for NMMs. These models have been chosen as they are highly nonlinear and underlie the first proposed DCM for M/EEG data [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>]. In order to validate our software implementation and fine tune parameters of the AIS algorithm, we additionally evaluate these algorithms in the simpler context of linear and nonlinear regression models.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>In what follows <inline-formula id="pcbi.1004797.e001"><alternatives><graphic id="pcbi.1004797.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>;</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes a multivariate Gaussian variable <italic>x</italic> with mean <italic>m</italic> and precision Λ. We consider Bayesian inference for data <italic>Y</italic>, or <italic>y</italic>, models with parameters <italic>w</italic>, priors <italic>p</italic>(<italic>w</italic>) and likelihoods <italic>p</italic>(<italic>Y</italic>|<italic>w</italic>) or <italic>p</italic>(<italic>y</italic>|<italic>w</italic>). All models in this paper use Gaussian priors with mean <italic>μ</italic> and precision Λ. In the subsections that follow we describe the AIS algorithm and show how LMC can be used within it to provide proposals. We then describe the linear regression, nonlinear regression and neural mass models that we will use to test the inference methods. To provide a convenient reference for some of the underlying concepts we provide supplementary material on Importance Sampling <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>, Fisher Information <xref ref-type="supplementary-material" rid="pcbi.1004797.s002">S2 Text</xref>, Neural Mass Models <xref ref-type="supplementary-material" rid="pcbi.1004797.s003">S3 Text</xref>, Variational Laplace <xref ref-type="supplementary-material" rid="pcbi.1004797.s004">S4 Text</xref> and Chib’s method for estimating model evidence <xref ref-type="supplementary-material" rid="pcbi.1004797.s005">S5 Text</xref>.</p>
<sec id="sec003">
<title>Annealed Importance Sampling</title>
<p>Annealed Importance Sampling (AIS) [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] provides samples from a posterior density using a sequence of densities at a series of monotonically increasing inverse temperatures <italic>β</italic><sub><italic>j</italic></sub> with <italic>j</italic> = 0..<italic>J</italic>, <italic>β</italic><sub>0</sub> = 0 and <italic>β</italic><sub><italic>J</italic></sub> = 1. For the <italic>j</italic>th temperature the algorithm produces a sample from the unnormalised density
<disp-formula id="pcbi.1004797.e002"><alternatives><graphic id="pcbi.1004797.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
An independent sample <italic>w</italic><sup>(<italic>i</italic>)</sup> from the posterior density is produced by generating a sequence of points <italic>w</italic><sub>1</sub>, <italic>w</italic><sub>2</sub>, … <italic>w</italic><sub><italic>J</italic></sub> as follows
<list list-type="bullet"><list-item><p>Generate <italic>w</italic><sub>1</sub> from <italic>p</italic>(<italic>w</italic>)</p></list-item> <list-item><p>Generate <italic>w</italic><sub>2</sub> from <italic>w</italic><sub>1</sub> using <italic>T</italic><sub>1</sub>(<italic>w</italic><sub>2</sub>|<italic>w</italic><sub>1</sub>)</p></list-item> <list-item><p>…</p></list-item> <list-item><p>Generate <italic>w</italic><sub><italic>j</italic></sub> from <italic>w</italic><sub><italic>j</italic>−1</sub> using <italic>T</italic><sub><italic>j</italic>−1</sub>(<italic>w</italic><sub><italic>j</italic></sub>|<italic>w</italic><sub><italic>j</italic>−1</sub>)</p></list-item> <list-item><p>…</p></list-item> <list-item><p>Generate <italic>w</italic><sub><italic>J</italic></sub> from <italic>w</italic><sub><italic>J</italic>−1</sub> using <italic>T</italic><sub><italic>J</italic>−1</sub>(<italic>w</italic><sub><italic>J</italic></sub>|<italic>w</italic><sub><italic>J</italic>−1</sub>)</p></list-item></list>
and then let <italic>w</italic><sup>(<italic>i</italic>)</sup> = <italic>w</italic><sub><italic>J</italic></sub>. We refer to the process of producing a single independent sample as a ‘trajectory’. The transition densities <italic>T</italic><sub><italic>j</italic></sub> can be chosen in any of the usual ways for constructing Markov chains [<xref ref-type="bibr" rid="pcbi.1004797.ref018">18</xref>] and may themselves involve several steps. The only requirement is that <italic>T</italic><sub><italic>j</italic></sub> is chosen to leave <italic>f</italic><sub><italic>j</italic></sub> as the invariant distribution. For example, for a simple density estimation problem, Neal [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] specified each <italic>T</italic><sub><italic>j</italic></sub> to be a sequence of Metropolis moves each defined using an isotropic Gaussian proposal with increasing width. For a linear regression problem with non-Gaussian priors he employed a Hamiltonian Monte-Carlo (HMC) approach [<xref ref-type="bibr" rid="pcbi.1004797.ref019">19</xref>]. In this paper we will use Langevin Monte Carlo (LMC), as recent work shows this to provide higher effective sample size per unit of computation time as compared to HMC [<xref ref-type="bibr" rid="pcbi.1004797.ref015">15</xref>].</p>
<p>The above process is repeated <italic>i</italic> = 1..<italic>I</italic> times to produce <italic>I</italic> independent samples from the posterior density. Because the samples are produced independently, without interaction among trajectories, the AIS algorithm is amenable to ‘embarrassing parallelization’ [<xref ref-type="bibr" rid="pcbi.1004797.ref020">20</xref>]. Specifically, trajectories can be assigned to individual computer processors or processor cores thus greatly speeding up the implementation.</p>
<p>Each sample is also accompanied by an importance weight
<disp-formula id="pcbi.1004797.e003"><alternatives><graphic id="pcbi.1004797.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:msup><mml:mi>v</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>…</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>J</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>J</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>J</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>J</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
which can be evaluated as
<disp-formula id="pcbi.1004797.e004"><alternatives><graphic id="pcbi.1004797.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
To avoid numerical overflow we first create adjusted weights <italic>u</italic><sub><italic>i</italic></sub> <disp-formula id="pcbi.1004797.e005"><alternatives><graphic id="pcbi.1004797.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mo>(</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>v</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
and let <inline-formula id="pcbi.1004797.e006"><alternatives><graphic id="pcbi.1004797.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> be the mean adjusted weight. The normalised importance weights are
<disp-formula id="pcbi.1004797.e007"><alternatives><graphic id="pcbi.1004797.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msub><mml:mi>q</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
A derivation of the formula for the importance weights is provided in [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] and included in <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>. The variance of the importance weights is an indicator of the quality of the approximation to the posterior density [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>].</p>
<sec id="sec004">
<title>Annealing schedule</title>
<p>An important choice in any AIS implementation is the annealing schedule, that is, how to space the <italic>β</italic><sub><italic>j</italic></sub> over the (0, 1) interval. Calderhead and Girolami [<xref ref-type="bibr" rid="pcbi.1004797.ref021">21</xref>] show that, for estimates of the model evidence for linear regression models, the annealing schedule that minimises the Monte Carlo variance has a power-law form. Following [<xref ref-type="bibr" rid="pcbi.1004797.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref022">22</xref>] the applications in this paper use a 5th-order geometric annealing schedule
<disp-formula id="pcbi.1004797.e008"><alternatives><graphic id="pcbi.1004797.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mfrac><mml:mi>j</mml:mi> <mml:mi>J</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>Additionally, one must choose the number of trajectories, and number of temperatures per trajectory. In the original AIS paper [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] <italic>I</italic> = 1000 trajectories were used with either <italic>J</italic> = 200 or 1000 temperatures. The AIS algorithm has also been compared to a Variational Bayes (VB) approach for scoring graphical models [<xref ref-type="bibr" rid="pcbi.1004797.ref023">23</xref>]. This implementation used only <italic>I</italic> = 5 trajectories with <italic>J</italic> = 16,384 temperatures. Proposals were made using a standard MH step which is perhaps one reason for the very large number of temperatures required. Only with <italic>J</italic> &gt; 5000 temperatures did the AIS model evidence estimate exceed that produced by VB (which provides a provable lower bound [<xref ref-type="bibr" rid="pcbi.1004797.ref007">7</xref>]). In an application of AIS to score differential equation models [<xref ref-type="bibr" rid="pcbi.1004797.ref024">24</xref>], <italic>I</italic> = 10 trajectories with <italic>J</italic> = 40 temperatures were used along a 4th order geometric schedule, with a transition kernel implemented using an MH step with 4000 samples at each temperature. Because LMC provides better proposals than MH we envisage that a finer grained schedule can be used at similar computational expense. This will be examined in the results section in the context of linear and nonlinear regression models.</p>
</sec>
<sec id="sec005">
<title>Model evidence</title>
<p>The importance weight, or the average importance weight across multiple trajectories, provides an approximation to the model evidence <italic>p</italic>(<italic>y</italic>|<italic>m</italic>) for model <italic>m</italic>, as shown below. This section uses the notation <italic>p</italic>(<italic>y</italic>|<italic>w</italic>, <italic>m</italic>) and <italic>p</italic>(<italic>w</italic>|<italic>m</italic>) to make it explicit that the likelihood and prior depend on model assumptions. We define the normalising constant at each temperature as
<disp-formula id="pcbi.1004797.e009"><alternatives><graphic id="pcbi.1004797.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Z</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
We then have
<disp-formula id="pcbi.1004797.e010"><alternatives><graphic id="pcbi.1004797.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Z</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>Z</mml:mi> <mml:mi>J</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
Therefore
<disp-formula id="pcbi.1004797.e011"><alternatives><graphic id="pcbi.1004797.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:msub><mml:mi>Z</mml:mi> <mml:mi>J</mml:mi></mml:msub> <mml:msub><mml:mi>Z</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:msub><mml:mi>Z</mml:mi> <mml:mi>1</mml:mi></mml:msub> <mml:msub><mml:mi>Z</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mfrac><mml:mfrac><mml:msub><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>Z</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mfrac><mml:msub><mml:mi>Z</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:msub><mml:mi>Z</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:mo>…</mml:mo> <mml:mfrac><mml:msub><mml:mi>Z</mml:mi> <mml:mi>J</mml:mi></mml:msub> <mml:msub><mml:mi>Z</mml:mi> <mml:mrow><mml:mi>J</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mi>J</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>r</italic><sub><italic>j</italic></sub> = <italic>Z</italic><sub><italic>j</italic>+1</sub>/<italic>Z</italic><sub><italic>j</italic></sub>. We can then write
<disp-formula id="pcbi.1004797.e012"><alternatives><graphic id="pcbi.1004797.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>Z</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfrac> <mml:mo>∫</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:msub><mml:mi>Z</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mfrac> <mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where the last line indicates a Monte-Carlo approximation of the integral with samples <italic>w</italic><sub><italic>n</italic></sub> drawn from the distribution at temperature <italic>β</italic><sub><italic>j</italic></sub>. This can in turn be written as
<disp-formula id="pcbi.1004797.e013"><alternatives><graphic id="pcbi.1004797.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
For <italic>N</italic> = 1 we can therefore see that log <italic>p</italic>(<italic>y</italic>) is equal to <xref ref-type="disp-formula" rid="pcbi.1004797.e004">Eq 3</xref>. To avoid numerical overflow we compute the log evidence as
<disp-formula id="pcbi.1004797.e014"><alternatives><graphic id="pcbi.1004797.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>A</mml:mi> <mml:mi>I</mml:mi> <mml:mi>S</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
We can now see that estimation of the model evidence using the Prior Arithmetic Mean (PAM) (see <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>), in which the average likelihood is computed over samples drawn from the prior, is a special case of the AIS estimate with just two temperatures, <italic>β</italic><sub>1</sub> = 1 and <italic>β</italic><sub>0</sub> = 0. It is also possible to define a reverse annealing schedule in which the temperature is gradually increased and defines a path from the posterior to the prior [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>]. Agreement between forward and reverse estimates of the model evidence can then be used to ensure one has a sufficiently fine-grained annealing schedule [<xref ref-type="bibr" rid="pcbi.1004797.ref023">23</xref>]. For reverse schedules the Posterior Harmonic Mean (PHM) emerges as a special case of AIS with two temperatures (see <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>). AIS therefore generalises both PAM and PHM. In high dimensional spaces PAM underestimates the model evidence because it doesn’t sufficiently explore regions of high probability, whereas PHM overestimates it because it doesn’t sufficiently explore regions of low probability. These problems are ameliorated in AIS by the use of intermediate densities that form ‘bridges’ as described in a related method called bridge sampling [<xref ref-type="bibr" rid="pcbi.1004797.ref025">25</xref>].</p>
<p>In this paper our empirical results are based on forward annealing schedules only. Confidence intervals in model evidence estimates are provided using bootstrapping [<xref ref-type="bibr" rid="pcbi.1004797.ref026">26</xref>], by resampling the <italic>I</italic> estimates <italic>N</italic><sub><italic>boot</italic></sub> = 1000 times with replacement, computing the evidence for each, and finding the 5th and 95th percentiles. Thus, bootstrapping is implemented over trajectories.</p>
</sec>
</sec>
<sec id="sec006">
<title>Langevin Monte Carlo</title>
<p>In this paper the transition densities <italic>T</italic><sub><italic>j</italic></sub> in AIS are implemented using a Langevin Monte Carlo (LMC) sampler, which leads to proposals being accepted with high probability even for nonlinear and high dimensional inference problems, as it uses information about the gradient and curvature of the unnormalised density, <italic>f</italic><sub><italic>j</italic></sub>.</p>
<p>The use of LMC follows from the definition of the log joint and its gradient as a function of <italic>w</italic> <disp-formula id="pcbi.1004797.e015"><alternatives><graphic id="pcbi.1004797.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>|</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>L</mml:mi> <mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
A proposal is drawn as
<disp-formula id="pcbi.1004797.e016"><alternatives><graphic id="pcbi.1004797.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>∼</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>;</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mi>m</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>C</mml:mi> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mi>C</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>h</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msup><mml:mrow><mml:mfenced close=")" open="" separators=""><mml:mo>(</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi></mml:mfenced></mml:mrow> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where Λ is the prior precision, <italic>w</italic><sub><italic>s</italic></sub> is the <italic>s</italic>th sample, and <italic>h</italic> is a step size parameter (fixed at 0.5 for all applications in this paper). The quantity <italic>F</italic> is the Fisher Information matrix (see <xref ref-type="supplementary-material" rid="pcbi.1004797.s002">S2 Text</xref>) and quantifies the precision of the parameters conferred by the data. This has analytic forms for many probabilistic models such as logistic regression [<xref ref-type="bibr" rid="pcbi.1004797.ref014">14</xref>] and is readily computed for differential equation models using an approach based on forward sensitivity analysis [<xref ref-type="bibr" rid="pcbi.1004797.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref028">28</xref>].</p>
<p>The Metropolis-Hastings (MH) criterion is then applied to accept proposals with probability
<disp-formula id="pcbi.1004797.e017"><alternatives><graphic id="pcbi.1004797.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where <italic>p</italic><sub><italic>w</italic></sub>(<italic>w</italic><sub><italic>s</italic></sub>) = exp[<italic>L</italic>(<italic>w</italic><sub><italic>s</italic></sub>)]. The proposal is always accepted if <italic>r</italic> &gt; 1. We set <inline-formula id="pcbi.1004797.e018"><alternatives><graphic id="pcbi.1004797.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> if the sample is accepted and <italic>w</italic><sub><italic>s</italic>+1</sub> = <italic>w</italic><sub><italic>s</italic></sub> if it is rejected.</p>
<p>The above proposal (<xref ref-type="disp-formula" rid="pcbi.1004797.e016">Eq 14</xref>) has the same functional form as the Simplified Manifold MALA algorithm as applied to ODEs [<xref ref-type="bibr" rid="pcbi.1004797.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref027">27</xref>]. Here the ‘manifold’ is defined by <italic>C</italic> and <italic>m</italic> and its computation has been ‘simplified’ as the curvature has been assumed to be locally constant. For Gaussian likelihoods, this same local linearity assumption is also the basis of the Gauss-Newton optimization algorithm [<xref ref-type="bibr" rid="pcbi.1004797.ref029">29</xref>].</p>
<p>In the usual application of LMC [<xref ref-type="bibr" rid="pcbi.1004797.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref015">15</xref>], Eqs <xref ref-type="disp-formula" rid="pcbi.1004797.e016">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1004797.e017">15</xref>, are repeatedly applied until one obtains samples from the posterior density. However, in this paper we use LMC to provide a single sample at each temperature in an AIS trajectory. Specifically, the transition kernel, <italic>T</italic><sub><italic>j</italic>−1</sub>(<italic>w</italic><sub><italic>j</italic></sub>|<italic>w</italic><sub><italic>j</italic>−1</sub>), starts at <italic>w</italic><sub><italic>s</italic></sub> = <italic>w</italic><sub><italic>j</italic>−1</sub> and produces <inline-formula id="pcbi.1004797.e019"><alternatives><graphic id="pcbi.1004797.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>s</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> using the modifield log joint <italic>L</italic><sub><italic>j</italic>−1</sub>. This modification requires multiplication of the likelihood, gradient and Fisher information by <italic>β</italic><sub><italic>j</italic>−1</sub>. The LMC updates are otherwise identical. Because LMC is used to produce only a single sample at each temperature the total number of LMC steps is equal to the number of temperatures.</p>
<p>We now briefly comment on the computational scalability of the combined AIS-LMC algorithm. Because AIS is based on importance sampling its accuracy is proportional to the number of annealing runs (“trajectories”) [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>]. As trajectories are independent, and can be assigned to cores on multiple core computer architectures, the accuracy will therefore scale with the number of cores (at almost no increase in computer time). For a fixed number of cores computer time scales linearly with the number of trajectories. The computational bottleneck within each AIS trajectory is the evaluation of the gradient of the log joint and the Fisher information, required for each LMC step. These quantities can be efficiently computed for ODE models using forward sensitivity or adjoint methods [<xref ref-type="bibr" rid="pcbi.1004797.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref028">28</xref>]. The computation time of these methods scales linearly with the length of time series being modelled, and adjoint methods are typically more efficient than forward sensitivity methods if the number of parameters is much larger than the number of dynamical states.</p>
</sec>
<sec id="sec007">
<title>Linear Regression</title>
<p>In multiple linear regression an [<italic>N</italic> × 1] data vector <italic>y</italic> is generated as
<disp-formula id="pcbi.1004797.e020"><alternatives><graphic id="pcbi.1004797.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mi>y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>X</mml:mi> <mml:mi>β</mml:mi> <mml:mo>+</mml:mo> <mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>X</italic> is an [<italic>N</italic> × <italic>p</italic>] design matrix, <italic>β</italic> is a [<italic>p</italic> × 1] vector of regression coefficients, and <italic>e</italic> is an [<italic>N</italic> × 1] zero-mean IID Gaussian noise vector with entries having variance <italic>σ</italic><sup>2</sup>.</p>
</sec>
<sec id="sec008">
<title>Nonlinear Regression</title>
<p>To provide a simple nonlinear model with multiple maxima, we consider a regression model where the parameters of interest are nonlinearly related to the regression coefficients
<disp-formula id="pcbi.1004797.e021"><alternatives><graphic id="pcbi.1004797.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>y</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>β</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:msubsup><mml:mi>w</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
This model will have multiple maxima over the various combinations of positive and negative values of <italic>w</italic><sub><italic>i</italic></sub>.</p>
<p>We also consider an exponential approach-to-limit or ‘approach’ model where
<disp-formula id="pcbi.1004797.e022"><alternatives><graphic id="pcbi.1004797.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>60</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mfenced close="]" open="[" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi>e</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
with parameters <italic>w</italic><sub>1</sub> = log <italic>τ</italic> and <italic>w</italic><sub>2</sub> = log <italic>V</italic><sub><italic>a</italic></sub>. This models the ramping up of a voltage from −60 to −60 + <italic>V</italic><sub><italic>a</italic></sub> with a time constant <italic>τ</italic>, and has the same mathematical form as Biochemical Oxygen Demand (BOD) models [<xref ref-type="bibr" rid="pcbi.1004797.ref030">30</xref>] previously used to evaluate Bayesian inference methods [<xref ref-type="bibr" rid="pcbi.1004797.ref031">31</xref>].</p>
</sec>
<sec id="sec009">
<title>Neural Mass Models</title>
<sec id="sec010">
<title>Single region</title>
<p>In Neural Mass Models (NMMs) [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>], postsynaptic potentials (PSPs) at excitatory synapses are related to firing rates via convolutions with synaptic kernels
<disp-formula id="pcbi.1004797.e023"><alternatives><graphic id="pcbi.1004797.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>o</mml:mi> <mml:mi>u</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>e</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊗</mml:mo> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
where the population firing rate function
<disp-formula id="pcbi.1004797.e024"><alternatives><graphic id="pcbi.1004797.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
has parameters <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub>, and the synaptic kernel is given by an alpha function
<disp-formula id="pcbi.1004797.e025"><alternatives><graphic id="pcbi.1004797.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>e</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>H</mml:mi> <mml:mi>e</mml:mi></mml:msub> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>e</mml:mi></mml:msub></mml:mfrac> <mml:mi>t</mml:mi> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>e</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
with magnitude <italic>H</italic><sub><italic>e</italic></sub> and time constant <italic>τ</italic><sub><italic>e</italic></sub>. Inhibitory synapses are similarly defined but with kernels <italic>h</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and parameters <italic>H</italic><sub><italic>i</italic></sub>, <italic>τ</italic><sub><italic>i</italic></sub>.</p>
<p>The activity of a single neocortical unit is then defined by the convolution equations
<disp-formula id="pcbi.1004797.e026"><alternatives><graphic id="pcbi.1004797.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>p</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>p</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>γ</mml:mi> <mml:mn>4</mml:mn></mml:msub> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊗</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
where <italic>v</italic><sub><italic>pe</italic></sub> and <italic>v</italic><sub><italic>pi</italic></sub> are potentials at excitatory and inhibitory synapses in the pyramidal cell population, <inline-formula id="pcbi.1004797.e027"><alternatives><graphic id="pcbi.1004797.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> denotes the potential after a delay <italic>δ</italic><sub><italic>ii</italic></sub> due to signalling delays among the different populations within a single brain region. Following [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>] a first order Taylor series approximation is used to capture these delays, <inline-formula id="pcbi.1004797.e028"><alternatives><graphic id="pcbi.1004797.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>v</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>ii</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˙</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. The connection strengths among neural populations are specified by the parameters <italic>γ</italic><sub>1..4</sub>. These within-region values are also referred to as the ‘intrinsic connectivity’.</p>
<p>Each of the above convolution equations can be written as a second order differential equation, or two first order DEs, as shown in [<xref ref-type="bibr" rid="pcbi.1004797.ref012">12</xref>] (see also <xref ref-type="supplementary-material" rid="pcbi.1004797.s003">S3 Text</xref>). Thus a single cortical unit has <italic>N</italic><sub><italic>x</italic></sub> = 9 state variables. The input to the cortical region, <italic>u</italic>, is a surrogate for event-related subcortical brain activity and is specified by a Gaussian function peaking at 64ms post-stimulus with width 16ms.</p>
</sec>
<sec id="sec011">
<title>Two region model</title>
<p>David et al. [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>] describe how cortical units can be connected into hierarchical networks that follow known anatomical connectivity patterns [<xref ref-type="bibr" rid="pcbi.1004797.ref032">32</xref>]. A two region network with forward connection <italic>a</italic><sub>21</sub> (from region 1 to 2) and backward connection <italic>a</italic><sub>12</sub> is shown in <xref ref-type="fig" rid="pcbi.1004797.g001">Fig 1</xref>. The convolution equations for this network are given in <xref ref-type="supplementary-material" rid="pcbi.1004797.s003">S3 Text</xref>.</p>
<fig id="pcbi.1004797.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Neural mass model of two cortical regions in a hierarchical network.</title>
<p>The first unit receives thalamic input <italic>u</italic>, and projects output <italic>v</italic><sub><italic>p</italic></sub>(1) via a forward connection of strength <italic>a</italic><sub>21</sub> to region 2. The second unit produces output <italic>v</italic><sub><italic>p</italic></sub>(2) and projects it via a backward connection of strength <italic>a</italic><sub>12</sub> to region 1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g001" xlink:type="simple"/>
</fig>
<p>There are two between-region or ‘extrinsic’ connectivity parameters (<italic>a</italic><sub>12</sub>, <italic>a</italic><sub>21</sub>) and two extrinsic delay parameters (<italic>δ</italic><sub>12</sub> and <italic>δ</italic><sub>21</sub>). Additionally, we have four ‘intrinsic’ connectivity parameters (<italic>γ</italic><sub>1..4</sub>) and parameters of firing rate functions (<italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub>) that are constrained to be identical in each region. This gives a total of <italic>N</italic><sub><italic>p</italic></sub> = 10 neurophysiological variables to estimate.</p>
<p>The intrinsic delay parameters (<italic>δ</italic><sub>11</sub>, <italic>δ</italic><sub>22</sub>—one for each region) are assumed known. The synaptic time constants (<italic>τ</italic><sub><italic>e</italic></sub>, <italic>τ</italic><sub><italic>i</italic></sub>) and synaptic response magnitudes (<italic>H</italic><sub><italic>e</italic></sub>, <italic>H</italic><sub><italic>i</italic></sub>) are fixed to be the same for all regions, and are also assumed known. This two region neural mass model has <italic>N</italic><sub><italic>x</italic></sub> = 18 state variables. The differential equations are integrated to produce time series of currents and potentials for each population in each cortical unit, at <italic>N</italic><sub><italic>t</italic></sub> time points. The resulting ‘neuronal state matrix’ <italic>X</italic> is of dimension [<italic>N</italic><sub><italic>x</italic></sub> × <italic>N</italic><sub><italic>t</italic></sub>]. The generative model is then specified as
<disp-formula id="pcbi.1004797.e029"><alternatives><graphic id="pcbi.1004797.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mrow><mml:mi>Y</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>X</mml:mi> <mml:mo>+</mml:mo> <mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
where <italic>L</italic><sub>2</sub> is a matrix that picks off the pyramidal cell activities in each of the regions, and <italic>e</italic> is zero mean Gaussian noise. For the simulations in this paper <italic>Y</italic> is therefore a [2 × <italic>N</italic><sub><italic>t</italic></sub>] data matrix containing the pyramidal cell activities of each of the brain regions. In applications to empirical M/EEG data [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref033">33</xref>] an [<italic>N</italic><sub><italic>d</italic></sub> × <italic>N</italic><sub><italic>x</italic></sub>] lead field matrix <italic>L</italic> is used to model Event-Related Potentials (ERPs) at <italic>N</italic><sub><italic>d</italic></sub> sensors.</p>
<p>We assume that the noise variance on the <italic>s</italic>th output (where <italic>s</italic> = 1..2) is <inline-formula id="pcbi.1004797.e030"><alternatives><graphic id="pcbi.1004797.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. The model likelihood is therefore
<disp-formula id="pcbi.1004797.e031"><alternatives><graphic id="pcbi.1004797.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mi mathvariant="sans-serif">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>C</mml:mi> <mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
where <italic>w</italic> are the parameters, <inline-formula id="pcbi.1004797.e032"><alternatives><graphic id="pcbi.1004797.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004797.e033"><alternatives><graphic id="pcbi.1004797.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mi>e</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mtext mathvariant="sans-serif">diag</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The unknown neurophysiological variables are related to model parameters according to the transformations shown in <xref ref-type="supplementary-material" rid="pcbi.1004797.s011">S1 Table</xref> which enforce positivity and constrain parameters within a physiologically plausible range. The Gaussian prior over model parameters has zero mean <italic>μ</italic>, and Λ<sup>−1</sup> is a diagonal matrix with entries of 0.16 for the first two parameters (<italic>a</italic><sub>12</sub> and <italic>a</italic><sub>21</sub>) and 0.0625 for the rest. The above choice of parameter transformation and prior are the same as that used in DCM for ERP [<xref ref-type="bibr" rid="pcbi.1004797.ref033">33</xref>].</p>
</sec>
</sec>
<sec id="sec012">
<title>Testing for Normality</title>
<p>As the VL algorithm assumes that the posterior distribution is Gaussian it will be interesting to see if this is indeed the case. We use Royston’s test for multivariate normality [<xref ref-type="bibr" rid="pcbi.1004797.ref034">34</xref>] using a Matlab implementation by Trujillo-Ortiz et al [<xref ref-type="bibr" rid="pcbi.1004797.ref035">35</xref>]. This is a multivariate extension of the Shapiro-Wilks test and we apply it to Monte Carlo samples from the posterior densities produced by AIS. As these samples are independent there is no need for ‘thinning’ or assessments of Effective Sample Size [<xref ref-type="bibr" rid="pcbi.1004797.ref036">36</xref>].</p>
</sec>
<sec id="sec013">
<title>Software</title>
<p>The algorithms on which this research is based have been implemented in Matlab in the ‘Monte Carlo Inference (MCI)’ toolbox and will be distributed as part of a forthcoming release of the Statistical Parametric Mapping (SPM) package. AIS and LMC, for example, are implemented in the spm_mci_ais.m and spm_mci_lgv.m functions available in the subdirectory /toolbox/mci/inference/.</p>
</sec>
<sec id="sec014">
<title>Variational Laplace</title>
<p>The Variational Laplace (VL) algorithm is instantiated in the SPM software [<xref ref-type="bibr" rid="pcbi.1004797.ref033">33</xref>] (in the function spm_nlsi_GN.m) and described elsewhere [<xref ref-type="bibr" rid="pcbi.1004797.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref037">37</xref>]. We also include a brief mathematical description in <xref ref-type="supplementary-material" rid="pcbi.1004797.s004">S4 Text</xref>. In VL, the posterior is assumed to factorise into a product of probability distributions, one over latent variables controlling noise variances and one over model parameters. Each distribution is multivariate Gaussian with mean and covariance that are iteratively updated to maximise an approximation to the model evidence [<xref ref-type="bibr" rid="pcbi.1004797.ref006">6</xref>]. Importantly, the multivariate nature of each Gaussian allows parameter dependencies to be accommodated. This optimiser is the standard approach used for the majority of DCM applications in neuroimaging. Known noise variances (see below) are implemented for the VL algorithm by setting the prior over the log noise precision to have a mean corresponding to the true (known) value, and a variance of 10<sup>−8</sup> (i.e. very tight).</p>
<p>By default, the implementation of VL in SPM initialises parameters at the prior mean. A simple way of potentially handling optimisation problems with multiple maxima, however, is to run the VL algorithm multiple times where each run is initialised using a different sample from the prior. We will refer to this procedure as Multistart VL.</p>
</sec>
</sec>
<sec id="sec015" sec-type="results">
<title>Results</title>
<p>We present results on linear and nonlinear regression models to demonstrate the effect of the number of temperatures <italic>J</italic> and trajectories <italic>I</italic> in AIS. The algorithms were run on a high-end desktop computer (Hewlett Packard Z440) with 32G memory, 8 cores, and a 64-bit operating system. All the results are derived from synthetic data for which the ground truth parameters are known. Following other recent comparisons of inference algorithms for differential equation models [<xref ref-type="bibr" rid="pcbi.1004797.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref039">39</xref>], our simulations assume that the noise variances are known for models with Gaussian likelihoods.</p>
<p>All AIS results were produced using a fifth order geometric annealing schedule and the posterior mean was computed using the mean over trajectories. The AIS implementation was parallelized using the Matlab Parallel Computing toolbox such that independent ‘pool workers’ (in this case cores) were assigned to different trajectories. The distribution of normalised importance weights, <italic>u</italic><sub><italic>i</italic></sub>, is characterised in two ways. Firstly, by the entropy. For <italic>I</italic> trajectories the maximum entropy is log<sub>2</sub> <italic>I</italic> e.g. 5 bits for <italic>I</italic> = 32. Secondly, by the number of significantly non-zero values, <italic>I</italic><sub><italic>q</italic></sub>, which we define as the number above 0.01.</p>
<sec id="sec016">
<title>Linear Regression</title>
<p>We first provide results on a multiple linear regression model, as there are analytic formulae for the posterior distribution and model evidence [<xref ref-type="bibr" rid="pcbi.1004797.ref007">7</xref>], and the Laplace approximation is exact. This comprised <italic>p</italic> = 7 regressors chosen from a discrete cosine basis set over <italic>N</italic> = 20 ‘time points’, with additive noise of standard deviation <italic>σ</italic> = 0.2. The prior variances, <inline-formula id="pcbi.1004797.e034"><alternatives><graphic id="pcbi.1004797.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004797.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msubsup><mml:mo>Λ</mml:mo> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>p</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> were set to 10 for each regressor and the prior means, <italic>μ</italic><sub><italic>p</italic></sub> to zero. The regression coefficients were drawn from the prior.</p>
<p>The AIS algorithm was applied to this data using <italic>J</italic> = 512 temperatures and <italic>I</italic> = 32 independent samples. We fitted the true model (with 7 regressors) and a reduced model to the same data but this time using only the first 6 regressors.</p>
<p>Using the 32 samples produced by AIS, we could not reject the hypothesis that the posterior was Gaussian using Royston’s test for the full (<italic>p</italic> = 0.67) and reduced (<italic>p</italic> = 0.68) models. This is of course to be expected as the posterior distribution is indeed Gaussian for linear regression models [<xref ref-type="bibr" rid="pcbi.1004797.ref007">7</xref>]. For the full model, the normalised importance weights had high entropy, <italic>H</italic> = 4.07, and many trajectories had significant weight, <italic>I</italic><sub><italic>q</italic></sub> = 21.</p>
<p>The AIS estimates of the log model evidences for the full, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), and reduced models, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>) and the corresponding log Bayes factor, and computation times, are provided in <xref ref-type="table" rid="pcbi.1004797.t001">Table 1</xref>. The estimates very closely match the analytic values. Note that the VL estimates correspond to the analytic values for the case of linear regression [<xref ref-type="bibr" rid="pcbi.1004797.ref007">7</xref>]. We then re-estimated the evidences using different numbers of AIS samples and temperatures, with results plotted in <xref ref-type="fig" rid="pcbi.1004797.g002">Fig 2</xref>. Theses results show good agreement with analytic values for <italic>J</italic> = 128 and above. The error bars on AIS model evidence estimates were computed using bootstrapping (over trajectories) as described in the section on ‘Annealed Importance Sampling’, in the subsection on ‘Model Evidence’.</p>
<table-wrap id="pcbi.1004797.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.t001</object-id>
<label>Table 1</label>
<caption>
<title>Evidence and Bayes Factor Approximations (Single Run).</title>
</caption>
<alternatives>
<graphic id="pcbi.1004797.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Model</th>
<th align="center" colspan="2">Estimate</th>
<th align="center" colspan="2">Time(s)</th>
</tr>
<tr>
<th align="left"/>
<th align="left">VL</th>
<th align="left">AIS</th>
<th align="left">VL</th>
<th align="left">AIS</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Linear, LogEv, Full</td>
<td align="char" char=".">-11.02*</td>
<td align="char" char=".">-11.00</td>
<td align="left">0.005</td>
<td align="left">15.4</td>
</tr>
<tr>
<td align="left">Linear, LogEv, Red</td>
<td align="char" char=".">-23.97*</td>
<td align="char" char=".">-23.94</td>
<td align="left">0.002</td>
<td align="left">3.1</td>
</tr>
<tr>
<td align="left">Linear, LogBF</td>
<td align="char" char=".">12.95*</td>
<td align="char" char=".">12.94</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">Approach, LogEv, Full</td>
<td align="char" char=".">-73.88</td>
<td align="char" char=".">-73.77</td>
<td align="left">0.58</td>
<td align="left">19.4</td>
</tr>
<tr>
<td align="left">Approach, LogEv, Red</td>
<td align="char" char=".">-783.62</td>
<td align="char" char=".">-783.61</td>
<td align="left">0.02</td>
<td align="left">2.9</td>
</tr>
<tr>
<td align="left">Approach, LogBF</td>
<td align="char" char=".">709.74</td>
<td align="char" char=".">709.84</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="left">Neural Mass, LogEv, Full</td>
<td align="char" char=".">1524.1</td>
<td align="char" char=".">1563.6</td>
<td align="left">22</td>
<td align="left">5290</td>
</tr>
<tr>
<td align="left">Neural Mass, LogEv, Red</td>
<td align="char" char=".">1288.4</td>
<td align="char" char=".">1293.4</td>
<td align="left">24</td>
<td align="left">4610</td>
</tr>
<tr>
<td align="left">Neural Mass, LogBF</td>
<td align="char" char=".">235.74</td>
<td align="char" char=".">270.2</td>
<td align="left">-</td>
<td align="left">-</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001">
<p>These results are for a single run of each inference algorithm (AIS or VL). AIS estimates from <italic>I</italic> = 32 samples and <italic>J</italic> = 512 trajectories. The results for the linear model here* are for the analytic solution, which also corresponds to the VL solution.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="pcbi.1004797.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Linear regression.</title>
<p>AIS approximations of log evidence for a ‘full’ model with 7 parameters (top left), and a ‘reduced’ model with 6 parameters (top right) as a function of number of temperatures <italic>J</italic>. These approximations use <italic>I</italic> = 16 (blue), <italic>I</italic> = 32 (red) and <italic>I</italic> = 64 (magenta) trajectories. The black lines show the equivalent analytic quantities. The bottom left plot shows the difference between the AIS estimated log Bayes factor and the true value. Vertical lines span the 5th and 95th percentiles from bootstrapping.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec017">
<title>Nonlinear Regression</title>
<sec id="sec018">
<title>Multiple maxima</title>
<p>We now report results for the nonlinear regression model that was designed to have multiple maxima. This model has two independent variables, <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub>, corresponding to two components of a discrete cosine basis set (the first two from the linear regression problem described above). The priors were set to be the same as for the linear regression simulation but the observation noise was increased to <italic>σ</italic> = 0.5. AIS was run using the same parameters as before and <xref ref-type="fig" rid="pcbi.1004797.g003">Fig 3</xref> shows samples from the posterior density which lie in all of the four posterior modes. The normalised importance weights had lower entropy than for the linear regression model, <italic>H</italic> = 3.42, and fewer trajectories with significant weight, <italic>I</italic><sub><italic>q</italic></sub> = 16. We can reject the hypothesis that the posterior is Gaussian using Royston’s test (<italic>p</italic> = 10<sup>−12</sup>). Thus AIS is able to accomodate multiple maxima as expected, and we correctly infer that the posterior is non-Gaussian. AIS is able to find the different maxima by virtue of employing multiple trajectories. <xref ref-type="fig" rid="pcbi.1004797.g003">Fig 3</xref> also shows the posterior mean for VL. We also ran Multistart VL (see section on Variational Laplace) with 32 starts and, as expected, it was also able to identify each of the four maxima. The posterior distribution for this example is multimodal and is therefore not well represented by the posterior mean. The AIS samples do, however, collectively provide a good description of the posterior distribution.</p>
<fig id="pcbi.1004797.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Log posterior of nonlinear regression model with multiple maxima.</title>
<p>The true parameters are <italic>w</italic><sub>1</sub> = <italic>w</italic><sub>2</sub> = 2. The circle denotes the prior mean. Samples from the posterior density as computed using AIS are shown as white dots (in each of the four maxima), and the blue cross close to the true parameters denotes the VL posterior mean.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec019">
<title>Approach to limit</title>
<p>We now report results for the approach-to-limit model. Data were generated with parameters <italic>V</italic><sub><italic>a</italic></sub> = 30, <italic>τ</italic> = 8 and Gaussian observation noise variance of unity. The prior has mean <italic>μ</italic> = [3, 1.6]<sup><italic>T</italic></sup> and precision Λ = <sans-serif>diag</sans-serif> ([16, 16]). A ‘reduced’ model was defined as only having the <italic>V</italic><sub><italic>a</italic></sub> parameter, thus producing a constant prediction over the time interval.</p>
<p>The AIS algorithm was applied to this data using <italic>J</italic> = 512 temperatures and <italic>I</italic> = 32 independent samples. Using the 32 samples produced by AIS, we could not reject the hypothesis that the posterior was Gaussian using Royston’s test (<italic>p</italic> = 0.96). The estimates of the model evidences and Bayes factors, shown in <xref ref-type="table" rid="pcbi.1004797.t001">Table 1</xref>, agree very well with those from VL. The normalised importance weights had high entropy, <italic>H</italic> = 4.27, and many trajectories had significant weight, <italic>I</italic><sub><italic>q</italic></sub> = 21.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004797.g004">Fig 4</xref> shows the AIS approximation to the log model evidence, using <italic>I</italic> = 32 samples, as a function of the number of temperatures <italic>J</italic>. We see good agreement with VL for <italic>J</italic> larger than 128. These simulation results were based on a second data set from the approach model created by sampling parameters from the prior and producing time series as above (because this is a different data set the log evidence values are different to those in <xref ref-type="table" rid="pcbi.1004797.t001">Table 1</xref>).</p>
<fig id="pcbi.1004797.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Approach-to-limit model.</title>
<p>AIS approximation of log evidence (red line) as a function of number of temperatures <italic>J</italic>. Vertical lines span the 5th and 95th percentiles from bootstrapping. These approximations use <italic>I</italic> = 32 samples. The VL approximation is shown as the black line.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec020">
<title>Neural Mass Models</title>
<p>To produce the following results the differential equations underlying the neural mass models (see <xref ref-type="supplementary-material" rid="pcbi.1004797.s003">S3 Text</xref>) were integrated using implicit backward-differentiation formulas (BDFs) and the resulting nonlinear equations solved using Newton’s method as implemented in the CVODES software [<xref ref-type="bibr" rid="pcbi.1004797.ref040">40</xref>]. With a relative tolerance of 10<sup>−2</sup> and an absolute tolerance 10<sup>−4</sup> this algorithm took an average of 75ms (averaged over ten runs) to produce the time series for the two-region model. This was lower than the 229ms for Matlab’s ODE15s integrator and the 90ms for SPM’s (implemented in the function spm_int_L.m). Both VL and AIS model estimation approaches therefore used the CVODES implementation. For the LMC algorithm used in AIS, gradients were computed using a forward sensitivity method as implemented in CVODES. For VL, gradients and curvatures were computed using central differences as implemented in the SPM function spm_nlsi_GN.m.</p>
<p>The simulations that follow make use of the two-region neural mass model depicted in <xref ref-type="fig" rid="pcbi.1004797.g001">Fig 1</xref> and described above. We generated data from a model with strong forward and backward connections. This is specified using the parameter values <italic>w</italic><sub>1</sub> = <italic>w</italic><sub>2</sub> = 1 which set the connections <italic>a</italic><sub>21</sub> and <italic>a</italic><sub>12</sub> according to <xref ref-type="supplementary-material" rid="pcbi.1004797.s011">S1 Table</xref>. The other parameters were set to zero. Data was then generated from the model as described above using zero mean additive Gaussian noise having standard deviation <italic>σ</italic><sub><italic>s</italic></sub> = 0.01. The resulting time series are shown in black in <xref ref-type="fig" rid="pcbi.1004797.g005">Fig 5</xref>. The priors over model parameters for Bayesian model fitting are as described at the end of the above subsection ‘Two-Region Model’ in the section on ‘Neural Mass Models’.</p>
<fig id="pcbi.1004797.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Time series from neural mass models.</title>
<p>The bottom figure shows the pyramidal cell potential in region 1 for the full model (black) and reduced model (red). The top figure shows the same for the pyramidal cells in region 2. The reduced model is identical to the full model except that it does not have the backward connection from region 2 to 1. All time series contain additive Gaussian observation noise with standard deviation <italic>σ</italic><sub><italic>s</italic></sub> = 0.01.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g005" xlink:type="simple"/>
</fig>
<p>We then fitted two models to the data using AIS, a ‘full’ model, which has the same structure as the model from which the data were generated, and a ‘reduced’ model which did not have the backward connection. We used <italic>I</italic> = 32, <italic>J</italic> = 512 and model estimation took 5290s and 4610s for the full and reduced models. The estimated log model evidences were 1563.6 for the full model and 1293.4 for the reduced model, corresponding to a Log Bayes Factor of 270.2 in favour of the full model. Using the 32 samples produced by AIS, we could not reject the hypothesis that the posterior was Gaussian using Royston’s test for the full (<italic>p</italic> = 0.32) and reduced (<italic>p</italic> = 0.15) models.</p>
<p>The AIS acceptance rates, <italic>a</italic><sub><italic>j</italic></sub>, averaged over the <italic>I</italic> = 32 trajectories, showed a gradual decrease with <italic>β</italic><sub><italic>j</italic></sub>. Averaging <italic>a</italic><sub><italic>j</italic></sub> over the high temperatures (<italic>β</italic><sub><italic>j</italic></sub> &lt; 0.5) gave a value of <italic>a</italic><sub><italic>high</italic></sub> = 0.43 and over the low temperatures of <italic>a</italic><sub><italic>low</italic></sub> = 0.19. These acceptance rates show that the cost function is being sufficiently explored and are in line with other Bayesian annealing methods [<xref ref-type="bibr" rid="pcbi.1004797.ref038">38</xref>]. The normalised importance weights had lower entropy than for the previous models above, <italic>H</italic> = 2.59, and fewer trajectories with significant weight, <italic>I</italic><sub><italic>q</italic></sub> = 12.</p>
<p>We also fitted the full and reduced models using VL, which took 22s and 24s (using 19 and 22 VL iterations) respectively. The estimated log model evidences were 1524.1 for the full model and 1288.4 for the reduced model, corresponding to a Log Bayes Factor of 235.74 in favour of the full model. Thus, the VL and AIS estimates agree reasonably well for the reduced model (within 0.4 per cent) but not for the full model (within only 2.5 per cent). Which are we to believe?</p>
<p>As described in <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>, it is also possible to use the VL posterior as a proposal density to provide an importance sampling estimate of the model evidence, without using any annealing. We refer to this procedure as ISVL and used it to generate 1000 samples. ISVL is highly computationally efficient, requiring only 90s of compute time. The estimate of the log evidence was 1562.8 for the full model which agrees very well with the AIS estimate (within 0.05 per cent).</p>
<p>
<xref ref-type="fig" rid="pcbi.1004797.g006">Fig 6</xref> plots the log evidences and log Bayes factor as a function of the number of temperatures <italic>J</italic>. These indicate that a fine-grained temperature resolution <italic>J</italic> is required to obtain good results. We also note that the log joint probability, <italic>L</italic> (see <xref ref-type="disp-formula" rid="pcbi.1004797.e015">Eq 13</xref>), of the posterior mean AIS solution increases with <italic>J</italic>, with values of <italic>L</italic> = 1583, 1584, 1588, 1589 for <italic>J</italic> = 64, 128, 256, 512. The log joint probability of the true parameters is <italic>L</italic> = 1589, whereas the log joint of the VL posterior mean is only <italic>L</italic> = 1157.</p>
<fig id="pcbi.1004797.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Two-region neural mass model: Temperature discretisation.</title>
<p>The red lines indicate the AIS approximation of log evidence for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>) (top left), log evidence for reduced model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>) (top right), and log Bayes factor for full versus reduced, as a function of number of temperatures <italic>J</italic>. The vertical lines span the 5th to 95th percentiles from bootstrapping. These approximations use <italic>I</italic> = 32 samples. The black lines show the equivalent quantities for VL.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g006" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1004797.g007">Fig 7</xref> plots the posterior densities from fitting the full model for VL and the AIS solution with <italic>J</italic> = 512. The estimates are generally in agreement but the AIS posterior means are closer to the true parameter values (<italic>w</italic><sub>1</sub> = <italic>w</italic><sub>2</sub> = 1, <italic>w</italic><sub>3</sub> to <italic>w</italic><sub>10</sub> equal to 0) for eight out of ten parameters. This is reflected in the higher joint probability mentioned above. Given that we know the true parameters we can also compute the Root Mean Squared Error (RMSE) between true and posterior mean parameters. For VL this is 0.21 and for AIS it is 0.11.</p>
<fig id="pcbi.1004797.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Two-region neural mass model: Posterior densities.</title>
<p>Univariate posterior densities from a single model fit for AIS (red) and VL (black). The vertical lines span the 5th to 95th percentiles and the circles denote the posterior means. AIS provides better estimates for eight out of ten parameters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g007" xlink:type="simple"/>
</fig>
<sec id="sec021">
<title>Multiple runs</title>
<p>Perhaps it is not surprising that AIS has found a better solution given that it requires 240 times as much computer time (for the full model and with <italic>J</italic> = 512). We therefore compared AIS to a Multistart VL procedure (see above description in the section ‘Variational Laplace’) using 240 multi-starts, so as to equate computation time with AIS. The best solution had a log joint of <italic>L</italic> = 1428. The remaining solutions had a log joint of less than <italic>L</italic> = 1275, with 84% having 1130 ≤ <italic>L</italic> ≤ 1175. Our initial solution (with <italic>L</italic> = 1157) is therefore fairly typical. On this evidence multi-start VL doesn’t seem to be the best strategy.</p>
<p>Both AIS and VL will produce slightly different results over different runs of the algorithm (sampling trajectories for AIS, initialisations for VL). To quantify this variation we ran each algorithm twenty times and report the mean results and standard deviations for estimates of the log Bayes Factors, log joint density and RMSE in tables <xref ref-type="table" rid="pcbi.1004797.t002">2</xref>, <xref ref-type="table" rid="pcbi.1004797.t003">3</xref> and <xref ref-type="table" rid="pcbi.1004797.t004">4</xref>.</p>
<table-wrap id="pcbi.1004797.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.t002</object-id>
<label>Table 2</label>
<caption>
<title>Evidence and Bayes Factor Approximations (Multiple Runs).</title>
</caption>
<alternatives>
<graphic id="pcbi.1004797.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">VL</th>
<th align="left">AIS</th>
<th align="left">AMC4-PHM</th>
<th align="left">AMC4-Chib</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Linear, LogEv, Full</td>
<td align="left">-11.02*</td>
<td align="left">-11.07 (0.39)</td>
<td align="left">-0.62 (3.49)</td>
<td align="left">-11.31 (0.48)</td>
</tr>
<tr>
<td align="left">Linear, LogEv, Red</td>
<td align="left">-23.97*</td>
<td align="left">-24.00 (0.31)</td>
<td align="left">-14.09 (1.84)</td>
<td align="left">-24.06 (0.24)</td>
</tr>
<tr>
<td align="left">Linear, LogBF</td>
<td align="left">12.95*</td>
<td align="left">12.94 (0.49)</td>
<td align="left">13.48 (3.49)</td>
<td align="left">12.75 (0.56)</td>
</tr>
<tr>
<td align="left">App, LogEv, Full</td>
<td align="left">-60.85 (0.02)</td>
<td align="left">-60.85 (0.27)</td>
<td align="left">-57.42 (0.29)</td>
<td align="left">-60.86 (0.04)</td>
</tr>
<tr>
<td align="left">App, LogEv, Red</td>
<td align="left">-662.67 (0.00)</td>
<td align="left">-666.52 (0.13)</td>
<td align="left">-664.36 (0.59)</td>
<td align="left">-666.53 (0.02)</td>
</tr>
<tr>
<td align="left">App, LogBF</td>
<td align="left">605.68 (0.02)</td>
<td align="left">605.67 (0.33)</td>
<td align="left">606.94 (0.63)</td>
<td align="left">605.67 (0.05)</td>
</tr>
<tr>
<td align="left">NMM, LogEv, Full</td>
<td align="left">1524.11 (0.00)</td>
<td align="left">1563.12 (1.22)</td>
<td align="left">1405.94 (130.88)</td>
<td align="left">1476.05 (50.73)</td>
</tr>
<tr>
<td align="left">NMM, LogEv, Red</td>
<td align="left">1288.37 (0.00)</td>
<td align="left">1291.26 (5.10)</td>
<td align="left">63.81 (567.51)</td>
<td align="left">451.93 (541.37)</td>
</tr>
<tr>
<td align="left">NMM, LogBF</td>
<td align="left">235.7 (0.01)</td>
<td align="left">271.86 (5.27)</td>
<td align="left">1342.13 (599.68)</td>
<td align="left">1024.13 (559.24)</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001">
<p>AIS estimates from <italic>I</italic> = 32 samples and <italic>J</italic> = 512 trajectories. The results for the linear model here* are for the analytic solution, which also corresponds to the VL solution. Entries shows the mean values from 20 runs of each algorithm with standard deviations shown in brackets. ‘App’ denotes the Approach model and ‘NMM’ the two-region neural mass model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pcbi.1004797.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.t003</object-id>
<label>Table 3</label>
<caption>
<title>Log Joint Density of Posterior Mean (Multiple Runs).</title>
</caption>
<alternatives>
<graphic id="pcbi.1004797.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">VL</th>
<th align="left">AIS</th>
<th align="left">AMC4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Linear, Full</td>
<td align="left">-11.74*</td>
<td align="left">-11.87 (0.06)</td>
<td align="left">-12.07 (0.40)</td>
</tr>
<tr>
<td align="left">Linear, Red</td>
<td align="left">-24.67*</td>
<td align="left">-24.78 (0.06)</td>
<td align="left">-24.78 (0.10)</td>
</tr>
<tr>
<td align="left">App, Full</td>
<td align="left">-54.83 (0.02)</td>
<td align="left">-54.97 (0.03)</td>
<td align="left">-54.83 (0.00)</td>
</tr>
<tr>
<td align="left">App, Red</td>
<td align="left">-662.67 (0.00)</td>
<td align="left">-662.70 (0.05)</td>
<td align="left">-662.67 (0.00)</td>
</tr>
<tr>
<td align="left">NMM, Full</td>
<td align="left">1158.12 (13.57)</td>
<td align="left">1588.43 (1.09)</td>
<td align="left">1509 (51.27)</td>
</tr>
<tr>
<td align="left">NMM, Red</td>
<td align="left">-15911.88 (214.35)</td>
<td align="left">1330.18 (5.50)</td>
<td align="left">477.96 (541.59)</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001">
<p>AIS estimates from <italic>I</italic> = 32 samples and <italic>J</italic> = 512 trajectories. The results for the linear model here* are for the analytic solution, which also corresponds to the VL solution. Entries shows the mean values from 20 runs of each algorithm with standard deviations shown in brackets. ‘App’ denotes the Approach model and ‘NMM’ the two-region neural mass model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pcbi.1004797.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.t004</object-id>
<label>Table 4</label>
<caption>
<title>RMSE between Posterior Mean and True Parameters for Full Model (Multiple Runs).</title>
</caption>
<alternatives>
<graphic id="pcbi.1004797.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">VL</th>
<th align="left">AIS</th>
<th align="left">AMC4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Linear</td>
<td align="left">0.57 *</td>
<td align="left">0.58 (0.04)</td>
<td align="left">0.57 (0.05)</td>
</tr>
<tr>
<td align="left">App</td>
<td align="left">0.015 (0.004)</td>
<td align="left">0.013 (0.008)</td>
<td align="left">0.016 (0.003)</td>
</tr>
<tr>
<td align="left">NMM</td>
<td align="left">0.21 (0.00)</td>
<td align="left">0.11 (0.01)</td>
<td align="left">0.38 (0.13)</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001">
<p>AIS estimates from <italic>I</italic> = 32 samples and <italic>J</italic> = 512 trajectories. The results for the linear model here* are for the analytic solution, which also corresponds to the VL solution. Entries shows the mean values from 20 runs of each algorithm with standard deviations shown in brackets. ‘App’ denotes the Approach model and ‘NMM’ the two-region neural mass model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The VL estimates of LogBF for the neural mass model have very low standard deviation, a point which we will comment on further in the next subsection. The corresponding AIS estimates have a standard deviation (or Monte Carlo error) of 5.27. However, this is a small proportion of the absolute value of 271.86. For smaller Bayes factors we expect the AIS Monte Carlo error to be commensurately smaller [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>]. This was confirmed by running AIS ten times on the same neural mass models, but with data (additive noise) chosen to produce a signal to noise ratio of unity (see next section). The mean Log Bayes Factor was 11.14 with a standard deviation of 0.66.</p>
<p>To provide an indication as to what level of performance other MCMC methods can provide, we also implemented an Adaptive Monte Carlo (AMC) approach which has been applied to related problems [<xref ref-type="bibr" rid="pcbi.1004797.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004797.ref041">41</xref>]. Specifically we implemented “Algorithm 4” in [<xref ref-type="bibr" rid="pcbi.1004797.ref042">42</xref>] (which we refer to as AMC4) and collected 2000 samples. The proposal density was adapted for the first 600 samples, and these were then discarded as burn-in. The remaining 1400 samples provided the estimate of the posterior density and were used to compute the model evidence using the Posterior Harmonic Mean (PHM) method (see equation 9 in <xref ref-type="supplementary-material" rid="pcbi.1004797.s001">S1 Text</xref>). As the PHM is known to overestimate the model evidence we also implemented Chib’s method [<xref ref-type="bibr" rid="pcbi.1004797.ref043">43</xref>]. This uses samples from the posterior density and an additional set of samples produced by applying the proposal density to a chosen parameter vector (e.g. posterior mean). For completeness, this is described in <xref ref-type="supplementary-material" rid="pcbi.1004797.s005">S5 Text</xref>.</p>
<p>Whilst AMC4 worked well for the linear and approach models (with PHM overestimating the model evidence, as expected) it does not work so well for the neural mass model, as the model evidence approximations are highly variable.</p>
<p>Although AMC4 was run with a modest number of samples this was the same number as in [<xref ref-type="bibr" rid="pcbi.1004797.ref039">39</xref>], and results were not improved by running the algorithm for longer (we tried collecting 38,000 samples with 3,000 adaption/burn-in). Moreover, we implemented another AMC approach which had two separate phases of adaption (i) tuning of a global scaling parameter for 300 samples, to ensure acceptance rates of between 20 and 40 percent, (ii) tuning of proposal covariance for 300 samples using updates in [<xref ref-type="bibr" rid="pcbi.1004797.ref044">44</xref>]. Results were again not improved.</p>
</sec>
<sec id="sec022">
<title>Effect of signal to noise ratio</title>
<p>The results presented so far have been found in a very high Signal to Noise (SNR) regime, using a very small value for the observation noise standard deviation (SD). Here the SNR is defined as the ratio of the observation noise SD to the signal SD in one of the brain regions (taken arbitrarily to be region 2). So far we have used SNR = 16.</p>
<p>Figs <xref ref-type="fig" rid="pcbi.1004797.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1004797.g009">9</xref> show results for simulations in which the SNR was varied over a broad range. The results indicate that VL and AIS are generally in agreement, with monotonically increasing estimates of the log evidence as a function of SNR. For both VL and AIS, the log Bayes factors in favour of the full model are increasingly positive with data generated from the full model, and (generally) increasingly negative with data from the reduced model.</p>
<fig id="pcbi.1004797.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Data from ‘full’ neural mass model: Effect of SNR.</title>
<p>Estimates of the log model evidence for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), and reduced model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>), and Bayes factors for full versus reduced for VL (black) and AIS (red) over a range of SNRs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g008" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004797.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Data from ‘reduced’ neural mass model: Effect of SNR.</title>
<p>Estimates of the log model evidence for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), and reduced model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>), and Bayes factors for full versus reduced for VL (black) and AIS (red) over a range of SNRs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g009" xlink:type="simple"/>
</fig>
<p>There are a number of discrepancies, however, with larger disagreements at high SNR. Overall, AIS tends to produce higher estimates of the log evidence. This is shown more clearly in <xref ref-type="supplementary-material" rid="pcbi.1004797.s006">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004797.s007">S2</xref> Figs for data generated from the full model. ISVL estimates of the model evidence (obtained using 10,000 samples) are also on the high side but have large error bars. Additionally, the ISVL estimates for the reduced model at high SNR were roughly 1000 or more less than for AIS/VL and had huge error bars, so were not plotted on the same figure. We therefore conclude that ISVL is unreliable.</p>
<p>At low SNRs the AIS acceptance rates, <italic>a</italic><sub><italic>j</italic></sub>, averaged over the <italic>I</italic> = 32 trajectories, were relatively constant over <italic>β</italic><sub><italic>j</italic></sub> whereas for high SNRs there was a gradual decrease with <italic>β</italic><sub><italic>j</italic></sub>. For example, at SNR = 2, <italic>a</italic><sub><italic>high</italic></sub> = 0.53 and <italic>a</italic><sub><italic>low</italic></sub> = 0.48 whereas at SNR = 8, <italic>a</italic><sub><italic>high</italic></sub> = 0.49 and <italic>a</italic><sub><italic>low</italic></sub> = 0.34.</p>
<p>As earlier, our AIS posterior means tend to have higher log joint probability, <italic>L</italic>, than those from VL. This is demonstrated in <xref ref-type="fig" rid="pcbi.1004797.g010">Fig 10</xref> which plots the increase in <italic>L</italic> (over baseline VL) as a function of SNR. Here our baseline VL result uses the standard approach of initialisation with the prior mean. A multistart VL approach, however, can also produce better solutions. We used 240 multistarts as before. The maximum number of VL iterations over all multistarts was 42 (which did not exceed our maximal number of 128) and this individual model fit took 58s. <xref ref-type="fig" rid="pcbi.1004797.g010">Fig 10</xref> plots the improvement offered by the best Multistart VL solution over the standard one showing, for example, an increase of Δ<italic>L</italic> = 73 at the highest SNR. Overall, however, we find the improvement offered by AIS to be superior, with an increase of Δ<italic>L</italic> = 300 at the highest SNR.</p>
<fig id="pcbi.1004797.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004797.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Data from ‘full’ neural mass model: AIS versus Multistart VL.</title>
<p>The red curve plots the difference in log joint of the posterior mean from AIS versus that from VL, <italic>L</italic><sub><italic>AIS</italic></sub> − <italic>L</italic><sub><italic>VL</italic></sub>, and the black curve plots the difference in log joint for the best multistart VL versus VL, <italic>L</italic><sub><italic>MVL</italic></sub> − <italic>L</italic><sub><italic>VL</italic></sub>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.g010" xlink:type="simple"/>
</fig>
<p>Perhaps surprisingly, there was hardly any improvement (or variation) in estimates of the VL model evidence or Bayes Factors over multistarts. As shown in <xref ref-type="supplementary-material" rid="pcbi.1004797.s009">S4 Fig</xref>, the variations in Log Bayes Factors are no greater than 0.1 (Bayes Factor = 1.1). Thus, for this neural mass model, VL model inferences show no meaningful variation over multistarts (according to [<xref ref-type="bibr" rid="pcbi.1004797.ref045">45</xref>] Bayes factors of less than 3 are ‘barely worth a mention’). This is to be contrasted with large variations in the posterior mean over parameters (which led to the improvements in Multistart over baseline VL in <xref ref-type="fig" rid="pcbi.1004797.g010">Fig 10</xref>—see also <xref ref-type="supplementary-material" rid="pcbi.1004797.s008">S3 Fig</xref>). This result can perhaps be understood by noting that the model evidence approximation is the cost function that is optimised by VL, as contrasted to more standard Laplace approaches which find parameters that maximise the log joint.</p>
<p>In our initial (high SNR) comparison of AIS and VL estimates of model evidence the discrepancy was larger for the full than for the reduced model (see <xref ref-type="fig" rid="pcbi.1004797.g006">Fig 6</xref>). This did not translate, however, into an incorrect sign in the resulting log Bayes factor as the difference between full and reduced model evidences dominated. A potential concern therefore is that when the model evidences of two models are more similar, errors in evidence estimates will result in errors in log Bayes factors that could produce radically different inferences. However, examples of when model evidences are more similar are provided in the low SNR regimes in Figs <xref ref-type="fig" rid="pcbi.1004797.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1004797.g009">9</xref>. <xref ref-type="fig" rid="pcbi.1004797.g008">Fig 8</xref>, for example, shows that even at the lowest SNR VL agrees with AIS in correctly favouring the more complex model. To examine this further we repeated the simulations at even lower SNR. The results in <xref ref-type="supplementary-material" rid="pcbi.1004797.s010">S5 Fig</xref> show, reassuringly, that as the SNR reduces to zero so does the log Bayes Factor (it does not become negative); VL Bayes Factors therefore do not show a bias towards simpler models. Additionally, the VL algorithm exhibits similar behaviour in the context of DCM for fMRI (see eg. Fig. 2 in [<xref ref-type="bibr" rid="pcbi.1004797.ref037">37</xref>]).</p>
<p>The p-values from Royston’s tests for the various data sets are provided in <xref ref-type="supplementary-material" rid="pcbi.1004797.s012">S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004797.s013">S3</xref> Tables. Results are provided for both 32 and 64 AIS trajectories. Most conservatively, considering the 40 multiple statistical comparisons a Bonferroni-corrected p-value of 1.25 × 10<sup>−3</sup> or less would be seen as significant at the nominal 0.05 level. Given this threshold there is one significantly non-Gaussian posterior distribution. More descriptively, seven of the ten tests with 64 trajectories on data from the full model (two rightmost columns of <xref ref-type="supplementary-material" rid="pcbi.1004797.s012">S2 Table</xref>) have p-values of less than 0.05. We can therefore summarise these results by saying we have evidence for non-Gaussianity.</p>
</sec>
</sec>
</sec>
<sec id="sec023" sec-type="conclusions">
<title>Discussion</title>
<p>Annealed Importance Sampling has a number of appealing properties. It can provide accurate estimates of the posterior parameter distribution and of the model evidence by avoiding local maxima and without making assumptions of Gaussianity. Samples from AIS converge in distribution to the true posterior density. Sub-optimal model evidence approximations [<xref ref-type="bibr" rid="pcbi.1004797.ref046">46</xref>] based on the Prior Arithmetic Mean (PAM) or Posterior Harmonic Mean (PHM) emerge as special cases of AIS with only two temperatures. Unlike Markov chain Monte Carlo methods, the samples produced are not serially correlated thus making any corrections involving effective sample size unnecessary.</p>
<p>We have described an implementation of AIS using a transition kernel based on an LMC sampler. The use of LMC here is critical as it allows proposals to be made based on local gradient and curvature information. Our empirical results show that the resulting proposals are accepted with probabilities in a desirable range (similar to the target of 20 to 40% in Zhou et al. [<xref ref-type="bibr" rid="pcbi.1004797.ref038">38</xref>]) even for nonlinear dynamical systems models at low temperature.</p>
<p>We have compared AIS to inferences based on the VL approximation in the context of neural mass models. In terms of the estimation of Bayes factors, the two methods agree as to which model is best but report different degrees of belief, especially at high signal to noise ratio. AIS tends to produce higher model evidence estimates both for optimal and suboptimal models. AIS finds better parameter estimates than does VL, as quantified by the joint log probability, especially in data regimes with high signal to noise ratio. A possible explanation as to the dependence on SNR could be that there are more or deeper local minima at high SNR. Moreover, a multistart VL procedure with computer time matched to AIS does not find better solutions. Additionally, we found evidence of non-Gaussianity in the AIS posteriors. Thus it appears that AIS is useful due to its ability to avoid local maxima, and its ability to characterise non-Gaussian parameter posteriors.</p>
<p>We have also used an Importance Sampling procedure to estimate the model evidence. This method, which we’ve referred to as ISVL, is highly computationally efficient as it uses the posterior from VL as a proposal density, but it proved unreliable. Similarly, other more standard approaches such as AMC worked well on linear and nonlinear regression problems but it was not possible to derive good AMC-based model evidence estimates for neural mass models.</p>
<p>In order to apply AIS one must decide upon an annealing schedule and in this paper we used a 5th-order geometric schedule, discretised using 512 temperatures and explored using 32 trajectories. This proved sufficient over a range of statistical models from linear and nonlinear regression to nonlinear differential equation models. Our empirical work has shown that the required number of temperatures and trajectories did not show a strong dependence on the number of model parameters or model nonlinearity. However, the need to specify the parametric form of the schedule, number of temperatures and trajectories is clearly a weakness of the AIS approach and is an area of ongoing research.</p>
<p>Previous work in this direction has focussed on the Sequential Monte Carlo (SMC) method which can be viewed as a generalisation of AIS. SMC represents probability densities using particles, as in the particle filter, but is applied at a sequence of temperatures rather than to a sequence of temporally ordered data. In particular Zhou et al. [<xref ref-type="bibr" rid="pcbi.1004797.ref038">38</xref>] have shown how SMC can be used for model comparison. Automatic annealing schedules can be derived by resampling at every temperature so as to maximise the effective sample size of the particle ensemble. An alternative approach grounded in statistical physics is based on the notion of contact flows and thermodynamic processes [<xref ref-type="bibr" rid="pcbi.1004797.ref047">47</xref>].</p>
<p>A potential drawback of SMC as compared to AIS, however, is that because particles interact during optimisation, SMC is not amenable to embarrasing parallelisation. Additionally, an application of SMC to nonlinear differential equations [<xref ref-type="bibr" rid="pcbi.1004797.ref038">38</xref>] used a similar number of temperatures as we do (500 as compared to our 512) but used many more trajectories (1000 as compared to our 32). This suggests that SMC may be more computationally demanding. Another development in this direction is Langevin Importance Sampling [<xref ref-type="bibr" rid="pcbi.1004797.ref048">48</xref>] which does not require specification of an annealing schedule as temperatures are sampled using Langevin dynamics. This flexibility again comes at the cost of interaction among trajectories (or particles) and therefore also compromises parallelisation.</p>
<p>Beal [<xref ref-type="bibr" rid="pcbi.1004797.ref023">23</xref>] has also suggested interesting ways of improving AIS. First, automatic annealing schedules could be produced by introducing finer graining of temperatures in regions of the path for which forward and reverse estimates are inconsistent. Second, <xref ref-type="disp-formula" rid="pcbi.1004797.e013">Eq 11</xref> suggest that better model evidence estimates could be produced by generating more samples at each temperature. This algorithim would then become more similar to thermodynamic integration [<xref ref-type="bibr" rid="pcbi.1004797.ref046">46</xref>] which, however, is naturally more computationally demanding than AIS [<xref ref-type="bibr" rid="pcbi.1004797.ref024">24</xref>].</p>
<p>Whilst our model fitting using AIS was parallelised over multiple cores, alternative efforts can be made to speed up implementation. For example, Wang et al. [<xref ref-type="bibr" rid="pcbi.1004797.ref049">49</xref>] have shown how the integration of neural mass models can be implemented on Graphical Processing Units (GPUs), resulting in a reduction of computing time by a factor of approximately seven. Additionally, Aponte et al. [<xref ref-type="bibr" rid="pcbi.1004797.ref050">50</xref>] have pursued a similar GPU approach for DCM for fMRI and shown how it can be used in the context of model evidence computation using thermodynamic integration. This GPU approach has been used to estimate parameters of DCM for fMRI models using an Adaptive Monte Carlo algorithm, again resulting in an order of magnitude reduction in computation time [<xref ref-type="bibr" rid="pcbi.1004797.ref041">41</xref>]. See also [<xref ref-type="bibr" rid="pcbi.1004797.ref051">51</xref>] for generic methods for parallelisation of single Markov chains.</p>
<p>Dynamical models have also been fitted to neuroimaging data using a range of global optimisation methods. For example, mean field models have been fitted to EEG using particle swarm optimisation [<xref ref-type="bibr" rid="pcbi.1004797.ref052">52</xref>] and stochastic nonlinear oscillator models have been fitted to EEG using a multi-start algorithm [<xref ref-type="bibr" rid="pcbi.1004797.ref053">53</xref>]. Additionally, DCMs have been fitted to fMRI data using a method that combines local search with Gaussian process approximation [<xref ref-type="bibr" rid="pcbi.1004797.ref041">41</xref>]. This method provides better parameter estimates than VL with only a modest increase in computational cost (much less than AIS). However, like the other global optimisation methods (see also [<xref ref-type="bibr" rid="pcbi.1004797.ref054">54</xref>]), it does not produce an estimate of the posterior distribution or model evidence.</p>
<p>This paper has compared the ability of VL and AIS to make inferences about two-region neural mass models based on simulated data. These simulations are a caricature of the DCM for ERP approach [<xref ref-type="bibr" rid="pcbi.1004797.ref017">17</xref>] as they are simplified in a number of respects (i) we have fixed parameters such as time delays between regions, synaptic time constants and synaptic response magnitudes, to known true values, (ii) we have not estimated observation noise, (iii) we have used only two brain regions whereas most practical applications use upwards of four [<xref ref-type="bibr" rid="pcbi.1004797.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1004797.ref057">57</xref>], (iv) we have assumed that the electrical activities of brain regions are directly observed, rather than being filtered through a lead field matrix to produce observations in M/EEG sensor space, (v) we have used simulated rather than empirical M/EEG data. Further work will be needed to establish whether the findings from our caricature follow over to DCM for ERP.</p>
<p>This paper has used an independent model optimisation approach to compute Bayes factors, in which the evidence is computed separately for each model of interest. But in the context of AIS one can traverse a path from the posterior of one model to the posterior of another, with the resulting importance weights providing a direct approximation of the corresponding Bayes factor [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>]. Direct computation of Bayes factors in this way is also possible in the context of SMC and a transdimensional AIS algorithm [<xref ref-type="bibr" rid="pcbi.1004797.ref058">58</xref>]. If one has a nested model, as in the empirical NMM examples in this paper in which the reduced model is nested within the full model model, Savage-Dickey approximations can also be used [<xref ref-type="bibr" rid="pcbi.1004797.ref059">59</xref>]. It would be interesting to compare Savage-Dickey against the direct path integral methods based on AIS.</p>
<p>This paper has explored one method for combining VL and sampling methods, ISVL, in which the VL posterior is used as a proposal density for importance sampling. However, this method did not provide good estimates of the model evidence. Other proposals for combining sampling with variational methods view the sequence of samples produced by a Markov chain as auxiliary variables in a variational inference problem [<xref ref-type="bibr" rid="pcbi.1004797.ref060">60</xref>]. An alternative approach, proposed in [<xref ref-type="bibr" rid="pcbi.1004797.ref013">13</xref>] would be to use AIS to traverse a path from the VL posterior to the true posterior at a series of intermediate temperatures, another interesting avenue for future work.</p>
</sec>
<sec id="sec024">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004797.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Importance sampling.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s002" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Fisher information.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s003" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Neural mass models.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s004" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>Variational laplace.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s005" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>Chib’s estimate of model evidence.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s006" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Model evidence estimates for neural mass model: Low SNR.</title>
<p>Estimates of the log model evidence for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), and reduced model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>), at low SNR. Vertical lines indicate 95% confidence intervals.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s007" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Model evidence estimates for neural mass model: High SNR.</title>
<p>Estimates of the log model evidence for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), and reduced model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>r</italic>), at high SNR. Vertical lines indicate 95% confidence intervals.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s008" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>VL estimates of log joint over multiple restarts.</title>
<p>Estimates of the log joint for full model, log <italic>p</italic>(<italic>y</italic>|<italic>m</italic> = <italic>f</italic>), over multiple restarts and range of SNRs. The baseline VL value (initialisation from prior mean) is shown in red.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s009" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>VL estimates of Log Bayes Factors over multiple restarts.</title>
<p>Estimates of the Log Bayes Factor for full versus reduced models, over multiple restarts and range of SNRs. Data was generated from the full model. The baseline VL value (initialisation from prior mean) is shown in red.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s010" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>VL estimates of Log Bayes Factors at very low SNR.</title>
<p>VL estimates of the Log Bayes Factor for full versus reduced models in very low SNR regime. Data was generated from the full model and the graph plots the mean and 95% confidence intervals computed over 5 data realisations at each SNR.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s011" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Neural mass model: Parameter transformations.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s012" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s012" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Neural mass model: Gaussianity tests on full models.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004797.s013" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004797.s013" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Neural mass model: Gaussianity tests on reduced models.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004797.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source>. <publisher-name>MIT Press</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jirsa</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>4</volume>(<issue>8</issue>):<fpage>e1000092</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000092" xlink:type="simple">10.1371/journal.pcbi.1000092</ext-link></comment> <object-id pub-id-type="pmid">18769680</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Dynamic Causal Modelling</article-title>. <source>Neuroimage</source>. <year>2003</year>;<volume>19</volume>(<issue>4</issue>):<fpage>1273</fpage>–<lpage>1302</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1053-8119(03)00202-7" xlink:type="simple">10.1016/S1053-8119(03)00202-7</ext-link></comment> <object-id pub-id-type="pmid">12948688</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Dynamic causal modelling of distributed electromagnetic responses</article-title>. <source>Neuroimage</source>. <year>2009</year>;<volume>47</volume>(<issue>2</issue>):<fpage>590</fpage>–<lpage>601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2009.04.062" xlink:type="simple">10.1016/j.neuroimage.2009.04.062</ext-link></comment> <object-id pub-id-type="pmid">19398015</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moran</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jung</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kumagai</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Endepols</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Graf</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Dynamic causal models and physiological inference: a validation study using isoflurane anaesthesia in rodents</article-title>. <source>PLoS One</source>. <year>2011</year>;<volume>6</volume>(<issue>8</issue>):<fpage>e22790</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0022790" xlink:type="simple">10.1371/journal.pone.0022790</ext-link></comment> <object-id pub-id-type="pmid">21829652</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mattout</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ashburner</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Variational free energy and the Laplace approximation</article-title>. <source>Neuroimage</source>. <year>2007</year>;<volume>34</volume>(<issue>1</issue>):<fpage>220</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2006.08.035" xlink:type="simple">10.1016/j.neuroimage.2006.08.035</ext-link></comment> <object-id pub-id-type="pmid">17055746</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref007">
<label>7</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tierney</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kadane</surname> <given-names>J</given-names></name>. <article-title>Accurate Approximations for Posterior Moments and Marginal Densities</article-title>. <source>Journal of the American Statistical Association</source>. <year>1986</year>;<volume>81</volume>:<fpage>82</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1986.10478240" xlink:type="simple">10.1080/01621459.1986.10478240</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Walker</surname> <given-names>A</given-names></name>. <article-title>On the asymptotic behaviour of posterior distributions</article-title>. <source>Journal of the Royal Statistical Society</source>. <year>1969</year>;<volume>31</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>K</given-names></name>. <article-title>Dynamic causal modelling: A critical review of the biophysical and statistical foundations</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>58</volume>:<fpage>312</fpage>–<lpage>322</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2009.11.062" xlink:type="simple">10.1016/j.neuroimage.2009.11.062</ext-link></comment> <object-id pub-id-type="pmid">19961941</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref011">
<label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Nocedal</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>S</given-names></name>. <source>Numerical Optimization</source>. <publisher-name>Springer</publisher-name>; <year>1999</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grimbert</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Faugeras</surname> <given-names>O</given-names></name>. <article-title>Bifurcation analysis of Jansen’s neural mass model</article-title>. <source>Neural Comput</source>. <year>2006</year>;<volume>18</volume>(<issue>12</issue>):<fpage>3052</fpage>–<lpage>68</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.12.3052" xlink:type="simple">10.1162/neco.2006.18.12.3052</ext-link></comment> <object-id pub-id-type="pmid">17052158</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Neal</surname> <given-names>RM</given-names></name>. <article-title>Annealed Importance Sampling</article-title>. <source>Statistics and Computing</source>. <year>2001</year>;<volume>11</volume>:<fpage>125</fpage>–<lpage>139</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1008923215028" xlink:type="simple">10.1023/A:1008923215028</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Calderhead</surname> <given-names>B</given-names></name>. <article-title>Riemann manifold Langevin and Hamiltonian Monte Carlo methods</article-title>. <source>Journal of the Royal Statistical Society Series B</source>. <year>2011</year> 03;<volume>73</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2010.00765.x" xlink:type="simple">10.1111/j.1467-9868.2010.00765.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Gradient-based MCMC samplers for Dynamic Causal Modelling</article-title>. <source>Neuroimage</source>, <volume>125</volume>:<fpage>1107</fpage>–<lpage>1118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2015.07.043" xlink:type="simple">10.1016/j.neuroimage.2015.07.043</ext-link></comment> <object-id pub-id-type="pmid">26213349</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chumbley</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Fearn</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>S</given-names></name>. <article-title>A Metropolis-Hastings algorithm for dynamic causal models</article-title>. <source>Neuroimage</source>. <year>2007</year>;<volume>38</volume>(<issue>3</issue>):<fpage>478</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2007.07.028" xlink:type="simple">10.1016/j.neuroimage.2007.07.028</ext-link></comment> <object-id pub-id-type="pmid">17884582</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>David</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Mattout</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kilner</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Dynamic causal modeling of evoked responses in EEG and MEG</article-title>. <source>Neuroimage</source>. <year>2006</year> <month>May</month>;<volume>30</volume>(<issue>4</issue>):<fpage>1255</fpage>–<lpage>1272</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2005.10.045" xlink:type="simple">10.1016/j.neuroimage.2005.10.045</ext-link></comment> <object-id pub-id-type="pmid">16473023</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref018">
<label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Stern</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <source>Bayesian Data Analysis</source>. <publisher-name>Chapman and Hall</publisher-name>, <publisher-loc>Boca Raton</publisher-loc>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref019">
<label>19</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Neal</surname> <given-names>R</given-names></name>. <chapter-title>MCMC using Hamiltonian Dynamics</chapter-title>. In: <name name-style="western"><surname>Brooks</surname> <given-names>S</given-names></name> <name name-style="western"><surname>Gelman</surname> <given-names>GJ A</given-names></name>, <name name-style="western"><surname>Meng</surname> <given-names>X</given-names></name>, editors. <source>Handbook of Markov Chain Monte Carlo</source>. <publisher-name>CRC Press</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref020">
<label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Foster</surname> <given-names>I</given-names></name>. <source>Designing and building parallel programs</source>. <publisher-name>Addison Wesley</publisher-name>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Calderhead</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>. <article-title>Estimating Bayes factors via thermodynamic integration and population MCMC</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2009</year>;<volume>53</volume>(<issue>12</issue>):<fpage>4028</fpage>–<lpage>4045</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csda.2009.07.025" xlink:type="simple">10.1016/j.csda.2009.07.025</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Pettitt</surname> <given-names>A</given-names></name>. <article-title>Marginal likelihood estimation via power posteriors</article-title>. <source>Journal of the Royal Statistical Society: Series B</source>. <year>2008</year>;<volume>70</volume>(<issue>3</issue>):<fpage>589</fpage>–<lpage>607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2007.00650.x" xlink:type="simple">10.1111/j.1467-9868.2007.00650.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Beal</surname> <given-names>M</given-names></name>. <chapter-title>Variational Algorithms for Approximate Bayesian Inference</chapter-title>. <source>Gatsby Computational Neuroscience Unit</source>, <publisher-name>University College</publisher-name> <publisher-loc>London</publisher-loc>; <year>2003</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vyshemirsky</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>. <article-title>Bayesian ranking of biochemical system models</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>6</issue>):<fpage>833</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btm607" xlink:type="simple">10.1093/bioinformatics/btm607</ext-link></comment> <object-id pub-id-type="pmid">18057018</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Meng</surname> <given-names>X</given-names></name>. <article-title>Simulating Normalizing Constants: From Importance Sampling to Bridge Sampling to Path Sampling</article-title>. <source>Statistical Science</source>. <year>1998</year>;<volume>13</volume>(<issue>2</issue>):<fpage>163</fpage>–<lpage>185</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref026">
<label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Efron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>RJ</given-names></name>. <source>An introduction to the bootstrap</source>. <publisher-name>Chapman and Hall</publisher-name>; <year>1993</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Calderhead</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Girolami</surname> <given-names>M</given-names></name>. <article-title>Statistical analysis of nonlinear dynamical systems using differential geometric sampling methods</article-title>. <source>Interface Focus</source>. <year>2011</year>;<volume>1</volume>:<fpage>821</fpage>–<lpage>235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsfs.2011.0051" xlink:type="simple">10.1098/rsfs.2011.0051</ext-link></comment> <object-id pub-id-type="pmid">23226584</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Efficient Gradient Computation for Dynamical Models</article-title>. <source>Neuroimage</source>. <year>2014</year>;<volume>98</volume>:<fpage>521</fpage>–<lpage>527</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2014.04.040" xlink:type="simple">10.1016/j.neuroimage.2014.04.040</ext-link></comment> <object-id pub-id-type="pmid">24769182</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref029">
<label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Press</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Teukolsky</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Vetterling</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Flannery</surname> <given-names>BP</given-names></name>. <source>Numerical Recipes in C</source> (<edition>Second Edition</edition>). <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge</publisher-name>; <year>1992</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bates</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Watts</surname> <given-names>D</given-names></name>. <source>Nonlinear Regression Analysis and its Applications</source>. <publisher-name>Wiley</publisher-name>; <year>1988</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DiCiccio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>L</given-names></name>. <article-title>Computing Bayes Factors by Combining Simulation and Asymptotic Approximations</article-title>. <source>Journal of the American Statistical Association</source>. <year>1997</year>;<volume>92</volume>(<issue>439</issue>):<fpage>903</fpage>–<lpage>915</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1997.10474045" xlink:type="simple">10.1080/01621459.1997.10474045</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Felleman</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Essen</surname> <given-names>DCV</given-names></name>. <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cerebral Cortex</source>. <year>1991</year>;<volume>1</volume>:<fpage>1</fpage>–<lpage>47</lpage>. <object-id pub-id-type="pmid">1822724</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Litvak</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Mattout</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Henson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kilner</surname></name>, <etal>et al</etal>. <article-title>EEG and MEG data analysis in SPM8</article-title>. <source>Comput Intell Neurosci</source>. <year>2011</year>;<volume>2011</volume>:<fpage>852961</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/2011/852961" xlink:type="simple">10.1155/2011/852961</ext-link></comment> <object-id pub-id-type="pmid">21437221</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Royston</surname> <given-names>J</given-names></name>. <article-title>Approximating the Shapiro-Wilk W-Test for non-normality</article-title>. <source>Statistics and Computing</source>. <year>1992</year>;<volume>2</volume>:<fpage>117</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF01891203" xlink:type="simple">10.1007/BF01891203</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Trujillo-Ortiz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Walls</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Barba-Rojo</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cupul-Magana</surname> <given-names>L</given-names></name>. <article-title>Roystest:Royston’s Multivariate Normality Test</article-title>. <source>A MATLAB file</source>. <year>2007</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Geyer</surname> <given-names>C</given-names></name>. <article-title>Practical Markov Chain Monte Carlo</article-title>. <source>Statistical Science</source>. <year>1992</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/ss/1177011147" xlink:type="simple">10.1214/ss/1177011147</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>. <article-title>Comparing Dynamic Causal Models using AIC, BIC and Free Energy</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>59</volume>(<issue>1</issue>):<fpage>319</fpage>–<lpage>330</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.07.039" xlink:type="simple">10.1016/j.neuroimage.2011.07.039</ext-link></comment> <object-id pub-id-type="pmid">21864690</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Johansen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Aston</surname> <given-names>J</given-names></name>. <article-title>Towards automatic model comparison: an adaptive sequential Monte Carlo approach</article-title>. <source>ArCHIve</source>. <year>2013</year>;p. <fpage>1</fpage>–<lpage>33</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Gradient-free MCMC methods for Dynamic Causal Modelling</article-title>. <source>Neuroimage</source>;<volume>112</volume>:<fpage>375</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2015.03.008" xlink:type="simple">10.1016/j.neuroimage.2015.03.008</ext-link></comment> <object-id pub-id-type="pmid">25776212</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref040">
<label>40</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hindmarsh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Serban</surname> <given-names>R</given-names></name>. <source>User Documentation for CVODES, and ODE Solver with Sensitivity Analysis Capabilities</source>. <publisher-name>Centre for Applied Scientific Computing, Lawrence Livermore National Laboratory</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lomakina</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Paliwal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Diaconescu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Brodersen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Aponte</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Buhmann</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Inversion of Hierarchical Bayesian models using Gaussian processes</article-title>. <source>Neuroimage</source>. <year>2015</year>;<volume>118</volume>:<fpage>133</fpage>–<lpage>145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2015.05.084" xlink:type="simple">10.1016/j.neuroimage.2015.05.084</ext-link></comment> <object-id pub-id-type="pmid">26048619</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Andrieu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Thoms</surname> <given-names>J</given-names></name>. <article-title>A tutorial on adaptive MCMC</article-title>. <source>Statistics and Computing</source>. <year>2008</year>;<volume>18</volume>:<fpage>343</fpage>–<lpage>373</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11222-008-9110-y" xlink:type="simple">10.1007/s11222-008-9110-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chib</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jeliazkov</surname> <given-names>I</given-names></name>. <article-title>Marginal Likelihood from the Metropolis-Hastings Output</article-title>. <source>Journal of the American Statistical Association</source>. <year>2001</year>;<volume>96</volume>(<issue>453</issue>):<fpage>270</fpage>–<lpage>281</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haario</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Saksman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tamminen</surname> <given-names>J</given-names></name>. <article-title>An adaptive Metropolis Algorithm</article-title>. <source>Bernoulli</source>. <year>2001</year>;<volume>7</volume>(<issue>2</issue>):<fpage>223</fpage>–<lpage>242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/3318737" xlink:type="simple">10.2307/3318737</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name>. <article-title>Bayes factors</article-title>. <source>Journal of the American Statistical Association</source>. <year>1995</year>;<volume>90</volume>:<fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1995.10476572" xlink:type="simple">10.1080/01621459.1995.10476572</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lartillot</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Philippe</surname> <given-names>H</given-names></name>. <article-title>Computing Bayes factors using thermodynamic integration</article-title>. <source>Systematic Biology</source>. <year>2006</year>;<volume>55</volume>(<issue>2</issue>):<fpage>195</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/10635150500433722" xlink:type="simple">10.1080/10635150500433722</ext-link></comment> <object-id pub-id-type="pmid">16522570</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref047">
<label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Betancourt</surname> <given-names>M</given-names></name>. <source>Thermodynamic Monte Carlo</source>. <publisher-name>Department of Statistics, University of Warwick</publisher-name>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Ma J, Peng J, Wang S, Xu J. Estimating the partition function of graphical models using Langevin Importance Sampling. In: 16th International Conference on Artifical Intelligence and Statistics (AISTATS); 2013.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>C</given-names></name>. <article-title>Accelerating computation of DCM for ERP in MATLAB by external function calls to the GPU</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>(<issue>6</issue>).</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aponte</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Raman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Heinzle</surname> <given-names>J</given-names></name>. <article-title>MPDCM: A toolbox for massively parallel dynamic causal modelling</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>257</volume>:<fpage>7</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jneumeth.2015.09.009" xlink:type="simple">10.1016/j.jneumeth.2015.09.009</ext-link></comment> <object-id pub-id-type="pmid">26384541</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Calderhead</surname> <given-names>B</given-names></name>. <article-title>A general construction for parallelizing Metropolis-Hastings algorithms</article-title>. <source>Proceedings of the National Academcy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>49</issue>):<fpage>17408</fpage>–<lpage>17413</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1408184111" xlink:type="simple">10.1073/pnas.1408184111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bojak</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Liley</surname> <given-names>DTJ</given-names></name>. <article-title>Modeling the effects of anesthesia on the electroencephalogram</article-title>. <source>Phys Rev E</source>. <year>2005</year>;<volume>71</volume>(<issue>4</issue>):<fpage>041902</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.71.041902" xlink:type="simple">10.1103/PhysRevE.71.041902</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>P Ghorbanian</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Ashrafiuon</surname> <given-names>H</given-names></name>. <article-title>Stochastic non-linear oscillator models of EEG: the Alzheimer’s disease case</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2015</year>;<volume>9</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2015.00048" xlink:type="simple">10.3389/fncom.2015.00048</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mesejo</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Saillet</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>David</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Benar</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Warnking</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Forbes</surname> <given-names>F</given-names></name>. <article-title>Estimating biophysical parameters from BOLD signals through evolutionary-based optimization</article-title>. In: <source>Medical Image Computing and Computer-Assisted Intervention (MICCAI)</source>; <volume>2015</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1004797.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Garrido</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kilner</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Evoked brain responses are generated by feedback loops</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2007</year> <month>Dec</month>;<volume>104</volume>(<issue>52</issue>):<fpage>20961</fpage>–<lpage>20966</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0706274105" xlink:type="simple">10.1073/pnas.0706274105</ext-link></comment> <object-id pub-id-type="pmid">18087046</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Garrido</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gosseries</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Bruno</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boveroux</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schnakers</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Preserved feedforward but impaired top-down processes in the vegetative state</article-title>. <source>Science</source>. <year>2011</year> <month>May</month>;<volume>332</volume>(<issue>6031</issue>):<fpage>858</fpage>–<lpage>862</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1202043" xlink:type="simple">10.1126/science.1202043</ext-link></comment> <object-id pub-id-type="pmid">21566197</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schofield</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Crinion</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Changes in Auditory Feedback Connections Determine the Severity of Speech Processing Deficits after Stroke</article-title>. <source>J Neurosci</source>. <year>2012</year> <month>Mar</month>;<volume>32</volume>(<issue>12</issue>):<fpage>4260</fpage>–<lpage>4270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4670-11.2012" xlink:type="simple">10.1523/JNEUROSCI.4670-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22442088</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Karagiannis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Andrieu</surname> <given-names>C</given-names></name>. <article-title>Annealed Importance Sampling Reversible Jump MCMC Algorithms</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>2013</year>;<volume>22</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/10618600.2013.805651" xlink:type="simple">10.1080/10618600.2013.805651</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Post-hoc Bayesian model selection</article-title>. <source>Neuroimage</source>. <year>2011</year> <month>Jun</month>;<volume>56</volume>(<issue>4</issue>):<fpage>2089</fpage>–<lpage>2099</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2011.03.062" xlink:type="simple">10.1016/j.neuroimage.2011.03.062</ext-link></comment> <object-id pub-id-type="pmid">21459150</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004797.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Salimans T, Welling M. Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. In: International Confernece on Machine Learning; 2014.</mixed-citation>
</ref>
</ref-list>
</back>
</article>