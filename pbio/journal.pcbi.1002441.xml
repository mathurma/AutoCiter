<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01292</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002441</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Neuroimaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Behavior</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Decoding Unattended Fearful Faces with Whole-Brain Correlations: An Approach to Identify Condition-Dependent Large-Scale Functional Connectivity</article-title><alt-title alt-title-type="running-head">Functional Connectivity Decodes Emotion Perception</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pantazatos</surname>
            <given-names>Spiro P.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Talati</surname>
            <given-names>Ardesheer</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pavlidis</surname>
            <given-names>Paul</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
          <xref ref-type="aff" rid="aff5">
            <sup>5</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Hirsch</surname>
            <given-names>Joy</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff6">
            <sup>6</sup>
          </xref>
          <xref ref-type="aff" rid="aff7">
            <sup>7</sup>
          </xref>
          <xref ref-type="aff" rid="aff8">
            <sup>8</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>fMRI Research Center, Columbia University, New York, New York, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Physiology and Cellular Biophysics, Columbia University, New York, New York, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Psychiatry, Columbia University, New York, New York, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Department of Psychiatry, University of British Columbia, Vancouver, British Columbia, Canada</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Centre for High-throughout Biology, University of British Columbia, Vancouver, British Columbia, Canada</addr-line>       </aff><aff id="aff6"><label>6</label><addr-line>Department of Neuroscience, Columbia University, New York, New York, United States of America</addr-line>       </aff><aff id="aff7"><label>7</label><addr-line>Department of Radiology, Columbia University, New York, New York, United States of America</addr-line>       </aff><aff id="aff8"><label>8</label><addr-line>Department of Psychology, Columbia University, New York, New York, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">spiropan@gmail.com</email> (SPP); <email xlink:type="simple">joyhirsch@yahoo.com</email> (JH)</corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: SPP. Performed the experiments: AT SPP. Analyzed the data: SPP. Contributed reagents/materials/analysis tools: JH PP. Wrote the paper: SPP. Provided substantive intellectual and editorial contributions: JH PP AT.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>3</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>29</day>
        <month>3</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>3</issue><elocation-id>e1002441</elocation-id><history>
        <date date-type="received">
          <day>1</day>
          <month>9</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>8</day>
          <month>2</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Pantazatos et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Processing of unattended threat-related stimuli, such as fearful faces, has been previously examined using group functional magnetic resonance (fMRI) approaches. However, the identification of features of brain activity containing sufficient information to decode, or “brain-read”, unattended (implicit) fear perception remains an active research goal. Here we test the hypothesis that patterns of large-scale functional connectivity (FC) decode the emotional expression of implicitly perceived faces within single individuals using training data from separate subjects. fMRI and a blocked design were used to acquire BOLD signals during implicit (task-unrelated) presentation of fearful and neutral faces. A pattern classifier (linear kernel Support Vector Machine, or SVM) with linear filter feature selection used pair-wise FC as features to predict the emotional expression of implicitly presented faces. We plotted classification accuracy vs. number of top N selected features and observed that significantly higher than chance accuracies (between 90–100%) were achieved with 15–40 features. During fearful face presentation, the most informative and positively modulated FC was between angular gyrus and hippocampus, while the greatest overall contributing region was the thalamus, with positively modulated connections to bilateral middle temporal gyrus and insula. Other FCs that predicted fear included superior-occipital and parietal regions, cerebellum and prefrontal cortex. By comparison, patterns of spatial <italic>activity</italic> (as opposed to <italic>interactivity</italic>) were relatively uninformative in decoding implicit fear. These findings indicate that whole-brain patterns of interactivity are a sensitive and informative signature of unattended fearful emotion processing. At the same time, we demonstrate and propose a sensitive and exploratory approach for the identification of large-scale, condition-dependent FC. In contrast to model-based, group approaches, the current approach does not discount the multivariate, joint responses of multiple functional connections and is not hampered by signal loss and the need for multiple comparisons correction.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Brain activity is increasingly characterized by patterns of pair-wise correlations (large-scale functional connectivity) across the whole brain obtained from Blood Oxygen Level Dependent (BOLD) functional magnetic resonance imaging (fMRI). Typically this is done during resting states (i.e. no presented stimulus) to differentiate subjects based on individual variation or diagnosis. In the current work, we identify such patterns that are a sensitive signature of unattended processing of threat-related stimuli, allowing one to “brain-read” whether an individual was presented with a neutral or fearful face while they attended to non-expression-related stimulus features. These results further the understanding of the neural mechanisms sub-serving threat-detection and facial affect processing in healthy subjects, and may also help further our understanding of various disorders, such as anxiety and autism, which exhibit anomalies in these processes. At the same time, we propose an exploratory and sensitive approach for the identification of condition-dependent, large-scale functional connectivity. This approach is not based on statistical inference on functional connections averaged across subjects and contrasted between two conditions, but rather based on the informative contribution of each functional connection when attempting to predict between two conditions, using machine-learning based multivariate pattern analysis on training data from separate subjects.</p>
      </abstract><funding-group><funding-statement>This work was supported by a predoctoral fellowship (NRSA) F31MH088104-02 (SP), a K01 DA029598-01 (National Institute of Drug Abuse) and NARSAD Young Investigator Award (AT), a US Army TARDEC W56HZV-04-P-L (JH), and by the Michael Smith Foundation for Health Research and the Canadian Institutes for Health Research (PP). Subject scanning and recruitment was also funded by the following grants: Clinical Studies of Human Anxiety Disorders, core 4 [PI: MM Weissman] of PO1MH60970, Molecular Genetic Studies of Fear and Anxiety [PI: Gilliam/Hen] and Clinical Studies of Fear and Anxiety [PI: A. Fyer] Project 3 in PO1MH60970. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="11"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Faces with a fearful expression are thought to signal the presence of a significant, yet undetermined source of danger within the environment, or ‘ambiguous threat’ <xref ref-type="bibr" rid="pcbi.1002441-Ewbank1">[1]</xref>. Evidence from fMRI and evoked potentials (ERPs) suggest that fearful face processing can strongly affect brain systems responsible for face recognition and memory during implicit (consciously perceived but unattended) presentation of these stimuli <xref ref-type="bibr" rid="pcbi.1002441-Vuilleumier1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Vuilleumier2">[3]</xref>. Group-based fMRI studies have shown that the perception and processing of facial emotional expression engages multiple brain regions including the fusiform gyrus, superior temporal sulcus, thalamus, as well as affect-processing regions such as amygdala, insula, anterior cingulate cortex among others <xref ref-type="bibr" rid="pcbi.1002441-Haxby1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Ishai1">[7]</xref>. However, to the authors' knowledge, no study to date has identified features of brain activity that contain sufficient information to reliably decode, or “brain-read”, the threat-related emotional expression of unattended (implicitly perceived) faces within individual subjects. The identification of such features, though less well quantified as in group model-based studies, would have a greater capacity for representing distinctions between different cognitive-emotional perceptual states <xref ref-type="bibr" rid="pcbi.1002441-Norman1">[8]</xref>, and hence could contribute in advancing our understanding of the neural mechanisms that underlie threat detection and facial emotion processing.</p>
      <p>Most group fMRI approaches that have studied the neural correlates of emotional face perception have relied on univariate approaches <xref ref-type="bibr" rid="pcbi.1002441-Etkin1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Bishop1">[11]</xref> which identify regions correlated with a regressor-of-interest, but ignores any interactions with other regions. Bivariate approaches have been applied, but assess the interactivity (functional connectivity) of only one seed region (usually amygdala) with the rest of the brain <xref ref-type="bibr" rid="pcbi.1002441-Pezawas1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Etkin2">[13]</xref>. Even though several notable studies have taken a multivariate approach in assessing the effective connectivity among multiple brain regions during emotional face processing <xref ref-type="bibr" rid="pcbi.1002441-Fairhall1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Stein1">[16]</xref>, a limited number of nodes were included in the networks and they were selected based on a priori anatomical knowledge or on their activation in conventional, General Linear Model (GLM)-based mass univariate analyses. However, univariate GLM approaches make strong assumptions about the hemodynamic response (i.e. sustained periods of activation or deactivation relative to baseline), while functional connectivity offers a complementary and more data-driven and exploratory measure that makes use of temporal correlations to estimate functional connectivity <xref ref-type="bibr" rid="pcbi.1002441-Li1">[17]</xref>.</p>
      <p>There has been a recent surge of interest in examining the large-scale (i.e. pair-wise connectivity throughout the whole-brain) functional network architecture of the brain as a function of various cognitive processes or individual variation <xref ref-type="bibr" rid="pcbi.1002441-Smith1">[18]</xref>. This is often done by first defining a set of functional “nodes” based on spatial ROIs and then conducting a connectivity analysis between the nodes based on their FMRI timeseries. Large-scale functional connectivity patterns have been successful in predicting age <xref ref-type="bibr" rid="pcbi.1002441-Dosenbach1">[19]</xref> as well as subject-driven mental states such as memory retrieval, silent-singing vs. mental arithmetic <xref ref-type="bibr" rid="pcbi.1002441-Shirer1">[20]</xref> and watching movies vs. rest <xref ref-type="bibr" rid="pcbi.1002441-Richiardi1">[21]</xref>. It remains to be determined however, whether whole-brain connectivity can be used to decode very similar stimuli that differ by only one or a few subtle characteristics, such as the emotional expression of an unattended face. If so, then functional connections that discriminate between the two conditions can be interpreted as being uniquely related to the parameter of interest that varies across both conditions.</p>
      <p>Although multivariate pattern analyses are more sensitive than group, model-based approaches, one disadvantage is decreased interpretability and quantification of the precise relationship among features related to a certain condition <xref ref-type="bibr" rid="pcbi.1002441-Norman1">[8]</xref>. However, since this approach exploits the information inherent in the joint responses of many functional connections, an advantage is that pattern classification of similar conditions coupled with feature selection and identification can be used as a means to identify condition-dependent, large-scale functional connectivity, without the need to correct for tens of thousands of multiple comparisons. This approach can be used for hypothesis generation to identify groups of functional connections associated with a condition, which can then serve as connections and regions of interest for more rigorous and mechanistically revealing approaches such as effective connectivity <xref ref-type="bibr" rid="pcbi.1002441-Marreiros1">[22]</xref>.</p>
      <p>Here we estimate the large-scale functional networks of implicit fear processing using a blocked design and Blood Oxygen Level Dependent (BOLD) image acquisition, during which subjects were instructed to identify the color of pseudo-colored fearful and neutral faces (<xref ref-type="fig" rid="pcbi-1002441-g001">Figure 1</xref>). We applied atlas-based parcellation to derive several hundred nodes throughout the whole-brain and computed thousands of pair-wise correlations (40 total time points, or 80 s worth of fMRI data) during each of two conditions: implicit processing of fearful and neutral faces. We then employed multivariate pattern analyses in conjunction with linear filter feature selection to identify functional connections whose pattern could distinguish between implicit processing of fearful and neutral faces within individual subjects, using training data from separate subjects. We plotted classification accuracy vs. number of included features to approximate the minimum number of informative features, and then identified these features (functional connections) on a neuroanatomical display. See <xref ref-type="fig" rid="pcbi-1002441-g002">Figure 2</xref> for an outline of the analysis scheme.</p>
      <fig id="pcbi-1002441-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Experimental paradigm for the interaction of attention and affect (adapted from Etkin, et. al. 2004).</title>
          <p>Stimuli were either fearful (F) or neutral (N) expression faces, pseudocolored in red, yellow,or blue. Each event was comprised of a face which was either masked (33 ms for a fearful or neutral face, followed by 167 ms of a neutral face mask of the same gender and color, but different individual; MF or MN, respectively), or unmasked (200 ms for each face; F or N) or masked. Ten events of the same type, spaced 2 seconds apart, were presented within each 20 second block, followed by 15 seconds of crosshair with black background. There were four blocks per condition, giving 40 time points in the correlation estimates per condition per subject. In view of our specific hypotheses, only the unmasked conditions are discussed in the main text, while results for unmasked conditions are presented elsewhere (manuscript in preparation).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.g001" xlink:type="simple"/>
      </fig>
      <fig id="pcbi-1002441-g002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Node definitions and anatomical locations.</title>
          <p>Cortical and subcortical regions (ROIs) were parcellated according to bilateralized versions of the Harvard-Oxford Cortical and subcortical-atlases, and the cerebellum was parcellated according to AAL (left panel). ROIs were trimmed to ensure there was no overlap between them and that they contained voxels present in each subject. The top two eigenvariates from each ROI was extracted, resulting in 270 total nodes throughout the brain (right panel). For display purposes, node locations (black spheres) correspond to the peak loading value from each time-course's associated eigenmap averaged over all subjects.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.g002" xlink:type="simple"/>
      </fig>
      <p>Our primary objective was to test the hypothesis that condition-specific, functional connectivity over the whole-brain (here Pearson correlation using 40 time points of fMRI data per example) contain enough information to discriminate between implicitly presented fearful and neutral faces, and to identify the functional connections that are most informative in this decoding task. A secondary objective was to compare the decoding accuracies achieved when using <italic>interactivity</italic> (pair-wise correlations) vs. <italic>activity</italic> (i.e. beta estimates from SPM maps). We show that a small subset of connections estimated across the whole-brain can predict, or “brain-read”, implicitly presented fearful faces with high peak accuracies using training and testing data from separate subjects. We propose that this is a valuable, exploratory approach to estimate condition-dependent, large-scale functional connectivity and demonstrate that whole-brain patterns of <italic>interactivity</italic> are a sensitive and informative signature of cognitive-emotional perceptual states.</p>
    </sec>
    <sec id="s2" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s2a">
        <title>Ethics statement</title>
        <p>All procedures and tasks were reviewed for ethical concerns and protection of human subjects by appropriate local IRB boards prior to subject recruitment and data collection. The procedures described in this study of healthy adults have been approved by the Columbia University Morningside IRB (#IRB-AAAA3690, PI: Joy Hirsch) and IRB (#IRB5290, PI: Myrna M. Weissman)</p>
      </sec>
      <sec id="s2b">
        <title>Subjects</title>
        <p>A total of 38 (19 female) healthy volunteers (mean age = 29, SD = 6.9) with emmetropic or corrected-to-emmetropic vision participated in the study in accordance with institutional guidelines for research with human subjects. All subjects were screened to be free of severe psychopathology including Bipolar Disorder and Psychotic Disorders.</p>
      </sec>
      <sec id="s2c">
        <title>Stimulus presentation paradigm</title>
        <p>Subjects performed a previously described task (Etkin, Klemenhagen et al. 2004) which consists of color identification of fearful and neutral faces (F and N respectively). Although backwardly masked (subliminal) fearful and neutral faces were also presented, here we discuss results based on the unmasked (supraliminal) conditions. Results based on comparisons of masked conditions are presented elsewhere (manuscript in preparation). <italic>Stimuli:</italic> Black and white pictures of male and female faces showing fearful and neutral facial expressions were chosen from a standardized series developed by Ekman and Friesen <xref ref-type="bibr" rid="pcbi.1002441-Ekman1">[23]</xref>. Faces were cropped into an elliptical shape that eliminated background, hair, and jewelry cues and were oriented to maximize inter-stimulus alignment of eyes and mouths. Faces were then artificially colorized (red, yellow, or blue) and equalized for luminosity. For the training task, only neutral expression faces were used from an unrelated set available in the lab. These faces were also cropped and colorized as above.</p>
      </sec>
      <sec id="s2d">
        <title>Behavioral task</title>
        <p>Each stimulus presentation involves a rapid (200 ms) fixation to cue subjects to fixate at the center of the screen, followed by a 400 ms blank screen and 200 ms of face presentation. Subjects have 1200 ms to respond with a key press indicating the color of the face. Behavioral responses and reaction times were recorded. Unmasked stimuli consist of 200 ms of a fearful or neutral expression face, while backwardly masked stimuli consist of 33 ms of a fearful or neutral face, followed by 167 ms of a neutral face mask belonging to a different individual, but of the same color and gender (see <xref ref-type="fig" rid="pcbi-1002441-g001">Figure 1</xref>). Each epoch consists of ten trials of the same stimulus type, but randomized with respect to gender and color. The functional run has 16 epochs (four for each stimulus type) that are randomized for stimulus type. To avoid stimulus order effects, we used two different counterbalanced run orders. Stimuli were presented using Presentation software (Neurobehavioral Systems, <ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com" xlink:type="simple">http://www.neurobs.com</ext-link>), and were triggered by the first radio frequency pulse for the functional run. The stimuli were displayed on VisuaStim XGA LCD screen goggles (Resonance Technology, Northridge, CA). The screen resolution was 800×600, with a refresh rate of 60 Hz. Prior to the functional run, subjects were trained in the color identification task using unrelated neutral face stimuli that were cropped, colorized, and presented in the same manner as the nonmasked neutral faces described above in order to avoid any learning effects during the functional run. After the functional run, subjects were shown all of the stimuli again, alerted to the presence of fearful faces, and asked to indicate whether they had seen fearful faces on masked epochs.</p>
      </sec>
      <sec id="s2e">
        <title>fMRI acquisition</title>
        <p>Functional data were acquired on a 1.5 Tesla GE Signa MRI scanner, using a gradient-echo, T2<sup>*</sup>-weighted echoplanar imaging (EPI) with blood oxygen level-dependent (BOLD) contrast pulse sequence. Twenty-four contiguous axial slices were acquired along the AC-PC plane, with a 64×64 matrix and 20 cm field of view (voxel size 3.125×3.125×4 mm, TR = 2000, TE = 40, flip angle = 60). Structural data were acquired using a 3D T1-weighted spoiled gradient recalled (SPGR) pulse sequence with isomorphic voxels (1×1×mm) in a 24 cm field of view (256×256 matrix, ∼186 slices, TR 34 ms, TE 3 ms).</p>
      </sec>
      <sec id="s2f">
        <title>GLM analysis</title>
        <p>Functional data were preprocessed and processed in SPM8 (Wellcome Department of Imaging Neuroscience, London, UK). For preprocessing, the realigned T2*-weighted volumes were slice-time corrected, spatially transformed and resampled to a standardized brain (Montreal Neurologic Institute, 2×2×2 mm<sup>3</sup> cube resolution) and smoothed with a 8-mm full-width half-maximum Gaussian kernel. 1st-level regressors were created by convolving the onset of each block (MF, MN, F and N) with the canonical HRF with duration of 20 seconds. Additional nuisance regressors included 6 motion parameters, white matter and csf signal, which were removed prior to time-series extraction. For the current work, the same GLM analysis served three purposes: 1) facilitate removal of nuisance effects from time series prior to FC estimation using structurally (atlas-based) and functionally defined ROIs, 2) produce beta-estimates of each condition for classification analysis of spatial activity patterns and 3) functionally define ROIs (nodes) prior to FC calculation (used for comparing results of structural vs. functional definition of nodes).</p>
      </sec>
      <sec id="s2g">
        <title>Node definitions</title>
        <p>Brain regions were parcellated according to bilateral versions of the Harvard-Oxford Cortical and sub-cortical atlases and the AAL atlas (cerebellum) and were trimmed to ensure no overlap with each other and to ensure inclusion of only voxels shared by all subjects (<xref ref-type="fig" rid="pcbi-1002441-g003">Figure 3</xref>, left panel). For each subject, time-series across the whole run (283 TRs) were extracted using Singular Value Decomposition (SVD) and custom modifications to the Volumes-of-Interest (VOI) code within SPM8 to retain the top 2 eigenvariates from each atlas-based region. Briefly, the data matrix for each atlas-based region is defined as A, an n×p matrix, in which the n rows represent the time points, and each p column represents a voxel within an atlas-based region. The SVD theorem states:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.e001" xlink:type="simple"/></disp-formula>where U<sup>T</sup>U = I<sub>nxn</sub> and V<sup>T</sup>V = I<sub>pxp</sub> (i.e. U and V are orthogonal). The columns of U are the left singular vectors (eigenvariates, or summary time courses of the region), S (the same dimensions as A) has singular values, arranged in descending order, that are proportional to total variance of data matrix explained by its corresponding eigenvariate, and is diagonal, and V<sup>T</sup> has rows that are the right singular vectors (spatial eigenmaps, representing the loading of each voxel onto its corresponding eigenvariate). Here we retain the top two eigenvariates (nodes) from each region.</p>
        <fig id="pcbi-1002441-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Data analysis scheme.</title>
            <p>Time series from each condition (unmasked fearful and unmasked neutral, F and N) and for N regions (R1 though RN) were segmented from each subject's whole run and concatenated (concatenation of two blocks for each condition shown in figure). There were four 20 second (10 TR) blocks of each condition; hence each example was comprised of 40 time points per condition per subject. For each of example, correlation matrices were estimated, in which each off-diagonal element contains Pearson's correlation coefficient between region <italic>i</italic> and region <italic>j</italic>. The lower <bold><italic>triangular</italic></bold> region of each of these matrices were used as input features in subsequent classifiers that learned to predict the example (i.e. F or N) based on their observed patterns of the correlations. Here, we used a filter feature selection based on t-scores in the training sets during each iteration of leave-two-out cross validation. The difference map consists of the set of most informative features (those that are included in the most rounds of cross-validation and have the highest SVM weights.)</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.g003" xlink:type="simple"/>
        </fig>
        <p>For each atlas-based region, we opted to apply SVD over the entire time-series from each subject and <italic>then</italic> segment and concatenate the eigenvariates according to the conditions/comparisons of interest (rather than segment and concatenate all the masks' voxels <italic>first</italic> and then apply SVD) in order to maximize the total number of observations (time points) per region and also to avoid potentially introducing any artifact and unnatural variation caused by the splicing together of signal from disparate time points, which could possibly bias the SVD results. However, a potential disadvantage of this approach is that important sub-regions and associated eigenvariates within a particular atlas-based region could be missed due to variation in other conditions/blocks within the run that are not considered in the current work. This is an additional motivation to retain the top two eigenvariates from each atlas-based region, as opposed to just one.</p>
        <p>The above step resulted in a total of 270 nodes with an associated time course (i.e. eigenvariates) and spatial eigenmaps from the 135 initial atlas-based regions. Thus, each atlas-based region was comprised of two nodes. Interestingly, when extracting only one eigenvariates per region, maximum accuracy did not surpass 46% (data not shown). This is possibly due to the fact that larger, atlas-based regions encompassed other functional sub-regions which were not included in the analysis. Another possible reason is that for many regions, the 1<sup>st</sup> eigenvariate may reflect artifact global or mean grey matter signal (while white matter and csf signal were regressed out from nodes' time-series, global and mean grey matter signals were not), or it may reflect variation caused by other conditions/blocks within the run that were not considered in the current classification analyses (see paradigm task description above), or a combination of all the above. Therefore we extracted two eigenvariates from each region. We note that this means it is likely that node 2 of a particular region shows functional connectivity that differentiates between conditions and node 1 of the same region has no differential connectivity. For clarity we therefore label each node using its Harvard-Oxford atlas label appended by either “_PC1” for the first eigenvariate and “_PC2” for the second. For display purposes, we calculated the MNI coordinates of the peak loading weight (locations averaged across subjects) for each eigenvariate from its associated eigenmap (<xref ref-type="fig" rid="pcbi-1002441-g003">Figure 3</xref>, right panel). <xref ref-type="supplementary-material" rid="pcbi.1002441.s001">Table S1</xref> lists these average MNI coordinates for each node.</p>
      </sec>
      <sec id="s2h">
        <title>Functional connectivity networks for implicit fearful and neutral face processing</title>
        <p>For each subject, functional connectivity matrices (i.e. where cell <italic>i,j</italic> contains the Pearson correlation between region <italic>i</italic> and region <italic>j</italic>) were generated for implicit fearful (F) and neutral (N) conditions. The above time-series were segmented and concatenated according to conditions of interest (40 total time points per condition, incorporating a lag of 2 or 3 s from the start of each block) before generating the correlation matrices. Fisher's R to Z transform was then applied to each correlation matrix. Finally for the binary classification of interest (i.e. F vs. N), correlation matrices were demeaned with respect to the average between the two conditions in order to remove the effects of inter-subject variability. The lower diagonal of the above preprocessed correlation matrices (38 subjects×2 conditions total) were then used as input features to predict viewed stimuli in subsequent pattern recognition experiments.</p>
      </sec>
      <sec id="s2i">
        <title>Differences in functional connectivity between implicit fearful and neutral face processing</title>
        <p>We first tested for significant differences between the primary conditions of interest (i.e. F&gt;N) while correcting for multiple comparisons (False Discovery Rate, FDR). This yielded no significant results when multiple comparison correction was applied (FDR, p&lt;0.05 and 0.1). This was not surprising, as multiple comparison correction was expected to be too conservative given the exceedingly high number of independent comparisons (36,315).</p>
      </sec>
      <sec id="s2j">
        <title>Pattern analysis of large-scale functional connectivity to predict implicit fear perception</title>
        <p>Support vector machines are pattern recognition methods that find functions of the data that facilitate classification <xref ref-type="bibr" rid="pcbi.1002441-Vapnik1">[24]</xref>. During the training phase, an SVM finds the hyperplane that separates the examples in the input space according to a class label. The SVM classifier is trained by providing examples of the form &lt;<italic>x</italic>,<italic>c</italic>&gt;, where x represents a spatial pattern and <italic>c</italic> is the class label. In particular, x represents the fMRI data (pattern of correlation strengths) and <italic>c</italic> is the condition or group label (i.e. <italic>c</italic> = 1 for F and <italic>c</italic> = −1 for N). Once the decision function is determined from the training data, it can be used to predict the class label of new test examples.</p>
        <p>For all binary classification tasks, we applied a linear kernel support vector machine (SVM) with a filtering feature selection based on t-test and leave-two-out cross validation (LTOCV). There were 38 examples for each condition (2 from each subject, 76 total). During each iteration of 38 rounds of LTOCV, both examples (1 from each class) from one subject were withheld from the dataset and 1) a 2-sample t-test was performed over the remaining training data (N = 37 in each group) 2) the features were ranked by absolute t-score and the top N were selected 3) these selected features were then used to predict the class of the withheld test examples during the classification stage. The full feature set for each example consisted of 36,315 correlations.</p>
        <p>If the classifier predicted all trials as positive or negative, the resulting accuracy would be 50% since the number of examples are equal for each class. We therefore report classification accuracy (number of true positives and negatives over all trials) vs. number of included features that have been ranked by their t-score. We assessed the significance of decoding results by computing the frequency in which actual values surpassed those from null distributions derived by randomly permuting class labels based on the method proposed by <xref ref-type="bibr" rid="pcbi.1002441-Golland1">[25]</xref>, with the a slight modification to account for the dependence between pairs of examples from each subject. Briefly, to derive this null distribution, class labels within each pair conditions from each subject were randomly flipped with a probability of 0.5 over 2000 iterations for each number of included features. P-values for the peak decoding accuracies (F vs. N: 100%, top 25 features) were also calculated with respect to classification results when shuffling labels 10,000 times, and then subjected to Bonferroni correction for the number of total Top N comparisons (in this case 20).</p>
        <p>For SVM learning and classification we used the Spider v1.71 Matlab toolbox (<ext-link ext-link-type="uri" xlink:href="http://people.kyb.tuebingen.mpg.de/spider/" xlink:type="simple">http://people.kyb.tuebingen.mpg.de/spider/</ext-link>) using all default parameters (i.e. linear kernel SVM, regularization parameter C = 1. Graphical neuro-anatomical connectivity maps of the top N features were displayed using Caret v5.61 software (<ext-link ext-link-type="uri" xlink:href="http://brainvis.wustl.edu/wiki/index.php/Caret:About" xlink:type="simple">http://brainvis.wustl.edu/wiki/index.php/Caret:About</ext-link>). We note that different features could be selected during the feature selection phase of each round of cross-validation. Therefore in ranking the top 25 features, we first rank by total number of times that feature was included in each round of cross-validation, and then among these features, we sort by absolute value of the average SVM weight.</p>
        <p>Our intent is not to estimate the true accuracy of prediction given a completely new data set, but rather to test whether there exists information in the pattern of functional connections relevant to unattended emotion perception, and to approximate the optimal number of features that containing this information. We note that our approach (plotting accuracy vs. number of top N features) is not biased, since for each number of top N features, and for each round of leave-two-out cross validation, the top N features were selected from a training set that was completely independent from the testing set. If there is a true signal present in the data, we expect, and in the current data in general observe, that there is an initial rise in accuracy as more informative features are added to the feature set, and a dip in accuracy as less informative features (i.e. noise) are added to the feature set. Therefore in reporting classification results, we report the range of features at which accuracies first reach maximum accuracy-10% (positive slope) to which they reach maximum accuracy-10% (negative slope), and also correct for multiple comparisons (i.e. number of top N features tested) using Bonferroni when reporting the p-value for the maximum accuracy achieved.</p>
        <p>For assessing the significance of the differences between decoding results (i.e. FC as features vs. beta estimates) we used the Accurate Confidence Intervals MATLAB toolbox for assessing whether the parameter <italic>p</italic> (probability of correct prediction) from two independent binomial distributions was significantly different (<ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/3031-accurate-confidence-intervals" xlink:type="simple">http://www.mathworks.com/matlabcentral/fileexchange/3031-accurate-confidence-intervals</ext-link>). Briefly, these methods search for confidence intervals using an integration of the Bayesian posterior with diffuse priors to measure the confidence level of the difference between two proportions <xref ref-type="bibr" rid="pcbi.1002441-Ross1">[26]</xref>. We used the code prop–diff(<italic>x</italic><sub>1</sub>,<italic>n</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,<italic>n</italic><sub>2</sub>,delta), (available from the above website) returning Pr(<italic>p</italic><sub>1</sub>−<italic>p</italic><sub>2</sub>&gt;<italic>δ</italic>), where <italic>x</italic><sub>1</sub>, <italic>n</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>n</italic><sub>2</sub>, are number of correct responses and total predictions in two distributions being compared, and delta (zero in our case) is the null hypothesis difference between the probabilities.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Behavioral results</title>
        <p>The average response rate in the color discrimination task was 98% (σ = 4.6%), mean accuracy was 97% (σ = 3.5%), and mean reaction time was 0.65 s (σ = 0.12), indicating that subjects performed the color discrimination task as instructed.</p>
      </sec>
      <sec id="s3b">
        <title>Discriminating between implicit processing of fearful and neutral faces with patterns of functional connectivity</title>
        <p>We applied atlas-based parcellation (see <xref ref-type="fig" rid="pcbi-1002441-g002">Figure 2</xref>) and computed pair-wise correlations between 270 nodes (derived from 135 atlas-based brain regions) using 40 total time points of fMRI data that were segmented and concatenated from two conditions; unattended and nonmasked (i.e. implicit) fearful (F) and neutral (N) faces (<xref ref-type="fig" rid="pcbi-1002441-g001">Figure 1</xref>). This resulted in 36,315 total functional connections (z-transformed Pearson correlations) for each condition of interest (F and N).</p>
        <p>We quantified the extent to which a subset of these functional connections could decode, or predict, the conditions from which they were derived by submitting them as features into a pattern classifier. We used a linear kernel Support Vector Machine (SVM) with a filter feature selection based on the t-score of each feature (functional connectivity) in each training set. Decoding accuracies for implicit fearful vs. neutral classifications (F vs. N) were plotted against the number of included features (ranked in descending order by t-score) in order to approximate the number of informative features relevant to the emotional expression of the facial stimulus.</p>
        <p>For implicit fearful vs. neutral (F vs. N) classification, accuracy reached 90% when learning was based on the top 15 features in each training set, a maximum of 100% (p&lt;0.002, corrected) at 25 features, and dipped back down to 90% at about 35 features (<xref ref-type="fig" rid="pcbi-1002441-g004">Figure 4A</xref>). Anatomical display of the top 25 overall features that differed between F and N conditions revealed functional connections among occipital regions, middle and superior temporal gyrus, lateral and medial prefrontal regions, thalamus, cerebellum and insula (<xref ref-type="fig" rid="pcbi-1002441-g004">Figure 4B–D</xref>, <xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref>). The connection that carried the most weight in the linear SVM classifier was between right angular gyrus and left hippocampus, which exhibited a greater correlation in the F vs. N condition (<xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref>, F# 1). To identify regions whose overall functional connectivity was greater during fear, the size of each node was made proportional to the sum of SVM weights of each of its connections. The node with the most positive functional connectivity during fear was the thalamus (<xref ref-type="fig" rid="pcbi-1002441-g004">Figure 4B–D</xref>, large red sphere in center), which exhibited positively modulated functional connections with bilateral middle temporal gyrus and right insula.</p>
        <fig id="pcbi-1002441-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Large-scale functional connectivity discriminates between unattended, conscious processing of fearful and neutral faces.</title>
            <p>(<bold>A</bold>) Decoding accuracy when classifying F vs. N as a function of the number of features (1 to 40) included ranked in descending order by their absolute t-score. Maximum accuracy for F vs. N classification (100%, p&lt;0.002, corrected) was achieved when learning was based on the top 25 features in each training set. Mean accuracy scores for shuffled data are plotted along the bottom, with error bars representing standard deviation about the mean. Posterior (<bold>B</bold>), ventral (<bold>C</bold>) and right lateralized (<bold>D</bold>) anatomical representation of the top 25 features when classifying supraliminal fearful vs. supraliminal neutral face conditions (F vs. N). The thalamus (large red sphere in the center of each view) is the largest contributor of connections the differentiate the F from N. Red indicates correlations that are greater in F, and blue represents correlations that are greater in N. For display purposes, the size of each sphere is scaled according to the sum of the SVM weights of each node's connections, while the color of each sphere is set according to the sign of this value; positive sign, red, F&gt;N and negative sign, blue, N&gt;F. In addition, the thickness of each connection was made proportional to its SVM weight.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.g004" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002441-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.t001</object-id><label>Table 1</label><caption>
            <title>F vs. N, Top 25 features (consensus features are in bold).</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002441-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">F#</td>
                <td align="left" colspan="1" rowspan="1">Edge label</td>
                <td align="left" colspan="1" rowspan="1">Mean R (F)</td>
                <td align="left" colspan="1" rowspan="1">Mean R (N)</td>
                <td align="left" colspan="1" rowspan="1">T-value</td>
                <td align="left" colspan="1" rowspan="1">SVM weight</td>
                <td align="left" colspan="1" rowspan="1">FSets</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Angular_Gyrus_PC1 - Left_Hippocampus_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.101</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.027</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.3419</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>1.1347</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Superior_Temporal_Gyrus_anterior_division_PC2 - Left_Ventral_Frontal_Pole_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.08</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.066</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−4.301</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.9976</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>3</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Dorsal_Frontal_Pole_PC2 - Cerebelum_6_L_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.07</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.092</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.3555</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.97075</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Vermis_7_PC2 - Midbrain_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.127</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>7E-04</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.2176</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.88976</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>5</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Temporal_Occipital_Fusiform_Cortex_PC2 - Pons_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.07</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.082</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−4.4395</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.8891</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>6</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Putamen_PC2 - Cerebelum_Crus1_R_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.07</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.094</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−5.5049</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.8803</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>7</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Left_Frontal_Orbital_Cortex_PC2 - Left_Cuneal_Cortex_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.052</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.082</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.4034</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.84121</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>8</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Frontal_Operculum_Cortex_PC2 - Right_Dorsal_Lateral_Occipital_Cortex_superior_division_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.118</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.027</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>5.5009</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.81892</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">9</td>
                <td align="left" colspan="1" rowspan="1">Right_Frontal_Medial_Cortex_PC1 - Right_Cingulate_Gyrus_posterior_division_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.003</td>
                <td align="left" colspan="1" rowspan="1">0.133</td>
                <td align="left" colspan="1" rowspan="1">−3.943</td>
                <td align="left" colspan="1" rowspan="1">−0.8083</td>
                <td align="left" colspan="1" rowspan="1">19</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">10</td>
                <td align="left" colspan="1" rowspan="1">Right_Amygdala_PC2 - Left_Putamen_PC1</td>
                <td align="left" colspan="1" rowspan="1">0.009</td>
                <td align="left" colspan="1" rowspan="1">0.131</td>
                <td align="left" colspan="1" rowspan="1">−4.1008</td>
                <td align="left" colspan="1" rowspan="1">−0.7664</td>
                <td align="left" colspan="1" rowspan="1">34</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>11</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Lingual_Gyrus_PC1 - Left_Dorsal_Lateral_Occipital_Cortex_superior_division_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.088</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.068</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.1602</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.7472</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>12</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Left_Thalamus_PC2 - Left_Planum_Polare_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.091</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.076</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.7585</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.65859</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>13</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Left_Temporal_Occipital_Fusiform_Cortex_PC2 - Cerebelum_8_L_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.043</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.102</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.3388</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.62211</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>14</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Central_Opercular_Cortex_PC2 - Left_Lingual_Gyrus_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.061</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.077</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.3741</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.61316</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">15</td>
                <td align="left" colspan="1" rowspan="1">Vermis_8_PC1 - Left_Planum_Polare_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.085</td>
                <td align="left" colspan="1" rowspan="1">−0.042</td>
                <td align="left" colspan="1" rowspan="1">3.9352</td>
                <td align="left" colspan="1" rowspan="1">0.59068</td>
                <td align="left" colspan="1" rowspan="1">19</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">16</td>
                <td align="left" colspan="1" rowspan="1">Right_Insular_Cortex_PC2 - Left_Caudate_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.028</td>
                <td align="left" colspan="1" rowspan="1">−0.089</td>
                <td align="left" colspan="1" rowspan="1">3.873</td>
                <td align="left" colspan="1" rowspan="1">0.57516</td>
                <td align="left" colspan="1" rowspan="1">11</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>17</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Parahippocampal_Gyrus_anterior_division_PC1 - Left_Middle_Temporal_Gyrus_anterior_division_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.02</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.151</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.1911</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.55492</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">18</td>
                <td align="left" colspan="1" rowspan="1">Right_Ventral_Lateral_Occipital_Cortex_superior_division_PC2 - Right_Middle_Temporal_Gyrus_posterior_division_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.011</td>
                <td align="left" colspan="1" rowspan="1">−0.074</td>
                <td align="left" colspan="1" rowspan="1">3.8763</td>
                <td align="left" colspan="1" rowspan="1">0.55272</td>
                <td align="left" colspan="1" rowspan="1">15</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>19</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Central_Opercular_Cortex_PC1 - Left_Planum_Polare_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.077</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.219</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−4.2479</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.5409</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">20</td>
                <td align="left" colspan="1" rowspan="1">Left_Juxtapositional_Lobule_Cortex_Supp_Motor_cortex_PC2 - Left_Inferior_Frontal_Gyrus_pars_triangularis_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.041</td>
                <td align="left" colspan="1" rowspan="1">−0.073</td>
                <td align="left" colspan="1" rowspan="1">3.9504</td>
                <td align="left" colspan="1" rowspan="1">0.48896</td>
                <td align="left" colspan="1" rowspan="1">20</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">21</td>
                <td align="left" colspan="1" rowspan="1">Right_Precuneous_Cortex_PC1 - Left_Middle_Temporal_Gyrus_anterior_division_PC1</td>
                <td align="left" colspan="1" rowspan="1">−0.01</td>
                <td align="left" colspan="1" rowspan="1">−0.12</td>
                <td align="left" colspan="1" rowspan="1">3.8799</td>
                <td align="left" colspan="1" rowspan="1">0.43938</td>
                <td align="left" colspan="1" rowspan="1">15</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>22</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Left_Thalamus_PC2 - Left_Insular_Cortex_PC1</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.085</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.057</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.2959</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.42672</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">23</td>
                <td align="left" colspan="1" rowspan="1">Right_Planum_Polare_PC2 - Cerebelum_Crus2_L_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.043</td>
                <td align="left" colspan="1" rowspan="1">−0.083</td>
                <td align="left" colspan="1" rowspan="1">3.8435</td>
                <td align="left" colspan="1" rowspan="1">0.41322</td>
                <td align="left" colspan="1" rowspan="1">12</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>24</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Right_Planum_Polare_PC1 - Left_Thalamus_PC2</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.068</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>−0.093</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>4.1779</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>0.39581</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <bold>38</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">25</td>
                <td align="left" colspan="1" rowspan="1">Left_Cingulate_Gyrus_anterior_division_PC1 - Hypothalamus_PC2</td>
                <td align="left" colspan="1" rowspan="1">0.049</td>
                <td align="left" colspan="1" rowspan="1">−0.059</td>
                <td align="left" colspan="1" rowspan="1">3.8567</td>
                <td align="left" colspan="1" rowspan="1">0.38869</td>
                <td align="left" colspan="1" rowspan="1">13</td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>In addition to parcelating the brain and defined nodes based on an atlas, we also functionally defined nodes using two approaches 1) using the same 160 MNI coordinates as used in Dosenbach et. al., 2010 <xref ref-type="bibr" rid="pcbi.1002441-Dosenbach1">[19]</xref> which were selected and defined based on separate meta-analyses of the fMRI literature, and 2) a biased approach based on 92 nodes (2 eigenvariates from each of 49 ROIs defined as 6 mm radius spheres centered at peak coordinates) that were based on the GLM results from the same, whole dataset (for F contrast F&gt;N thresholded at p = 0.05, k = 30). For 1) achieved accuracies were 63–73% when using 75 to 130 features, and for 2) accuracies between 76–86% were obtained when using 80 to 140 features (data not shown). Approach 2) is biased in that we defined our nodes based on the GLM results of the whole data set, and as such provides an upper bound on the expected accuracies when functionally defining nodes based on the GLM results in separate training sets during each iteration of LTOCV. Therefore we conclude that the above whole-brain, atlas-based approach, which achieved 90–100% accuracy with 15–35 features when using unbiased feature selection, is optimal to using functionally defined nodes.</p>
      </sec>
      <sec id="s3c">
        <title>Discriminating between F and N faces using spatial patterns of activation</title>
        <p>To compare the information content of patterns of <italic>interactivity</italic> (i.e. functional connections used above) vs. patterns of <italic>activity</italic> we also attempted F vs. N classifications using beta estimates, which are considered summary measures of activation in response to each condition. In order to make feature-selection/LTOCV and SVM learning more computationally tractable, preprocessed functional data were resized from 2×2×2 mm voxel resolution to 4×4×4 mm resolution, and subject-specific GLM models were re-estimated, resulting in a reduction of total feature space per example from ∼189,500 betas to ∼23,500. Feature selection, LTOCV and SVM learning proceeded exactly as above for FC data. We observed accuracies of 66%–76% with ∼500 to 2600 features, with peak accuracy at 76% (p = 0.0044, uncorrected) at ∼1900 features (<xref ref-type="fig" rid="pcbi-1002441-g005">Figure 5A</xref>). The most informative voxels encompassed many distributed regions that included dorsolateral prefrontal/opercular cortex, fusiform gyrus, lateral occipital cortex, superior temporal gyrus, anterior cingulate, amygdala, parahippocampal gyrus, ventrolateral prefrontal cortex, pulvinar, precuneus, cerebellum, inferior parietal lobe and insula (<xref ref-type="fig" rid="pcbi-1002441-g005">Figure 5B</xref>). Although significantly above chance, and despite the involvement of many more regions, maximum accuracy using betas was significantly less than the maximum accuracy achieved with FC (76%&lt;100%, p = 5.37×10−7).</p>
        <fig id="pcbi-1002441-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002441.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Classification results using beta estimates as features.</title>
            <p>(<bold>A</bold>) Feature selection, cross-validation and SVM learning were performed exactly the same as for FC, but over the range of 1 to 4000 ranked features (voxels). Accuracies for F vs. N classification reached 66–76% with ∼500–2500 features, with maximum accuracy (76%, p = 0.0044, uncorrected) at ∼1,900 features. (<bold>B</bold>) The most informative voxels with positive SVM weights (F&gt;N, yellow) included fusiform gyrus (−28,−20,−12), cerebellum (−28, −20), amygdala (−20), insula (−12), orbital and ventrolateral prefrontal cortex (−20, −12, −4), midbrain (−12), parahippocampal gyrus (−12), middle temporal gyrus and superior temporal sulcus (−12,−4,4), thalamus/pulvinar (4), dorsolateral prefrontal/opercular cortex (12,20,28), dorsomedial prefrontal cortex (20,28), and superior occipital cortex (20,28) and inferior parietal lobe (36). Informative voxels with negative SVM weights (N&gt;F, blue) included temporal-occipital cortex (−20), subgenual anterior cingulate (−12,−4), striatum (−4,4), lingual gyrus (4,12), precuneus (20) and dorsolateral prefrontal cortex (28,36). (B). Brain images are displayed using Neurological convention (i.e. L = R), and top left number in each panel represents the MNI coordinate (z) of depicted axial slice.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.g005" xlink:type="simple"/>
        </fig>
        <p>We performed additional classifications using betas derived from the original, smaller voxel-sizes and with the addition of an initial (positively biased) feature selection step over the whole-dataset for the same issues of technicality stated above. This also served to estimate an upper bound on the expected accuracy when using beta-values: if maximum accuracy achieved was still less than when using functional connectivity with unbiased feature selection, then we can more readily conclude that functional connectivity features are more “informative” than beta estimates (when using the Canonical Hemodynamic Response Function (HRF) to model activation). For this analysis, the initial (biased) feature selection employed an F-test of the contrast F&gt;N thresholded at p&lt;0.01, cluster threshold = 20, resulting in 4,226 total initial features. Feature selection/LTOCV and classification again proceeded as above across the range of 1 to 4000 features. In spite of initially biased feature selection, F vs. N classification reached 92% maximum accuracy (data not shown).</p>
        <p>In addition to using beta maps throughout the whole-brain, we derived beta weights using the same summary time courses (eigenvariates) that were extracted and used to compute pair-wise FC (270 total betas per condition per subject). For this, the GLM analysis was kept the same as above except that previously included nuisance regressors (6 motion, mean white and mean csf) and a low-pass filter were not included, since they were already removed from the time courses during extraction. Resulting estimated beta weights were then used as features to predict fearful vs. neutral faces using the exact same procedure when using whole-brain FC. Accuracies of between 69–79% were achieved with between 40 to 150 features (data not shown).</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>Here we demonstrate that pattern analysis of large-scale functional connectivity can reliably decode the emotional expression of implicitly perceived faces, and that pair-wise functional connections are modulated by implicit fear perception. This work also demonstrates a whole-brain, large-scale and exploratory approach for the identification of condition-specific, functional connectivity that avoids correcting for multiple comparisons among thousands of connections (discussed more below).</p>
      <p>The most significantly modulated functional connection during implicit presentation of fearful faces was between left hippocampus and right angular gyrus. The left hippocampus is a key region for memory (i.e. autobiographical memory retrieval) and the right angular gyrus has been implicated in mentalizing, or inferring the thoughts and feelings of others <xref ref-type="bibr" rid="pcbi.1002441-Spreng1">[27]</xref>. Interestingly, during resting states, these two regions were found not to correlate with each other, but instead correlated with other regions that substantially overlapped, such as superior temporal sulcus (STS), anterior temporal lobe, posterior cingulate cortex, dorsomedial and ventral prefrontal cortex, inferior frontal gyrus, and the amygdala. It has been proposed that this functional overlap facilitates the integration of personal and interpersonal information and provides a means for personal experiences to become social conceptual knowledge <xref ref-type="bibr" rid="pcbi.1002441-Spreng1">[27]</xref>. Here, we observed the left hippocampus and right angular gyrus were correlated during implicit emotion (fear) perception, suggesting the integration of autobiographical memory with mentalizing during implicit perception of emotional faces.</p>
      <p>Other connections that discriminated between implicitly presented fearful and neutral faces included thalamus, superior occipital, frontal operculum, dorsal-lateral prefrontal cortex, cerebellum, parietal and posterior and anterior temporal regions (in the vicinity of the superior temporal sulcus, STS). This latter observation is consistent with previous models and group studies that identify the STS and middle temporal gyrus as a primary neural substrate for processing the emotional expression of faces <xref ref-type="bibr" rid="pcbi.1002441-Haxby2">[28]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Engell1">[30]</xref>, and recent work demonstrating that multivariate pattern analyses applied to these regions could decode explicit emotional face recognition <xref ref-type="bibr" rid="pcbi.1002441-Said1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Tsuchiya1">[33]</xref>. Importantly, the current findings suggest that interactions of temporal regions and STS with areas such ventral frontal pole, thamalus, parahippocampal gyrus and central opercular cortex (<xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref> F# 2, 12, 24 and 17) are also critically involved in implicit emotion perception.</p>
      <p>Contrary to our expectations, other than a connection between amygdala and putamen (<xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref>, F# 10), the top 25 features that discriminated between the implicit fear and neutral conditions did not include any connections with the amygdala. This is not inconsistent with the observation in a recent meta-analysis that amygdala activity was significantly greater for explicit (attended) fear perception vs. implicit fear perception <xref ref-type="bibr" rid="pcbi.1002441-FusarPoli1">[34]</xref>. In addition, the finding that amygdala demonstrates a distinct temporal profile from other structures during emotional face processing could also explain why more functional connections with amygdala were not observed in the current analysis <xref ref-type="bibr" rid="pcbi.1002441-Haas1">[10]</xref>. Instead, the structure which contributed the most in discriminating between the fear and neutral conditions was thalamus (<xref ref-type="fig" rid="pcbi-1002441-g004">Fig. 4C and D</xref>, largest red sphere in center), which exhibited greater correlations with bilateral middle temporal gyrus (STS) and left insula during the fear condition (<xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref> rows 12, 22 and 24). This observation is consistent with its purported role as a hub integrating cortical networks during the evaluation of the biological significance of affective visual stimuli <xref ref-type="bibr" rid="pcbi.1002441-Pessoa2">[35]</xref>, and with the observation of direct structural connectivity between several sub-regions of the thalamus with the STS <xref ref-type="bibr" rid="pcbi.1002441-Yeterian1">[36]</xref>. The current results suggest that functional connectivity between thalamus and STS and insula play a prominent role during implicit fear perception.</p>
      <p>Interestingly, functional connections of the cerebellum were also significantly modulated during the fear condition. In particular, functional connections of the cerebellum with dorsal frontal pole (<xref ref-type="table" rid="pcbi-1002441-t001">Table 1</xref> F# 3) and fusiform gyrus (F# 13) were increased during fear, while connections with putamen (F# 6) were decreased. Although cerebellum has been frequently reported to be activated or involved during emotion processing <xref ref-type="bibr" rid="pcbi.1002441-FusarPoli1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-FusarPoli2">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Karama1">[38]</xref>, the specific roles the various subregions play during affective processing remain to be elucidated <xref ref-type="bibr" rid="pcbi.1002441-Stoodley1">[39]</xref>.</p>
      <p>Previous studies have shown that emotional faces modulate amygdala-fusiform (FG) interactions <xref ref-type="bibr" rid="pcbi.1002441-Fairhall1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Sabatinelli2">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Vuilleumier3">[41]</xref>. Although amygdala-FG interactions did not appear among the top features for discriminating between implicit fearful and neutral faces, we did observe increased amygdala-FG connectivity during implicit fear relative to implicit neutral when we isolated that connection (Right_Temporal_Occipital_Fusiform_Cortex_PC1, MNI = [26,−48,−18] and Right_Amygdala_PC1, MNI = [18,0,−20], t = 2.6, p&lt;0.01), which is consistent with the above works.</p>
      <sec id="s4a">
        <title>Large-scale functional network of fear processing</title>
        <p>It is clear that fearful emotion processing and its behavioral consequences involve the complex interactions among many distributed regions <xref ref-type="bibr" rid="pcbi.1002441-Gorman1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-Coplan1">[44]</xref>. Among these, the amygdala and its interactions with the frontal and visual cortex are critically involved in attended and pre-attentive threat and emotion processing <xref ref-type="bibr" rid="pcbi.1002441-Etkin1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Etkin2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Bishop2">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Banks1">[46]</xref>. Numerous previous studies have examined functional interactions between amygdala and several other regions in the fear and facial emotion processing pathway. Usually these have used Psycho-Physiological Interaction (PPI) analysis to study the functional connectivity of a seed region, often the amygdala, with the rest of the brain during a fearful relative to non-fear perceptual or cognitive state <xref ref-type="bibr" rid="pcbi.1002441-Pezawas1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Banks1">[46]</xref>. Other studies employed effective connectivity measures such as structural equation modeling (SEM) and dynamic casual modeling (DCM) to examine multiple interactions among a more limited set of a priori defined regions <xref ref-type="bibr" rid="pcbi.1002441-Fairhall1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Stein1">[16]</xref>.</p>
        <p>In contrast to the above-mentioned studies, the current approach is relatively model-free in that we estimate functional connectivity throughout the whole-brain without a priori restrictions based on anatomically defined areas or seed regions. We estimate network connections using simple correlation measures, similar to a previous study that demonstrated condition dependent modulations in large-scale (41 nodes) functional connectivity across various syntactical language production tasks <xref ref-type="bibr" rid="pcbi.1002441-Dodel1">[47]</xref>, but on a much larger scale (270 nodes in the current analysis). We then identified a subset of functional connections whose pattern could discriminate between implicit fearful and neutral face processing.</p>
      </sec>
      <sec id="s4b">
        <title>An approach to estimate condition specific large-scale functional connectivity</title>
        <p>There is considerable interest in examining the large-scale functional network architecture of the brain as a function of various cognitive processes or individual variation <xref ref-type="bibr" rid="pcbi.1002441-Smith1">[18]</xref>. This is often done by first defining a set of functional “nodes” based on spatial ROIs and then conducting a connectivity analysis between the nodes based on their FMRI timeseries. Group-based statistical parametric mapping can then be applied to resulting connections <xref ref-type="bibr" rid="pcbi.1002441-Ginestet1">[48]</xref>. However, as the number of nodes (N) increases, the number of connections increases exponentially (# connections = (N*(N−1))/2) resulting in a multiple comparisons problem, and hindering the exploration-based query of condition-specific whole-brain functional connectivity on a large-scale. The equivalent of cluster-extent thresholding for graphs has been proposed, such as the Network Based Statistic <xref ref-type="bibr" rid="pcbi.1002441-Zalesky1">[49]</xref>, which estimates the probability of observing groups of linked, suprathreshold edges based on chance. However, inferences can only be made on groups of interconnected edges, not individual ones. In addition, there is a substantial loss of information in model-based approaches when conducting statistical inference on signals (functional connections) averaged over a group of subjects, and discounting the joint responses among many functional connections.</p>
        <p>Here, we present a novel alternative to identify functional connections of interest based on their information content in machine-learning based multivariate pattern analyses that attempt to discriminate between two conditions that differ based on a parameter of interest (in this case the emotion expression of a presented face). For this we used linear filter feature selection and plotted classification accuracy vs. number of included features in order to determine the number of features required to distinguish between conditions, and then identified the top N features on neuroanatomical display.</p>
      </sec>
      <sec id="s4c">
        <title>“Information content” of neural activity vs. neural interactivity</title>
        <p>Large-scale functional connectivity and network analysis has been increasingly used as the tool of choice for extracting meaningful and understanding complex brain organization <xref ref-type="bibr" rid="pcbi.1002441-Li1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Smith1">[18]</xref>. In the current work we applied simple Pearson correlation to estimate the large-scale functional connectivity of implicit threat-related emotion and ambiguous facial processing using a block-design. Previous work based on simulations has indicated that correlation-based methods, including Pearson correlation, are in general quite successful in capturing true network connections <xref ref-type="bibr" rid="pcbi.1002441-Smith1">[18]</xref>. Here we “validated” the estimated connections by testing whether a subset of features could be used to decode (“brain-read”) the emotional expression of the facial stimulus that was presented during each block. For this we applied Multivariate Pattern Analyses (MVPA) techniques similar to those used previously to decode categories of viewed stimuli <xref ref-type="bibr" rid="pcbi.1002441-Haxby3">[50]</xref>–<xref ref-type="bibr" rid="pcbi.1002441-MouraoMiranda1">[54]</xref>, orientation <xref ref-type="bibr" rid="pcbi.1002441-Kamitani1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1002441-Haynes1">[56]</xref>, and the decisions made during a near-threshold fearful face discrimination task <xref ref-type="bibr" rid="pcbi.1002441-Pessoa3">[57]</xref>.</p>
        <p>In contrast to the above-mentioned studies, which applied MVPA to the activity of spatially distributed regions and/or voxels, in the current work we applied pattern analysis to the correlations, or <italic>interactivity</italic>, between regions distributed throughout the whole-brain. We compared the decoding accuracy when using correlations as features versus beta estimates, (i.e. summary measures of activation amplitudes for each condition for each voxel). We observed that the peak classification rate when using betas (76%, ∼1900 features) was significantly lower than that achieved using FC (100%, ∼25 features). Even with an additional, initial feature-selection based on the entire data set which positively biased results, peak decoding accuracies when using ∼4,000 beta values (92%) were lower than those reached when using only ∼25 correlations as features and unbiased feature selection (100%). This suggests that there is substantially more information, relevant to cognitive-emotional neural processing, that is contained in the interactions between regions than is typically realized through standard univariate approaches. However, it should be noted that this requires enough TRs (time-points) to compute meaningful correlations between brain regions for a particular condition, and would thus in general be impractical for decoding single-trial or event-related data.</p>
        <p>We observed that using whole-brain, anatomically defined ROIs to define nodes for whole-brain FC estimation yielded much higher classification rates than using nodes that were functionally defined (either from other meta-analyses or coordinates defined from GLM analysis of these same data). This was not too surprising, as these functionally defined ROIs were smaller (6 mm radius spheres centered around peak F-value coordinates from the contrast of F&gt;N obtained from the GLM vs. atlas-based masks), and hence provided considerably less coverage of the brain. In addition, the GLM framework relies on multiple assumptions (i.e. model/shape of hemodynamic response function, effects add linearly, etc.) <xref ref-type="bibr" rid="pcbi.1002441-Monti1">[58]</xref> and regions that show activation to a stimulus (i.e. sustained increase in signal amplitude during the duration of a block) may not necessarily exhibit differential functional connectivity and vice versa. These observations further the notion that there exists substantial information in whole-brain large-scale functional connectivity patterns, the nodes of which may not be captured or revealed adequately through standard GLM approaches.</p>
      </sec>
      <sec id="s4d">
        <title>Limitations</title>
        <p>Previous simulations have raised concerns regarding the use of atlas-based approaches for parcellating the brain <xref ref-type="bibr" rid="pcbi.1002441-Smith1">[18]</xref>. Because the spatial ROIs used to extract average time-series for a brain region do not likely match well the actual functional boundaries, BOLD time-series from neighboring nodes are likely mixed with each other. While this hampers the ability to detect true functional connections between neighboring regions, it has minimal effect on estimating functional connectivity between distant regions. This perhaps explains why in this study most of the functional connections that discriminated between fearful and neutral faces are long-distance. Future experiments using non-atlas based approaches would likely lead to better estimates of shorter-range functional connections. We also note that the current atlas-based approach may have under-sampled the prefrontal cortex, and that possible future improvements could break up the prefrontal regions into smaller pieces in order to sample more nodes from this area.</p>
        <p>Using Pearson correlation, it is possible that any association between two brain regions is the result of a spurious association with a third brain region. Another limitation of the current study is the required amount of data used to extract quality features of brain activity. Our use of correlations as features required a substantial number of time points (i.e. 40 scans per condition per subject) relative to previous studies of decoding emotion perception. Given this, it was not feasible to sample enough examples within a single or few subjects as is typical in multivariate pattern analysis studies, and we instead pooled examples across multiple subjects. On the other hand, the fact that reliable classifiers could be learned using examples from separate subjects speaks to the generalizability of our obtained results.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002441.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002441.s001" xlink:type="simple">
        <label>Table S1</label>
        <caption>
          <p>Node labels and MNI coordinates (spatial eigenmap peaks averaged over all subjects) used for whole-brain results presented in <xref ref-type="fig" rid="pcbi-1002441-g004">Figure 4</xref> of main text.</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We wish to thank Stephen Dashnaw and Andrew Kogan for technical assistance with image acquisition, Matthew Malter Cohen, Lindsey Kupferman and Aviva Olsavsky for assistance with subject recruitment and project management and Xian Zhang and Tor Wager for helpful discussion and guidance. Nico Dosenbach provided scripts which aided in the 3D network visualizations using Caret software.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002441-Ewbank1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ewbank</surname><given-names>MP</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>AD</given-names></name><name name-style="western"><surname>Passamonti</surname><given-names>L</given-names></name><name name-style="western"><surname>Keane</surname><given-names>J</given-names></name><name name-style="western"><surname>Peers</surname><given-names>PV</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Anxiety predicts a differential neural response to attended and unattended facial signals of anger and fear.</article-title>             <source>Neuroimage</source>             <volume>44</volume>             <fpage>1144</fpage>             <lpage>1151</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Vuilleumier1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name><name name-style="western"><surname>Pourtois</surname><given-names>G</given-names></name></person-group>             <year>2007</year>             <article-title>Distributed and interactive brain mechanisms during emotion face perception: Evidence from functional neuroimaging.</article-title>             <source>Neuropsychologia</source>             <volume>45</volume>             <fpage>174</fpage>             <lpage>194</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Vuilleumier2">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name><name name-style="western"><surname>Armony</surname><given-names>JL</given-names></name><name name-style="western"><surname>Clarke</surname><given-names>K</given-names></name><name name-style="western"><surname>Husain</surname><given-names>M</given-names></name><name name-style="western"><surname>Driver</surname><given-names>J</given-names></name><etal/></person-group>             <year>2002</year>             <article-title>Neural response to emotional faces with and without awareness: Event-related fMRI in a parietal patient with visual extinction and spatial neglect.</article-title>             <source>Neuropsychologia</source>             <volume>40</volume>             <fpage>2156</fpage>             <lpage>2166</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Haxby1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name><name name-style="western"><surname>Hoffman</surname><given-names>EA</given-names></name><name name-style="western"><surname>Gobbini</surname><given-names>MI</given-names></name></person-group>             <year>2000</year>             <article-title>The distributed human neural system for face perception.</article-title>             <source>Trends Cogn Sci</source>             <volume>4</volume>             <fpage>223</fpage>             <lpage>233</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Adolphs1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adolphs</surname><given-names>R</given-names></name><name name-style="western"><surname>Tranel</surname><given-names>D</given-names></name><name name-style="western"><surname>Damasio</surname><given-names>AR</given-names></name></person-group>             <year>2003</year>             <article-title>Dissociable neural systems for recognizing emotions.</article-title>             <source>Brain Cogn</source>             <volume>52</volume>             <fpage>61</fpage>             <lpage>69</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Pessoa1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pessoa</surname><given-names>L</given-names></name><name name-style="western"><surname>Kastner</surname><given-names>S</given-names></name><name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group>             <year>2002</year>             <article-title>Attentional control of the processing of neural and emotional stimuli.</article-title>             <source>Brain Res Cogn Brain Res</source>             <volume>15</volume>             <fpage>31</fpage>             <lpage>45</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Ishai1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ishai</surname><given-names>A</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>CF</given-names></name><name name-style="western"><surname>Boesiger</surname><given-names>P</given-names></name></person-group>             <year>2005</year>             <article-title>Face perception is mediated by a distributed cortical network.</article-title>             <source>Brain Res Bull</source>             <volume>67</volume>             <fpage>87</fpage>             <lpage>93</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Norman1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Norman</surname><given-names>KA</given-names></name><name name-style="western"><surname>Polyn</surname><given-names>SM</given-names></name><name name-style="western"><surname>Detre</surname><given-names>GJ</given-names></name><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name></person-group>             <year>2006</year>             <article-title>Beyond mind-reading: Multi-voxel pattern analysis of fMRI data.</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>424</fpage>             <lpage>430</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Etkin1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Etkin</surname><given-names>A</given-names></name><name name-style="western"><surname>Klemenhagen</surname><given-names>KC</given-names></name><name name-style="western"><surname>Dudman</surname><given-names>JT</given-names></name><name name-style="western"><surname>Rogan</surname><given-names>MT</given-names></name><name name-style="western"><surname>Hen</surname><given-names>R</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Individual differences in trait anxiety predict the response of the basolateral amygdala to unconsciously processed fearful faces.</article-title>             <source>Neuron</source>             <volume>44</volume>             <fpage>1043</fpage>             <lpage>1055</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Haas1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haas</surname><given-names>BW</given-names></name><name name-style="western"><surname>Constable</surname><given-names>RT</given-names></name><name name-style="western"><surname>Canli</surname><given-names>T</given-names></name></person-group>             <year>2009</year>             <article-title>Functional magnetic resonance imaging of temporally distinct responses to emotional facial expressions.</article-title>             <source>Soc Neurosci</source>             <volume>4</volume>             <fpage>121</fpage>             <lpage>134</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Bishop1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Jenkins</surname><given-names>R</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>AD</given-names></name></person-group>             <year>2007</year>             <article-title>Neural processing of fearful faces: Effects of anxiety are gated by perceptual capacity limitations.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>1595</fpage>             <lpage>1603</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Pezawas1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pezawas</surname><given-names>L</given-names></name><name name-style="western"><surname>Meyer-Lindenberg</surname><given-names>A</given-names></name><name name-style="western"><surname>Drabant</surname><given-names>EM</given-names></name><name name-style="western"><surname>Verchinski</surname><given-names>BA</given-names></name><name name-style="western"><surname>Munoz</surname><given-names>KE</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>5-HTTLPR polymorphism impacts human cingulate-amygdala interactions: A genetic susceptibility mechanism for depression.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>828</fpage>             <lpage>834</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Etkin2">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Etkin</surname><given-names>A</given-names></name><name name-style="western"><surname>Egner</surname><given-names>T</given-names></name><name name-style="western"><surname>Peraza</surname><given-names>DM</given-names></name><name name-style="western"><surname>Kandel</surname><given-names>ER</given-names></name><name name-style="western"><surname>Hirsch</surname><given-names>J</given-names></name></person-group>             <year>2006</year>             <article-title>Resolving emotional conflict: A role for the rostral anterior cingulate cortex in modulating activity in the amygdala.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>871</fpage>             <lpage>882</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Fairhall1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fairhall</surname><given-names>SL</given-names></name><name name-style="western"><surname>Ishai</surname><given-names>A</given-names></name></person-group>             <year>2007</year>             <article-title>Effective connectivity within the distributed cortical network for face perception.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>2400</fpage>             <lpage>2406</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Ishai2">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ishai</surname><given-names>A</given-names></name></person-group>             <year>2008</year>             <article-title>Let's face it: It's a cortical network.</article-title>             <source>Neuroimage</source>             <volume>40</volume>             <fpage>415</fpage>             <lpage>419</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Stein1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>JL</given-names></name><name name-style="western"><surname>Wiedholz</surname><given-names>LM</given-names></name><name name-style="western"><surname>Bassett</surname><given-names>DS</given-names></name><name name-style="western"><surname>Weinberger</surname><given-names>DR</given-names></name><name name-style="western"><surname>Zink</surname><given-names>CF</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>A validated network of effective amygdala connectivity.</article-title>             <source>Neuroimage</source>             <volume>36</volume>             <fpage>736</fpage>             <lpage>745</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Li1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>K</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L</given-names></name><name name-style="western"><surname>Nie</surname><given-names>J</given-names></name><name name-style="western"><surname>Li</surname><given-names>G</given-names></name><name name-style="western"><surname>Liu</surname><given-names>T</given-names></name></person-group>             <year>2009</year>             <article-title>Review of methods for functional brain connectivity detection using fMRI.</article-title>             <source>Comput Med Imaging Graph</source>             <volume>33</volume>             <fpage>131</fpage>             <lpage>139</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Smith1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>SM</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KL</given-names></name><name name-style="western"><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name name-style="western"><surname>Webster</surname><given-names>M</given-names></name><name name-style="western"><surname>Beckmann</surname><given-names>CF</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Network modelling methods for FMRI.</article-title>             <source>Neuroimage</source>             <volume>54</volume>             <fpage>875</fpage>             <lpage>891</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Dosenbach1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dosenbach</surname><given-names>NU</given-names></name><name name-style="western"><surname>Nardos</surname><given-names>B</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>AL</given-names></name><name name-style="western"><surname>Fair</surname><given-names>DA</given-names></name><name name-style="western"><surname>Power</surname><given-names>JD</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Prediction of individual brain maturity using fMRI.</article-title>             <source>Science</source>             <volume>329</volume>             <fpage>1358</fpage>             <lpage>1361</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Shirer1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shirer</surname><given-names>WR</given-names></name><name name-style="western"><surname>Ryali</surname><given-names>S</given-names></name><name name-style="western"><surname>Rykhlevskaia</surname><given-names>E</given-names></name><name name-style="western"><surname>Menon</surname><given-names>V</given-names></name><name name-style="western"><surname>Greicius</surname><given-names>MD</given-names></name></person-group>             <year>2011</year>             <article-title>Decoding subject-driven cognitive states with whole-brain connectivity patterns.</article-title>             <source>Cereb Cortex</source>             <volume>22</volume>             <fpage>158</fpage>             <lpage>65</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Richiardi1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Richiardi</surname><given-names>J</given-names></name><name name-style="western"><surname>Eryilmaz</surname><given-names>H</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>S</given-names></name><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name><name name-style="western"><surname>Van De Ville</surname><given-names>D</given-names></name></person-group>             <year>2011</year>             <article-title>Decoding brain states from fMRI connectivity graphs.</article-title>             <source>Neuroimage</source>             <volume>56</volume>             <fpage>616</fpage>             <lpage>626</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Marreiros1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marreiros</surname><given-names>AC</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2008</year>             <article-title>Dynamic causal modelling for fMRI: A two-state model.</article-title>             <source>Neuroimage</source>             <volume>39</volume>             <fpage>269</fpage>             <lpage>278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Ekman1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ekman</surname><given-names>P</given-names></name><name name-style="western"><surname>Friesen</surname><given-names>WV</given-names></name></person-group>             <year>1971</year>             <article-title>Constants across cultures in the face and emotion.</article-title>             <source>J Pers Soc Psychol</source>             <volume>17</volume>             <fpage>124</fpage>             <lpage>129</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Vapnik1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vapnik</surname><given-names>VN</given-names></name></person-group>             <year>1999</year>             <article-title>An overview of statistical learning theory.</article-title>             <source>IEEE Trans Neural Netw</source>             <volume>10</volume>             <fpage>988</fpage>             <lpage>999</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Golland1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Golland</surname><given-names>P</given-names></name><name name-style="western"><surname>Fischl</surname><given-names>B</given-names></name></person-group>             <year>2003</year>             <article-title>Permutation tests for classification: Towards statistical significance in image-based studies.</article-title>             <source>Inf Process Med Imaging</source>             <volume>18</volume>             <fpage>330</fpage>             <lpage>341</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Ross1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ross</surname><given-names>TD</given-names></name></person-group>             <year>2003</year>             <article-title>Accurate confidence intervals for binomial proportion and poisson rate estimation.</article-title>             <source>Comput Biol Med</source>             <volume>33</volume>             <fpage>509</fpage>             <lpage>531</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Spreng1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Spreng</surname><given-names>RN</given-names></name><name name-style="western"><surname>Mar</surname><given-names>RA</given-names></name></person-group>             <year>2010</year>             <article-title>I remember you: A role for memory in social cognition and the functional neuroanatomy of their interaction.</article-title>             <source>Brain Res</source>             <volume>1428</volume>             <fpage>43</fpage>             <lpage>50</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Haxby2">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name><name name-style="western"><surname>Hoffman</surname><given-names>EA</given-names></name><name name-style="western"><surname>Gobbini</surname><given-names>MI</given-names></name></person-group>             <year>2002</year>             <article-title>Human neural systems for face recognition and social communication.</article-title>             <source>Biol Psychiatry</source>             <volume>51</volume>             <fpage>59</fpage>             <lpage>67</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Sabatinelli1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sabatinelli</surname><given-names>D</given-names></name><name name-style="western"><surname>Fortune</surname><given-names>EE</given-names></name><name name-style="western"><surname>Li</surname><given-names>Q</given-names></name><name name-style="western"><surname>Siddiqui</surname><given-names>A</given-names></name><name name-style="western"><surname>Krafft</surname><given-names>C</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Emotional perception: Meta-analyses of face and natural scene processing.</article-title>             <source>Neuroimage</source>             <volume>54</volume>             <fpage>2524</fpage>             <lpage>2533</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Engell1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Engell</surname><given-names>AD</given-names></name><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name></person-group>             <year>2007</year>             <article-title>Facial expression and gaze-direction in human superior temporal sulcus.</article-title>             <source>Neuropsychologia</source>             <volume>45</volume>             <fpage>3234</fpage>             <lpage>3241</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Said1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Said</surname><given-names>CP</given-names></name><name name-style="western"><surname>Moore</surname><given-names>CD</given-names></name><name name-style="western"><surname>Engell</surname><given-names>AD</given-names></name><name name-style="western"><surname>Todorov</surname><given-names>A</given-names></name><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name></person-group>             <year>2010</year>             <article-title>Distributed representations of dynamic facial expressions in the superior temporal sulcus.</article-title>             <source>J Vis</source>             <volume>10</volume>             <fpage>11</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Peelen1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Peelen</surname><given-names>MV</given-names></name><name name-style="western"><surname>Atkinson</surname><given-names>AP</given-names></name><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group>             <year>2010</year>             <article-title>Supramodal representations of perceived emotions in the human brain.</article-title>             <source>J Neurosci</source>             <volume>30</volume>             <fpage>10127</fpage>             <lpage>10134</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Tsuchiya1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsuchiya</surname><given-names>N</given-names></name><name name-style="western"><surname>Kawasaki</surname><given-names>H</given-names></name><name name-style="western"><surname>Oya</surname><given-names>H</given-names></name><name name-style="western"><surname>Howard</surname><given-names>MA</given-names><suffix>3rd</suffix></name><name name-style="western"><surname>Adolphs</surname><given-names>R</given-names></name></person-group>             <year>2008</year>             <article-title>Decoding face information in time, frequency and space from direct intracranial recordings of the human brain.</article-title>             <source>PLoS One</source>             <volume>3</volume>             <fpage>e3892</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-FusarPoli1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusar-Poli</surname><given-names>P</given-names></name><name name-style="western"><surname>Placentino</surname><given-names>A</given-names></name><name name-style="western"><surname>Carletti</surname><given-names>F</given-names></name><name name-style="western"><surname>Landi</surname><given-names>P</given-names></name><name name-style="western"><surname>Allen</surname><given-names>P</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>Functional atlas of emotional faces processing: A voxel-based meta-analysis of 105 functional magnetic resonance imaging studies.</article-title>             <source>J Psychiatry Neurosci</source>             <volume>34</volume>             <fpage>418</fpage>             <lpage>432</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Pessoa2">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pessoa</surname><given-names>L</given-names></name><name name-style="western"><surname>Adolphs</surname><given-names>R</given-names></name></person-group>             <year>2010</year>             <article-title>Emotion processing and the amygdala: From a ‘low road’ to ‘many roads’ of evaluating biological significance.</article-title>             <source>Nat Rev Neurosci</source>             <volume>11</volume>             <fpage>773</fpage>             <lpage>783</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Yeterian1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yeterian</surname><given-names>EH</given-names></name><name name-style="western"><surname>Pandya</surname><given-names>DN</given-names></name></person-group>             <year>1991</year>             <article-title>Corticothalamic connections of the superior temporal sulcus in rhesus monkeys.</article-title>             <source>Exp Brain Res</source>             <volume>83</volume>             <fpage>268</fpage>             <lpage>284</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-FusarPoli2">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusar-Poli</surname><given-names>P</given-names></name><name name-style="western"><surname>Bhattacharyya</surname><given-names>S</given-names></name><name name-style="western"><surname>Allen</surname><given-names>P</given-names></name><name name-style="western"><surname>Crippa</surname><given-names>JA</given-names></name><name name-style="western"><surname>Borgwardt</surname><given-names>S</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Effect of image analysis software on neurofunctional activation during processing of emotional human faces.</article-title>             <source>J Clin Neurosci</source>             <volume>17</volume>             <fpage>311</fpage>             <lpage>4</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Karama1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Karama</surname><given-names>S</given-names></name><name name-style="western"><surname>Armony</surname><given-names>J</given-names></name><name name-style="western"><surname>Beauregard</surname><given-names>M</given-names></name></person-group>             <year>2011</year>             <article-title>Film excerpts shown to specifically elicit various affects lead to overlapping activation foci in a large set of symmetrical brain regions in males.</article-title>             <source>PLoS One</source>             <volume>6</volume>             <fpage>e22343</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Stoodley1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stoodley</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Schmahmann</surname><given-names>JD</given-names></name></person-group>             <year>2010</year>             <article-title>Evidence for topographic organization in the cerebellum of motor control versus cognitive and affective processing.</article-title>             <source>Cortex</source>             <volume>46</volume>             <fpage>831</fpage>             <lpage>844</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Sabatinelli2">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sabatinelli</surname><given-names>D</given-names></name><name name-style="western"><surname>Bradley</surname><given-names>MM</given-names></name><name name-style="western"><surname>Fitzsimmons</surname><given-names>JR</given-names></name><name name-style="western"><surname>Lang</surname><given-names>PJ</given-names></name></person-group>             <year>2005</year>             <article-title>Parallel amygdala and inferotemporal activation reflect emotional intensity and fear relevance.</article-title>             <source>Neuroimage</source>             <volume>24</volume>             <fpage>1265</fpage>             <lpage>1270</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Vuilleumier3">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name><name name-style="western"><surname>Richardson</surname><given-names>MP</given-names></name><name name-style="western"><surname>Armony</surname><given-names>JL</given-names></name><name name-style="western"><surname>Driver</surname><given-names>J</given-names></name><name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name></person-group>             <year>2004</year>             <article-title>Distant influences of amygdala lesion on visual cortical activation during emotional face processing.</article-title>             <source>Nat Neurosci</source>             <volume>7</volume>             <fpage>1271</fpage>             <lpage>1278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Gorman1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gorman</surname><given-names>JM</given-names></name><name name-style="western"><surname>Kent</surname><given-names>JM</given-names></name><name name-style="western"><surname>Sullivan</surname><given-names>GM</given-names></name><name name-style="western"><surname>Coplan</surname><given-names>JD</given-names></name></person-group>             <year>2000</year>             <article-title>Neuroanatomical hypothesis of panic disorder, revised.</article-title>             <source>Am J Psychiatry</source>             <volume>157</volume>             <fpage>493</fpage>             <lpage>505</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Kent1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kent</surname><given-names>JM</given-names></name><name name-style="western"><surname>Rauch</surname><given-names>SL</given-names></name></person-group>             <year>2003</year>             <article-title>Neurocircuitry of anxiety disorders.</article-title>             <source>Curr Psychiatry Rep</source>             <volume>5</volume>             <fpage>266</fpage>             <lpage>273</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Coplan1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Coplan</surname><given-names>JD</given-names></name><name name-style="western"><surname>Lydiard</surname><given-names>RB</given-names></name></person-group>             <year>1998</year>             <article-title>Brain circuits in panic disorder.</article-title>             <source>Biol Psychiatry</source>             <volume>44</volume>             <fpage>1264</fpage>             <lpage>1276</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Bishop2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name><name name-style="western"><surname>Lawrence</surname><given-names>AD</given-names></name></person-group>             <year>2004</year>             <article-title>State anxiety modulation of the amygdala response to unattended threat-related stimuli.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>10364</fpage>             <lpage>10368</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Banks1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Banks</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Eddy</surname><given-names>KT</given-names></name><name name-style="western"><surname>Angstadt</surname><given-names>M</given-names></name><name name-style="western"><surname>Nathan</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Phan</surname><given-names>KL</given-names></name></person-group>             <year>2007</year>             <article-title>Amygdala-frontal connectivity during emotion regulation.</article-title>             <source>Soc Cogn Affect Neurosci</source>             <volume>2</volume>             <fpage>303</fpage>             <lpage>312</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Dodel1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dodel</surname><given-names>S</given-names></name><name name-style="western"><surname>Golestani</surname><given-names>N</given-names></name><name name-style="western"><surname>Pallier</surname><given-names>C</given-names></name><name name-style="western"><surname>Elkouby</surname><given-names>V</given-names></name><name name-style="western"><surname>Le Bihan</surname><given-names>D</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>Condition-dependent functional connectivity: Syntax networks in bilinguals.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>360</volume>             <fpage>921</fpage>             <lpage>935</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Ginestet1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ginestet</surname><given-names>CE</given-names></name><name name-style="western"><surname>Simmons</surname><given-names>A</given-names></name></person-group>             <year>2011</year>             <article-title>Statistical parametric network analysis of functional connectivity dynamics during a working memory task.</article-title>             <source>Neuroimage</source>             <volume>55</volume>             <fpage>688</fpage>             <lpage>704</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Zalesky1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zalesky</surname><given-names>A</given-names></name><name name-style="western"><surname>Fornito</surname><given-names>A</given-names></name><name name-style="western"><surname>Bullmore</surname><given-names>ET</given-names></name></person-group>             <year>2010</year>             <article-title>Network-based statistic: Identifying differences in brain networks.</article-title>             <source>Neuroimage</source>             <volume>53</volume>             <fpage>1197</fpage>             <lpage>1207</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Haxby3">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name><name name-style="western"><surname>Gobbini</surname><given-names>MI</given-names></name><name name-style="western"><surname>Furey</surname><given-names>ML</given-names></name><name name-style="western"><surname>Ishai</surname><given-names>A</given-names></name><name name-style="western"><surname>Schouten</surname><given-names>JL</given-names></name><etal/></person-group>             <year>2001</year>             <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex.</article-title>             <source>Science</source>             <volume>293</volume>             <fpage>2425</fpage>             <lpage>2430</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Cox1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name><name name-style="western"><surname>Savoy</surname><given-names>RL</given-names></name></person-group>             <year>2003</year>             <article-title>Functional magnetic resonance imaging (fMRI) “brain reading”: Detecting and classifying distributed patterns of fMRI activity in human visual cortex.</article-title>             <source>Neuroimage</source>             <volume>19</volume>             <fpage>261</fpage>             <lpage>270</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Hanson1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hanson</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Matsuka</surname><given-names>T</given-names></name><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name></person-group>             <year>2004</year>             <article-title>Combinatorial codes in ventral temporal lobe for object recognition: Haxby (2001) revisited: Is there a “face” area?</article-title>             <source>Neuroimage</source>             <volume>23</volume>             <fpage>156</fpage>             <lpage>166</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-OToole1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>O'Toole</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>F</given-names></name><name name-style="western"><surname>Abdi</surname><given-names>H</given-names></name><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name></person-group>             <year>2005</year>             <article-title>Partially distributed representations of objects and faces in ventral temporal cortex.</article-title>             <source>J Cogn Neurosci</source>             <volume>17</volume>             <fpage>580</fpage>             <lpage>590</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-MouraoMiranda1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mourao-Miranda</surname><given-names>J</given-names></name><name name-style="western"><surname>Bokde</surname><given-names>AL</given-names></name><name name-style="western"><surname>Born</surname><given-names>C</given-names></name><name name-style="western"><surname>Hampel</surname><given-names>H</given-names></name><name name-style="western"><surname>Stetter</surname><given-names>M</given-names></name></person-group>             <year>2005</year>             <article-title>Classifying brain states and determining the discriminating activation patterns: Support vector machine on functional MRI data.</article-title>             <source>Neuroimage</source>             <volume>28</volume>             <fpage>980</fpage>             <lpage>995</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Kamitani1">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kamitani</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tong</surname><given-names>F</given-names></name></person-group>             <year>2005</year>             <article-title>Decoding the visual and subjective contents of the human brain.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>679</fpage>             <lpage>685</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Haynes1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haynes</surname><given-names>JD</given-names></name><name name-style="western"><surname>Rees</surname><given-names>G</given-names></name></person-group>             <year>2005</year>             <article-title>Predicting the orientation of invisible stimuli from activity in human primary visual cortex.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>686</fpage>             <lpage>691</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Pessoa3">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pessoa</surname><given-names>L</given-names></name><name name-style="western"><surname>Padmala</surname><given-names>S</given-names></name></person-group>             <year>2007</year>             <article-title>Decoding near-threshold perception of fear from distributed single-trial brain activation.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>691</fpage>             <lpage>701</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002441-Monti1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Monti</surname><given-names>MM</given-names></name></person-group>             <year>2011</year>             <article-title>Statistical analysis of fMRI time-series: A critical review of the GLM approach.</article-title>             <source>Front Hum Neurosci</source>             <volume>5</volume>             <fpage>28</fpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>