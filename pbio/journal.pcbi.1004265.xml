<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00697</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004265</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Energy Efficient Sparse Connectivity from Imbalanced Synaptic Plasticity Rules</article-title>
<alt-title alt-title-type="running-head">Energy Efficient Sparse Connectivity from Imbalanced Plasticity</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Sacramento</surname> <given-names>João</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wichert</surname> <given-names>Andreas</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>van Rossum</surname> <given-names>Mark C. W.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>INESC-ID &amp; Instituto Superior Técnico, Universidade de Lisboa, Porto Salvo, Portugal</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Institute for Adaptive and Neural Computation, School of Informatics, University of Edinburgh, Edinburgh, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: JS AW MCWvR. Performed the experiments: JS. Analyzed the data: JS MCWvR. Contributed reagents/materials/analysis tools: JS AW MCWvR. Wrote the paper: JS MCWvR.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">joao.sacramento@ist.utl.pt</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>6</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>5</day>
<month>6</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>6</issue>
<elocation-id>e1004265</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>4</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>4</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Sacramento et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004265" xlink:type="simple"/>
<abstract>
<p>It is believed that energy efficiency is an important constraint in brain evolution. As synaptic transmission dominates energy consumption, energy can be saved by ensuring that only a few synapses are active. It is therefore likely that the formation of sparse codes and sparse connectivity are fundamental objectives of synaptic plasticity. In this work we study how sparse connectivity can result from a synaptic learning rule of excitatory synapses. Information is maximised when potentiation and depression are balanced according to the mean presynaptic activity level and the resulting fraction of zero-weight synapses is around 50%. However, an imbalance towards depression increases the fraction of zero-weight synapses without significantly affecting performance. We show that imbalanced plasticity corresponds to imposing a regularising constraint on the <italic>L</italic><sub>1</sub>-norm of the synaptic weight vector, a procedure that is well-known to induce sparseness. Imbalanced plasticity is biophysically plausible and leads to more efficient synaptic configurations than a previously suggested approach that prunes synapses after learning. Our framework gives a novel interpretation to the high fraction of silent synapses found in brain regions like the cerebellum.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Recent estimates point out that a large part of the energetic budget of the mammalian cortex is spent in transmitting signals between neurons across synapses. Despite this, studies of learning and memory do not usually take energy efficiency into account. In this work we address the canonical computational problem of storing memories with synaptic plasticity. However, instead of optimising solely for information capacity, we search for energy efficient solutions. This implies that the number of functional synapses needs to be small (sparse connectivity) while maintaining high information. We suggest imbalanced plasticity, a learning regime where net depression is stronger than potentiation, as a simple and plausible means to learn more efficient neural circuits. Our framework gives a novel interpretation to the high fraction of silent synapses found in brain regions like the cerebellum.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by national funds through Fundação para a Ciência e a Tecnologia (FCT) with reference UID/CEC/50021/2013 and two individual grants awarded to JS with references SFRH/BD/66398/2009 and Incentivo/EEI/LA0021/2014. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The brain is not only a very powerful device, but it also has remarkable energy efficiency compared to computers [<xref ref-type="bibr" rid="pcbi.1004265.ref001">1</xref>]. It has been estimated that most of the energy used by the brain is associated to synaptic transmission [<xref ref-type="bibr" rid="pcbi.1004265.ref002">2</xref>]. Therefore to minimise energy consumption, the number of active synapses should be as low as possible while maintaining computational power [<xref ref-type="bibr" rid="pcbi.1004265.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref004">4</xref>]. The number of active synapses is the product of the activity and the number of synapses. Energy can thus be reduced in two ways: 1) by employing a <italic>sparse neural code</italic>, in which only few neurons are active at any time, 2) by removing synapses leading to <italic>sparse connectivity</italic>, leaving only few synapses out of many potential ones. This latter process is also called dilution of the connectivity. Remarkably, during human development brain metabolism neatly tracks synapse density, rapidly increasing after birth followed by a reduction into adolescence (e.g. compare the data in [<xref ref-type="bibr" rid="pcbi.1004265.ref005">5</xref>] to [<xref ref-type="bibr" rid="pcbi.1004265.ref006">6</xref>]).</p>
<p>Most computational algorithms of learning, however, optimise storage capacity without taking energy efficiency into account (but see [<xref ref-type="bibr" rid="pcbi.1004265.ref003">3</xref>]) and as a result only limited agreement between models and experimental data can be expected. The best studied artificial example of learning is the perceptron which learns to classify two sets of input patterns. Despite its simplicity, results of perceptron learning are crucial as they for instance guide the design of recurrent attractor networks [<xref ref-type="bibr" rid="pcbi.1004265.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref009">9</xref>]. Provided the task can be learned, the perceptron learning rule is guaranteed to find the correct synaptic weights. The traditional perceptron learning algorithm assumes that weights can have any value and can change sign. In that case a perceptron with <italic>N</italic> synapses can on average learn 2<italic>N</italic> random patterns. At the maximum load the corresponding weight distribution is Gaussian, i.e., the connectivity is dense and hence energy inefficient [<xref ref-type="bibr" rid="pcbi.1004265.ref010">10</xref>]. If one restricts the synapses to be excitatory, the capacity is halved [<xref ref-type="bibr" rid="pcbi.1004265.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref011">11</xref>].</p>
<p>In this work we ask which learning algorithm maximises energy efficient storage, and thus maximises the number of silent synapses while still being able to perform a learning task [<xref ref-type="bibr" rid="pcbi.1004265.ref003">3</xref>]. However, finding the weight configuration with the fewest possible (non-zero) synapses is a combinatorial <italic>L</italic><sub>0</sub>-norm minimisation task. This is in general a NP-hard problem [<xref ref-type="bibr" rid="pcbi.1004265.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref013">13</xref>] and thus difficult to solve exactly. Using the replica method from statistical mechanics it is possible to calculate limits on the achievable memory performance with a fixed number of synapses [<xref ref-type="bibr" rid="pcbi.1004265.ref010">10</xref>], but such methods do not yield insight on how to accomplish this. An earlier approach prunes the smallest synapses after learning. If synapses are to be removed after learning, this procedure is optimal [<xref ref-type="bibr" rid="pcbi.1004265.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref015">15</xref>]. Yet, as we will show it is far better to incorporate a sparse connectivity objective during the learning process.</p>
<p>Here we explore imbalanced plasticity as a simple and biologically plausible way to reduce the number of required synapses and thus improve information storage efficiency. In many memory models the amount of potentiation and depression are precisely matched to the statistics of the neural activity [<xref ref-type="bibr" rid="pcbi.1004265.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>], but here we deliberately perturb the optimal plasticity rule by introducing a bias towards depression. This imbalanced plasticity finds weight configurations that require less functional synapses and that are thus more energy efficient.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>The model</title>
<p>We consider a recognition task from positive examples [<xref ref-type="bibr" rid="pcbi.1004265.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref022">22</xref>]. The perceptron should learn to give a response whenever a sample from a given category is presented. In contrast to the standard perceptron algorithm, which ‘unlearns patterns’ for which the neuron should not be active, the synapses are not modified for negative samples. It has been argued that this setup is relevant to biology in particular when the set of negative samples is very large and/or its statistics unknown [<xref ref-type="bibr" rid="pcbi.1004265.ref022">22</xref>]. For instance, one might want to train a neuron to recognise fruits, but not update the synapses for all other objects. This setup is also relevant when studying reinforcement learning, where learning is gated by reward feedback elicited by positive samples. Finally, it resembles the one-class support vector machine used in statistical learning, which detects whether a sample belongs to a class and which has applications in anomaly detection [<xref ref-type="bibr" rid="pcbi.1004265.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref024">24</xref>].</p>
<p>The setup is illustrated in <xref ref-type="fig" rid="pcbi.1004265.g001">Fig 1</xref>. A single postsynaptic neuron calculates the weighted sum of its <italic>N</italic> excitatory inputs and compares it to a positive threshold <inline-formula id="pcbi.1004265.e001"><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. Whenever <inline-formula id="pcbi.1004265.e002"><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> is non-negative, the perceptron fires. The inputs <italic>x</italic><sub><italic>i</italic></sub> are randomly chosen to be -1 or +1 with equal probability, and independently of the other inputs (see below for extensions). The <inline-formula id="pcbi.1004265.e003"><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> in the threshold is a mathematical convenience that ensures scaling of the system as the number of inputs is varied [<xref ref-type="bibr" rid="pcbi.1004265.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref025">25</xref>].</p>
<fig id="pcbi.1004265.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Diagram of our single neuron setup.</title>
<p>A group of <italic>N</italic> input presynaptic neurons are connected to a single postsynaptic neuron. The input activity can be low, <italic>x</italic><sub><italic>i</italic></sub> = −1, or high, <italic>x</italic><sub><italic>i</italic></sub> = 1. The postsynaptic neuron performs a weighted sum of the inputs and fires whenever that sum is larger than a threshold <inline-formula id="pcbi.1004265.e004"><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, otherwise it remains quiet. Each synapse <italic>w</italic><sub><italic>i</italic></sub> is adjusted as a function of the input activity so that the neuron remembers a set of previously seen patterns. Ideally, only these patterns should trigger the neuron; all other patterns should not.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g001"/>
</fig>
<p>During learning the neuron is provided with a set of <italic>K</italic> positive patterns, <bold>x</bold><sup>1</sup>, …, <bold>x</bold><sup><italic>k</italic></sup>, …, <bold>x</bold><sup><italic>K</italic></sup>. As in the standard perceptron, we cycle through the set of patterns until the task is learned. The goal of the perceptron is to ‘fire’ for all these patterns. This should be contrasted to setups in which samples are presented only once (one-shot learning), which generally lead to a lower capacity [<xref ref-type="bibr" rid="pcbi.1004265.ref025">25</xref>]. We assume that initially all weights <italic>w</italic><sub><italic>i</italic></sub> are zero (<italic>tabula rasa</italic>). The learning rule is as follows: whenever a positive pattern is presented and only if it does not lead to postsynaptic activity, the synapse is updated. For high inputs, i.e., <italic>x</italic><sub><italic>i</italic></sub> = 1, potentiation occurs
<disp-formula id="pcbi.1004265.e005"><alternatives><graphic id="pcbi.1004265.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>+</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where Θ(⋅) is the Heaviside step function which is zero if its argument is negative and one otherwise, and <italic>a</italic> ≪ 1 is the potentiation rate. Similarly, when an input <italic>x</italic><sub><italic>i</italic></sub> is low, the synapse depresses
<disp-formula id="pcbi.1004265.e006"><alternatives><graphic id="pcbi.1004265.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>-</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>b</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>b</italic> is the amount of depression. Depression is followed by rectification so that all synapses remain excitatory, <italic>w</italic><sub><italic>i</italic></sub> ≥ 0 [<xref ref-type="bibr" rid="pcbi.1004265.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref030">30</xref>]. If the pattern does already lead to firing of the perceptron, no synapse is altered. This stop-learning condition is also present in a standard perceptron; possible biophysical mechanisms are discussed in [<xref ref-type="bibr" rid="pcbi.1004265.ref031">31</xref>].</p>
<p>For the simple, random pattern statistics used here, the non-negativity constraint limits the maximal amount of patterns that can be learned to <italic>K</italic><sub>max</sub> = <italic>N</italic> [<xref ref-type="bibr" rid="pcbi.1004265.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref011">11</xref>], which is half of the number of patterns an unconstrained perceptron can learn. Below this limit the learning process finishes with high probability in a number of steps that is polynomial in <italic>N</italic>. We define the memory load <italic>α</italic> = <italic>K</italic>/<italic>N</italic>, which becomes <italic>α</italic><sub>max</sub> = 1 at the maximal load in the balanced case.</p>
</sec>
<sec id="sec004">
<title>Imbalancing plasticity promotes sparseness</title>
<p>Unlike the traditional perceptron rule, we allow for distinct amounts of potentiation and depression. By introducing imbalance in favour of depression the learning dynamics is biased towards the hard bound of the weight at zero. We rewrite the plasticity rule using the learning rate <italic>ε</italic> ≡ (<italic>a</italic>+<italic>b</italic>)/2 and an imbalance parameter <italic>λ</italic> ≡ (<italic>b</italic>−<italic>a</italic>)/2<italic>ε</italic>. Provided the synapse does not hit the zero bound, the weight update is
<disp-formula id="pcbi.1004265.e007"><alternatives><graphic id="pcbi.1004265.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
The parameter <italic>λ</italic> is zero for balanced learning; depression is stronger than potentiation if 0 &lt; <italic>λ</italic> ≤ 1. We find somewhat improved faster learning when we also depress even when the pattern has already been learned, i.e.
<disp-formula id="pcbi.1004265.e008"><alternatives><graphic id="pcbi.1004265.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>λ</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
For that case it can be shown that the learning dynamics minimises the energy function
<disp-formula id="pcbi.1004265.e009"><alternatives><graphic id="pcbi.1004265.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where [⋅]<sub>+</sub> denotes rectification. The first term of the energy sums over all patterns and promotes low false negative rates; it is zero if the perceptron fires, while it attributes a cost proportional to the distance to the firing threshold whenever a pattern is not yet learned. The second term acts as a linear regulariser; the depression-potentiation imbalance <italic>λ</italic> penalises synaptic weight configurations that have large linear norms <inline-formula id="pcbi.1004265.e010"><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The regularisation term has a simple interpretation, as it is proportional to the mean synaptic weight, ∣<bold>w</bold>∣ = <italic>N</italic>⟨<italic>w</italic>⟩. The plasticity rule, <xref ref-type="disp-formula" rid="pcbi.1004265.e008">Eq 4</xref>, minimises this energy by performing a stochastic sub-gradient descent [<xref ref-type="bibr" rid="pcbi.1004265.ref032">32</xref>], projected onto the subspace {<bold>w</bold>: <italic>w</italic><sub><italic>i</italic></sub> ≥ 0, <italic>i</italic> = 1, …, <italic>N</italic>}.</p>
<p>Rewriting the learning rule as the minimisation of the energy <xref ref-type="disp-formula" rid="pcbi.1004265.e009">Eq (5)</xref> shows explicitly why introducing imbalance towards depression promotes weight sparseness. In linear regression and classification, optimising over regularised energy functions that penalise the <italic>L</italic><sub>1</sub>-norm <inline-formula id="pcbi.1004265.e011"><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mo stretchy="false">∣</mml:mo> <mml:mn>1</mml:mn></mml:msub> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">∣</mml:mo></mml:mrow></mml:math></inline-formula> of the weights is well-known to induce sparseness [<xref ref-type="bibr" rid="pcbi.1004265.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref035">35</xref>]. Below the critical load <italic>α</italic><sub>max</sub> the weight configuration with minimal linear norm is known to be sparse [<xref ref-type="bibr" rid="pcbi.1004265.ref027">27</xref>]. Thus, the learning rule <xref ref-type="disp-formula" rid="pcbi.1004265.e008">Eq (4)</xref> with imbalance <italic>λ</italic> &gt; 0 will try to find solutions that satisfy the learning conditions but that are sparser than those obtained when <italic>λ</italic> = 0.</p>
<p>While the linear norm constraint promotes sparseness, it is not guaranteed to produce the sparsest possible solution. The true optimisation problem would be to minimise the <italic>L</italic><sub>0</sub>-pseudo-norm ∣∣<bold>w</bold>∣∣<sub>0</sub>. The <italic>L</italic><sub>0</sub>-pseudo-norm simply counts the number of non-zero synapses. However, this leads to a difficult NP-hard combinatorial optimisation task [<xref ref-type="bibr" rid="pcbi.1004265.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref013">13</xref>]. Instead, optimising under the <italic>L</italic><sub>1</sub>-norm constraint is a convex relaxation of the original problem for which efficient computer algorithms exist (e.g. [<xref ref-type="bibr" rid="pcbi.1004265.ref036">36</xref>]). Moreover, imbalancing plasticity has the advantage of being an online procedure that only requires tuning the potentiation and depression event sizes and is thus biologically plausible.</p>
</sec>
<sec id="sec005">
<title>Information and efficiency</title>
<p>Ideally our perceptron learns all examples, and minimises the false positive rate. To characterise the performance we present the perceptron with learned samples and lures (other random patterns), both presented with equal probability. The mutual information between the class of the input pattern and the perceptron’s output on a given trial is
<disp-formula id="pcbi.1004265.e012"><alternatives><graphic id="pcbi.1004265.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munder> <mml:mspace width="1pt"/><mml:mi mathvariant="normal">P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="normal">P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo form="prefix">log</mml:mo> <mml:mn>2</mml:mn></mml:msub> <mml:mfrac><mml:mrow><mml:mi mathvariant="normal">P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>∣</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi mathvariant="normal">P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where P(<italic>x</italic>) = 1/2 is the probability that the test pattern is a positive pattern (<italic>p</italic>) or negative lure pattern (<italic>l</italic>), P(<italic>r</italic>) is the probability that the perceptron remains silent or fires, and P(<italic>r</italic>∣<italic>x</italic>) is the conditional probability that we observe a given response given the true pattern class.</p>
<p>The information can be expressed in terms of the false positive rate <italic>p</italic><sub>01</sub> and the false negative rate <italic>p</italic><sub>10</sub>. Below the critical capacity (<italic>α</italic> ≤ <italic>α</italic><sub>max</sub>), the positive samples are recognised perfectly after learning, i.e. there are no false negatives (<italic>p</italic><sub>10</sub> = 0), so that the information is determined by the false positive rate only. As we have 2<italic>K</italic> trials, the total information normalised per synapse, <inline-formula id="pcbi.1004265.e013"><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>K</mml:mi></mml:mrow> <mml:mi>N</mml:mi></mml:mfrac> <mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula>, equals
<disp-formula id="pcbi.1004265.e014"><alternatives><graphic id="pcbi.1004265.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>K</mml:mi></mml:mrow> <mml:mi>N</mml:mi></mml:mfrac> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo form="prefix">log</mml:mo> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:msub><mml:mo form="prefix">log</mml:mo> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>p</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Although this type of information calculation is common, we note that testing with equiprobable lures and learned patterns is somewhat sub-optimal in terms of information [<xref ref-type="bibr" rid="pcbi.1004265.ref037">37</xref>]. For the one-class perceptron, testing exhaustively with all 2<sup><italic>N</italic></sup>−<italic>K</italic> possible lures gives about 60.6% more information when <italic>p</italic><sub>01</sub> = 1/2 with a weak dependence on <italic>p</italic><sub>01</sub>.</p>
<p>As the mutual information does not take energy efficiency into account, we consider a recently suggested capacity measure that includes the sparseness of the final weight configuration [<xref ref-type="bibr" rid="pcbi.1004265.ref003">3</xref>]. The <italic>memory efficiency</italic> <italic>S</italic> measures the information per non-zero synapse by normalising the information to the fraction of non-zero synapses <italic>F</italic>,
<disp-formula id="pcbi.1004265.e015"><alternatives><graphic id="pcbi.1004265.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e015"/><mml:math id="M15" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>C</mml:mi> <mml:mi>F</mml:mi></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
Memory efficiency is thus measured in <italic>bits per functional synapse</italic>. Learning rules that achieve high information <italic>C</italic> using few resources will have high efficiency. If one assumes that a non-zero synapse has a certain energy cost (independent of synaptic weight) and a zero synapse has none, the memory efficiency <italic>S</italic> measures the energy cost of the stored memory.</p>
</sec>
<sec id="sec006">
<title>Imbalanced plasticity improves memory efficiency</title>
<p>A variant of the sign-constrained perceptron convergence theorem (see <xref ref-type="sec" rid="sec013">Methods</xref>) shows that the learning algorithm <xref ref-type="disp-formula" rid="pcbi.1004265.e007">Eq 3</xref> converges below a critical imbalance <italic>λ</italic><sub>max</sub>(<italic>α</italic>) that depends on the memory load <italic>α</italic>. In computer simulations we focus on the two extreme cases, i.e., balanced (<italic>λ</italic> = 0) and maximally-imbalanced <italic>λ</italic> = <italic>λ</italic><sub>max</sub>(<italic>α</italic>) plasticity. In principle it is possible to find the maximum imbalance by trying various values of <italic>λ</italic> and checking convergence of the learning process. However, it is much quicker to use that the problem is equivalent to learn the patterns while minimising the linear norm ∣<bold>w</bold>∣, see <xref ref-type="disp-formula" rid="pcbi.1004265.e009">Eq 5</xref>. This was done with a linear programming solver (see <xref ref-type="sec" rid="sec013">Methods</xref>) which requires no manual search for the maximal imbalance.</p>
<p>For strongest depression (<italic>λ</italic> = <italic>λ</italic><sub>max</sub>), the information <italic>C</italic> is only slightly below the information of balanced learning, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2A</xref> (magenta vs. blue curve). However, imbalanced plasticity provides a large increase in memory efficiency <italic>S</italic>, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2B</xref>. The reason is that the learning dynamics converges to synaptic configurations with a considerably larger number of silent synapses, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2C</xref>. As the memory load <italic>α</italic> increases, the efficiency approaches that of the balanced solution. This is expected; by increasing the task difficulty we are imposing additional constraints on the synaptic weights. As a result the volume of the solution space shrinks and the constraint on the mean weight has to be relieved, therefore leading to smaller gains in memory efficiency. As <italic>α</italic> approaches its critical value, the space of solutions collapses to a single point, i.e., no additional constraints can be imposed at critical capacity and <italic>λ</italic><sub>max</sub> = 0 [<xref ref-type="bibr" rid="pcbi.1004265.ref007">7</xref>].</p>
<fig id="pcbi.1004265.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Information <italic>C</italic> in bits per synapse (bps), memory efficiency <italic>S</italic> in bits per functional synapse (bpfs) and the fraction of silent synapses 1−<italic>F</italic> as a function of the memory load <italic>α</italic> = <italic>K</italic>/<italic>N</italic>.</title>
<p>Results from a simulation with <italic>N</italic> = 1000 synapses. Shown are: balanced learning where depression equals potentiation (<italic>λ</italic> = 0); maximal imbalance learning; the maximal-information solution found with offline quadratic programming (QP); minimal-value synapse deletion, where all weights below some threshold are set to zero; and random pruning. The two latter rules were set to delete the same number of synapses as imbalanced learning. The results for online learning were obtained under a large threshold (<italic>θ</italic> = 1, learning rate <italic>ε</italic> = 1/<italic>N</italic>) to maximise information (see <xref ref-type="sec" rid="sec013">Methods</xref>). <bold>A</bold>. Information. Imbalanced plasticity leads to a small information decrease and significantly outperforms thresholded pruning. Random deletion performs very poorly. Truly maximising information (QP) gives only a slight improvement in performance. <bold>B</bold>. Memory efficiency (information per non-zero synapse). In particular at low <italic>α</italic>, the imbalanced perceptron finds sparser weight configurations, boosting the memory efficiency. The curves converge as the critical loading <italic>α</italic> = 1 is approached. The maximal information solution (QP) is more efficient than balanced learning, but still inferior to imbalanced learning. <bold>C</bold>. The fraction of silent synapses. Balanced online learning (<italic>λ</italic> = 0) under a large threshold always leads to the appearance of silent synapses, due to the imposed hard bound at zero together with the large firing threshold. Imbalanced plasticity significantly increases sparseness, especially at lower memory loads. QP learning leads to a few more zero-weight synapses compared to balanced learning, the fraction of which remains close to 50% irrespectively of the memory load.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g002"/>
</fig>
<p>We also considered alternative learning algorithms: first, a minimal-value pruning rule, where all weights below a certain threshold are set to zero after learning has converged. We set the deletion threshold of the offline pruning algorithm to produce the same number of zero-weight synapses as the imbalanced solution. This is optimal in the one-shot learning case [<xref ref-type="bibr" rid="pcbi.1004265.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref015">15</xref>]. In this case we find a more pronounced loss of information and, interestingly, almost no efficiency increase (dark green curve). The superiority of imbalancing makes intuitive sense: imbalanced plasticity is an online protocol that accommodates for sparseness constraints by redistributing weights dynamically, while the pruning procedure is performed after learning and does not allow for further re-adjustments. Finally, we also tried random pruning after learning, which as expected, performs very poorly (light green curve).</p>
<p>For completeness, we compared these results to the solution that maximises information without requiring sparseness. The optimisation can be formulated as a quadratic programming (QP) problem (see <xref ref-type="sec" rid="sec013">Methods</xref>), and the best solution can be found with a high performance barrier method convex optimiser [<xref ref-type="bibr" rid="pcbi.1004265.ref038">38</xref>]. This algorithm clearly lacks biological plausibility, and does not provide a significant improvement in information over balanced (<italic>λ</italic> = 0) online learning, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2A</xref>. In other words, perceptron learning works well for our problem, provided that the firing threshold <italic>θ</italic> is large enough (see <xref ref-type="sec" rid="sec013">Methods</xref>). Under QP the fraction of silent synapses slightly increases to around 50%, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2C</xref>, which leads to a moderate improvement in memory efficiency, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2B</xref>. Finally, one can resort to the min-over learning rule, which only applies a weight update for the pattern that evokes the minimal output <italic>h</italic> [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>]. The synaptic weights are guaranteed to asymptotically converge (as <italic>θ</italic> → ∞) to the QP solution and unsurprisingly the information matches that which is obtained with the quadratic solver. This procedure is difficult to reconcile with biology as well, as each single learning iteration requires access to every pattern.</p>
</sec>
<sec id="sec007">
<title>Synaptic weight distributions</title>
<p>The learning algorithm and the threshold setting also determine the shape of the synaptic weight distribution. This distribution is of importance, as it can be compared to experimental data. For instance, the electro-physiologically determined synaptic weight distribution was used to link Purkinje cell learning to perceptron learning theory [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref040">40</xref>]. We recorded the obtained synaptic weight histograms (see <xref ref-type="sec" rid="sec013">Methods</xref>), averaged over many trials (each with different pattern sets). While collecting results across trials is strictly only approximates the synaptic weight density, it is a good estimate of the actual observed distribution for a single realisation of the system, since the underlying weight density is strongly self-averaging [<xref ref-type="bibr" rid="pcbi.1004265.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>].</p>
<p>Balanced learning (<italic>λ</italic> = 0) leads to an approximately exponential distribution, <xref ref-type="fig" rid="pcbi.1004265.g003">Fig 3A</xref>. Interestingly, although the QP solution did not increase information compared to online balanced learning (<xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2A</xref>), the shape of the distribution of synaptic weights changes considerably (cf. Fig <xref ref-type="fig" rid="pcbi.1004265.g003">3A</xref> and <xref ref-type="fig" rid="pcbi.1004265.g003">3B</xref>). At any memory load <italic>α</italic> ≤ <italic>α</italic><sub>max</sub> the fraction of zero-weight synapses always remains close to 50% while the remaining weights assume a truncated Gaussian distribution centred around <italic>w</italic> = 0. The problem that we are dealing with is thus not ‘intrinsically sparse’ in weight space. This should be contrasted with the non-negative perce ptron classifier with 0/1-coded inputs that was recently studied [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref030">30</xref>]. In that case, maximising information in the presence of postsynaptic noise automatically leads to sparse weight configurations (<italic>F</italic> &lt; 0.5), provided that the memory load is below the critical point. Interestingly, at the critical load, the distribution becomes identical to the truncated Gaussian that we report here as the optimal one.</p>
<fig id="pcbi.1004265.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Synaptic weight histograms, information and memory efficiency at low memory load (<italic>α</italic> = 0.1).</title>
<p>Data obtained averaging over a thousand simulations (<italic>N</italic> = 1000). <bold>A</bold>. For balanced learning the distribution is stretched due to the optimised learning (large threshold choice <italic>θ</italic> = 1 under a small learning rate <italic>ε</italic> = 1/<italic>N</italic>). As with the non-negative perceptron classifier [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>], a large number of zero synapses appear. <bold>B</bold>. Maximal-information solution obtained via quadratic programming, with the objective set at minimising the Euclidean norm ∣∣<bold>w</bold>∣∣<sub>2</sub>. The quadratic objective function leads to a hemi-Gaussian weight distribution, again with a large fraction of silent synapses arising from the non-negativity constraint. <bold>C</bold>. Minimal linear norm solution (largest imbalance). As the learning task is ‘easy’ (low <italic>α</italic>), strong depression leads to a highly sparse synaptic configuration.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g003"/>
</fig>
<p>Imbalanced plasticity boosts the fraction of zero-weight synapses and stretches the weight distribution, <xref ref-type="fig" rid="pcbi.1004265.g003">Fig 3C</xref>. Although the mean weight is lower due to the increased sparseness of the weight configuration, the surviving synapses are stronger. This can be understood through theoretical arguments (see <xref ref-type="sec" rid="sec013">Methods</xref>). It can be shown that learning rules that lead to a large minimum postsynaptic sum, <inline-formula id="pcbi.1004265.e016"><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mtext mathvariant="normal">min</mml:mtext> <mml:mi>k</mml:mi></mml:msub> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> (together with a normalisation condition that fixes the Euclidean norm ∣∣<bold>w</bold>∣∣<sub>2</sub>) give better recognition performance against lures. As some synapses are zeroed-out, specific strengthening keeps the postsynaptic sum large for learned patterns.</p>
<p>The non-zero weight distribution for maximal imbalance can be reasonably fitted to a compressed exponential <italic>P</italic>(<italic>w</italic>) ∼ exp(−<italic>cw</italic><sup><italic>β</italic></sup>), with an exponent <italic>β</italic> = 1.4. The two-class perceptron model yields <italic>β</italic> = 2 (a truncated Gaussian) at critical capacity [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>]. The best fit of this type of distribution to the cerebellar data published [<xref ref-type="bibr" rid="pcbi.1004265.ref040">40</xref>] has an exponent <italic>β</italic> = 0.7±0.4, however it should be noted that the limited amount of data allows for a broad range of possible <italic>β</italic>.</p>
</sec>
<sec id="sec008">
<title>Homeostatic excitability regulation and sparse codes</title>
<p>Next we explore if our findings depend on the details of the coding. So far we assumed the inputs were -1 or +1, as in earlier studies of the non-negative perceptron [<xref ref-type="bibr" rid="pcbi.1004265.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref027">27</xref>]. This is hard to imagine biologically, unless an inhibitory partner neuron is introduced [<xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref042">42</xref>]. An arguably more faithful biological model is obtained by representing low inputs as silent, <italic>x</italic><sub><italic>i</italic></sub> = 0 [<xref ref-type="bibr" rid="pcbi.1004265.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref043">43</xref>]. Furthermore, we wish to generalise to a case where the probability for a high input is variable rather than fixed to 1/2.</p>
<p>The capacity of the above model can be fully recovered without drastically changing the neural circuit. In fact, two ingredients suffice: one has to rebalance the plasticity rules as a function of the activity level <italic>f</italic>, and, secondly, introduce a dynamic mechanism that adapts the firing threshold as a function of the linear norm ∣<bold>w</bold>∣. With these modifications, both the information <italic>C</italic> and the memory efficiency <italic>S</italic> are exactly identical to those reported in the previous section.</p>
<p>First, we generalise the model to deal with an arbitrary coding level <italic>f</italic>. When <italic>f</italic> = 1/2, the original model is recovered up to scale factors. To preserve the zero mean, we consider activity patterns that are coded as <italic>z</italic><sub><italic>i</italic></sub> ∈ {−<italic>f</italic>,1−<italic>f</italic>}, with P(<italic>z</italic><sub><italic>i</italic></sub> = 1−<italic>f</italic>) = <italic>f</italic>. Stochastic sub-gradient descent dynamics over the energy <xref ref-type="disp-formula" rid="pcbi.1004265.e009">Eq (5)</xref> gives the adjusted potentiation rule for high inputs
<disp-formula id="pcbi.1004265.e017"><alternatives><graphic id="pcbi.1004265.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e017"/><mml:math id="M17" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>+</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
while depression at low inputs becomes
<disp-formula id="pcbi.1004265.e018"><alternatives><graphic id="pcbi.1004265.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e018"/><mml:math id="M18" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>-</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
followed by rectification. Here <inline-formula id="pcbi.1004265.e019"><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p>
<p>Next, a zero-mean input <italic>z</italic><sub><italic>i</italic></sub> is related to 0/1 coding by the simple relation <italic>x</italic><sub><italic>i</italic></sub> = <italic>z</italic><sub><italic>i</italic></sub>+<italic>f</italic>, <italic>x</italic><sub><italic>i</italic></sub> ∈ {0,1}. Therefore the net input of the neuron in response to a 0/1 pattern can be written through a change of variables as
<disp-formula id="pcbi.1004265.e020"><alternatives><graphic id="pcbi.1004265.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where we defined a new threshold variable
<disp-formula id="pcbi.1004265.e021"><alternatives><graphic id="pcbi.1004265.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mi>γ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Note that this threshold grows during learning so as to compensate the increasing weights. This can be viewed as a kind of homeostatic adaptation process: as learning progresses, the neuron self-regulates so that it becomes harder to reach the firing threshold. While the incorporation of an auxiliary feed-forward inhibition circuit has been used in related models to increase capacity in the presence of non-negativity constraints [<xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref042">42</xref>], the mechanism here does not directly depend on the precise pattern <bold>x</bold> of the presented input. It thereby obviates the need for coordinated plasticity with a partner interneuron as well as for precise temporal integration of inhibitory signals. Instead it could be implemented sub-cellularly without the aid of additional circuitry. Using the adaptive threshold, the information becomes independent of the input coding level <italic>f</italic> (<xref ref-type="fig" rid="pcbi.1004265.g004">Fig 4</xref> solid line), while it decreases when the threshold is fixed (dashed curve). We note that, unlike for two-class learning, for one-class learning a high threshold suffices to implement a large-margin classifier.</p>
<fig id="pcbi.1004265.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Information <italic>C</italic> in bits per synapse for binary (0 or 1) input patterns as a function of the input coding level <italic>f</italic>.</title>
<p>Average values for the dynamic-threshold model, where <italic>h</italic> is given by <xref ref-type="disp-formula" rid="pcbi.1004265.e020">Eq 11</xref>, and average values obtained with a fixed threshold <italic>θfN</italic> — note the threshold scaling with <italic>fN</italic> instead of <inline-formula id="pcbi.1004265.e022"><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> due to the 0/1 input activity. Potentiation and depression were balanced (Eqs <xref ref-type="disp-formula" rid="pcbi.1004265.e017">9</xref> and <xref ref-type="disp-formula" rid="pcbi.1004265.e018">10</xref>) to match the coding level. While the adjusted model is insensitive to <italic>f</italic>, the information achieved by the uncorrected model approaches that of the original one for sparse input patterns. Simulations performed at moderate memory load <italic>α</italic> = 0.5 and system size <italic>N</italic> = 1000.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g004"/>
</fig>
<p>An alternative route to recover capacity is to employ sparse coding, a finding that has been previously reported for the non-negative perceptron in a more general classification framework [<xref ref-type="bibr" rid="pcbi.1004265.ref043">43</xref>]. Here the asymptotic situation is rather simple, because as <italic>f</italic> → 0 and <italic>N</italic> → ∞ the original model is recovered and performance at low <italic>f</italic> approaches the ideal performance, <xref ref-type="fig" rid="pcbi.1004265.g004">Fig 4</xref>.</p>
</sec>
<sec id="sec009">
<title>Input correlations</title>
<p>Activity correlations can severely limit the performance of learning rules, depending on the task and the nature of the correlations. For instance, in supervised memory tasks, Hebbian learning deteriorates under almost any type of correlation in the patterns [<xref ref-type="bibr" rid="pcbi.1004265.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref044">44</xref>]. In contrast, more powerful plasticity rules equipped with a stop-learning condition, like the perceptron rule, are resistant to spatial input correlations [<xref ref-type="bibr" rid="pcbi.1004265.ref045">45</xref>], and can in some cases take advantage of input-output redundancies to store more patterns [<xref ref-type="bibr" rid="pcbi.1004265.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref046">46</xref>].</p>
<p>To test the robustness of imbalanced plasticity to correlated activity we draw random patterns from a generative model that induces spatial presynaptic activity correlations (characterised by a parameter <italic>g</italic>, see <xref ref-type="sec" rid="sec013">Methods</xref>, [<xref ref-type="bibr" rid="pcbi.1004265.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref045">45</xref>]). We first correlated the patterns such that the mean activity remained homogeneous across the inputs. Consistent with the standard two-class perceptron without synaptic sign-constraints [<xref ref-type="bibr" rid="pcbi.1004265.ref045">45</xref>], neither the imbalanced learning, nor the balanced rule are affected by input correlation, <xref ref-type="fig" rid="pcbi.1004265.g005">Fig 5A</xref>.</p>
<fig id="pcbi.1004265.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Memory efficiency vs input correlations.</title>
<p><bold>A</bold>. In case the mean input remains homogeneous, the three learning algorithms considered — balanced (<italic>λ</italic> = 0), maximally-imbalanced (<italic>λ</italic> = <italic>λ</italic><sub>max</sub>) and maximal-information (QP) — are unaffected by spatial presynaptic activity correlations. <bold>B</bold>. In case of heterogenous inputs, the balanced rule (<italic>λ</italic> = 0) and the QP algorithm deteriorate. Imbalanced plasticity performs well, however, as it regularises the high-activity synapses while ignoring the remaining ones. As a result the memory efficiency of the maximally-imbalanced solution is approximately constant. Data obtained by averaging a hundred independent simulations at <italic>α</italic> = 0.1, <italic>f</italic> = 1/2, and <italic>N</italic> = 1000 synapses.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g005"/>
</fig>
<p>Next, we implemented a variation of the generative model that introduces heterogeneities in the input activity rates where some inputs tend to be active more often than others. Interestingly the imbalanced rule is robust to this type of correlation, <xref ref-type="fig" rid="pcbi.1004265.g005">Fig 5B</xref>. Whereas the efficiency of the other rules drops off, the efficiency of the imbalanced rule remains constant. The intuitive explanation is that the high activity synapses effectively experience balanced net potentiation and depression for non-zero imbalance <italic>λ</italic>. The imbalanced rule finds a high-information solution by silencing and ignoring the low activity inputs and subjecting the remaining synapses to the usual imbalanced learning protocol.</p>
</sec>
<sec id="sec010">
<title>Robustness to noise</title>
<p>So far we have considered the recall of noise-free patterns, however, in the light of the many noise sources in the nervous system, it is important to confirm the noise robustness of the results.</p>
<p>First, we introduce transmission failures and spontaneous presynaptic activity, and test the learning with corrupted patterns, denoted <inline-formula id="pcbi.1004265.e023"><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. An active input is switched off with probability <inline-formula id="pcbi.1004265.e024"><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mn>10</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mtext mathvariant="normal">P</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="-0.167em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.167em"/><mml:mn>0</mml:mn> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="-0.167em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.167em"/><mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, while an otherwise silent presynaptic input fires with probability <inline-formula id="pcbi.1004265.e025"><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mtext mathvariant="normal">P</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="-0.167em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.167em"/><mml:mn>1</mml:mn> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="-0.167em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.167em"/><mml:mn>0</mml:mn> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The lures are generated with a matching mean activity, ⟨<italic>x</italic>⟩ = (1−<italic>f</italic>)<italic>δ</italic><sub>01</sub>+<italic>f</italic>(1−<italic>δ</italic><sub>10</sub>), to ensure that lure statistics match the patterns.</p>
<p>We examined the performance of the balanced and maximally-imbalanced rules, as well as thresholded synaptic pruning, under this stochastic synapse model, Fig <xref ref-type="fig" rid="pcbi.1004265.g006">6A</xref> and <xref ref-type="fig" rid="pcbi.1004265.g006">6B</xref>. The information of all three rules decreases smoothly as the input distortion increases. For dense patterns, <italic>f</italic> = 1/2, the efficiency of the maximally-imbalanced rule is initially the most affected by the introduction of noise, and becomes comparable to the thresholded deletion one for higher noise levels. For sparse patterns, <xref ref-type="fig" rid="pcbi.1004265.g006">Fig 6B</xref>, the efficiency is affected similarly by the noise for all three rules. The maximally-imbalanced and the thresholded solutions remain more efficient than balanced plasticity.</p>
<fig id="pcbi.1004265.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Information <italic>C</italic> and memory efficiency <italic>S</italic> versus noise level.</title>
<p>The three solutions — balanced (<italic>λ</italic> = 0) and maximally-imbalanced (<italic>λ</italic> = <italic>λ</italic><sub>max</sub>) plasticity, and thresholded synaptic pruning — were obtained once for a single set of <italic>K</italic> = 0.1<italic>N</italic> positive patterns (<italic>N</italic> = 1000 synapses) and then tested against a large number 100<italic>K</italic> of distorted learned patterns and lures, generated for each noise level. The firing threshold of each solution is numerically optimised to maximise information. The presynaptic noise level varied under the setting <italic>δ</italic><sub>01</sub> = <italic>δ</italic><sub>10</sub> = <italic>δ</italic> (see main text for details). The scale of the postsynaptic noise standard deviation was set by normalising the weights to give a unit size mean response to learned patterns. <bold>A</bold>. For dense patterns, <italic>f</italic> = 1/2, the falloff in information is steeper for imbalanced plasticity than thresholded deletion. The two solutions remain more efficient than balanced learning for all noise levels. <bold>B</bold>. For sparse input patterns, <italic>f</italic> = 0.01, the balanced solution also suffers and as long as the information is not practically zero, both the maximally-imbalanced and the thresholded pruning rules are more efficient than the balanced one. <bold>C</bold>. Results for a postsynaptic noise model, where the current <italic>h</italic> is perturbed with an additive zero-mean Gaussian random variable with standard deviation <italic>σ</italic>. As the postsynaptic noise does not depend on the actual learned weights, imbalanced and balanced plasticity show similar noise robustness profiles.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g006"/>
</fig>
<p>Next, we examined the role of postsynaptic current noise by adding a zero-mean Gaussian variable to the postsynaptic current <italic>h</italic> [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>], the variance of which sets the noise intensity, <xref ref-type="fig" rid="pcbi.1004265.g006">Fig 6C</xref>. In contrast to the above, the magnitude of the random contributions is decoupled from the actual learned weights. For this noise model, the relative information reduction is comparable for both balanced and imbalanced plasticity.</p>
</sec>
<sec id="sec011">
<title>Tuning of the imbalance parameter</title>
<p>In the above the imbalance parameter <italic>λ</italic> was optimised for automatically in an unbiological fashion. To examine suboptimal values we simulated learning while raising <italic>λ</italic> towards the critical imbalance <italic>λ</italic><sub>max</sub>, above which the learning algorithm no longer converges. The memory task difficulty, set by the memory load <italic>α</italic>, limits the allowed imbalance (see <xref ref-type="sec" rid="sec013">Methods</xref>). Indeed, we find that <italic>λ</italic><sub>max</sub> shrinks as <italic>α</italic> increases, <xref ref-type="fig" rid="pcbi.1004265.g007">Fig 7</xref>. Akin to the margin parameter which sets the noise robustness of the non-negative perceptron [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref029">29</xref>], the actual <italic>λ</italic><sub>max</sub> depends on the exact set of patterns the neuron should learn. However, for random patterns drawn from the same distribution, the system is self-averaging as <italic>N</italic> → ∞ [<xref ref-type="bibr" rid="pcbi.1004265.ref007">7</xref>]. In simulations we observe a similar behaviour across different runs, although some finite-size effects are still apparent in networks of moderate dimension, <xref ref-type="fig" rid="pcbi.1004265.g007">Fig 7</xref> (rightmost curves). In other words, <italic>λ</italic><sub>max</sub> can be reasonably estimated independent of the precise pattern set. Finally note that the figure implies that the parameter can be set conservatively, based on the maximum number of patterns to be expected. Of course, the efficiency gain is not maximised in this case, but still better than the balanced case.</p>
<fig id="pcbi.1004265.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Memory efficiency <italic>S</italic> increases with imbalance <italic>λ</italic>.</title>
<p>Efficiency is a function of imbalance for a given set of patterns. The curve stops when the learning dynamics no longer converges. Dashed horizontal lines indicate the corresponding efficiency values achieved by the linear programming solver (see <xref ref-type="sec" rid="sec013">Methods</xref>). The results for five independent runs at <italic>α</italic> = 0.1 (rightmost curves) are very similar, although finite-size effects are visible as the number of inputs is not particularly large. As predicted, the critical imbalance <italic>λ</italic><sub>max</sub> decreases with the memory load <italic>α</italic>. The learning rule only updated the synapses for patterns that did not yet lead to firing activity, <xref ref-type="disp-formula" rid="pcbi.1004265.e007">Eq 3</xref>. Simulations of a neuron with <italic>N</italic> = 1000 synapses and coding level <italic>f</italic> = 0.01.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g007"/>
</fig>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>The brain’s energy consumption is thought to be dominated by synaptic transmission [<xref ref-type="bibr" rid="pcbi.1004265.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref048">48</xref>]. We have considered how synaptic learning rules can lead to sparse connectivity and thus to energy efficient computation. We studied a one-class perceptron problem in which a neuron learns from positive examples only. One-class learning is relevant for learning paradigms such as recognition and reinforcement learning. One-class learning is also well-known in machine learning [<xref ref-type="bibr" rid="pcbi.1004265.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref050">50</xref>]. The two-class perceptron requires sampling the space of ‘negative’ patterns that is necessarily large under a sparse firing constraint [<xref ref-type="bibr" rid="pcbi.1004265.ref022">22</xref>] and secondly, it requires reversing plasticity (‘unlearning’) whenever appropriate. For instance, it is unclear how can a pattern be actively unlearned under spike-timing-dependent plasticity [<xref ref-type="bibr" rid="pcbi.1004265.ref051">51</xref>]. In contrast to two-class perceptrons, negative samples in the one-class perceptron do not cause plasticity which leads to further energy saving as plasticity itself is an energetically costly process [<xref ref-type="bibr" rid="pcbi.1004265.ref052">52</xref>].</p>
<p>We imbalance potentiation and depression to achieve sparse connectivity. In other memory tasks, the information loss can be substantial for imbalanced plasticity; for instance, postsynaptic-independent (i.e., without a stop-learning mechanism) online learning rules are severely affected when depression does not match potentiation [<xref ref-type="bibr" rid="pcbi.1004265.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>]. However, here imbalance leads to a substantial energy reduction in storage as long as the task is below maximal capacity. Furthermore, it is robust against noise and correlated patterns. Imbalanced plasticity is not only a local and biophysically plausible mechanism, but it is also theoretically well-grounded, as it implements <italic>L</italic><sub>1</sub>-norm regularisation, which is well-known to induce sparseness [<xref ref-type="bibr" rid="pcbi.1004265.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref053">53</xref>]. Due to the biased drift towards zero in the learning rule, the probability of finding silent synapses is increased. Our learning rule reaches high information using a novel, biologically-plausible adaptive threshold without the need for an inhibitory partner neuron. The learning rule is unlike a previous approach to achieve sparse connectivity in which a pruning procedure removes the weakest synapses after learning [<xref ref-type="bibr" rid="pcbi.1004265.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref015">15</xref>]. Such strategy can lead to as much weight sparseness as desired, but a significant drop in information and efficiency occurs.</p>
<p>Despite the large efficiency gain found, it should be noted that imbalanced plasticity probably does not maximise the efficiency fully. In the limit of many synapses the replica technique from statistical mechanics can provide an estimate on the minimal number of synapses required for a given performance. Extrapolation of such an analysis of the traditional perceptron without sign constraints [<xref ref-type="bibr" rid="pcbi.1004265.ref010">10</xref>], suggests that even more efficient solutions exist, although it is unclear how to obtain them via online learning. Unfortunately, the weight configuration that truly maximises memory efficiency requires resorting to an impractical and unbiological exhaustive search method, with a search time exponential in the number of synapses. A feasible alternative is to use greedy <italic>L</italic><sub>0</sub>-norm minimisation methods [<xref ref-type="bibr" rid="pcbi.1004265.ref054">54</xref>], that are in general not guaranteed to achieve the theoretical limiting weight sparseness. Preliminary simulations suggest that the efficiency in this case is not substantially higher than when minimising the linear norm, as the increased number of zero-weight synapses is offset by a steep loss in information.</p>
<p>We note that sparse network connectivity can arise even when energy efficiency is not explicitly optimised for. Weight sparseness also emerges when maximising the information output of a sign-constrained classifier that is required to operate in the presence of postsynaptic noise [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref030">30</xref>]. The reported weight distribution displays a large fraction of silent synapses [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>]. In that learning setup, depression occurs for negative examples to drive the postsynaptic potential well below threshold and thus ensures that the activity of the neuron is suppressed even if noise is present.</p>
<p>In order to implement imbalanced learning various ingredients are needed. 1) As in the classical perceptron a stop-learning condition needs to be implemented. While in the cerebellum the complex spike might fulfil this role, neuromodulatory systems have also been suggested [<xref ref-type="bibr" rid="pcbi.1004265.ref031">31</xref>]. 2) The balance parameter needs to be precisely set to obtain the most efficient solution and its value depends on the task to be learned. A conservative imbalance setting will increase efficiency, but not as much. We note that the need for precisely tuned parameters is common in this type of studies, just like the standard perceptron requires a precise balance between potentiation and depression, which is also not trivially achieved biologically. 3) For one-class learning, plasticity only occurs when the neural output should be high but it is not (which contrasts the model in [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>], where plasticity only occurs when the input is high). A separate supervisory input to the neuron could achieve this. Nevertheless, despite the details of this particular study the general imbalancing principle could well carry over to other systems. In particular including precise spike-timing perceptron learning [<xref ref-type="bibr" rid="pcbi.1004265.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref056">56</xref>], or temporal STDP [<xref ref-type="bibr" rid="pcbi.1004265.ref057">57</xref>]. In the latter case, interestingly, energy constraints have also been used to define unsupervised learning rules.</p>
<p>Our study is agnostic about the precise mechanism of pruning. There is a number of biophysical ways a synapse can be inactivated [<xref ref-type="bibr" rid="pcbi.1004265.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref059">59</xref>]: 1) The presynaptic neuron releases neurotransmitter, but no receptors are present (postsynaptically silent synapse). 2) Alternatively, presynaptic release is turned off (mute synapses). Finally, 3) the synapse is anatomically pruned and thus absent altogether (although it could be recruited again [<xref ref-type="bibr" rid="pcbi.1004265.ref060">60</xref>]). The first and second would presumably allow the system to rapidly re-recruit the synapse, while the third option not only saves energy, but also reduces anatomical wiring length and volume.</p>
<p>It is worthwhile to ask if our model is consistent with neuroscience data. Naively, one might think that imbalance would predict that LTD would be stronger than LTP, which would contradict typical experimental findings. However, for sparse patterns LTD has to be weakened to prevent saturation, so that the imbalance condition becomes <italic>f</italic> ⋅ LTP &lt; (1−<italic>f</italic>) ⋅ LTD. It is unclear whether this condition is fulfilled in biology. Next, one could expect that the theory would predict a net decrease of synaptic strength during learning. However, this is not the case: after all, in the simulations all weights are zero initially, so that synaptic weights can only grow during learning. The reason for this apparent paradox is that learning is gated, unlike unsupervised learning, so the number of LTP and LTD events on a synapse does not necessarily match. While our findings also hold when we start from random weights, there is no obvious initial value for biological synaptic weights.</p>
<p>Finally, one can compare the resulting weight distributions and the number of silent synapses to the data. An advantage of the cerebellum is that also the fraction of zero-weight synapses is known, which is not the case for other brain regions. The weight distribution in the cerebellum matches theory very well when the capacity of a two-class perceptron is maximised in the presence of noise. The fraction of silent synapses exhibits a strong dependence on the required noise tolerance; it is significantly decreased in the low noise limit [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>]. Our current model finds a similar distribution from a very different objective function, namely minimising the energy of a one-class perceptron. Which of these two is the appropriate objective for the cerebellum or other brain regions remains a question for future research.</p>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Criteria for optimising information</title>
<p>Provided that the memory problem is realisable, perceptron learning ensures that each of the <italic>K</italic> patterns leads to postsynaptic firing activity (<italic>h</italic> ≥ 0), i.e., the FN error probability is zero, <italic>p</italic><sub>10</sub> = 0. In this case the information increases as the FP error probability <italic>p</italic><sub>01</sub> decreases (see main text, <xref ref-type="disp-formula" rid="pcbi.1004265.e014">Eq 7</xref>). With the additional assumption that the lures are uncorrelated to the learned patterns, it can be shown that our perceptron learning rule leads to a decrease of the FP error.</p>
<p>To see why, we write <italic>p</italic><sub>01</sub> as a function of the learned synaptic weights. As the lure patterns are uncorrelated to the learned ones, each input <italic>x</italic><sub><italic>i</italic></sub> will be uncorrelated to its corresponding weight <italic>w</italic><sub><italic>i</italic></sub>. The total synaptic current is given by a sum of many terms. Assuming that there are sufficient non-zero weights, the probability distribution <italic>p</italic>(<italic>h</italic><sub><italic>l</italic></sub>) of the net input <italic>h</italic><sub><italic>l</italic></sub> in response to a lure is Gaussian, <inline-formula id="pcbi.1004265.e026"><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mo>𝓝</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:mo stretchy="false">⟨</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>,</mml:mo> <mml:mo stretchy="false">⟨</mml:mo> <mml:mi>δ</mml:mi> <mml:msubsup><mml:mi>h</mml:mi> <mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Under this approximation,
<disp-formula id="pcbi.1004265.e027"><alternatives><graphic id="pcbi.1004265.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mn>01</mml:mn></mml:msub> <mml:mo>≈</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi mathvariant="normal">d</mml:mi> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mspace width="0.222222em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>erfc</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>⟩</mml:mo></mml:mrow> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mo>⟨</mml:mo> <mml:mi>δ</mml:mi> <mml:msubsup><mml:mi>h</mml:mi> <mml:mrow><mml:mi>l</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1004265.e028"><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mtext mathvariant="normal">erfc</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mfrac> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup> <mml:mtext mathvariant="normal">d</mml:mtext> <mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is the complementary error function. The approximation improves as <italic>N</italic> → ∞, as the fraction of non-zero synapses <italic>F</italic> remains finite irrespective of the imbalance λ (for <italic>λ</italic> ≤ <italic>λ</italic><sub>max</sub>) and as long as the memory load <italic>α</italic> does not vanish [<xref ref-type="bibr" rid="pcbi.1004265.ref010">10</xref>].</p>
<p>As the inputs are in zero-mean bipolar form, ⟨<italic>x</italic>⟩ = 0, ⟨<italic>x</italic><sup>2</sup>⟩ = 1. The mean current elicited by lures is just <inline-formula id="pcbi.1004265.e029"><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mo stretchy="false">⟨</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo stretchy="false">⟨</mml:mo> <mml:mi>w</mml:mi> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, independent of the weights. The variance in the current
<disp-formula id="pcbi.1004265.e030"><alternatives><graphic id="pcbi.1004265.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>δ</mml:mi> <mml:msubsup><mml:mi>h</mml:mi> <mml:mrow><mml:mi>l</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>⟨</mml:mo> <mml:mi>x</mml:mi> <mml:mo>⟩</mml:mo> <mml:mo>⟨</mml:mo> <mml:mi>w</mml:mi> <mml:mo>⟩</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
is proportional to the second raw moment <inline-formula id="pcbi.1004265.e031"><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mn>0</mml:mn> <mml:mo>∞</mml:mo></mml:msubsup> <mml:mtext mathvariant="normal">d</mml:mtext> <mml:mi>w</mml:mi> <mml:mspace width="0.167em"/><mml:mi>p</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>w</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> of the weight distribution. For a particular realisation of the system one has <inline-formula id="pcbi.1004265.e032"><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi> <mml:mo stretchy="false">⟨</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>=</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, the squared Euclidean norm of the synaptic weight vector. This is illustrated in <xref ref-type="fig" rid="pcbi.1004265.g008">Fig 8</xref>. The information of the system is thus given by the Euclidean norm of the weight vector alone. This is true for the learned-vs-lure discrimination task as long as the Gaussianity of the lure current <italic>h</italic><sub><italic>l</italic></sub> holds, irrespective of the particular learning rule that is employed. For instance, <italic>p</italic><sub>01</sub> takes the same form for postsynaptic-independent learning [<xref ref-type="bibr" rid="pcbi.1004265.ref019">19</xref>] or for rate-coded inputs that are learned via the offline pseudo-inverse rule [<xref ref-type="bibr" rid="pcbi.1004265.ref022">22</xref>].</p>
<fig id="pcbi.1004265.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004265.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Schematic illustration of the postsynaptic current distributions.</title>
<p>In the large <italic>N</italic> limit, the postsynaptic current elicited by lures (dashed line) is well described by a zero-mean Gaussian, whose variance <inline-formula id="pcbi.1004265.e033"><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo> <mml:mi>δ</mml:mi> <mml:msubsup><mml:mi>h</mml:mi> <mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is determined by the squared Euclidean norm <inline-formula id="pcbi.1004265.e034"><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> of the weight vector. Assuming that the learning dynamics converged, the postsynaptic current distribution provided that the input pattern is a learned one (solid line) is characteristic of perceptron learning: a significant number of patterns lie on the decision boundary and thus provoke a current that is exactly at the firing threshold, while the remaining ones generate super-threshold Gaussian tail currents [<xref ref-type="bibr" rid="pcbi.1004265.ref028">28</xref>]. The integral of the shaded area gives the FP probability <italic>p</italic><sub>01</sub>, which depends on the variance of the lure current distribution.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004265.g008"/>
</fig>
<p>Thus, the perceptron with the most information satisfies the firing condition <italic>h</italic> ≥ 0 for every learned pattern, but has a minimal Euclidean length weight vector. This coincides exactly with the traditional perceptron that is optimal with respect to the maximal-stability criterion [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>], that prescribes the weight configuration with largest stability <inline-formula id="pcbi.1004265.e035"><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>≡</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>/</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:msub><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. This is a widely used principle that enlarges the basins of attraction in recurrent networks and improves the ability to generalise in classifiers [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004265.ref061">61</xref>]. Notice that for a fixed threshold, increasing Δ can only increase information, as it is inversely proportional to the Euclidean weight vector length. Information maximisation thus reveals an interesting close link between recognition memory and the more usual two-class learning problems.</p>
<p>Furthermore, at least for random patterns, we can expect the perceptron learning rule to perform well. Below the critical load <italic>α</italic><sub>max</sub> the algorithm is known to converge to solutions with large Δ [<xref ref-type="bibr" rid="pcbi.1004265.ref062">62</xref>]. In other words, although the learning dynamics is not guaranteed to maximise information, it should achieve high <italic>C</italic> in the recognition task. As shown in the main text, <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2</xref>, the improvement indeed is minimal when the full quadratic program is actually solved.</p>
<p>The crucial condition that must be observed to achieve good performance is that the firing threshold <italic>θ</italic> should be large. Here <italic>θ</italic> plays the role of an indirect (unnormalised) stability parameter. It can be shown [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>] that raising <italic>θ</italic> will indirectly lead to solutions with larger Δ. Lower bounds on how close the learning rule gets to maximal stability with a certain setting of <italic>θ</italic> and <italic>a</italic>, <italic>b</italic> can be extracted from the perceptron convergence proof [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>].</p>
<p>Note that the above reasoning requires zero-mean inputs and balanced plasticity. For 0 or 1 inputs, the distribution of the unthresholded output <italic>h</italic><sub><italic>l</italic></sub> that is obtained in response to lures is still well characterised by a Gaussian, as an uncorrelated input pattern gives a sum over on average <italic>fN</italic> randomly selected weights. The expressions for the mean ⟨<italic>h</italic><sub><italic>l</italic></sub>⟩ and the variance <inline-formula id="pcbi.1004265.e036"><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo> <mml:mi>δ</mml:mi> <mml:msubsup><mml:mi>h</mml:mi> <mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> now include terms that depend on first- and second-order moments of the synaptic weight distribution. For a particular realisation of the random system the mean is <inline-formula id="pcbi.1004265.e037"><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mi>N</mml:mi> <mml:mo stretchy="false">⟨</mml:mo> <mml:mi>w</mml:mi> <mml:mo stretchy="false">⟩</mml:mo> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>, and the variance <inline-formula id="pcbi.1004265.e038"><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mo>〈</mml:mo><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow> <mml:mi>f</mml:mi><mml:mo>〈</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msup> <mml:mo>〉</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow> <mml:mo>〈</mml:mo><mml:mi>w</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>‖</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:msubsup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>N</mml:mi><mml:mrow> <mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:msup><mml:mo>∣</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Thus, when the inputs are in 0 or 1 form, the information per synapse <italic>C</italic> is no longer a simple function of the squared Euclidean norm as before. The output error probability <italic>p</italic><sub>01</sub>, and therefore the information, is affected by the coding level <italic>f</italic> and the linear norm ∣<bold>w</bold>∣ as well.</p>
</sec>
<sec id="sec015">
<title>Imbalanced plasticity affects convergence of the learning dynamics</title>
<p>To gain further insight on the effects of allowing a depression-potentiation imbalance, we prove the convergence of perceptron learning rule <xref ref-type="disp-formula" rid="pcbi.1004265.e007">Eq 3</xref> for non-zero <italic>λ</italic>, a variation of the detailed proof given by [<xref ref-type="bibr" rid="pcbi.1004265.ref029">29</xref>]. Besides the inclusion of the parameter <italic>λ</italic>, differences arise because our inputs are in bipolar form and because all patterns should elicit a high output.</p>
<p>We study a problem that can provably be solved in a finite number of learning steps by balanced postsynaptic-dependent learning (<italic>λ</italic> = 0). Therefore we can assume the existence of a weight configuration <bold>w</bold>* that solves the recognition task
<disp-formula id="pcbi.1004265.e039"><alternatives><graphic id="pcbi.1004265.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e039"/><mml:math id="M39" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>κ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mspace width="2.em"/><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>K</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
while simultaneously satisfying the <italic>N</italic> non-negativity constraints <inline-formula id="pcbi.1004265.e040"><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <italic>i</italic> = 1, …, <italic>N</italic>. The variable <italic>κ</italic> ≥ 0 relates the threshold <inline-formula id="pcbi.1004265.e041"><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>κ</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> of the solution to the threshold <inline-formula id="pcbi.1004265.e042"><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> that is used in the learning algorithm.</p>
<p>We assume that initially all synapses are silent, i.e., we start from the <italic>tabula rasa</italic> condition <italic>w</italic><sub><italic>i</italic></sub> = 0, <italic>i</italic> = 1, …, <italic>N</italic>. Learning proceeds by presenting patterns in random order. Since plasticity only occurs when the postsynaptic current <inline-formula id="pcbi.1004265.e043"><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> is not large enough to activate the perceptron, we index time with <italic>m</italic> = 1, …, <italic>M</italic>, <italic>m</italic> being incremented only when <italic>h</italic> &lt; 0. Whenever each synapse <italic>w</italic><sub><italic>i</italic></sub> changes, it does so according to the update, <xref ref-type="disp-formula" rid="pcbi.1004265.e007">Eq 3</xref> <disp-formula id="pcbi.1004265.e044"><alternatives><graphic id="pcbi.1004265.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e044"/><mml:math id="M44" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo movablelimits="true" form="prefix">max</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>ϵ</mml:mi> <mml:msub><mml:mi>η</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where <italic>η</italic><sub><italic>i</italic></sub>(<italic>m</italic>) = <italic>x</italic><sub><italic>i</italic></sub>(<italic>m</italic>)−<italic>λ</italic> is the weight update before rectification and <bold>x</bold>(<italic>m</italic>) ∈ {<bold>x</bold><sup>1</sup>, …, <bold>x</bold><sup><italic>K</italic></sup>} is the pattern that led to the update at time <italic>m</italic>.</p>
<p>The analysis is carried out by tracking the quantity
<disp-formula id="pcbi.1004265.e045"><alternatives><graphic id="pcbi.1004265.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e045"/><mml:math id="M45" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:msub><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
over time. If we find that after a finite number of updates <italic>a</italic>(<italic>m</italic>) would become larger than one, then the learning process is convergent, as the Cauchy-Schwarz inequality implies that <italic>a</italic>(<italic>m</italic>) ≤ 1. To monitor the time evolution of <italic>a</italic>(<italic>m</italic>) we bound the scalar product <bold>w</bold>* ⋅ <bold>w</bold>(<italic>m</italic>) from below and the norm ∣∣<bold>w</bold>(<italic>m</italic>)∣∣<sub>2</sub> from above.</p>
<p>After one update, the change Δ(<bold>w</bold>* ⋅ <bold>w</bold>(<italic>m</italic>)) ≡ <bold>w</bold>* ⋅ <bold>w</bold>(<italic>m</italic>+1)−<bold>w</bold>* ⋅ <bold>w</bold>(<italic>m</italic>) in the scalar product is
<disp-formula id="pcbi.1004265.e046"><alternatives><graphic id="pcbi.1004265.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>&gt;</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>κ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>B</italic>(<italic>m</italic>) = {<italic>i</italic>: <italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>) &lt; <italic>ε</italic>+<italic>ελ</italic> ∧ <italic>x</italic><sub><italic>i</italic></sub>(<italic>m</italic>) = −1, <italic>i</italic> = 1, …, <italic>N</italic>} is the set of all synapses that are set to zero due to the lower bound. Note that the lower bound can only be triggered by depression, which in turn can only occur for low inputs. The inequality is obtained by plugging in the definition <xref ref-type="disp-formula" rid="pcbi.1004265.e039">Eq (14)</xref> of <bold>w</bold>*.</p>
<p>A bound on the scalar product <bold>w</bold>* ⋅ <bold>w</bold>(<italic>m</italic>) itself after <italic>m</italic> such updates can then be obtained by iteratively applying <xref ref-type="disp-formula" rid="pcbi.1004265.e046">Eq (17)</xref>:
<disp-formula id="pcbi.1004265.e047"><alternatives><graphic id="pcbi.1004265.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>κ</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>λ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>Meanwhile, the change <inline-formula id="pcbi.1004265.e048"><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>≡</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>−</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> in the squared norm of <bold>w</bold>(<italic>m</italic>) after one step can be obtained by expanding the square <inline-formula id="pcbi.1004265.e049"><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, so that
<disp-formula id="pcbi.1004265.e050"><alternatives><graphic id="pcbi.1004265.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e050"/><mml:math id="M50" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
We have Δ<italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>) ∈ {<italic>εη</italic><sub><italic>i</italic></sub>(<italic>m</italic>), −<italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>)}, with <italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>) &lt; <italic>ε</italic>+<italic>ελ</italic>, as Δ<italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>) = −<italic>w</italic><sub><italic>i</italic></sub>(<italic>m</italic>) only for <italic>i</italic> ∈ <italic>B</italic>(<italic>m</italic>). Thus, the squared norm of the update is dominated by the terms that come from low inputs at synapses that do not cross the lower bound. This gives the inequality
<disp-formula id="pcbi.1004265.e051"><alternatives><graphic id="pcbi.1004265.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e051"/><mml:math id="M51" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:msup><mml:mi>ϵ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
where <inline-formula id="pcbi.1004265.e052"><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi> <mml:mo>≡</mml:mo> <mml:msub><mml:mi>max</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>N</mml:mi> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the maximum fraction of low inputs observed across the <italic>K</italic> patterns.</p>
<p>The scalar product is expanded as before:
<disp-formula id="pcbi.1004265.e053"><alternatives><graphic id="pcbi.1004265.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>·</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi>η</mml:mi> <mml:mi>m</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>&lt;</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
Note that the update condition <italic>h</italic> &lt; 0 is always satisfied at time <italic>m</italic>, so that <inline-formula id="pcbi.1004265.e054"><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mi>ε</mml:mi> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>⋅</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>&lt;</mml:mo> <mml:mi>ε</mml:mi> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. Together with the bound <xref ref-type="disp-formula" rid="pcbi.1004265.e051">Eq (20)</xref>, iterating over <xref ref-type="disp-formula" rid="pcbi.1004265.e050">Eq (19)</xref> gives
<disp-formula id="pcbi.1004265.e055"><alternatives><graphic id="pcbi.1004265.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e055"/><mml:math id="M55" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>&lt;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>θ</mml:mi></mml:mrow> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula> <disp-formula id="pcbi.1004265.e056"><alternatives><graphic id="pcbi.1004265.e056g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e056"/><mml:math id="M56" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&lt;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>θ</mml:mi></mml:mrow> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>q</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
The last inequality is obtained by noticing that <italic>w</italic><sub><italic>i</italic></sub>(<italic>l</italic>) &lt; <italic>ε</italic>+<italic>ελ</italic> inside the sum over <italic>l</italic>; the factor <italic>q</italic> arises from the iteration over the <italic>N</italic> synapses, conditioning on the low inputs. The bound <xref ref-type="disp-formula" rid="pcbi.1004265.e056">Eq (23)</xref> implies that as learning proceeds ∣∣<bold>w</bold>(<italic>m</italic>)∣∣<sub>2</sub> cannot grow faster than <inline-formula id="pcbi.1004265.e057"><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>m</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p>
<p>From <xref ref-type="disp-formula" rid="pcbi.1004265.e055">Eq (22)</xref> we collect
<disp-formula id="pcbi.1004265.e058"><alternatives><graphic id="pcbi.1004265.e058g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e058"/><mml:math id="M58" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>&gt;</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mi>ϵ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
Turning back to <xref ref-type="disp-formula" rid="pcbi.1004265.e047">Eq (18)</xref> and using the previous result <xref ref-type="disp-formula" rid="pcbi.1004265.e058">Eq (24)</xref> yields
<disp-formula id="pcbi.1004265.e059"><alternatives><graphic id="pcbi.1004265.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mo>&gt;</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mi>κ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>λ</mml:mi> <mml:mi>N</mml:mi></mml:mfrac> <mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2.em"/><mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:munder> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>λ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>&gt;</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mi>m</mml:mi> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mi>κ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>λ</mml:mi> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>λ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>λ</mml:mi> <mml:mi>N</mml:mi></mml:mfrac> <mml:mrow><mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold">w</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>q</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula>
The last inequality stems from <italic>w</italic><sub><italic>i</italic></sub>(<italic>l</italic>) &lt; <italic>ε</italic>+<italic>ελ</italic>. The first bracketed factor is always larger than −(<italic>ε</italic>+<italic>ελ</italic>), while the second one is bounded from above by <italic>ε</italic>+<italic>ελ</italic>. Iterating over the constrained sum introduces the factor <italic>Nq</italic> as before.</p>
<p>We now have a bound for the cosine <italic>a</italic>(<italic>m</italic>). Substituting in Eqs (<xref ref-type="disp-formula" rid="pcbi.1004265.e056">23</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004265.e059">25</xref>) gives
<disp-formula id="pcbi.1004265.e060"><alternatives><graphic id="pcbi.1004265.e060g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004265.e060"/><mml:math id="M60" display="block" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:mi>ϵ</mml:mi><mml:mi>m</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>κ</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>λ</mml:mi><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mi>ϵ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>θ</mml:mi><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>λ</mml:mi><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>q</mml:mi><mml:mi>ϵ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
Note that while the neural parameters {<italic>ε</italic>, <italic>θ</italic>, <italic>λ</italic>} can be set at will, for a certain task the solution margin <italic>κ</italic> and the norms are constrained by the existence of a vector <bold>w</bold>* that can satisfy the learning conditions. Thus, they cannot be varied arbitrarily. In fact, if one keeps ∣∣<bold>w</bold>*∣∣<sub>2</sub> fixed, it will only be possible to increase <italic>κ</italic> up to a certain point, where we will have found the maximally-stable configuration. Similarly, the linear norm ∣<bold>w</bold>*∣ will have a minimum value. Furthermore, in general it is not possible to achieve simultaneously minimal ∣<bold>w</bold>*∣ and maximal <italic>κ</italic> with a single configuration.</p>
<p>From <xref ref-type="disp-formula" rid="pcbi.1004265.e060">Eq (26)</xref> a number of conclusions can be drawn. The straightforward condition for convergence is to check whether that bound becomes larger than one. Another way to show that the learning algorithm stops is to check if <italic>a</italic>(<italic>m</italic>) is a monotonically increasing function of <italic>m</italic>. When <italic>λ</italic> = 0, the process is convergent, as long as <inline-formula id="pcbi.1004265.e061"><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mi>ε</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>2</mml:mn> <mml:mi>κ</mml:mi> <mml:mo>/</mml:mo> <mml:mo stretchy="false">[</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mn>2</mml:mn> <mml:mi>q</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. For <italic>λ</italic> &gt; 0, the crucial observation is that we can only show that learning converges if <italic>κ</italic> can be raised so as to compensate for the negative terms in the numerator.</p>
<p>Thus, as expected, we find that the imbalance <italic>λ</italic> is related to the linear norm of the solution vector (one can increase <italic>λ</italic> as ∣<bold>w</bold>*∣ can be made smaller), and to the occurrence of depression events (through <italic>q</italic>). But more importantly, <italic>λ</italic><sub>max</sub> writes directly as a function of <italic>κ</italic> as well, which here sets the task difficulty, since the maximal value for <italic>κ</italic> shrinks as the memory load <italic>α</italic> increases. What is more, the minimum of ∣<bold>w</bold>*∣ depends itself on <italic>α</italic>. This theoretical prediction is confirmed by our numerical work. As <italic>α</italic> increases, the achievable imbalance <italic>λ</italic><sub>max</sub> becomes closer to zero, and the fraction of silent synapses approaches that which is obtained with balanced (<italic>λ</italic> = 0) learning, cf. <xref ref-type="fig" rid="pcbi.1004265.g002">Fig 2C</xref>.</p>
</sec>
<sec id="sec016">
<title>Generating correlated patterns</title>
<p>We generate correlated patterns following previous work in recognition memory [<xref ref-type="bibr" rid="pcbi.1004265.ref021">21</xref>]. In the first model we generate a template pattern <inline-formula id="pcbi.1004265.e062"><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with each input <inline-formula id="pcbi.1004265.e063"><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> being set high (+1) or low (-1) independently and with equal probability 1/2. To maintain balance we also use its negative, <inline-formula id="pcbi.1004265.e064"><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow> <mml:mo>−</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo>−</mml:mo> <mml:mover accent="true"><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, as a template.</p>
<p>The <italic>K</italic> patterns the neuron should learn are generated conditioned on either template, such that <inline-formula id="pcbi.1004265.e065"><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mtext mathvariant="normal">P</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>g</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. Lure patterns follow the statistics of the learned patterns and are produced from the same templates. The parameter <italic>g</italic> controls the level of input correlations; with the choice <italic>g</italic> = 0 the original statistics are recovered, while at <italic>g</italic> = 1 the recognition task is impossible, as all patterns are perfect copies or reversals of one another.</p>
<p>In the second model patterns generated according to the process described above, but only using a single template. This procedure introduces inter-pattern correlations at the same presynaptic site <italic>x</italic><sub><italic>i</italic></sub>, as the arriving patterns become more similar to one another. It also leads to heterogeneous mean activity levels across neurons; although the mean number of active presynaptic neurons per pattern remains 1/2, increasing <italic>g</italic> leads to a bimodal presynaptic firing distribution. For <italic>g</italic> &gt; 0, neurons that are active in the template fire more often and, conversely, the remaining neurons fire less frequently.</p>
</sec>
<sec id="sec017">
<title>Computer simulations</title>
<p>All our computer simulations were implemented on Matlab R2013a (MathWorks) and were performed on a standard desktop computer. We simulated a single postsynaptic neuron that was driven by <italic>N</italic> = 1000 presynaptic random inputs. We varied the memory load parameter within the range <italic>α</italic> ∈ [0.1,0.8] to avoid both the appearance of unsolvable problem instances and excessive simulation time. We chose a small learning rate <italic>ε</italic> = 1/<italic>N</italic> and a sufficiently large firing threshold at <inline-formula id="pcbi.1004265.e066"><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (i.e., <italic>θ</italic> = 1) except when otherwise noted. The threshold was set so that typically no increase in information could be obtained by raising it further. In the figures we included second-degree polynomial fits to average values.</p>
<p>The online perceptron learning rule was iterated until all patterns were learned. To obtain the maximally-imbalanced solution (<italic>λ</italic> = <italic>λ</italic><sub>max</sub>) we minimised the linear norm ∣<bold>w</bold>∣ using a linear programming algorithm [<xref ref-type="bibr" rid="pcbi.1004265.ref038">38</xref>], subject to the set of inequality constraints that ensured that every pattern would lead the neuron to fire. Specifically, using Matlab’s interior-point solver, available via the <monospace>linprog</monospace> command (Optimization Toolbox), we minimised ∣<bold>w</bold>∣ subject to <italic>N</italic> non-negativity constraints <italic>w</italic><sub><italic>i</italic></sub> ≥ 0 and <italic>K</italic> linear pattern imprinting constraints specified in matrix form as <inline-formula id="pcbi.1004265.e067"><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mtext mathvariant="bold">X</mml:mtext> <mml:mo>⊤</mml:mo></mml:msup> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo>≥</mml:mo> <mml:mi>θ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mtext mathvariant="bold">1</mml:mtext></mml:mrow></mml:math></inline-formula>, where <bold>X</bold><sup>⊤</sup> is the <italic>K</italic> × <italic>N</italic> design matrix whose rows are the positive examples.</p>
<p>For the balanced case, the maximum-information weight configurations were obtained using the Krauth-Mézard min-over algorithm [<xref ref-type="bibr" rid="pcbi.1004265.ref039">39</xref>], followed by rectification after each learning step in order to enforce the non-negativity synaptic constraints. This is a batch learning algorithm that employs the balanced rule (<xref ref-type="disp-formula" rid="pcbi.1004265.e007">Eq 3</xref>, <italic>λ</italic> = 0). At each step the pattern <inline-formula id="pcbi.1004265.e068"><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:msub><mml:mi>k</mml:mi> <mml:mtext mathvariant="normal">min</mml:mtext></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula> with lowest stability, <inline-formula id="pcbi.1004265.e069"><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>k</mml:mi> <mml:mtext mathvariant="normal">min</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mi>arg</mml:mi> <mml:msubsup><mml:mi>min</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, is determined on the forehand. Then, only <inline-formula id="pcbi.1004265.e070"><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:msub><mml:mi>k</mml:mi> <mml:mtext mathvariant="normal">min</mml:mtext></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula> is learned; plasticity is silenced for all other patterns. To confirm optimality and validate our mathematical results we also resorted to an interior-point convex optimiser [<xref ref-type="bibr" rid="pcbi.1004265.ref038">38</xref>] and solved the quadratic programming problem of finding the weight vector with minimal Euclidean norm ∣∣<bold>w||</bold><sub>2</sub>. We resorted to Matlab’s <monospace>quadprog</monospace> command (Optimization Toolbox) to minimise <inline-formula id="pcbi.1004265.e071"><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">∣</mml:mo> <mml:mo stretchy="false">∣</mml:mo> <mml:mtext mathvariant="bold">w</mml:mtext> <mml:mo stretchy="false">∣</mml:mo> <mml:msubsup><mml:mo stretchy="false">∣</mml:mo> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> subject to the same <italic>N</italic> non-negativity and the <italic>K</italic> pattern imprinting constraints imposed on the linear program. Up to numerical precision the obtained pattern stabilities Δ matched those given by the min-over algorithm.</p>
<p>To calculate the information <xref ref-type="disp-formula" rid="pcbi.1004265.e014">Eq (7)</xref> we tested the neuron with a set of <italic>K</italic> lures generated with the same statistics as the <italic>K</italic> learned patterns and recorded the number of FP errors. To determine the fraction of silent synapses, one has to take care of numerical rounding errors as it might be unclear when a synapse can truly be considered zero. We removed the weakest synapses one by one while probing the neuron with a large number of lures, until a drop in information occurred. With this procedure we could distinguish the true zero-weight synapses from small ones while avoiding numerical precision issues and arbitrary threshold setting. The results did not qualitatively change if we simply counted the number of synapses below some small weight <inline-formula id="pcbi.1004265.e072"><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mtext mathvariant="normal">zero</mml:mtext></mml:msub> <mml:mo>≪</mml:mo> <mml:msubsup><mml:mtext mathvariant="normal">max</mml:mtext> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, held constant across trials.</p>
<p>Since we expected self-averaging of the synaptic weights distribution from the validity of the replica trick [<xref ref-type="bibr" rid="pcbi.1004265.ref007">7</xref>], the averaged synaptic weight histograms were collected from 1000 trials. To set a common weight scale across different learning rules and input statistics, we normalised the synaptic weights so that the threshold became unity, i.e., we re-scaled the weights by a factor <inline-formula id="pcbi.1004265.e073"><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>/</mml:mo> <mml:msubsup><mml:mtext mathvariant="normal">min</mml:mtext> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank Paulo Aguiar, Ângelo Cardoso, Rui P Costa, Paolo Puggioni and Diogo Rendeiro for helpful discussions and comments on earlier versions of the manuscript. JS is very grateful to Prof Ana Paiva for sponsoring a visit to the Institute for Adaptive and Neural Computation.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004265.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Levy</surname> <given-names>WB</given-names></name>, <name name-style="western"><surname>Baxter</surname> <given-names>RA</given-names></name>. <article-title>Energy efficient neural codes</article-title>. <source>Neural Computation</source>. <year>1996</year>;<volume>8</volume>(<issue>3</issue>):<fpage>531</fpage>–<lpage>543</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1996.8.3.531" xlink:type="simple">10.1162/neco.1996.8.3.531</ext-link></comment> <object-id pub-id-type="pmid">8868566</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Howarth</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gleeson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Attwell</surname> <given-names>D</given-names></name>. <article-title>Updated energy budgets for neural computation in the neocortex and cerebellum</article-title>. <source>Journal of Cerebral Blood Flow &amp; Metabolism</source>. <year>2012</year>;<volume>32</volume>(<issue>7</issue>):<fpage>1222</fpage>–<lpage>1232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/jcbfm.2012.35" xlink:type="simple">10.1038/jcbfm.2012.35</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Knoblauch</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palm</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>FT</given-names></name>. <article-title>Memory capacities for synaptic and structural plasticity</article-title>. <source>Neural Computation</source>. <year>2010</year>;<volume>22</volume>(<issue>2</issue>):<fpage>289</fpage>–<lpage>341</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.08-07-588" xlink:type="simple">10.1162/neco.2009.08-07-588</ext-link></comment> <object-id pub-id-type="pmid">19925281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sengupta</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Niven</surname> <given-names>JE</given-names></name>. <article-title>Balanced excitatory and inhibitory synaptic currents promote efficient coding and metabolic efficiency</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>10</issue>):<fpage>e1003263</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003263" xlink:type="simple">10.1371/journal.pcbi.1003263</ext-link></comment> <object-id pub-id-type="pmid">24098105</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chugani</surname> <given-names>HT</given-names></name>. <article-title>Review: Metabolic imaging: A window on brain development and plasticity</article-title>. <source>The Neuroscientist</source>. <year>1999</year>;<volume>5</volume>(<issue>1</issue>):<fpage>29</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/107385849900500105" xlink:type="simple">10.1177/107385849900500105</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Huttenlocher</surname> <given-names>PR</given-names></name>. <article-title>Synapse elimination and plasticity in developing human cerebral cortex</article-title>. <source>American Journal of Mental Deficiency</source>. <year>1984</year>;<volume>88</volume>(<issue>5</issue>):<fpage>488</fpage>–<lpage>496</lpage>. <object-id pub-id-type="pmid">6731486</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gardner</surname> <given-names>E</given-names></name>. <article-title>The space of interactions in neural network models</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1988</year>;<volume>21</volume>(<issue>1</issue>):<fpage>257</fpage>–<lpage>270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/21/1/030" xlink:type="simple">10.1088/0305-4470/21/1/030</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kepler</surname> <given-names>TB</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Domains of attraction in neural networks</article-title>. <source>Journal de Physique</source>. <year>1988</year>;<volume>49</volume>(<issue>10</issue>):<fpage>1657</fpage>–<lpage>1662</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1051/jphys:0198800490100165700" xlink:type="simple">10.1051/jphys:0198800490100165700</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>On the storage capacity with sign-constrained synaptic couplings</article-title>. <source>Network: Computation in Neural Systems</source>. <year>1990</year>;<volume>1</volume>(<issue>4</issue>):<fpage>463</fpage>–<lpage>466</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0954-898X/1/4/006" xlink:type="simple">10.1088/0954-898X/1/4/006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bouten</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Engel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Komoda</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Serneels</surname> <given-names>R</given-names></name>. <article-title>Quenched versus annealed dilution in neural networks</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1990</year>;<volume>23</volume>:<fpage>4643</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/23/20/025" xlink:type="simple">10.1088/0305-4470/23/20/025</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Campbell</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>KYM</given-names></name>. <article-title>The interaction space of neural networks with sign-constrained synapses</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1989</year>;<volume>22</volume>(<issue>21</issue>):<fpage>4687</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/22/21/030" xlink:type="simple">10.1088/0305-4470/22/21/030</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Natarajan</surname> <given-names>BK</given-names></name>. <article-title>Sparse approximate solutions to linear systems</article-title>. <source>SIAM Journal on Computing</source>. <year>1995</year>;<volume>24</volume>(<issue>2</issue>):<fpage>227</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1137/S0097539792240406" xlink:type="simple">10.1137/S0097539792240406</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ge</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Ye</surname> <given-names>Y</given-names></name>. <article-title>A note on the complexity of <italic>L<sub>p</sub></italic> minimization</article-title>. <source>Mathematical programming</source>. <year>2011</year>;<volume>129</volume>(<issue>2</issue>):<fpage>285</fpage>–<lpage>299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10107-011-0470-2" xlink:type="simple">10.1007/s10107-011-0470-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chechik</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Meilijson</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ruppin</surname> <given-names>E</given-names></name>. <article-title>Synaptic pruning in development: A computational account</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>7</issue>):<fpage>1759</fpage>–<lpage>1777</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976698300017124" xlink:type="simple">10.1162/089976698300017124</ext-link></comment> <object-id pub-id-type="pmid">9744896</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mimura</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimoto</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>M</given-names></name>. <article-title>Synapse efficiency diverges due to synaptic pruning following overgrowth</article-title>. <source>Physical Review E</source>. <year>2003</year> 09;<volume>68</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.68.031910" xlink:type="simple">10.1103/PhysRevE.68.031910</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tsodyks</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Feigel’man</surname> <given-names>MV</given-names></name>. <article-title>The enhanced storage capacity in neural networks with low activity level</article-title>. <source>Europhysics Letters</source>. <year>1988</year>;<volume>6</volume>(<issue>2</issue>):<fpage>101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1209/0295-5075/6/2/002" xlink:type="simple">10.1209/0295-5075/6/2/002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Willshaw</surname> <given-names>D</given-names></name>. <article-title>Optimising synaptic learning rules in linear associative memories</article-title>. <source>Biological Cybernetics</source>. <year>1991</year>;<volume>65</volume>(<issue>4</issue>):<fpage>253</fpage>–<lpage>265</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00206223" xlink:type="simple">10.1007/BF00206223</ext-link></comment> <object-id pub-id-type="pmid">1932282</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nature Neuroscience</source>. <year>2007</year>;<volume>10</volume>(<issue>4</issue>):<fpage>485</fpage>–<lpage>493</lpage>. <object-id pub-id-type="pmid">17351638</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>van Rossum</surname> <given-names>MCW</given-names></name>, <name name-style="western"><surname>Shippi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barrett</surname> <given-names>AB</given-names></name>. <article-title>Soft-bound synaptic plasticity increases storage capacity</article-title>. <source>PLoS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>12</issue>):<fpage>e1002836</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002836" xlink:type="simple">10.1371/journal.pcbi.1002836</ext-link></comment> <object-id pub-id-type="pmid">23284281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Willshaw</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname> <given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname> <given-names>HC</given-names></name>. <article-title>Non-holographic associative memory</article-title>. <source>Nature</source>. <year>1969</year>;<volume>222</volume>(<issue>5197</issue>):<fpage>960</fpage>–<lpage>962</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/222960a0" xlink:type="simple">10.1038/222960a0</ext-link></comment> <object-id pub-id-type="pmid">5789326</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>MW</given-names></name>. <article-title>Comparison of computational models of familiarity discrimination in the perirhinal cortex</article-title>. <source>Hippocampus</source>. <year>2003</year>;<volume>13</volume>(<issue>4</issue>):<fpage>494</fpage>–<lpage>524</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/hipo.10093" xlink:type="simple">10.1002/hipo.10093</ext-link></comment> <object-id pub-id-type="pmid">12836918</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Itskov</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Pattern capacity of a perceptron for sparse discrimination</article-title>. <source>Physical Review Letters</source>. <year>2008</year>;<volume>101</volume>(<issue>1</issue>):<fpage>018101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.101.018101" xlink:type="simple">10.1103/PhysRevLett.101.018101</ext-link></comment> <object-id pub-id-type="pmid">18764154</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tax</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Duin</surname> <given-names>RP</given-names></name>. <article-title>Support vector domain description</article-title>. <source>Pattern Recognition Letters</source>. <year>1999</year>;<volume>20</volume>(<issue>11</issue>):<fpage>1191</fpage>–<lpage>1199</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0167-8655(99)00087-2" xlink:type="simple">10.1016/S0167-8655(99)00087-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Williamson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Smola</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Shawe-Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Platt</surname> <given-names>JC</given-names></name>. <chapter-title>Support vector method for novelty detection</chapter-title>. In: <name name-style="western"><surname>Solla</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Leen</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>K</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 12</source>. <publisher-name>MIT Press</publisher-name>; <year>1999</year>. p. <fpage>582</fpage>–<lpage>588</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>AS</given-names></name>. <source>Introduction to the theory of neural computation</source>. <publisher-name>Perseus Publishing</publisher-name>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Campbell</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>KYM</given-names></name>. <article-title>Perceptron learning with sign-constrained weights</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1989</year>;<volume>22</volume>(<issue>12</issue>):<fpage>2039</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/22/12/009" xlink:type="simple">10.1088/0305-4470/22/12/009</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Köhler</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Widmaier</surname> <given-names>D</given-names></name>. <article-title>Sign-constrained linear learning and diluting in neural networks</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1991</year>;<volume>24</volume>(<issue>9</issue>):<fpage>L495</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/24/9/008" xlink:type="simple">10.1088/0305-4470/24/9/008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Isope</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Barbour</surname> <given-names>B</given-names></name>. <article-title>Optimal information storage and the distribution of synaptic weights: Perceptron versus Purkinje cell</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>43</volume>(<issue>5</issue>):<fpage>745</fpage>–<lpage>757</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(04)00528-8" xlink:type="simple">10.1016/S0896-6273(04)00528-8</ext-link></comment> <object-id pub-id-type="pmid">15339654</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Storage of correlated patterns in standard and bistable Purkinje cell models</article-title>. <source>PLoS Computational Biology</source>. <year>2012</year>;<volume>8</volume>(<issue>4</issue>):<fpage>e1002448</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002448" xlink:type="simple">10.1371/journal.pcbi.1002448</ext-link></comment> <object-id pub-id-type="pmid">22570592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Optimal properties of analog perceptrons with excitatory weights</article-title>. <source>PLoS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>2</issue>):<fpage>e1002919</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002919" xlink:type="simple">10.1371/journal.pcbi.1002919</ext-link></comment> <object-id pub-id-type="pmid">23436991</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Learning only when necessary: Better memories of correlated patterns in networks with bounded synapses</article-title>. <source>Neural Computation</source>. <year>2005</year>;<volume>17</volume>(<issue>10</issue>):<fpage>2106</fpage>–<lpage>2138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766054615644" xlink:type="simple">10.1162/0899766054615644</ext-link></comment> <object-id pub-id-type="pmid">16105220</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>. <chapter-title>Large-scale machine learning with stochastic gradient descent</chapter-title>. In: <name name-style="western"><surname>Lechevallier</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Saporta</surname> <given-names>G</given-names></name>, editors. <source>Proceedings of COMPSTAT’2010</source>. <publisher-loc>Heidelberg</publisher-loc>: <publisher-name>Physica-Verlag</publisher-name>; <year>2010</year>. p. <fpage>177</fpage>–<lpage>186</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Williams</surname> <given-names>PM</given-names></name>. <article-title>Bayesian regularization and pruning using a Laplace prior</article-title>. <source>Neural Computation</source>. <year>1995</year>;<volume>7</volume>(<issue>1</issue>):<fpage>117</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1995.7.1.117" xlink:type="simple">10.1162/neco.1995.7.1.117</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regression shrinkage and selection via the Lasso</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>1996</year>;<volume>58</volume>(<issue>1</issue>):<fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Figueiredo</surname> <given-names>MAT</given-names></name>, <name name-style="western"><surname>Nowak</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>SJ</given-names></name>. <article-title>Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</article-title>. <source>IEEE Journal of Selected Topics in Signal Processing</source>. <year>2007</year>;<volume>1</volume>(<issue>4</issue>):<fpage>586</fpage>–<lpage>597</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/JSTSP.2007.910281" xlink:type="simple">10.1109/JSTSP.2007.910281</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sacramento</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wichert</surname> <given-names>A</given-names></name>. <article-title>Binary Willshaw learning yields high synaptic capacity for long-term familiarity memory</article-title>. <source>Biological Cybernetics</source>. <year>2012</year>;<volume>106</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-012-0488-4" xlink:type="simple">10.1007/s00422-012-0488-4</ext-link></comment> <object-id pub-id-type="pmid">22481645</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Boyd</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Vandenberghe</surname> <given-names>L</given-names></name>. <source>Convex optimization</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Krauth</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Mézard</surname> <given-names>M</given-names></name>. <article-title>Learning algorithms with optimal stability in neural networks</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1987</year>;<volume>20</volume>(<issue>11</issue>):<fpage>L745</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/20/11/013" xlink:type="simple">10.1088/0305-4470/20/11/013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barbour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>What can we learn from synaptic weight distributions?</article-title> <source>Trends in Neurosciences</source>. <year>2007</year>;<volume>30</volume>(<issue>12</issue>):<fpage>622</fpage>–<lpage>629</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2007.09.005" xlink:type="simple">10.1016/j.tins.2007.09.005</ext-link></comment> <object-id pub-id-type="pmid">17983670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Leibold</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Bendels</surname> <given-names>MH</given-names></name>. <article-title>Learning to discriminate through long-term changes of dynamical synaptic transmission</article-title>. <source>Neural Computation</source>. <year>2009</year>;<volume>21</volume>(<issue>12</issue>):<fpage>3408</fpage>–<lpage>3428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2009.12-08-929" xlink:type="simple">10.1162/neco.2009.12-08-929</ext-link></comment> <object-id pub-id-type="pmid">19764877</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>J</given-names></name>. <article-title>Recurrent network of perceptrons with three state synapses achieves competitive classification on real inputs</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2012</year>;<volume>6</volume>:<fpage>39</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2012.00039" xlink:type="simple">10.3389/fncom.2012.00039</ext-link></comment> <object-id pub-id-type="pmid">22737121</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>On the classification capability of sign-constrained perceptrons</article-title>. <source>Neural Computation</source>. <year>2008</year>;<volume>20</volume>(<issue>1</issue>):<fpage>288</fpage>–<lpage>309</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.20.1.288" xlink:type="simple">10.1162/neco.2008.20.1.288</ext-link></comment> <object-id pub-id-type="pmid">18045010</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Engel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Van den Broeck</surname> <given-names>C</given-names></name>. <source>Statistical mechanics of learning</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Monasson</surname> <given-names>R</given-names></name>. <article-title>Properties of neural networks storing spatially correlated patterns</article-title>. <source>Journal of Physics A: Mathematical and General</source>. <year>1992</year>;<volume>25</volume>(<issue>13</issue>):<fpage>3701</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/25/13/019" xlink:type="simple">10.1088/0305-4470/25/13/019</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Monasson</surname> <given-names>R</given-names></name>. <article-title>Storage of spatially correlated patterns in autoassociative memories</article-title>. <source>Journal de Physique I</source>. <year>1993</year>;<volume>3</volume>(<issue>5</issue>):<fpage>1141</fpage>–<lpage>1152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1051/jp1:1993107" xlink:type="simple">10.1051/jp1:1993107</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Attwell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>. <article-title>An energy budget for signaling in the grey matter of the brain</article-title>. <source>Journal of Cerebral Blood Flow &amp; Metabolism</source>. <year>2001</year>;<volume>21</volume>(<issue>10</issue>):<fpage>1133</fpage>–<lpage>1145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/00004647-200110000-00001" xlink:type="simple">10.1097/00004647-200110000-00001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schreiber</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Herz</surname> <given-names>AVM</given-names></name>, <name name-style="western"><surname>Laughlin</surname> <given-names>SB</given-names></name>. <article-title>Energy-efficient coding with discrete stochastic events</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>6</issue>):<fpage>1323</fpage>–<lpage>1346</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602753712963" xlink:type="simple">10.1162/089976602753712963</ext-link></comment> <object-id pub-id-type="pmid">12020449</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="other">Chen Y, Zhou XS, Huang TS. One-class SVM for learning in image retrieval. In: Proceedings of the International Conference on Image Processing. vol. 1. IEEE; 2001. p. 34–37.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kowalczyk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Raskutti</surname> <given-names>B</given-names></name>. <article-title>One class SVM for yeast regulation prediction</article-title>. <source>ACM SIGKDD Explorations Newsletter</source>. <year>2002</year>;<volume>4</volume>(<issue>2</issue>):<fpage>99</fpage>–<lpage>100</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/772862.772878" xlink:type="simple">10.1145/772862.772878</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Naeger</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <source>What can a neuron learn with spike-timing-dependent plasticity? Neural Computation</source>. <year>2005</year>;<volume>17</volume>(<issue>11</issue>):<fpage>2337</fpage>–<lpage>2382</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mery</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kawecki</surname> <given-names>TJ</given-names></name>. <article-title>A cost of long-term memory in Drosophila</article-title>. <source>Science</source>. <year>2005</year> <month>May</month>;<volume>308</volume>(<issue>5725</issue>):<fpage>1148</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1111331" xlink:type="simple">10.1126/science.1111331</ext-link></comment> <object-id pub-id-type="pmid">15905396</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chen</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Donoho</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Saunders</surname> <given-names>MA</given-names></name>. <article-title>Atomic decomposition by basis pursuit</article-title>. <source>SIAM Journal on Scientific Computing</source>. <year>1998</year>;<volume>20</volume>(<issue>1</issue>):<fpage>33</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1137/S1064827596304010" xlink:type="simple">10.1137/S1064827596304010</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mallat</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Z</given-names></name>. <article-title>Matching pursuits with time-frequency dictionaries</article-title>. <source>IEEE Transactions on Signal Processing</source>. <year>1993</year>;<volume>41</volume>(<issue>12</issue>):<fpage>3397</fpage>–<lpage>3415</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/78.258082" xlink:type="simple">10.1109/78.258082</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gütig</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>The tempotron: a neuron that learns spike timing-based decisions</article-title>. <source>Nature Neuroscience</source>. <year>2006</year>;<volume>9</volume>(<issue>3</issue>):<fpage>420</fpage>–<lpage>428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1643" xlink:type="simple">10.1038/nn1643</ext-link></comment> <object-id pub-id-type="pmid">16474393</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ölveczky</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Learning precisely timed spikes</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>(<issue>4</issue>):<fpage>925</fpage>–<lpage>938</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.03.026" xlink:type="simple">10.1016/j.neuron.2014.03.026</ext-link></comment> <object-id pub-id-type="pmid">24768299</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Toyoizumi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pfister</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Aihara</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Optimality model of unsupervised spike-timing-dependent plasticity: synaptic memory and weight distribution</article-title>. <source>Neural Computation</source>. <year>2007</year>;<volume>19</volume>(<issue>3</issue>):<fpage>639</fpage>–<lpage>671</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2007.19.3.639" xlink:type="simple">10.1162/neco.2007.19.3.639</ext-link></comment> <object-id pub-id-type="pmid">17298228</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kullmann</surname> <given-names>DM</given-names></name>. <article-title>Silent synapses: what are they telling us about long-term potentiation?</article-title> <source>Philosophical Transactions of the Royal Society of London Series B: Biological Sciences</source>. <year>2003</year>;<volume>358</volume>(<issue>1432</issue>):<fpage>727</fpage>–<lpage>733</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2002.1229" xlink:type="simple">10.1098/rstb.2002.1229</ext-link></comment> <object-id pub-id-type="pmid">12740119</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Voronin</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Cherubini</surname> <given-names>E</given-names></name>. <article-title>Deaf, mute and whispering silent synapses: their role in synaptic plasticity</article-title>. <source>The Journal of Physiology</source>. <year>2004</year>;<volume>557</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.2003.058966" xlink:type="simple">10.1113/jphysiol.2003.058966</ext-link></comment> <object-id pub-id-type="pmid">15034124</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hofer</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Bonhoeffer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hübener</surname> <given-names>M</given-names></name>. <article-title>Experience leaves a lasting structural trace in cortical circuits</article-title>. <source>Nature</source>. <year>2009</year>;<volume>457</volume>(<issue>7227</issue>):<fpage>313</fpage>–<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07487" xlink:type="simple">10.1038/nature07487</ext-link></comment> <object-id pub-id-type="pmid">19005470</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004265.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Vapnik</surname> <given-names>VN</given-names></name>. <source>The nature of statistical learning theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2000</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004265.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Opper</surname> <given-names>M</given-names></name>. <article-title>Learning times of neural networks: Exact solution for a PERCEPTRON algorithm</article-title>. <source>Physical Review A</source>. <year>1988</year>;<volume>38</volume>(<issue>7</issue>):<fpage>3824</fpage>–<lpage>3826</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevA.38.3824" xlink:type="simple">10.1103/PhysRevA.38.3824</ext-link></comment> <object-id pub-id-type="pmid">9900833</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>