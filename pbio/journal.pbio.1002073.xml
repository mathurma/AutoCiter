<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.1002073</article-id>
<article-id pub-id-type="publisher-id">PBIOLOGY-D-14-02149</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Cortical Hierarchies Perform Bayesian Causal Inference in Multisensory Perception</article-title>
<alt-title alt-title-type="running-head">Cortical Hierarchies Perform Causal Inference</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Rohe</surname>
<given-names>Tim</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Noppeney</surname>
<given-names>Uta</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Max Planck Institute for Biological Cybernetics, Tuebingen, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Computational Neuroscience and Cognitive Robotics Centre, University of Birmingham, Birmingham, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kayser</surname>
<given-names>Christoph</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Glasgow University, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: TR UN. Performed the experiments: TR. Analyzed the data: TR UN. Contributed reagents/materials/analysis tools: TR UN. Wrote the paper: TR UN.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">tim.rohe@tuebingen.mpg.de</email> (TR); <email xlink:type="simple">U.noppeney@bham.ac.uk</email> (UN)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>2</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>2</month>
<year>2015</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<elocation-id>e1002073</elocation-id>
<history>
<date date-type="received">
<day>18</day>
<month>6</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>1</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Rohe, Noppeney</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.1002073" xlink:type="simple"/>
<related-article ext-link-type="uri" id="related001" related-article-type="companion" xlink:href="info:doi/10.1371/journal.pbio.1002075" xlink:type="simple">
<article-title>Multisensory Causal Inference in the Brain</article-title>
</related-article>
<abstract>
<p>To form a veridical percept of the environment, the brain needs to integrate sensory signals from a common source but segregate those from independent sources. Thus, perception inherently relies on solving the “causal inference problem.” Behaviorally, humans solve this problem optimally as predicted by Bayesian Causal Inference; yet, the underlying neural mechanisms are unexplored. Combining psychophysics, Bayesian modeling, functional magnetic resonance imaging (fMRI), and multivariate decoding in an audiovisual spatial localization task, we demonstrate that Bayesian Causal Inference is performed by a hierarchy of multisensory processes in the human brain. At the bottom of the hierarchy, in auditory and visual areas, location is represented on the basis that the two signals are generated by independent sources (= segregation). At the next stage, in posterior intraparietal sulcus, location is estimated under the assumption that the two signals are from a common source (= forced fusion). Only at the top of the hierarchy, in anterior intraparietal sulcus, the uncertainty about the causal structure of the world is taken into account and sensory signals are combined as predicted by Bayesian Causal Inference. Characterizing the computational operations of signal interactions reveals the hierarchical nature of multisensory perception in human neocortex. It unravels how the brain accomplishes Bayesian Causal Inference, a statistical computation fundamental for perception and cognition. Our results demonstrate how the brain combines information in the face of uncertainty about the underlying causal structure of the world.</p>
</abstract>
<abstract abstract-type="toc">
<p>The human brain uses Bayesian Causal Inference to integrate and segregate information by encoding multiple estimates of spatial relationships at different levels of the auditory and visual processing hierarchies. Read the accompanying Primer.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>How can the brain integrate signals into a veridical percept of the environment without knowing whether they pertain to same or different events? For example, I can hear a bird and I can see a bird, but is it one bird singing on the branch, or is it two birds (one sitting on the branch and the other singing in the bush)? Recent studies demonstrate that human observers solve this problem optimally as predicted by Bayesian Causal Inference; yet, the neural mechanisms remain unclear. By combining psychophysics, Bayesian modelling, functional magnetic resonance imaging (fMRI), and multivariate decoding in an audiovisual localization task, we show that Bayesian Causal Inference is performed by a neural hierarchy of multisensory processes. At the bottom of the hierarchy, in auditory and visual areas, location is represented on the basis that the two signals are generated by independent sources (= segregation). At the next stage, in posterior intraparietal sulcus, location is estimated under the assumption that the two signals are from a common source (= forced fusion). Only at the top of the hierarchy, in anterior intraparietal sulcus, the uncertainty about the world’s causal structure is taken into account and sensory signals are combined as predicted by Bayesian Causal Inference.</p>
</abstract>
<funding-group>
<funding-statement>This work was funded by the Max Planck Society and the European Research Council (ERC-StG-MultSens). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="1"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files (Data S1).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Our senses are constantly bombarded with many different signals. Imagine you are crossing a street and suddenly hear a loud motor noise. Is that motor noise coming from the car on the opposite side of the street or from a rapidly approaching car that you have not yet spotted? To locate the source of the motor noise more precisely, you should integrate the auditory signal with the sight of the car only if the two inputs pertain to the same object. Thus, estimating an environmental property (e.g., spatial location) in multisensory perception inherently relies on inferring whether sensory signals are caused by common or independent sources [<xref rid="pbio.1002073.ref001" ref-type="bibr">1</xref>,<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>].</p>
<p>Past research in perception and cue combination has mostly ignored the causal inference problem and focused on the special case in which sensory signals arise from a common source. A large body of research has demonstrated that observers integrate signals near-optimally weighted by their reliability in these “forced fusion” settings [<xref rid="pbio.1002073.ref003" ref-type="bibr">3</xref>–<xref rid="pbio.1002073.ref009" ref-type="bibr">9</xref>]. Yet, in our complex natural environment forced fusion would be detrimental and the brain needs to balance integration and segregation according to the underlying causal structure (i.e., common versus independent sources) [<xref rid="pbio.1002073.ref010" ref-type="bibr">10</xref>].</p>
<p>Hierarchical Bayesian Causal Inference provides a rational strategy to arbitrate between information integration and segregation in perception and cognition. In case of a common source, signals should be integrated weighted by their relative sensory reliabilities [<xref rid="pbio.1002073.ref003" ref-type="bibr">3</xref>,<xref rid="pbio.1002073.ref004" ref-type="bibr">4</xref>]. In case of independent sources, they should be processed independently. Critically, the observer does not know the underlying causal structure and needs to infer it from spatiotemporal or higher order (e.g., semantic) congruency cues [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]. To account for the uncertainty about the causal structure, an observer should compute a final estimate by averaging the estimates (e.g., spatial location) under the two potential causal structures weighted by the posterior probabilities of these structures (i.e., model averaging).</p>
<p>Indeed, recent psychophysics and modeling efforts have demonstrated that human observers locate audiovisual signal sources in line with Bayesian Causal Inference by combining the spatial estimates under the assumptions of common and independent sources weighted by their posterior probabilities [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]. For small spatial disparities, audiovisual spatial signals are integrated weighted by their relative sensory reliabilities leading to strong crossmodal spatial biases [<xref rid="pbio.1002073.ref003" ref-type="bibr">3</xref>]; for large spatial disparities, these crossmodal biases are greatly attenuated [<xref rid="pbio.1002073.ref011" ref-type="bibr">11</xref>,<xref rid="pbio.1002073.ref012" ref-type="bibr">12</xref>], because the final spatial estimate relies predominantly on the segregated option.</p>
<p>However, the neural mechanisms that enable Bayesian Causal Inference are unknown. In particular, it is unclear whether the brain encodes the spatial estimates under the assumptions of common and independent sources in order to perform Bayesian Causal Inference. Does the brain explicitly represent several spatial estimates that enter into Bayesian Causal Inference?</p>
</sec>
<sec id="sec002" sec-type="conclusions">
<title>Results and Discussion</title>
<p>We combined psychophysics, Bayesian statistical modeling, and a multivariate functional magnetic resonance imaging (fMRI) decoding approach to characterize how the human brain performs Bayesian Causal Inference along the auditory [<xref rid="pbio.1002073.ref013" ref-type="bibr">13</xref>] and visual [<xref rid="pbio.1002073.ref014" ref-type="bibr">14</xref>] spatial cortical hierarchies. During fMRI scanning, we presented five participants with synchronous auditory (white noise) and visual (Gaussian cloud of dots) spatial signals that were independently sampled from four possible locations along the azimuth (i.e., −10°, −3.3°, 3.3°, or 10°) (<xref rid="pbio.1002073.g001" ref-type="fig">Fig 1A</xref>). Further, we manipulated the reliability of the visual signal by varying the standard deviation of the visual cloud (2° or 14° standard deviation). Participants were asked selectively to report either the visual or the auditory signal location (without feed-back). Thus, the 4 (auditory locations) × 4 (visual locations) × 2 (visual reliability) × 2 (visual versus auditory report) factorial design included 64 conditions (<xref rid="pbio.1002073.g001" ref-type="fig">Fig 1B</xref>). Importantly, as auditory and visual spatial locations were sampled independently on each trial, our design implicitly manipulated audiovisual spatial disparity, a critical cue informing the brain whether signals emanate from common or independent sources (cf. supporting <xref rid="pbio.1002073.s007" ref-type="supplementary-material">S5 Table</xref>).</p>
<fig id="pbio.1002073.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002073.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Example trial and experimental design.</title>
<p>(A) In a spatial ventriloquist paradigm, participants were presented with synchronous audiovisual (AV) signals originating from four possible locations along the azimuth. The visual signal was a cloud of white dots. The auditory signal was a brief burst of white noise. Participants localized either the auditory or the visual signal (n.b. for illustrational purposes the visual angles of the cloud have been scaled in a non-uniform fashion in this scheme). (B) The four-factorial experimental design manipulated (1) the location of the visual (V) signal (−10°, −3.3°, 3.3°, 10°) (2) the location of the auditory (A) signal (−10°, −3.3°, 3.3°, 10°), (3) the reliability of the visual signal (high versus low standard deviation of the visual cloud), and (4) task-relevance (auditory versus visual report).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.g001" position="float" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Behavioral Results</title>
<p>At the behavioral level, we first investigated how participants integrate and segregate sensory signals for auditory and visual spatial localization. <xref rid="pbio.1002073.g002" ref-type="fig">Fig 2</xref> shows the histograms of response deviations as a function of task-relevance (i.e., auditory versus visual report), audiovisual spatial disparity, and visual reliability. If participants were able to determine the location of the task-relevant auditory or visual signal precisely, the histogram over response deviations would reduce to a delta function centered on zero. Thus, the difference in widths of the histograms for auditory and visual report indicates that participants were less precise when locating auditory (green) as compared to the visual signals (red). Likewise, as expected visual localization was less precise for low (red dashed) relative to high visual (red solid) reliability. Importantly, for auditory localization, the response distribution was shifted towards a concurrent spatially discrepant visual signal. This visual spatial bias on the perceived auditory location was increased when the visual signal was reliable, thus replicating the classical profile of the spatial ventriloquist effect [<xref rid="pbio.1002073.ref003" ref-type="bibr">3</xref>]. Moreover, it was more pronounced for 13.3° than for 20° disparity. In other words, as expected under Bayesian Causal Inference, the influence of a concurrent visual signal on the perceived auditory location was attenuated for large spatial discrepancies, when it was less likely that auditory and visual signals came from a common source.</p>
<fig id="pbio.1002073.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002073.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Histograms of response deviations (across-subjects mean fraction ± standard error of the mean [SEM]) as a function of (i) task relevance (i.e., auditory versus visual report) (ii) audiovisual disparity, and (iii) visual reliability.</title>
<p>If participants were able to locate the task-relevant auditory or visual signal precisely, the histogram over response deviations would reduce to a delta function centered on zero. The histograms of response deviations for auditory report indicate that a spatially disparate visual signal biases participants’ perceived sound location in particular when the visual signal is reliable. In each panel, stimulus symbols (i.e., auditory: loudspeaker; visual: cloud of dots) indicate the location of the task-relevant signal (centered on zero) and the task-irrelevant signal (centered on the discrepant spatial location). The data used to make this figure are available in file <xref rid="pbio.1002073.s001" ref-type="supplementary-material">S1 Data</xref>.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.g002" position="float" xlink:type="simple"/>
</fig>
<p>Next, we analyzed visual and auditory localization reports more formally by comparing three models. (i) The full-segregation model assumes that auditory and visual signals are processed independently. (ii) The forced-fusion model assumes that auditory and visual signals are integrated weighted by their reliabilities in a mandatory fashion irrespective of the environmental causal structure. (iii) The Bayesian Causal Inference model computes a final auditory (or visual) spatial estimate by averaging the spatial estimates under forced-fusion and full-segregation assumptions weighted by the posterior probabilities of each causal structure (i.e., model averaging, see <xref rid="pbio.1002073.s005" ref-type="supplementary-material">S3</xref> and <xref rid="pbio.1002073.s006" ref-type="supplementary-material">S4</xref> Tables for other decision functions). Using a maximum likelihood procedure, we fitted the parameters (e.g., visual variances σ<sub>V1</sub><sup>2</sup> − σ<sub>V2</sub><sup>2</sup> for the two reliability levels) of the three models individually to each participant’s behavioral localization responses. Bayesian model comparison corroborated previous results [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>] and demonstrated that the Bayesian Causal Inference model outperformed the full-segregation and forced-fusion models (82.4% variance explained, exceedance probability of 0.95) (<xref rid="pbio.1002073.t001" ref-type="table">Table 1</xref>). In other words, human observers integrate audiovisual spatial signals predominantly when they are close in space and hence likely to come from a common source.</p>
<table-wrap id="pbio.1002073.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002073.t001</object-id>
<label>Table 1</label> <caption><title>Model parameters (across-subjects mean ± standard error of the mean) and fit indices of the three computational models.</title></caption>
<alternatives>
<graphic id="pbio.1002073.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"><bold>Model</bold></th>
<th align="left" rowspan="1" colspan="1"><bold>p<sub>C</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>σ<sub>P</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>σ<sub>A</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>σ<sub>V1</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>σ<sub>V2</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>R<sup>2</sup></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>relBIC<sub>Group</sub></bold></th>
<th align="left" rowspan="1" colspan="1"><bold>EP</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Causal Inference (model averaging)</td>
<td align="left" rowspan="1" colspan="1">0.48 ± 0.10</td>
<td align="left" rowspan="1" colspan="1">14.9 ± 3.8</td>
<td align="left" rowspan="1" colspan="1">17.1 ± 7.0</td>
<td align="left" rowspan="1" colspan="1">3.8 ± 0.5</td>
<td align="left" rowspan="1" colspan="1">8.3 ± 0.8</td>
<td align="left" rowspan="1" colspan="1">82.4 ± 3.8</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0.953</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Forced fusion</td>
<td align="left" rowspan="1" colspan="1">—</td>
<td align="left" rowspan="1" colspan="1">14.4 ± 1.6</td>
<td align="left" rowspan="1" colspan="1">14.3 ± 2.0</td>
<td align="left" rowspan="1" colspan="1">6.5 ± 0.5</td>
<td align="left" rowspan="1" colspan="1">10.7 ± 0.7</td>
<td align="left" rowspan="1" colspan="1">60.7 ± 3.4</td>
<td align="left" rowspan="1" colspan="1">7,027.5</td>
<td align="left" rowspan="1" colspan="1">0.017</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Full segregation</td>
<td align="left" rowspan="1" colspan="1">—</td>
<td align="left" rowspan="1" colspan="1">13.1 ± 2.7</td>
<td align="left" rowspan="1" colspan="1">24.1 ± 9.9</td>
<td align="left" rowspan="1" colspan="1">4.1 ± 0.7</td>
<td align="left" rowspan="1" colspan="1">7.5 ± 0.9</td>
<td align="left" rowspan="1" colspan="1">79.1 ± 4.3</td>
<td align="left" rowspan="1" colspan="1">992.6</td>
<td align="left" rowspan="1" colspan="1">0.030</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>p<sub>C</sub>, prior common-source probability; σ<sub>P</sub>, standard deviation of the spatial prior (in °); σ<sub>A</sub>, standard deviation of the auditory likelihood (in °); σ<sub>V</sub>, standard deviation of the visual likelihood at two levels of visual reliability (1, high; 2, low) (in °); R<sup>2</sup>, coefficient of determination; relBIC<sub>Group</sub>, Bayesian information criterion at the group level, i.e., subject-specific BICs summed over all subjects (BIC = LL − 0.5 M ln(N), LL = log likelihood, M = number of parameters, N = number of data points) of a model relative to the Bayesian Causal Inference (“model averaging”) model (n.b. a smaller relBIC<sub>Group</sub> indicates that a model provides a better explanation of our data); EP, exceedance probability, i.e., probability that a model is more likely than any other model.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec004">
<title>fMRI Results</title>
<p>Next, we asked how Bayesian Causal Inference emerged along the auditory and visual cortical hierarchies (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3</xref>). In particular, Bayesian Causal Inference entails four spatial estimates: the full-segregation unisensory (i) auditory (Ŝ<sub>A,C = 2</sub>) and (ii) visual estimates (Ŝ<sub>V,C = 2</sub>), (iii) the “audiovisual forced-fusion estimate” (Ŝ<sub>AV,C = 1</sub>), and (iv) the final Bayesian Causal Inference estimate (Ŝ<sub>A</sub> &amp; Ŝ<sub>V</sub>, pooled over conditions of auditory and visual report) that is obtained by averaging the forced-fusion and the task-relevant unisensory estimates weighted by the posterior probability of each causal structure. We obtained these four spatial estimates for each of the 64 conditions and each participant from the Causal Inference model fitted individually to participant’s behavioral data (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3B</xref>, bottom). Using cross-validation, we trained a support vector regression model to decode each of these four spatial estimates from fMRI voxel response patterns in regions along the cortical hierarchies defined by visual retinotopic and auditory localizers (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3C</xref>). We quantified the decoding accuracies for each of these four spatial estimates in terms of their correlation between (i) the spatial estimates obtained from the Causal Inference model fitted individually to participants’ localization responses (i.e., training labels for fMRI decoding) and (ii) the spatial estimates decoded from fMRI voxel response patterns. To determine which of the four spatial estimates is primarily encoded in a particular region, we computed the exceedance probability that a correlation coefficient of one spatial estimate was greater than that of any other spatial estimate by bootstrapping the decoding accuracies (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3D</xref>).</p>
<fig id="pbio.1002073.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.1002073.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Bayesian Causal Inference model and cortical hierarchies.</title>
<p>(A) Participants were presented with auditory and visual spatial signals. We recorded participants’ psychophysical localization responses and fMRI BOLD responses. (B) The Bayesian Causal Inference model [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>] was fitted to participants’ localization responses and then used to obtain four spatial estimates for each condition: the unisensory auditory (Ŝ<sub>A,C=2</sub>) and visual (Ŝ<sub>V,C=2</sub>) estimates under full segregation (C = 2), the forced-fusion estimate (Ŝ<sub>AV,C=1</sub>) under full integration (C = 1), and the final spatial estimate (Ŝ<sub>A</sub>, Ŝ<sub>V</sub>) that averages the task-relevant unisensory and the forced-fusion estimate weighted by the posterior probability of each causal structure (i.e., for a common source: p(C = 1|x<sub>A</sub>, x<sub>V</sub>) or independent sources: 1 − p(C = 1|x<sub>A</sub>, x<sub>V</sub>). (C) fMRI voxel response patterns were obtained from regions along the visual and auditory hierarchies (V, visual sensory regions; A1, primary auditory cortex; hA, higher auditory area; IPS, intraparietal sulcus). (D) Exceedance probabilities index the belief that a given spatial estimate is more likely represented within a region of interest than any other spatial estimate. The exceedance probabilities for the different spatial estimates are indexed in the length of the colored areas of each bar (n.b. the <italic>y</italic>-axis indicates the cumulative exceedance probabilities). The data used to make this figure are available in file <xref rid="pbio.1002073.s001" ref-type="supplementary-material">S1 Data</xref>.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.g003" position="float" xlink:type="simple"/>
</fig>
<p>The profile of exceedance probabilities demonstrates that Bayesian Causal Inference is performed by a hierarchy of multisensory processes in the human brain: At the bottom of the hierarchy, in auditory and visual areas, location is represented on the basis that the two signals are generated by independent sources. Thus, primary sensory areas predominantly encoded the spatial estimate of their preferred sensory modality under information segregation, even though they also showed limited multisensory influences as previously reported [<xref rid="pbio.1002073.ref015" ref-type="bibr">15</xref>–<xref rid="pbio.1002073.ref021" ref-type="bibr">21</xref>]. At the next stage, in posterior intraparietal sulcus (IPS1–2), location is estimated under the assumption that the two signals are from a common source. In other words, IPS1–2 represented primarily the reliability-weighted integration estimate under forced-fusion assumptions. It is only at the top of the hierarchy, in anterior intraparietal sulcus (IPS3–4), that the uncertainty about whether signals are generated by common or independent sources is taken into account. As predicted by Bayesian Causal Inference, location is estimated in IPS3–4 by combining the full-segregation and the forced-fusion estimates weighted by the posterior probabilities of common and independent sources. Thus, according to Bayesian Causal Inference the spatial estimates in IPS3–4 should be influenced by task-irrelevant sensory signals primarily for small spatial disparities, when signals were likely to be generated by a common event. Critically, while no region could uniquely be assigned one type of spatial estimate, the profile of exceedance probabilities reveals a hierarchical organization of the computational operations in human neocortex.</p>
<p>Recent elegant neurophysiological research in non-human primates has shown how single neurons and neuronal populations implement reliability-weighted integration under forced-fusion assumptions [<xref rid="pbio.1002073.ref022" ref-type="bibr">22</xref>–<xref rid="pbio.1002073.ref024" ref-type="bibr">24</xref>]. In other words, they presented visual and vestibular signals only with a very small discrepancy, so that signals could be assumed to arise from a common source. Yet, to our knowledge this is the first neuroimaging study that moves beyond traditional forced-fusion models and demonstrates how the brain performs hierarchical Bayesian Causal Inference [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]. Thus, future neurophysiological and modelling research will need to define how single neurons and neuronal populations implement computational operations of Bayesian Causal Inference, potentially via probabilistic population codes [<xref rid="pbio.1002073.ref025" ref-type="bibr">25</xref>].</p>
<p>Accumulating evidence has suggested that multisensory interactions are pervasive in human neocortex [<xref rid="pbio.1002073.ref018" ref-type="bibr">18</xref>,<xref rid="pbio.1002073.ref026" ref-type="bibr">26</xref>–<xref rid="pbio.1002073.ref031" ref-type="bibr">31</xref>] starting already at the primary cortical level [<xref rid="pbio.1002073.ref015" ref-type="bibr">15</xref>–<xref rid="pbio.1002073.ref021" ref-type="bibr">21</xref>]. Indeed, our multivariate decoding analysis also revealed multisensory influences ubiquitously along the auditory and visual processing streams with limited multisensory influences emerging already in primary sensory areas.</p>
<p>To link our study more closely with previous fMRI results of spatial ventriloquism, we have interrogated our data also with a conventional univariate analysis of regional blood-oxygen-level dependent (BOLD) responses. Converging with our model-based findings, this conventional analysis also suggested that low-level sensory areas are predominantly driven by signals of their preferred sensory modality (e.g., visual cortex by visual signals). Yet, in line with previous reports [<xref rid="pbio.1002073.ref027" ref-type="bibr">27</xref>,<xref rid="pbio.1002073.ref031" ref-type="bibr">31</xref>], visual signals influenced the BOLD response already in the “higher auditory area” (hA) encompassing the planum temporale. Moreover, while activations in parietal areas were still influenced by visual location, they were progressively susceptible to effects of task-context mediated either directly or in interaction with visual reliability (see supporting results and discussion in <xref rid="pbio.1002073.s002" ref-type="supplementary-material">S1 Fig</xref>, <xref rid="pbio.1002073.s004" ref-type="supplementary-material">S2 Table</xref>, and <xref rid="pbio.1002073.s009" ref-type="supplementary-material">S1 Text</xref>). Thus, both the regional BOLD response and the spatial representations encoded in parietal areas and to some extent in auditory areas were influenced by whether the location of the visual or the auditory signal needed to be attended to and reported in line with the principles of Bayesian Causal Inference.</p>
<p>As the current paradigm manipulated the factor of task-relevance over sessions, participants knew the sensory modality that needed to be reported prior to stimulus presentation. Thus, the regional BOLD-response in higher auditory cortices is likely to be modulated by attentional top-down effects [<xref rid="pbio.1002073.ref032" ref-type="bibr">32</xref>–<xref rid="pbio.1002073.ref036" ref-type="bibr">36</xref>]. Future studies may investigate Bayesian Causal Inference when auditory and visual report trials are presented in a randomized fashion to minimize attention- and expectation-related effects. Alternatively, studies could factorially manipulate (i) the attended and (ii) the reported sensory modality. For instance, participants may be cued to attend to the auditory modality prior to stimulus presentation and yet be instructed to report the visual modality after stimulus presentation.</p>
<p>Yet, despite these attempts Bayesian Causal Inference may inherently entail processes associated with “attentional modulation” in a wider sense, as it computationally requires combining the multisensory forced-fusion estimate with the “task-relevant” unisensory estimate. Critically, however, the effects of attentional modulation or task-relevance invoked by Bayesian Causal Inference should interact with the spatial discrepancy between the sensory signals. Effects of task-relevance should be most pronounced for large spatial discrepancies.</p>
<p>In conclusion, the multivariate analysis based on Bayesian Causal Inference moves significantly beyond identifying multisensory interactions, towards characterizing their computational operations that prove to differ across cortical levels. This methodological approach provides a novel hierarchical perspective on multisensory integration in human neocortex. We demonstrate that the brain simultaneously encodes multiple spatial estimates based on segregation, forced fusion, and model averaging along the cortical hierarchy. Only at the top of the hierarchy, higher-order anterior IPS3–4 takes into account the uncertainty about the causal structure of the world and combines sensory signals as predicted by Bayesian Causal Inference. To our knowledge, this study is the first compelling demonstration of how the brain performs Bayesian Causal Inference, a statistical operation fundamental for perception and cognition.</p>
</sec>
</sec>
<sec id="sec005" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec006">
<title>Participants</title>
<p>The study was approved by the human research review committee of the University of Tuebingen (approval number 432 2007 BO1). After giving written informed consent, six healthy volunteers without a history of neurological or psychiatric disorders (all university students or graduates; 2 female; mean age 28.8 years, range 22–36 years) participated in the fMRI study. All participants had normal or corrected-to-normal vision and reported normal hearing. One participant was excluded because of excessive head motion (4.21/3.52 standard deviations above the mean of the translational/rotational volume-wise head motion based on the included five participants).</p>
</sec>
<sec id="sec007">
<title>Stimuli</title>
<p>The visual stimulus was a cloud of 20 white dots (diameter: 0.43° visual angle) sampled from a bivariate Gaussian with a vertical standard deviation of 2.5° and a horizontal standard deviation of 2° or 14° presented on a black background (i.e., 100% contrast). Participants were told that the 20 dots were generated by one underlying source in the center of the cloud.</p>
<p>The auditory stimulus was a burst of white noise with a 5 ms on/off ramp. To create a virtual auditory spatial signal, the noise was convolved with spatially specific head-related transfer functions (HRTFs) thereby providing binaural (interaural time and amplitude differences) and monoaural spatial filtering signals. The HRTFs were pseudo-individualized by matching participants’ head width, height, depth, and circumference to the anthropometry of participants in the CIPIC database [<xref rid="pbio.1002073.ref037" ref-type="bibr">37</xref>]. HRTFs from the available locations in the database were interpolated to the desired location of the auditory signal. The behavioral responses from the auditory localizer session (see below) indicated that participants were able to localize the virtual auditory spatial signals in the magnetic resonance (MR) scanner. They were significantly better than chance at discriminating whether two subsequent auditory signals were presented from the same or different locations (mean accuracy = 0.88; mean d’ = 3.14, <italic>p</italic> = 0.001 in a one sample <italic>t</italic>-test against zero).</p>
</sec>
<sec id="sec008">
<title>Experimental Design</title>
<p>In a spatial ventriloquist paradigm, participants were presented with synchronous, yet spatially congruent or disparate visual and auditory signals (<xref rid="pbio.1002073.g001" ref-type="fig">Fig 1A</xref>). On each trial, visual and auditory locations were independently sampled from four possible locations along the azimuth (i.e., −10°, −3.3°, 3.3°, or 10°) leading to four levels of spatial discrepancy (i.e., 0°, 6.6°, 13.3°, or 20°). In addition, we manipulated the reliability of the visual signal by setting the horizontal standard deviation of the Gaussian cloud to 2° (high reliability) or 14° (low reliability) visual angle. In an inter-sensory selective-attention paradigm, participants reported their auditory or visual perceived signal location and ignored signals in the other modality. For the visual modality, they were asked to determine the location of the center of the visual cloud of dots. Hence, the 4 × 4 × 2 × 2 factorial design manipulated (i) the location of the visual stimulus ({−10°, −3.3°, 3.3°, 10°}, i.e., the mean of the Gaussian); (ii) the location of the auditory stimulus ({−10°, −3.3°, 3.3°, 10°}); (iii) the reliability of the visual signal ({2°,14°}, standard deviation of the Gaussian); and (iv) task-relevance (auditory-/visual-selective report) resulting in 64 conditions (<xref rid="pbio.1002073.g001" ref-type="fig">Fig 1B</xref>). Please note that in contrast to our inter-sensory attention paradigm, Koerding and colleagues [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>] employed a dual task paradigm where participants reported auditory and visual locations on each trial. Thus, the two paradigms differ in terms of attentional and task-induced processes.</p>
<p>On each trial, synchronous audiovisual spatial signals were presented for 50 ms followed by a variable inter-stimulus fixation interval from 1.75–2.75 s. Participants localized the signal in the task-relevant sensory modality as accurately as possible by pushing one of four spatially corresponding buttons. Throughout the experiment, they fixated a central cross (1.6° diameter).</p>
<p>To maximize design efficiency, stimuli and conditions were presented in a pseudorandomized fashion. Only the factor task-relevance was held constant within a session and counterbalanced across sessions. In each session, each of the 32 audiovisual spatial stimuli was presented exactly 11 times either under auditory- or visual-selective report. On average, 5.9% of the trials were interspersed as null-events in the sequence of 352 stimuli per session. Each participant completed 20 sessions (ten auditory and ten visual localization reports; apart from one participant who performed nine auditory and 11 visual localization sessions). Before the fMRI study, participants completed one practice session outside the scanner.</p>
</sec>
<sec id="sec009">
<title>Experimental Setup</title>
<p>Audiovisual stimuli were presented using Psychtoolbox 3.09 (www.psychtoolbox.org) [<xref rid="pbio.1002073.ref038" ref-type="bibr">38</xref>] running under MATLAB R2010a (MathWorks). Auditory stimuli were presented at ~75 dB SPL using MR-compatible headphones (MR Confon). Visual stimuli were back-projected onto a Plexiglas screen using an LCoS projector (JVC DLA-SX21). Participants viewed the screen through an extra-wide mirror mounted on the MR head-coil resulting in a horizontal visual field of approximately 76° at a viewing distance of 26 cm. Participants performed the localization task using an MR-compatible custom-built button device. Participants’ eye movements and fixation were monitored by recording participants’ pupil location using an MR-compatible custom-build infrared camera (sampling rate 50 Hz) mounted in front of the participants’ right eye and iView software 2.2.4 (SensoMotoric Instruments).</p>
</sec>
<sec id="sec010">
<title>Eye Movement Recording and Analysis</title>
<p>To address potential concerns that our results may be confounded by eye movements, we evaluated participants’ eye movements based on eye tracking data recorded concurrently during fMRI acquisition. Eye recordings were calibrated with standard eccentricities between ±3° and ±10° to determine the deviation from the fixation cross. Fixation position was post-hoc offset corrected. Eye position data were automatically corrected for blinks and converted to radial velocity. For each condition, the number of saccades (defined by a radial eye-velocity threshold of 15° s<sup>−1</sup> for a minimum of 60 ms duration and radial amplitude larger than 1°) were quantified (0–875 ms after stimulus onset). Fixation was well maintained throughout the experiment with post-stimulus saccades detected in only 2.293% ± 1.043% (mean ± SEM) of the trials. Moreover, 4 (visual location) × 4 (auditory location) × 2 (visual reliability) × 2 (visual versus auditory report) repeated measure ANOVAs performed separately for (i) % saccades or (ii) % eye blinks revealed no significant main effects or interactions.</p>
</sec>
<sec id="sec011">
<title>Behavioral Analysis</title>
<p>To characterize how participants integrate auditory and visual signals into spatial representations, we computed the deviation between the responded location and the mean responded location in the corresponding congruent condition for each trial and in each subject. For instance, for trial i (e.g., auditory location = 3.3°, visual location = −3.3°, visual reliability = low, visual report) we computed the response deviation by comparing the responded visual location in trial i to the mean responded visual location for the corresponding congruent condition (e.g., auditory location = −3.3°, visual location = −3.3°, visual reliability = low, visual report). We then averaged the individual histograms of response deviations across subjects (<xref rid="pbio.1002073.g002" ref-type="fig">Fig 2</xref>, for an additional analysis of response accuracy see supporting results in <xref rid="pbio.1002073.s009" ref-type="supplementary-material">S1 Table</xref> and <xref rid="pbio.1002073.s009" ref-type="supplementary-material">S1 Text</xref>). <xref rid="pbio.1002073.g002" ref-type="fig">Fig 2</xref> shows the histograms of the response deviations as a function of task-relevance, visual reliability and audiovisual disparity (i.e., disparity = visual location − auditory location). Please note that we flipped the histograms for negative spatial disparities and auditory report and the histograms for positive spatial disparities and visual report, so that for both types of reports increasing disparity corresponded to a rightward shift of the task-irrelevant signal in <xref rid="pbio.1002073.g002" ref-type="fig">Fig 2</xref>. We then combined the histograms for positive and negative spatial disparities to reduce the number of conditions and the complexity of <xref rid="pbio.1002073.g002" ref-type="fig">Fig 2</xref>.</p>
</sec>
<sec id="sec012">
<title>Bayesian Causal Inference Model</title>
<p>Details of the Bayesian Causal Inference model of audiovisual perception can be found in Koerding and colleagues [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]. The generative model (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3B</xref>) assumes that common (C = 1) or independent (C = 2) sources are determined by sampling from a binomial distribution with the common-source prior P(C = 1) = p<sub>common</sub>. For a common source, the “true” location S<sub>AV</sub> is drawn from the spatial prior distribution N(μ<sub>P</sub>, σ<sub>P</sub>). For two independent causes, the “true” auditory (S<sub>A</sub>) and visual (S<sub>V</sub>) locations are drawn independently from this spatial prior distribution. For the spatial prior distribution, we assumed a central bias (i.e., μ<sub>P</sub> = 0). We introduced sensory noise by drawing x<sub>A</sub> and x<sub>V</sub> independently from normal distributions centered on the true auditory (respectively, visual) locations with parameters σ<sub>A</sub> (respectively, σ<sub>V</sub>). Thus, the generative model included the following free parameters: the common-source prior p<sub>common</sub>, the spatial prior variance σ<sub>P</sub><sup>2</sup>, the auditory variance σ<sub>A</sub><sup>2</sup>, and the two visual variances σ<sub>V</sub><sup>2</sup> corresponding to the two visual reliability levels.</p>
<p>Under the assumption of a squared loss function, the posterior probability of the underlying causal structure can be inferred by combining the common-source prior with the sensory evidence according to Bayes rule (cf. <xref rid="pbio.1002073.s007" ref-type="supplementary-material">S5 Table</xref>):
<disp-formula id="pbio.1002073.e001">
<alternatives>
<graphic id="pbio.1002073.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e001" xlink:type="simple"/>
<mml:math display="block" id="M01" overflow="scroll">
<mml:mrow>
<mml:mtext>p</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>C</mml:mtext><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/>
<mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/>
<mml:mfrac>
<mml:mrow>
<mml:mtext>p</mml:mtext><mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub><mml:mo>∣</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>common</mml:mtext></mml:mrow></mml:msub></mml:mrow>
<mml:mrow><mml:mtext>p</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
In the case of a common source (C = 1) (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3B</xref> left), the optimal estimate of the audiovisual location is a reliability-weighted average of the auditory and visual percepts and the spatial prior.</p>
<disp-formula id="pbio.1002073.e002">
<alternatives>
<graphic id="pbio.1002073.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e002" xlink:type="simple"/>
<mml:math display="block" id="M02" overflow="scroll">
<mml:mrow>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>AV</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>A</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>V</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mtext>P</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>A</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>V</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
<p>In the case of independent sources (C = 2) (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3B</xref> right), the optimal estimates of the auditory and visual signal locations (for the auditory and visual location report, respectively) are independent from each other.</p>
<disp-formula id="pbio.1002073.e003">
<alternatives>
<graphic id="pbio.1002073.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e003" xlink:type="simple"/>
<mml:math display="block" id="M03" overflow="scroll">
<mml:mrow>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>A</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>A</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mtext>P</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>A</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>V</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>V</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="normal">μ</mml:mi><mml:mtext>P</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>V</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mtext>P</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac>
</mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
<p>To provide a final estimate of the auditory and visual locations, the brain can combine the estimates under the two causal structures using various decision functions such as “model averaging,” “model selection,” and “probability matching” [<xref rid="pbio.1002073.ref039" ref-type="bibr">39</xref>]. In the main paper, we present results using “model averaging” as the decision function that was associated with the highest model evidence and exceedance probability at the group level (see <xref rid="pbio.1002073.s006" ref-type="supplementary-material">S4 Table</xref>; please note that at the within-subject level, model averaging was the most likely decision strategy in only three subjects, see <xref rid="pbio.1002073.s005" ref-type="supplementary-material">S3 Table</xref>, and Wozny and colleagues [<xref rid="pbio.1002073.ref039" ref-type="bibr">39</xref>]). According to the “model averaging” strategy, the brain combines the integrated forced-fusion spatial estimate with the segregated, task-relevant unisensory (i.e., either auditory or visual) spatial estimates weighted in proportion to the posterior probability of the underlying causal structures.</p>
<disp-formula id="pbio.1002073.e004">
<alternatives>
<graphic id="pbio.1002073.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e004" xlink:type="simple"/>
<mml:math display="block" id="M04" overflow="scroll">
<mml:mrow>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mtext>A</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>p</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>AV</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>A</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
<disp-formula id="pbio.1002073.e005">
<alternatives>
<graphic id="pbio.1002073.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e005" xlink:type="simple"/>
<mml:math display="block" id="M05" overflow="scroll">
<mml:mrow>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mtext>V</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>p</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>AV</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>p</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>A</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>V</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub><mml:mover accent="true"><mml:mtext>S</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>V</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<p>Thus, Bayesian Causal Inference formally requires three spatial estimates (Ŝ<sub>AV,C = 1</sub>, Ŝ<sub>A,C = 2</sub>, Ŝ<sub>V,C = 2</sub>) which are combined weighted by the posterior probability of each causal structure into a final estimate (Ŝ<sub>A</sub> / Ŝ<sub>V</sub>, depending on which sensory modality is task-relevant).</p>
<p>We evaluated whether and how participants integrate auditory and visual signals based on their behavioral localization responses by comparing three models: (i) The observers may process and report auditory and visual signals independently (i.e., the full-segregation model, Equation <xref rid="pbio.1002073.e003" ref-type="disp-formula">3</xref>). (ii) They may integrate auditory and visual signals in a mandatory fashion irrespective of spatial disparity (i.e., the forced-fusion model, Equation <xref rid="pbio.1002073.e002" ref-type="disp-formula">2</xref>). (iii) The observer may perform Bayesian Causal Inference, i.e., combine estimates from the forced-fusion and the task-relevant estimate from the full-segregation model weighted by the probability of the underlying causal structures (Equations <xref rid="pbio.1002073.e004" ref-type="disp-formula">4</xref> and <xref rid="pbio.1002073.e005" ref-type="disp-formula">5</xref>, i.e., model averaging, for other decision functions see <xref rid="pbio.1002073.s005" ref-type="supplementary-material">S3 Table</xref> and <xref rid="pbio.1002073.s006" ref-type="supplementary-material">S4 Table</xref>).</p>
<p>To arbitrate between full segregation, forced fusion, and Bayesian Causal Inference, we fitted each model to participants’ localization responses (<xref rid="pbio.1002073.t001" ref-type="table">Table 1</xref>) based on the predicted distributions of the auditory spatial estimates (i.e., p(Ŝ<sub>A</sub>|S<sub>A</sub>,S<sub>V</sub>)) and the visual spatial estimates (i.e., p(Ŝ<sub>V</sub>|S<sub>A</sub>,S<sub>V</sub>)). These distributions were obtained by marginalizing over the internal variables x<sub>A</sub> and x<sub>V</sub> that are not accessible to the experimenter (for further details of the fitting procedure see Koerding and colleagues [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]). These distributions were generated by simulating x<sub>A</sub> and x<sub>V</sub> 5,000 times for each of the 64 conditions and inferring Ŝ<sub>A</sub> and Ŝ<sub>V</sub> from Equations <xref rid="pbio.1002073.e001" ref-type="disp-formula">1</xref>–<xref rid="pbio.1002073.e005" ref-type="disp-formula">5</xref>. To link p(Ŝ<sub>A</sub>|S<sub>A</sub>,S<sub>V</sub>) and p(Ŝ<sub>V</sub>|S<sub>A</sub>,S<sub>V</sub>) to participants’ auditory or visual discrete localization responses, we assumed that participants selected the button that is closest to Ŝ<sub>A</sub> or Ŝ<sub>V</sub> and binned the Ŝ<sub>A</sub> and Ŝ<sub>V</sub> accordingly into a histogram (with four bins corresponding to the four buttons). Thus, we obtained a histogram of predicted auditory or visual localization responses for each condition and participant. Based on these histograms we computed the probability of a participant’s counts of localization responses using the multinomial distribution (see Koerding and colleagues [<xref rid="pbio.1002073.ref002" ref-type="bibr">2</xref>]). This gives the likelihood of the model given participants’ response data. Assuming independence of experimental conditions, we summed the log likelihoods across conditions.</p>
<p>To obtain maximum likelihood estimates for the parameters of the models (p<sub>common</sub>, σ<sub>P</sub>, σ<sub>A</sub>, σ<sub>V1</sub> − σ<sub>V2</sub> for the two levels of visual reliability; formally, the forced-fusion and full-segregation models assume p<sub>common</sub> = 1 or = 0, respectively), we used a non-linear simplex optimization algorithm as implemented in MATLAB’s fmin search function (MATLAB R2010b). This optimization algorithm was initialized with 200 different parameter settings that were defined based on a prior grid search. We report the results (across-subjects' mean and standard error) from the parameter setting with the highest log likelihood across the 200 initializations (<xref rid="pbio.1002073.t001" ref-type="table">Table 1</xref>). This fitting procedure was applied individually to each participant’s data set for the Bayesian Causal Inference, the forced-fusion, and the full-segregation models.</p>
<p>The model fit was assessed by the coefficient of determination R<sup>2</sup> [<xref rid="pbio.1002073.ref040" ref-type="bibr">40</xref>] defined as
<disp-formula id="pbio.1002073.e006">
<alternatives>
<graphic id="pbio.1002073.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e006" xlink:type="simple"/>
<mml:math display="block" id="M06" overflow="scroll">
<mml:mrow>
<mml:msup><mml:mtext>R</mml:mtext><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>n</mml:mtext></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mtext>l</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">ß</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>l</mml:mtext><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where l(<inline-formula id="pbio.1002073.e007"><alternatives><graphic id="pbio.1002073.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e007" xlink:type="simple"/><mml:math display="inline" id="M07" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">ß</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) and l(0) denote the log likelihoods of the fitted and the null model, respectively, and n is the number of data points. For the null model, we assumed that an observer randomly chooses one of the four response options, i.e., we assumed a discrete uniform distribution with a probability of 0.25. As in our case the Bayesian Causal Inference model’s responses were discretized to relate them to the four discrete response options, the coefficient of determination was scaled (i.e., divided) by the maximum coefficient (cf. [<xref rid="pbio.1002073.ref040" ref-type="bibr">40</xref>]) defined as
<disp-formula id="pbio.1002073.e008">
<alternatives>
<graphic id="pbio.1002073.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1002073.e008" xlink:type="simple"/>
<mml:math display="block" id="M08" overflow="scroll">
<mml:mrow><mml:mtext>max</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mtext>R</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>n</mml:mtext></mml:mrow></mml:mfrac><mml:mtext>l</mml:mtext><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo>
</mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
To identify the optimal model for explaining participants’ data, we compared the candidate models using the Bayesian information criterion (BIC) as an approximation to the model evidence [<xref rid="pbio.1002073.ref041" ref-type="bibr">41</xref>]. The BIC depends on both model complexity and model fit. We performed Bayesian model selection [<xref rid="pbio.1002073.ref042" ref-type="bibr">42</xref>] at the group level as implemented in SPM8 [<xref rid="pbio.1002073.ref043" ref-type="bibr">43</xref>] to obtain the exceedance probability for the candidate models (i.e., the probability that a given model is more likely than any other model given the data).</p>
</sec>
<sec id="sec013">
<title>MRI Data Acquisition</title>
<p>A 3T Siemens Magnetom Trio MR scanner was used to acquire both T1-weighted anatomical images and T2*-weighted axial echoplanar images with BOLD contrast (gradient echo, parallel imaging using GRAPPA with an acceleration factor of 2, TR = 2,480 ms, TE = 40 ms, flip angle = 90°, FOV = 192 × 192 mm<sup>2</sup>, image matrix 78 × 78, 42 transversal slices acquired interleaved in ascending direction, voxel size = 2.5 × 2.5 × 2.5 mm<sup>3</sup> + 0.25 mm interslice gap).</p>
<p>In total, 353 volumes times 20 sessions were acquired for the ventriloquist paradigm, 161 volumes times 2–4 sessions for the auditory localizer and 159 volumes times 10–16 sessions for the visual retinotopic localizer resulting in approximately 18 hours of scanning in total per participant assigned over 7–11 days. The first three volumes of each session were discarded to allow for T1 equilibration effects.</p>
</sec>
<sec id="sec014">
<title>fMRI Data Analysis</title>
<p><bold>Ventriloquist paradigm.</bold> The fMRI data were analyzed with SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) [<xref rid="pbio.1002073.ref043" ref-type="bibr">43</xref>]. Scans from each participant were corrected for slice timing, were realigned and unwarped to correct for head motion and spatially smoothed with a Gaussian kernel of 3 mm FWHM. The time series in each voxel was high-pass filtered to 1/128 Hz. All data were analyzed in native participant space. The fMRI experiment was modelled in an event-related fashion with regressors entering into the design matrix after convolving each event-related unit impulse with a canonical hemodynamic response function and its first temporal derivative. In addition to modelling the 32 conditions in our 4 (auditory locations) × 4 (visual locations) × 2 (visual reliability) factorial design, the general linear model included the realignment parameters as nuisance covariates to account for residual motion artefacts. The factor task-relevance (visual versus auditory report) was modelled across sessions.</p>
<p>The parameter estimates pertaining to the canonical hemodynamic response function defined the magnitude of the BOLD response to the audiovisual stimuli in each voxel. For the multivariate decoding analysis, we extracted the parameter estimates of the canonical hemodynamic response function for each condition and session from voxels of the regions of interest (= fMRI voxel response patterns) defined in separate auditory and retinotopic localizer experiments (see below). Each fMRI voxel response pattern for the 64 conditions in our 4 × 4 × 2 × 2 factorial design was based on 11 trials within a particular session. To avoid the effects of image-wide activity changes, each fMRI voxel response pattern was normalized to have mean zero and standard deviation one.</p>
<p><bold>Decoding of spatial estimates.</bold> To investigate whether and how regions along the auditory and visual spatial processing hierarchy (defined below; cf. <xref rid="pbio.1002073.g003" ref-type="fig">Fig 3C</xref>) represent spatial estimates of the Causal Inference model, we used a multivariate decoding approach where we decoded each of the four spatial estimates from the regions of interest: (i) the full-segregation visual estimate: Ŝ<sub>V,C = 2</sub>, (ii) the full-segregation auditory estimate: Ŝ<sub>A,C = 2</sub>, (iii) the forced-fusion audiovisual estimate: Ŝ<sub>AV,C = 1</sub>, and (iv) the Bayesian Causal Inference (i.e., model averaging) estimate: Ŝ<sub>A</sub> &amp; Ŝ<sub>V</sub>, pooled over auditory and visual report (i.e., for each condition we selected the model averaging estimate that needs to be reported in a particular task context). Thus, our decoding approach implicitly assumed that the forced-fusion as well as the auditory and visual estimates under full segregation are computed automatically irrespective of task-context. By contrast, the final auditory or visual Bayesian Causal Inference estimates are flexibly computed depending on the particular task-context according to a decision function such as model averaging. After fitting the Causal Inference model individually to behavioral localization responses (see above), the fitted model predicted these four spatial estimates’ values in 10,000 simulated trials for each of the 64 conditions. The spatial estimates’ values as an index of participants’ perceived location are of a continuous nature. Finally, we summarized the posterior distribution of spatial estimates (i.e., participant’s perceived location) by averaging the values across those 10,000 simulated trials for each of the four spatial estimates separately for each condition and participant. Please note that using the maximum a posteriori estimate as a summary index for the posterior distribution provided nearly equivalent results.</p>
<p>For decoding, we trained a linear support vector regression model (SVR, as implemented in LIBSVM 3.14 [<xref rid="pbio.1002073.ref044" ref-type="bibr">44</xref>]) to accommodate the continuous nature of these mean spatial estimates that reflect the perceived signal location for a particular condition and subject. More specifically, we employed a leave “one session” out cross-validation scheme: First, we extracted the voxel response patterns in a particular region of interest (e.g., V1) from the parameter estimate images pertaining to the magnitude of the BOLD response for each condition and session (i.e., 32 conditions × 10 sessions for auditory report + 32 conditions × 10 sessions for visual report = 640 voxel response patterns). For each of the four spatial estimates (e.g., Ŝ<sub>V,C = 2</sub>), we trained one SVR model to learn the mapping from the condition-specific fMRI voxel response patterns (i.e., examples) to the condition-specific spatial estimate’s values (i.e., labels) from all but one session (i.e., 640 − 32 = 608 voxel responses patterns). The model then used this learnt mapping to decode the spatial estimates from the 32 voxel response patterns from the single remaining session. In a leave-one-session-out cross-validation scheme, the training-test procedure was repeated for all sessions. The SVRs’ parameters (C and ν) were optimized using a grid search within each cross-validation fold (i.e., nested cross-validation).</p>
<p>We quantified the decoding accuracies for each of these four spatial estimates in terms of the correlation coefficient between (i) the spatial estimates obtained from the Causal Inference model fitted individually to a participant’s localization responses (i.e., these spatial estimates were used as training labels for fMRI decoding, e.g., Ŝ<sub>V,C = 2</sub>) and (ii) the spatial estimates decoded from fMRI voxel response patterns using SVR. To determine whether the spatial estimates (i.e., labels) can be decoded from the voxel response patterns, we entered the Fisher z-transformed correlation coefficients for each participant into a between-subject one-sample <italic>t</italic>-test and tested whether the across-participants mean correlation coefficient was significantly different from zero separately for the (i) segregated auditory or (ii) visual, (iii) forced-fusion audiovisual, or (iv) auditory and visual Bayesian Causal Inference estimate. As these four spatial estimates were inherently correlated, most regions showed significant positive correlation coefficients for several or even all spatial estimates (see <xref rid="pbio.1002073.s008" ref-type="supplementary-material">S6 Table</xref>). Thus, to determine which of the four spatial estimates was predominantly represented in a region, we computed the exceedance probabilities (i.e., the probability that the correlation coefficient of one spatial estimate is greater than the correlation coefficient of any other spatial estimate) using non-parametric bootstrapping across participants (<italic>N</italic> = 1,000 times). For each bootstrap, we resampled the 5 (= number of participants) individual Fisher z-transformed correlation coefficients with replacement from the set of participants for each of the four spatial estimates and formed the across participants’ mean correlation coefficient for each of the four spatial estimates [<xref rid="pbio.1002073.ref045" ref-type="bibr">45</xref>]. In each bootstrap, we then determined which of the four spatial estimates obtained the largest mean correlation coefficient. We repeated this procedure for 1,000 bootstraps. The fraction of bootstraps in which a decoded spatial estimate (e.g., the segregated auditory estimate) had the largest mean correlation coefficient (indexing decoding accuracy) was defined as a spatial estimate’s exceedance probability (<xref rid="pbio.1002073.g003" ref-type="fig">Fig 3D</xref>). Please note that under the null hypothesis, we would expect that none of the four spatial estimates is related to the voxel response pattern resulting in a uniform distribution of exceedance probabilities for all four spatial estimates (i.e., exceedance probability of 0.25).</p>
<p><bold>Auditory and visual retinotopic localizer.</bold> Auditory and visual retinotopic localizers were used to define regions of interest along the auditory and visual processing hierarchies in a participant-specific fashion. In the auditory localizer, participants were presented with brief bursts of white noise at −10° or 10° visual angle (duration 500 ms, stimulus onset asynchrony 1 s). In a one-back task, participants indicated via a key press when the spatial location of the current trial was different from the previous trial. 20 s blocks of auditory conditions (i.e., 20 trials) alternated with 13 s fixation periods. The auditory locations were presented in a pseudorandomized fashion to optimize design efficiency. Similar to the main experiment, the auditory localizer sessions were modelled in an event-related fashion with the onset vectors of left and right auditory stimuli being entered into the design matrix after convolution with the hemodynamic response function and its first temporal derivative. Auditory responsive regions were defined as voxels in superior temporal and Heschl’s gyrus showing significant activations for auditory stimulation relative to fixation (<italic>p</italic> &lt; 0.05, family-wise error corrected). Within these regions, we defined primary auditory cortex (A1) based on cytoarchitectonic probability maps [<xref rid="pbio.1002073.ref046" ref-type="bibr">46</xref>] and referred to the remainder (i.e., planum temporale and posterior superior temporal gyrus) as higher-order auditory area (hA, see <xref rid="pbio.1002073.g003" ref-type="fig">Fig 3C</xref>).</p>
<p>Standard phase-encoded retinotopic mapping [<xref rid="pbio.1002073.ref047" ref-type="bibr">47</xref>] was used to define visual regions of interest (<ext-link ext-link-type="uri" xlink:href="http://sampendu.wordpress.com/retinotopy-tutorial/" xlink:type="simple">http://sampendu.wordpress.com/retinotopy-tutorial/</ext-link>). Participants viewed a checkerboard background flickering at 7.5 Hz through a rotating wedge aperture of 70° width (polar angle mapping) or an expanding/contracting ring (eccentricity mapping). The periodicity of the apertures was 42 s. Visual responses were modelled by entering a sine and cosine convolved with the hemodynamic response function as regressors in a general linear model. The preferred polar angle was determined as the phase lag for each voxel, which is the angle between the parameter estimates for the sine and the cosine. The preferred phase lags for each voxel were projected on the reconstructed, inflated cortical surface using Freesurfer 5.1.0 [<xref rid="pbio.1002073.ref048" ref-type="bibr">48</xref>]. Visual regions V1–V3, V3AB, and IPS0-IPS4 were defined as phase reversal in angular retinotopic maps. IPS0–4 were defined as contiguous, approximately rectangular regions based on phase reversals along the anatomical IPS [<xref rid="pbio.1002073.ref049" ref-type="bibr">49</xref>]. For the decoding analyses, the auditory and visual regions were combined from the left and right hemispheres.</p>
</sec>
</sec>
<sec id="sec015">
<title>Supporting Information</title>
<supplementary-material id="pbio.1002073.s001" xlink:href="info:doi/10.1371/journal.pbio.1002073.s001" mimetype="application/zip" position="float" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Zip file containing datasets underlying Figs. <xref rid="pbio.1002073.g002" ref-type="fig">2</xref> and <xref rid="pbio.1002073.g003" ref-type="fig">3D</xref>.</title>
<p>The data is stored in MATLAB structures.</p>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s002" xlink:href="info:doi/10.1371/journal.pbio.1002073.s002" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>BOLD activation (across-subjects mean ± SEM parameter estimates) as a function of visual reliability (VR) and visual location (V) (left subpanel), visual reliability and auditory location (A) (middle subpanel), and visual reliability and task-relevance (T) (right subpanel), separately for the regions of interest in the left (solid) and right (dotted) hemisphere.</title>
<p>Using singular value decomposition, parameter estimates were pooled across the voxels of each region of interest by averaging the first eigenvariate of voxel response patterns across the ten replications of each parameter estimate. Significant effects (<italic>p</italic> &lt; 0.05, cf. <xref rid="pbio.1002073.s004" ref-type="supplementary-material">S2 Table</xref>) are noted at the top of each subpanel for the left (L) and right (R) hemisphere.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s003" xlink:href="info:doi/10.1371/journal.pbio.1002073.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Statistical results of main and interaction effects of the 4 (visual location, V) × 4 (auditory location, A) × 2 (task-relevance, T) × 2 (visual reliability, VR) design on the accuracy of behavioral localization responses.</title>
<p><italic>p</italic> &lt; 0.05 in bold.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s004" xlink:href="info:doi/10.1371/journal.pbio.1002073.s004" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Statistical results of main and interaction effects of the factors task-relevance (T), visual reliability (VR), visual signal location (V), and auditory signal location (A) on the mean activation in the left (L) and right (R) regions of interest.</title>
<p>Significant effects (<italic>p</italic> &lt; 0.05) are marked in red. Degrees of freedom (df1, df2) for each effect are indicated below F and <italic>p</italic> values.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s005" xlink:href="info:doi/10.1371/journal.pbio.1002073.s005" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Individual R<sup>2</sup> and relative Bayesian information criterion of the Causal Inference model using the ‚model averaging’ (MA), ‚model selection’ (MS), and 'probability matching' (PM) decision strategies.</title>
<p>R<sup>2</sup> = coefficient of determination. relBIC = Bayesian information criterion (BIC = LL − 0.5 M ln(N), LL = log likelihood, M = number of parameters, N = number of data points) of a model relative to a participant’s best model (smaller relBIC indicates that a model provides a better explanation of a participant’s data).</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s006" xlink:href="info:doi/10.1371/journal.pbio.1002073.s006" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Model parameters, R<sup>2</sup> (across-subjects mean ± SEM) and group Bayesian information iriterion of the Bayesian Causal Inference model for the three different decision strategies.</title>
<p>Note: p<sub>C</sub> = prior common-source probability. σ<sub>P</sub> = standard deviation of the spatial prior (in °). σ<sub>A</sub> = standard deviation of the auditory likelihood (in °). σ<sub>V</sub> = standard deviation of the visual likelihood at two levels of visual reliability (1 = high, 2 = low) (in °). R<sup>2</sup> = coefficient of determination. relBIC<sub>Group</sub> = Bayesian information criterion at the group level, i.e., subject-specific BICs summed over all subjects (BIC = LL − 0.5 M ln(N), LL = log likelihood, M = number of parameters, N = number of data points) of a model relative to the best “model averaging” model (n.b. a smaller relBIC<sub>Group</sub> indicates that a model provides a better explanation of our data). EP = exceedance probability, i.e., probability that a model is more likely than any other model from the random effects model comparison (see <xref rid="sec005" ref-type="sec">Materials and Methods</xref> section in main paper).</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s007" xlink:href="info:doi/10.1371/journal.pbio.1002073.s007" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Posterior common-source probability of the fitted Causal Inference model (across-subjects mean ± SEM; “model averaging”) as a function of absolute audiovisual disparity and visual reliability.</title>
<p>Note: Visual reliability is the inverse of the visual variance determined by the standard deviation of the visual cloud of dots (i.e., 2° = high, 14° = low visual reliability). Both visual reliability and audiovisual disparity are specified in degree visual angle as units.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s008" xlink:href="info:doi/10.1371/journal.pbio.1002073.s008" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Decoding accuracies (across-subjects' mean correlation) for the four spatial estimates of the Bayesian Causal Inference model (i.e., model averaging) in the regions of interest.</title>
<p>Significant decoding accuracies (across-subjects' one sample <italic>t</italic>-tests against zero of Fisher z-transformed correlation coefficients) are marked by asterisks (*<italic>p</italic> &lt; 0.05; **<italic>p</italic> &lt; 0.01; ***<italic>p</italic> &lt; 0.001).</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.1002073.s009" xlink:href="info:doi/10.1371/journal.pbio.1002073.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supporting methods, results, and discussion.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank P Dayan, C Frith, and U Beierholm for very helpful discussions and comments on a previous version of this manuscript, W Penny for statistical advice, and P Ehses for advice on the fMRI sequence.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>BOLD</term>
<def><p>blood-oxygen-level dependent</p></def>
</def-item>
<def-item><term>fMRI</term>
<def><p>functional magnetic resonance imaging</p></def>
</def-item>
<def-item><term>MR</term>
<def><p>magnetic resonance</p></def>
</def-item>
<def-item><term>SEM</term>
<def><p>standard error of the mean</p></def>
</def-item>
<def-item><term>SVR</term>
<def><p>support vector regression</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.1002073.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name> (<year>2010</year>) <article-title>Causal inference in perception</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>425</fpage>–<lpage>432</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.07.001" xlink:type="simple">10.1016/j.tics.2010.07.001</ext-link></comment> <object-id pub-id-type="pmid">20705502</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koerding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS ONE</source> <volume>2</volume>: <fpage>e943</fpage>. <object-id pub-id-type="pmid">17895984</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Burr</surname> <given-names>D</given-names></name> (<year>2004</year>) <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Curr Biol</source> <volume>14</volume>: <fpage>257</fpage>–<lpage>262</lpage>. <object-id pub-id-type="pmid">14761661</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ernst</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name> (<year>2002</year>) <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source> <volume>415</volume>: <fpage>429</fpage>–<lpage>433</lpage>. <object-id pub-id-type="pmid">11807554</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hillis</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Watt</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Banks</surname> <given-names>MS</given-names></name> (<year>2004</year>) <article-title>Slant from texture and disparity cues: optimal cue combination</article-title>. <source>J Vis</source> <volume>4</volume>: <fpage>967</fpage>–<lpage>992</lpage>. <object-id pub-id-type="pmid">15669906</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jacobs</surname> <given-names>RA</given-names></name> (<year>1999</year>) <article-title>Optimal integration of texture and motion cues to depth</article-title>. <source>Vision Res</source> <volume>39</volume>: <fpage>3621</fpage>−<lpage>3629</lpage>. <object-id pub-id-type="pmid">10746132</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Saunders</surname> <given-names>JA</given-names></name> (<year>2003</year>) <article-title>Do humans optimally integrate stereo and texture information for judgments of surface slant?</article-title> <source>Vision Res</source> <volume>43</volume>: <fpage>2539</fpage>–<lpage>2558</lpage>. <object-id pub-id-type="pmid">13129541</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Beers</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Haggard</surname> <given-names>P</given-names></name> (<year>2002</year>) <article-title>When feeling is more important than seeing in sensorimotor adaptation</article-title>. <source>Curr Biol</source> <volume>12</volume>: <fpage>834</fpage>–<lpage>837</lpage>. <object-id pub-id-type="pmid">12015120</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Maloney</surname> <given-names>LT</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>EB</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>M</given-names></name> (<year>1995</year>) <article-title>Measurement and modeling of depth cue combination: in defense of weak fusion</article-title>. <source>Vision Res</source> <volume>35</volume>: <fpage>389</fpage>–<lpage>412</lpage>. <object-id pub-id-type="pmid">7892735</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roach</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Heron</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McGraw</surname> <given-names>PV</given-names></name> (<year>2006</year>) <article-title>Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration</article-title>. <source>Proc Biol Sci</source> <volume>273</volume>: <fpage>2159</fpage>–<lpage>2168</lpage>. <object-id pub-id-type="pmid">16901835</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Radeau</surname> <given-names>M</given-names></name> (<year>1981</year>) <article-title>Cross-modal bias and perceptual fusion with auditory-visual spatial discordance</article-title>. <source>Attention, Perception, &amp; Psychophysics</source> <volume>29</volume>: <fpage>578</fpage>–<lpage>584</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/xhp0000031" xlink:type="simple">10.1037/xhp0000031</ext-link></comment> <object-id pub-id-type="pmid">25621585</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallace</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Roberson</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Hairston</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Stein</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Vaughan</surname> <given-names>JW</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Unifying multisensory signals across time and space</article-title>. <source>Exp Brain Res</source> <volume>158</volume>: <fpage>252</fpage>–<lpage>258</lpage>. <object-id pub-id-type="pmid">15112119</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tian</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Reser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Durham</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kustov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rauschecker</surname> <given-names>JP</given-names></name> (<year>2001</year>) <article-title>Functional specialization in rhesus monkey auditory cortex</article-title>. <source>Science</source> <volume>292</volume>: <fpage>290</fpage>–<lpage>293</lpage>. <object-id pub-id-type="pmid">11303104</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>, <name name-style="western"><surname>Macko</surname> <given-names>KA</given-names></name> (<year>1983</year>) <article-title>Object vision and spatial vision: two cortical pathways</article-title>. <source>Trends in neurosciences</source> <volume>6</volume>: <fpage>414</fpage>–<lpage>417</lpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>O’Connell</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Mills</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name> (<year>2007</year>) <article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex</article-title>. <source>Neuron</source> <volume>53</volume>: <fpage>279</fpage>–<lpage>292</lpage>. <object-id pub-id-type="pmid">17224408</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Petkov</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Augath</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name> (<year>2007</year>) <article-title>Functional imaging reveals visual modulation of specific fields in auditory cortex</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>1824</fpage>–<lpage>1835</lpage>. <object-id pub-id-type="pmid">17314280</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name> (<year>2010</year>) <article-title>Audiovisual synchrony improves motion discrimination via enhanced connectivity between early visual and auditory areas</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>12329</fpage>–<lpage>12339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5745-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5745-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20844129</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werner</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name> (<year>2010</year>) <article-title>Distinct functional contributions of primary sensory and association areas to audiovisual integration in object categorization</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>2662</fpage>–<lpage>2675</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5091-09.2010" xlink:type="simple">10.1523/JNEUROSCI.5091-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20164350</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Noppeney</surname> <given-names>U</given-names></name> (<year>2014</year>) <article-title>Temporal prediction errors in visual and auditory cortices</article-title>. <source>Curr Biol</source> <volume>24</volume>: <fpage>R309</fpage>–<lpage>R310</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2014.02.007" xlink:type="simple">10.1016/j.cub.2014.02.007</ext-link></comment> <object-id pub-id-type="pmid">24735850</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Foxe</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Morocz</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Higgins</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Javitt</surname> <given-names>DC</given-names></name>, <etal>et al</etal>. (<year>2000</year>) <article-title>Multisensory auditory-somatosensory interactions in early cortical processing revealed by high-density electrical mapping</article-title>. <source>Brain Res Cogn Brain Res</source> <volume>10</volume>: <fpage>77</fpage>–<lpage>83</lpage>. <object-id pub-id-type="pmid">10978694</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rieger</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Schoenfeld</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kanowski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hinrichs</surname> <given-names>H</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Audiovisual temporal correspondence modulates human multisensory superior temporal sulcus plus primary sensory cortices</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>11431</fpage>–<lpage>11441</lpage>. <object-id pub-id-type="pmid">17942738</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name> (<year>2012</year>) <article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title>. <source>Nat Neurosci</source> <volume>15</volume>: <fpage>146</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2983" xlink:type="simple">10.1038/nn.2983</ext-link></comment> <object-id pub-id-type="pmid">22101645</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morgan</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Deangelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name> (<year>2008</year>) <article-title>Multisensory integration in macaque visual cortex depends on cue reliability</article-title>. <source>Neuron</source> <volume>59</volume>: <fpage>662</fpage>–<lpage>673</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.06.024" xlink:type="simple">10.1016/j.neuron.2008.06.024</ext-link></comment> <object-id pub-id-type="pmid">18760701</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fetsch</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Deangelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name> (<year>2013</year>) <article-title>Bridging the gap between theories of sensory cue integration and the physiology of multisensory neurons</article-title>. <source>Nat Rev Neurosci</source> <volume>14</volume>: <fpage>429</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3503" xlink:type="simple">10.1038/nrn3503</ext-link></comment> <object-id pub-id-type="pmid">23686172</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2006</year>) <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>1432</fpage>–<lpage>1438</lpage>. <object-id pub-id-type="pmid">17057707</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchamp</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Argall</surname> <given-names>BD</given-names></name>, <name name-style="western"><surname>Bodurka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Duyn</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>A</given-names></name> (<year>2004</year>) <article-title>Unraveling multisensory integration: patchy organization within human STS multisensory cortex</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>1190</fpage>–<lpage>1192</lpage>. <object-id pub-id-type="pmid">15475952</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bonath</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mishra</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schwiecker</surname> <given-names>K</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Neural basis of the ventriloquist illusion</article-title>. <source>Curr Biol</source> <volume>17</volume>: <fpage>1697</fpage>–<lpage>1703</lpage>. <object-id pub-id-type="pmid">17884498</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghazanfar</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name> (<year>2006</year>) <article-title>Is neocortex essentially multisensory?</article-title> <source>Trends Cogn Sci</source> <volume>10</volume>: <fpage>278</fpage>–<lpage>285</lpage>. <object-id pub-id-type="pmid">16713325</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Driver</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name> (<year>2003</year>) <article-title>Multimodal spatial representations engaged in human parietal cortex during both saccadic and manual spatial orienting</article-title>. <source>Curr Biol</source> <volume>13</volume>: <fpage>990</fpage>–<lpage>999</lpage>. <object-id pub-id-type="pmid">12814544</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hein</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Doehrmann</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Kaiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Muckli</surname> <given-names>L</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Object familiarity and semantic congruency modulate responses in cortical audiovisual integration areas</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>7881</fpage>–<lpage>7887</lpage>. <object-id pub-id-type="pmid">17652579</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bonath</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Noesselt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Krauel</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tyll</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tempelmann</surname> <given-names>C</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Audio-visual synchrony modulates the ventriloquist illusion and its neural/spatial representation in the auditory cortex</article-title>. <source>Neuroimage</source> <volume>98</volume>: <fpage>25</fpage>–<lpage>434</lpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Colby</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>ME</given-names></name> (<year>1999</year>) <article-title>Space and attention in parietal cortex</article-title>. <source>Annu Rev Neurosci</source> <volume>22</volume>: <fpage>319</fpage>−<lpage>349</lpage>. <object-id pub-id-type="pmid">10202542</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gottlieb</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Kusunoki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>ME</given-names></name> (<year>1998</year>) <article-title>The representation of visual salience in monkey parietal cortex</article-title>. <source>Nature</source> <volume>391</volume>: <fpage>481</fpage>–<lpage>484</lpage>. <object-id pub-id-type="pmid">9461214</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shomstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name> (<year>2004</year>) <article-title>Control of attention shifts between vision and audition in human cortex</article-title>. <source>J Neurosci</source> <volume>24</volume>: <fpage>10702</fpage>−<lpage>10706</lpage>. <object-id pub-id-type="pmid">15564587</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Szczepanski</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Konen</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name> (<year>2010</year>) <article-title>Mechanisms of spatial attention control in frontal and parietal cortex</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>148</fpage>–<lpage>160</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3862-09.2010" xlink:type="simple">10.1523/JNEUROSCI.3862-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20053897</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Talsma</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Senkowski</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Soto-Faraco</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Woldorff</surname> <given-names>MG</given-names></name> (<year>2010</year>) <article-title>The multifaceted interplay between attention and multisensory integration</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>400</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.06.008" xlink:type="simple">10.1016/j.tics.2010.06.008</ext-link></comment> <object-id pub-id-type="pmid">20675182</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref037"><label>37</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Algazi</surname> <given-names>VR</given-names></name>, <name name-style="western"><surname>Duda</surname> <given-names>RO</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Avendano</surname> <given-names>C</given-names></name> (<year>2001</year>) <chapter-title>The cipic hrtf database</chapter-title>. <publisher-name>IEEE</publisher-name>. pp. <fpage>99</fpage>−<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/S1463924601000141" xlink:type="simple">10.1155/S1463924601000141</ext-link></comment> <object-id pub-id-type="pmid">18968351</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name> (<year>1997</year>) <article-title>The psychophysics toolbox</article-title>. <source>Spatial vision</source> <volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wozny</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Beierholm</surname> <given-names>UR</given-names></name>, <name name-style="western"><surname>Shams</surname> <given-names>L</given-names></name> (<year>2010</year>) <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000871</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000871" xlink:type="simple">10.1371/journal.pcbi.1000871</ext-link></comment> <object-id pub-id-type="pmid">20700493</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagelkerke</surname> <given-names>NJ</given-names></name> (<year>1991</year>) <article-title>A note on a general definition of the coefficient of determination</article-title>. <source>Biometrika</source> <volume>78</volume>: <fpage>691</fpage>–<lpage>692</lpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name> (<year>1995</year>) <article-title>Bayesian model selection in social research</article-title>. <source>Sociological Methodology</source> <volume>25</volume>: <fpage>111</fpage>–<lpage>163</lpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Bayesian model selection for group studies</article-title>. <source>Neuroimage</source> <volume>46</volume>: <fpage>1004</fpage>−<lpage>1017</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2009.03.025" xlink:type="simple">10.1016/j.neuroimage.2009.03.025</ext-link></comment> <object-id pub-id-type="pmid">19306932</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Holmes</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Worsley</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Poline</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>, <etal>et al</etal>. (<year>1994</year>) <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>. <source>Human brain mapping</source> <volume>2</volume>: <fpage>189</fpage>–<lpage>210</lpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>CJ</given-names></name> (<year>2011</year>) <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Transactions on Intelligent Systems and Technology (TIST)</source> <volume>2</volume>: <fpage>27</fpage>.</mixed-citation></ref>
<ref id="pbio.1002073.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Efron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>RJ</given-names></name> (<year>1994</year>) <chapter-title>An introduction to the bootstrap</chapter-title>. <publisher-loc>London</publisher-loc>: <publisher-name>Chapmann and Hall</publisher-name>. <object-id pub-id-type="pmid">25144107</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eickhoff</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Mohlberg</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Grefkes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>GR</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>Neuroimage</source> <volume>25</volume>: <fpage>1325</fpage>–<lpage>1335</lpage>. <object-id pub-id-type="pmid">15850749</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Reppas</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Belliveau</surname> <given-names>JW</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source> <volume>268</volume>: <fpage>889</fpage>–<lpage>893</lpage>. <object-id pub-id-type="pmid">7754376</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Fischl</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name> (<year>1999</year>) <article-title>Cortical surface-based analysis. I. Segmentation and surface reconstruction</article-title>. <source>Neuroimage</source> <volume>9</volume>: <fpage>179</fpage>–<lpage>194</lpage>. <object-id pub-id-type="pmid">9931268</object-id></mixed-citation></ref>
<ref id="pbio.1002073.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swisher</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Halko</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Merabet</surname> <given-names>LB</given-names></name>, <name name-style="western"><surname>McMains</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Somers</surname> <given-names>DC</given-names></name> (<year>2007</year>) <article-title>Visual topography of human intraparietal sulcus</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>5326</fpage>–<lpage>5337</lpage>. <object-id pub-id-type="pmid">17507555</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>