<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00485</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002759</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory perception</subject>
              <subj-group>
                <subject>Psychoacoustics</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Auditory system</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Music in Our Ears: The Biological Bases of Musical Timbre Perception</article-title>
        <alt-title alt-title-type="running-head">Biological Bases of Musical Timbre Perception</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Patil</surname>
            <given-names>Kailash</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Pressnitzer</surname>
            <given-names>Daniel</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Shamma</surname>
            <given-names>Shihab</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Elhilali</surname>
            <given-names>Mounya</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Department of Electrical and Computer Engineering, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, Maryland, United States of America</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Laboratoire Psychologie de la Perception, CNRS-Université Paris Descartes &amp; DEC, Ecole normale supérieure, Paris, France</addr-line>
      </aff>
      <aff id="aff3">
        <label>3</label>
        <addr-line>Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park, Maryland, United States of America</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Theunissen</surname>
            <given-names>Frederic E.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>University of California at Berkeley, United States of America</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mounya@jhu.edu</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: KP DP SS ME. Performed the experiments: KP DP SS ME. Analyzed the data: KP DP SS ME. Contributed reagents/materials/analysis tools: KP DP SS ME. Wrote the paper: KP DP SS ME.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>11</month>
        <year>2012</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>1</day>
        <month>11</month>
        <year>2012</year>
      </pub-date>
      <volume>8</volume>
      <issue>11</issue>
      <elocation-id>e1002759</elocation-id>
      <history>
        <date date-type="received">
          <day>23</day>
          <month>3</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>12</day>
          <month>9</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2012</copyright-year>
        <copyright-holder>Patil et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Timbre is the attribute of sound that allows humans and other animals to distinguish among different sound sources. Studies based on psychophysical judgments of musical timbre, ecological analyses of sound's physical characteristics as well as machine learning approaches have all suggested that timbre is a multifaceted attribute that invokes both spectral and temporal sound features. Here, we explored the neural underpinnings of musical timbre. We used a neuro-computational framework based on spectro-temporal receptive fields, recorded from over a thousand neurons in the mammalian primary auditory cortex as well as from simulated cortical neurons, augmented with a nonlinear classifier. The model was able to perform robust instrument classification irrespective of pitch and playing style, with an accuracy of 98.7%. Using the same front end, the model was also able to reproduce perceptual distance judgments between timbres as perceived by human listeners. The study demonstrates that joint spectro-temporal features, such as those observed in the mammalian primary auditory cortex, are critical to provide the rich-enough representation necessary to account for perceptual judgments of timbre by human listeners, as well as recognition of musical instruments.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Music is a complex acoustic experience that we often take for granted. Whether sitting at a symphony hall or enjoying a melody over earphones, we have no difficulty identifying the instruments playing, following various beats, or simply distinguishing a flute from an oboe. Our brains rely on a number of sound attributes to analyze the music in our ears. These attributes can be straightforward like loudness or quite complex like the identity of the instrument. A major contributor to our ability to recognize instruments is what is formally called ‘timbre’. Of all perceptual attributes of music, timbre remains the most mysterious and least amenable to a simple mathematical abstraction. In this work, we examine the neural underpinnings of musical timbre in an attempt to both define its perceptual space and explore the processes underlying timbre-based recognition. We propose a scheme based on responses observed at the level of mammalian primary auditory cortex and show that it can accurately predict sound source recognition and perceptual timbre judgments by human listeners. The analyses presented here strongly suggest that rich representations such as those observed in auditory cortex are critical in mediating timbre percepts.</p>
      </abstract>
      <funding-group>
        <funding-statement>This work was partly supported by grants from NSF CAREER IIS-0846112, AFOSR FA9550-09-1-0234, NIH 1R01AG036424-01 and ONR N000141010278. S. Shamma was partly supported by a Blaise-Pascal Chair, Région Ile de France, and by the program Research in Paris, Mairie de Paris. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="16"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>A fundamental role of auditory perception is to infer the likely source of a sound; for instance to identify an animal in a dark forest, or to recognize a familiar voice on the phone. Timbre, often referred to as the color of sound, is believed to play a key role in this recognition process <xref ref-type="bibr" rid="pcbi.1002759-Handel1">[1]</xref>. Though timbre is an intuitive concept, its formal definition is less so. The ANSI definition of timbre describes it as that attribute that allows us to distinguish between sounds having the same perceptual duration, loudness, and pitch, such as two different musical instruments playing exactly the same note <xref ref-type="bibr" rid="pcbi.1002759-Ansi1">[2]</xref>. In other words, it is neither duration, nor loudness, nor pitch; but is likely “everything else”.</p>
      <p>As has been often been pointed out, this definition by the negative does not state what are the perceptual dimensions underlying timbre perception. Spectrum is obviously a strong candidate: physical objects produce sounds with a spectral profile that reflects their particular sets of vibration modes and resonances <xref ref-type="bibr" rid="pcbi.1002759-Helmholtz1">[3]</xref>. Measures of spectral shape have thus been proposed as basic dimensions of timbre (e.g., formant position for voiced sounds in speech, sharpness, and brightness) <xref ref-type="bibr" rid="pcbi.1002759-Grey1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-McAdams1">[5]</xref>. But timbre is not only spectrum, as changes of amplitude over time, the so-called temporal envelope, also have strong perceptual effects <xref ref-type="bibr" rid="pcbi.1002759-Patterson1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-krumhansl1">[7]</xref>. To identify the most salient timbre dimensions, statistical techniques such as multidimensional scaling have been used: perceptual differences between sound samples were collected and the underlying dimensionality of the timbre space inferred <xref ref-type="bibr" rid="pcbi.1002759-McAdams2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Grey2">[9]</xref>. These studies suggest a combination of spectral <italic>and</italic> temporal dimensions to explain the perceptual distance judgments, but the precise nature of these dimensions varies across studies and sound sets <xref ref-type="bibr" rid="pcbi.1002759-JABurgoyne1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Donnadieu1">[11]</xref>. Importantly, almost all timbre dimensions that have been proposed to date on the basis of psychophysical studies <xref ref-type="bibr" rid="pcbi.1002759-Peeters1">[12]</xref> are either purely spectral or purely temporal. The only spectro-temporal aspect of sound that has been considered in this context is related to the asynchrony of partials around the onset of a sound (8,9), but the salience of this spectro-temporal dimension was found to be weak and context-dependent <xref ref-type="bibr" rid="pcbi.1002759-Caclin1">[13]</xref>.</p>
      <p>Technological approaches, not concerned with biology nor human perception, have explored much richer feature representations that span both spectral, temporal, and spectro-temporal dimensions. The motivation for these engineering techniques is an accurate recognition of specific sounds or acoustic events in a variety of applications (e.g. automatic speech recognition; voice detection; music information retrieval; target tracking in multisensor networks and surveillance systems; medical diagnosis, etc.). Myriad spectral features have been proposed for audio content analysis, ranging from simple summary statistics of spectral shape (e.g. spectral amplitude, peak, centroid, flatness) to more elaborate descriptions of spectral information such as Mel-Frequency Cepstral Coefficients (MFCC) and Linear or Perceptual Predictive Coding (LPC or PLP) <xref ref-type="bibr" rid="pcbi.1002759-Waibel1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1002759-Rabiner1">[16]</xref>. Such metrics have often been augmented with temporal information, which was found to improve the robustness of content identification <xref ref-type="bibr" rid="pcbi.1002759-McKinney1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Lerch1">[18]</xref>. Common modeling of temporal dynamics also ranged from simple summary statistics such as onsets, attack time, velocity, acceleration and higher-order moments to more sophisticated statistical temporal modeling using Hidden Markov Models, Artificial Neural Networks, Adaptive Resonance Theory models, Liquid State Machine systems and Self-Organizing Maps <xref ref-type="bibr" rid="pcbi.1002759-HerreraBoyer1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Burred1">[20]</xref>. Overall, the choice of features was very dependent on the task at hand, the complexity of the dataset, and the desired performance level and robustness of the system.</p>
      <p>Complementing perceptual and technological approaches, brain-imaging techniques have been used to explore the neural underpinnings of timbre perception. Correlates of musical timbre dimensions suggested by multidimensional scaling studies have been observed using event-related potentials <xref ref-type="bibr" rid="pcbi.1002759-Caclin2">[21]</xref>. Other studies have attempted to identify the neural substrates of natural sound recognition, by looking for brain areas that would be selective to specific sound categories, such as voice-specific regions in secondary cortical areas <xref ref-type="bibr" rid="pcbi.1002759-Belin1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Uppenkamp1">[23]</xref> and other sound categories such as tools <xref ref-type="bibr" rid="pcbi.1002759-Lewis1">[24]</xref> or musical instruments <xref ref-type="bibr" rid="pcbi.1002759-Leaver1">[25]</xref>. A hierarchical model consistent with these findings has been proposed in which selectivity to different sound categories is refined as one climbs the processing chain <xref ref-type="bibr" rid="pcbi.1002759-DeLucia1">[26]</xref>. An alternative, more distributed scheme has also been suggested <xref ref-type="bibr" rid="pcbi.1002759-Staeren1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Formisano1">[28]</xref>, which includes the contribution of low-level cues to the large perceptual differences between these high-level sound categories.</p>
      <p>A common issue for the psychophysical, technological, and neurophysiological investigations of timbre is that the generality of the results is mitigated by the particular characteristics of the sound set used. For multi-dimensional scaling behavioral studies, by construction, the dimensions found will be the most salient within the sound set; but they may not capture other dimensions which could nevertheless be crucial for the recognition of sounds outside the set. For engineering studies, dimensions may be designed arbitrarily as long as they afford good performance in a specific task. For the imaging studies, there is no suggestion yet as to which low-level acoustic features may be used to construct the various selectivity for high-level categories while preserving invariance within a category. Furthermore, there is a major gap between these studies and what is known from electrophysiological recordings in animal models. Decades of work have established that auditory cortical responses display rich and complex spectro-temporal receptive fields, even within primary areas <xref ref-type="bibr" rid="pcbi.1002759-Miller1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Atencio1">[30]</xref>. This seems at odds with the limited set of spectral or temporal dimensions that are classically used to characterize timbre in perceptual studies.</p>
      <p>To bridge this gap, we investigate how cortical processing of spectro-temporal modulations can subserve both sound source recognition of musical instruments and perceptual timbre judgments. Specifically, cortical receptive fields and computational models derived from them are shown to be suited to classify a sound source from its evoked neural activity, across a wide range of instruments, pitches and playing styles, and also to predict accurately human judgments of timbre similarities</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Cortical processing of complex musical sounds</title>
        <p>Responses in primary auditory cortex (A1) exhibit rich selectivity that extends beyond the tonotopy observed in the auditory nerve. A1 neurons are not only tuned to the spectral energy at a given frequency, but also to the specifics of the local spectral shape such as its bandwidth <xref ref-type="bibr" rid="pcbi.1002759-Schreiner1">[31]</xref>, spectral symmetry <xref ref-type="bibr" rid="pcbi.1002759-Versnel1">[32]</xref>, and temporal dynamics <xref ref-type="bibr" rid="pcbi.1002759-Schreiner2">[33]</xref> (<xref ref-type="fig" rid="pcbi-1002759-g001">Figure 1</xref>). Put together, one can view the resulting representation of sound in A1 as a <italic>multidimensional</italic> mapping that spans at least three dimensions: (1) <italic>Best frequencies</italic> that span the entire auditory range; (2) <italic>Spectral shapes</italic> (including bandwidth and symmetry) that span a wide range from very broad (2–3 octaves) to narrowly tuned (&lt;0.25 octaves); and (3) <italic>Dynamics</italic> that range from very slow to relatively fast (1–30 Hz).</p>
        <fig id="pcbi-1002759-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Neurophysiological receptive fields.</title>
            <p>Each panel shows the receptive field of 1 neuron with red indicating excitatory (preferred) responses, and blue indicating inhibitory (suppressed) responses. Examples vary from narrowly tuned neurons (top row) to broadly tuned ones (middle and bottom row). They also highlight variability in temporal dynamics and orientation (upward or downward sweeps).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>This rich cortical mapping may reflect an elegant strategy for extracting acoustic cues that subserve the perception of various acoustic attributes (pitch, loudness, location, and timbre) as well as the recognition of complex sound objects, such as different musical instruments. This hypothesis was tested here by employing a database of spectro-temporal receptive fields (STRFs) recorded from 1110 single units in primary auditory cortex of 15 awake non-behaving ferrets. These receptive fields are <italic>linear</italic> descriptors of the selectivity of each cortical neuron to the spectral and temporal modulations evident in the cochlear “spectrogram-like” representation of complex acoustic signals that emerges in the auditory periphery. Such STRFs (with a variety of nonlinear refinements) have been shown to capture and predict well cortical responses to a variety of complex sounds like speech, music, and modulated noise <xref ref-type="bibr" rid="pcbi.1002759-Theunissen1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1002759-Sadagopan1">[38]</xref>.</p>
        <p>To test the efficacy of STRFs in generating a representation of sound that can distinguish among a variety of complex categories, sounds from a large database of musical instruments were mapped onto cortical responses using the physiological STRFs described above. The time-frequency spectrogram for each note was convolved with each STRF in our neurophysiology database to yield a firing rate that is then integrated over time. This initial mapping was then reduced in dimensionality using singular value decomposition to a compact eigen-space; then augmented with a nonlinear statistical analysis using support vector machine (SVM) with Gaussian kernels <xref ref-type="bibr" rid="pcbi.1002759-Cristianini1">[39]</xref> (see <xref ref-type="sec" rid="s4">METHODS</xref> for details). Briefly, support vector machines are classifiers that learn to separate, in our specific case, the patterns of cortical responses induced by the different instruments. The use of Gaussian kernels is a standard technique that allows to map the data from its original space (where data may not be linearly separable) onto a new representational space that is linearly separable. Ultimately, the analysis constructed a set of hyperplanes that outline the boundaries between different instruments. The identity of a new sample was then defined based on its configuration in this expanded space relative to the set of learned hyperplanes (<xref ref-type="fig" rid="pcbi-1002759-g002">Figure 2</xref>).</p>
        <fig id="pcbi-1002759-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Schematic of the timbre recognition model.</title>
            <p>An acoustic waveform from a test instrument is processed through a model of cochlear and midbrain processing; yielding a time-frequency representation called auditory spectrogram. This later is further processed through the cortical processing stage through neurophysiological or model spectro-temporal receptive fields. Cortical responses of the target instrument are tested against boundaries of a statistical SVM timbre model in order to identify the instrument's identity.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g002" position="float" xlink:type="simple"/>
        </fig>
        <p>Based on the configuration above and a 10% cross-validation technique, the model trained using the physiological cortical receptive fields achieved a classification accuracy of <bold>87.22%±0.81</bold> (the number following the mean accuracy represents standard deviation, see <xref ref-type="table" rid="pcbi-1002759-t001">Table 1</xref>). Remarkably, this result was obtained with a large database of 11 instruments playing between 30 and 90 different pitches with 3 to 19 playing styles (depending on the instrument), 3 style dynamics (mezzo, forte and piano), and 3 manufacturers for each instrument (an average of 1980 notes/instrument). This high classification accuracy was a strong indicator that neural processing at the level of primary auditory cortex could not only provide a basis for distinguishing between different instruments, but also had a robust invariant representation of instruments over a wide range of pitches and playing styles.</p>
        <table-wrap id="pcbi-1002759-t001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.t001</object-id>
          <label>Table 1</label>
          <caption>
            <title>Classification performance for the different models.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002759-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.t001" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1"/>
                  <td align="left" rowspan="1" colspan="1">Mean</td>
                  <td align="left" rowspan="1" colspan="1">STD</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Auditory Spectrum (Gaussian kernel SVM)</td>
                  <td align="left" rowspan="1" colspan="1">79.1%</td>
                  <td align="left" rowspan="1" colspan="1">0.7%</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Neurophysiological STRFs (Gaussian kernel SVM)</td>
                  <td align="left" rowspan="1" colspan="1">87.2%</td>
                  <td align="left" rowspan="1" colspan="1">0.8%</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Full Cortical Model (Linear SVM)</td>
                  <td align="left" rowspan="1" colspan="1">96.2%</td>
                  <td align="left" rowspan="1" colspan="1">0.5%</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Full Cortical Model (Gaussian kernel SVM)</td>
                  <td align="left" rowspan="1" colspan="1">98.7%</td>
                  <td align="left" rowspan="1" colspan="1">0.2%</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="nt101">
              <p>The middle column indicates the mean of the accuracy scores for the 10 fold cross validation experiment and the right column indicates their standard deviation. Models differ either in their feature set (e.g. full cortical model versus auditory spectrogram) or in the classifier used (linear SVM versus Gaussian kernel SVM).</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="s2b">
        <title>The cortical model</title>
        <p>Despite the encouraging results obtained using cortical receptive fields, the classification based on neurophysiological recordings was hampered by various shortcomings including recording noise and other experimental constraints. Also, the limited selection of receptive fields (being from ferrets) tended to under-represent parameter ranges relevant to humans such as lower frequencies, narrow bandwidths (limited to a maximum resolution of 1.2 octaves), and coarse sampling of STRF dynamics.</p>
        <p>To circumvent these biases, we employed a model that mimics the basic transformations along the auditory pathway up to the level of A1. Effectively, the model mapped the one-dimensional acoustic waveform onto a multidimensional feature space. Importantly, the model allowed us to sample the cortical space more uniformly than physiological data available to us, in line with findings in the literature <xref ref-type="bibr" rid="pcbi.1002759-Miller1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Atencio1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Elhilali2">[40]</xref>.</p>
        <p>The model operates by first mapping the acoustic signal into an auditory spectrogram. This initial transformation highlights the time varying spectral energies of different instruments which is at the core of most acoustic correlates and machine learning analyses of musical timbre <xref ref-type="bibr" rid="pcbi.1002759-McAdams1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Donnadieu1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Caclin1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Livshin1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Burred2">[42]</xref>. For instance, temporal features in a musical note include fast dynamics that reflect the quality of the sound (scratchy, whispered, or purely voiced), as well as slower modulations that carry nuances of musical timbre such as attack and decay times, subtle fluctuations of pitch (vibrato) or amplitude (shimmer). Some of these characteristics can be readily seen in the auditory spectrograms, but many are only implicitly represented. For example, <xref ref-type="fig" rid="pcbi-1002759-g003">Figure 3A</xref> contrasts the auditory spectrogram of a piano vs. violin note. For violin, the temporal cross-section reflects the soft onset and sustained nature of bowing and typical vibrato fluctuations; the spectral slice captures the harmonic structure of the musical note with the overall envelope reflecting the resonances of the violin body. By contrast, the temporal and spectral modulations of a piano (playing the same note) are quite different. Temporally, the onset of piano rises and falls much faster, and its spectral envelope is much smoother.</p>
        <fig id="pcbi-1002759-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Spectro-temporal modulation profiles highlighting timbre differences between piano and violin notes.</title>
            <p>(<bold>A</bold>) The plot shows the time-frequency auditory spectrogram of piano and violin notes. The temporal and spectral slices shown on the right are marked. (<bold>B</bold>) The plots show magnitude cortical responses of four piano notes (left panels), played in normal (left) and Staccato (right) at F4 (top) and F#4 (bottom); and four violin notes (right panels), played in normal (left) and Pizzicatto (right) also at pitch F4(top) and F#4 (bottom). The white asterisks (upper leftmost notes in each quadruplet) indicate the notes shown in part (<bold>A</bold>) of this figure.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g003" position="float" xlink:type="simple"/>
        </fig>
        <p>The cortical stage of the auditory model further analyzes the spectral and temporal modulations of the spectrogram along multiple spectral and temporal resolutions. The model projects the auditory spectrogram onto a 4-dimensional space, representing time, tonotopic frequency, spectral modulations (or scales) and temporal modulations (or rates). The four dimensions of the cortical output can be interpreted in various ways. In one view, the cortical model output is a parallel repeated representation of the auditory spectrogram viewed at different resolutions. A different view is one of a bank of spectral and temporal modulation filters with different tuning (from narrowband to broadband spectrally, and slow to fast modulations temporally). In such view, the cortical representation is a display of spectro-temporal modulations of each channel as they evolve over time. Ultimately each filter acts as a model cortical neuron whose output reflects the tuning of that neuronal site. The model employed here had 30,976 filters (128freq×22 rates×11 scales), hence allowing us to obtain a full uniform coverage of the cortical space and bypassing the limitations of neurophysiological data. Note that we are not suggesting that ∼30 K neurons are needed for timbre classification, as the feature space is reduced in further stages of the model (see below). We have not performed an analysis of the number of neurons needed for such task. Nonetheless, a large and uniform sampling of the space seemed desirable.</p>
        <p>By collapsing the cortical display over frequency and averaging over time, one would obtain a two-dimensional display that preserves the “global” distribution of modulations over the remaining two dimensions of scale and rates. This “scale-rate” view is shown in <xref ref-type="fig" rid="pcbi-1002759-g003">Figure 3B</xref> for the same piano and violin notes in <xref ref-type="fig" rid="pcbi-1002759-g003">Figure 3A</xref> as well as others. Each instrument here is played at two distinct pitches with two different playing styles. The panels provide estimates of the overall distribution of spectro-temporal modulation of each sound. The left panel highlights the fact that the violin vibrato concentrates its peak energy near 6 Hz (across all pitches and styles); which matches the speed of pulsating pitch change caused by the rhythmic rate of 6 pulses per second chosen for the vibrato of this violin note. By contrast, the rapid onset of piano distributes its energy across a wider range of temporal modulations. Similarly, the unique pattern of peaks and valleys in spectral envelopes of each instrument produces a broad distribution along the spectral modulation axis, with the violin's sharper spectral peaks activating higher spectral modulations while the piano's smoother profile activates broad bandwidths. Each instrument, therefore, produces a correspondingly unique spectro-temporal activation pattern that could potentially be used to recognize it or distinguish it from others.</p>
      </sec>
      <sec id="s2c">
        <title>Musical timbre classification</title>
        <p>Several computational models were compared in the same classification task analysis of the database of musical instruments as described earlier with real neurophysiological data. <xref ref-type="sec" rid="s2">Results</xref> comparing all models are summarized in <xref ref-type="table" rid="pcbi-1002759-t001">Table 1</xref>. For what we refer to as the full model, we used the 4-D cortical model. The analysis started with a linear mapping through the model receptive fields, followed by dimensionality reduction and statistical classification using support vector machines with re-optimized Gaussian kernels (see <xref ref-type="sec" rid="s4">Methods</xref>). Tests used a 10% cross-validation method. The cortical model yielded an excellent classification accuracy of <bold>98.7%±0.2</bold>.</p>
        <p>We also explored the use of linear support vector machine, by bypassing the use of the Gaussian kernel. We performed a classification of instruments using the cortical responses obtained from the model receptive fields and a linear SVM. After optimization of the decision boundaries, we obtained an accuracy of <bold>96.2%±0.5</bold>. This result supports our initial assessment that the cortical space does indeed capture most of the subtleties that are unique to a common instrument but distinct between different classes. It is mostly the richness of the representation that underlies the classification performance: only a small improvement in accuracy is observed by adding the non-linear warping in the full model.</p>
        <p>In order to better understand the contribution of the cortical analysis beyond the time-frequency representation, we explored reduced versions of the full model. First we performed the timbre classification task using the auditory spectrogram as input. The feature spectra were obtained by processing the time waveform of each note through the cochlear-like filterbank front-end and averaging the auditory spectrograms over time, yielding a one-dimensional spectral profile for each note. These were then processed through the same statistical SVM model, with Gaussian functions optimized for this new representation using the exact same methods as used for cortical features. The classification accuracy for the spectral slices with SVM optimization attained a good but limited accuracy of <bold>79.1%±0.7</bold>. It is expected that a purely spectral model would not be able to classify all instruments. Whereas basic instrument classes differing by their physical characteristics (wind, percussion, strings) may have the potential to produce different spectral shapes, preserved in the spectral vector, more subtle differences in the temporal domain should prove difficult to recognize on this basis (see <xref ref-type="fig" rid="pcbi-1002759-g004">Figure 4</xref>). We shall revisit this issue of contribution and interactions between spectral and temporal features later (see Control Experiments section).</p>
        <fig id="pcbi-1002759-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>The confusion matrix for instrument classification using the auditory spectrum.</title>
            <p>Each row sums to 100% classification (with red representing high values and blue low values). Rows represent instruments to be identified and columns are instrument classes. Off diagonal values that are non-dark blue represent errors in classification. The overall accuracy from this confusion matrix is 79.1%±0.7.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g004" position="float" xlink:type="simple"/>
        </fig>
        <p>We performed a post-hoc analysis of the decision space based on cortical features in an attempt to get a better understanding of the configuration of the decision hyperplanes between different instrument classes. The analysis treated the support vectors (i.e. samples of each instrument that fall right on the boundary that distinguishes it from another instrument) for each instrument as samples from an underlying high-dimensional probability density function. Then, a measure of similarity between pairs of probability functions (symmetric Kullback–Leibler (KL) divergence <xref ref-type="bibr" rid="pcbi.1002759-Cover1">[43]</xref>) was employed to provide a sense of distance between each instrument pair in the decision space. Because of the size and variability in the timbre decision space, we pooled the comparisons by instrument class (winds, strings and percussions). We also focused our analysis on the reduced dimensions of the cortical space; called ‘eigen’-rate, ‘eigen’-scale and ‘eigen’-frequencies; obtained by projecting the equivalent dimensions in the cortical tensor (rate, scale and frequency, respectively) into a reduced dimensional space using singular-value decomposition (see <xref ref-type="sec" rid="s4">METHODS</xref>). The analysis revealed a number of observations (see <xref ref-type="fig" rid="pcbi-1002759-g005">Figure 5</xref>). For instance, wind and percussion classes were the most different (occupy distant regions in the decision space), followed by strings and percussions then strings and winds (average KL distances were 0.58, 0.41, 0.35, respectively). This observation was consistent with the subjective judgments of human listeners presented next (see off-diagonal entries in <xref ref-type="fig" rid="pcbi-1002759-g006">Figure 6B</xref>). All 3 pair comparisons were statistically significantly different from each other (Wilcoxon ranksum test, p&lt;10<sup>−5</sup> for all 3 pairs). Secondly, the analysis revealed that the 2 first ‘eigen’-rates captured most of the difference between the instrument classes (statistical significance in comparing the first 2 eigenrates with the others; Wilcoxon ranksum test, p = 0.0046). In contrast, <italic>all</italic> ‘eigen’-scales were variable across classes (Kruskal-Wallis test, p = 0.9185 indicating that all ‘eigen’-scales contributed equally in distinguishing the broad classes). A similar analysis indicated that the first four ‘eigen’-frequencies were also significantly different from the remaining features (Wilcoxon ranksum test, p&lt;10<sup>−5</sup>). One way to interpret these observations is that the first two principal orientations along the rate axis captured most of the differences that distinguish winds, strings and percussions. This seems consistent with the large differences in temporal envelope shape for these instruments classes, which can be represented by a few rates. By contrast, the scale dimension (which captures mostly spectral shape, symmetry and bandwidth) was required in its entirety to draw a boundary between these classes, suggesting that unlike the coarser temporal characteristics, differentiating among instruments entails detailed spectral distinctions of a subtle nature.</p>
        <fig id="pcbi-1002759-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>The average KL divergence between support vectors of instruments belonging to different broad classes.</title>
            <p>Each panel depicts the values of the 3 dimensional average distances between pairs of instruments of a given couple of classes: (<bold>A</bold>) wind vs. percussion; (<bold>B</bold>) string vs. percussion; (<bold>C</bold>) wind vs. string. The 3 dimensional vectors are displayed along eigenrates (x-axis), eigenscales (y-axis) and eigenfrequency (across small subpanels). Red indicates high values of KL divergence and blue indicates low values.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g005" position="float" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002759-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Human listener's judgment of musical timbre similarity.</title>
            <p>(<bold>A</bold>) The mean (top row) and standard deviation (bottom row) of the listeners' responses show the similarity between every pair of instruments for three notes A3, D4 and G#4. Red (values close to 1) indicates high dissimilarity and blue (values close to 0) indicates similarity. (<bold>B</bold>) Timbre similarity is averaged across subjects, musical notes and upper and lower half-matrices, and used for validation of the physiological and computational model. (<bold>C</bold>) Multidimensional scaling (MDS) applied to the human similarity matrix projected over 2 dimensions (shown to correlate with attack time and spectral centroid).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g006" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2d">
        <title>Comparison with standard classification algorithms</title>
        <p>Spectral features have been extensively used for tasks of musical timbre classification of isolated notes, solo performances or even multi-instrument recordings. Features such as Cepstral Coefficients or Linear Prediction of the spectrum resonances yielded performance in the range of 77% to 90% when applied to databases similar to the one used in the present study <xref ref-type="bibr" rid="pcbi.1002759-Krishna1">[44]</xref>–<xref ref-type="bibr" rid="pcbi.1002759-Brown1">[46]</xref>.</p>
        <p>There is wide agreement in the literature that inclusion of simple temporal features, such as zero-crossing rate, or more complex ones such as trajectory estimation of spectral envelopes, is often desirable and results in improvement of the system performance. Tests on the RWC database with both spectral and temporal features reported an accuracy of 79.7% using 19 instruments <xref ref-type="bibr" rid="pcbi.1002759-Kitahara1">[47]</xref> or 94.9% using 5 instruments <xref ref-type="bibr" rid="pcbi.1002759-Burred2">[42]</xref>. Tests of spectrotemporal features on other music databases has often yielded a range of performances between 70–95% <xref ref-type="bibr" rid="pcbi.1002759-Eronen1">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1002759-Kostek1">[51]</xref>.</p>
        <p>Whereas a detailed comparisons with our results is beyond the scope of this paper, we can still note that, if anything, the recognition rates we report for the full auditory model are generally in the range or above those reported by state-of-the-art signal processing techniques.</p>
      </sec>
      <sec id="s2e">
        <title>Psychophysics timbre judgments</title>
        <p>Given the ability of the cortical model to capture the diversity of musical timbre across a wide range of instruments in a classification task, we next explored how well the cortical representation (from both real and model neurons) does in capturing human perceptual judgments of distance in the musical timbre space. To this end, we used human judgments of musical timbre distances using a psychoacoustic comparison paradigm.</p>
        <p>Human listeners were asked to rate the similarity between musical instruments. We used three different notes (A3, D4 and G#4) in three different experiments. Similarity matrices for all three notes yielded reasonably balanced average ratings across subjects, instrument pair order (e.g. piano/violin vs. violin/piano) and pitches, in agreement with other studies <xref ref-type="bibr" rid="pcbi.1002759-Marozeau1">[52]</xref> (<xref ref-type="fig" rid="pcbi-1002759-g006">Figure 6A</xref>). Therefore, we combined the matrices across notes and listeners into an upper half matrix shown in <xref ref-type="fig" rid="pcbi-1002759-g006">Figure 6B</xref>, and used it for all subsequent analyses. For comparison with previous studies, we also ran a multidimensional scaling (MDS) analysis <xref ref-type="bibr" rid="pcbi.1002759-Cox1">[53]</xref> on this average timbre similarity rating and confirmed that the general configuration of the perceptual space was consistent with previous studies (<xref ref-type="fig" rid="pcbi-1002759-g006">Figure 6C</xref>) <xref ref-type="bibr" rid="pcbi.1002759-McAdams2">[8]</xref>. Also for comparison, we tested acoustical dimensions suggested in those studies. The first dimension of our space correlated strongly with the logarithm of attack-time (Pearson's correlation coefficient: ρ = 0.97, p&lt;10<sup>−3</sup>), and the second dimension correlated reasonably well with the center of mass of the auditory spectrogram, also known as spectral centroid (Pearson's correlation coefficient: ρ = 0.62, p = 0.04).</p>
      </sec>
      <sec id="s2f">
        <title>Human vs. model timbre judgments</title>
        <p>The perceptual results obtained above, reflecting subjective timbre distances between different instruments, summarizes an elaborate set of judgments that potentially reveal other facets of timbre perception than the listeners' ability to recognize instruments. We then explored whether the cortical representation could account for these judgments. Specifically, we asked whether the cortical analysis maps musical notes onto a feature space where instruments like violin and cello are distinct, yet closer to each other than a violin and a trumpet. We used the same 11 instruments and 3 pitches (A3, D4 and G#4) employed in the psychoacoustics experiment above and mapped them onto a cortical representation using both neurophysiological and model STRFs. Each note was then vectorized into a feature data-point and mapped via Gaussian kernels. These kernels are similar to the radial basis functions used in the previous section, and aimed at mapping the data from its original cortical space to a linearly separable space. Unlike the generic SVM used in the classification of musical timbre, the kernel parameters here were optimized based on the human scores following a similarity-based objective function. The task here was not merely to classify instruments into distinct classes, but rather to map the cortical features according to a complex set of rules. Using this learnt mapping, a confusion matrix was constructed based on the instrument distances, which was then compared with the human confusion matrix using a Pearson's correlation metric. We performed a comparison with the physiological as well as model STRFs. The simulated confusion matrices are shown in <xref ref-type="fig" rid="pcbi-1002759-g007">Figure 7A–B</xref>.</p>
        <fig id="pcbi-1002759-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Model musical timbre similarity.</title>
            <p>Instrument similarity matrices based kernel optimization technique of the (<bold>A</bold>) neurophysiological receptive field and (<bold>B</bold>) cortical model receptive fields. (<bold>C</bold>) Control experiments using the auditory spectral features (left), separable spectro-temporal modulation feature (middle), and global modulation features [separable spectral and temporal modulations integrated across time and frequency] (right). Red depicts high dissimilarity. All the matrices show only the upper half-matrix with the diagonal not shown.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g007" position="float" xlink:type="simple"/>
        </fig>
        <p>The success or otherwise of the different models was estimated by correlating the human dissimilarity matrix to that generated by the model. No attempt was made at producing MDS analyses of the model output, as meaningfully comparing MDS spaces is not a trivial problem <xref ref-type="bibr" rid="pcbi.1002759-Marozeau1">[52]</xref>. Physiological STRFs yielded a correlation coefficient of <bold>0.73</bold>, while model STRFs yielded a correlation of <bold>0.94</bold> (<xref ref-type="table" rid="pcbi-1002759-t002">Table 2</xref>).</p>
        <table-wrap id="pcbi-1002759-t002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.t002</object-id>
          <label>Table 2</label>
          <caption>
            <title>Correlation coefficients for different feature sets.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002759-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.t002" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1"/>
                  <td align="left" rowspan="1" colspan="1">L2 on features</td>
                  <td align="left" rowspan="1" colspan="1">L2 on reduced features</td>
                  <td align="left" rowspan="1" colspan="1">Gaussian kernels on reduced features</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Fourier-based Spectrum</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">0.69</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Auditory Spectrum</td>
                  <td align="left" rowspan="1" colspan="1">0.473</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">0.739</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Global Spectro-temporal Modulations</td>
                  <td align="left" rowspan="1" colspan="1">0.509</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">0.701</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Separable Spectro-temporal Modulations</td>
                  <td align="left" rowspan="1" colspan="1">0.561</td>
                  <td align="left" rowspan="1" colspan="1">0.561</td>
                  <td align="left" rowspan="1" colspan="1">0.830</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Full cortical Model</td>
                  <td align="left" rowspan="1" colspan="1">0.611</td>
                  <td align="left" rowspan="1" colspan="1">0.607</td>
                  <td align="left" rowspan="1" colspan="1">0.944</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Neurophysiological STRFs</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">-</td>
                  <td align="left" rowspan="1" colspan="1">0.73</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="nt102">
              <p>Each row represents the correlation coefficient between the model and human similarity matrix using a direct Euclidian distance the specific feature itself (left); on a reduced dimension of the features (middle column) or using the Gaussian kernel distance (right column). Auditory Spectrum: time-average of the cochlear filterbank; Global Spectro-temporal Modulations: model STRFs averaged in time and frequency; Separable Spectro-temporal modulations: model STRFs averaged separately in rate and scale, and then in time; Full cortical model: STRFs averaged in time; Neurophysiological STRFs: as the full cortical model, but with STRFs collected in primary auditory cortex of ferrets.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="s2g">
        <title>Control experiments</title>
        <p>In order to disentangle the contribution of the “input” cortical features versus the “back-end” machine learning in capturing human behavioral data, we recomputed confusion matrices using alternative representations such as the auditory spectrogram and various marginals of the cortical distributions. In all these control experiments, the Gaussian kernels were re-optimized separately to fit the data representation being explored.</p>
        <p>We first investigated the performance using auditory spectrum features with optimized Gaussian kernels. The spectrogram representation yielded a similarity matrix that captures the main trends in human distance judgments, with a correlation coefficient of <bold>0.74</bold> (<xref ref-type="fig" rid="pcbi-1002759-g007">Figure 7C</xref>, leftmost panel). Similar experiments using a traditional spectrum (based on Fourier analysis of the signal) yield a correlation of <bold>0.69</bold>.</p>
        <p>Next, we examined the effectiveness of the model cortical features by reducing them to various marginal versions with fewer dimensions as follows. First, we performed an analysis of the spectral and temporal modulations as a <italic>separable</italic> cascade of two operations. Specifically, we analyzed the spectral profile of the auditory spectrogram (scales) <italic>independently</italic> from the temporal dynamics (rates) and stack the two resulting feature vectors together. This analysis differed from the full cortical analysis that assumes an inseparable analysis of spectro-temporal features. An inseparable function is one that cannot be factorized into a function of time and a function of frequency; i.e. a matrix of rank greater than 1 (see <xref ref-type="sec" rid="s4">Methods</xref>). By construction, a separable function consists of temporal cross sections that are scaled versions of the same essential temporal function. A consequence of such constraint is that a separable function cannot capture orientation in time-frequency space (e.g. FM sweeps). In contrast, the full cortical analysis estimates modulations along <italic>both</italic> time and frequency axes in addition to an integrated view of the two axes including orientation information The analysis based on the separable model achieved a correlation coefficient of <bold>0.83</bold> (<xref ref-type="table" rid="pcbi-1002759-t002">Table 2</xref>).</p>
        <p>Second, we further reduced the separable spectro-temporal space by analyzing the modulation content along both time and frequency without maintaining the distribution along the tonotopic axis. This was achieved by simply integrating the modulation features along the spectral axis thus exploring the global characteristic of modulation regardless of tonotopy (<xref ref-type="fig" rid="pcbi-1002759-g007">Figure 7C</xref>, rightmost panel). This representation is somewhat akin to what would result from a 2-dimensional Fourier analysis of the auditory spectrogram. This experiment yielded a correlation coefficient of <bold>0.70</bold> (<xref ref-type="table" rid="pcbi-1002759-t002">Table 2</xref>), supporting the value of an explicit tonotopic axis in capturing subtle difference between instruments.</p>
        <p>Next, we addressed the concern that the mere number of features included in the full cortical model enough to explain the observed performance. We therefore undersampled the full cortical model by employing only 6 scale filters; 10 rate filters and 64 frequency filters by coarsely sampling the range of spectro-temporal modulations. This mapping resulted in a total number of dimensions of 3840; to be comparable to the 4224 dimensions obtained from the separable model. We then performed the dimensionality reduction to 420 dimensions, similar to that used for the separable analysis discussed above. The correlation obtained was <bold>0.86</bold>; which is better than that of the separable spectro-temporal model (see <xref ref-type="fig" rid="pcbi-1002759-g008">Figure 8</xref>). This result supports our main claim that the <italic>coverage</italic> provided by the cortical space allows extracting specific details in the musical notes that highlight information about the physical properties of each instrument; hence enabling classification and recognition.</p>
        <fig id="pcbi-1002759-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002759.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Correlation between human and model similarity matrices as a function of reduced feature dimensionality.</title>
            <p>In each simulation, a cortical representation is projected onto a lower dimensional space before passing it onto the SVM classifier. Each projection maintains a given percentage of the variability in the original data (shown adjacent to each correlation data point). We contrast performance using full cortical model (black curve) vs. separable spectro-temporal modulation model (red curve). The empirical optimal performance is achieved around 420 dimensions; which are the parameters reported in the main text in <xref ref-type="table" rid="pcbi-1002759-t002">table 2</xref>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002759.g008" position="float" xlink:type="simple"/>
        </fig>
        <p>Finally, we examined the value of the kernel-learning compared to using a simple Euclidian L2 distance at various stages of the model (e.g. peripheral model, cortical stage, reduced cortical model using tensor singular value decomposition). <xref ref-type="table" rid="pcbi-1002759-t002">Table 2</xref> summarizes the results of this analysis along various stages of the model shown in <xref ref-type="fig" rid="pcbi-1002759-g002">Figure 2</xref>. The analysis revealed that the kernel-based mapping does provide noticeable improvement to the predictive power of the model but cannot –by itself– explain the results since the same technique applied directly on the spectrum only yielded a correlation of <bold>0.74</bold>.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>This study demonstrates that perception of musical timbre could be effectively based on neural activations patterns that sounds evoke at the level of primary auditory cortex. Using neurophysiological recordings in mammalian auditory cortex as well as a simplified model of cortical processing, it is possible to accurately replicate human perceptual similarity judgments and classification performance among sounds from a large number of musical instruments. Of course, showing that the information is available at the level of primary auditory cortex does not imply that all neural correlates of sound identification will be found at this level. Nevertheless, it suggests that the spectro-temporal transforms as observed at this stage are critical for timbre perception. Moreover, our analysis highlights the ability of the cortical mapping to capture timbre properties of musical notes and instrument-specific characteristics regardless of pitch and playing style. Unlike static or reduced views of timbre that emphasize three or four parameters extracted from the acoustic waveform, the cortical analysis provides a dynamic view of the spectro-temporal modulations in the signal as they vary over time. A close examination of the contribution of different auditory features and processing stages to the timbre percepts highlights three key points.</p>
      <p>First, neither the traditional spectrum nor its variants (e.g. average auditory spectrum <xref ref-type="bibr" rid="pcbi.1002759-Yang1">[54]</xref>) are well-suited to account for timbre perception in full. According to our simulations, these representations encode the relevant spectral and temporal acoustic features too implicitly to lend themselves for exploitation by classifiers and other machine learning techniques. In some sense, this conclusion is expected given the multidimensional nature of the timbre percept compared to the dense two-dimensional spectrogram; and is in agreement with other findings from the literature <xref ref-type="bibr" rid="pcbi.1002759-HerreraBoyer1">[19]</xref>.</p>
      <p>Second, when considering more elaborate spectro-temporal cortical representations, it appears that the <italic>full</italic> representation accounts best for human performance. The match worsens if instead <italic>marginals</italic> are used by collapsing the cortical representation onto one or more dimensions to extract the purely spectral or temporal axes or scale-rate map (<xref ref-type="fig" rid="pcbi-1002759-g003">Figure 3</xref>, <xref ref-type="table" rid="pcbi-1002759-t001">Tables 1</xref> and <xref ref-type="table" rid="pcbi-1002759-t002">2</xref>). This is the case even if all dimensions are used separately, suggesting that there are <italic>joint</italic> spectro-temporal features that are key to a full accounting of timbre. While the role of both purely spectral and temporal cues in musical timbre is quite established <xref ref-type="bibr" rid="pcbi.1002759-Peeters1">[12]</xref>, our analysis emphasizes the crucial contribution of a joint spectro-temporal representation. For example, FM modulations typical of vibrato in string instruments are joint features that cannot be easily captured by the marginal spectral or temporal representations. Interestingly, acoustical analyses and fMRI data in monkeys suggest that the spectro-temporal processing scheme used here may be able to differentiate between broad sound categories (such as monkey calls vs. bird calls vs. human voice), with corresponding neural correlates when listening to those sounds <xref ref-type="bibr" rid="pcbi.1002759-Joly1">[55]</xref>.</p>
      <p>Third, a nonlinear decision boundary in the SVM classifier is essential to attain the highest possible match between the cortical representation and human perception. Linear metrics such as L2 are less optimal, indicating that the linear cortical representation may not be sufficiently versatile to capture the nuances of various timbres. The inadequacy of the <italic>linear</italic> cortical mapping has previously been described when analyzing neural responses to complex sounds such as speech at the level of auditory cortex <xref ref-type="bibr" rid="pcbi.1002759-Elhilali1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Christianson1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Sadagopan1">[38]</xref>. In these cases, it is necessary to postulate the existence of nonlinearities such as divisive normalization or synaptic depression that follows a linear spectro-temporal analysis so as to account fully for the observed responses. In the current study, the exact nature of the nonlinearity remains unclear as it is implicitly subsumed in the Gaussian kernels and subsequent decisions.</p>
      <p>In summary, this study leads to the general conclusion that timbre percepts can be effectively explained by the joint spectro-temporal analysis performed at the level of mammalian auditory cortex. However, unlike the small number of spectral or temporal dimensions that have been traditionally considered in the timbre literature, we cannot highlight a simple set of neural dimensions subserving timbre perception. Instead, the model suggests that subtle perceptual distinctions exhibited by human listeners are based on ‘opportunistic’ acoustic dimensions <xref ref-type="bibr" rid="pcbi.1002759-Agus1">[56]</xref> that are selected and enhanced, when required, on the rich baseline provided by the cortical spectro-temporal representation.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Ethics statement</title>
        <p>All behavioral recordings of timbre similarity judgments with human listeners were approved by the local ethics committee of the Université Paris Descartes. All procedures for recordings of single unit neural activity in ferrets were in accordance with the Institutional Animal Care and Use Committee at the University of Maryland, College Park and the Guidelines of the National Institutes of Health for use of animals in biomedical research.</p>
      </sec>
      <sec id="s4b">
        <title>Psychoacoustics</title>
        <sec id="s4b1">
          <title>Stimuli</title>
          <p>Recordings of single musical notes were extracted from the RWC Music Database <xref ref-type="bibr" rid="pcbi.1002759-Goto1">[57]</xref>, using the notes designated “medium-volume” and “staccato”, with pitches of A3, D4, and G#4. The set used for the experiments comprised 13 sound sources: Piano, Vibraphone, Marimba, Cello, Violin, Oboe, Clarinet, Trumpet, Bassoon, Trombone, Saxophone, male singer singing the vowel /a/, male singer singing the vowel /i/. Each note was edited into a separate sound file, truncated to 250 ms duration with 50 ms raised cosine offset ramp (the onset was preserved), and normalized in RMS power. More details on the sound set can be found in <xref ref-type="bibr" rid="pcbi.1002759-Agus1">[56]</xref>. The analyses presented in the current study exclude the results from the 2 vowels, as only musical instruments were considered in the model classification experiments.</p>
        </sec>
        <sec id="s4b2">
          <title>Participants and apparatus</title>
          <p>A total of twenty listeners participated in the study (14 totally naïve participants, 6 participants experienced in psychoacoustics experiment but naïve to the aim of the present study; mean age: 28 y; 10 female). They had no self-reported history of hearing problems. All twenty subjects performed the test with the D4 pitch. Only six took part in the remaining tests with notes A3 and G#4. <xref ref-type="sec" rid="s2">Results</xref> from the 6 subjects tested on all 3 notes are reported here, even though we checked that including all subjects would not change our conclusions. Stimuli were played through an RME Fireface sound-card at a 16-bit resolution and a 44.1 kHz sample-rate. They were presented to both ears simultaneously through Sennheiser HD 250 Linear II headphones. Presentation level was 65 dB(A). Listeners were tested individually in a double-walled IAC sound booth.</p>
        </sec>
        <sec id="s4b3">
          <title>Procedure</title>
          <p>Subjective similarity ratings were collected. For a given trial, two sounds were played with a 500 ms silent interval. Participants had to indicate how similar they perceived the sounds to be. Responses were collected by means of a graphical interface with a continuous slider representing the perceptual similarity scale. The starting position of the slider was randomized for each trial. Participants could repeat the sound pair as often as needed before recording their rating. In an experimental block, each sound was compared to all others (with both orders of presentations) but not with itself. This gave a total of 156 trials per block, presented in random order. Before collecting the experimental data, participants could hear the whole sound set three times. A few practice trials were also provided until participants reported having understood the task and instructions. A single pitch was used for all instruments in each block; the three types of blocks (pitch A3, D4, or G#4) were run in counterbalanced order across participants. Two blocks per pitch were run for each participants, and only the second block was retained for the analysis.</p>
        </sec>
        <sec id="s4b4">
          <title>Multidimensional scaling (MDS) and acoustical correlates</title>
          <p>To compare the results with previous studies, we ran an MDS analysis on the dissimilarity matrix obtained from human judgments. A standard non-metric MDS was performed (Matlab, the MathWorks). Stress values were generally small, with a knee-point for the solution at two dimensions (0.081, Kruskal normalized stress1). We also computed acoustical descriptors corresponding to the classic timbre dimensions. Attack time was computed by taking the logarithm of the time taken to go from −40 dB to −12 dB relative to the maximum waveform amplitude. Spectral centroid was computed by running the stimuli in an auditory filterbank, compressing the energy in each channel (exponent: 0.3), and taking the center of mass of the resulting spectral distribution.</p>
        </sec>
      </sec>
      <sec id="s4c">
        <title>Auditory model</title>
        <p>The cortical model is comprised of two main stages: an early stage mimicking peripheral processing up to the level of the midbrain, and a central stage capturing processing in primary auditory cortex (A1). Full details about the model can be found in <xref ref-type="bibr" rid="pcbi.1002759-Yang1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002759-Chi1">[58]</xref>; but are described briefly here.</p>
        <p>The processing of the acoustic signal in the cochlea is modeled as a bank of 128 constant-Q asymmetric bandpass filters equally spaced on the logarithmic frequency scale spanning 5.3 octaves. The cochlear output is then transduced into inner hair cells potentials via a high pass and low pass operation. The resulting auditory nerve signals undergo further spectral sharpening via a lateral inhibitory network. Finally, a midbrain model resulting in additional loss in phase locking is performed using short term integration with time constant 4 ms resulting in a time frequency representation called as the auditory spectrogram.</p>
        <p>The central stage further analyzes the spectro-temporal content of the auditory spectrogram using a bank of modulation selective filters centered at each frequency along the tonotopic axis, modeling neurophysiological receptive fields. This step corresponds to a 2D affine wavelet transform, with a spectro-temporal mother wavelet, define as Gabor-shaped in frequency and exponential in time. Each filter is tuned (Q = 1) to a specific rate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e001" xlink:type="simple"/></inline-formula> in Hz) of temporal modulations and a specific scale of spectral modulations (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e002" xlink:type="simple"/></inline-formula> in cycles/octave), and a directional orientation (+ for upward and − for downward).</p>
        <p>For input spectrogram <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e003" xlink:type="simple"/></inline-formula> the response of each STRF in the model is given by:<disp-formula id="pcbi.1002759.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e004" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e005" xlink:type="simple"/></inline-formula> denotes convolution in time and frequency and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e006" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e007" xlink:type="simple"/></inline-formula> are the characteristic phases of the STRF's which determine the degree of asymmetry in the time and frequency axes respectively. The model filters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e008" xlink:type="simple"/></inline-formula> filters can be decomposed in each quadrant (upward + or downward −) into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e009" xlink:type="simple"/></inline-formula> into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e010" xlink:type="simple"/></inline-formula> corresponding to rate and scale filters respectively. Details of the design of the filter functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e011" xlink:type="simple"/></inline-formula> can be found in <xref ref-type="bibr" rid="pcbi.1002759-Chi1">[58]</xref>. The present study uses 11 spectral filters with characteristic scales [0.25, 0.35, 0.50, 0.71, 1.00, 1.41, 2.00, 2.83, 4.00, 5.66, 8.00] (cycles/octave) and 11 temporal filters with characteristic rates [4.0, 5.7, 8.0, 11.3, 16.0, 22.6, 32.0, 45.3, 64.0, 90.5, 128.0] (Hz), each with upward and downward directionality. All outputs are integrated over the time duration of each note. In order to simplify the analysis, we limit our computations to the magnitude of the cortical output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e012" xlink:type="simple"/></inline-formula> (i.e. responses corresponding to zero-phase filters).</p>
        <p>Finally, dimensionality reduction is performed using tensor singular-value decomposition <xref ref-type="bibr" rid="pcbi.1002759-DeLathauwer1">[59]</xref>. This technique unfolds the cortical tensor along each dimension (frequency, rate and scale axes) and applies singular value decomposition on the unfolded matrix. We choose 5 eigenscales, 4 eigenrates and 21 eignefrequencies resulting in 420 features with the highest eigenvalues, preserving 99.9% of the variance in the original data. The motivation for this cutoff choice is presented later.</p>
      </sec>
      <sec id="s4d">
        <title>Cortical receptive fields</title>
        <p>Data used here was collected in the context of a number of studies <xref ref-type="bibr" rid="pcbi.1002759-Fritz1">[60]</xref>–<xref ref-type="bibr" rid="pcbi.1002759-Fritz3">[62]</xref> and full details of the experimental paradigm are described in these publications. Briefly, extracellular recordings were performed in 15 awake non-behaving domestic ferrets (<italic>Mustela putorius</italic>) with surgically implanted headposts. Tungsten electrodes (3–8 MΩ) were used to record neural responses from single and multi-units at different depths. All data was processed off-line and sorted to extract single-unit activity.</p>
        <p>Spectro-Temporal Receptive fields (STRF) were characterized using TORC (Temporally-Orthogonal Ripple Combination) stimuli <xref ref-type="bibr" rid="pcbi.1002759-Klein1">[63]</xref>, consisting of superimposed ripple noises with rates between 4–24 (Hz) and scales between 0 (flat) and 1.4 peaks/octave. Each stimulus was 3 sec with inter-stimulus intervals of 1–1.2 sec, and a full set of 30 TORCs was typically repeated 6–15 times. All sounds were computer-generated and delivered to the animal's ear through inserted earphones calibrated in-situ. TORC amplitude is fixed between 55–75 dB SPL.</p>
        <p>STRFs were derived using standard reverse correlation techniques, and a signal-to-noise ratio (SNR) for each STRF was measured using a bootstrap technique (see <xref ref-type="bibr" rid="pcbi.1002759-Klein1">[63]</xref> for details). Only STRFs with SNR≥2 were included in the current study, resulting in a database of 1110 STRFs (average 74 STRFs/animal). Note because of the experimental paradigm, STRFs spanned a 5-octave range with low frequencies 125, 250 or 500 Hz. In the current study, all STRFs were aligned to match the frequency range of musical note spectrograms. Since all our spectrograms start at 180 Hz and cover 5.3 octaves, we scaled and shifted the STRF's to fit this range.</p>
        <p>The neurophysiological STRFs were employed to perform the timbre analysis by convolving each note's auditory spectrogram z(t,f) with each STRF in the database as in <xref ref-type="disp-formula" rid="pcbi.1002759.e013">Equation (2)</xref>.<disp-formula id="pcbi.1002759.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e013" xlink:type="simple"/><label>(2)</label></disp-formula>The resulting firing rate vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e014" xlink:type="simple"/></inline-formula> was then integrated over time yielding an average response across the tonotopic axis. The output from all STRFs were then stacked together, resulting in a 142080 (128 frequency channels ×1110 STRFs) dimensional vector. We reduced this vector using singular value decomposition and mapped it onto 420 dimensions, which preserve 99.9% of the data variance in agreement with dimensionality used for model STRFs.</p>
      </sec>
      <sec id="s4e">
        <title>Timbre classification</title>
        <p>In order to test the cortical representation's ability to discriminate between different musical instruments, we augmented the basic auditory model with a statistical clustering model based on support vector machines (SVM) <xref ref-type="bibr" rid="pcbi.1002759-Cristianini1">[39]</xref>. Support vector machines are classifiers that learn a set of hyperplanes (or decision boundaries) in order to maximally separate the patterns of cortical responses caused by the different instruments.</p>
        <p>Each cortical pattern was projected via Gaussian kernel to a new dimensional space. The use of kernels is a standard technique used with support vector machines, aiming to map the data from its original space (where data may not be linearly separable) onto a new representational space that is linearly separable. This mapping of data to a new (more linear space) through a the use of a kernel or transform is commonly referred to as the “kernel trick” <xref ref-type="bibr" rid="pcbi.1002759-Cristianini1">[39]</xref>. In essence, kernel functions aim to determine the relative position or similarity between pairs of points in the data. Because the data may lie in a space that is not linearly separable (not possible to use simple lines or planes to separate the different classes), it is desirable to map the data points onto a different space where this linear separability is possible. However, instead of simply projecting the data points themselves onto a high-dimensional feature space which would increase complexity as a function of dimensionality, the “kernel trick” avoids this direct mapping. Instead, it provides a method for mapping the data into an inner product space without explicitly computing the mapping of the observations directly. In other words, it computes the inner product between the data points in the new space without computing the mapping explicitly.</p>
        <p>The kernel used here is given by<disp-formula id="pcbi.1002759.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e015" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e017" xlink:type="simple"/></inline-formula> are the feature vectors of 2 sound samples. The parameter for the Gaussian kernel and the cost parameter for the SVM algorithm were optimized on a subset of the training data.</p>
        <p>A classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e018" xlink:type="simple"/></inline-formula> is trained for every pair of classes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e019" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e020" xlink:type="simple"/></inline-formula>. Each of these classifiers then gives a label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e021" xlink:type="simple"/></inline-formula> for a test sample. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e022" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e023" xlink:type="simple"/></inline-formula>. We count the number of labels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e024" xlink:type="simple"/></inline-formula>. The test sample is then assigned to the class with maximum count given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e025" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e026" xlink:type="simple"/></inline-formula>. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e027" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1002759.e015">Equation (3)</xref> was chosen by doing a grid search over a large parameter span in order to optimize the classifier performance in correctly distinguishing different instruments. This tuning was done by training and testing on a subset of the training data. For model testing, we performed a standard k-fold cross validation procedure with k = 10 (90% training, 10% testing). The dataset was divided into 10 parts. We then left out one part at a time and trained on the remaining 9 parts. The results reported are the average performance over all 10 iterations. A single Gaussian parameter was optimized for all the pair-wise classifiers across all the 10-fold cross validation experiments.</p>
      </sec>
      <sec id="s4f">
        <title>Analysis of support vector distribution</title>
        <p>In order to better understand the mapping of the different notes in the high-dimensional space used to classify them, we performed a closer analysis of the support vectors for each instrument pair <italic>i</italic> and <italic>j</italic>. Support vectors are the samples from each class that fall exactly on the margin between class <italic>i</italic> and class <italic>j</italic>, and therefore are likely to be more confusable between the classes. Since we are operating in the ‘classifier space’, each of the support vectors is defined in a reduced dimensional hyperspace consisting of 5 eigen-scales, 4 eigen-rates, and 21 eigen-frequencies as explained above (a total of 420 dimensions). The collection of all support vectors for each class <italic>i</italic> can be pulled together to estimate a high-dimensional probability density function. The density function estimate was derived using a histogram method by partitioning the sample space along each dimension into 100 bins, counting how many samples fall into each bin and dividing the counts by the total number of samples. We label the probability distribution for the <italic>d</italic>-th dimension (d = 1,..,420) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e028" xlink:type="simple"/></inline-formula>. We then computed the symmetric KL divergence, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e029" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002759-Cover1">[43]</xref>, between the support vectors for classes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e030" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e031" xlink:type="simple"/></inline-formula> from the classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e032" xlink:type="simple"/></inline-formula> as shown in <xref ref-type="disp-formula" rid="pcbi.1002759.e033">Equation (4)</xref>. The KL divergence is simply a measure of difference between pairs of probability distributions, is defined is next:<disp-formula id="pcbi.1002759.e033"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e033" xlink:type="simple"/><label>(4)</label></disp-formula>The bins with zero probability were disregarded from the computation of the KL divergence. An alternative method that smoothed the probability distribution over the zero bins was also tested and yielded virtually comparable results. Overall, this analysis is meant to inform about the layout of the timbre decision space. We analyzed the significance of the results between the broad timbre classes (winds, percussions and strings) by pooling individual comparisons between instruments within each group (See <xref ref-type="fig" rid="pcbi-1002759-g005">Figure 5</xref>).</p>
        <sec id="s4f1">
          <title>Dataset</title>
          <p>We used the RWC music database <xref ref-type="bibr" rid="pcbi.1002759-Goto1">[57]</xref> for testing the model. 11 instruments were used for this task, which included string (violin, piano, cello), percussion (vibraphone, marimba) and wind instruments (saxophone, trumpet, trombone, clarinet, oboe, and bassoon). We extracted an average of 1980 notes per instrument ranging over different makes of the instruments, as well as a wide range of pitches and styles of playing (staccato, vibrato, etc.). The notes were on average 2.7 sec in duration but varied between 0.1–18 sec. The sampling frequency of the wave files in the database was 44.1 kHz. We performed preprocessing on the sound files by first down sampling to 16 kHz then filtering using a pre-emphasis filter (FIR filter with coefficients 1 and −0.97).</p>
        </sec>
      </sec>
      <sec id="s4g">
        <title>Human vs. model correlation</title>
        <p>We tested the auditory model's ability to predict human listeners' judgment of musical timbre distances. Just like the timbre classification task, we used the cortical model augmented with Gaussian Kernels. In order to optimize the model to the test data, we employed a variation of the Gaussian kernel that performs an optimized feature embedding on every data dimension. The kernel is defined as follows:<disp-formula id="pcbi.1002759.e034"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e034" xlink:type="simple"/><label>(5)</label></disp-formula>where N is the number of dimensions of the features x and y. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e035" xlink:type="simple"/></inline-formula>'s are parameters for the kernel that need to be optimized. We define an objective function that optimizes the correlation between the human perceptual distances and the distances in the embedded space.<disp-formula id="pcbi.1002759.e036"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e036" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e037" xlink:type="simple"/></inline-formula> is the average profile for the i<sup>th</sup> instrument over all notes; D(i,j) is the average perceived distance between the i<sup>th</sup> and j<sup>th</sup> instrument based on psychoacoustic results <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e039" xlink:type="simple"/></inline-formula> are the average distances from the kernel and the psychoacoustic experiment respectively. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e040" xlink:type="simple"/></inline-formula> represents the variance of the kernel distances over all samples (all instrument pairs). Similarly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e041" xlink:type="simple"/></inline-formula> is the variance of the human perceived distances. We used a gradient ascent algorithm to learn <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e042" xlink:type="simple"/></inline-formula> which optimize the objective function.</p>
        <p>The correlation analysis employed the same dataset used for the human psychophysical experiment described above. Each note was 0.25 s in duration with sampling rate 44.1 kHz and underwent the same preprocessing as mentioned earlier. The absolute value of the model output was derived for each note and averaged over duration following a similar procedure as the timbre classification described above. The cortical features obtained for the three notes (A3, D4, G#4) were averaged for each instrument i to obtain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e043" xlink:type="simple"/></inline-formula>. Similarly the perceived human distances between instrument i and j were obtained by averaging the (i,j)<sup>th</sup> and (j,i)<sup>th</sup> entry in the human distance matrix over all the 3 notes to obtain D(i,j).</p>
        <p>Finally, the human and model similarity matrices were compared using the Pearson's correlation metric. In order to avoid overestimating the correlation between the two matrices (the two symmetric values appearing twice in the correlation), we correlated only the upper triangle of each matrix.</p>
        <sec id="s4g1">
          <title>Dimensionality reduction of cortical features</title>
          <p>As is the case with any classification problem in high-dimensional spaces, all analyses above had to be performed on a reduced number of features which we obtained using tensor singular value decomposition (TSVD), as described earlier. This step is necessary in order to avoid the curse of dimensionality which reduces the predictive power of the classifier as the dimensionality increases <xref ref-type="bibr" rid="pcbi.1002759-Donoho1">[64]</xref>. In order to determine the ‘optimal’ size of the reduced features, we ran a series of investigations with a range of TSVD thresholds. The analysis comparing the correlation between the cortical model and human judgments of timbre similarity is shown in <xref ref-type="fig" rid="pcbi-1002759-g008">Figure 8</xref>. The analysis led to the choice of 420 dimensions as near optimal. It is important to note that our tests were not fine-grained enough in order to determine the exact point of optimality. Moreover, this choice is only valid with regards to the data at hand and classifier used in this study, namely a support vector machine. If one were to choose a different classifier, the optimal reduced dimensionality may be different. It is merely a number that reflects the tradeoff between keeping a rich dimensionality that captures the diversity of the data; while reducing the dimensionality in order to fit the predictive power of the classifier.</p>
          <p>To further emphasize this point, we ran a second analysis contrasting the system performance with the full cortical model (joint spectro-temporal modulations) against a model with separable modulations; all while maintaining the dimensionality of the reduced space fixed. This experiment (<xref ref-type="fig" rid="pcbi-1002759-g008">Figure 8</xref> – red curve) confirmed that the original space indeed biases the system performance, irrespective of the size of reduced data. Results from <xref ref-type="table" rid="pcbi-1002759-t002">Table 2</xref> are also overlaid in the same figure for ease of comparison.</p>
        </sec>
      </sec>
      <sec id="s4h">
        <title>Control experiments</title>
        <sec id="s4h1">
          <title>i) Auditory spectrum analysis</title>
          <p>The auditory spectrum was obtained by analyzing the input waveform with the 128 cochlear filters described above, and integrating over the time dimension. The resulting feature vector was 128×1representation of the spectral profile of each signal. Unlike a simple Fourier analysis of the signal, the cochlear filtering stage operated on a logarithmic axis with highly asymmetric filters.</p>
        </sec>
        <sec id="s4h2">
          <title>ii) Separable spectro-temporal modulation analysis</title>
          <p>For an input spectrogram <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e044" xlink:type="simple"/></inline-formula>, the response of each rate filter (RF) and scale Filter(SF) was obtained separately as follows:<disp-formula id="pcbi.1002759.e045"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e045" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula id="pcbi.1002759.e046"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002759.e046" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e047" xlink:type="simple"/></inline-formula> denotes convolution in time, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e048" xlink:type="simple"/></inline-formula> denotes convolution in frequency and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e049" xlink:type="simple"/></inline-formula> is the characteristic phase of the RF and φ is the characteristic phase of the SF which determine the degree of asymmetry in the time and frequency axis respectively. Details of the design of the filter functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e050" xlink:type="simple"/></inline-formula> can be found in <xref ref-type="bibr" rid="pcbi.1002759-Chi1">[58]</xref>. Unlike the analysis given in <xref ref-type="disp-formula" rid="pcbi.1002759.e004">Equation (1)</xref>, the spectral and temporal modulations were derived separately using one-dimensional complex-valued filters (either along time or along frequency axis). The resulting magnitude outputs from <xref ref-type="disp-formula" rid="pcbi.1002759.e045">Equations (7</xref>–<xref ref-type="disp-formula" rid="pcbi.1002759.e046">8)</xref> were then stacked together to form the feature vector with 4224 (11scales×128frequecies+22rates×128frequecies) dimensions. The dimensionality was then reduced to 420 using tensor singular value decomposition retaining 99.9% of the variance.</p>
        </sec>
        <sec id="s4h3">
          <title>iii) Global modulation analysis</title>
          <p>For this experiment, we used the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002759.e052" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1002759.e045">Equations (7</xref>–<xref ref-type="disp-formula" rid="pcbi.1002759.e046">8)</xref>, and integrated the output over time and frequency for each note. The resulting rate and scale responses were then stacked together to form the feature vector.</p>
        </sec>
        <sec id="s4h4">
          <title>iv) Under sampled joint modulation</title>
          <p>In this experiment we aimed to make the dimensionality of the cortical model comparable to the separable model by under sampling the rate, scale and frequency axes. The auditory spectrogram was down sampled along the frequency axis by a factor of 2. This auditory spectrogram representation was then analyzed by 6 spectral filters with characteristic scales [0.25, 0.47, 0.87, 1.62, 3.03, 5.67] (cycles/octave) and 5 temporal filters with characteristic rates [4.0, 8.0, 16.0, 32.0, 64.0] (Hz), each with upward and downward directionality resulting in a 3840 dimensional representation. The dimensionality was then reduced to 420 using tensor singular value decomposition retaining 99.99% of the variance</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002759-Handel1">
        <label>1</label>
        <mixed-citation publication-type="other" xlink:type="simple">Handel S (1993) Listening: An introduction to the perception of auditory events. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Ansi1">
        <label>2</label>
        <mixed-citation publication-type="other" xlink:type="simple">Ansi PT (1973) Psychoacoustical Terminology. New York: American National Standards Institute.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Helmholtz1">
        <label>3</label>
        <mixed-citation publication-type="other" xlink:type="simple">Helmholtz H (1877) On the Sensations of Tone. New York: Dover Publications.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Grey1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grey</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Gordon</surname><given-names>JW</given-names></name> (<year>1978</year>) <article-title>Perceptual effects of spectral modifications on musical timbres</article-title>. <source>J Acoust Soc Am</source> <volume>63</volume>: <fpage>1493</fpage>–<lpage>1500</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-McAdams1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Beauchamp</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Meneguzzi</surname><given-names>S</given-names></name> (<year>1999</year>) <article-title>Discrimination of musical instrument sounds resynthesized with simplified spectrotemporal parameters</article-title>. <source>J Acoust Soc Am</source> <volume>105</volume>: <fpage>882</fpage>–<lpage>897</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Patterson1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name> (<year>1994</year>) <article-title>The sound of a sinusoid: Time-interval models</article-title>. <source>J Acoust Soc Am</source> <volume>96</volume>: <fpage>1419</fpage>–<lpage>1428</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-krumhansl1">
        <label>7</label>
        <mixed-citation publication-type="other" xlink:type="simple">krumhansl C (1989) Why is musical timbre so hard to understand? In: Olsson SNaO, editor. structure and perception of electroacoustic sound and music. Amsterdam: Excerpta medica. pp. 43–53.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-McAdams2">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Winsberg</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Donnadieu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>De Soete</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Krimphoff</surname><given-names>J</given-names></name> (<year>1995</year>) <article-title>Perceptual scaling of synthesized musical timbres: common dimensions, specificities, and latent subject classes</article-title>. <source>Psychol Res</source> <volume>58</volume>: <fpage>177</fpage>–<lpage>192</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Grey2">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grey</surname><given-names>JM</given-names></name> (<year>1977</year>) <article-title>Multidimensional perceptual scaling of musical timbres</article-title>. <source>J Acoust Soc Am</source> <volume>61</volume>: <fpage>1270</fpage>–<lpage>1277</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-JABurgoyne1">
        <label>10</label>
        <mixed-citation publication-type="other" xlink:type="simple">J A Burgoyne SM (2007) A Meta-analysis of Timbre Perception Using Nonlinear Extensions to CLASCAL. In: Proceedings of the Computer Music Modeling and Retrieval. Copenhagen, Denmark. pp. 181–202.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Donnadieu1">
        <label>11</label>
        <mixed-citation publication-type="other" xlink:type="simple">Donnadieu S (2007) Mental Representation of the Timbre of Complex Sounds. In: Beauchamp J, editor. Analysis, Synthesis, and Perception of Musical Sounds. New York: Springer. pp. 272–319.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Peeters1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peeters</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Giordano</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Susini</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Misdariis</surname><given-names>N</given-names></name>, <name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>The Timbre Toolbox: extracting audio descriptors from musical signals</article-title>. <source>J Acoust Soc Am</source> <volume>130</volume>: <fpage>2902</fpage>–<lpage>2916</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Caclin1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caclin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>BK</given-names></name>, <name name-style="western"><surname>Winsberg</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>Acoustic correlates of timbre space dimensions: a confirmatory study using synthetic tones</article-title>. <source>J Acoust Soc Am</source> <volume>118</volume>: <fpage>471</fpage>–<lpage>482</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Waibel1">
        <label>14</label>
        <mixed-citation publication-type="other" xlink:type="simple">Waibel A, Lee K (1990) Readings in speech recognition: Morgan Kaufmann Pub. Inc. 680 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Eidenberger1">
        <label>15</label>
        <mixed-citation publication-type="other" xlink:type="simple">Eidenberger H (2011) Fundamental Media Understanding. Norderstedt, Germany: atpress.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Rabiner1">
        <label>16</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rabiner L, Juang B (1993) Fundamentals of Speech Recognition. New Jersey, USA: PTR Prentice Hall.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-McKinney1">
        <label>17</label>
        <mixed-citation publication-type="other" xlink:type="simple">McKinney M, Breebaart J (2003) Features for Audio and Music Classification. In: Proceedings of International Symposium on Music Information Retrieval. Washington D.C, USA. pp. 151–158.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Lerch1">
        <label>18</label>
        <mixed-citation publication-type="other" xlink:type="simple">Lerch A (2012) An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics. New Jersey, USA: Wiley-IEEE Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-HerreraBoyer1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herrera-Boyer</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Peeters</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Dubnov</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Automatic classification of musical instrument sounds</article-title>. <source>Journal of New Music Research</source> <volume>32</volume>: <fpage>3</fpage>–<lpage>21</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Burred1">
        <label>20</label>
        <mixed-citation publication-type="other" xlink:type="simple">Burred JJ, Haller M, Jin S, Samour A, Sikora T (2008) Audio Content Analysis. In: Kompatsiaris Y, Hobson P, editors. Semantic Multimedia and Ontologies: Theory and Applications. London, UK: Springer. pp. 123–162.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Caclin2">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caclin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brattico</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Naatanen</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Morlet</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Separate neural processing of timbre dimensions in auditory sensory memory</article-title>. <source>J Cogn Neurosci</source> <volume>18</volume>: <fpage>1959</fpage>–<lpage>1972</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Belin1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belin</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Lafaille</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ahad</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Pike</surname><given-names>B</given-names></name> (<year>2000</year>) <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source> <volume>403</volume>: <fpage>309</fpage>–<lpage>312</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Uppenkamp1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uppenkamp</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name>, <name name-style="western"><surname>Norris</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name> (<year>2006</year>) <article-title>Locating the initial stages of speech-sound processing in human temporal cortex</article-title>. <source>NeuroImage</source> <volume>31</volume>: <fpage>1284</fpage>–<lpage>1296</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Lewis1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Brefczynski</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Phinney</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Janik</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>DeYoe</surname><given-names>EA</given-names></name> (<year>2005</year>) <article-title>Distinct cortical pathways for processing tool versus animal sounds</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>5148</fpage>–<lpage>5158</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Leaver1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leaver</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Rauschecker</surname><given-names>JP</given-names></name> (<year>2010</year>) <article-title>Cortical representation of natural complex sounds: effects of acoustic features and auditory object category</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>7604</fpage>–<lpage>7612</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-DeLucia1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Lucia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Clarke</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Murray</surname><given-names>MM</given-names></name> (<year>2010</year>) <article-title>A temporal hierarchy for conspecific vocalization discrimination in humans</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>11210</fpage>–<lpage>11221</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Staeren1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Staeren</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Renvall</surname><given-names>H</given-names></name>, <name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name> (<year>2009</year>) <article-title>Sound categories are represented as distributed patterns in the human auditory cortex</article-title>. <source>Curr Biol</source> <volume>19</volume>: <fpage>498</fpage>–<lpage>502</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Formisano1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name>, <name name-style="western"><surname>De Martino</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Bonte</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>“Who” is saying “what”? Brain-based decoding of human voice and speech</article-title>. <source>Science</source> <volume>322</volume>: <fpage>970</fpage>–<lpage>973</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Miller1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Escabi</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname><given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>2002</year>) <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title>. <source>J Neurophysiol</source> <volume>87</volume>: <fpage>516</fpage>–<lpage>527</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Atencio1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atencio</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>2010</year>) <article-title>Laminar diversity of dynamic sound processing in cat primary auditory cortex</article-title>. <source>J Neurophysiol</source> <volume>103</volume>: <fpage>192</fpage>–<lpage>205</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Schreiner1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Sutter</surname><given-names>ML</given-names></name> (<year>1992</year>) <article-title>Topography of excitatory bandwidth in cat primary auditory cortex: single-neuron versus multiple-neuron recordings</article-title>. <source>J Neurophysiol</source> <volume>68</volume>: <fpage>1487</fpage>–<lpage>1502</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Versnel1">
        <label>32</label>
        <mixed-citation publication-type="other" xlink:type="simple">Versnel H, Shamma S, Kowalski N (1995) Ripple Analysis in the Ferret Primary Auditory Cortex. III. Topographic and Columnar Distribution of Ripple Response. Auditory Neuroscience: 271–285.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Schreiner2">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Mendelson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Raggio</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Brosch</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Krueger</surname><given-names>K</given-names></name> (<year>1997</year>) <article-title>Temporal processing in cat primary auditory cortex</article-title>. <source>Acta Otolaryngol</source> <volume>Suppl 532</volume>: <fpage>54</fpage>–<lpage>60</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Theunissen1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>, <name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name> (<year>2000</year>) <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title>. <source>J Neurosci</source> <volume>20</volume>: <fpage>2315</fpage>–<lpage>2331</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Elhilali1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Klein</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2004</year>) <article-title>Dynamics of precise spike timing in primary auditory cortex</article-title>. <source>J Neurosci</source> <volume>24</volume>: <fpage>1159</fpage>–<lpage>1172</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Christianson1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Christianson</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>JF</given-names></name> (<year>2008</year>) <article-title>The consequences of response nonlinearities for interpretation of spectrotemporal receptive fields</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>446</fpage>–<lpage>455</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-David1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2009</year>) <article-title>Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>3374</fpage>–<lpage>3386</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Sadagopan1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sadagopan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name> (<year>2009</year>) <article-title>Nonlinear spectrotemporal interactions underlying selectivity for complex sounds in auditory cortex</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>11192</fpage>–<lpage>11202</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Cristianini1">
        <label>39</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cristianini N, Shawe-Taylor J (2000) Introduction to support vector machines and other kernel-based learning methods. Cambridge, UK: Cambridge University Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Elhilali2">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Chi</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2007</year>) <article-title>Auditory cortical receptive fields: stable entities with plastic abilities</article-title>. <source>Journal of Neuroscience</source> <volume>27</volume>: <fpage>10372</fpage>–<lpage>10382</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Livshin1">
        <label>41</label>
        <mixed-citation publication-type="other" xlink:type="simple">Livshin A, X.Rodet (2004) Musical instrument identification in continuous recordings. In: Proceedings of 7th International Conference on Digital Audio Effects. Naples, Italy. pp. 222–227.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Burred2">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burred</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Robel</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sikora</surname><given-names>T</given-names></name> (<year>2010</year>) <article-title>Dynamic Spectral Envelope Modeling for Timbre Analysis of Musical Instrument Sounds</article-title>. <source>Ieee Transactions on Audio Speech and Language Processing</source> <volume>18</volume>: <fpage>663</fpage>–<lpage>674</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Cover1">
        <label>43</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cover T, Thomas J (2006) Elements of information theory. New York: Wiley-Interscience.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Krishna1">
        <label>44</label>
        <mixed-citation publication-type="other" xlink:type="simple">Krishna AG, Sreenivas TV (2004) Music instrument recognition: from isolated notes to solo phrases. In: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing. Montreal,Quebec,Canada. pp. iv265–iv268.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Marques1">
        <label>45</label>
        <mixed-citation publication-type="other" xlink:type="simple">Marques J, Moreno PJ (1999) A study of musical instrument classification using Gaussian Mixture Models and Support Vector Machines. Compaq Corporation, Cambridge Research laboratory.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Brown1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Houix</surname><given-names>O</given-names></name>, <name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name> (<year>2001</year>) <article-title>Feature dependence in the automatic identification of musical woodwind instruments</article-title>. <source>J Acoust Soc Am</source> <volume>109</volume>: <fpage>1064</fpage>–<lpage>1072</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Kitahara1">
        <label>47</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kitahara T, Goto M, Okuno HG (2003) Musical instrument identification based on f0-dependent multivariate normal distribution. In: Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing. Hong Kong. pp. 409–412.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Eronen1">
        <label>48</label>
        <mixed-citation publication-type="other" xlink:type="simple">Eronen A, Klapuri A (2000) Musical instrument recognition using cepstral coefficients and temporal features. In: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing. Istanbul, Turkey. pp. II753–II756.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Agostini1">
        <label>49</label>
        <mixed-citation publication-type="other" xlink:type="simple">Agostini G, Longari M, Pollastri E (2001) Musical instrument timbres classification with spectral features. In: Proceedings of IEEE Fourth Workshop on Multimedia Signal Processing. Cannes, France. pp. 97–102.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Livshin2">
        <label>50</label>
        <mixed-citation publication-type="other" xlink:type="simple">Livshin A, Rodet X (2006) The Significance of the Non-Harmonic “Noise” Versus the Harmonic Series for Musical Instrument Recognition. In: Proceedings of the 7th International Conference on Music Information Retrieval. Victoria, Canada pp. 95–100.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Kostek1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kostek</surname><given-names>B</given-names></name> (<year>2004</year>) <article-title>Musical instrument classification and duet analysis employing music information retrieval techniques</article-title>. <source>Proceedings of the IEEE</source> <volume>92</volume>: <fpage>712</fpage>–<lpage>729</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Marozeau1">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marozeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>de Cheveigne</surname><given-names>A</given-names></name>, <name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Winsberg</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>The dependency of timbre on fundamental frequency</article-title>. <source>J Acoust Soc Am</source> <volume>114</volume>: <fpage>2946</fpage>–<lpage>2957</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Cox1">
        <label>53</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cox TF, Cox MAA (2001) Multidimensional Scaling. London, UK: Chapman and Hall.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Yang1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>1992</year>) <article-title>Auditory representations of acoustic signals</article-title>. <source>IEEE Trans Inf Theory</source> <volume>38</volume>: <fpage>824</fpage>–<lpage>839</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Joly1">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joly</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Ramus</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Vanduffel</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>GA</given-names></name> (<year>2012</year>) <article-title>Interhemispheric Differences in Auditory Processing Revealed by fMRI in Awake Rhesus Monkeys</article-title>. <source>Cereb Cortex</source> <volume>22</volume>: <fpage>838</fpage>–<lpage>853</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Agus1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agus</surname><given-names>TR</given-names></name>, <name name-style="western"><surname>Suied</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>Fast recognition of musical sounds based on timbre</article-title>. <source>J Acoust Soc Am</source> <volume>131</volume>: <fpage>4124</fpage>–<lpage>4133</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Goto1">
        <label>57</label>
        <mixed-citation publication-type="other" xlink:type="simple">Goto M, Hashiguchi H, Nishimura T, Oka R (2003) RWC music database: Music genre database and musical instrument sound database. In: Proceedings of International Symposium on Music Information Retrieval. Washington D.C, USA. pp. 229–230.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Chi1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ru</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2005</year>) <article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title>. <source>J Acoust Soc Am</source> <volume>118</volume>: <fpage>887</fpage>–<lpage>906</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-DeLathauwer1">
        <label>59</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Lathauwer</surname><given-names>L</given-names></name>, <name name-style="western"><surname>De Moor</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Vandewalle</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>A multilinear singular value decomposition</article-title>. <source>SIAM Journal on Matrix Analysis and Applications</source> <volume>21</volume>: <fpage>1253</fpage>–<lpage>1278</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Fritz1">
        <label>60</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fritz</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Klein</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title>. <source>Nat Neurosci</source> <volume>6</volume>: <fpage>1216</fpage>–<lpage>1223</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Fritz2">
        <label>61</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2005</year>) <article-title>Differential dynamic plasticity of A1 receptive fields during multiple spectral tasks</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>7623</fpage>–<lpage>7635</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Fritz3">
        <label>62</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fritz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2007</year>) <article-title>Adaptive changes in cortical receptive fields induced by attention to complex sounds</article-title>. <source>Journal of Neurophysiology</source> <volume>98</volume>: <fpage>2337</fpage>–<lpage>2346</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Klein1">
        <label>63</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klein</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Depireux</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Simon</surname><given-names>JZ</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2000</year>) <article-title>Robust spectrotemporal reverse correlation for the auditory system: optimizing stimulus design</article-title>. <source>J Comput Neurosci</source> <volume>9</volume>: <fpage>85</fpage>–<lpage>111</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002759-Donoho1">
        <label>64</label>
        <mixed-citation publication-type="other" xlink:type="simple">Donoho D (2000) High-Dimensional Data Analysis : The Curses and Blessings of Dimensionality. Aide-Memoire of a Lecture at AMS Conference on Math Challenges of the 21st Century. Los Angeles, California, USA.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>