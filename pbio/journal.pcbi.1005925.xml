<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005925</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01264</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Prefrontal cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Prefrontal cortex</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A neural network model for the orbitofrontal cortex and task space acquisition during reinforcement learning</article-title>
<alt-title alt-title-type="running-head">A neural network model for task space representations in orbitofrontal cortex</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Zhang</surname>
<given-names>Zhewei</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2185-7006</contrib-id>
<name name-style="western">
<surname>Cheng</surname>
<given-names>Zhenbo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lin</surname>
<given-names>Zhongqiao</given-names>
</name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Nie</surname>
<given-names>Chechang</given-names>
</name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6976-9246</contrib-id>
<name name-style="western">
<surname>Yang</surname>
<given-names>Tianming</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Institute of Neuroscience, Key Laboratory of Primate Neurobiology, CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai Institutes for Biological Sciences, Chinese Academy of Sciences</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>University of Chinese Academy of Sciences, Beijing, China</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname>
<given-names>Samuel J.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Harvard University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">tyang@ion.ac.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>4</day>
<month>1</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>1</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>1</issue>
<elocation-id>e1005925</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>7</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>12</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005925"/>
<abstract>
<p>Reinforcement learning has been widely used in explaining animal behavior. In reinforcement learning, the agent learns the value of the states in the task, collectively constituting the task state space, and uses the knowledge to choose actions and acquire desired outcomes. It has been proposed that the orbitofrontal cortex (OFC) encodes the task state space during reinforcement learning. However, it is not well understood how the OFC acquires and stores task state information. Here, we propose a neural network model based on reservoir computing. Reservoir networks exhibit heterogeneous and dynamic activity patterns that are suitable to encode task states. The information can be extracted by a linear readout trained with reinforcement learning. We demonstrate how the network acquires and stores task structures. The network exhibits reinforcement learning behavior and its aspects resemble experimental findings of the OFC. Our study provides a theoretical explanation of how the OFC may contribute to reinforcement learning and a new approach to understanding the neural mechanism underlying reinforcement learning.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Many studies employ reinforcement learning models to explain human or animal behavior, in which it is assumed that the animals know the task structure. Yet in the real life, the task structure also has to be acquired through learning. The orbitofrontal cortex has been proposed to play important roles in representing task structure, yet it is poorly understood how it does it and why it can do it. Here, we use a neural network model based on reservoir computing to approach these questions. We show that it is critical for the network to receive reward information as part of its input. Just as the orbitofrontal cortex that receives converging sensory and reward inputs, the network is able to acquire task structure and support reinforcement learning by encoding a combination of sensory and reward events. The importance of reward inputs in the model explains the sophisticate representations of reward information in the orbitofrontal cortex and provides a theoretic account of the current experimental data.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002367</institution-id>
<institution>Chinese Academy of Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>Hundreds of Talents Program</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6976-9246</contrib-id>
<name name-style="western">
<surname>Yang</surname>
<given-names>Tianming</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003399</institution-id>
<institution>Science and Technology Commission of Shanghai Municipality</institution>
</institution-wrap>
</funding-source>
<award-id>15JC1400104</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6976-9246</contrib-id>
<name name-style="western">
<surname>Yang</surname>
<given-names>Tianming</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Public Projects of Zhejiang Province</institution>
</funding-source>
<award-id>2016C31G2020069</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2185-7006</contrib-id>
<name name-style="western">
<surname>Cheng</surname>
<given-names>Zhenbo</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>Zhejiang Province</institution>
</funding-source>
<award-id>3rd Level in 151 talents project</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2185-7006</contrib-id>
<name name-style="western">
<surname>Cheng</surname>
<given-names>Zhenbo</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work is supported by the Chinese Academy of Sciences (<ext-link ext-link-type="uri" xlink:href="http://english.cas.cn/" xlink:type="simple">http://english.cas.cn/</ext-link>) Hundreds of Talents Program and Science and Technology Commission of Shanghai Municipality (<ext-link ext-link-type="uri" xlink:href="http://www.stcsm.gov.cn/english/" xlink:type="simple">http://www.stcsm.gov.cn/english/</ext-link>) (15JC1400104) to TY, and by Public Projects of Zhejiang Province (<ext-link ext-link-type="uri" xlink:href="http://www.zjzwfw.gov.cn/" xlink:type="simple">http://www.zjzwfw.gov.cn/</ext-link>) (2016C31G2020069) and the 3rd Level in Zhejiang Province (<ext-link ext-link-type="uri" xlink:href="http://www.zjzwfw.gov.cn/" xlink:type="simple">http://www.zjzwfw.gov.cn/</ext-link>) "151 talents project” to ZC. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-01-17</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files. The source code for the model in the manuscript and the corresponding analyses has been uploaded and can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/tyangLab/ReservoirNet_OFC_TaskState" xlink:type="simple">https://github.com/tyangLab/ReservoirNet_OFC_TaskState</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Even the simplest reinforcement learning (RL) algorithm captures the essence of operant conditioning in psychology and animal learning [<xref ref-type="bibr" rid="pcbi.1005925.ref001">1</xref>]. That is, actions that are rewarded tend to be repeated more frequently; actions that are punished are more likely to be avoided. RL requires one to understand the structures of the task and evaluate the value of the states in the task state space. Several studies have investigated the possible brain structures that may be involved in RL [<xref ref-type="bibr" rid="pcbi.1005925.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref006">6</xref>]. Notably, the orbitofrontal cortex (OFC) has been hypothesized to represent the task space and encode task states [<xref ref-type="bibr" rid="pcbi.1005925.ref007">7</xref>]. Several lesion studies showed that the animals with OFC lesions exhibited deficits acquiring task information for building a task structure [<xref ref-type="bibr" rid="pcbi.1005925.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref010">10</xref>]. Consistent with this idea, single unit recording experiments have revealed that the OFC neurons encode many aspects of task information, including reward value [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref015">15</xref>], probability [<xref ref-type="bibr" rid="pcbi.1005925.ref016">16</xref>], risk [<xref ref-type="bibr" rid="pcbi.1005925.ref017">17</xref>], information value [<xref ref-type="bibr" rid="pcbi.1005925.ref018">18</xref>], abstract rules [<xref ref-type="bibr" rid="pcbi.1005925.ref019">19</xref>], and strategies [<xref ref-type="bibr" rid="pcbi.1005925.ref020">20</xref>]. Yet, there is a lack of theoretical understanding how task structures may be encoded and represented by a neural network, and what sort of neuronal firing properties we expect to find in neurophysiological experiments. Furthermore, we do not know how to teach a task-agnostic neural network to acquire the structure of the task just based on reward feedbacks.</p>
<p>In the current study, we provide a solution based on the reservoir network [<xref ref-type="bibr" rid="pcbi.1005925.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref023">23</xref>]. Reservoir networks are recurrent networks with fixed connections. Within a reservoir network, neurons are randomly and sparsely connected. Importantly, the internal states of a reservoir exhibit rich temporal dynamics, which represents a nonlinear transformation of its input history and can be useful for encoding task state sequences. The information encoded by the network can be extracted with a linear output, which can be trained during learning. Reservoir networks have been shown to exhibit dynamics similar to that observed in the prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1005925.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref026">26</xref>]. Furthermore, it has been shown that reservoir networks may be combined with reinforcement learning to learn action values [<xref ref-type="bibr" rid="pcbi.1005925.ref027">27</xref>].</p>
<p>One key feature of our reservoir-based network model that makes learning task structures possible is including reward itself as an input to the reservoir. Thereby, the network dynamics represents a combination of not only the sensory events, but also the reward outcome. Reinforcement learning helps to shape the output of the reservoir, essentially picking out the action that will lead to the event sequences with desired rewards.</p>
<p>We demonstrate with two commonly used learning paradigms how the network model works. Task event sequences, including reward events, are provided as inputs to the network. A simple yet biologically feasible reward-dependent Hebbian learning algorithm is used to adjust its output weights. We show that our network model can solve problems with different task structures and reproduce behavior experiments previously conducted in animals and humans. We further demonstrate the similarities between the reservoir network and the OFC. Manipulations to our network reproduce the behavior of animals with OFC lesions. Moreover, the reservoir neurons’ response patterns resemble characteristics of the OFC neurons reported from previous electrophysiological experiments. Taken together, these results suggest a simple mechanism that naturally leads to the acquisition of task structure and supports RL. Finally, we propose some future experiments that may be used to test our model.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We describe our results in three parts. We start with using our network to model a classical reversal learning task. We take advantage of the simplicity of the task to explain the principal ideas behind the network model and why we think the network resembles the OFC. Then we show such a network may be applied to a more complex scenario, both in the task structure and in the temporal dynamics, in which the OFC has been shown to play important roles. Finally, to further illustrate the similarities between our network model and the OFC, we demonstrate how the selectivity of the neurons in the network may resemble experimental findings in the OFC during value-based decision making.</p>
<sec id="sec003">
<title>Reversal learning</title>
<p>In a classical reversal learning task, the animals have to keep track of the reward contingency of two choice options that may be reversed during a test session [<xref ref-type="bibr" rid="pcbi.1005925.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref028">28</xref>]. Normal animals were found to learn reversals faster and faster, which has been used as an indication of their ability of learning the structure of the task [<xref ref-type="bibr" rid="pcbi.1005925.ref007">7</xref>]. Such behavior was however found to be impaired in animals with OFC lesions and/or with lesions that contained fibers passing near the OFC [<xref ref-type="bibr" rid="pcbi.1005925.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref029">29</xref>]. These animals were not able to learn reversals faster and faster when they were repeatedly tested. The learning impairments could be explained by a deficit in acquiring and representing the task structure [<xref ref-type="bibr" rid="pcbi.1005925.ref007">7</xref>].</p>
<p>Our neural network model consists of a state encoding layer (SEL), which is a reservoir network. It receives three inputs and generates two outputs (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1A</xref>). The three inputs from the input layer (IL) to the SEL are the two choice options <italic>A</italic> and <italic>B</italic>, together with a reward input that indicates whether the choice yields a reward or not in the current trial. The outputs units in the decision-making output layer (DML) represent choice actions A and B for the next trial. The inputs are provided concurrently and the neural activity of the SEL at the end of the trial is used to determine the SEL’s output (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1B</xref>). The connections from the IL to the SEL and the connections within the SEL are fixed. Only the connection weights from the SEL to the DML are modified during the training with a reward dependent Hebbian rule, in which the weight changes are proportional to the reward prediction error and the pre- and post-synaptic neuronal activities.</p>
<fig id="pcbi.1005925.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Reversal learning task.</title>
<p><bold>A.</bold> The schematic diagram of the model. The network is composed of three parts: input layer (IL), the state encoding layer (SEL) and the decision-making output layer (DML). <bold>B.</bold> The event sequence. The stimulus and reward inputs are given concurrently at 200 ms after the trial onset and last for 500 ms. After a 200 ms delay, the decision is computed with the neural activity at 900 ms after the trial onset. <bold>C.</bold> The number of the error trials made before the network achieves the performance threshold. The dark line indicates the performance of the network with the reward input; the light line indicates the performance of the network without the reward input as a model for animals of OFC lesions. Stars indicate significant difference (One-way ANOVA, p&lt;0.05).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g001" xlink:type="simple"/>
</fig>
<p>The network is able to reproduce animals’ behavior. The number of the error trials that takes for the network to achieve the performance threshold, which is set at 93% in the initial learning and at 80% in the subsequent reversals, decreases as the network goes through more and more reversals (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1C</xref>). Interestingly, a learning deficit similar to that found in OFC-lesion animals is observed if we remove the reward input to the SEL (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1C</xref>). As the OFC and its neighboring brain areas such as the ventromedial prefrontal cortex (vmPFC) are known to receive both the sensory inputs and reward inputs from sensory and reward circuitry in the brain [<xref ref-type="bibr" rid="pcbi.1005925.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref032">32</xref>], removing the reward input from our model mimics the situation where the brain has to learn without functional structures in or near the OFC.</p>
<p>Neurons in the SEL, as expected from a typical reservoir network, show highly heterogeneous response patterns. Some neurons are found to encode the stimulus identity, some neurons encode reward, and others show mixed tuning (<xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2A</xref>). A principal component analysis (PCA) based on the population activity shows that the network can distinguish all four possible task states: choice <italic>A</italic> rewarded, choice <italic>A</italic> not rewarded, choice <italic>B</italic> rewarded, and choice <italic>B</italic> not rewarded (<xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2B</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005925.s001">S1 Fig</xref>). The first three principal components capture 92.0% variance of the population activity.</p>
<fig id="pcbi.1005925.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Network analyses for the reversal learning task.</title>
<p><bold>A.</bold> Selectivity of three example neurons in the reservoir network. Input units are set to 1 from 200ms to 700ms. Left panel: an example neuron that encodes choice options; middle panel: an example neuron that encodes reward outcomes; right panel: an example neuron with mixed selectivity. <bold>B.</bold> PCA on the network population activity. The network states are plotted in the space spanned by the first 3 PCA components. The activities in different conditions are differentiated after the cue onset. <bold>C.</bold> The difference between the SEL neurons’ connection weights to DML unit <italic>A</italic> and DML unit <italic>B</italic>. The SEL neurons are grouped according to their selectivities. For example, <italic>AR</italic> represents the group of neurons that respond most strongly when the input units <italic>A</italic> and <italic>R</italic> are both activated. The gray and white area indicates the blocks in which the option <italic>A</italic> and the option <italic>B</italic> leads to the reward, respectively. <bold>D.</bold> Left. The proportion of the blocks in which the network does not reach the performance criterion within a block after we remove 50 neurons that are random chosen (control), A selective, or AR selective. Right. The number of errors that the network makes before reaching the criterion with the same 3 types of inactivation. Only the data from the A-rewarding blocks are analyzed. The error bars are s.e.m. based on 10 simulation runs. A one-way ANOVA is used to determine the significance (p&lt;0.05). <bold>E.</bold> The number of errors needed to reach the performance criterion is maintained after the training stops at the 50<sup>th</sup> reversal. The error bars are s.e.m. calculated based on 10 simulation runs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g002" xlink:type="simple"/>
</fig>
<p>The ability to distinguish these states is essential for learning. To understand the task acquisition behavior exhibited by our model, we study how neurons with different selectivity contribute to the learning (<xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2C</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005925.s002">S2 Fig</xref>). We find that readout weights of the SEL neurons that are selective to the combination of stimulus and reward inputs (e.g. <italic>AR</italic> and <italic>BR</italic>) are mostly affected by the learning. The difference between the weights of their connections to the outputs <italic>A</italic> and <italic>B</italic> keeps evolving despite repeated reversals. In contrast, the weights of the output connections of pure stimulus-selective neurons (e.g. <italic>A</italic> and <italic>B</italic>) only wiggle around the baseline between reversals. Once the network is trained, the expected rewards from <italic>AR</italic>/<italic>BN</italic> and <italic>BR</italic>/<italic>AN</italic> inputs are exactly the opposite (<xref ref-type="supplementary-material" rid="pcbi.1005925.s003">S3 Fig</xref>).</p>
<p>The difference between these two groups of neurons explains why our network achieves flexible learning behavior only when the reward input is available. Let us first consider the <italic>AR</italic> neurons, which are selective for the situation when choice <italic>A</italic> leads to reward. In these <italic>A</italic>-rewarded blocks, the connections between the <italic>AR</italic> neurons and the DML neuron of choice <italic>A</italic> are strengthened. When the reward contingency is reversed and now choice <italic>A</italic> leads to no reward, the connections between the <italic>AR</italic> neurons and choice <italic>A</italic> are not affected very much. That is because the group of <italic>AN</italic> and then <italic>BR</italic> neurons instead of the <italic>AR</italic> neurons are activated in the blocks when choice <italic>A</italic> is not rewarded. As the result, the connections between the <italic>AN</italic> neurons and the DML neuron of choice <italic>B</italic> are strengthened and the connections between the <italic>AN</italic> neurons and the DML neuron of choice <italic>A</italic> are weakened. When the reward contingency is flipped again, the connections between the <italic>AR</italic> neurons and the DML neuron of choice <italic>A</italic> are strengthened further. This way, the learning is never erased by the reversals, and the network learns faster and faster. In comparison, let us now consider the <italic>A</italic> neurons, which encode only the sensory inputs and are activated whenever input <italic>A</italic> is present. In the <italic>A</italic>-rewarded blocks, the connections between the <italic>A</italic> neurons and the DML neuron of choice <italic>A</italic> are strengthened. In <italic>B</italic>-rewarded blocks, the connections between the <italic>A</italic> neurons and the DML neuron of choice <italic>A</italic> are however weakened when the network chooses <italic>A</italic> and gets no reward, and the learning in the previous block is reversed. Thus, the output connections of <italic>A</italic> neurons only fluctuate around the baseline with the reversals. They do not contribute much to the learning, and the overall behavior of the network is mostly driven by neurons that are activated by the combination of reward input and sensory inputs. Removing <italic>R</italic> deactivates these neurons and leads to the structure agnostic behavior.</p>
<p>The importance of the neurons that are selective for the combination of stimulus and reward inputs can be further illustrated by a simulated lesion experiment. After the network is well-trained, we stop the training and test the network’s performance with a proportion of neurons randomly removed at the time of decision (<xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2D</xref>). The neurons that are removed are either 50 randomly chosen neurons, 50 <italic>A</italic> neurons, or 50 <italic>AR</italic> neurons. This inactivation happens only at the time of decision making, therefore the state encoding in the reservoir is not affected. The inactivation of <italic>AR</italic> neurons produces the largest impairment in the network’s performance. Compared to the network with random inactivation, the network with <italic>AR</italic>-specific inactivation cannot reach the criterion we set previously within a block in more than 50% of the blocks and makes significantly more errors to reach the criterion in the blocks that it does. Inactivation of A-selective neurons produces much smaller performance deficits.</p>
<p>It is important to note that although the reinforcement learning algorithm employs the same small learning rate for both the intact network and the “OFC-lesion” network, the former only requires a few number of trials to acquire a reversal in the later stage of training, indicating the reversal behavior may not have to be slow with a small learning rate. In fact, once the network is trained, learning is no longer necessary for the reversal behavior. The network takes very few trials to adapt to reversals without learning (<xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2E</xref>). That is because the association between input <italic>AR</italic>/<italic>BN</italic> and decision <italic>A</italic> and the association between input <italic>BR</italic>/<italic>AN</italic> and decision <italic>B</italic> have been established in the network.</p>
</sec>
<sec id="sec004">
<title>Two-stage Markov decision task</title>
<p>We further test our network with a two-stage decision making task. The task is similar to the Markov decision task used previously in several human fMRI studies and used to study the model-based reinforcement learning behavior in humans [<xref ref-type="bibr" rid="pcbi.1005925.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref036">36</xref>]. In this task, the subjects have to choose between two options <italic>A1</italic> and <italic>A2</italic>. Their choices then lead to two intermediate outcomes <italic>B1</italic> and <italic>B2</italic> at different but fixed probabilities. The choice of <italic>A1</italic> more likely leads to <italic>B1</italic>, and the choice of <italic>A2</italic> is more likely followed by <italic>B2</italic>. Importantly, the final reward is contingent only on these intermediate outcomes, and the contingency is reversed across blocks (<xref ref-type="fig" rid="pcbi.1005925.g003">Fig 3A</xref>). Thus, the probability of getting a reward is higher for <italic>B1</italic> in one block and becomes lower in the next block. The probabilistic association between the initial choices and the intermediate outcomes never changes. The subjects are not informed of the structure of the task, and they have to figure out the best option by tracking not only the reward outcomes but also the intermediate outcomes.</p>
<fig id="pcbi.1005925.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Two-stage Markov decision task.</title>
<p><bold>A.</bold> Task structure of the two-stage Markov decision task. Two options <italic>A1</italic> and <italic>A2</italic> are available, they lead to two intermediate outcomes <italic>B1</italic> and <italic>B2</italic> at different probabilities. The width of the arrows indicates the transition probability. Intermediate outcomes <italic>B1</italic> and <italic>B2</italic> lead to rewards at different probability, and the reward contingency of the intermediate outcomes is reversed between blocks. <bold>B.</bold> The schematic diagram of the model. It is similar to the model in <xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1A</xref>. The only difference is that there are more input units. <bold>C.</bold> The event sequence. Units in the input layer are activated sequentially. In the example trial, option <italic>A1</italic> is chosen, <italic>B1</italic> is presented, and a reward is obtained.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g003" xlink:type="simple"/>
</fig>
<p>We keep our network model mostly the same as in the previous task. Here, we have two additional input units that reflect the intermediate outcomes (<xref ref-type="fig" rid="pcbi.1005925.g003">Fig 3B</xref>). To demonstrate our network model’s capability of encoding sequential events, the input units are activated sequentially in our simulations as they are in the real experiment (<xref ref-type="fig" rid="pcbi.1005925.g003">Fig 3C</xref>). We also add a non-reward input unit whose activity is set to 1 when a reward is not obtained at the end of a trial. The additional non-reward input facilitates learning but does not change the results qualitatively.</p>
<p>For a simple temporal difference learning strategy without using any knowledge of task structure, the probability of repeating the previous choice only depends on its reward outcome. The probability of repeating the previous choice is higher when a reward is obtained than when no reward is obtained. The intermediate outcome is ignored. However, this is no longer the case when the task structure is taken into account. For example, consider the situation when the subject initially chooses <italic>A1</italic>, the intermediate outcome happens to be <italic>B2</italic>, and a reward is obtained. If the subject understands <italic>B2</italic> is an unlikely outcome of choice <italic>A1</italic> (rare), but a likely outcome of choice <italic>A2</italic> (common), a reward obtained after the rare event <italic>B2</italic> should actually motivate the subject to switch from the previous choice <italic>A1</italic> and choose <italic>A2</italic> the next time. The subject should always choose the option that is more likely to lead to the intermediate outcome that is currently associated with the better reward.</p>
<p>To quantify the learning behavior, we first evaluate the impact of the previous trial’s outcome on the current trial. We classify all trial outcomes into four categories: common-rewarded (<italic>CR</italic>), common-unrewarded (<italic>CN</italic>), rare-rewarded (<italic>RR</italic>) and rare-unrewarded (<italic>RN</italic>). Here, common and rare indicate whether the intermediate outcome is the more likely outcome of the chosen option or not. Glascher et al [<xref ref-type="bibr" rid="pcbi.1005925.ref006">6</xref>] showed that the model based learning led to a higher probability of repeating the previous choice in the <italic>CR</italic> and <italic>RN</italic> conditions. This is also what we observe in our network model’s behavior (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4A</xref>).</p>
<fig id="pcbi.1005925.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Network analyses for the Two-stage Markov decision task.</title>
<p><bold>A.</bold> Factorial analysis of choice behavior. The network is more likely to repeat the choice under the conditions common-rewarded (<italic>CR</italic>) and rare-unrewarded (<italic>RN</italic>) than under the conditions common-unrewarded (<italic>CU</italic>) and rare-rewarded (<italic>RR</italic>). <bold>B.</bold> The task structure index keeps growing in the intact network (blue line), but stays at a low level when the reward input is missing (red line). Stars indicate significant difference (One-way ANOVA, p&lt;0.05). <bold>C.</bold> Fitting the behavioral performance with a mixture of task-agnostic and task-aware algorithms. The weight parameter <italic>w</italic> for learning with the knowledge of the task structure is significantly larger for the intact network (blue data points) than the network without the reward input (red data points). Each data point represents a simulation run. A one-way ANOVA is used to determine the significance (p&lt;0.05). <bold>D.</bold> PCA on the network population activity. The network states are plotted in the space spanned by the first 3 PCA components. The network can distinguish all 8 different states. <bold>E.</bold> The weight differences between the connections between SEL neurons and the DML unit <italic>A1</italic> and DML unit <italic>A2</italic>. The gray and white areas indicate the blocks in which intermediate outcome <italic>B1</italic> is more likely to lead to a reward and the blocks in which <italic>B2</italic> is more likely to lead to a reward, respectively. <bold>F.</bold> Logistic regression shows that only the last trial’s state affect the choice. The regression includes four different states (intermediate outcome x reward outcome) for each trial up to 10 trials before the current trials. Error bars show s.e.m. across simulation runs. <bold>G.</bold> Logistic regression reveals that only the combination of the intermediate states and the reward outcome in the last trial affects the decision. The factors being evaluated are: Correct—a tendency to choose the better choice in current block; Reward—a tendency to repeat the previous choice if it is rewarded; Stay—a tendency to repeat the previous choice; Transition—a tendency to repeat the same choice following common intermediate outcomes and switch the choice following rare intermediate outcomes; Trans x Out–a tendency to repeat the same choice if a common intermediate outcome is rewarded or a rare intermediate outcome unrewarded, and to switch the choice if a common intermediate outcome is unrewarded or a rare intermediate outcome rewarded.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g004" xlink:type="simple"/>
</fig>
<p>To illustrate how the network acquires the task structure, we define the task-structure index, which represents the tendency of employing task structure information (see the <xref ref-type="sec" rid="sec013">Method</xref>). The task-structure index grows larger as the training goes on (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4B</xref>). It indicates that the network learns the structure of the task gradually and transits to a more efficient behavior from an initially task-agnostic behavior.</p>
<p>Similar to our findings in the first task, the network without the reward input in the SEL behaves in a task-agnostic manner. It does not show the transition that indicates the learning of the task structure (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4B</xref>). We further quantify the contribution of task structure information to the network behavior using a model fitting procedure previously described by Glascher et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref006">6</xref>], and the network without the reward input shows a significantly smaller weight for the usage of task structure, suggesting it is worse at picking up the task structure (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4C</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005925.s004">S4 Fig</xref>). When the network time constant is sufficiently long, the task-structure dependent behavior is not because the intermediate outcomes occur after the first stage outcomes so that the former having a stronger representation in the network at the time of decision (<xref ref-type="supplementary-material" rid="pcbi.1005925.s005">S5 Fig</xref>).</p>
<p>Again, a PCA on the SEL population activity shows that the SEL distinguishes different task states (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4D</xref>). The first three principal components explain 83.97% variance of the population activity. Because the structure of the task in which the contingency between the first stage options and the intermediate outcomes is fixed, the network only needs to find out the current reward contingency of the intermediate outcomes. We found that the learning picks out the most relevant neurons that encode the contingency between the intermediate outcomes and the reward outcomes (<italic>B1R</italic>, <italic>B2R</italic>, etc.). Their connection weights to the DML neurons show better and better differentiation of the two choices throughout the training (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4E</xref>). In contrast, the connection weights of the neurons that encode the association between the first stage options and the reward outcomes (<italic>A1R</italic>, <italic>A2R</italic>, etc.) are less differentiated.</p>
<p>These results suggest that the network acquires the task structure. It understands that the contingency between intermediate outcomes and reward outcomes is the key to the performance. Thus, its choice only depends on the interaction between the intermediate outcome and the reward outcome of the last trial, but not on the other factors (<xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4F and 4G</xref>). The network behavior is similar to the <italic>Reward-as-cue agent</italic> described by Akam et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref037">37</xref>].</p>
</sec>
<sec id="sec005">
<title>Value representation by the OFC</title>
<p>Previous electrophysiology studies have shown that OFC neurons encode value during economic choices [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref013">13</xref>]. In a series of studies carried out by Padoa-Schioppa and his colleagues, monkeys were required to make choices between two types of juice in different amounts. The monkeys’ choices depended on both their juice preference and the reward magnitude. Recordings in the OFC revealed multiple classes of neurons encoding a variety of information, including the value of individual offers (offer value), the value of the chosen option (chosen value), and the identity of the chosen option (chosen identity) [<xref ref-type="bibr" rid="pcbi.1005925.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref039">39</xref>].</p>
<p>Here we show that our network model may explain this apparent heterogeneous value encoding in the OFC. We model the two-alternative economic choice task by providing two inputs to the SEL, representing the reward magnitude of each option with range adaption (<xref ref-type="fig" rid="pcbi.1005925.g005">Fig 5A</xref>). The input dynamics are similar to that of the sensory neurons [<xref ref-type="bibr" rid="pcbi.1005925.ref040">40</xref>]. The network model reproduces the choice behavior of monkeys (<xref ref-type="fig" rid="pcbi.1005925.g005">Fig 5C</xref>)[<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>].</p>
<fig id="pcbi.1005925.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Value-based decision-making task.</title>
<p><bold>A.</bold> The schematic diagram of the model. <bold>B.</bold> The event sequence. The stimuli are presented between 300 ms and 1300 ms after the trial onset. The decision is computed with the neural activity at 1400 ms after the trial onset. The input neurons’ activity profiles mimic those of real neurons (see <xref ref-type="sec" rid="sec013">Methods</xref>). <bold>C.</bold> Choice pattern. The relative value preference calculated based on the network behavior is indicated on the top left, and the actual relative value preference used in the simulation is 1<italic>A</italic> = 2<italic>B</italic>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g005" xlink:type="simple"/>
</fig>
<p>Then we study the selectivity of the SEL neurons. Just as in the previous experimental findings in the OFC, we find not only neurons that encode the value of each option (offer value neurons, middle panel in <xref ref-type="fig" rid="pcbi.1005925.g006">Fig 6A</xref>), but also neurons that encode the value of the chosen option (chosen value neurons, left panel in <xref ref-type="fig" rid="pcbi.1005925.g006">Fig 6A</xref>). Furthermore, a proportion of neurons show the selectivity for the choice as previously reported (chosen identity neurons, right panel in <xref ref-type="fig" rid="pcbi.1005925.g006">Fig 6A</xref>). We classify the neurons in the reservoir network into 10 categories as described in Padoa-Schioppa and Assad [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>]. Interestingly, we are able to find neurons in the reservoir in 9 of the 10 previous described categories (<xref ref-type="fig" rid="pcbi.1005925.g006">Fig 6B and 6C</xref>). The only missing category (neurons encoding other/chosen value) was also rarely found in the experimental data. Although the proportions of neurons encoding each category are not an exact copy of the experimental data, but the similarity is apparent. This is surprising given that we do not tune the internal connections of the SEL to the task. The results are robust across different input connection gains, noise levels in the SEL, and dynamics of the input profiles (<xref ref-type="supplementary-material" rid="pcbi.1005925.s006">S6 Fig</xref>). The heterogeneity that is naturally expected from a reservoir network takes much more effort to explain with recurrent network models that have a well-defined structure [<xref ref-type="bibr" rid="pcbi.1005925.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref041">41</xref>].</p>
<fig id="pcbi.1005925.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005925.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Value selectivity of the network neurons.</title>
<p><bold>A.</bold> Three example neurons in the SEL. Left panel: a neuron that encodes chosen value; middle panel: a neuron that encodes offer value; right panel: a neuron that encodes chosen juice. <bold>B.</bold> The proportions of the neurons with different selectivities from a previous experimental study [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>]. <bold>C.</bold> The proportions of the neurons in the reservoir network with different selectivities.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.g006" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>So far, we have shown that a simple reservoir-based network model may acquire task structures. The more interesting question is that why the network is capable of doing so and how this network model may help us to understand the functions of the OFC.</p>
<sec id="sec007">
<title>Encoding of the task space</title>
<p>We place a reservoir network as the centerpiece of our model. Reservoir networks are large, distributed, nonlinear dynamical recurrent neural networks with fixed weights. Because of recurrent networks’ complicated dynamics, they are especially useful in modeling temporal sequences including languages [<xref ref-type="bibr" rid="pcbi.1005925.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref043">43</xref>]. Neurons in reservoir networks exhibit mixed selectivity that maps inputs into a high dimensional space. Such selectivity has been shown to be crucial in complex cognitive tasks, and experimental works have provided evidence that neurons in the prefrontal cortex exhibit mixed selectivity [<xref ref-type="bibr" rid="pcbi.1005925.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref046">46</xref>]. In our model, the reservoir network encodes the combinations of inputs that constitute the task state space. States are encoded by the activities of the reservoir neurons, and the learned action values are represented by the weights of the readout connections.</p>
<p>There are several reasons why we choose reservoir networks to construct our model. First reason is that we would like to pair our network model with reinforcement learning. Reservoir networks have fixed internal connections; the training occurs only at the readout. The number of parameters for training is thus much smaller, which could be important for efficient reinforcement learning. Generality is another benefit offered by reservoir networks. Because the internal connections are fixed, we may use the same network to solve a different problem by just training a different readout. The reservoir can serve as a general-purpose task state representation network layer. Lastly, our results as well as several other studies show that neurons in reservoir networks–even with untrained connections weights–show properties similar to that observed in the real brain [<xref ref-type="bibr" rid="pcbi.1005925.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref047">47</xref>], suggesting training within the network for specific tasks may not play a role as important as previously thought.</p>
<p>The fact that the internal connections are fixed in a reservoir network means that the selectivity of the reservoir neurons is also fixed. This may seem at odds with the experimental findings of many OFC neurons shifting their encodings rapidly during reversals [<xref ref-type="bibr" rid="pcbi.1005925.ref048">48</xref>]. However, these observations may be interpreted differently when we take into account rewards. The neurons that were found to have different responses during reversals might in fact encode a combination of sensory events and rewards. On the other hand, there is evidence that OFC neurons with inflexible encodings during reversals might be more important for flexible behavior [<xref ref-type="bibr" rid="pcbi.1005925.ref049">49</xref>].</p>
<p>The choice of a reservoir network as the center piece of task event encoding may appear questionable to some. We do not train the network to learn task event sequences. Instead, we use the dynamic patterns elicited by task event sequences as bases for learning. This approach has obvious weaknesses. One is that the chaotic nature of network dynamics limits how well the task states can be encoded in the network. We have illustrated the network works well for relatively simple tasks. However, when we consider tasks that have many stages or many events, the combination of possible states grows quickly and may exceed the capacity of the network. The fact that we do not train the internal network connections does not help in this regard. However, the purpose of our network model is not to solve very complicated tasks. Instead, we would like to argue this is a more biologically-realistic model than many other recurrent networks. First, it does not depend on supervised learning to learn task event sequences [<xref ref-type="bibr" rid="pcbi.1005925.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref050">50</xref>]. Second, although the network performance may appear to be limited by task complexity, the real brain, however, also has limited capacity in learning multi-stage tasks [<xref ref-type="bibr" rid="pcbi.1005925.ref037">37</xref>]. Lastly, we show that a reservoir network can describe OFC neuronal responses during value-based decision making. Several other studies have also shown that reservoir networks may be a useful model of the prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1005925.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref025">25</xref>].</p>
</sec>
<sec id="sec008">
<title>Reward input to the reservoir</title>
<p>One key observation is that reward events must also be provided as inputs to the reservoir layer for the network model to perform well. Including reward events allows the network to establish associations between sensory stimuli and rewards, thus facilitates task structure acquisition. Although reward modulates neural activities almost everywhere in the cortex, the OFC plays a central role in establishing the association between sensory stimuli and rewards [<xref ref-type="bibr" rid="pcbi.1005925.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref052">52</xref>]. Anatomically, The OFC receives visual sensory inputs from inferior temporal and perirhinal cortex, as well as reward information from the brain areas in the reward circuitry, including the amygdala and ventral striatum, allowing it to have the information for establishing the association between visual information and reward [<xref ref-type="bibr" rid="pcbi.1005925.ref030">30</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref032">32</xref>]. Removing the reward input to the reservoir mimics the situation when animals cannot rely on such an association to learn tasks. In this case, the reservoir is still perfectly functional in terms of encoding task events other than rewards. This is similar to the situation when animals have to depend on their other memory structures in the brain–such as hippocampus or other medial temporal lobe structures–for learning. Consistent with this idea, it has been shown both the OFC and the ventral striatum are important for model-based RL [<xref ref-type="bibr" rid="pcbi.1005925.ref053">53</xref>]. The importance of the reward input to the reservoir explains the key role that the OFC plays in RL.</p>
<p>Several recent studies reported that selective lesions in the OFC did not reproduce the behavior deficits in reversal learning previously seen if the fibers passing through or near the OFC were spared [<xref ref-type="bibr" rid="pcbi.1005925.ref029">29</xref>]. Since these fibers probably carry the reward information from the midbrain areas, these results do not undermine the importance of reward inputs. Presumably, when the lesion is limited to the OFC, the projections that carrying the reward information are still available to or might even be redirected to other neighboring prefrontal structures, including ventromedial prefrontal cortex, which might take over the role of the OFC and contribute to the learning in animals with selective OFC lesions.</p>
</sec>
<sec id="sec009">
<title>Model-based reinforcement learning</title>
<p>The acquisition of task structure is a prerequisite for model-based learning. Therefore, it is interesting to ask whether our network model is able to achieve model-based learning. The two-stage task that we model has been used in human literature to study model-based learning [<xref ref-type="bibr" rid="pcbi.1005925.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref033">33</xref>–<xref ref-type="bibr" rid="pcbi.1005925.ref036">36</xref>]. Our model, although exhibiting behavior similar to human subjects, can be categorized as the Reward-as-cue agent that was described and categorized as a form of model-free reinforcement learning agent by Akam et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref037">37</xref>]. Yet, with reward incorporated as part of the task state space, goal-directed behavior can be achieved by searching in the state space for a task event sequence that ends with the desired goal and associating the sequence with appropriate actions. Thus, our network could in theory support model-based learning by providing the task structure to the downstream network layers.</p>
</sec>
<sec id="sec010">
<title>Extending the network</title>
<p>The performance of our network depends on several factors. First, it is important that reservoir should be able to distinguish between different task states. The number of possible task states may be only 4 or 8 as in our examples, or may be impossibly large even if the number of inputs increases only modestly. The latter is due to the infamous combinatorial explosion problem. One may alleviate the problem by introducing learning in the reservoir to enhance the representation of relevant stimulus combinations and weed out irrelevant ones. A recent study showed that the selectivity pattern in the prefrontal neurons may be better explained by a random network with Hebbian learning [<xref ref-type="bibr" rid="pcbi.1005925.ref054">54</xref>]. Second, the dynamics of the reservoir should allow information to be maintained long enough in a decipherable form until the decision is made. The recent developed gated recurrent neural networks may provide a solution with units that may maintain information for long periods [<xref ref-type="bibr" rid="pcbi.1005925.ref055">55</xref>]. Third, the model exhibits substantial variability between runs, suggesting the initialization may impact its performance. Further investigation is needed to make the model more robust. Last, we show that a reinforcement learning algorithm is capable of solving the relatively simple tasks in this study. However, it has been shown that reinforcement learning is in general not very efficient for extracting information from reservoir networks. Especially, when the task demands the information to be held for an extended period, for example, across different trials, the current learning algorithm fails to extract such relevant information from the reservoir. A possible solution is to introduce additional layers to help with the readout [<xref ref-type="bibr" rid="pcbi.1005925.ref025">25</xref>].</p>
</sec>
<sec id="sec011">
<title>Testable predictions</title>
<p>Our model makes several testable predictions. First, because of the reservoir structure, the inputs from the same source should be represented evenly in the network. For example, in a visual task, different visual stimuli should be represented at roughly the same strength in the OFC, even if their task relevance may be drastically different. Second, we should be able to find neurons encoding all relevant task parameters in the network, even when a particular combination of task parameters is never experienced by the brain. Third, reducing the number of inputs may make the network to be more efficient in certain tasks. This may seem counter-intuitive. But removing inputs reduces the number of states that the network has to encode, thus improves learning efficiency for tasks that do not require those additional states. For example, if we remove the reward input to the SEL, which is essential for learning tasks with volatile rewards, the network should however be more efficient at learning tasks in a more stable environment. Indeed, animals with OFC lesions were found to perform better than control animals when reward history was not important [<xref ref-type="bibr" rid="pcbi.1005925.ref056">56</xref>].</p>
</sec>
<sec id="sec012">
<title>Summary</title>
<p>Our network does not intend to be a complete model of how the OFC works. Instead of creating a complete neural network solution of reinforcement learning or the OFC, which is improbable at the moment, we are aiming at the modest goal of providing a proof of concept that approaches the critical problem of how the brain acquires the task structure with a biologically realistic neural network model. By demonstrating the network’s similarity to the experimental findings in the OFC, our study opens up new possibilities in future investigation.</p>
</sec>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Neural network model</title>
<p>The model is composed of three layers: an input layer (IL), a state encoding layer (SEL), and a decision-making output layer (DML) (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1A</xref>).</p>
<p>The units in the input layer represent the identities of sensory stimuli and the obtained reward. The input neurons are sparsely connected to the SEL units. The connection weights <inline-formula id="pcbi.1005925.e001"><alternatives><graphic id="pcbi.1005925.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> are set to 0 at a probability of 1-<italic>p</italic><sub>IR</sub>. Nonzero weights are assigned independently from a Gaussian distribution with zero mean and a variance of <italic>g</italic><sub>IR</sub><sup>2</sup></p>
<p>In the SEL, there are <italic>N</italic> = 500 neurons. The neurons in the SEL are connected with a low probability <italic>p</italic> = 0.1 and the connections are randomly and independently set from a Gaussian distribution with zero mean and a variance of <italic>g</italic><sup>2</sup>/(<italic>p*N)</italic>, where the gain <italic>g</italic> acts as the control parameter in the SEL. Connections in the SEL could be either positive or negative; a neuron may project both types of connections.</p>
<p>Each neuron in the SEL is described by an activation variable <italic>x</italic><sub><italic>i</italic></sub> for <italic>i</italic> = 1, 2, …, <italic>N</italic>, which is initialized according to a normal distribution <italic>N</italic>(0, <italic>σ</italic><sub>ini</sub><sup>2</sup>) at the beginning of each trial. <italic>x</italic><sub><italic>i</italic></sub> is updated at each time step (<italic>dt</italic> = 1ms) as follows:
<disp-formula id="pcbi.1005925.e002">
<alternatives>
<graphic id="pcbi.1005925.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mi mathvariant="normal">τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>τ</italic> represents the time constant, <italic>w</italic><sub><italic>ij</italic></sub> is the synaptic weight between neurons <italic>i</italic> and <italic>j</italic>, <italic>dW</italic><sub><italic>i</italic></sub> stands for the white noise, which is sampled from a uniform distribution [0, 1], and <italic>σ</italic><sub>noise</sub> is its variance. The firing rate <italic>y</italic><sub><italic>i</italic></sub> of neuron <italic>i</italic> is a function of the activation variable <italic>x</italic><sub><italic>i</italic></sub> relative to a minimal firing rate <italic>y</italic><sub>min</sub> = 0 and the maximal rate <italic>y</italic><sub>max</sub> = 1:
<disp-formula id="pcbi.1005925.e003">
<alternatives>
<graphic id="pcbi.1005925.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mspace width="1em"/><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="8em"/><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="1em"/><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>Here <italic>y</italic><sub>0</sub> = 0.1 is the baseline firing rate.</p>
<p>The SEL neurons project to the DML. The two competing neurons in the DML represent the two choices respectively. The total input of neuron <italic>k</italic> in the DML is
<disp-formula id="pcbi.1005925.e004">
<alternatives>
<graphic id="pcbi.1005925.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.75em"/><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.25em"/><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>w</italic><sub><italic>ik</italic></sub><sup><italic>(2)</italic></sup> is the weight of the synapse between neuron <italic>i</italic> in the SEL circuit and neuron <italic>k</italic> in the DML. The synaptic weights between the SEL and DML are randomly initialized according to uniform distribution [0, 1], and normalized to keep the squared sum of synaptic weights projecting to the same DML unit equal to 1.</p>
<p>The synaptic weights between the SEL and DML are updated based on the choice and the reward outcome during the training phase. The decision is based on the activities of output neuron. The stochastic choice behavior of our model is described by a softmax function:
<disp-formula id="pcbi.1005925.e005">
<alternatives>
<graphic id="pcbi.1005925.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
where <italic>E</italic>[<italic>r</italic><sub><italic>k</italic></sub>] denotes the expected value of choice <italic>a</italic><sub><italic>k</italic></sub>, <italic>p</italic><sub><italic>k</italic></sub> represents the probability of choosing choice <italic>a</italic><sub><italic>k</italic>,</sub> and the other choice is chosen with probability 1- <italic>p</italic><sub><italic>k</italic></sub>. <italic>β</italic> adjusts the competition strength of two choices, and <italic>v</italic><sub><italic>k</italic></sub> is the summed input of the DML unit <italic>k</italic>. The firing rate of the unit <italic>k</italic>, <italic>z</italic><sub><italic>k</italic></sub>, is set to 1 if choice <italic>a</italic><sub><italic>k</italic></sub> is chosen, otherwise it is set to 0.</p>
</sec>
<sec id="sec015">
<title>Reinforcement learning</title>
<p>At the end of each trial, the weights between the SEL and the DML neurons are updated based on the choice and the reward feedback.</p>
<p>The plastic weights in Eq (<xref ref-type="disp-formula" rid="pcbi.1005925.e004">3</xref>) in trial <italic>n</italic>+1 are updated as follows:
<disp-formula id="pcbi.1005925.e006">
<alternatives>
<graphic id="pcbi.1005925.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>The update term Δ<italic>w</italic><sub><italic>ik</italic></sub> depends on the reward prediction error and the responses of the neurons in the SEL circuit and DML:
<disp-formula id="pcbi.1005925.e007">
<alternatives>
<graphic id="pcbi.1005925.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
where <italic>η</italic> is the learning rate, and <italic>r</italic> is the reward. <italic>E</italic>[<italic>r</italic>] denotes the expected value of the chosen option, which is equal to the probability of choosing choice <italic>a</italic><sub><italic>k</italic></sub> and calculated with <xref ref-type="disp-formula" rid="pcbi.1005925.e005">Eq 4</xref> [<xref ref-type="bibr" rid="pcbi.1005925.ref057">57</xref>, <xref ref-type="bibr" rid="pcbi.1005925.ref058">58</xref>]. When the reward <italic>r</italic> is larger than <italic>E</italic>[<italic>r</italic>], the connections between the SEL neurons whose firing rate is above the threshold <italic>y</italic><sub><italic>th</italic></sub> and the neurons in the DML would be strengthened, and the connections between the neurons whose firing rate is below <italic>y</italic><sub><italic>th</italic></sub> and the neurons in the DML would be weakened. After each update, the weights <inline-formula id="pcbi.1005925.e008"><alternatives><graphic id="pcbi.1005925.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> are normalized:
<disp-formula id="pcbi.1005925.e009">
<alternatives>
<graphic id="pcbi.1005925.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
so that the vector length of <inline-formula id="pcbi.1005925.e010"><alternatives><graphic id="pcbi.1005925.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> remains constant. The normalization stops the weights from growing infinitely [<xref ref-type="bibr" rid="pcbi.1005925.ref059">59</xref>].</p>
<p>In the very first trial of a simulation run, the choice input is randomly selected, and the reward input is set according to the reward contingency in the block. The weights are not updated in the first trial. The choice output from the first trial and its associated reward outcome are then fed into the network as the 2nd trial’s input, which are used to calculate the decision for the 2nd trial and update the weights as described above.</p>
</sec>
<sec id="sec016">
<title>Behavior task</title>
<sec id="sec017">
<title>Reversal learning</title>
<p>The network has to choose between two options. One option leads to a reward, and the other does not. The stimulus-reward contingency is reversed every 100 trials. The criterion for learning is set to 28 correct trials in 30 successive trials for the initial learning and 24 correct trials in 30 successive trials for subsequent reversals.</p>
<p>The input layer units represent the identities of the two options and the reward. An option unit’s response is set to 1 for if the corresponding option is chosen in the current trial, otherwise it is set to 0. The reward unit’s response is set to 1 if the choice is rewarded in the current trial. The output of the network indicates its choice for the next trial. The input units representing choice options and reward are activated between 200 and 700ms after the trial onset. There is a delay period of 200 ms, at the end of which (900 ms after the trial onset) the neurons’ activities at 900ms are used for decision making (<xref ref-type="fig" rid="pcbi.1005925.g001">Fig 1C</xref>).</p>
<p>The network parameters are set as follows. Time constant <italic>τ</italic> = 100ms, network gain <italic>g</italic> = 2, training threshold <italic>y</italic><sub><italic>th</italic></sub> = 0.2, temperature parameter <italic>β</italic> = 4, learning rate <italic>η</italic> = 0.001, noise gain <italic>σ</italic><sub>noise</sub> = 0.01, initial noise gain <italic>σ</italic><sub>ini</sub> = 0.01, input connection gain g<sub>IR</sub> = 4, input connection probability <italic>p</italic><sub>IR</sub> = 0.2.</p>
<p>The selectivity of neurons in the SEL is determined at 900ms after the trial onset. A unit is defined as selective to a certain input or a combination of inputs if its responses are significantly higher under the condition when the input or all inputs of the combination are set to 1 than the other conditions (one-way ANOVA with multiple comparison and Bonferroni correction).</p>
</sec>
<sec id="sec018">
<title>Two-stage Markov decision task</title>
<p>The network has to make a choice between options <italic>A1</italic> and <italic>A2</italic>. <italic>A1</italic> leads to intermediate outcome <italic>B1</italic> at the probability of 80%, and <italic>B2</italic> at the probability of 20%. Vice versa, option <italic>A2</italic> leads to <italic>B2</italic> at the probability of 80%, and <italic>B1</italic> at a lower probability of 20%. The contingency between options (<italic>A1</italic>, <italic>A2</italic>) and intermediate outcomes (<italic>B1</italic>, <italic>B2</italic>) is fixed. Initially, <italic>B1</italic> leads to a reward at the probability of 80% and <italic>B2</italic> leads to reward at the probability of 20%. The reward contingency is reversed every 50 trials.</p>
<p>The input layer contains 6 units, representing the identities of two first stage options <italic>A1</italic> and <italic>A2</italic>, two intermediate outcomes <italic>B1</italic> and <italic>B2</italic>, and the reward and non-reward conditions, respectively. The activity of option unit <italic>A1</italic> or <italic>A2</italic> is set to 1 when the respective option is chosen. The activity of intermediate outcome unit <italic>B1</italic> or <italic>B2</italic> is set to 1 when the respective intermediate outcome is presented. The reward unit’s activity is set to 1 when a reward is obtained, and the non-reward unit’s activity is set to 1 when no reward is obtained. The units are activated sequentially, reflecting the sequential nature of the task. The <italic>A</italic> units are activated between 200 and 700ms after the trial onset, the <italic>B</italic> units between 700 and 1200ms, and the reward units between 1200 and 1700ms. Decision is made based on the neurons’ activity at 1900ms after the trial onset (<xref ref-type="fig" rid="pcbi.1005925.g003">Fig 3C</xref>).</p>
<p>The network parameters are set as follows. Time constant <italic>τ</italic> = 500ms, Network gain <italic>g</italic> = 2.25, training threshold <italic>y</italic><sub><italic>th</italic></sub> = 0.2, temperature parameter <italic>β</italic> = 2, learning rate <italic>η</italic> = 0.001, noise gain <italic>σ</italic><sub>noise</sub> = 0.01, initial noise gain <italic>σ</italic><sub>ini</sub> = 0.01, input connection gain g<sub>IR</sub> = 2, input connection probability <italic>p</italic><sub>IR</sub> = 0.2.</p>
<p>The selectivity of neurons in the SEL is determined at the time point when the decision is made. There are 8 conditions in this task, namely <italic>A1B1R</italic>, <italic>A1B1N</italic>, <italic>A2B1R</italic>, <italic>A2B1N</italic>, <italic>A1B2R</italic>, <italic>A1B2N</italic>, <italic>A2B2R</italic>, and <italic>A2B2N</italic>. For example, <italic>A1B1R</italic> indicates the condition when <italic>A1</italic> is chosen, intermediate outcome <italic>B1</italic> is presented, and a reward is obtained. A neuron’s preferred condition is the condition under which its activity is the largest and significantly higher than its activity under any other conditions (one-way ANOVA with multiple comparison and Bonferroni correction). Then the neurons are grouped into different categories based on their preferred conditions. The neurons in category <italic>A1R</italic> are the neurons whose preferred condition may be <italic>A1B1R</italic>, <italic>A1B2R</italic>, <italic>A2B1N</italic>, or <italic>A2B2N</italic>. All the preferred conditions of the neurons in category <italic>A1R</italic> provide evidence for associating <italic>A1</italic> with the reward. Similarly, the preferred conditions of the neurons in the category <italic>B1N</italic> are <italic>A1B1N</italic>, <italic>A1B2R</italic>, <italic>A2B1N</italic> and <italic>A2B2R</italic>. They provide evidence that <italic>B1</italic> is not associated with the reward.</p>
<p>In order to test how well the network uses the task structure information, we fit our data based on a simplified version of the model introduced by Daw et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref005">5</xref>]. The model fits the behavioral results with a mixture of model-free (task-agnostic) and model-based (task-aware) learning algorithm. In our simplified task, the network makes only one choice in each trial. The network first undergoes 2,000 trials of training before the analysis.</p>
<p>For the model-free agent, the state values for B<sub>1</sub>/B<sub>2</sub> and A<sub>1</sub>/A<sub>2</sub> are updated as follows:
<disp-formula id="pcbi.1005925.e011">
<alternatives>
<graphic id="pcbi.1005925.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
<disp-formula id="pcbi.1005925.e012">
<alternatives>
<graphic id="pcbi.1005925.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mi>λ</mml:mi><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where <italic>V</italic><sub><italic>MF</italic></sub> (<italic>s</italic><sub><italic>chosen_A</italic>, <italic>t</italic></sub>) and <italic>V</italic><sub><italic>MF</italic></sub> (<italic>s</italic><sub><italic>observed_B</italic>, <italic>t</italic></sub>) are the value of states A<sub>i</sub> and B<sub>i</sub> that are observed in trial <italic>t</italic>, <italic>r</italic><sub><italic>t</italic></sub> represents the reward feedback in trial <italic>t</italic>, <italic>α</italic><sub>1</sub> is the learning rate of the model-free learning algorithm, and the eligibility <italic>λ</italic> represents how large the proportion of credit from the reward can be given to states A<sub>i</sub> and actions in our task paradigm. The state value for unobserved states is not changed.</p>
<p>For the model-based agent, the state values for B<sub>1</sub>/B<sub>2</sub> and A<sub>1</sub>/A<sub>2</sub> are updated as follows:
<disp-formula id="pcbi.1005925.e013">
<alternatives>
<graphic id="pcbi.1005925.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
<disp-formula id="pcbi.1005925.e014">
<alternatives>
<graphic id="pcbi.1005925.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
<disp-formula id="pcbi.1005925.e015">
<alternatives>
<graphic id="pcbi.1005925.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
<italic>where α</italic><sub>2</sub> is the learning rate of the model-based learning algorithm. <italic>P</italic><sub><italic>Ai</italic>-<italic>Bi</italic></sub> indicates the probability of transition from state <italic>A</italic><sub><italic>i</italic></sub> to state <italic>B</italic><sub><italic>i</italic></sub>. The state value for unobserved states is not changed.</p>
<p>The net state value is defined as the weighted sum of the action values from the model-free agent and the model-based agent:
<disp-formula id="pcbi.1005925.e016">
<alternatives>
<graphic id="pcbi.1005925.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">w</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">w</mml:mi><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
where <italic>w</italic> is the weight. When <italic>w</italic> equals 1, the behavior uses full task information. When <italic>w</italic> equals 0, the behavior is completely task agnostic. The fitting is done by a maximum likelihood estimation procedure.</p>
<p>Finally, the probability of choosing option <italic>i</italic> is a softmax function of <italic>V</italic><sub><italic>net</italic></sub>.
<disp-formula id="pcbi.1005925.e017">
<alternatives>
<graphic id="pcbi.1005925.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>*</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>*</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
where <italic>rep</italic>(<italic>a</italic>) is set to 1 if action <italic>a</italic> is chosen in the previous trial. The inverse temperature parameter <italic>β</italic> is set to 2, which equals to the <italic>β</italic> term in e.q. (4) that generates the behavior. The parameter <italic>p</italic>, which captures the tendency for perseveration and switching, is set to 0. This is because we reset the network activity every trial. The conclusions hold when <italic>p</italic> is allowed to vary. Thus, there are only four free parameters, <italic>α</italic><sub><italic>1</italic></sub>, <italic>α</italic><sub><italic>2</italic></sub>, <italic>λ</italic> and <italic>w</italic>. Sessions with <italic>w</italic> deviating more than 3 standard deviations from the mean are excluded in <xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4C</xref> for cosmetic reasons. Including them increases the significance of weight difference between the two models without affecting the conclusion.</p>
<p>Inspired by the factorial analysis from Daw et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref005">5</xref>], we define a task-structure (TS) index (<xref ref-type="disp-formula" rid="pcbi.1005925.e018">Eq 15</xref>) to quantify how much task structure information is used in the network behavior. It is based on the tendency of repeating the choice in the last trial under different situations. The combination of the two reward outcomes and the two intermediate outcomes, common and rare, gives us four possible outcomes: common-rewarded (<italic>CR</italic>), common-unrewarded (<italic>CN</italic>), rare-rewarded (<italic>RR</italic>) and rare-unrewarded (<italic>RN</italic>). When the task structure is known, the agent is more likely to repeat the previous choice if the last trial is a <italic>CR</italic> or an <italic>RN</italic> trial. Higher TS index means that the behavioral pattern takes into account more task structure information.</p>
<disp-formula id="pcbi.1005925.e018">
<alternatives>
<graphic id="pcbi.1005925.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<p>The task state analysis is similar to what was used by Akam et al. [<xref ref-type="bibr" rid="pcbi.1005925.ref037">37</xref>]. Briefly, a logistic regression is used to estimate how states of past trials influence the current choice. The regression includes four different states (2 intermediate outcomes x 2 reward outcomes) for each trial up to 10 trials before the current trials.</p>
<p>Another logistic regression is used to estimate how several other potentially relevant factors affect choices. The factors considers include: Correct—a tendency to choose the better choice in current block; Reward—a tendency to repeat the previous choice if it is rewarded; Stay—a tendency to repeat the previous choice; Transition—a tendency to repeat the same choice following common intermediate outcomes and switch the choice following rare intermediate outcomes; Trans x Out–a tendency to repeat the same choice if a common intermediate outcome is rewarded or a rare intermediate outcome unrewarded, and to switch the choice if a common intermediate outcome is unrewarded or a rare intermediate outcome rewarded.</p>
</sec>
<sec id="sec019">
<title>Value-based economic choice task</title>
<p>Unlike the two previous paradigms, both options in this paradigm lead to a reward. Two input units represent the rewards associated with the two options, respectively. The input strength is proportional to reward magnitude. In our simulations, the reward <italic>A</italic> is valued twice as much as reward <italic>B</italic> for the same reward magnitude. The relative value preference between the two options is not provided as an input to the network directly. It is only used for calculating the expected value. Thus, it does not affect the SEL. The value of the reward is defined as the product of the relative value and the reward magnitude. The reward options are presented between 300 and 1300 ms after the trial onset. After a 100 ms delay period, the network activity is used to calculate decisions (<xref ref-type="fig" rid="pcbi.1005925.g005">Fig 5B</xref>).</p>
<p>The activity of the input unit f(<italic>t</italic>) during the stimulus period (between 300ms and 1300ms after the trial onset), is described by the following equations [<xref ref-type="bibr" rid="pcbi.1005925.ref040">40</xref>].
<disp-formula id="pcbi.1005925.e019">
<alternatives>
<graphic id="pcbi.1005925.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mi mathvariant="normal">g</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>475</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>30</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>700</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>100</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
<disp-formula id="pcbi.1005925.e020">
<alternatives>
<graphic id="pcbi.1005925.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mi mathvariant="normal">f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac bevelled="true"><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="normal">g</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
where <italic>t</italic> is the time in the unit of ms within a trial, <inline-formula id="pcbi.1005925.e021"><alternatives><graphic id="pcbi.1005925.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the magnitude of the reward type <italic>i</italic> in each trial, <inline-formula id="pcbi.1005925.e022"><alternatives><graphic id="pcbi.1005925.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the maximal reward magnitude of reward type <italic>i</italic> within the block, and <inline-formula id="pcbi.1005925.e023"><alternatives><graphic id="pcbi.1005925.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the minimal reward magnitude of reward type <italic>i</italic>, which is always 0 in our simulations.</p>
<p>The network activity at 1400ms is used for decision making and network training. The expected value is the sum of the product of the probability of choosing the option and corresponding reward magnitude.
<disp-formula id="pcbi.1005925.e024">
<alternatives>
<graphic id="pcbi.1005925.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi><mml:mo>*</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where <italic>p</italic><sub><italic>i</italic></sub> and <italic>m</italic><sub><italic>i</italic></sub> are the probability of choosing option <italic>i</italic> and its reward magnitude, and γ = 2 is the relative value preference between the two reward options. Only the data from the trials after 8000 trials training are included for the analyses. The network parameters are set as follows. Time constant <italic>τ</italic> = 100ms, Network gain <italic>g</italic> = 2.5, training threshold <italic>y</italic><sub><italic>th</italic></sub> = 0.2, temperature parameter <italic>β</italic> = 4, learning rate <italic>η</italic> = 0.005, noise gain <italic>σ</italic><sub>noise</sub> = 0.05, initial noise gain <italic>σ</italic><sub>ini</sub> = 0.2, input connection gain g<sub>IR</sub> = 2, input connection probability <italic>p</italic><sub>IR</sub> = 0.2.</p>
<p>As in Padoa-Schioppa and Assad [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>], the following variables are defined for further analysis: total value (the sum of the value of two options), chosen value (the value of the chosen option), other value (the value of the unchosen option), value difference (chosen-other value), value ratio (other/chosen value), offer value (the value of the one option), chosen juice (the identity of the chosen option), value A chosen (the value of the option A when option A is chosen), and value B chosen (the value of the option B when option B is chosen).</p>
<p>We use an analysis similar to that in Padoa-Schioppa and Assad [<xref ref-type="bibr" rid="pcbi.1005925.ref011">11</xref>] to study the selectivity of SEL units during the post-offer period (0-500ms after the stimulus onset). Linear regressions are applied to each variable to fit the neural responses in this time window for each SEL unit separately.
<disp-formula id="pcbi.1005925.e025">
<alternatives>
<graphic id="pcbi.1005925.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005925.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>*</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>var</italic> represents the variables previously mentioned. A variable is considered to explain the response of a neuron in the SEL if the slope of the fitting linear function, <italic>a</italic>, is significantly different from zero (p&lt;0.05, one-way ANOVA with Bonferroni correction).</p>
</sec>
</sec>
</sec>
<sec id="sec020">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005925.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>PCA on the population activity of the sub-networks AR (grey traces) and BR (blue traces).</title>
<p>The sub-networks AR and BR consist of neurons that are selective to AR and BR, respectively. The network states are plotted in the space spanned by the first 3 PCA components, which are from the same PC space as in <bold><xref ref-type="fig" rid="pcbi.1005925.g002">Fig 2B</xref></bold> and are calculated from all neurons in the network. Each trace represents a different stimulus condition.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005925.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Connection weight evolution.</title>
<p><bold>A.</bold> The difference between the connection weights of the AN and BN neurons in the SEL layer to DML unit <italic>A</italic> and DML unit <italic>B</italic>. Positive values indicate an SEL neuron has a stronger connection to DML unit A than to DML unit B and supports choice A. The gray and white area indicates the blocks in which the option <italic>A</italic> and the option <italic>B</italic> leads to the reward, respectively. <bold>B.</bold> Top row: AN neurons’ connections weight to DML unit A (left) and DML unit B (right). Bottom row: BN neurons’ connections weight to DML unit A (left) and DML unit B (right).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005925.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title><italic>E</italic>[<italic>r</italic>] for choice A in AR and BN trials (top panel) and AN and BR trials (bottom row) in A blocks.</title>
<p>Shade area indicates the standard s.e.m. <italic>E</italic>[<italic>r</italic>] at 900 ms (decision time) is used for updating the weights.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005925.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Factorial analysis of choice behavior for the full network model (left panel) and the model without reward input (right panel).</title>
<p>The full model exhibits stronger task-structure effects.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005925.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>The model-based behavior in networks with different settings.</title>
<p><bold>A.</bold> tau = 100 ms, A1/A2 occurs first at 200ms after the trial onset, and B1/B2 occurs at 700ms after the trial onset. <bold>B.</bold> tau = 500 ms, B1/B2 occurs first at 200ms after the trial onset, and A1/A2 occurs at 700ms after the trial onset. <bold>C.</bold> tau = 100 ms, B1/B2 occurs first at 200ms after the trial onset, and A1/A2 occurs at 700ms after the trial onset. Left column: the model structure index. Right column: weights for the model-based behavior. All the significance is evaluated by one-way ANOVA. See <xref ref-type="fig" rid="pcbi.1005925.g004">Fig 4B and 4C</xref> for details.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005925.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005925.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>The proportions of the neurons in the reservoir network with different selectivities are stable across network models with different network parameters stable.</title>
<p><bold>A.</bold> input connection gain = 0.5. <bold>B.</bold> noise gain = 0.5. <bold>C.</bold> A step function is used to model the reward inputs. The step function’s onset is 300 ms and its offset is 1300 ms after the trial onset. All results are based on 10 simulation runs.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Yu Shan and Xiao-Jing Wang for discussions and comments during the study.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005925.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rescorla</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>AR</given-names></name>. <chapter-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. In: <name name-style="western"><surname>Black</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Prokasy</surname> <given-names>WF</given-names></name>, editors. <source>Classical conditioning II: Current research and theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>; <year>1972</year>. p. <fpage>64</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haber</surname> <given-names>SN</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Mailly</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Calzavara</surname> <given-names>R</given-names></name>. <article-title>Reward-related cortical inputs define a large striatal region in primates that interface with associative cortical connections, providing a substrate for incentive-based learning</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>(<issue>32</issue>):<fpage>8368</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0271-06.2006" xlink:type="simple">10.1523/JNEUROSCI.0271-06.2006</ext-link></comment> <object-id pub-id-type="pmid">16899732</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5306</issue>):<fpage>1593</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">9054347</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kennerley</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>. <article-title>Double dissociation of value computations in orbitofrontal and anterior cingulate neurons</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>12</issue>):<fpage>1581</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2961" xlink:type="simple">10.1038/nn.2961</ext-link></comment> <object-id pub-id-type="pmid">22037498</object-id>; PubMed Central PMCID: PMCPMC3225689.</mixed-citation></ref>
<ref id="pcbi.1005925.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Model-Based Influences on Humans' Choices and Striatal Prediction Errors</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>6</issue>):<fpage>1204</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> WOS:000288886900015. <object-id pub-id-type="pmid">21435563</object-id></mixed-citation></ref>
<ref id="pcbi.1005925.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glascher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name>. <article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>66</volume>(<issue>4</issue>):<fpage>585</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.04.016" xlink:type="simple">10.1016/j.neuron.2010.04.016</ext-link></comment> <object-id pub-id-type="pmid">20510862</object-id>; PubMed Central PMCID: PMCPMC2895323.</mixed-citation></ref>
<ref id="pcbi.1005925.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Takahashi</surname> <given-names>YK</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Orbitofrontal cortex as a cognitive map of task space</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>2</issue>):<fpage>267</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.11.005" xlink:type="simple">10.1016/j.neuron.2013.11.005</ext-link></comment> <object-id pub-id-type="pmid">24462094</object-id>; PubMed Central PMCID: PMC4001869.</mixed-citation></ref>
<ref id="pcbi.1005925.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hornak</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bramham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Bullock</surname> <given-names>PR</given-names></name>, <etal>et al</etal>. <article-title>Reward-related reversal learning after surgical excisions in orbito-frontal or dorsolateral prefrontal cortex in humans</article-title>. <source>J Cogn Neurosci</source>. <year>2004</year>;<volume>16</volume>(<issue>3</issue>):<fpage>463</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892904322926791" xlink:type="simple">10.1162/089892904322926791</ext-link></comment> <object-id pub-id-type="pmid">15072681</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izquierdo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Suda</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>EA</given-names></name>. <article-title>Bilateral orbital prefrontal cortex lesions in rhesus monkeys disrupt choices guided by both reward value and reward contingency</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>(<issue>34</issue>):<fpage>7540</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1921-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1921-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15329401</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname> <given-names>YK</given-names></name>, <name name-style="western"><surname>Roesch</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Toreson</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>O'Donnell</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Expectancy-related changes in firing of dopamine neurons depend on orbitofrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>12</issue>):<fpage>1590</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2957" xlink:type="simple">10.1038/nn.2957</ext-link></comment> <object-id pub-id-type="pmid">22037501</object-id>; PubMed Central PMCID: PMCPMC3225718.</mixed-citation></ref>
<ref id="pcbi.1005925.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>JA</given-names></name>. <article-title>Neurons in the orbitofrontal cortex encode economic value</article-title>. <source>Nature</source>. <year>2006</year>;<volume>441</volume>(<issue>7090</issue>):<fpage>223</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04676" xlink:type="simple">10.1038/nature04676</ext-link></comment> <object-id pub-id-type="pmid">16633341</object-id>; PubMed Central PMCID: PMC2630027.</mixed-citation></ref>
<ref id="pcbi.1005925.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>. <article-title>Neurobiology of economic choice: a good-based model</article-title>. <source>Annu Rev Neurosci</source>. <year>2011</year>;<volume>34</volume>:<fpage>333</fpage>–<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-061010-113648" xlink:type="simple">10.1146/annurev-neuro-061010-113648</ext-link></comment> <object-id pub-id-type="pmid">21456961</object-id>; PubMed Central PMCID: PMCPMC3273993.</mixed-citation></ref>
<ref id="pcbi.1005925.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>. <article-title>Neuronal activity in primate dorsolateral and orbital prefrontal cortex during performance of a reward preference task</article-title>. <source>The European journal of neuroscience</source>. <year>2003</year>;<volume>18</volume>(<issue>7</issue>):<fpage>2069</fpage>–<lpage>81</lpage>. <object-id pub-id-type="pmid">14622240</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Esber</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>McDannald</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Gruber</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Hernandez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mirenzi</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Orbitofrontal cortex supports behavior and learning using inferred but not cached values</article-title>. <source>Science</source>. <year>2012</year>;<volume>338</volume>(<issue>6109</issue>):<fpage>953</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1227489" xlink:type="simple">10.1126/science.1227489</ext-link></comment> <object-id pub-id-type="pmid">23162000</object-id>; PubMed Central PMCID: PMC3592380.</mixed-citation></ref>
<ref id="pcbi.1005925.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rudebeck</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Mitz</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Chacko</surname> <given-names>RV</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>EA</given-names></name>. <article-title>Effects of amygdala lesions on reward-value coding in orbital and medial prefrontal cortex</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>(<issue>6</issue>):<fpage>1519</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.09.036" xlink:type="simple">10.1016/j.neuron.2013.09.036</ext-link></comment> <object-id pub-id-type="pmid">24360550</object-id>; PubMed Central PMCID: PMC3872005.</mixed-citation></ref>
<ref id="pcbi.1005925.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kennerley</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>. <article-title>Evaluating choices by single neurons in the frontal lobe: outcome value encoded across multiple decision variables</article-title>. <source>Eur J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>10</issue>):<fpage>2061</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1460-9568.2009.06743.x" xlink:type="simple">10.1111/j.1460-9568.2009.06743.x</ext-link></comment> <object-id pub-id-type="pmid">19453638</object-id>; PubMed Central PMCID: PMC2715849.</mixed-citation></ref>
<ref id="pcbi.1005925.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Neill</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>. <article-title>Economic risk coding by single neurons in the orbitofrontal cortex</article-title>. <source>J Physiol Paris</source>. <year>2015</year>;<volume>109</volume>(<issue>1–3</issue>):<fpage>70</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jphysparis.2014.06.002" xlink:type="simple">10.1016/j.jphysparis.2014.06.002</ext-link></comment> <object-id pub-id-type="pmid">24954027</object-id>; PubMed Central PMCID: PMC4451954.</mixed-citation></ref>
<ref id="pcbi.1005925.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blanchard</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Hayden</surname> <given-names>BY</given-names></name>, <name name-style="western"><surname>Bromberg-Martin</surname> <given-names>ES</given-names></name>. <article-title>Orbitofrontal cortex uses distinct codes for different choice attributes in decisions motivated by curiosity</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>85</volume>(<issue>3</issue>):<fpage>602</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.12.050" xlink:type="simple">10.1016/j.neuron.2014.12.050</ext-link></comment> <object-id pub-id-type="pmid">25619657</object-id>; PubMed Central PMCID: PMC4320007.</mixed-citation></ref>
<ref id="pcbi.1005925.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>. <article-title>Single neurons in prefrontal cortex encode abstract rules</article-title>. <source>Nature</source>. <year>2001</year>;<volume>411</volume>(<issue>6840</issue>):<fpage>953</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35082081" xlink:type="simple">10.1038/35082081</ext-link></comment> <object-id pub-id-type="pmid">11418860</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsujimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Genovesio</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wise</surname> <given-names>SP</given-names></name>. <article-title>Comparison of strategy signals in the dorsolateral and orbital prefrontal cortex</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>12</issue>):<fpage>4583</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5816-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5816-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21430158</object-id>; PubMed Central PMCID: PMC3082703.</mixed-citation></ref>
<ref id="pcbi.1005925.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year>;<volume>10</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2558" xlink:type="simple">10.1038/nrn2558</ext-link></comment> <object-id pub-id-type="pmid">19145235</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laje</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>(<issue>7</issue>):<fpage>925</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3405" xlink:type="simple">10.1038/nn.3405</ext-link></comment> <object-id pub-id-type="pmid">23708144</object-id>; PubMed Central PMCID: PMCPMC3753043.</mixed-citation></ref>
<ref id="pcbi.1005925.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Natschlager</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source>. <year>2002</year>;<volume>14</volume>(<issue>11</issue>):<fpage>2531</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602760407955" xlink:type="simple">10.1162/089976602760407955</ext-link></comment> <object-id pub-id-type="pmid">12433288</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>From fixed points to chaos: three models of delayed discrimination</article-title>. <source>Prog Neurobiol</source>. <year>2013</year>;<volume>103</volume>:<fpage>214</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2013.02.002" xlink:type="simple">10.1016/j.pneurobio.2013.02.002</ext-link></comment> <object-id pub-id-type="pmid">23438479</object-id>; PubMed Central PMCID: PMC3622800.</mixed-citation></ref>
<ref id="pcbi.1005925.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cheng</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>T</given-names></name>. <article-title>Efficient reinforcement learning of a reservoir network model of parametric working memory achieved with a cluster population winner-take-all readout mechanism</article-title>. <source>J Neurophysiol</source>. <year>2015</year>;<volume>114</volume>(<issue>6</issue>):<fpage>3296</fpage>–<lpage>305</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00378.2015" xlink:type="simple">10.1152/jn.00378.2015</ext-link></comment> <object-id pub-id-type="pmid">26445865</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Enel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Procyk</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Quilodran</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dominey</surname> <given-names>PF</given-names></name>. <article-title>Reservoir Computing Properties of Neural Dynamics in Prefrontal Cortex</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>6</issue>):<fpage>e1004967</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004967" xlink:type="simple">10.1371/journal.pcbi.1004967</ext-link></comment> <object-id pub-id-type="pmid">27286251</object-id>; PubMed Central PMCID: PMCPMC4902312.</mixed-citation></ref>
<ref id="pcbi.1005925.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Szita</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gyenes</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Lőrincz</surname> <given-names>A</given-names></name>, editors. <source>Reinforcement Learning with Echo State Networks</source>. <publisher-name>Artificial Neural Networks–ICANN</publisher-name> <year>2006</year>; 2006 2006.</mixed-citation></ref>
<ref id="pcbi.1005925.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>. <article-title>Limbic lesions and the problem of stimulus—reinforcement associations</article-title>. <source>Exp Neurol</source>. <year>1972</year>;<volume>36</volume>(<issue>2</issue>):<fpage>362</fpage>–<lpage>77</lpage>. <object-id pub-id-type="pmid">4626489</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rudebeck</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Saunders</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Chau</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>EA</given-names></name>. <article-title>Prefrontal mechanisms of behavioral flexibility, emotion regulation and value updating</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>(<issue>8</issue>):<fpage>1140</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3440" xlink:type="simple">10.1038/nn.3440</ext-link></comment> <object-id pub-id-type="pmid">23792944</object-id>; PubMed Central PMCID: PMC3733248.</mixed-citation></ref>
<ref id="pcbi.1005925.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carmichael</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>JL</given-names></name>. <article-title>Sensory and premotor connections of the orbital and medial prefrontal cortex of macaque monkeys</article-title>. <source>The Journal of comparative neurology</source>. <year>1995</year>;<volume>363</volume>(<issue>4</issue>):<fpage>642</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cne.903630409" xlink:type="simple">10.1002/cne.903630409</ext-link></comment> <object-id pub-id-type="pmid">8847422</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carmichael</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>JL</given-names></name>. <article-title>Limbic connections of the orbital and medial prefrontal cortex in macaque monkeys</article-title>. <source>The Journal of comparative neurology</source>. <year>1995</year>;<volume>363</volume>(<issue>4</issue>):<fpage>615</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cne.903630408" xlink:type="simple">10.1002/cne.903630408</ext-link></comment> <object-id pub-id-type="pmid">8847421</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eblen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Graybiel</surname> <given-names>AM</given-names></name>. <article-title>Highly restricted origin of prefrontal cortical inputs to striosomes in the macaque monkey</article-title>. <source>J Neurosci</source>. <year>1995</year>;<volume>15</volume>(<issue>9</issue>):<fpage>5999</fpage>–<lpage>6013</lpage>. <object-id pub-id-type="pmid">7666184</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Mapping value based planning and extensively trained choice in the human brain</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>5</issue>):<fpage>786</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3068" xlink:type="simple">10.1038/nn.3068</ext-link></comment> <object-id pub-id-type="pmid">22406551</object-id>; PubMed Central PMCID: PMCPMC3378641.</mixed-citation></ref>
<ref id="pcbi.1005925.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Smittenaar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dopamine enhances model-based over model-free choice behavior</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>75</volume>(<issue>3</issue>):<fpage>418</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.03.042" xlink:type="simple">10.1016/j.neuron.2012.03.042</ext-link></comment> <object-id pub-id-type="pmid">22884326</object-id>; PubMed Central PMCID: PMCPMC3417237.</mixed-citation></ref>
<ref id="pcbi.1005925.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smittenaar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>FitzGerald</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Romei</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Disruption of dorsolateral prefrontal cortex decreases model-based in favor of model-free control in humans</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>(<issue>4</issue>):<fpage>914</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.08.009" xlink:type="simple">10.1016/j.neuron.2013.08.009</ext-link></comment> <object-id pub-id-type="pmid">24206669</object-id>; PubMed Central PMCID: PMCPMC3893454.</mixed-citation></ref>
<ref id="pcbi.1005925.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Actions, action sequences and habits: evidence that goal-directed and habitual action control are hierarchically organized</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>12</issue>):<fpage>e1003364</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003364" xlink:type="simple">10.1371/journal.pcbi.1003364</ext-link></comment> <object-id pub-id-type="pmid">24339762</object-id>; PubMed Central PMCID: PMCPMC3854489.</mixed-citation></ref>
<ref id="pcbi.1005925.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akam</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Costa</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Simple Plans or Sophisticated Habits? State, Transition and Learning Interactions in the Two-Step Task</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>12</issue>):<fpage>e1004648</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004648" xlink:type="simple">10.1371/journal.pcbi.1004648</ext-link></comment> <object-id pub-id-type="pmid">26657806</object-id>; PubMed Central PMCID: PMCPMC4686094.</mixed-citation></ref>
<ref id="pcbi.1005925.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>. <article-title>Neuronal origins of choice variability in economic decisions</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>(<issue>5</issue>):<fpage>1322</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.09.013" xlink:type="simple">10.1016/j.neuron.2013.09.013</ext-link></comment> <object-id pub-id-type="pmid">24314733</object-id>; PubMed Central PMCID: PMCPMC3857585.</mixed-citation></ref>
<ref id="pcbi.1005925.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>. <article-title>Contributions of orbitofrontal and lateral prefrontal cortices to economic choice and the good-to-action transformation</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>81</volume>(<issue>5</issue>):<fpage>1140</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.01.008" xlink:type="simple">10.1016/j.neuron.2014.01.008</ext-link></comment> <object-id pub-id-type="pmid">24529981</object-id>; PubMed Central PMCID: PMCPMC3951647.</mixed-citation></ref>
<ref id="pcbi.1005925.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rustichini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>. <article-title>A neuro-computational model of economic decisions</article-title>. <source>J Neurophysiol</source>. <year>2015</year>;<volume>114</volume>(<issue>3</issue>):<fpage>1382</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00184.2015" xlink:type="simple">10.1152/jn.00184.2015</ext-link></comment> <object-id pub-id-type="pmid">26063776</object-id>; PubMed Central PMCID: PMCPMC4556855.</mixed-citation></ref>
<ref id="pcbi.1005925.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Goldman</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Aksay</surname> <given-names>ER</given-names></name>. <article-title>Spatial patterns of persistent neural activity vary with the behavioral context of short-term memory</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>85</volume>(<issue>4</issue>):<fpage>847</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.01.006" xlink:type="simple">10.1016/j.neuron.2015.01.006</ext-link></comment> <object-id pub-id-type="pmid">25661184</object-id>; PubMed Central PMCID: PMCPMC4336549.</mixed-citation></ref>
<ref id="pcbi.1005925.ref042"><label>42</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Suykens</surname> <given-names>JAK</given-names></name>, <name name-style="western"><surname>Vandewalle</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moor</surname> <given-names>BLRd</given-names></name>. <chapter-title>Artificial neural networks for modelling and control of non-linear systems</chapter-title>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>; <year>1996</year>. <volume>xii</volume>, <fpage>235</fpage> pages p.</mixed-citation></ref>
<ref id="pcbi.1005925.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodriguez</surname> <given-names>P</given-names></name>. <article-title>Simple recurrent networks learn context-free and context-sensitive languages by counting</article-title>. <source>Neural Comput</source>. <year>2001</year>;<volume>13</volume>(<issue>9</issue>):<fpage>2093</fpage>–<lpage>118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976601750399326" xlink:type="simple">10.1162/089976601750399326</ext-link></comment> <object-id pub-id-type="pmid">11516359</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>The sparseness of mixed selectivity neurons controls the generalization-discrimination trade-off</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>9</issue>):<fpage>3844</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2753-12.2013" xlink:type="simple">10.1523/JNEUROSCI.2753-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23447596</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <etal>et al</etal>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>(<issue>7451</issue>):<fpage>585</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id>; PubMed Central PMCID: PMC4412347.</mixed-citation></ref>
<ref id="pcbi.1005925.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ben Dayan Rubin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Internal representation of task rules by recurrent dynamics: the importance of the diversity of neural responses</article-title>. <source>Front Comput Neurosci</source>. <year>2010</year>;<volume>4</volume>:<fpage>24</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2010.00024" xlink:type="simple">10.3389/fncom.2010.00024</ext-link></comment> <object-id pub-id-type="pmid">21048899</object-id>; PubMed Central PMCID: PMCPMC2967380.</mixed-citation></ref>
<ref id="pcbi.1005925.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>4</issue>):<fpage>544</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation></ref>
<ref id="pcbi.1005925.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Critchley</surname> <given-names>HD</given-names></name>, <name name-style="western"><surname>Mason</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wakeman</surname> <given-names>EA</given-names></name>. <article-title>Orbitofrontal cortex neurons: role in olfactory and visual association learning</article-title>. <source>J Neurophysiol</source>. <year>1996</year>;<volume>75</volume>(<issue>5</issue>):<fpage>1970</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1996.75.5.1970" xlink:type="simple">10.1152/jn.1996.75.5.1970</ext-link></comment> <object-id pub-id-type="pmid">8734596</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Saddoris</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Stalnaker</surname> <given-names>TA</given-names></name>. <article-title>Reconciling the roles of orbitofrontal cortex in reversal learning and the encoding of outcome expectancies</article-title>. <source>Ann N Y Acad Sci</source>. <year>2007</year>;<volume>1121</volume>:<fpage>320</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1196/annals.1401.001" xlink:type="simple">10.1196/annals.1401.001</ext-link></comment> <object-id pub-id-type="pmid">17698988</object-id>; PubMed Central PMCID: PMCPMC2430624.</mixed-citation></ref>
<ref id="pcbi.1005925.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.21492" xlink:type="simple">10.7554/eLife.21492</ext-link></comment> <object-id pub-id-type="pmid">28084991</object-id>; PubMed Central PMCID: PMCPMC5293493.</mixed-citation></ref>
<ref id="pcbi.1005925.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Maddison</surname> <given-names>S</given-names></name>. <article-title>The orbitofrontal cortex: neuronal activity in the behaving monkey</article-title>. <source>Exp Brain Res</source>. <year>1983</year>;<volume>49</volume>(<issue>1</issue>):<fpage>93</fpage>–<lpage>115</lpage>. <object-id pub-id-type="pmid">6861938</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Buckley</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Rudebeck</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MF</given-names></name>. <article-title>Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>6</issue>):<fpage>927</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.02.027" xlink:type="simple">10.1016/j.neuron.2010.02.027</ext-link></comment> <object-id pub-id-type="pmid">20346766</object-id>; PubMed Central PMCID: PMC3566584.</mixed-citation></ref>
<ref id="pcbi.1005925.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDannald</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Lucantonio</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Burke</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name>. <article-title>Ventral striatum and orbitofrontal cortex are both required for model-based, but not model-free, reinforcement learning</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>7</issue>):<fpage>2700</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5499-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5499-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21325538</object-id>; PubMed Central PMCID: PMCPMC3079289.</mixed-citation></ref>
<ref id="pcbi.1005925.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lindsay</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Hebbian Learning in a Random Network Captures Selectivity Properties of Prefrontal Cortex</article-title>. <source>J Neurosci</source>. <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1222-17.2017" xlink:type="simple">10.1523/JNEUROSCI.1222-17.2017</ext-link></comment> <object-id pub-id-type="pmid">28986463</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chung</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gulcehre</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Cho</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</article-title>. <source>ArXiv e-prints</source> [Internet]. 2014 <month>December</month> <day>1</day>, <year>2014</year>; <fpage>1412</fpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2014arXiv1412.3555C" xlink:type="simple">http://adsabs.harvard.edu/abs/2014arXiv1412.3555C</ext-link>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riceberg</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Shapiro</surname> <given-names>ML</given-names></name>. <article-title>Reward stability determines the contribution of orbitofrontal cortex to adaptive behavior</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>(<issue>46</issue>):<fpage>16402</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0776-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0776-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23152622</object-id>; PubMed Central PMCID: PMCPMC3568518.</mixed-citation></ref>
<ref id="pcbi.1005925.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Law</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>. <article-title>Reinforcement learning can account for associative and perceptual learning on a visual-decision task</article-title>. <source>Nature neuroscience</source>. <year>2009</year>;<volume>12</volume>(<issue>5</issue>):<fpage>655</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2304" xlink:type="simple">10.1038/nn.2304</ext-link></comment> <object-id pub-id-type="pmid">19377473</object-id></mixed-citation></ref>
<ref id="pcbi.1005925.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>40</volume>(<issue>6</issue>):<fpage>1063</fpage>–<lpage>73</lpage>. <object-id pub-id-type="pmid">14687542</object-id>.</mixed-citation></ref>
<ref id="pcbi.1005925.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Royer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pare</surname> <given-names>D</given-names></name>. <article-title>Conservation of total synaptic weight through balanced synaptic depression and potentiation</article-title>. <source>Nature</source>. <year>2003</year>;<volume>422</volume>(<issue>6931</issue>):<fpage>518</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature01530" xlink:type="simple">10.1038/nature01530</ext-link></comment> <object-id pub-id-type="pmid">12673250</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>