<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01088</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004237</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Is Model Fitting Necessary for Model-Based fMRI?</article-title>
<alt-title alt-title-type="running-head">Is Model Fitting Necessary for Model-Based fMRI?</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Wilson</surname> <given-names>Robert C.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Niv</surname> <given-names>Yael</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Psychology and Cognitive Science Program, University of Arizona, Tucson Arizona, United States of America,</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Boorman</surname> <given-names>Erie Dell</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Oxford, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: RCW YN. Performed the experiments: RCW. Analyzed the data: RCW. Contributed reagents/materials/analysis tools: RCW YN. Wrote the paper: RCW YN.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">bob@email.arizona.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>6</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>18</day>
<month>6</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>6</issue>
<elocation-id>e1004237</elocation-id>
<history>
<date date-type="received">
<day>20</day>
<month>6</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>3</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Wilson, Niv</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004237" xlink:type="simple"/>
<abstract>
<p>Model-based analysis of fMRI data is an important tool for investigating the computational role of different brain regions. With this method, theoretical models of behavior can be leveraged to find the brain structures underlying variables from specific algorithms, such as prediction errors in reinforcement learning. One potential weakness with this approach is that models often have free parameters and thus the results of the analysis may depend on how these free parameters are set. In this work we asked whether this hypothetical weakness is a problem in practice. We first developed general closed-form expressions for the relationship between results of fMRI analyses using different regressors, e.g., one corresponding to the true process underlying the measured data and one a model-derived approximation of the true generative regressor. Then, as a specific test case, we examined the sensitivity of model-based fMRI to the learning rate parameter in reinforcement learning, both in theory and in two previously-published datasets. We found that even gross errors in the learning rate lead to only minute changes in the neural results. Our findings thus suggest that precise model fitting is not always necessary for model-based fMRI. They also highlight the difficulty in using fMRI data for arbitrating between different models or model parameters. While these specific results pertain only to the effect of learning rate in simple reinforcement learning models, we provide a template for testing for effects of different parameters in other models.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>In recent years, model-based fMRI has emerged as a powerful technique in psychology and neuroscience. With this method, computational models of behavior can be leveraged to identify where, whether and how different algorithms are implemented in the brain. Yet this approach seems to have an Achilles heel in that the models frequently have free parameters, and errors in setting these parameters could lead to errors in interpretation of the data. Here we asked whether this potential weakness, in theory, is an actual weakness in practice. In particular, we tested whether errors in estimating participants’ learning rate in a trial-and-error reinforcement learning setting would have adverse effects on identifying the neural substrates of the learning process. Amazingly, it turns out that even gross errors in the learning rate lead to only minute changes in the neural results. The good news is that precise identification of free parameters is not always necessary; the corollary bad news is that it may be harder to identify the precise computational roles of different brain areas than we had previously appreciated. Based on our analytical results, we offer suggestions for designing experiments that maximize or minimize sensitivity to model parameters, as needed.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by National Institute of Health grant R01MH098861 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.nih.gov/">http://www.nih.gov/</ext-link>) to YN and National Institute of Mental Health and National Institute of Drug Abuse, training grant 5T32MH065214 (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.drugabuse.gov/">http://www.drugabuse.gov/</ext-link>) to the Princeton Neuroscience Institute. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="21"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Relevant data from Niv et al. (2012) are provided as part of the Supporting Information. Because the data from Daw et al. (2006) was acquired by a third party it is not part of the Supporting Information but can be obtained by contacting Nathaniel Daw (<email xlink:type="simple">ndd204@nyu.edu</email>) and John O’Doherty (<email xlink:type="simple">jdoherty@caltech.edu</email>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The advent of fMRI revolutionized psychology as it allowed, for the first time, the noninvasive mapping of human cognition. Despite this progress, traditional fMRI analyses are limited in that they can, for the most part, only ascertain the involvement of an area in a task but not its precise <italic>role</italic> in that task. Recently, model-based fMRI methods have been developed to overcome this limitation by using computational models of behavior to shed light on latent variables of the models (such as prediction errors) and their mapping to neural structures. This approach has led to important insights into the algorithms employed by the brain and has been particularly successful in understanding the neural basis of reinforcement learning (e.g. [<xref ref-type="bibr" rid="pcbi.1004237.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004237.ref013">13</xref>]).</p>
<p>In a typical model-based fMRI analysis, one first specifies a model that describes the hypothesized cognitive processes underlying the behavior in question. Typically these models have one or more free parameters (e.g. learning rate in a model of trial-and-error learning). These parameters must be set to fully specify the model, which is commonly done by fitting them to the observed behavior [<xref ref-type="bibr" rid="pcbi.1004237.ref014">14</xref>]. For instance, given the model, one can find subject-specific learning rates that best explain the subject’s behavioral choices. The fully specified model is then used to generate trial-by-trial measures of latent variables in the model (e.g. action values and prediction errors) that can be regressed against neural data in order to find areas whose activity correlates with these variables in the brain.</p>
<p>One potential weakness of this approach is the requirement for model fitting. In many cases, the data are insufficient to precisely identify the parameter values. This can be due to limited number of trials, interactions between parameters that make them hard to disentangle [<xref ref-type="bibr" rid="pcbi.1004237.ref014">14</xref>] or lack of behavior that can be used for the fitting process (e.g., in some Pavlovian conditioning experiments). Thus a key question is: How important is the model fitting step? In other words, to what extent is model-based fMRI sensitive to errors in parameter estimation? The answer to this question will determine how hard we should work to obtain the best possible parameter fits, and will affect not only how we analyze data, but also how we design experiments in the first place.</p>
<p>Here we show how this question can be addressed, by analyzing the sensitivity of model-based fMRI to the learning rate parameter in simple reinforcement learning tasks. We provide analytical bounds on the sensitivity of the model-based analysis to errors in estimating the learning rate, and show through simulation how value and prediction error signals generated with one learning rate would be interpreted by a model-based analysis that used the wrong learning rate. Amazingly, we find that the results of model-based fMRI are remarkably robust to settings of the learning rate to the extent that, in some situations, setting the parameters of the model as far as possible from their true value barely affects the results. This theoretical prediction of robustness is borne out by analysis of fMRI data from two recent experiments.</p>
<p>Our findings are both good and bad news for model-based fMRI. The good news is that it is robust, thus errors in the learning rate will not dramatically change the results of studies seeking to localize a particular signal. The bad news, however, is that model-based fMRI is insensitive to differences in parameters, which means that one should use extreme caution when attempting to determine the computational role of a neural area (e.g., when asking whether a brain area corresponds to an outcome signal or a prediction error signal). In the Discussion we consider the extent to which this result generalizes to other parameters and other models and offer suggestions to diagnose parameter sensitivity in other models.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Ethics statement</title>
<p>Both experiments were approved by their respective institutions. The experiment in [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>] was approved by the Institutional Review Board of the California Institute of Technology. The experiment in [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>] was approved by Ethics Committee at University College London. In both cases participants gave informed consent in writing.</p>
</sec>
<sec id="sec004">
<title>Theoretical analysis</title>
<p>We begin by laying out a formal analysis of the sensitivity of model-based fMRI to model parameters. The rationale behind the mathematical derivations below is as follows. Assume that there is some signal in the brain (corresponding to some ‘ground truth’ regressor <bold>x</bold><sub><italic>g</italic></sub>) that we have a noisy measurement of (e.g., via fMRI). We first derive the somewhat intuitive result that if we analyze the brain data with a different, incorrect regressor <bold>x</bold><sub><italic>f</italic></sub> (where the subscript, <italic>f</italic>, denotes that the regressor is derived from our model with <italic>fit</italic> parameter values), the quality of our results depends on the correlation between the ground truth regressor and the incorrect regressor, <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>).</p>
<p>To assess the sensitivity of model-based fMRI to errors in parameter estimation, we then focus on trial-and-error learning tasks. We assume a ground truth regressor derived from a reinforcement learning model with the learning rate parameter set to its true (though unknown) value, and analyze the correlation between this regressor and one that is derived from the same model but with a different setting of the learning rate, for some of the most commonly used task designs. Finally, we illustrate and flesh out the implications of these analytical results using both simulated and empirical data in the Results.</p>
<sec id="sec005">
<title>The effect of an incorrect regressor on fMRI analysis</title>
<p>Assume a ‘ground truth’ regressor <bold>x</bold><sub><italic>g</italic></sub> = (<italic>x</italic><sub><italic>g</italic>1</sub>, <italic>x</italic><sub><italic>g</italic>2</sub>, …, <italic>x</italic><sub><italic>gT</italic></sub>) (where <italic>x</italic><sub><italic>gt</italic></sub> is the size of the variable of interest at time point <italic>t</italic>) that underlies the activity in a brain region, such that the measured signal in this region takes the form
<disp-formula id="pcbi.1004237.e001"><alternatives><graphic id="pcbi.1004237.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>β</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
with <italic>β</italic> being a coefficient that controls the size of the effect and <italic>ϵ</italic> being zero-mean noise. What would be the magnitude of the estimated regression coefficient <inline-formula id="pcbi.1004237.e002"><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> if we analyzed the brain data using an incorrect regressor, <bold>x</bold><sub><italic>f</italic></sub> (for example one that is derived from an incorrect model, or from the correct model with the wrong setting of the free parameters)? Using ordinary least squares regression, we have
<disp-formula id="pcbi.1004237.e003"><alternatives><graphic id="pcbi.1004237.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>β</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <inline-formula id="pcbi.1004237.e004"><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>σ</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:msqrt><mml:mfrac><mml:mrow><mml:msup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>T</mml:mi></mml:msup> <mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:math></inline-formula> is the standard deviation of regressor <bold>x</bold>, <inline-formula id="pcbi.1004237.e005"><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>f</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>g</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>f</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>σ</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>g</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo> <mml:mi>σ</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>f</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the correlation coefficient between <bold>x</bold><sub><italic>g</italic></sub> and <bold>x</bold><sub><italic>f</italic></sub>, and <italic>T</italic> is the number of data points in the regression. Thus, if we normalize the regressors to have unit variance, i.e. <italic>σ</italic>(<bold>x</bold><sub><italic>g</italic></sub>) = <italic>σ</italic>(<bold>x</bold><sub><italic>f</italic></sub>) = 1, then <inline-formula id="pcbi.1004237.e006"><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is related to the ground truth regression coefficient through the correlation between the two regressors:
<disp-formula id="pcbi.1004237.e007"><alternatives><graphic id="pcbi.1004237.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
Thus, the more correlated the fit regressor is to the true regressor, the larger the regression coefficient for the fit regressor, <inline-formula id="pcbi.1004237.e008"><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. How does this affect the statistical significance of <inline-formula id="pcbi.1004237.e009"><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, that is, the results of a statistical analysis that asks whether <inline-formula id="pcbi.1004237.e010"><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is reliably different from zero? To answer this question, we must compute <inline-formula id="pcbi.1004237.e011"><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, the Student <italic>t</italic> statistic of <inline-formula id="pcbi.1004237.e012"><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> relative to the null hypothesis <inline-formula id="pcbi.1004237.e013"><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Making the further simplifying assumption that the fMRI noise, <italic>ϵ</italic>, is Gaussian with variance <inline-formula id="pcbi.1004237.e014"><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>M</mml:mi> <mml:mi>R</mml:mi> <mml:mi>I</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, we have
<disp-formula id="pcbi.1004237.e015"><alternatives><graphic id="pcbi.1004237.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e015"/><mml:math id="M15" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <inline-formula id="pcbi.1004237.e016"><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is standard error of <inline-formula id="pcbi.1004237.e017"><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. For simple regression, <inline-formula id="pcbi.1004237.e018"><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be written in terms of the standard deviation of the regression residuals, <inline-formula id="pcbi.1004237.e019"><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, as
<disp-formula id="pcbi.1004237.e020"><alternatives><graphic id="pcbi.1004237.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mfrac> <mml:mfrac><mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
To compute the standard deviation of the residuals we first note that, by definition,
<disp-formula id="pcbi.1004237.e021"><alternatives><graphic id="pcbi.1004237.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>β</mml:mi> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
Thus, because the residuals have zero mean
<disp-formula id="pcbi.1004237.e022"><alternatives><graphic id="pcbi.1004237.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>σ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:menclose notation="updiagonalstrike"><mml:mrow><mml:mn>2</mml:mn><mml:mi>β</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>ϵ</mml:mi></mml:mrow></mml:menclose><mml:mo>−</mml:mo><mml:menclose notation="updiagonalstrike"><mml:mrow><mml:mn>2</mml:mn><mml:mi>β</mml:mi><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>f</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi>ϵ</mml:mi></mml:mrow></mml:menclose></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>M</mml:mi> <mml:mi>R</mml:mi> <mml:mi>I</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where we have used the fact that <inline-formula id="pcbi.1004237.e023"><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>g</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>ϵ</mml:mi> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="pcbi.1004237.e024"><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>f</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mi>ϵ</mml:mi> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to cancel out the terms in the second line, and the definition of the correlation coefficient to make the simplification in the third line. Combining this expression with Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e015">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e020">5</xref> and keeping in mind that <italic>σ</italic>(<bold>x</bold>) = 1 allows us to write down the <italic>t</italic> statistic as
<disp-formula id="pcbi.1004237.e025"><alternatives><graphic id="pcbi.1004237.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msqrt><mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>M</mml:mi> <mml:mi>R</mml:mi> <mml:mi>I</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>CNR</mml:mtext> <mml:msqrt><mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mtext>CNR</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
where CNR = <italic>β</italic>/<italic>σ</italic><sub><italic>fMRI</italic></sub> denotes the ‘contrast-to-noise’ ratio [<xref ref-type="bibr" rid="pcbi.1004237.ref015">15</xref>]—the ratio between the strength of the fMRI signal, <italic>β</italic>, and the standard deviation of the fMRI noise, <italic>σ</italic><sub><italic>fMRI</italic></sub>. Note that this <italic>t</italic> statistic, like the regression coefficient, <inline-formula id="pcbi.1004237.e026"><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, is a function of the correlation between the ground truth and incorrect regressors, <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>), as well as the contrast-to-noise ratio, CNR, and the number of data points in the regression, <italic>T</italic>.</p>
</sec>
<sec id="sec006">
<title>Correlations between regressors for different parameter settings in a Pavlovian task</title>
<p>We now turn to analyzing the effects of incorrectly specifying free parameters on model-based fMRI. For this, we concentrate on the particular case of reinforcement learning models. These models, which have formed the bulk of model-based fMRI, attempt to predict how values of different options, as computed by the subject, change through experience. One important free parameter in such learning scenarios is the <italic>learning rate</italic> by which values are updated as a result of prediction errors [<xref ref-type="bibr" rid="pcbi.1004237.ref016">16</xref>]. We assume that there is some ground truth setting of this parameter that corresponds to the true learning rate of the subject, however, this setting is unknown and our fMRI analysis might utilize an incorrect learning rate. Thus to assess the quality of the results we may hope to obtain from the fMRI analysis, we analyze the properties of <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>) assuming two regressors that are generated from the same model using different learning rates.</p>
<p>For simplicity, we concentrate on Pavlovian tasks in which subjects experience the rewards associated with different options passively, and use the Rescorla-Wagner learning rule [<xref ref-type="bibr" rid="pcbi.1004237.ref017">17</xref>]. On each trial, <italic>t</italic>, subjects are presented with a reward <italic>r</italic><sub><italic>t</italic></sub> and learn to estimate a value <italic>V</italic><sub><italic>t</italic>+1</sub> that represents the reward predicted on the next trial. These values are updated on every trial according to
<disp-formula id="pcbi.1004237.e027"><alternatives><graphic id="pcbi.1004237.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>α</italic> is the learning rate and
<disp-formula id="pcbi.1004237.e028"><alternatives><graphic id="pcbi.1004237.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
is the prediction error quantifying the difference between the actual and predicted reward on the current trial. We denote the ground truth learning rate as <italic>α</italic><sub><italic>g</italic></sub>, and the fit learning rate as <italic>α</italic><sub><italic>f</italic></sub>. Accordingly, we denote the vector of values learned with learning rate <italic>α</italic><sub><italic>i</italic></sub> as <bold>V</bold><sub><italic>i</italic></sub> = (<italic>V</italic><sub><italic>i</italic>1</sub>, <italic>V</italic><sub><italic>i</italic>2</sub>, …, <italic>V</italic><sub><italic>iT</italic></sub>) and the corresponding vector of prediction errors as <bold>δ</bold><sub><italic>i</italic></sub>.</p>
<p>Model-based fMRI studies of reinforcement learning typically use values and prediction errors as regressors for neural activity [<xref ref-type="bibr" rid="pcbi.1004237.ref018">18</xref>]. Thus, in order to determine the sensitivity of the results to the accuracy of the parameter fits, we need to compute the correlation coefficients <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>) and <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>). To do this, we first note that in the general case, the values and prediction errors computed according to Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e027">9</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e028">10</xref> will not have zero mean or unit variance. This does not invalidate the previously derived results but does require us to work with the more general form of the correlation coefficient
<disp-formula id="pcbi.1004237.e029"><alternatives><graphic id="pcbi.1004237.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where cov(<bold>a</bold>, <bold>b</bold>) is the covariance between vectors <bold>a</bold> and <bold>b</bold> defined by
<disp-formula id="pcbi.1004237.e030"><alternatives><graphic id="pcbi.1004237.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>cov</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">b</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
Here <italic>μ</italic>(<bold>a</bold>) is the mean of vector <bold>a</bold>, and <italic>σ</italic>(<bold>a</bold>), the standard deviation of an uncentered regressor, is given by
<disp-formula id="pcbi.1004237.e031"><alternatives><graphic id="pcbi.1004237.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e031"/><mml:math id="M31" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mi mathvariant="bold">a</mml:mi></mml:mrow> <mml:mi>T</mml:mi></mml:mfrac> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
Thus to compute the correlation coefficients <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>) and <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>) we must compute the mean, variance and covariance of the value and prediction error regressors. These statistics turn out to be completely determined by the learning rates and the statistics of the reward vector, <bold>r</bold>.</p>
<p>In particular, for the value regressors it can be shown that
<disp-formula id="pcbi.1004237.e032"><alternatives><graphic id="pcbi.1004237.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>σ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:msup> <mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>cov</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Δ</mml:mo></mml:msup> <mml:mo>)</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where the approximations hold in the limit of many trials and <italic>R</italic><sub>Δ</sub>(<bold>r</bold>) is the (uncentered) autocorrelation of <bold>r</bold> at delay Δ defined as
<disp-formula id="pcbi.1004237.e033"><alternatives><graphic id="pcbi.1004237.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e033"/><mml:math id="M33" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo></mml:mrow></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
Equivalently, for the prediction errors we have
<disp-formula id="pcbi.1004237.e034"><alternatives><graphic id="pcbi.1004237.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e034"/><mml:math id="M34" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>σ</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mo>Δ</mml:mo> <mml:mi>T</mml:mi></mml:mfrac> <mml:mo>)</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>cov</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>≈</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>α</mml:mi> <mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>α</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mo>Δ</mml:mo> <mml:mi>T</mml:mi></mml:mfrac> <mml:mo>)</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
See the Supplementary Information for detailed derivation of these equations. It is important to note that the approximations in Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e032">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e034">16</xref> only hold when the number of trials, <italic>T</italic>, is large relative to the reciprocal of the run lengths (1/<italic>α</italic><sub><italic>g</italic></sub> and 1/<italic>α</italic><sub><italic>f</italic></sub>). This approximation simplifies the expressions greatly, by removing the dependence on initial values, but we urge caution in interpreting the results when both <italic>T</italic> and the learning rates are small. In particular, this implies that that our analysis holds for non-zero learning rates only.</p>
<p>Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e032">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e034">16</xref> imply that to compute the required correlations we only need the statistics of the rewards: <italic>μ</italic>(<bold>r</bold>), <italic>μ</italic>(<bold>r</bold><sup>2</sup>) and <italic>R</italic><sub>Δ</sub>(<bold>r</bold>). The exact form of these averages depends on the dynamics of the reward-generating process in the experiment. In the Results section we consider two commonly used experimental designs.</p>
</sec>
</sec>
<sec id="sec007">
<title>fMRI experiments</title>
<p>To test our theoretical predictions we used data from two different experiments corresponding to two different reward dynamics: fixed and drifting. In the following sections we briefly describe the two experiments along with details of our analyses. More precise descriptions of each experiment can be found in the original papers, also available as part of the supplementary information (<xref rid="pcbi.1004237.s003" ref-type="supplementary-material">S1 Dataset</xref>).</p>
<sec id="sec008">
<title>Fixed reward distribution</title>
<p>We used data from Niv, Edlund, Dayan &amp; O’Doherty [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>]. Preprocessing of the fMRI data were performed by the original authors, as described in [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>]. In this experiment, 16 subjects made a series of choices between stimuli that paid out different amounts of monetary reward. There were five possible stimuli: two options paid out 0¢ with 100% probability, one paid out 20¢, one paid out 40¢, and one paid out either 0¢ or 40¢ with 50% probability (henceforth the risky 0/40 stimulus). The experiment involved two types of trials, intermingled: on ‘choice trials’ subjects were required to choose between two stimuli, while on ‘forced trials’ subjects were presented with only one of the five stimuli and had to choose it. These forced trials ensured that subjects continued to experience all of the stimuli regardless of their subjective value (for example, the 0¢ stimuli were very rarely chosen on choice trials). Choices were made immediately after the stimuli appeared on screen and reward feedback was given 5s after the choice.</p>
<p>Reinforcement learning theory predicts that there would be two prediction errors on each trial: one at the time of stimulus onset/choice, equal to the value of the to-be-chosen stimulus <bold>V</bold>, and one at the time of reward, given by the difference between reward outcome and the value of the chosen option, <bold>r</bold> − <bold>V</bold>. Because subjects were trained on the task prior to the scan, we assumed that they knew the values for the constantly rewarding stimuli, and therefore concentrated on learning estimated values for the 0/40 stimulus, for which rewards were probabilistic. Note that the simple reinforcement-learning model we use is slightly different from the best-fitting model used in [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>], which involved two different learning rates for positive and negative prediction errors. We used the simpler model to maintain consistency with our theoretical analysis. However, in any case our results show that such a difference in the model would not affect the regressors and the neural results to a large extent.</p>
<p>We focused our analysis on fMRI activations in the nucleus accumbens (NAc), an area whose BOLD activity has been repeatedly shown to correlate with prediction errors for both primary [<xref ref-type="bibr" rid="pcbi.1004237.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004237.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1004237.ref024">24</xref>] and monetary rewards [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1004237.ref025">25</xref>–<xref ref-type="bibr" rid="pcbi.1004237.ref029">29</xref>], putatively due to the strong dopaminergic afferents to that area. We used average BOLD signals extracted by the original authors from anatomically defined regions of interest (ROIs) in the left and right NAc, and regressed this signal vector, <bold>Y</bold>, against parametric regressors for value and prediction error of the risky 0/40 stimulus as well as regressors for variables of less interest such as event onsets, value of the certain options and nuisance variables such as head motion and scanner drift. In keeping with our theoretical analysis, we mean-centered all model-based regressors and normalized them to have a standard deviation of 1. We repeated this analysis using different settings of the learning rate between 0.01 and 1, in steps of 0.01.</p>
</sec>
<sec id="sec009">
<title>Drifting reward distribution</title>
<p>For the drifting reward distribution, we analyzed data from Daw, O’Doherty, Dayan, Seymour &amp; Dolan [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>]. Preprocessing of the fMRI data was done by the original authors, as described in [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>]. In this experiment, 16 subjects performed two blocks of 150 choices between four options. Each option paid out probabilistic rewards sampled from a Gaussian distribution with a drifting mean and a noise standard deviation <italic>σ</italic><sub><italic>n</italic></sub> = 4. The drifting mean <italic>m</italic><sub><italic>t</italic></sub> on trial <italic>t</italic> was updated according to
<disp-formula id="pcbi.1004237.e035"><alternatives><graphic id="pcbi.1004237.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e035"/><mml:math id="M35" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>m</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>50</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>n</italic><sub><italic>t</italic></sub> was Gaussian random noise with drift standard deviation, <italic>σ</italic><sub><italic>d</italic></sub> = 2.8 and <italic>γ</italic> = 0.9836 was the decay rate that ensured that the mean decayed towards 50.</p>
<p>As in the experiment with fixed reward distributions [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>], presentation of the stimulus and outcome were separated in time, and as before, we expected the signal at the stimulus to reflect the value of the to-be-chosen stimulus, <bold>V</bold>, and the signal at the outcome to reflect the prediction error <bold>r</bold> − <bold>V</bold>. We thus used a GLM with 5 regressors: two stick regressors, one at the onset of stimuli and one at the onset of the outcomes, a <bold>V</bold> parametric modulation on stimulus onset, a prediction error parametric modulation on outcome onset, and a parametric modulation on outcome onset by the magnitude of outcome itself, <bold>r</bold>. Because we were primarily interested in the value signal, we focused on data from an ROI in the ventromedial prefrontal cortex (vmPFC). Specifically, we used an ROI centered at (-3, 33, -6), a location that was reported in [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>] to correlate strongly with choice probability, which is closely related to chosen value. As before, we analyzed this ROI with GLMs created using a variety of different learning rates between 0.01 and 1, in steps of 0.01.</p>
</sec>
</sec>
</sec>
<sec id="sec010" sec-type="results">
<title>Results</title>
<p>In the Methods, we developed general expressions that describe how changes in the learning rate affect model-based fMRI in simple reinforcement learning tasks. In particular, we showed that regression coefficients when using a model-based regressor, and their corresponding Student <italic>t</italic> values, depend on three factors: the contrast-to-noise ratio (CNR) in the signal, the number of data points in the regression, and <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>)—the correlation between the regressor generated with the fit parameter values, <bold>x</bold><sub><italic>f</italic></sub>, and a regressor generated with the ground truth parameter values, <bold>x</bold><sub><italic>g</italic></sub>.</p>
<p>We now investigate how these factors play out in two cases with qualitatively different reward dynamics: a reward distribution that is fixed throughout the experiment, or one that changes over time. In both cases we show that model-based fMRI analysis of value and prediction error signals is relatively insensitive to the setting of the learning rate parameter and that this insensitivity can be, to a certain extent, manipulated by altering the design of the task.</p>
<sec id="sec011">
<title>Fixed reward distribution</title>
<p>We first consider a situation in which the reward distribution is fixed throughout the experiment. An example of such a distribution with mean <italic>m</italic> and variance <inline-formula id="pcbi.1004237.e037"><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="pcbi.1004237.g001">Fig 1A</xref>. In panels B and C of the same figure we show rewards from Gaussian and Bernoulli distributions, but it is important to note that the following theoretical results apply to <italic>any</italic> fixed reward distribution with finite variance.</p>
<fig id="pcbi.1004237.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Illustration of a fixed reward distribution.</title>
<p>(A) Schematic of a general reward distribution that is fixed over time, showing the relevant parameters: the mean <italic>m</italic> and standard deviation <italic>σ</italic><sub><italic>n</italic></sub>. (B) Example of continuous rewards sampled from a Gaussian distribution with mean <italic>m</italic> = 0.3 and standard deviation <italic>σ</italic><sub><italic>n</italic></sub> = 0.2. (C) Bar plot showing an example of binary reward data sampled from a Bernoulli distribution with the same mean <italic>m</italic> = 0.3 and standard deviation <inline-formula id="pcbi.1004237.e036"><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mi>m</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>m</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>46</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g001"/>
</fig>
<sec id="sec012">
<title>Analytic results</title>
<p>Based on the methods developed above, the sensitivity of a model-based analysis looking for a value signal in the brain depends on the correlation, <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>), between the value computed using the ground truth learning rate, <bold>V</bold><sub><italic>g</italic></sub>, and the value computed using the fit learning rate, <bold>V</bold><sub><italic>f</italic></sub>. Likewise prediction error signals depend on <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>). Moreover, expressions for these correlations rely only on a few statistics of the reward distribution: its mean <italic>μ</italic>(<bold>r</bold>) = <italic>m</italic>, the mean of the squared rewards <inline-formula id="pcbi.1004237.e038"><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:msup><mml:mtext mathvariant="bold">r</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:msup><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and the reward autocorrelation <italic>R</italic><sub>Δ</sub>(<bold>r</bold>) = <italic>m</italic><sup>2</sup>. Given these expressions for the reward statistics, we can compute the sums in Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e032">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e034">16</xref> exactly (as sums of geometric series), leading to the following expressions for the value and prediction error correlations using different learning rates <italic>α</italic><sub><italic>g</italic></sub> and <italic>α</italic><sub><italic>f</italic></sub>:
<disp-formula id="pcbi.1004237.e039"><alternatives><graphic id="pcbi.1004237.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e039"/><mml:math id="M39" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:msqrt><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt> <mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>In Fig <xref ref-type="fig" rid="pcbi.1004237.g002">2A</xref> and <xref ref-type="fig" rid="pcbi.1004237.g002">2B</xref> we plot these correlations as functions of the two learning rates. Strikingly, the correlations for both value and prediction error are relatively insensitive to mismatch in learning rates. Indeed, for prediction errors, the minimum possible value of the correlation (at <italic>α</italic><sub><italic>g</italic></sub> → 0 and <italic>α</italic><sub><italic>f</italic></sub> → 1 or vice versa) is <inline-formula id="pcbi.1004237.e040"><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:msqrt><mml:mn>2</mml:mn></mml:msqrt> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>. This implies that even in the worst case scenario, when the true learning rate had an extreme value of 0 or 1 and the fit learning rate was as far as possible from the true learning rate, the resulting prediction error regressor would still be highly correlated with the true signal. This result has important implications for the qualitative interpretation of prediction error signals, since with a true learning rate of 0, the ‘prediction error’ is simply the reward signal. Therefore, when the reward distribution is fixed throughout the experiment, prediction errors will always be strongly correlated with the reward signal, making it difficult to tease apart these two neural signals using linear regression.</p>
<fig id="pcbi.1004237.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Correlations and <italic>t</italic> statistics for experiments with a fixed reward distribution, computed by evaluating <xref ref-type="disp-formula" rid="pcbi.1004237.e039">Eq (18)</xref> at learning rates between 0.001 and 1, in steps of 0.001.</title>
<p>(A) Correlation, <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>), between regressors for value as a function of the true, <italic>α</italic><sub><italic>g</italic></sub>, and fitted, <italic>α</italic><sub><italic>f</italic></sub>, learning rates. (B) Correlation between the regressors for prediction error, <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>). (C,D) Single-subject <italic>t</italic> statistics (assuming 49 degrees of freedom) as a function of the two learning rates for value (C) and prediction error (D). Black, gray and white contours denote significance at <italic>p</italic> = 0.01, <italic>p</italic> = 10<sup>−4</sup> and <italic>p</italic> = 10<sup>−6</sup>, respectively. Dashed black line in C: values that will be analyzed in more detail in <xref ref-type="fig" rid="pcbi.1004237.g003">Fig 3</xref>.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g002"/>
</fig>
<p>In Fig <xref ref-type="fig" rid="pcbi.1004237.g002">2C</xref> and <xref ref-type="fig" rid="pcbi.1004237.g002">2D</xref> we show the corresponding <italic>t</italic> statistics assuming 49 degrees of freedom (or a very conservative total number of trials, <italic>T</italic> = 50) and a contrast-to-noise ratio, CNR of 1 (a fairly typical value in fMRI experiments [<xref ref-type="bibr" rid="pcbi.1004237.ref015">15</xref>] and also consistent with the range of CNR values seen in the data sets analyzed here, where CNR was 0.4 in one case and 11 in the other). As can be seen, the dependence of the <italic>t</italic> statistic for the regressor on the true and fit learning rates closely matches that of the regressor correlations. This is because at low CNR the <italic>t</italic> statistic in <xref ref-type="disp-formula" rid="pcbi.1004237.e025">Eq 8</xref> is approximately proportional to <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>). For reward prediction error in particular, all possible values of <italic>α</italic><sub><italic>g</italic></sub> and <italic>α</italic><sub><italic>f</italic></sub> result in a significant <italic>t</italic> statistic at <italic>p</italic> &lt; 0.001 (a commonly used uncorrected threshold for significance of prediction error signals in the brain). This result further exemplifies both the strength and the limitation of such an analysis: a neural signal will likely be identified even if model fitting produced especially poor learning rate parameter settings, however, from this regression alone we cannot know for sure that the identified signal is indeed a prediction error signal rather than a reward signal.</p>
<p>In <xref ref-type="fig" rid="pcbi.1004237.g003">Fig 3</xref> we investigate the effect of CNR and <italic>T</italic> on the <italic>t</italic> statistic more explicitly. For exposition, we focus on the diagonal <italic>α</italic><sub><italic>g</italic></sub> + <italic>α</italic><sub><italic>f</italic></sub> = 1 (i.e. along the dashed line in <xref ref-type="fig" rid="pcbi.1004237.g002">Fig 2C</xref>) and plot the results as a function of the difference in learning rates, <italic>α</italic><sub><italic>g</italic></sub> − <italic>α</italic><sub><italic>f</italic></sub>, with the constraint that <italic>α</italic><sub><italic>g</italic></sub> + <italic>α</italic><sub><italic>f</italic></sub> = 1. Panels A and B show the effect of changing the contrast-to-noise ratio from 1 to 100 at fixed <italic>T</italic> = 50 for the value and prediction error regressors, respectively. In both cases, as the contrast-to-noise ratio increases, the curves become increasingly peaked at 0 (i.e. at <italic>α</italic><sub><italic>g</italic></sub> = <italic>α</italic><sub><italic>f</italic></sub>) indicating greater sensitivity of the resulting <italic>t</italic> statistics to accuracy of the fit learning rate. This shows that, when the underlying signal is strong, model-based fMRI will be much more sensitive to parameter settings than when the signal is weak. Of course, for a given statistical threshold, a higher CNR also results in a wider range of fit learning rates achieving statistical significance. Thus, not surprisingly, high CNR is always better than low CNR in that it makes the result more robust to model-fitting errors in addition to making it easier to use the fMRI signal to inform the parameter search.</p>
<fig id="pcbi.1004237.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Effect of contrast-to-noise ratio, CNR, (A,B) and number of trials, <italic>T</italic>, (C,D) on the <italic>t</italic> values as a function of the difference between <italic>α</italic><sub><italic>g</italic></sub> − <italic>α</italic><sub><italic>f</italic></sub> when <italic>α</italic><sub><italic>g</italic></sub> + <italic>α</italic><sub><italic>f</italic></sub> = 1, for experiments with a fixed reward distribution.</title>
<p>The range of differences is from -0.999 to +0.999 in steps of 0.001. Dashed line: significance at <italic>p</italic> &lt; 0.05 at the single subject level.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g003"/>
</fig>
<p>In panels C and D we illustrate the effect of changing the number of trials, <italic>T</italic>, from 10 to 200 assuming a fixed CNR = 1. Again, as <italic>T</italic> increases, the <italic>t</italic> statistic becomes increasingly peaked around <italic>α</italic><sub><italic>g</italic></sub> = <italic>α</italic><sub><italic>f</italic></sub> (note the logarithmic scale on the y-axis). These results show that the sensitivity of a model-based analysis can also be increased by increasing the number of trials in the experiment.</p>
</sec>
<sec id="sec013">
<title>fMRI data</title>
<p>To test these theoretical predictions, we used data from from Niv, Edlund, Dayan &amp; O’Doherty [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>]. As <xref ref-type="fig" rid="pcbi.1004237.g004">Fig 4</xref> shows, consistent with the theory, for both value and prediction error regressors, the regression coefficient depended relatively weakly on the learning rate used to generate the regressor. This was true both for single subjects (Fig <xref ref-type="fig" rid="pcbi.1004237.g004">4A</xref> and <xref ref-type="fig" rid="pcbi.1004237.g004">4B</xref>) and at the group level (Fig <xref ref-type="fig" rid="pcbi.1004237.g004">4C</xref> and <xref ref-type="fig" rid="pcbi.1004237.g004">4D</xref>).</p>
<fig id="pcbi.1004237.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The effect of learning rate on the fMRI signal in the NAc in [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>], an experiment with a fixed reward distribution (binary rewards with <italic>p</italic> = 0.5, see <xref ref-type="sec" rid="sec002">Methods</xref>).</title>
<p>(A) Regression coefficients for value of the chosen option at the time of stimulus onset, as a function of learning rate. Each curve represents a single subject. (B) Single subject regression coefficients for prediction error at the time of reward. Note the relative lack of modulation of the regression coefficient by the value of the learning rate parameter. (C) Group analysis at the time of stimulus showing the group <italic>t</italic> statistic as a function of (group-wise) learning rate (blue). Red lines denote the best and worst case scenarios obtained by taking the value of the learning rate that either maximizes or minimizes the <italic>t</italic> statistic for each subject. Dashed black line: <italic>p</italic> = 0.05 threshold. (D) Group analysis at the time of reward. In all cases, as predicted, the effect of the learning rate parameter is small.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g004"/>
</fig>
<p>Although the plots in Fig <xref ref-type="fig" rid="pcbi.1004237.g004">4A</xref> and <xref ref-type="fig" rid="pcbi.1004237.g004">4B</xref> are relatively insensitive to parameter value, many of the curves for individual subjects do appear to have a maximum, suggesting that there is a learning rate that best describes the neural data. To evaluate the extent to which these best-fitting learning rates could be estimated, we computed the log likelihood of the fMRI data for a linear model assuming that the data are generated by the a combination of the model-based regressors for value and prediction error plus additive Gaussian noise, for each value of the learning rate parameter. For a linear regression model, the log likelihood has a closed form as follows:
<disp-formula id="pcbi.1004237.e041"><alternatives><graphic id="pcbi.1004237.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e041"/><mml:math id="M41" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi> <mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi> <mml:mo>(</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula>
where <italic>σ</italic><sub><italic>r</italic></sub>(<italic>α</italic>) denotes the standard deviation of the residuals of the linear model with model-based regressors generated using learning rate <italic>α</italic>.</p>
<p><xref ref-type="fig" rid="pcbi.1004237.g005">Fig 5</xref> shows <italic>LL</italic>(<italic>α</italic>) relative to its maximum value, for each subject, Δ<italic>LL</italic>(<italic>α</italic>) = <italic>LL</italic>(<italic>α</italic>) − max<sub><italic>α</italic></sub> <italic>LL</italic>(<italic>α</italic>). This analysis further highlights the insensitivity of these results to learning rate—for most subjects the range of log likelihood between best and worst fits being less than 2, a difference in fit usually considered ‘barely worth mentioning’ [<xref ref-type="bibr" rid="pcbi.1004237.ref030">30</xref>].</p>
<fig id="pcbi.1004237.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Fit of a linear model comprised of a single model-based regressor generated with different learning rates <italic>α</italic>, to fMRI data in the fixed reward probability experiment [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>] as a function of learning rate.</title>
<p>Each grey curve corresponds to a different subject and in blue is the mean across subjects. All curves are shifted to have a maximum value of 0. For most subjects the quality of the fit depends only weakly on the learning rate. Note that the y-axis is the same as in <xref ref-type="fig" rid="pcbi.1004237.g010">Fig 10</xref>, to highlight the differences between the sensitivities of the two experiments to the setting of the learning rate parameter.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g005"/>
</fig>
</sec>
</sec>
<sec id="sec014">
<title>Drifting reward distribution</title>
<p>Our approach can also be applied to scenarios in which the reward distribution is not fixed. To illustrate, we analyze experiments with rewards that are drawn from a Gaussian distribution whose mean, <italic>m</italic><sub><italic>t</italic></sub>, is generated by a discretized Ornstein-Uhlenbeck process (<xref ref-type="fig" rid="pcbi.1004237.g006">Fig 6</xref>) [<xref ref-type="bibr" rid="pcbi.1004237.ref031">31</xref>]. Specifically, <italic>m</italic><sub><italic>t</italic></sub>, undergoes a random walk defined by
<disp-formula id="pcbi.1004237.e042"><alternatives><graphic id="pcbi.1004237.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e042"/><mml:math id="M42" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>m</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
where <italic>n</italic><sub><italic>t</italic></sub> is zero mean noise with drift variance <inline-formula id="pcbi.1004237.e043"><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <italic>γ</italic> (&lt; 1) is a decay parameter. Because <italic>γ</italic> is smaller than one, the mean tends to decay to zero over time (illustrated by the arrows in <xref ref-type="fig" rid="pcbi.1004237.g006">Fig 6A</xref>). This helps to keep the means of different options from diverging too far as the experiment progresses.</p>
<fig id="pcbi.1004237.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g006</object-id>
<label>Fig 6</label>
<caption>
<title>An example drifting reward distribution.</title>
<p>(A) Evolution of the mean <italic>m</italic><sub><italic>t</italic></sub> over time, <italic>t</italic>, diffusing with a drift standard deviation <italic>σ</italic><sub><italic>d</italic></sub>. The decay, <italic>γ</italic>, is indicated by the gray arrows and the shaded region indicates the standard deviation of the Gaussian noise distribution, <italic>σ</italic><sub><italic>n</italic></sub>. (B) A set of rewards sampled from the distribution in panel A.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g006"/>
</fig>
<sec id="sec015">
<title>Analytic results</title>
<p>Again, to compute the correlations we require the statistics of the reward distribution. For simplicity, we focus on situations in which <italic>T</italic> is large, where the reward statistics are asymptotically
<disp-formula id="pcbi.1004237.e044"><alternatives><graphic id="pcbi.1004237.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e044"/><mml:math id="M44" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula>
<disp-formula id="pcbi.1004237.e045"><alternatives><graphic id="pcbi.1004237.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e045"/><mml:math id="M45" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>μ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
<disp-formula id="pcbi.1004237.e046"><alternatives><graphic id="pcbi.1004237.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi> <mml:mo>Δ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi> <mml:mo>Δ</mml:mo></mml:msup> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
These reward statistics allow us to compute the sums in Eqs <xref ref-type="disp-formula" rid="pcbi.1004237.e032">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1004237.e034">16</xref> exactly, leading to the following (slightly messy, but nonetheless fully tractable) expressions for <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>) and <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>):
<disp-formula id="pcbi.1004237.e047"><alternatives><graphic id="pcbi.1004237.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004237.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ρ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mi>γ</mml:mi></mml:mrow></mml:mfrac> <mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:msqrt> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula></p>
<p>Note that these are functions of only two parameters of the reward distribution: the decay, <italic>γ</italic>, and the ratio of drift variance to noise variance, <inline-formula id="pcbi.1004237.e048"><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. When using a drifting reward probability, these experimentally determined parameters exert much greater control over the form of the correlations than can be achieved when reward probability is fixed. This control is demonstrated in <xref ref-type="fig" rid="pcbi.1004237.g007">Fig 7</xref> where we tuned these two parameters to achieve sensitivity to prediction errors (panels A,B), value (panels E,F), or both (panels C,D). The parameters in panels A and B, <italic>γ</italic> = 0.98 and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub> = 0.7, closely match those in the experiment of Daw et al. [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>], whose neural results we discuss below.</p>
<fig id="pcbi.1004237.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Correlations between model based regressors derived using different learning rates, in an experiment with drifting rewards, for three different settings of the decay of the reward mean to 0, <italic>γ</italic>, and the drift-to-noise ratio of the reward mean, <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub>.</title>
<p>Plots were generated by evaluating <xref ref-type="disp-formula" rid="pcbi.1004237.e047">Eq (24)</xref> for learning rates between 0.001 and 1 in steps of 0.001. (A,B) When <italic>γ</italic> is high (0.98) and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub> is low (0.7), values are not sensitive to fit learning rate, but prediction errors are sensitive. (C,D) Intermediate <italic>γ</italic> and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub> lead to intermediate sensitivity of both value and prediction error to learning rate. (E, F) When <italic>γ</italic> is low (0.1) and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub> is high (4.5), the results mimic those obtained with a fixed reward distribution (values are more sensitive to fit learning rate than are prediction errors, compare Fig <xref ref-type="fig" rid="pcbi.1004237.g002">2A</xref> and <xref ref-type="fig" rid="pcbi.1004237.g002">2B</xref>).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g007"/>
</fig>
<p>To explore the parameter space more thoroughly, we quantified the ‘insensitivity to learning rate’ as the fraction of (<italic>α</italic><sub><italic>g</italic></sub>, <italic>α</italic><sub><italic>f</italic></sub>)-space in which the correlations are greater than 0.7. This metric is 1 when the correlations are only weakly dependent on learning rate (as for the prediction error in the case of fixed rewards) and 0 when they are exquisitely sensitive. <xref ref-type="fig" rid="pcbi.1004237.g008">Fig 8</xref> shows this metric as a function of the two parameters, <italic>γ</italic> and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub>, for the value and prediction error regressors. The plot demonstrates the somewhat reciprocal relationship between <italic>ρ</italic>(<bold>V</bold><sub><italic>g</italic></sub>, <bold>V</bold><sub><italic>f</italic></sub>) and <italic>ρ</italic>(<bold>δ</bold><sub><italic>g</italic></sub>, <bold>δ</bold><sub><italic>f</italic></sub>): when prediction errors have higher sensitivity to learning rate, the values tend to have lower sensitivity, and vice versa. Thus, while the sensitivity to learning rate can be tuned, there is a tradeoff between sensitivity to model-based regressors for prediction errors and value.</p>
<fig id="pcbi.1004237.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Insensitivity of value (A) and prediction error (B) regressors to the fit learning rate as a function of decay of the reward mean to zero, <italic>γ</italic>, and the drift variance to noise variance ratio of the reward mean, <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub>, in experiments with drifting rewards.</title>
<p>The three black crosses indicate the parameter values in the examples in <xref ref-type="fig" rid="pcbi.1004237.g007">Fig 7</xref>.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g008"/>
</fig>
</sec>
<sec id="sec016">
<title>fMRI results</title>
<p>To test the theoretical predictions, we analyzed BOLD data from Daw et al. [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>], performing a GLM analysis with a variety of different learning rates and examining value and prediction error regressors. The resulting regression coefficients and group <italic>t</italic> statistics are shown in <xref ref-type="fig" rid="pcbi.1004237.g009">Fig 9</xref>. Here we see much less sensitivity to the learning rate of the chosen value signal than the prediction error signal at both the single subject and group levels. This is in line with our predictions, as the reward parameters in the experiment (<italic>γ</italic> ≈ 0.98 and <italic>σ</italic><sub><italic>d</italic></sub>/<italic>σ</italic><sub><italic>n</italic></sub> = 0.7) place it in the upper left of <xref ref-type="fig" rid="pcbi.1004237.g008">Fig 8</xref>, where values are more sensitive to the fit learning rate than are prediction errors.</p>
<fig id="pcbi.1004237.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The effect of learning rate on the BOLD signal in vmPFC in [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>], an experiment with drifting rewards.</title>
<p>(A) Regression coefficients for value of the chosen option at the time of stimulus onset, as a function of learning rate. Each curve represents a single subject. (B) Single subject regression coefficients for prediction error at the time of reward. (C) Group analysis of value signals at the time of stimulus showing the group <italic>t</italic> statistic as a function of (group-wise) learning rate (blue). Red lines denote the best and worst case scenarios obtained by taking the value of the learning rate that either maximizes or minimizes the <italic>t</italic> statistic for each subject. Dashed black line: <italic>p</italic> = 0.05 threshold. (D) Group analysis of prediction error signals at the time of reward.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g009"/>
</fig>
<p>A notable feature of these results is that the different regressors are significantly correlated with the neural signal in different regions of reward-parameter space, with prediction errors significantly correlated with BOLD signals when using low learning rates and values significant at higher learning rate. This reflects the fact that with low learning rates value changes slowly, and so prediction errors are more correlated with the (surprising and drifting) outcomes, whereas for high learning rates it is the value that closely tracks the drifting outcomes. It may not be surprising that the correlation between the different regressors and trial outcomes drives the significance of the regression result in vmPFC, as this area has been repeatedly associated with encoding of outcome magnitude [<xref ref-type="bibr" rid="pcbi.1004237.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1004237.ref033">33</xref>]. However, this result highlights again an important (and worrying) point: while the overall regression coefficients can be remarkably robust to the fit learning rate, interpretation of what function a neural area fulfills can change significantly as the fit parameter values change.</p>
<p>To further investigate the sensitivity of our results to settings of the learning rate, we again computed the log likelihood of the neural data for linear regression models using model-based regressors with different learning rates. These results are shown in <xref ref-type="fig" rid="pcbi.1004237.g010">Fig 10</xref>. Unlike the case of constant reward probability (<xref ref-type="fig" rid="pcbi.1004237.g005">Fig 5</xref>), in this experiment we found much stronger dependence of the log likelihood on learning rate, likely due to the increased contrast-to-noise ratio for the larger vmPFC ROI (11 here, compared to 0.4 for the NAc ROI). This increased sensitivity also allows us to extract a potentially meaningful fit of the learning rate to the fMRI data—on average 0.36 (± 0.08 [s.e.m.]). Of course, this analysis is only suggestive and one should be carefully interpreting group-averaged statistics.</p>
<fig id="pcbi.1004237.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004237.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Fit of a linear model comprised of a single model-based regressor generated with different learning rates <italic>α</italic>, to fMRI data in the drifting reward experiment [<xref ref-type="bibr" rid="pcbi.1004237.ref003">3</xref>] as a function of learning rate.</title>
<p>Each grey curve corresponds to a different subject and in blue is the mean across subjects. All curves are shifted to have a maximum value of 0. The y-axis is the same as that for <xref ref-type="fig" rid="pcbi.1004237.g005">Fig 5</xref>, to highlight the differences between the two experiments.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.g010"/>
</fig>
<p>To decide between different accounts of vmPFC activity—value, prediction error, or both—one could use a similar method to compare the goodness of fit of different models and assess, at the group level, which model fits the data best. In particular, one could compare three distinct linear models: one with a regressor for value but not prediction error, one with a regressor for prediction error but not value and one with both (for instance, generated using the best-fit learning rate). The log-likelihood measure (corrected for the different number of parameters, in this case, the number of regressors) could then be compared to determine the best model. We note that while such <italic>model comparison</italic> is closely related to the questions of parameter fitting and parameter estimation we consider here, it comes with none of the guarantees that we have established for parameter fitting.</p>
</sec>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper, we considered the extent to which errors in the estimation of model parameters impact model-based fMRI. We showed that, in general, the answer to this question depends crucially on the correlation between regressors derived from different parameterizations of the model, <italic>ρ</italic>(<bold>x</bold><sub><italic>g</italic></sub>, <bold>x</bold><sub><italic>f</italic></sub>), and is further affected by the contrast-to-noise ratio in the data, CNR, and the number of trials, <italic>T</italic>, in the experiment. In the specific case where the fit parameter is the learning rate in a reinforcement learning model, we found that regressors for both value and prediction error signals were fairly insensitive to the fit learning rate, such that for realistic values of CNR and <italic>T</italic>, the results of the model-based analysis were predicted to be robust to different parameterizations. Indeed for an experiment with a fixed reward distribution, the estimated learning rate had close to no effect on the detection of prediction error signals in the NAc either in theory or in the experimental data. Similar results also held when rewards were drawn from a Gaussian distribution with a randomly drifting mean.</p>
<p>These findings are consistent with the report from one of the earliest model-based fMRI papers [<xref ref-type="bibr" rid="pcbi.1004237.ref018">18</xref>], in which changing the learning rate from 0.2 to 0.7 was found to have relatively little effect on the results. However, when either the contrast-to-noise ratio or number of trials is high, sensitivity of the model-based analysis to learning rate can increase. This might explain the anecdotal finding (personal communication, J.P. O’Doherty) that the results reported in Bray &amp; O’Doherty [<xref ref-type="bibr" rid="pcbi.1004237.ref034">34</xref>] were relatively sensitive to learning rate. In particular, this study had more trials (<italic>T</italic> = 288) than in either [<xref ref-type="bibr" rid="pcbi.1004237.ref018">18</xref>] or [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>] and also used ‘natural’ rewards (in the form of good- and bad-looking faces) instead of monetary rewards, which might lead to a larger effect and hence greater CNR.</p>
<p>Our results hold important consequences for the interpretation of model-based fMRI experiments. As regards learning rate, the relative insensitivity to this parameter is both good news and bad news. For studies investigating what areas in the brain are involved in reinforcement learning, these results are good news as the robustness to the fit parameters will make errors in the fitting procedure inconsequential. In this sense, our philosophy diverges slightly from that of Forstmann and colleagues [<xref ref-type="bibr" rid="pcbi.1004237.ref035">35</xref>] who suggest redesigning either the model or the experiment if parameters cannot be estimated with sufficient accuracy. In contrast, we espouse the position that imperfect parameter recovery can be tolerated if the scientific question of interest can be answered without it, as it can, for example, when we wish to know <italic>where</italic> reinforcement learning signals are located in the brain.</p>
<p>For studies that ask more nuanced questions, such as whether a particular signal is a reward signal, a value signal or a prediction error signal, or whether different areas use different learning rates, the insensitivity of the neural analysis to learning rate means that a simple analysis is not sufficient. In these cases, there is special premium for clever task design [<xref ref-type="bibr" rid="pcbi.1004237.ref029">29</xref>], and a more detailed analysis, for instance requiring that a putative neural prediction error signal correlate significantly with all its theoretical subcomponents [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>]. Our analysis also suggests a way to minimize this problem: changing the experiment, either by optimizing the dynamics of the reward distribution or increasing the number of trials, can substantially change the sensitivity to learning rate.</p>
<p>The analysis we are suggesting bears resemblance to calculations of statistical power. Statistical power refers to the probability that a specific experiment will be successful in detecting an effect that truly exists—it is obvious why this is an important quantity to optimize in experiment design. Indeed many of the manipulations that we suggest—such as increasing the number of trials—will also improve statistical power. For cases in which the effect one is looking for involves differences in model parameters, we suggest a formula for testing in advance whether these differences are likely to be detectable neurally.</p>
<p>Of course, the fact that parameter values may be difficult to infer from brain data does not mean that they are not inferable at all. In many (if not all) cases, suitable behavioral data can provide strong constraints on model selection and parameter fitting. The ‘power’ of this type of analysis can also be tested, for example by recovering parameters from simulated data [<xref ref-type="bibr" rid="pcbi.1004237.ref036">36</xref>] and using data simulated by different models to test for confusion between these models [<xref ref-type="bibr" rid="pcbi.1004237.ref037">37</xref>]. Nevertheless, it is not obvious that parameters that provide a good description of behavior will necessarily correspond to processes in any brain area. For example, behavior could be driven by a combination of several distinct processes each with different parameter values [<xref ref-type="bibr" rid="pcbi.1004237.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1004237.ref040">40</xref>].</p>
<p>More generally, for parameters other than the learning rate (for example, the discount factor in inter-temporal choice, or the softmax parameter in bandits tasks) our results highlight the importance of testing parameter sensitivity <italic>before</italic> running the experiment. This need not be done analytically (as was the case here) but can be approximated easily using simulations. As our results show, it is often possible to increase or decrease sensitivity to a particular variable by changing the parameters of the task and, with a clear focus on the goal of the model-based analysis, one could use such simulations to optimize experiment design.</p>
<p>Finally, while in this paper we have focused on the sensitivity of model-based fMRI to the parameters of a single model, an important question for future work is the extent to which fMRI can be used to adjudicate between <italic>different</italic> models. Such model comparison would involve computing goodness-of-fit measures (such as the log likelihoods we computed above) for each model and asking which model fit the fMRI data best. The extent to which models can be distinguished based on neural data is related to the degree of divergence of the predictions of the two models (i.e., the correlation between the regressors of the different models). However, it is also likely related to how close the compared models are to the ground-truth generative process that underlies the fMRI data, for which we unfortunately have no <italic>a priori</italic> guarantees.</p>
</sec>
<sec id="sec018">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004237.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Detailed derivation of the statistics of the value and prediction error regressors.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004237.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.s002" mimetype="application/pdf" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Effect of number of trials, <italic>T</italic>, in real data.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004237.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004237.s003" mimetype="application/zip" xlink:type="simple">
<label>S1 Dataset</label>
<caption>
<title>NAc ROI data from [<xref ref-type="bibr" rid="pcbi.1004237.ref010">10</xref>].</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Nathaniel Daw, Peter Dayan, John O’Doherty, and Ray Dolan for helpful comments and for generously sharing their data and Catherine Hartley, Reka Daniel, Nicolas Schuck and Amitai Shenhav for helpful suggestions and comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004237.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>887</fpage>–<lpage>93</lpage>. <object-id pub-id-type="pmid">15235607</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Deichmann</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>Science</source> <volume>304</volume>: <fpage>452</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1094285" xlink:type="simple">10.1126/science.1094285</ext-link></comment> <object-id pub-id-type="pmid">15087550</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name> (<year>2006</year>) <article-title>Cortical substrates for exploratory decisions in humans</article-title>. <source>Nature</source> <volume>441</volume>: <fpage>876</fpage>–<lpage>879</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature04766" xlink:type="simple">10.1038/nature04766</ext-link></comment> <object-id pub-id-type="pmid">16778890</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Preuschoff</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>SR</given-names></name> (<year>2006</year>) <article-title>Neural differentiation of expected reward and risk in human subcortical structures</article-title>. <source>Neuron</source> <volume>51</volume>: <fpage>381</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.06.024" xlink:type="simple">10.1016/j.neuron.2006.06.024</ext-link></comment> <object-id pub-id-type="pmid">16880132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Hampton</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name> (<year>2007</year>) <article-title>Model-based fMRI and its application to reward learning and decision making</article-title>. <source>Ann N Y Acad Sci</source> <volume>1104</volume>: <fpage>35</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1196/annals.1390.022" xlink:type="simple">10.1196/annals.1390.022</ext-link></comment> <object-id pub-id-type="pmid">17416921</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name> (<year>2007</year>) <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>1214</fpage>–<lpage>1221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1954" xlink:type="simple">10.1038/nn1954</ext-link></comment> <object-id pub-id-type="pmid">17676057</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Boorman</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Behrens</surname> <given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Rushworth</surname> <given-names>MFS</given-names></name> (<year>2009</year>) <article-title>How green is the grass on the other side? frontopolar cortex and the evidence in favor of alternative courses of action</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>733</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.05.014" xlink:type="simple">10.1016/j.neuron.2009.05.014</ext-link></comment> <object-id pub-id-type="pmid">19524531</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name> (<year>2011</year>) <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>1204</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Badre</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Doll</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name> (<year>2012</year>) <article-title>Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration</article-title>. <source>Neuron</source> <volume>73</volume>: <fpage>595</fpage>–<lpage>607</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.12.025" xlink:type="simple">10.1016/j.neuron.2011.12.025</ext-link></comment> <object-id pub-id-type="pmid">22325209</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Edlund</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name> (<year>2012</year>) <article-title>Neural prediction errors reveal a risk-sensitive reinforcement-learning process in the human brain</article-title>. <source>The Journal of Neuroscience</source> <volume>32</volume>: <fpage>551</fpage>–<lpage>562</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5498-10.2012" xlink:type="simple">10.1523/JNEUROSCI.5498-10.2012</ext-link></comment> <object-id pub-id-type="pmid">22238090</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Eppinger</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Schuck</surname> <given-names>NW</given-names></name>, <name name-style="western"><surname>Nystrom</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name> (<year>2013</year>) <article-title>Reduced striatal responses to reward prediction errors in older compared with younger adults</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>9905</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2942-12.2013" xlink:type="simple">10.1523/JNEUROSCI.2942-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23761885</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Boorman</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Adolphs</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name> (<year>2013</year>) <article-title>The behavioral and neural mechanisms underlying the tracking of expertise</article-title>. <source>Neuron</source> <volume>80</volume>: <fpage>1558</fpage>–<lpage>1571</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.10.024" xlink:type="simple">10.1016/j.neuron.2013.10.024</ext-link></comment> <object-id pub-id-type="pmid">24360551</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="other">Donoso M, Collins AGE, Koechlin E (In Press) Foundations of human reasoning in the prefrontal cortex. Science.</mixed-citation>
</ref>
<ref id="pcbi.1004237.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name> (<year>2011</year>) <chapter-title>Trial by trial data analysis using computational models</chapter-title>. In: <name name-style="western"><surname>Delgado</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>T</given-names></name>, editors, <source>Decision making, affect, and learning: attention and performance</source>, <publisher-name>Oxford UP</publisher-name>, <publisher-loc>Oxford, UK</publisher-loc>, <volume>XXIII</volume>. pp. <fpage>3</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004237.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Welvaert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rosseel</surname> <given-names>Y</given-names></name> (<year>2013</year>) <article-title>On the definition of signal-to-noise ratio and contrast-to-noise ratio for fmri data</article-title>. <source>PLoS One</source> <volume>8</volume>: <fpage>e77089</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0077089" xlink:type="simple">10.1371/journal.pone.0077089</ext-link></comment> <object-id pub-id-type="pmid">24223118</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name> (<year>2008</year>) <article-title>Dialogues on prediction errors</article-title>. <source>Trends Cogn Sci</source> <volume>12</volume>: <fpage>265</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2008.03.006" xlink:type="simple">10.1016/j.tics.2008.03.006</ext-link></comment> <object-id pub-id-type="pmid">18567531</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="other">Rescorla RA, Wagner AR (1972) A theory of Pavlovian conditioning variations in the effectiveness of reinforcement and nonreinforcement.</mixed-citation>
</ref>
<ref id="pcbi.1004237.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Critchley</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name> (<year>2003</year>) <article-title>Temporal difference models and reward-related learning in the human brain</article-title>. <source>Neuron</source> <volume>38</volume>: <fpage>329</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(03)00169-7" xlink:type="simple">10.1016/S0896-6273(03)00169-7</ext-link></comment> <object-id pub-id-type="pmid">12718865</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Berns</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name> (<year>2003</year>) <article-title>Temporal prediction errors in a passive learning task activate human striatum</article-title>. <source>Neuron</source> <volume>38</volume>: <fpage>339</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(03)00154-5" xlink:type="simple">10.1016/S0896-6273(03)00154-5</ext-link></comment> <object-id pub-id-type="pmid">12718866</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Abler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Walter</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Erk</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kammerer</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Spitzer</surname> <given-names>M</given-names></name> (<year>2006</year>) <article-title>Prediction error as a linear function of reward probability is coded in human nucleus accumbens</article-title>. <source>Neuroimage</source> <volume>31</volume>: <fpage>790</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2006.01.001" xlink:type="simple">10.1016/j.neuroimage.2006.01.001</ext-link></comment> <object-id pub-id-type="pmid">16487726</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McClure</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>King-Casas</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name> (<year>2006</year>) <article-title>Policy adjustment in a dynamic economic game</article-title>. <source>PLoS One</source> <volume>1</volume>: <fpage>e103</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0000103" xlink:type="simple">10.1371/journal.pone.0000103</ext-link></comment> <object-id pub-id-type="pmid">17183636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Preuschoff</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Quartz</surname> <given-names>SR</given-names></name> (<year>2006</year>) <article-title>Neural differentiation of expected reward and risk in human subcortical structures</article-title>. <source>Neuron</source> <volume>51</volume>: <fpage>381</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.06.024" xlink:type="simple">10.1016/j.neuron.2006.06.024</ext-link></comment> <object-id pub-id-type="pmid">16880132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tobler</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>2006</year>) <article-title>Human neural learning depends on reward prediction errors in the blocking paradigm</article-title>. <source>J Neurophysiol</source> <volume>95</volume>: <fpage>301</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00762.2005" xlink:type="simple">10.1152/jn.00762.2005</ext-link></comment> <object-id pub-id-type="pmid">16192329</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>R</given-names></name> (<year>2007</year>) <article-title>Differential encoding of losses and gains in the human striatum</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>4826</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0400-07.2007" xlink:type="simple">10.1523/JNEUROSCI.0400-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17475790</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kuhnen</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Knutson</surname> <given-names>B</given-names></name> (<year>2005</year>) <article-title>The neural basis of financial risk taking</article-title>. <source>Neuron</source> <volume>47</volume>: <fpage>763</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.08.008" xlink:type="simple">10.1016/j.neuron.2005.08.008</ext-link></comment> <object-id pub-id-type="pmid">16129404</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Knutson</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Peterson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Glover</surname> <given-names>G</given-names></name> (<year>2005</year>) <article-title>Distributed neural representation of expected value</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>4806</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0642-05.2005" xlink:type="simple">10.1523/JNEUROSCI.0642-05.2005</ext-link></comment> <object-id pub-id-type="pmid">15888656</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shimojo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name> (<year>2006</year>) <article-title>Is avoiding an aversive outcome rewarding? neural substrates of avoidance learning in the human brain</article-title>. <source>PLoS Biol</source> <volume>4</volume>: <fpage>e233</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0040233" xlink:type="simple">10.1371/journal.pbio.0040233</ext-link></comment> <object-id pub-id-type="pmid">16802856</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schönberg</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Joel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name> (<year>2007</year>) <article-title>Reinforcement learning signals in the human striatum distinguish learners from nonlearners during reward-based decision making</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>12860</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2496-07.2007" xlink:type="simple">10.1523/JNEUROSCI.2496-07.2007</ext-link></comment> <object-id pub-id-type="pmid">18032658</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hare</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Camerer</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name> (<year>2008</year>) <article-title>Dissociating the role of the orbitofrontal cortex and the striatum in the computation of goal values and prediction errors</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>5623</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1309-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1309-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18509023</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kass</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname> <given-names>AE</given-names></name> (<year>1995</year>) <article-title>Bayes factors</article-title>. <source>Journal of the American Statistical Association</source> <volume>430</volume>: <fpage>773</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1995.10476572" xlink:type="simple">10.1080/01621459.1995.10476572</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Uhlenbeck</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Ornstein</surname> <given-names>LS</given-names></name> (<year>1930</year>) <article-title>On the theory of Brownian motion</article-title>. <source>Physical Review</source> <volume>36</volume>: <fpage>823</fpage>–<lpage>841</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRev.36.823" xlink:type="simple">10.1103/PhysRev.36.823</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McNamee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name> (<year>2013</year>) <article-title>Category-dependent and category-independent goal-value codes in human ventromedial prefrontal cortex</article-title>. <source>Nat Neurosci</source> <volume>16</volume>: <fpage>479</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3337" xlink:type="simple">10.1038/nn.3337</ext-link></comment> <object-id pub-id-type="pmid">23416449</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bartra</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>McGuire</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Kable</surname> <given-names>JW</given-names></name> (<year>2013</year>) <article-title>The valuation system: A coordinate-based meta-analysis of bold fmri experiments examining neural correlates of subjective value</article-title>. <source>Neuroimage</source> <volume>76</volume>: <fpage>412</fpage>–<lpage>427</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2013.02.063" xlink:type="simple">10.1016/j.neuroimage.2013.02.063</ext-link></comment> <object-id pub-id-type="pmid">23507394</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bray</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>O’Doherty</surname> <given-names>J</given-names></name> (<year>2007</year>) <article-title>Neural coding of reward-prediction error signals during classical conditioning with attractive faces</article-title>. <source>J Neurophysiol</source> <volume>97</volume>: <fpage>3036</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01211.2006" xlink:type="simple">10.1152/jn.01211.2006</ext-link></comment> <object-id pub-id-type="pmid">17303809</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Forstmann</surname> <given-names>BU</given-names></name>, <name name-style="western"><surname>Wagenmakers</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Eichele</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Serences</surname> <given-names>JT</given-names></name> (<year>2011</year>) <article-title>Reciprocal relations between cognitive neuroscience and formal cognitive models: opposites attract?</article-title> <source>Trends Cogn Sci</source> <volume>15</volume>: <fpage>272</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2011.04.002" xlink:type="simple">10.1016/j.tics.2011.04.002</ext-link></comment> <object-id pub-id-type="pmid">21612972</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="other">Halpern D, Gureckis T (2013). On the identifiability of parameters in reinforcement learning models. URL <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://gureckislab.org/blog/?p=3450">http://gureckislab.org/blog/?p=3450</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004237.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name> (<year>2011</year>) <article-title>Inferring relevance in a changing world</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>5</volume>: <fpage>189</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnhum.2011.00189" xlink:type="simple">10.3389/fnhum.2011.00189</ext-link></comment> <object-id pub-id-type="pmid">22291631</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bornstein</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name> (<year>2012</year>) <article-title>Dissociating hippocampal and striatal contributions to sequential prediction learning</article-title>. <source>Eur J Neurosci</source> <volume>35</volume>: <fpage>1011</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2011.07920.x" xlink:type="simple">10.1111/j.1460-9568.2011.07920.x</ext-link></comment> <object-id pub-id-type="pmid">22487032</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wilson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nassar</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name> (<year>2013</year>) <article-title>A mixture of Delta-rules approximation to Bayesian inference in change-point problems</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003150</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003150" xlink:type="simple">10.1371/journal.pcbi.1003150</ext-link></comment> <object-id pub-id-type="pmid">23935472</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004237.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name> (<year>2010</year>) <chapter-title>Neural models of temporal discounting</chapter-title>. In: <source>Impulsivity: Theory, Science, and Neuroscience of Discounting</source>, <publisher-name>APA books</publisher-name>, <volume>chapter 5</volume>. pp. <fpage>123</fpage>–<lpage>158</lpage>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>