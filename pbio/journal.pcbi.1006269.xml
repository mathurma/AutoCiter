<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00931</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006269</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Deep learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Histology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Histology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Medical personnel</subject><subj-group><subject>Medical doctors</subject><subj-group><subject>Pathologists</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Health care</subject><subj-group><subject>Health care providers</subject><subj-group><subject>Medical doctors</subject><subj-group><subject>Pathologists</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computer architecture</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Biomarkers</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Support vector machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Evaluating reproducibility of AI algorithms in digital pathology with DAPPER</article-title>
<alt-title alt-title-type="running-head">Evaluating reproducibility of AI algorithms in digital pathology with DAPPER</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1586-8350</contrib-id>
<name name-style="western">
<surname>Bizzego</surname> <given-names>Andrea</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2609-4354</contrib-id>
<name name-style="western">
<surname>Bussola</surname> <given-names>Nicole</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9791-9301</contrib-id>
<name name-style="western">
<surname>Chierici</surname> <given-names>Marco</given-names></name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4824-893X</contrib-id>
<name name-style="western">
<surname>Maggio</surname> <given-names>Valerio</given-names></name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1232-7302</contrib-id>
<name name-style="western">
<surname>Francescatto</surname> <given-names>Margherita</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cima</surname> <given-names>Luca</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0127-1342</contrib-id>
<name name-style="western">
<surname>Cristoforetti</surname> <given-names>Marco</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2705-5728</contrib-id>
<name name-style="western">
<surname>Jurman</surname> <given-names>Giuseppe</given-names></name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5384-3605</contrib-id>
<name name-style="western">
<surname>Furlanello</surname> <given-names>Cesare</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Fondazione Bruno Kessler, Trento, Italy</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>DIPSCO, University of Trento, Trento, Italy</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department CIBIO, University of Trento, Trento, Italy</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Pathology Unit, Santa Chiara Hospital, Trento, Italy</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Rohde</surname> <given-names>Gustavo</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Virginia, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jurman@fbk.eu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>27</day>
<month>3</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e1006269</elocation-id>
<history>
<date date-type="received">
<day>3</day>
<month>6</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>2</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Bizzego et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006269"/>
<abstract>
<p>Artificial Intelligence is exponentially increasing its impact on healthcare. As deep learning is mastering computer vision tasks, its application to digital pathology is natural, with the promise of aiding in routine reporting and standardizing results across trials. Deep learning features inferred from digital pathology scans can improve validity and robustness of current clinico-pathological features, up to identifying novel histological patterns, <italic>e.g</italic>., from tumor infiltrating lymphocytes. In this study, we examine the issue of evaluating accuracy of predictive models from deep learning features in digital pathology, as an hallmark of reproducibility. We introduce the DAPPER framework for validation based on a rigorous Data Analysis Plan derived from the FDA’s MAQC project, designed to analyze causes of variability in predictive biomarkers. We apply the framework on models that identify tissue of origin on 787 Whole Slide Images from the Genotype-Tissue Expression (GTEx) project. We test three different deep learning architectures (VGG, ResNet, Inception) as feature extractors and three classifiers (a fully connected multilayer, Support Vector Machine and Random Forests) and work with four datasets (5, 10, 20 or 30 classes), for a total of 53, 000 tiles at 512 × 512 resolution. We analyze accuracy and feature stability of the machine learning classifiers, also demonstrating the need for diagnostic tests (<italic>e.g</italic>., random labels) to identify selection bias and risks for reproducibility. Further, we use the deep features from the VGG model from GTEx on the KIMIA24 dataset for identification of slide of origin (24 classes) to train a classifier on 1, 060 annotated tiles and validated on 265 unseen ones. The DAPPER software, including its deep learning pipeline and the Histological Imaging—Newsy Tiles (HINT) benchmark dataset derived from GTEx, is released as a basis for standardization and validation initiatives in AI for digital pathology.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>In this study, we examine the issue of evaluating accuracy of predictive models from deep learning features in digital pathology, as an hallmark of reproducibility. It is indeed a top priority that reproducibility-by-design gets adopted as standard practice in building and validating AI methods in the healthcare domain. Here we introduce DAPPER, a first framework to evaluate deep features and classifiers in digital pathology, based on a rigorous data analysis plan originally developed in the FDA’s MAQC initiative for predictive biomarkers from massive omics data. We apply DAPPER on models trained to identify tissue of origin from the HINT benchmark dataset of 53, 000 tiles from 787 Whole Slide Images in the Genotype-Tissue Expression (GTEx) project, available at the web address <ext-link ext-link-type="uri" xlink:href="https://gtexportal.org" xlink:type="simple">https://gtexportal.org</ext-link>. We analyze accuracy and feature stability of different deep learning architectures (VGG, ResNet and Inception) as feature extractors and classifiers (a fully connected multilayer, Support Vector Machine and Random Forests) on up to 20 classes. Further, we use the deep features from the VGG model (trained on HINT) on the 1, 300 annotated tiles of the KIMIA24 dataset for identification of slide of origin (24 classes). The DAPPER software is available together with the scripts to generate the HINT benchmark dataset.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Microsoft (US)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5384-3605</contrib-id>
<name name-style="western">
<surname>Furlanello</surname> <given-names>Cesare</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>Microsoft Azure Research grant for Deep Learning for Precision Medicine assigned to CF. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="9"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-04-16</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper, in its supporting information file or otherwise available from the software gitlab repositories: <ext-link ext-link-type="uri" xlink:href="https://gitlab.fbk.eu/mpba-histology/dapper" xlink:type="simple">https://gitlab.fbk.eu/mpba-histology/dapper</ext-link> <ext-link ext-link-type="uri" xlink:href="https://gitlab.fbk.eu/mpba-histology/histolib" xlink:type="simple">https://gitlab.fbk.eu/mpba-histology/histolib</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Artificial Intelligence (AI) methods for health data hold great promise but still have to deal with disease complexity: patient cohorts are most frequently an heterogeneous group of subtypes diverse for disease trajectories, with highly variable characteristics in terms of phenotypes (<italic>e.g</italic>. bioimages in radiology or pathology), response to therapy, clinical course, thus a challenge for machine-learning based prognoses. Nevertheless, the increased availability of massive annotated medical data from health systems and a rapid progress of machine learning frameworks has led to high expectations about the impact of AI on challenging biomedical problems [<xref ref-type="bibr" rid="pcbi.1006269.ref001">1</xref>]. In particular, Deep Learning (DL) is now surpassing pattern recognition methods in the most complex medical images challenges such as those proposed by the Medical Image Computing &amp; Computer Assisted Intervention conferences (MICCAI, <ext-link ext-link-type="uri" xlink:href="https://www.miccai2018.org/en/WORKSHOP---CHALLENGE---TUTORIAL.html" xlink:type="simple">https://www.miccai2018.org/en/WORKSHOP---CHALLENGE---TUTORIAL.html</ext-link>), and it is comparable to expert accuracy in the diagnosis of skin lesions [<xref ref-type="bibr" rid="pcbi.1006269.ref002">2</xref>], classification of colon polyps [<xref ref-type="bibr" rid="pcbi.1006269.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref004">4</xref>], ophthalmology [<xref ref-type="bibr" rid="pcbi.1006269.ref005">5</xref>], radiomics [<xref ref-type="bibr" rid="pcbi.1006269.ref006">6</xref>] and other areas [<xref ref-type="bibr" rid="pcbi.1006269.ref007">7</xref>]. However, the reliable comparison of DL with other baseline ML models and human experts is not a diffuse practice yet [<xref ref-type="bibr" rid="pcbi.1006269.ref008">8</xref>], and also the reproducibility and interpretation of the challenges’ outcome have been recently criticized [<xref ref-type="bibr" rid="pcbi.1006269.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref010">10</xref>]. DL refers to a class of machine learning methods that model high-level abstractions in data through the use of modular architectures, typically composed by multiple nonlinear transformations estimated by training procedures. Notably, deep learning architectures based on Convolutional Neural Networks (CNNs) hold state-of-the-art accuracy in numerous image classification tasks without prior feature selection. Further, intermediate steps in the pipeline of transformations implemented by CNNs or other deep learning architectures can provide a mapping (<italic>embedding</italic>) from the original feature space into a <italic>deep feature</italic> space. Of interest for medical diagnosis, deep features can be used for interpretation of the model and can be directly employed as inputs to other machine learning models.</p>
<p>Deep learning methods have been applied to analysis of histological images for diagnosis and prognosis. Mobadersany and colleagues [<xref ref-type="bibr" rid="pcbi.1006269.ref011">11</xref>] combine in the Survival Convolutional Neural Network (SCNN) architecture a CNN with traditional survival models to learn survival-related patterns from histology images, predicting overall survival of patients diagnosed with gliomas. Predictive accuracy of SCNN is comparable with manual histologic grading by neuropathologists. Further, by incorporation of genomic variables for gliomas in the model, the extended model significantly outperforms the WHO paradigm based on genomic subtype and histologic grading. Similarly, deep learning models have been successfully applied to histology for colorectal cancer [<xref ref-type="bibr" rid="pcbi.1006269.ref012">12</xref>], gastric cancer [<xref ref-type="bibr" rid="pcbi.1006269.ref013">13</xref>], breast cancer [<xref ref-type="bibr" rid="pcbi.1006269.ref014">14</xref>] and lung cancer [<xref ref-type="bibr" rid="pcbi.1006269.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref016">16</xref>].</p>
<p>As human assessments of histology are subjective and hard to repeat, computational analysis of histology imaging within the information environment generated from a digital slide (<italic>digital pathology</italic>) and advances in scanning microscopes have already allowed pathologists to gain a much more effective diagnosis capability and dramatically reduce time for information sharing. Starting from the principle that underlying differences in the molecular expressions of the disease may manifest as tissue architecture and nuclear morphological alterations [<xref ref-type="bibr" rid="pcbi.1006269.ref017">17</xref>], it is clear that automatic evaluation of disease aggressiveness level and patient subtyping has a key role aiding therapy in cancer and other diseases. Digital pathology is in particular a key tool for the immunotherapy approach, which stands on the characterization of tumor-infiltrating lymphocytes (TILs) [<xref ref-type="bibr" rid="pcbi.1006269.ref018">18</xref>]. Indeed, quantitative analysis of the immune microenvironment by histology is crucial for personalized treatment of cancer [<xref ref-type="bibr" rid="pcbi.1006269.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref020">20</xref>], with high clinical utility of TILs assessment for risk prediction models, adjuvant, and neoadjuvant chemotherapy decisions, and for developing the potential of immunotherapy [<xref ref-type="bibr" rid="pcbi.1006269.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref022">22</xref>]. Digital pathology is thus a natural application domain for machine learning, with the promise of accelerating routine reporting and standardizing results across trials. Notably, deep learning features learned from digital pathology scans can improve validity and robustness of current clinico-pathological features, up to identifying novel histological patterns, <italic>e.g</italic>. from TILs.</p>
<p>On the technical side, usually deep learning models for digital pathology are built upon imaging architectures originally aimed at tasks in other domains and trained on non-medical datasets. This is a foundational approach in machine learning, known as <italic>transfer learning</italic>. Given domain data and a network pretrained to classify on huge generic databases (<italic>e.g</italic>. ImageNet, with over 14 million items and 20 thousand categories [<xref ref-type="bibr" rid="pcbi.1006269.ref023">23</xref>]), there are three basic options for transfer learning, <italic>i.e</italic>. to adapt the classifier to the new domain: a) train a new machine learning model on the features preprocessed by the pretrained network from the domain data; b) retrain only the deeper final layers (the <italic>domain layers</italic>) of the pretrained network; c) retrain the whole network starting from the pretrained state. A consensus about the best strategy to use for medical images is still missing [<xref ref-type="bibr" rid="pcbi.1006269.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref025">25</xref>].</p>
<p>In this study we aim to address the issue of reproducibility and validation of machine learning models for digital pathology. Reproducibility is a paramount concern in biomarker research [<xref ref-type="bibr" rid="pcbi.1006269.ref026">26</xref>], and in science in general [<xref ref-type="bibr" rid="pcbi.1006269.ref027">27</xref>], with scientific communities, institutions, industry, and publishers struggling to foster adoption of best practices, with initiatives ranging from enhancing reproducibility of high-throughput technologies [<xref ref-type="bibr" rid="pcbi.1006269.ref028">28</xref>] to improving the overall reuse of scholarly data and analytics solutions (<italic>e.g</italic>. the FAIR Data Principles [<xref ref-type="bibr" rid="pcbi.1006269.ref029">29</xref>]). As an example, the MAQC initiatives [<xref ref-type="bibr" rid="pcbi.1006269.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref031">31</xref>], led by the US FDA, investigate best practices and causes of variability in the development of biomarkers and predictive classifiers from massive omics data (<italic>e.g</italic>. microarrays, RNA-Seq or DNA-Seq data) for precision medicine. The MAQC projects adopt a Data Analysis Plan (DAP) that forces bioinformatics teams to submit classification models, top features ranked for importance and performance estimates all built on training data only, before testing on unseen external validation data. The DAP approach is methodologically more robust than a simple cross validation (CV) [<xref ref-type="bibr" rid="pcbi.1006269.ref030">30</xref>] as the internal CV and model selection phase is replicated multiple times (<italic>e.g</italic>., 10 times) to smooth the impact of a single training/test split; the performance metrics is thus evaluated on a much larger statistics. Also, features are analyzed and ranked multiple times, averaging the impact of a small round of partitions. The ranked feature lists are fused in a single ranked list using the Borda method [<xref ref-type="bibr" rid="pcbi.1006269.ref032">32</xref>] and the bootstrap method is applied to compute the confidence intervals. This approach helps mitigating the risk of selection bias in complex learning pipelines [<xref ref-type="bibr" rid="pcbi.1006269.ref033">33</xref>], where the bias can stem in one of many preprocessing steps as well as in the downstream machine learning model. Further, it clarifies that increasing task difficulty is often linked to a decrease not only in accuracy measures but also of stability of the biomarker lists [<xref ref-type="bibr" rid="pcbi.1006269.ref032">32</xref>], <italic>i.e</italic>. the consistency in the selection of the top discriminating features across all repeated cross validation runs.</p>
<p>Although openness in sharing algorithms and benchmark data is a solid attitude of the machine learning community, the reliable estimation on a given training dataset of predictive accuracy and stability of deep learning models (in terms of performance range as a function of variations of training data) and the stability of deep features used by external models (as the limited difference of top ranking variables selected by different models) is still a gray area. The underlying risk is that of overfitting the training data, or worse to overfit the validation data if the labels are visible, which is typical when datasets are fully released at the end of a data science challenge on medical image data. As the number of DL-based studies in digital pathology is exponentially growing, we suggest that the progress of this field needs environments (<italic>e.g</italic>., DAPs) to prevent such pitfalls, especially if features distilled by the network are used as radiomics biomarkers to inform medical decision. Further, given an appropriate DAP, alternative model choices should be benchmarked on publicly available datasets, as usual in the general computer vision domain (<italic>e.g</italic>., ImageNet [<xref ref-type="bibr" rid="pcbi.1006269.ref023">23</xref>] or COCO [<xref ref-type="bibr" rid="pcbi.1006269.ref034">34</xref>]).</p>
<p>This study provides three main practical contributions to controlling for algorithmic bias and improving reproducibility of machine learning algorithms for digital pathology:</p>
<list list-type="order">
<list-item>
<p>A Data Analysis Plan (DAP) specialized for digital pathology, tuned on the predictive evaluation of deep features, extracted by a network and used by alternative classification heads. To the best of our knowledge, this is the first study where a robust model validation method (the DAP) is applied in combination with the deep learning approach. We highlight that the approach can be adopted in other medical/biology domains in which Artificial Intelligence is rapidly emerging, <italic>e.g</italic>., in the analysis of radiological images.</p>
</list-item>
<list-item>
<p>A benchmark dataset (HINT) of 53, 727 tiles of histological images from 30 tissue types, derived from GTEx [<xref ref-type="bibr" rid="pcbi.1006269.ref035">35</xref>] for the recognition of tissue of origin of up to 30 classes. The HINT dataset can be used by other researchers to pretrain the weights of DL architectures that shall be applied on digital pathology tasks (<italic>e.g</italic>., detection of TILs) thus accelerating the training of application-specific models. In the past 5 years, having a shared image dataset (<italic>e.g</italic>., the ImageNet) allowed the development of a number of deep learning models for general image classification (<italic>e.g</italic>. VGG, ResNet, AlexNet). Such pretrained networks have then been effectively applied on a variety of different tasks. With the HINT dataset we aim at favouring a similar boost on digital pathology.</p>
</list-item>
<list-item>
<p>An end-to-end machine learning framework (DAPPER) as a baseline environment for predictive models in digital pathology, where end-to-end indicates that the DAPPER framework is directly applied to the digital pathology images, with the deep learning component producing features for the machine learning head, without an external procedure (<italic>e.g</italic>., a handcrafted feature extraction) to preprocess the features. To the best of our knowledge, this is the first example of a DL approach for the classification of up to 30 different tissues, all with the same staining, which represents, <italic>per se</italic>, a valuable contribution to the digital pathology community.</p>
</list-item>
</list>
<p>We first apply DAPPER to a set of classification experiments on 787 Whole Slide Images (WSIs) from GTEx. The framework (see <xref ref-type="fig" rid="pcbi.1006269.g001">Fig 1</xref>) is composed by (A) a preprocessing component to derive patches from WSIs; (B) a 3-step machine learning pipeline with a data augmentation preprocessor, a backend deep learning model, and an adapter extracting the deep features; (C) a downstream machine learning/deep learning head, <italic>i.e</italic>. the task specific predictor. In our experiments, we evaluate the accuracy and the feature stability in a multiclass setting for the combination of three different deep learning architectures, namely VGG, ResNet and Inception, used as feature extractors, and three classifiers, a fully connected multilayer network, Support Vector Machine (SVM) [<xref ref-type="bibr" rid="pcbi.1006269.ref036">36</xref>] and Random Forest (RF) [<xref ref-type="bibr" rid="pcbi.1006269.ref037">37</xref>]. This component is endowed with the DAP, <italic>i.e</italic>., a 10 × 5 CV (5-fold cross validation iterated 10 times). The 50 internal validation sets are used to estimate a vector of metrics (with confidence intervals) that are then used for model selection. In the fourth component (D) we finally provide an unsupervised data analysis based on the UMAP projection method, and methods for feature exploration. The DAPPER software is available together with the Python scripts and the instructions to generate the HINT benchmark dataset as a collection of Jupyter notebooks at <monospace>gitlab.fbk.eu/mpba-histology/dapper</monospace>, released under the GNU General Public License v3. Notably, the DAP estimates are provided in this paper only for the downstream machine learning/deep learning head in component (C); whenever computational resources are available, the DAP can be expanded also to component (B). Here we kept as a separate problem the model selection exercise on the backend deep learning architecture in order to clarify the change of perspective with respect to optimization of machine learning models in the usual training-validation setting.</p>
<fig id="pcbi.1006269.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The DAPPER environment.</title>
<p>Components: A) The WSI preprocessing pipeline; B) the deep learning backend to extract deep features; C) the Data Analysis Plan (DAP) for the machine learning models; and D) the UMAP module and other modules for unsupervised analysis.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g001" xlink:type="simple"/>
</fig>
<p>As a second experiment, in order to study the DAPPER framework in a transfer learning condition, we use the deep features from the VGG model trained on a subset of HINT on the 1, 300 annotated tiles of the KIMIA Path24 dataset [<xref ref-type="bibr" rid="pcbi.1006269.ref038">38</xref>] to identify in this case the slide of origin (24 classes).</p>
<p>Previous work on classifying WSIs by means of neural networks was introduced by [<xref ref-type="bibr" rid="pcbi.1006269.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref039">39</xref>], also with the purpose of distributing the two original datasets KIMIA Path960 (KIMIA960) and KIMIA Path24 (KIMIA24). KIMIA24 consists of 24 WSIs chosen on purely visual distinctions. Babaie and coauthors [<xref ref-type="bibr" rid="pcbi.1006269.ref038">38</xref>] manually selected a total of 1, 325 binary patches with 40% overlap. On this dataset, in addition to two models based on Local Binary Patterns (LBP) and Bag-of-Visual-Words (BoVW), they applied two shallow CNNs, achieving at most 41.8% accuracy. On the other hand, KIMIA960 contains 960 histopathological images belonging to 20 different WSIs that, again on visual clues, were used to represent different texture/pattern/staining types. The very same experimental settings as the one for KIMIA24, <italic>i.e</italic>., LBP, BoVW and CNN, has been replicated on this dataset by Kumar and coauthors [<xref ref-type="bibr" rid="pcbi.1006269.ref039">39</xref>]. In particular, the authors applied AlexNet or VGG16, both pretrained on ImageNet, to extract deep features; instead of a classifier, accuracy was established by computing similarity distances between the 4, 096 features extracted. Also, Kieffer and coauthors in [<xref ref-type="bibr" rid="pcbi.1006269.ref025">25</xref>] explored the use of deep features from several pretrained structures on KIMIA24, controlling for the impact of transfer learning and finding an advantage of pretrained networks against training from scratch. Conversely, Alhindi and coworkers [<xref ref-type="bibr" rid="pcbi.1006269.ref040">40</xref>] analyzed KIMIA960 for slide of origin (20 slides preselected by visual inspection), and similarly to our study they compared alternative classifiers as well as feature extraction models in a 3-fold CV setup. Considering the importance of clinical validation of predictive results [<xref ref-type="bibr" rid="pcbi.1006269.ref008">8</xref>], we finally compared the performance of the DAPPER framework with an expert pathologist. DAPPER outperforms the pathologist in classifying tissues at tile level, while at WSI level performance are similar.</p>
<p>DAPPER represents an advancement over previous studies, due to the DAP structure and its application to the large HINT dataset free of any visual preselection.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Dataset</title>
<p>The images used to train the models were derived from the Genotype-Tissue Expression (GTEx) Study [<xref ref-type="bibr" rid="pcbi.1006269.ref035">35</xref>]. The study collects gene expression profiles and whole-slide images (WSIs) of 53 human tissues histologies used to investigate the relationship between genetic variation and tissue-specific gene expression in healthy individuals. To ensure that the collected tissues meet prescribed standard criteria, a Pathology Resource Center validated each sample origin, content, integrity and target tissue (<ext-link ext-link-type="uri" xlink:href="https://biospecimens.cancer.gov/resources/sops/" xlink:type="simple">https://biospecimens.cancer.gov/resources/sops/</ext-link>). After sectioning and Haemotoxylin and Eosin staining (H&amp;E), tissue samples were scanned using a digital whole slide imaging system (Aperio) and stored in .<italic>svs</italic> format [<xref ref-type="bibr" rid="pcbi.1006269.ref041">41</xref>].</p>
<p>A custom Python script was used to download 787 WSIs through the Biospecimen Research Database (total size: 192 GB, average 22 WSIs for each tissue). The list of the downloaded WSIs is available in <xref ref-type="supplementary-material" rid="pcbi.1006269.s001">S1 Table</xref>.</p>
<p>A data preprocessing pipeline was developed to prepare the WSIs as training data (see <xref ref-type="fig" rid="pcbi.1006269.g002">Fig 2</xref>). The WSIs have a resolution of 0.275 <italic>μ</italic>m/pixel (Magnification 40<italic>X</italic>) and variable dimensions. Further, the region interested by the tissue is only a portion of the WSI and it varies across the samples. Hence first we identified the region of the tissue in the image (see <xref ref-type="fig" rid="pcbi.1006269.g002">Fig 2</xref>), then we extracted at most 100 tiles (512 × 512 pixel) from the WSIs, by randomly sampling the tissue region. We applied the algorithm for the detection of the tissue region (see <xref ref-type="fig" rid="pcbi.1006269.g002">Fig 2</xref>) on each tile and rejected those where the portion of the tissue was below 85%. A total number of 53, 727 tiles was extracted, with a number of tiles per tissue varying between 59 (for Adipose—Visceral (Omentum)) and 2, 689 (for Heart—Left Ventricle). Four datasets (HINT5, HINT10, HINT20, HINT30) have been derived with increasing number of tissues for a total of 52, 991 tiles; the full number of tiles per anatomical zone, for each dataset, is available in <xref ref-type="supplementary-material" rid="pcbi.1006269.s002">S2 Table</xref> and summarized in <xref ref-type="table" rid="pcbi.1006269.t001">Table 1</xref>. We refer to the four sets as the HINT collection, or the HINT dataset in brief. We choose the five tissues composing HINT5 based on exploratory experiments, while the other three datasets were composed including the tissues with higher number of tiles. The class imbalance is accounted for by weighting the error on predictions. In detail, the weight <italic>w</italic> of the class <italic>i</italic> used in the cross entropy function is computed as: <italic>w</italic><sub><italic>i</italic></sub> = <italic>n</italic><sub>max</sub>/<italic>n</italic><sub><italic>i</italic></sub>, where <italic>n</italic><sub>max</sub> is the number of tiles in the class with more tiles and <italic>n</italic><sub><italic>i</italic></sub> is the number of tiles in the class <italic>i</italic>.</p>
<fig id="pcbi.1006269.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The tissue detection pipeline.</title>
<p>The identification of the tissue bounding box is performed on the WSI thumbnail in three steps: a) Binarization of the grayscale image by applying Otsu’s thresholding; b) Binary dilation and filling of the holes; c) Selection of the biggest connected region as tissue region and computation of the vertex of the containing rectangle.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g002" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1006269.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t001</object-id>
<label>Table 1</label>
<caption>
<title>Summary of the HINT datasets.</title>
<p>Total: total number of tiles composing the dataset; Min: number of tiles in the class with less samples; Max: number of tiles in the class with more samples; Average; average number of tiles for each class.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Name</th>
<th align="right"># tissues</th>
<th align="right">Total</th>
<th align="right">Min</th>
<th align="right">Max</th>
<th align="right">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">HINT5</td>
<td align="right">5</td>
<td align="right">8, 218</td>
<td align="right">1, 009</td>
<td align="right">2, 424</td>
<td align="char" char=".">1, 643.6</td>
</tr>
<tr>
<td align="left">HINT10</td>
<td align="right">10</td>
<td align="right">22, 885</td>
<td align="right">1, 890</td>
<td align="right">2, 689</td>
<td align="char" char=".">2, 288.5</td>
</tr>
<tr>
<td align="left">HINT20</td>
<td align="right">20</td>
<td align="right">40, 516</td>
<td align="right">1, 574</td>
<td align="right">2, 689</td>
<td align="char" char=".">2, 025.8</td>
</tr>
<tr>
<td align="left">HINT30</td>
<td align="right">30</td>
<td align="right">52, 991</td>
<td align="right">957</td>
<td align="right">2, 689</td>
<td align="char" char=".">1, 766.4</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Since image orientation should not be relevant for the tissue recognition, the tiles are randomly flipped (horizontally and vertically) and scaled, following a common practice in deep learning known as <italic>data augmentation</italic>. Data augmentation consists of different techniques (such as cropping, flipping, rotating images) performed each time a sample is loaded, so that the resulting input image is different at each epoch. Augmentation has proven effective in multiple problems, increasing the generalization capabilities of the network, preventing overfitting and improving models performance [<xref ref-type="bibr" rid="pcbi.1006269.ref042">42</xref>–<xref ref-type="bibr" rid="pcbi.1006269.ref044">44</xref>].</p>
<p>Such randomized transformations were found to provide more comparable performance between the prognostic accuracy of the deep learning SCNN architecture and that of standard models (<italic>i.e</italic>., Support Vector Machine, Random Forest) based on combined molecular subtype and histologic grade [<xref ref-type="bibr" rid="pcbi.1006269.ref011">11</xref>]. In addition, each tile is cropped to a fixed size, which is dependent on the type of network used to extract the deep features.</p>
</sec>
<sec id="sec004">
<title>Deep learning architectures and training strategies</title>
<p>We exploited three backend architectures commonly used in computer vision tasks:</p>
<list list-type="order">
<list-item>
<p>VGG, Net-E version (19 layers) with Batch Normalization (BN) layers [<xref ref-type="bibr" rid="pcbi.1006269.ref045">45</xref>];</p>
</list-item>
<list-item>
<p>ResNet, 152-layer model [<xref ref-type="bibr" rid="pcbi.1006269.ref046">46</xref>];</p>
</list-item>
<list-item>
<p>Inception, version 3 [<xref ref-type="bibr" rid="pcbi.1006269.ref047">47</xref>].</p>
</list-item>
</list>
<p>These architectures have reached highest accuracy in multiclass classification problems over the last 4 years [<xref ref-type="bibr" rid="pcbi.1006269.ref048">48</xref>] and differ in resource utilization (see <xref ref-type="table" rid="pcbi.1006269.t002">Table 2</xref>). The feature extraction layer of each backend network is obtained as the output of an end-to-end pipeline composed of the following main blocks (see panel B in <xref ref-type="fig" rid="pcbi.1006269.g001">Fig 1</xref>):</p>
<list list-type="order">
<list-item>
<p>Data augmentation: the input tiles are processed and assembled into batches of size 32;</p>
</list-item>
<list-item>
<p>Feature Extractor: series of convolutional layers (Conv2d: with different number of channels and kernel size), normalization layers (Batch Norm) and pooling layers (MaxPool2d: with different kernel size) designed to fit with the considered backend architecture (VGG, ResNet, Inception). The number of output features of the Feature Extractor depends on the structure of the backend architecture used;</p>
</list-item>
<list-item>
<p>Adapter: as the backend networks have output features of different sizes, we add a linear layer at the end of the Feature Extractor, in order to make the pipeline uniform. The Adapter takes the features of the backend network as input and output a fixed number of features (1, 000).</p>
</list-item>
</list>
<table-wrap id="pcbi.1006269.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t002</object-id>
<label>Table 2</label>
<caption>
<title>Backend architectures statistics.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Name</th>
<th align="right">Output features</th>
<th align="right">#Parameters</th>
<th align="right">Layers</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VGG</td>
<td align="right">25, 088</td>
<td align="right">155 × 10<sup>6</sup></td>
<td align="right">19</td>
</tr>
<tr>
<td align="left">ResNet</td>
<td align="right">2, 048</td>
<td align="right">95 × 10<sup>6</sup></td>
<td align="right">152</td>
</tr>
<tr>
<td align="left">Inception</td>
<td align="right">2, 048</td>
<td align="right">35 × 10<sup>6</sup></td>
<td align="right">42</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The 1, 000 Adapter features are then used as input for a classifier providing predicted tissue labels as output. As predictive models, we used a linear SVM with regularization parameter <italic>C</italic> set to 1, a RF classifier with 500 trees (both implemented in <italic>scikit-learn</italic>, <monospace>v0.19.1</monospace>) and a fully connected head (FCH), namely a series of fully connected layers (see panel C in <xref ref-type="fig" rid="pcbi.1006269.g001">Fig 1</xref>). Inspired by [<xref ref-type="bibr" rid="pcbi.1006269.ref011">11</xref>] and [<xref ref-type="bibr" rid="pcbi.1006269.ref049">49</xref>], our FCH consists of four dense layers with 1, 000, 1, 000, 256 and <italic>number of tissue classes</italic> nodes, respectively. The feature extraction block was initialized with the weights already trained on the ImageNet dataset [<xref ref-type="bibr" rid="pcbi.1006269.ref023">23</xref>], provided by <italic>PyTorch</italic> (<monospace>v0.4.0</monospace>) and frozen. The Adapter block is trained together with the FCH as a one network. Training also the weights of the feature extraction block improves accuracy (see <xref ref-type="supplementary-material" rid="pcbi.1006269.s003">S3 Table</xref>). However, these results were not validated rigorously within the DAP and therefore they not are not claimed as generalized in this study.</p>
<p>For the optimization of the other weights (Adapter and FCH) we used the Adam algorithm [<xref ref-type="bibr" rid="pcbi.1006269.ref050">50</xref>] with the learning rate set to 10<sup>−5</sup> and fixed for the whole training. We used the cross entropy as the loss function, which is appropriate for multiclass models.</p>
<p>The strategy to optimize the learning rate was selected based on results of a preparatory study with the VGG network and HINT5. The strategy approach with fixed learning rate achieved the best results (see <xref ref-type="supplementary-material" rid="pcbi.1006269.s004">S4 Table</xref>) and was therefore adopted in the rest of the study.</p>
</sec>
<sec id="sec005">
<title>Data analysis plan</title>
<p>Following the rigorous model validation techniques proposed by the MAQC projects [<xref ref-type="bibr" rid="pcbi.1006269.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref031">31</xref>], we adopted a DAP to assess the validity of the features extracted by the networks, namely a 10 × 5-fold cross validation (CV) schema. The input dataset is first partitioned in two separate datasets, the <italic>training set</italic> and the <italic>test set</italic>, also referred as <italic>external validation set</italic> as reported in [<xref ref-type="bibr" rid="pcbi.1006269.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref031">31</xref>]. The external validation set will be kept completely unseen to the model, and it will be only used in the very last step of the DAP for the final model evaluation. In our experimental settings, we used 80% of the total samples for the training set, and the remaining 20% for the external validation set. A stratification strategy upon the classes of tiles, <italic>i.e</italic>., 5, 10, or 20, has been adopted in the partitioning. The training set further undergoes a 5-fold CV iterated 10 times, resulting in 50 separated <italic>internal validation sets</italic> used for model evaluation within the DAP. The same stratification strategy is used in the creation of the folds.</p>
<p>At each CV iteration, features are ranked by KBest, with ANOVA F-score as the scoring function [<xref ref-type="bibr" rid="pcbi.1006269.ref051">51</xref>], and four separate models are trained on sets of increasing number of ranked features (namely: 10%, 25%, 50%, 100% of the total number of features). A list of top-ranked features is obtained by Borda aggregation of the ranked lists, for which we also compute the Canberra stability with a computational framework designed for sets of ranked biomarker lists [<xref ref-type="bibr" rid="pcbi.1006269.ref032">32</xref>].</p>
<p>As for model evaluation, we considered the accuracy (ACC), and the Matthews Correlation Coefficient (MCC) in their multiclass generalization [<xref ref-type="bibr" rid="pcbi.1006269.ref052">52</xref>–<xref ref-type="bibr" rid="pcbi.1006269.ref054">54</xref>]:
<disp-formula id="pcbi.1006269.e001"><alternatives><graphic id="pcbi.1006269.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006269.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>ACC</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mfrac> <mml:mo>,</mml:mo> <mml:mspace width="14.22636pt"/><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:mtext>ACC</mml:mtext> <mml:mo>≤</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
<disp-formula id="pcbi.1006269.e002"><alternatives><graphic id="pcbi.1006269.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006269.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>MCC</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle> <mml:mrow><mml:msqrt><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>g</mml:mi> <mml:mi>f</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:msqrt> <mml:msqrt><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd> <mml:mi>f</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>g</mml:mi></mml:mrow></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:msqrt></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo> <mml:mspace width="14.22636pt"/><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>≤</mml:mo> <mml:mtext>MCC</mml:mtext> <mml:mo>≤</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>N</italic> is the number of classes and <italic>C</italic><sub><italic>st</italic></sub> is the number of elements of true class <italic>s</italic> that have been predicted as class <italic>t</italic>.</p>
<p>MCC is widely used in Machine Learning as a performance metric, especially for unbalanced sets, for which ACC can be misleading [<xref ref-type="bibr" rid="pcbi.1006269.ref055">55</xref>]. In particular, MCC gives an indication of prediction robustness among classes: MCC = 1 is perfect classification, MCC = −1 is extreme misclassification, and MCC = 0 corresponds to random prediction.</p>
<p>Finally, the overall performance of the model is evaluated across all the iterations (<italic>i.e</italic>., internal validation sets), in terms of average MCC and ACC with 95% Studentized bootstrap confidence intervals (CI) [<xref ref-type="bibr" rid="pcbi.1006269.ref056">56</xref>], and then on the external validation set.</p>
<p>As a sanity check to avoid unwanted selection bias effects, the DAP is repeated stochastically scrambling the training set labels (<italic>random labels</italic> mode) or by randomly ranking features before building models (<italic>random ranking</italic> mode: in presence of pools of highly correlated variables, top features can be interchanged with others, possibly of higher biological interest). In both modes, a procedure unaffected by selection bias should achieve an average MCC close to 0.</p>
</sec>
<sec id="sec006">
<title>Experiments on HINT</title>
<p>We designed a set of experiments reported in <xref ref-type="table" rid="pcbi.1006269.t003">Table 3</xref> to provide indications about the optimal architecture for deep feature extraction, while keeping fixed the other hyper-parameters. In particular we set batch size (32) and number of epochs (50), large enough to let the network converge: we explored increasing numbers of epochs (10, 30, 50, 100) and, since the loss stabilizes after about 35 epochs, we set the number of epochs to 50. First, we compared the three backend architectures on the smallest dataset HINT5, with fixed learning rate. Both VGG and ResNet architectures achieved good results, outperforming Inception as shown in Tables <xref ref-type="table" rid="pcbi.1006269.t004">4</xref> and <xref ref-type="table" rid="pcbi.1006269.t005">5</xref>. In successive analyses we thus restricted to use VGG and ResNet as feature extractors and validated performance and features with the DAP. The same process was adopted on HINT10 and HINT20. An experiment with 30 tissues has also been performed. Results are listed in <xref ref-type="supplementary-material" rid="pcbi.1006269.s005">S5 Table</xref>.</p>
<table-wrap id="pcbi.1006269.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t003</object-id>
<label>Table 3</label>
<caption>
<title>Summary of experiments with the backend architectures.</title>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="right">Dataset</th>
<th align="right">Feature extractor</th>
<th align="right">Version/Model</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VGG-5</td>
<td align="right">HINT5</td>
<td align="right">VGG</td>
<td align="right">Net-E+BN</td>
</tr>
<tr>
<td align="left">ResNet-5</td>
<td align="right">HINT5</td>
<td align="right">ResNet</td>
<td align="right">152-layer</td>
</tr>
<tr>
<td align="left">Inception-5</td>
<td align="right">HINT5</td>
<td align="right">Inception</td>
<td align="right">3</td>
</tr>
<tr>
<td align="left">VGG-10</td>
<td align="right">HINT10</td>
<td align="right">VGG</td>
<td align="right">Net-E+BN</td>
</tr>
<tr>
<td align="left">ResNet-10</td>
<td align="right">HINT10</td>
<td align="right">ResNet</td>
<td align="right">152-layer</td>
</tr>
<tr>
<td align="left">VGG-20</td>
<td align="right">HINT20</td>
<td align="right">VGG</td>
<td align="right">Net-E+BN</td>
</tr>
<tr>
<td align="left">ResNet-20</td>
<td align="right">HINT20</td>
<td align="right">ResNet</td>
<td align="right">152-layer</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1006269.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t004</object-id>
<label>Table 4</label>
<caption>
<title>Matthew correlation coefficient values for each experiment, and classifier head pairs on HINT dataset.</title>
<p>The average cross validation MCC with 95% CI (<bold>H-MCCt</bold>), and MCC on the external validation set (<bold>H-MCCv</bold>) are reported. Best-performing backend network, and classifier head combination on each dataset are reported in bold.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">FCH</th>
<th align="center" colspan="2">SVM</th>
<th align="center" colspan="2">RF</th>
</tr>
<tr>
<th align="left">Experiment</th>
<th align="center">H-MCCt</th>
<th align="right">H-MCCv</th>
<th align="center">H-MCCt</th>
<th align="right">H-MCCv</th>
<th align="center">H-MCCt</th>
<th align="right">H-MCCv</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VGG-5</td>
<td align="left">0.841 (0.838, 0.843)</td>
<td align="char" char=".">0.820</td>
<td align="left">0.786 (0.783, 0.789)</td>
<td align="char" char=".">0.777</td>
<td align="left">0.750 (0.748, 0.753)</td>
<td align="char" char=".">0.747</td>
</tr>
<tr>
<td align="left">ResNet-5</td>
<td align="left"><bold>0.879 (0.877, 0.881)</bold></td>
<td align="char" char="."><bold>0.883</bold></td>
<td align="left">0.852 (0.850, 0.854)</td>
<td align="char" char=".">0.840</td>
<td align="left">0.829 (0.827, 0.832)</td>
<td align="char" char=".">0.849</td>
</tr>
<tr>
<td align="left">Inception-5</td>
<td align="left"/>
<td align="right"/>
<td align="left">0.747 (0.744, 0.750)</td>
<td align="char" char=".">0.734</td>
<td align="left">0.703 (0.699, 0.706)</td>
<td align="char" char=".">0.701</td>
</tr>
<tr>
<td align="left">VGG-10</td>
<td align="left"><bold>0.896 (0.894, 0.897)</bold></td>
<td align="char" char="."><bold>0.894</bold></td>
<td align="left">0.861 (0.859, 0.862)</td>
<td align="char" char=".">0.866</td>
<td align="left">0.889 (0.888, 0.891)</td>
<td align="char" char=".">0.886</td>
</tr>
<tr>
<td align="left">ResNet-10</td>
<td align="left">0.857 (0.856, 0.859)</td>
<td align="char" char=".">0.860</td>
<td align="left">0.825 (0.824, 0.827)</td>
<td align="char" char=".">0.832</td>
<td align="left">0.845 (0.843, 0.847)</td>
<td align="char" char=".">0.850</td>
</tr>
<tr>
<td align="left">VGG-20</td>
<td align="left">0.771 (0.770, 0.772)</td>
<td align="char" char=".">0.774</td>
<td align="left">0.729 (0.727, 0.730)</td>
<td align="char" char=".">0.731</td>
<td align="left">0.761 (0.760, 0.762)</td>
<td align="char" char=".">0.766</td>
</tr>
<tr>
<td align="left">ResNet-20</td>
<td align="left">0.756 (0.754, 0.757)</td>
<td align="char" char=".">0.757</td>
<td align="left"><bold>0.788 (0.787, 0.789)</bold></td>
<td align="char" char="."><bold>0.792</bold></td>
<td align="left">0.738 (0.737, 0.739)</td>
<td align="char" char=".">0.738</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1006269.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t005</object-id>
<label>Table 5</label>
<caption>
<title>Accuracy values for each experiment, and classifier head pairs on HINT dataset.</title>
<p>The average cross validation ACC with 95% CI and ACC on the external validation set are reported. Best-performing backend network, and classifier head combination on each dataset are reported in bold.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">FCH</th>
<th align="center" colspan="2">SVM</th>
<th align="center" colspan="2">RF</th>
</tr>
<tr>
<th align="left">Experiment</th>
<th align="center">H-ACCt</th>
<th align="right">H-ACCv</th>
<th align="center">H-ACCt</th>
<th align="right">H-ACCv</th>
<th align="center">H-ACCt</th>
<th align="right">H-ACCv</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VGG-5</td>
<td align="left">87.2 (87.0, 87.5)</td>
<td align="char" char=".">85.6</td>
<td align="left">82.9 (82.7, 83.1)</td>
<td align="char" char=".">82.1</td>
<td align="left">79.9 (79.7, 80.1)</td>
<td align="char" char=".">79.7</td>
</tr>
<tr>
<td align="left">ResNet-5</td>
<td align="left"><bold>90.3 (90.1, 90.5)</bold></td>
<td align="char" char="."><bold>90.7</bold></td>
<td align="left">88.1 (88.0, 88.3)</td>
<td align="char" char=".">87.2</td>
<td align="left">86.3 (86.1, 86.5)</td>
<td align="char" char=".">87.9</td>
</tr>
<tr>
<td align="left">Inception-5</td>
<td align="left"/>
<td align="right"/>
<td align="left">79.8 (79.5, 80.0)</td>
<td align="char" char=".">78.7</td>
<td align="left">76.2 (75.9, 76.4)</td>
<td align="char" char=".">75.9</td>
</tr>
<tr>
<td align="left">VGG-10</td>
<td align="left"><bold>90.6 (90.5, 90.7)</bold></td>
<td align="char" char="."><bold>90.5</bold></td>
<td align="left">87.5 (87.3, 87.6)</td>
<td align="char" char=".">88.0</td>
<td align="left">90.0 (89.9, 90.2)</td>
<td align="char" char=".">89.7</td>
</tr>
<tr>
<td align="left">ResNet-10</td>
<td align="left">87.2 (87.0, 87.3)</td>
<td align="char" char=".">87.4</td>
<td align="left">84.3 (84.1, 84.4)</td>
<td align="char" char=".">84.9</td>
<td align="left">86.1 (85.9, 86.2)</td>
<td align="char" char=".">86.5</td>
</tr>
<tr>
<td align="left">VGG-20</td>
<td align="left">78.2 (78.1, 78.4)</td>
<td align="char" char=".">78.5</td>
<td align="left">74.1 (74.0, 74.2)</td>
<td align="char" char=".">74.4</td>
<td align="left">77.3 (77.2, 77.4)</td>
<td align="char" char=".">77.7</td>
</tr>
<tr>
<td align="left">ResNet-20</td>
<td align="left">76.7 (76.6, 76.9)</td>
<td align="char" char=".">76.9</td>
<td align="left"><bold>79.9 (79.8, 80.0)</bold></td>
<td align="char" char="."><bold>80.3</bold></td>
<td align="left">75.1 (75.0, 75.2)</td>
<td align="char" char=".">75.2</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec007">
<title>Experiments on KIMIA24</title>
<p>In the second experiment, we used VGG on the KIMIA24 dataset with the deep features extracted by VGG on GTEx; the task is the identification of the slide of origin (24 classes). In the DAPPER framework, classifiers were trained on 1, 060 annotated tiles and validated on 265 unseen ones.</p>
</sec>
<sec id="sec008">
<title>UMAP analysis</title>
<p>In order to perform an unsupervised exploration of the features extracted by the Feature Extractor module, we projected the deep features onto a bi-dimensional space by using the Uniform Manifold Approximation and Projection (UMAP) multidimensional projection method. This dimension reduction technique, which relies on topological descriptors, has proven competitive with state-of-the-art visualization algorithms such as t-SNE [<xref ref-type="bibr" rid="pcbi.1006269.ref057">57</xref>], preserving both global and local structure of the data [<xref ref-type="bibr" rid="pcbi.1006269.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref059">59</xref>]. We used the <italic>R</italic> <italic>umap</italic> package with following parameters: <monospace>n_neighbors</monospace> = 40, <monospace>min_dist</monospace> = 0.01, <monospace>n_components</monospace> = 2, and Euclidean <monospace>metric</monospace>.</p>
</sec>
<sec id="sec009">
<title>Implementation</title>
<p>All the code of the DAPPER framework is written in <italic>Python</italic> (<monospace>v3.6</monospace>) and <italic>R</italic> (<monospace>v3.4.4</monospace>). In addition to the general scientific libraries for Python, the scripts for the creation and training of the networks are based on <italic>PyTorch</italic>; the backend networks are implemented in <italic>torchvision</italic>. The library for processing histological images (available at <monospace>gitlab.fbk.eu/mpba-histology/histolib</monospace>) is based on <italic>OpenSlide</italic> and <italic>scikit-image</italic>.</p>
<p>The computations were performed on Microsoft Azure Virtual Machines with 4 NVIDIA K80 GPUs, 24 Intel Xeon E5-2690 cores and 256 GB RAM.</p>
</sec>
</sec>
<sec id="sec010" sec-type="results">
<title>Results</title>
<p>Results of the tissue classification tasks in the DAPPER framework are listed in <xref ref-type="table" rid="pcbi.1006269.t004">Table 4</xref> for Matthews Correlation Coefficient (MCC) and <xref ref-type="table" rid="pcbi.1006269.t005">Table 5</xref> for Accuracy (ACC), respectively. See also <xref ref-type="fig" rid="pcbi.1006269.g003">Fig 3</xref> for a comparison of MCC in internal cross validation with external validation.</p>
<fig id="pcbi.1006269.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison of DAPPER cross validation MCC (H-MCCt), vs MCC on external validation (H-MCCv) performance for each classifier.</title>
<p>(a) FCH; (b) SVM; (c) RF.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g003" xlink:type="simple"/>
</fig>
<p>All backend network-head pairs on HINT have MCC&gt; 0.7 with narrow CIs, with estimates from internal validation close to performance on the external validation set (<xref ref-type="fig" rid="pcbi.1006269.g003">Fig 3</xref>). Agreement of internal estimates with values on external validation set is a good indicator of generalization and potential for reproducibility. All models reached their top MCC accuracy with 1, 000 features. On HINT5 and HINT10, the FCH neural network performs better than SVMs and RF. As expected, MCC ranged close to 0 for random labels; random ranking for increasing feature set sizes reached top MCC only for all features (tested for SVMs).</p>
<p>The most accurate models both for internal and external validation estimates were the ResNet+FCH model with MCC = 0.883 on HINT5, the VGG+FCH model on HINT10, and the ResNet+SVM model on HINT20. In <xref ref-type="supplementary-material" rid="pcbi.1006269.s006">S6 Table</xref> we show the results with a lower number of dense layers in the FCH, which are comparable with the FCH with 4 dense layers. Results on HINT30 are detailed in <xref ref-type="supplementary-material" rid="pcbi.1006269.s005">S5 Table</xref>; on external validation set, the VGG model reaches accuracy ACC = 61.8% and MCC = 0.61. Performance decreases for more complex multiclass problems. Notably the difficulty of the task is also complicated by tissue classes that are likely to have similar histological patterns, such as misclassification of Esophagus-Muscularis (ACC: 72.1%) with Esophagus-Mucosa (ACC: 53.2%), or the two Heart tissue subtypes or the 58 Ovary(ACC: 68.3%) tiles predicted as Uterus (ACC: 72.8%). The full confusion matrix for ResNet with SVMs on HINT20 is reported in <xref ref-type="fig" rid="pcbi.1006269.g004">Fig 4</xref>. In this paper we establish a methodology to evaluate reproducibility and predictive accuracy of machine learning models, in particular of the model selection phase. This is obtained by moving from single training-test split procedure to an evaluation environment that uses data replicates and averaged statistical indicators, thus enabling to select a model on the basis of statistical indicators derived from the internal validation loop. In this framework, we can honestly evaluate model performance differences along a set of experiments on a group of tasks. The DAPPER framework cannot by itself identify the reason of such difference, and indeed the emergence of optimal architectures for a specific task may be due to different factors, as revealed by appropriate experimental design. In terms of the experimental design described in this paper, for any model type we expect and find a decrease in accuracy for increasing number of classes, which requires learning more decision surfaces with less data per class. Notably, the best model in the internal DAPPER validation is confirmed to be the best also on the unseen test data, with a value within the confidence interval or immediately close.</p>
<fig id="pcbi.1006269.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Confusion matrix for ResNet+SVM model on HINT20.</title>
<p>Red shaded cells indicate the most confused classes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g004" xlink:type="simple"/>
</fig>
<sec id="sec011">
<title>Results on KIMIA24</title>
<p>Regardless of difference in image types, VGG-KIMIA24 with both RF and SVM heads with ACC = 43.4% (see <xref ref-type="table" rid="pcbi.1006269.t006">Table 6</xref>), improving on published results (ACC = 41.8% [<xref ref-type="bibr" rid="pcbi.1006269.ref038">38</xref>]).</p>
<table-wrap id="pcbi.1006269.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t006</object-id>
<label>Table 6</label>
<caption>
<title>Performance of DAPPER framework for VGG backend network, and classifier heads (FCH, SVM, RF) on KIMIA24 dataset.</title>
<p>The average cross validation MCC (<bold>K24-MCCt</bold>), and ACC (<bold>K24-ACCt</bold>) with 95% CI, as well as MCC (<bold>K24-MCCv</bold>), and ACC (<bold>K24-ACCv</bold>) on external validation set are reported.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t006" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">K24-MCCt</th>
<th align="right">K24-MCCv</th>
<th align="center">K24-ACCt</th>
<th align="right">K24-ACCv</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">VGG+FCH</td>
<td align="left">0.317 (0.306, 0.327)</td>
<td align="char" char=".">0.207</td>
<td align="left">34.4 (33.2, 35.2)</td>
<td align="char" char=".">23.8</td>
</tr>
<tr>
<td align="left">VGG+SVM</td>
<td align="left">0.446 (0.439, 0.454)</td>
<td align="char" char=".">0.409</td>
<td align="left">47.1 (46.4, 47.8)</td>
<td align="char" char=".">43.4</td>
</tr>
<tr>
<td align="left">VGG+RF</td>
<td align="left"><bold>0.457</bold> (<bold>0.449</bold>, <bold>0.465</bold>)</td>
<td align="char" char="."><bold>0.409</bold></td>
<td align="left"><bold>48.0</bold> (<bold>47.3</bold>, <bold>48.8</bold>)</td>
<td align="char" char="."><bold>43.4</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>It is worth noting that transfer learning from ImageNet to HINT restricts training to the Adapter and Fully Connected Head blocks. In one-shot experiments, MCC further improves when the whole feature extraction block is retrained (see <xref ref-type="supplementary-material" rid="pcbi.1006269.s003">S3 Table</xref>). However, the result still needs to be consolidated by extending the DAP also to the training or retraining of the deep learning backend networks to check for actual generalization. The Canberra stability indicator was also computed for all the experiments, with minimal median stability for ResNet-20 (<xref ref-type="fig" rid="pcbi.1006269.g005">Fig 5</xref>).</p>
<fig id="pcbi.1006269.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Canberra stability indicator on HINT and KIMIA datasets.</title>
<p>For each architecture, a set of deep feature lists is generated, one list for each internal run of training in the nested cross validation schema, each ranked with KBest. Canberra stability is computed as in [<xref ref-type="bibr" rid="pcbi.1006269.ref032">32</xref>]: lower stability is better.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012">
<title>Results at WSI-level</title>
<p>We evaluated the performance of DAPPER at WSI-level on the HINT20 external validation set, with the ResNet+SVM model. In particular, all the predictions for the tiles are aggregated by WSI, and the resulting tissue will be the most common one among those predicted on the corresponding tiles. However, it is worth noting that the number of tiles per WSI in the HINT20 external validation set varies (min 1, max 31) due to a stratification strategy only considering the tissues-per-sample distribution (see Section <italic>Data Analysis Plan</italic>). Therefore, we restricted our evaluation to a subset of 15 WSI per class (300 WSI in total), each of which associated to 10 tiles randomly selected. This value represents a reasonable number of Regions of Interest (ROIs) a human pathologist would likely consider in his/her evaluations. In this regard, we further investigate how the DAPPER framework performs on an increasing number of tiles per WSI, namely 3, 5, 7, and 10. As expected, the overall accuracy improves as the number of tiles per WSI increases, reaching 98.3% when considering all 10 tiles per WSI. Notably, the accuracy is high even when reducing to 3 tiles per WSI (see <xref ref-type="table" rid="pcbi.1006269.t007">Table 7</xref>).</p>
<table-wrap id="pcbi.1006269.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t007</object-id>
<label>Table 7</label>
<caption>
<title>Metrics at WSI-level for increasing number of tiles per WSI.</title>
<p>Metrics are computed on a subset of HINT20 external validation set, consisting of 15 WSI per class (300 WSI in total). The WSI class is determined by the most frequent predicted class by the <italic>ResNet+SVM</italic> model for the considered tiles.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t007" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="right"># Tiles per WSI</th>
<th align="right">MCC</th>
<th align="right">ACC (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">3</td>
<td align="char" char=".">0.86</td>
<td align="char" char=".">86.3</td>
</tr>
<tr>
<td align="right">5</td>
<td align="char" char=".">0.93</td>
<td align="char" char=".">93.7</td>
</tr>
<tr>
<td align="right">7</td>
<td align="char" char=".">0.96</td>
<td align="char" char=".">96.0</td>
</tr>
<tr>
<td align="right">10</td>
<td align="char" char=".">0.98</td>
<td align="char" char=".">98.3</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec013">
<title>Comparison with pathologist</title>
<p>We tested the performance of DAPPER against an expert pathologist on about 25% of the HINT20 external validation set, 2, 000 tiles out of 8, 103, with 100 randomly selected tiles for each class. We asked the pathologist to classify each tile by choosing among the 20 classes of the HINT20 dataset, without imposing any time constraint. The confusion matrix resulting from the evaluation of tiles as produced by the pathologist is shown in <xref ref-type="fig" rid="pcbi.1006269.g006">Fig 6</xref>. Predictions produced by the DAPPER framework for comparative results are then collected on the same data. The best-performing model on the HINT20 dataset, namely the ResNet+SVM model, has been considered for this experiment. As reported in <xref ref-type="table" rid="pcbi.1006269.t008">Table 8</xref>, DAPPER outperforms the pathologist in the prediction of tissues at a tile-level.</p>
<fig id="pcbi.1006269.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Confusion matrix for pathologist classification on a subset of HINT20 external validation set.</title>
<p>Red shaded cells indicate the most confused classes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g006" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1006269.t008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t008</object-id>
<label>Table 8</label>
<caption>
<title>Tissue classification performance of DAPPER vs pathologist.</title>
<p>DAPPER with <italic>ResNet+SVM</italic> model outperforms the pathologist at tile-level. Metrics are computed on a subset of HINT20 external validation set (2, 000 tiles).</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t008g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t008" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Classifier</th>
<th align="right">MCC</th>
<th align="right">ACC (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Pathologist</td>
<td align="char" char=".">0.542</td>
<td align="char" char=".">56.3</td>
</tr>
<tr>
<td align="left">DAPPER</td>
<td align="char" char=".">0.786</td>
<td align="char" char=".">79.6</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To provide an unbiased estimation of the performance of DAPPER, we repeated the same evaluation on 10 other randomly generated subsets of 2, 000 tiles extracted from the HINT20 external validation set. The obtained average MCC and ACC with 95% CI are 0.786 (0.783, 0.789), and 79.6 (79.3, 79.9), respectively.</p>
<p>Finally, since the classification at tile-level is an unusual task for a pathologist, who is instead trained on examining the whole context of a tissue scan, as a second task we asked the pathologist to classify 200 randomly chosen WSIs (10 for each class of HINT20). As expected, the results in this case are better than those at tile-level, <italic>i.e</italic>., MCC = 0.788, and ACC = 79.5%, to be compared with the DAPPER performances reported in <xref ref-type="table" rid="pcbi.1006269.t007">Table 7</xref>.</p>
</sec>
<sec id="sec014">
<title>The HINT benchmark dataset</title>
<p>As a second contribution of this study, we are making available the HINT dataset, generated by the first component of tools in the DAPPER framework, as a benchmark dataset for validating machine learning models in digital pathology. The HINT dataset is currently composed of 53, 727 tiles at 512 × 512 resolution, based on histology from GTEx. HINT can be easily expanded to over 78, 000 tiles, as for this study we used a fraction of the GTEx images and at most 100 tiles from each WSI were extracted. Digital pathology still misses a universally adopted dataset to compare deep learning models as already established in vision (<italic>e.g</italic>., ImageNet for image classification, COCO for image and instance segmentation). Several initiatives for a “BioImageNet” will eventually improve this scenario. Histology data are available in the generalist repository Image Data Resource (IDR) [<xref ref-type="bibr" rid="pcbi.1006269.ref060">60</xref>, <xref ref-type="bibr" rid="pcbi.1006269.ref061">61</xref>]. Further, the International Immuno-Oncology Biomarker Working Group in Breast Cancer and the MAQC Society have launched a collaborative project to develop data resources and quality control schemes on Machine Learning algorithms to assess TILs in Breast Cancer.</p>
<p>HINT is conceptually similar to KIMIA24. However, HINT inherits from GTEx more variability in terms of sample characteristics, validation of donors and additional access to molecular data. Further, we used a random sampling approach to process tiles excluding background and minimize human intervention in the choice and preparation of the images.</p>
</sec>
<sec id="sec015">
<title>Deep features</title>
<p>We applied an unsupervised projection on all the features extracted by VGG and ResNet networks on all tissues tasks. In the following, we discuss an example for features extracted by VGG on the HINT20 task, displayed as UMAP projection (<xref ref-type="fig" rid="pcbi.1006269.g007">Fig 7</xref>), points are coloured for 20 tissue labels. The UMAP displays for the other tasks are available in <xref ref-type="supplementary-material" rid="pcbi.1006269.s007">S1</xref>–<xref ref-type="supplementary-material" rid="pcbi.1006269.s010">S4</xref> Figs. The UMAP display is in agreement with the count distributions in the confusion matrix (<xref ref-type="fig" rid="pcbi.1006269.g004">Fig 4</xref>). The deep learning embedding separates well a set of histology types, including Muscle-Skeletal, Spleen, Pancreas, Brain-Cortex and Cerebellum, Heart-Left Ventricle and Atrial Appendage which group into distinct clusters (See <xref ref-type="fig" rid="pcbi.1006269.g007">Fig 7</xref> and <xref ref-type="table" rid="pcbi.1006269.t009">Table 9</xref>). The distributions of the activations for the top-3 deep features of the VGG backend network on the HINT10 dataset are displayed in <xref ref-type="supplementary-material" rid="pcbi.1006269.s011">S5 Fig</xref>; the top ranked deep feature (<monospace>#668</monospace>) is clearly selective for Spleen. The UMAP projection also shows an overlapping for tissues such as Ovary and Uterus, or Vagina and Esophagus-Mucosa, or the two Esophagus histotypes, consistently with the confusion matrix (<xref ref-type="fig" rid="pcbi.1006269.g004">Fig 4</xref>).</p>
<fig id="pcbi.1006269.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g007</object-id>
<label>Fig 7</label>
<caption>
<title>UMAP projection of external validation set for VGG-20 experiment.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g007" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1006269.t009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.t009</object-id>
<label>Table 9</label>
<caption>
<title>Histology types well separated by SVM+ResNet model for HINT20.</title>
<p>Accuracy is computed with respect to the confusion matrix in <xref ref-type="fig" rid="pcbi.1006269.g004">Fig 4</xref> and expressed in percentage, together with the total number of samples for each class.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006269.t009g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.t009" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Histology type</th>
<th align="right">ACC(%)</th>
<th align="right">#samples</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Spleen</td>
<td align="char" char=".">94.6</td>
<td align="right">446</td>
</tr>
<tr>
<td align="left">Brain—Cortex</td>
<td align="char" char=".">94.3</td>
<td align="right">333</td>
</tr>
<tr>
<td align="left">Muscle—Skeletal</td>
<td align="char" char=".">93.4</td>
<td align="right">347</td>
</tr>
<tr>
<td align="left">Brain—Cerebellum</td>
<td align="char" char=".">93.4</td>
<td align="right">376</td>
</tr>
<tr>
<td align="left">Heart—Left Ventricle</td>
<td align="char" char=".">90.1</td>
<td align="right">565</td>
</tr>
<tr>
<td align="left">Pancreas</td>
<td align="char" char=".">87.9</td>
<td align="right">463</td>
</tr>
<tr>
<td align="left">Heart—Atrial Appendage</td>
<td align="char" char=".">84.7</td>
<td align="right">317</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Examples of five tiles from two well separated clusters, Muscle-Skeletal (ACC: 93.4%) and Spleen (ACC: 94.6%), are displayed in panel A of <xref ref-type="fig" rid="pcbi.1006269.g008">Fig 8</xref>. Tiles from three clusters partially overlapping in the neural embedding and mislabeled in both the VGG-20 and ResNet-20 embeddings with SVM (Esophagus- Mucosa ACC = 53.2%, Esophagus-Muscularis ACC = 72.1%, Vagina ACC = 59.0%) are similarly visualized in <xref ref-type="fig" rid="pcbi.1006269.g008">Fig 8B</xref>. While the aim of this paper is to introduce a framework for honest comparison of models that will be used for clinical purposes rather than fine-tuning accuracy in this experiment, it is evident that these tiles have morphologies that are hard to classify. This challenge requires more complex models (<italic>e.g</italic>. ensembles) and a structured output labeling, already applied in dermatology [<xref ref-type="bibr" rid="pcbi.1006269.ref002">2</xref>]. Further, we are exploring the combination of DAPPER with image analysis packages, such as HistomicsTK (<ext-link ext-link-type="uri" xlink:href="https://digitalslidearchive.github.io/HistomicsTK/" xlink:type="simple">https://digitalslidearchive.github.io/HistomicsTK/</ext-link>) or CellProfiler [<xref ref-type="bibr" rid="pcbi.1006269.ref062">62</xref>], to extract features useful for interpretation and feedback from pathologists.</p>
<fig id="pcbi.1006269.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006269.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Representative tiles predicted from VGG-20 experiment.</title>
<p>A) Examples from two well-separated clusters observed in the UMAP embedding. B) Samples of mislabeled tiles from tissues partially overlapping in the UMAP embedding.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.g008" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec016" sec-type="conclusions">
<title>Discussion</title>
<p>Digital pathology would greatly benefit from the adoption of machine learning, shifting human assessment of histology to higher quality, non-repetitive tasks. Unfortunately, there is no fast, easy route to improve reproducibility of automated analysis. The adoption of the DAP clearly sets in a computational aggravation not usually considered for image processing exercises. However, this is an established practice with massive omics data [<xref ref-type="bibr" rid="pcbi.1006269.ref028">28</xref>], and reproducibility by design can handle secondary results useful for diagnostics and for interpretation.</p>
<p>We designed the DAPPER framework as a tool for evaluating accuracy and stability of deep learning models, currently only backend elements in a sequence of processing steps, and possibly in the future end-to-end solutions. We choose as test domain H&amp;E stained WSIs for prediction of tissue of origin, which is not a primary task for trained pathologists, but a reasonable benchmark for machine learning methods. Also, we are aware that tissue classification is only a step in real digital pathology applications. Mobadersany and colleagues [<xref ref-type="bibr" rid="pcbi.1006269.ref011">11</xref>] used a deep learning classifier to score and visualize risk on the WSIs. Similarly, deep learning tile classification may be applied to quantify histological differences in association to a genomic pattern, <italic>e.g</italic>., a specific mutation or a high-dimensional protein expression signature. In this vision, the attention to model selection supported by our framework is a prerequisite for developing novel AI algorithms for digital pathology, <italic>e.g</italic>., for analytics over TILs.</p>
<p>Although we are building on deep learning architectures known for applications on generic images, they adapted well to WSIs in combination with established machine learning models (SVM, RF); we expect that large scale bioimaging resources will give the chance of improving the characterization of deep features, as already emerged with the HINT dataset that we are providing as public resource. In this direction, we plan to release the network weights of the backend DAPPER models that are optimized for histopathology as alternative pretrained weights for digital pathology, similarly to those for the ImageNet dataset and available in <monospace>torchvision</monospace>.</p>
</sec>
<sec id="sec017">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006269.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Summary of available samples, downloaded WSIs and extracted tiles.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s002" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Summary of the datasets.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s003" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Impact of retraining the backend network.</title>
<p>Accuracy and Matthews Correlation Coefficient improve when retraining also the feature extraction block (VGG backend network, not in DAP). We observe an improvement of the accuracy from 5.5% to 24.8% for the four chosen experiments. Possibly the neural network benefits from adjusting also the initial weights because the layers learn characteristics of the images diverse from the ImageNet dataset.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s004" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Comparison of the three optimization methods to set the learning rate.</title>
<p>The best method for setting the learning rate was assessed using the VGG as backend network on the 5 tissues dataset HINT5. Three methods were tested: Fixed (FIX): the learning rate is set to 10<sup>−5</sup> for the whole training; Step-wise (STEP): the learning rate is initialized at λ<sub>init</sub> = 10<sup>−3</sup> and updated every 10 epochs with the following rule: λ<sub>new</sub> = λ<sub>old</sub>/10; Polynomial (POLY): the learning rate is initialized at 10<sup>−3</sup> and updated every 10 iterations with a polynomial law: <inline-formula id="pcbi.1006269.e003"><alternatives><graphic id="pcbi.1006269.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006269.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mo>λ</mml:mo> <mml:mtext>new</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mtext>init</mml:mtext></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>i</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mtext>max</mml:mtext></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>i</italic> is the index of the iteration and <italic>I</italic><sub><italic>max</italic></sub> is the total number of iterations.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s005" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Impact of task complexity (VGG backend network).</title>
<p>Performance decreases when the number of tissues increases. Adding more classes to the task is possibly complicated by the introduction of tissues with similar histological patterns.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s006" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Impact (MCC) of number of internal layers on FCH (&lt; 4 dense layers) on HINT dataset.</title>
<p>FCH3: three dense layers with 1000, 256 and # tissue classes nodes, respectively; FCH2: two dense layers with 256 and # tissue classes nodes, respectively. The average cross validation MCC with 95% CI (H-MCCt), and MCC on the external validation set (H-MCCv) are reported. In bold: MCC (bold) values of <xref ref-type="table" rid="pcbi.1006269.t004">Table 4</xref> of the main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s007" mimetype="image/png" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s007" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>UMAP projection on training (circles) and external validation (crosses) set for VGG-5 experiment.</title>
<p>(PNG)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s008" mimetype="image/png" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s008" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>UMAP projection on training (circles) and external validation (crosses) set for ResNet-5 experiment.</title>
<p>(PNG)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s009" mimetype="image/png" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s009" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>UMAP projection on training (circles) and external validation (crosses) set for VGG-10 experiment.</title>
<p>(PNG)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s010" mimetype="image/png" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s010" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>UMAP projection on training (circles) and external validation (crosses) set for ResNet-10 experiment.</title>
<p>(PNG)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006269.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006269.s011" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Deep features and tissue of origin.</title>
<p>Distributions of the values of the top-3 deep features computed with the VGG backend architecture for the 10 classes of the HINT10 dataset.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We derived the HINT dataset from the Genotype-Tissue Expression (GTEx) Project, supported by the Common Fund of the Office of the Director of the National Institutes of Health, and by NCI, NHGRI, NHLBI, NIDA, NIMH, and NINDS (data downloaded from the GTEx Portal on <italic>05/10/18</italic>).</p>
<p>The authors thank Intel Italy for technical support and availability of high performance computing resources. We also thank H. Tizhoosh for availability of KIMIA Path24 dataset, L. Coviello for his help in the networks’ optimization and G. Franch for the realization of the striking image.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006269.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zheng</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Carneiro</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>L</given-names></name>. <source>Deep Learning and Convolutional Neural Networks for Medical Image Computing</source>. <publisher-name>Springer</publisher-name>; <year>2017</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Esteva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kuprel</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Novoa</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Swetter</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Blau</surname> <given-names>HM</given-names></name>, <etal>et al</etal>. <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>. <source>Nature</source>. <year>2017</year>;<volume>542</volume>(<issue>7639</issue>):<fpage>115</fpage>–<lpage>118</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature21056" xlink:type="simple">10.1038/nature21056</ext-link></comment> <object-id pub-id-type="pmid">28117445</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Komeda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Handa</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Nomura</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kitahashi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Computer-Aided Diagnosis Based on Convolutional Neural Network System for Colorectal Polyp Classification: Preliminary Experience</article-title>. <source>Oncology</source>. <year>2017</year>;<volume>93</volume>(<issue>Suppl. 1</issue>):<fpage>30</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000481227" xlink:type="simple">10.1159/000481227</ext-link></comment> <object-id pub-id-type="pmid">29258081</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Korbar</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Olofson</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Miraflor</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Nicka</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Suriawinata</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Torresani</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Deep Learning for Classification of Colorectal Polyps on Whole-Slide Images</article-title>. <source>Journal of Pathology Informatics</source>. <year>2017</year>;<volume>8</volume>:<fpage>30</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.4103/jpi.jpi_34_17" xlink:type="simple">10.4103/jpi.jpi_34_17</ext-link></comment> <object-id pub-id-type="pmid">28828201</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>De Fauw</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ledsam</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Romera-Paredes</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Nikolov</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tomasev</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Blackwell</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Clinically applicable deep learning for diagnosis and referral in retinal disease</article-title>. <source>Nature Medicine</source>. <year>2018</year>;<volume>24</volume>(<issue>9</issue>):<fpage>1342</fpage>–<lpage>1350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41591-018-0107-6" xlink:type="simple">10.1038/s41591-018-0107-6</ext-link></comment> <object-id pub-id-type="pmid">30104768</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ciompi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Chung</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Van Riel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Setio</surname> <given-names>AAA</given-names></name>, <name name-style="western"><surname>Gerke</surname> <given-names>PK</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Towards automatic pulmonary nodule management in lung cancer screening with deep learning</article-title>. <source>Scientific Reports</source>. <year>2017</year>;<volume>7</volume>:<fpage>46479</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep46479" xlink:type="simple">10.1038/srep46479</ext-link></comment> <object-id pub-id-type="pmid">28422152</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Litjens</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kooi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bejnordi</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Setio</surname> <given-names>AAA</given-names></name>, <name name-style="western"><surname>Ciompi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ghafoorian</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Medical Image Analysis</source>. <year>2017</year>;<volume>42</volume>:<fpage>60</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.media.2017.07.005" xlink:type="simple">10.1016/j.media.2017.07.005</ext-link></comment> <object-id pub-id-type="pmid">28778026</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keane</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Topol</surname> <given-names>EJ</given-names></name>. <article-title>With an eye to AI and autonomous diagnosis</article-title>. <source>NPJ Digital Medicine</source>. <year>2018</year>;<volume>1</volume>(<issue>1</issue>):<fpage>40</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41746-018-0048-y" xlink:type="simple">10.1038/s41746-018-0048-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maier-Hein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Eisenmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Reinke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Onogur</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stankovic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Scholz</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Why rankings of biomedical image analysis competitions should be interpreted with care</article-title>. <source>Nature Communications</source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>5217</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-018-07619-7" xlink:type="simple">10.1038/s41467-018-07619-7</ext-link></comment> <object-id pub-id-type="pmid">30523263</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maier-Hein</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Eisenmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Reinke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Onogur</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stankovic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Scholz</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Author Correction: Why rankings of biomedical image analysis competitions should be interpreted with care</article-title>. <source>Nature Communications</source>. <year>2019</year>;<volume>10</volume>(<issue>1</issue>):<fpage>588</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-019-08563-w" xlink:type="simple">10.1038/s41467-019-08563-w</ext-link></comment> <object-id pub-id-type="pmid">30700735</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mobadersany</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Yousefi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Amgad</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gutman</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Barnholtz-Sloan</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Velázquez Vega</surname> <given-names>JE</given-names></name>, <etal>et al</etal>. <article-title>Predicting cancer outcomes from histology and genomics using convolutional networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2018</year>;<volume>115</volume>(<issue>13</issue>):<fpage>E2970</fpage>–<lpage>E2979</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1717139115" xlink:type="simple">10.1073/pnas.1717139115</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bychkov</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Linder</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Turkki</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nordling</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kovanen</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Verrill</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Deep learning based tissue analysis predicts outcome in colorectal cancer</article-title>. <source>Scientific Reports</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>3395</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-018-21758-3" xlink:type="simple">10.1038/s41598-018-21758-3</ext-link></comment> <object-id pub-id-type="pmid">29467373</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sharma</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zerbe</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Klempert</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hellwich</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Hufnagl</surname> <given-names>P</given-names></name>. <article-title>Deep convolutional neural networks for automatic classification of gastric carcinoma using whole slide images in digital histopathology</article-title>. <source>Computerized Medical Imaging and Graphics</source>. <year>2017</year>;<volume>61</volume>:<fpage>2</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.compmedimag.2017.06.001" xlink:type="simple">10.1016/j.compmedimag.2017.06.001</ext-link></comment> <object-id pub-id-type="pmid">28676295</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Paeng K, Hwang S, Park S, Kim M. A unified framework for tumor proliferation score prediction in breast histopathology. In: Proceedings of the Third International Workshop on Deep Learning in Medical Image Analysis (DLMIA 2017) and the Sixth International Workshop on Multimodal Learning for Clinical Decision Support (ML-CDS 2017), held in conjunction with the Twentieth International Conference on Medical Imaging and Computer-Assisted Intervention (MICCAI 2017). Springer; 2017. p. 231–239.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coudray</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ocampo</surname> <given-names>PS</given-names></name>, <name name-style="western"><surname>Sakellaropoulos</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Narula</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Snuderl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fenyö</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Classification And Mutation Prediction From Non-Small Cell Lung Cancer Histopathology Images Using Deep Learning</article-title>. <source>Nature Medicine</source>. <year>2018</year>;<volume>24</volume>(<issue>10</issue>):<fpage>1559</fpage>–<lpage>1567</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41591-018-0177-5" xlink:type="simple">10.1038/s41591-018-0177-5</ext-link></comment> <object-id pub-id-type="pmid">30224757</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coudray</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Moreira</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Sakellaropoulos</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fenyö</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Razavian</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tsirigos</surname> <given-names>A</given-names></name>. <article-title>Determining EGFR and STK11 mutational status in lung adenocarcinoma histopathology images using deep learning</article-title>. <source>Cancer Research</source>. <year>2018</year>;<volume>78</volume>(<issue>Supp.13</issue>):<fpage>5309</fpage>–<lpage>5309</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1158/1538-7445.AM2018-5309" xlink:type="simple">10.1158/1538-7445.AM2018-5309</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Basavanhally</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ganesan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shih</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mies</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tomaszewski</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Multi-field-of-view framework for distinguishing tumor grade in ER+ breast cancer from entire histopathology slides</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2013</year>;<volume>60</volume>:<fpage>2089</fpage>–<lpage>2099</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TBME.2013.2245129" xlink:type="simple">10.1109/TBME.2013.2245129</ext-link></comment> <object-id pub-id-type="pmid">23392336</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Denkert</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wienert</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Poterie</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Loibl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Budczies</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Badve</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Standardized evaluation of tumor-infiltrating lymphocytes in breast cancer: results of the ring studies of the international immuno-oncology biomarker working group</article-title>. <source>Modern Pathology</source>. <year>2016</year>;<volume>29</volume>(<issue>10</issue>):<fpage>1155</fpage>–<lpage>1164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/modpathol.2016.109" xlink:type="simple">10.1038/modpathol.2016.109</ext-link></comment> <object-id pub-id-type="pmid">27363491</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mina</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boldrini</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Citti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Romania</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>D’Alicandro</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>De Ioris</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Tumor-infiltrating T lymphocytes improve clinical outcome of therapy-resistant neuroblastoma</article-title>. <source>Oncoimmunology</source>. <year>2015</year>;<volume>4</volume>(<issue>9</issue>):<fpage>e1019981</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/2162402X.2015.1019981" xlink:type="simple">10.1080/2162402X.2015.1019981</ext-link></comment> <object-id pub-id-type="pmid">26405592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salgado</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sherene</surname> <given-names>L</given-names></name>. <article-title>Tumour infiltrating lymphocytes in breast cancer: increasing clinical relevance</article-title>. <source>The Lancet Oncology</source>. <year>2018</year>;<volume>19</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1470-2045(17)30905-1" xlink:type="simple">10.1016/S1470-2045(17)30905-1</ext-link></comment> <object-id pub-id-type="pmid">29233560</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stovgaard</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>Nielsen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hogdall</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Balslev</surname> <given-names>E</given-names></name>. <article-title>Triple negative breast cancer–prognostic role of immune-related factors: a systematic review</article-title>. <source>Acta Oncologica</source>. <year>2018</year>;<volume>57</volume>(<issue>1</issue>):<fpage>74</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/0284186X.2017.1400180" xlink:type="simple">10.1080/0284186X.2017.1400180</ext-link></comment> <object-id pub-id-type="pmid">29168430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shibutani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maeda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nagahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fukuoka</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Iseki</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Matsutani</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Tumor-infiltrating Lymphocytes Predict the Chemotherapeutic Outcomes in Patients with Stage IV Colorectal Cancer</article-title>. <source>In Vivo</source>. <year>2018</year>;<volume>32</volume>(<issue>1</issue>):<fpage>151</fpage>–<lpage>158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.21873/invivo.11218" xlink:type="simple">10.21873/invivo.11218</ext-link></comment> <object-id pub-id-type="pmid">29275313</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li LJ, Li K, Li FF. Imagenet: A large-scale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; 2009. p. 248–255.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tajbakhsh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Shin</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Gurudu</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Hurst</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>Kendall</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>Gotway</surname> <given-names>MB</given-names></name>, <etal>et al</etal>. <article-title>Convolutional neural networks for medical image analysis: Full training or fine tuning?</article-title> <source>IEEE Transactions on Medical Imaging</source>. <year>2016</year>;<volume>35</volume>(<issue>5</issue>):<fpage>1299</fpage>–<lpage>1312</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TMI.2016.2535302" xlink:type="simple">10.1109/TMI.2016.2535302</ext-link></comment> <object-id pub-id-type="pmid">26978662</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Kieffer B, Babaie M, Kalra S, Tizhoosh HR. Convolutional Neural Networks for Histopathology Image Classification: Training vs. Using Pre-Trained Networks. In: Proceedings of the Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA 2017). IEEE; 2017. p. 1–6.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ioannidis</surname> <given-names>JPA</given-names></name>, <name name-style="western"><surname>Allison</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Ball</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Coulibaly</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Culhane</surname> <given-names>AC</given-names></name>, <etal>et al</etal>. <article-title>Repeatability of published microarray gene expression analyses</article-title>. <source>Nature Genetics</source>. <year>2009</year>;<volume>41</volume>(<issue>2</issue>):<fpage>149</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ng.295" xlink:type="simple">10.1038/ng.295</ext-link></comment> <object-id pub-id-type="pmid">19174838</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baker</surname> <given-names>M</given-names></name>. <article-title>1,500 scientists lift the lid on reproducibility</article-title>. <source>Nature News</source>. <year>2016</year>;<volume>533</volume>(<issue>7604</issue>):<fpage>452</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/533452a" xlink:type="simple">10.1038/533452a</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kusko</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wolfinger</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Haibe-Kains</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sansone</surname> <given-names>SA</given-names></name>, <etal>et al</etal>. <article-title>The international MAQC Society launches to enhance reproducibility of high-throughput technologies</article-title>. <source>Nature Biotechnology</source>. <year>2017</year>;<volume>35</volume>(<issue>12</issue>):<fpage>1127</fpage>–<lpage>1128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.4029" xlink:type="simple">10.1038/nbt.4029</ext-link></comment> <object-id pub-id-type="pmid">29220036</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilkinson</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Dumontier</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Aalbersberg</surname> <given-names>IJ</given-names></name>, <name name-style="western"><surname>Appleton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Axton</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Baak</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>. <source>Scientific Data</source>. <year>2016</year>;<volume>3</volume>:<fpage>160018</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/sdata.2016.18" xlink:type="simple">10.1038/sdata.2016.18</ext-link></comment> <object-id pub-id-type="pmid">26978244</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<collab>The MAQC Consortium</collab>. <article-title>The MAQC-II Project: A comprehensive study of common practices for the development and validation of microarray-based predictive models</article-title>. <source>Nature Biotechnology</source>. <year>2010</year>;<volume>28</volume>(<issue>8</issue>):<fpage>827</fpage>–<lpage>838</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.1665" xlink:type="simple">10.1038/nbt.1665</ext-link></comment> <object-id pub-id-type="pmid">20676074</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<collab>The SEQC/MAQC-III Consortium</collab>. <article-title>A comprehensive assessment of RNA-seq accuracy, reproducibility and information content by the Sequence Quality Control consortium</article-title>. <source>Nature Biotechnology</source>. <year>2014</year>;<volume>32</volume>:<fpage>903</fpage>–<lpage>914</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.2957" xlink:type="simple">10.1038/nbt.2957</ext-link></comment> <object-id pub-id-type="pmid">25150838</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jurman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Merler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Barla</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Paoli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Galea</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Furlanello</surname> <given-names>C</given-names></name>. <article-title>Algebraic stability indicators for ranked lists in molecular profiling</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>2</issue>):<fpage>258</fpage>–<lpage>264</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/btm550" xlink:type="simple">10.1093/bioinformatics/btm550</ext-link></comment> <object-id pub-id-type="pmid">18024475</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Furlanello</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Serafini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Merler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jurman</surname> <given-names>G</given-names></name>. <article-title>Entropy-based gene ranking without selection bias for the predictive classification of microarray data</article-title>. <source>BMC Bioinformatics</source>. <year>2003</year>;<volume>4</volume>(<issue>1</issue>):<fpage>54</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1471-2105-4-54" xlink:type="simple">10.1186/1471-2105-4-54</ext-link></comment> <object-id pub-id-type="pmid">14604446</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Lin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D, et al. Microsoft COCO: Common Objects in Context. In: Proceedings of the Thirteenth European Conference on Computer Vision (ECCV 2014). Springer; 2014. p. 740–755.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<collab>The GTEx Consortium</collab>. <article-title>The genotype-tissue expression (GTEx) project</article-title>. <source>Nature Genetics</source>. <year>2013</year>;<volume>45</volume>(<issue>6</issue>):<fpage>580</fpage>–<lpage>585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ng.2653" xlink:type="simple">10.1038/ng.2653</ext-link></comment> <object-id pub-id-type="pmid">23715323</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cortes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vapnik</surname> <given-names>V</given-names></name>. <article-title>Support-vector networks</article-title>. <source>Machine Learning</source>. <year>1995</year>;<volume>20</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00994018" xlink:type="simple">10.1007/BF00994018</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Ho TK. Random decision forests. In: Proceedings of 3rd International Conference on Document Analysis and Recognition (ICDAR 1995). IEEE; 1995. p. 278–282).</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Babaie M, Kalra S, Sriram A, Mitcheltree C, Zhu S, Khatami A, et al. Classification and Retrieval of Digital Pathology Scans: A New Dataset. In: Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE; 2017. p. 8–16.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref039">
<label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Kumar MD, Babaie M, Zhu S, Kalra S, Tizhoosh HR. A Comparative Study of CNN, BoVW and LBP for Classification of Histopathological Images. In: Proceedings of the 2017 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE; 2017. p. 1–7.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Alhindi TJ, Kalra S, Ng KH, Afrin A, Tizhoosh HR. Comparing LBP, HOG and Deep Features for Classification of Histopathology Images. In: Proceedings of the 2018 International Joint Conference on Neural Networks (IJCNN). IEEE; 2018. p. 1–7.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carithers</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Ardlie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Barcus</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Branton</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Britton</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Buia</surname> <given-names>SA</given-names></name>, <etal>et al</etal>. <article-title>A Novel Approach to High-Quality Postmortem Tissue Procurement: The GTEx Project</article-title>. <source>Biopreservation and Biobanking</source>. <year>2015</year>;<volume>13</volume>(<issue>5</issue>):<fpage>311</fpage>–<lpage>319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1089/bio.2015.0032" xlink:type="simple">10.1089/bio.2015.0032</ext-link></comment> <object-id pub-id-type="pmid">26484571</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Nader Vasconcelos C, Nader Vasconcelos B. Increasing deep learning melanoma classification by classical and expert knowledge based image transforms. arXiv. 2017;1702.07025.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Cubuk ED, Zoph B, Mane D, Vasudevan V, Le QV. AutoAugment: Learning Augmentation Policies from Data. arXiv. 2018;1805.09501.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Wang J, Perez L. The effectiveness of data augmentation in image classification using deep learning. arXiv. 2017;1712.04621.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: Proceedings of the Third International Conference on Learning Representations (ICLR 2015). arXiv:1409.1556; 2015. p. 1–14.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref046">
<label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; 2016. p. 770–778.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the Inception Architecture for Computer Vision. In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE; 2016. p. 2818–2826.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Canziani A, Paszke A, Culurciello E. An analysis of deep neural network models for practical applications. arXiv. 2017;1605.076784:1–7.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref049">
<label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improving neural networks by preventing co-adaptation of feature detectors. arXiv. 2012;1207.0580:1–18.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Kinga D, Adam JB. Adam: A Method for Stochastic Optimization. In: Proceedings of the Third International Conference on Learning Representations (ICLR 2015). arXiv:1412.6980; 2014. p. 1–15.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref051">
<label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>. <source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source>. <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Matthews</surname> <given-names>BW</given-names></name>. <article-title>Comparison of the predicted and observed secondary structure of T4 phage lysozyme</article-title>. <source>Biochimica et Biophysica Acta</source>. <year>1975</year>;<volume>405</volume>(<issue>2</issue>):<fpage>442</fpage>–<lpage>451</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0005-2795(75)90109-9" xlink:type="simple">10.1016/0005-2795(75)90109-9</ext-link></comment> <object-id pub-id-type="pmid">1180967</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baldi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brunak</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chauvin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>CAF</given-names></name>, <name name-style="western"><surname>Nielsen</surname> <given-names>H</given-names></name>. <article-title>Assessing the accuracy of prediction algorithms for classification: an overview</article-title>. <source>Bioinformatics</source>. <year>2000</year>;<volume>16</volume>(<issue>5</issue>):<fpage>412</fpage>–<lpage>424</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/bioinformatics/16.5.412" xlink:type="simple">10.1093/bioinformatics/16.5.412</ext-link></comment> <object-id pub-id-type="pmid">10871264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jurman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Riccadonna</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Furlanello</surname> <given-names>C</given-names></name>. <article-title>A comparison of MCC and CEN error measures in multi-class prediction</article-title>. <source>PLOS ONE</source>. <year>2012</year>;<volume>7</volume>(<issue>8</issue>):<fpage>e41882</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0041882" xlink:type="simple">10.1371/journal.pone.0041882</ext-link></comment> <object-id pub-id-type="pmid">22905111</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chicco</surname> <given-names>D</given-names></name>. <article-title>Ten quick tips for machine learning in computational biology</article-title>. <source>BioData Mining</source>. <year>2017</year>;<volume>10</volume>:<fpage>35</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13040-017-0155-3" xlink:type="simple">10.1186/s13040-017-0155-3</ext-link></comment> <object-id pub-id-type="pmid">29234465</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Di Ciccio</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Efron</surname> <given-names>B</given-names></name>. <article-title>Bootstrap confidence intervals (with Discussion)</article-title>. <source>Statistical Science</source>. <year>1996</year>;<volume>11</volume>:<fpage>189</fpage>–<lpage>228</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/ss/1032280214" xlink:type="simple">10.1214/ss/1032280214</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Van der Maaten</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source>. <year>2008</year>;<volume>9</volume>:<fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006269.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McInnes</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Healy</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Saul</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Großberger</surname> <given-names>L</given-names></name>. <article-title>UMAP: Uniform Manifold Approximation and Projection</article-title>. <source>Journal of Open Source Software</source>. <year>2018</year>;<volume>3</volume>(<issue>29</issue>):<fpage>861</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.21105/joss.00861" xlink:type="simple">10.21105/joss.00861</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Becht</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>McInnes</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Healy</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dutertre</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Kwok</surname> <given-names>IWH</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>LG</given-names></name>, <etal>et al</etal>. <article-title>Dimensionality reduction for visualizing single-cell data using UMAP</article-title>. <source>Nature Biotechnology</source>. <year>2018</year>;Online:2018/12/03. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nbt.4314" xlink:type="simple">10.1038/nbt.4314</ext-link></comment> <object-id pub-id-type="pmid">30531897</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Williams</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>SW</given-names></name>, <name name-style="western"><surname>Rustici</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tarkowska</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chessel</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Image Data Resource: a bioimage data integration and publication platform</article-title>. <source>Nature Methods</source>. <year>2017</year>;<volume>14</volume>(<issue>8</issue>):<fpage>775</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.4326" xlink:type="simple">10.1038/nmeth.4326</ext-link></comment> <object-id pub-id-type="pmid">28775673</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nirschl</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Janowczyk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peyster</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Margulies</surname> <given-names>KB</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>MD</given-names></name>, <etal>et al</etal>. <article-title>A deep-learning classifier identifies patients with clinical heart failure using whole-slide images of H&amp;E tissue</article-title>. <source>PLOS ONE</source>. <year>2018</year>;<volume>13</volume>(<issue>4</issue>):<fpage>e0192726</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0192726" xlink:type="simple">10.1371/journal.pone.0192726</ext-link></comment> <object-id pub-id-type="pmid">29614076</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006269.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Lamprecht</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kang</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Friman</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>CellProfiler: image analysis software for identifying and quantifying cell phenotypes</article-title>. <source>Genome Biology</source>. <year>2006</year>;<volume>7</volume>(<issue>10</issue>):<fpage>R100</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/gb-2006-7-10-r100" xlink:type="simple">10.1186/gb-2006-7-10-r100</ext-link></comment> <object-id pub-id-type="pmid">17076895</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>