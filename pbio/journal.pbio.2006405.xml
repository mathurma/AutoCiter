<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2006405</article-id>
<article-id pub-id-type="publisher-id">pbio.2006405</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Sensory cues</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Sensory cues</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Sensory cues</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>Old World monkeys</subject><subj-group><subject>Macaque</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Animal management</subject><subj-group><subject>Animal performance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Visual cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Areal differences in depth cue integration between monkey and human</article-title>
<alt-title alt-title-type="running-head">Depth cue integration in monkey</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Armendariz</surname>
<given-names>Marcelo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ban</surname>
<given-names>Hiroshi</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Welchman</surname>
<given-names>Andrew E.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9399-343X</contrib-id>
<name name-style="western">
<surname>Vanduffel</surname>
<given-names>Wim</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Laboratory of Neuro- and Psychophysiology, Department of Neurosciences, KU Leuven Medical School, Leuven, Belgium</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Center for Information and Neural Networks, National Institute of Information and Communications Technology, Osaka, Japan</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Graduate School of Frontier Biosciences, Osaka University, Osaka, Japan</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Psychology, University of Cambridge, Cambridge, United Kingdom</addr-line></aff>
<aff id="aff005"><label>5</label> <addr-line>Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, Massachusetts, United States of America</addr-line></aff>
<aff id="aff006"><label>6</label> <addr-line>Department of Radiology, Harvard Medical School, Boston, Massachusetts, United States of America</addr-line></aff>
<aff id="aff007"><label>7</label> <addr-line>Leuven Brain Institute, Leuven, Belgium</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Tong</surname>
<given-names>Frank</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Vanderbilt University, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">wim@nmr.mgh.harvard.edu</email> (WV); <email xlink:type="simple">aew69@cam.ac.uk</email> (AW)</corresp>
</author-notes>
<pub-date pub-type="epub">
<day>29</day>
<month>3</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2019</year>
</pub-date>
<volume>17</volume>
<issue>3</issue>
<elocation-id>e2006405</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>3</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Armendariz et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2006405"/>
<abstract>
<p>Electrophysiological evidence suggested primarily the involvement of the middle temporal (MT) area in depth cue integration in macaques, as opposed to human imaging data pinpointing area V3B/kinetic occipital area (V3B/KO). To clarify this conundrum, we decoded monkey functional MRI (fMRI) responses evoked by stimuli signaling near or far depths defined by binocular disparity, relative motion, and their combination, and we compared results with those from an identical experiment previously performed in humans. Responses in macaque area MT are more discriminable when two cues concurrently signal depth, and information provided by one cue is diagnostic of depth indicated by the other. This suggests that monkey area MT computes fusion of disparity and motion depth signals, exactly as shown for human area V3B/KO. Hence, these data reconcile previously reported discrepancies between depth processing in human and monkey by showing the involvement of the dorsal stream in depth cue integration using the same technique, despite the engagement of different regions.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>In everyday life, we interact with a three-dimensional world that we perceive via our two-dimensional retinas. Our brain can reconstruct the third dimension from these flat retinal images using multiple sources of visual information, or cues. The horizontal displacement of the two retinal images, known as binocular disparity, and the relative motion between different objects are two important depth cues. However, to make the most of the information provided by each cue, our brains must efficiently integrate across them. To examine this process, we used neuroimaging in monkeys to record brain responses evoked by stimuli signaling depths defined by either binocular disparity or relative motion in isolation, and also when the two cues are combined congruently or incongruently. We found that cortical area MT in monkeys is involved in the fusion of these two particular depth cues, in contrast to previous human imaging data that pinpoint a more posterior cortical area, V3B/KO. Our findings support the existence of depth cue integration mechanisms in primates; however, this fusion appears to be computed in slightly different areas in humans and monkeys.</p>
</abstract>
<funding-group>
<funding-statement>Research Foundation Flanders <ext-link ext-link-type="uri" xlink:href="https://www.fwo.be" xlink:type="simple">https://www.fwo.be</ext-link> (grant number G0A5613N). KU Leuven Programme Financing (grant number PFV/10/008 and C14/17/109). European Union’s Horizon 2020 Framework Programme for Research and Innovation (SGA2) (grant number 785907). Hercules Foundation. JSPS KAKENHI (grant number 17H04790 and 17K20021). Wellcome Trust (grant number 95183/Z/10/Z). ERATO (grant number JPMJER1801). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="1"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-04-10</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data are available from the Dryad database at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Visual environments provide a range of cues that allow the brain to extract depth structure from the ambiguous images projected onto the two-dimensional (2D) retinas. A fundamental challenge in visual neuroscience is to understand how this 2D information is processed and integrated, to allow the viewer to perceive and act in a three-dimensional (3D) world. While multiple regions of the macaque [<xref ref-type="bibr" rid="pbio.2006405.ref001">1</xref>] and human [<xref ref-type="bibr" rid="pbio.2006405.ref002">2</xref>] brain have been found to respond to images that depict depth, it is only recently that we have begun to understand how information from different signals is fused together. While many studies demonstrated that regions of cortex could respond to information conveyed by two different cues (such as depth from binocular disparity, and depth from motion), this alone does not imply that information is fused into a common representation. For instance, information from the two cues might be locally segregated within the cortex. Differentiating responses to fused versus independent signals requires careful assessment of neural responses to presentations of stimuli in which information from the two different cues is manipulated independently.</p>
<p>Electrophysiological recordings from macaque middle temporal (MT) area have suggested representation of surface structures defined by combinations of binocular disparity and relative motion cues [<xref ref-type="bibr" rid="pbio.2006405.ref003">3</xref>]. This is consistent with a large number of studies that have highlighted the importance of area MT in signalling structured motion information [<xref ref-type="bibr" rid="pbio.2006405.ref004">4</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref006">6</xref>] and disparity signals [<xref ref-type="bibr" rid="pbio.2006405.ref007">7</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref009">9</xref>]. Surprisingly, however, human imaging identified a different neural locus for the fusion of depth signals from disparity and motion [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>]. Specifically, dorsal visual area V3B/kinetic occipital area (V3B/KO) [<xref ref-type="bibr" rid="pbio.2006405.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref013">13</xref>] which is located more caudally relative to human MT, showed neuronal responses that match the predictions of cue fusion using multiple tests. What lies behind the apparent discrepancy between the neural locus of fusion in humans and macaques? Is it a difference between species or is it merely technique related: functional MRI (fMRI) versus single unit electrophysiology? It is known that putatively homologous areas in human and macaque (i.e., macaque MT and human MT+) may carry partially different functions [<xref ref-type="bibr" rid="pbio.2006405.ref014">14</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref016">16</xref>]. It is an open question, however, whether this also holds for cue fusion mechanisms. Here, we aim to rule out differences induced by technique by using comparative brain imaging [<xref ref-type="bibr" rid="pbio.2006405.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref017">17</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref018">18</xref>], thereby exploiting stimuli, experimental designs, and analysis tools previously used in human studies [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>] to test for responses in the macaque brain.</p>
<p>Identifying neural responses for integrated depth signals requires an experimental design that allows us to differentiate collocated, but independent, responses to two different depth cues from an integrated representation that fuses the signals together. To this end, we used random dot displays depicting near or far depth positions of a planar target square relative to its surround. We represent these stimuli as bivariate probability density functions in the space of disparity-motion stimuli (i.e., green and magenta blobs in <xref ref-type="fig" rid="pbio.2006405.g001">Fig 1A</xref>). By manipulating dot positions in the two eyes (binocular disparity) and differences in the target’s speed relative to its surround (relative motion) we could produce different impressions of depth. Using this stimulus space, we created four conditions in which the target’s near versus far depth was defined by (1) Disparity (where the motion cue indicated zero depth); (2) Motion (where the disparity cue indicated a flat surface in the fronto-parallel plane); (3) Congruent cues (where disparity and motion both indicated the same depth); or (4) Incongruent cues (where disparity indicated one depth position [e.g., near] and motion indicated the other [e.g., far]).</p>
<fig id="pbio.2006405.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Schematics of stimuli, fusion, and independence mechanisms.</title>
<p>(A) The depth of a central target can be defined by disparity and/or motion. Congruent stimuli (disparity and motion presented consistently) are represented as bivariate Gaussian distributions (magenta versus green blobs for near versus far stimuli, respectively). A single cue detector would sense depth along only one dimension (disparity or motion detector): distinguishing the stimuli in this case depends on making a judgment using the marginal distribution (illustrated along the top and left-hand sides of the disparity-motion space). A fusion mechanism (bottom left) combines disparity and motion distributions into a single dimension: this reduces the variance of the combined estimate (solid distributions) relative to the components (dotted distributions). The independence mechanism (bottom right) finds the optimal separating boundary between the stimuli: this increases the separation between the distributions to improve discrimination performance; this corresponds to the quadratic sum of performance along the component axes (by the Pythagorean theorem this means greater separation along the diagonal). Black, magenta, and green dashed lines overlaying the stimuli (not shown during the experiment) are used here to delineate the reference plane and the near and far target planes, respectively. Black, magenta, and green arrows represent the amount of displacement of the reference and target planes. Both dotted planes move sinusoidally (from left to right and vice versa) within the margins determined by the squares of the background (never overlapping with them). (B) Performance of the fusion (left) versus independence (right) mechanisms for the single-cue and incongruent-cue conditions. In both scenarios, the fusion mechanism is compromised and performance decreases, but the independence mechanism is unaffected because depth differences are detected independently. (C) Decoding predictions for an area that responds (ideally) based on fusion or independence. An example of a hypothetical mixed neuronal population response (i.e., neurons tuned to independent cues or to fusion) is shown in the middle panel. Red dotted line depicts the quadratic summation of the marginal cues. D, disparity; M, relative motion; DM, consistent combination of disparity and motion. <italic>Figure was adapted from Welchman</italic>, <italic>2016</italic> [<xref ref-type="bibr" rid="pbio.2006405.ref002">2</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g001" xlink:type="simple"/>
</fig>
<p>To understand the experimental logic, we outline the way in which fusion versus independence representational schemes should be engaged by these stimuli. First, it is important to understand that when two cues specify the same depth arrangement (Congruent condition), performance is expected to be best under both scenarios, but for different reasons. For an optimal fusion mechanism, information from disparity and motion is averaged together to produce a depth estimate with lower variance (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1A</xref>, left). By contrast, an optimal independence mechanism uses the outputs of separate detectors for the two cues. This corresponds to finding the maximal separation between the two stimuli (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1A</xref>, right), which can be intuitively computed as the Pythagorean quadratic sum of the separations along the disparity and motion dimensions. Thus, stimuli are more discriminable because their effective separation is increased (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1A</xref>, right).</p>
<p>To distinguish fusion from independence, we can measure performance when we manipulate the conflict between the depth information specified by the two cues. First, “single” cue performance (i.e., conditions [<xref ref-type="bibr" rid="pbio.2006405.ref001">1</xref>] Disparity or [<xref ref-type="bibr" rid="pbio.2006405.ref002">2</xref>] Motion) are useful because they involve one cue indicating no difference in depth between pairs of stimuli (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1B</xref>, left). In this case, the independence mechanism effectively ignores the cue signalling zero depth (e.g., geometrically, the hypotenuse can never be shorter than one of the catheti), while the fusion mechanism is compromised because it averages together one signal that specifies depth (e.g., near) with another that indicates a flat surface. Second, incongruent stimuli can cause radically different responses from the two mechanisms. For an independence mechanism, sensitivity should be comparable to that of congruent stimuli: the separation between the two can become greater for incongruent stimuli, for which the cues specify an opposite depth sign (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1B</xref>, right). The independence mechanism is not affected by incongruence, as the depth sign is effectively ignored (i.e., the Pythagorean separation still increases whether the cues agree or disagree). However, the fusion mechanism is affected: a strict fusion mechanism could be completely insensitive, although (more realistically) a robust fusion mechanism would revert to the sensitivity of a single cue component.</p>
<p>Using the responses to the different stimuli, we can generate a set of predictions for performance under the fusion and independence scenarios (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1C</xref>). For the fusion mechanism, we would expect sensitivity to depth differences in the congruent case to exceed the quadratic summation of performance for the single cue cases. This is because the fusion mechanism is compromised by the conflicts in the single cue stimuli. In addition, performance for incongruent stimuli will be similar to that for single cues (assuming a robust fusion mechanism). By contrast, for the independence mechanism, we would expect congruent cue performance to match the quadratic summation prediction established from the single cue conditions, and no difference in performance for incongruent cues (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1C</xref>, right). While a neuronal population might be weighted towards a depth cue integration mechanism, it would be unrealistic to expect a pure fusion-tuned region. In <xref ref-type="fig" rid="pbio.2006405.g001">Fig 1C</xref> (middle) we show an example of the response of a hypothetical hybrid population response, in which fusion and independent units are collocated.</p>
<p>Here, we test for cortical regions that respond on the basis of cue fusion using functional brain imaging in monkeys and compare that with previous results obtained in humans [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]. We use multivoxel pattern analysis to quantify sensitivity to differences in brain activity evoked by stimuli depicting different depth configurations and contrast empirical performance with the predictions for independence versus fusion.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>To identify areas involved in processing depth cues in the monkey cortex, we started by performing a searchlight analysis that discriminated fMRI responses within an aperture that was moved systematically through the cortex. In particular, we used fMRI monocrystalline iron oxide nanoparticle (MION) responses measured while subjects were presented with stimuli depicting near versus far depths defined on the basis of binocular disparity, relative motion, and their combination. We quantified the discriminability of fMRI responses by training a support vector machine (SVM) to classify patterns of activity evoked by stimuli depicting near versus far depth configurations.</p>
<p>We projected the results of this analysis onto a flattened representation of the cortex (<xref ref-type="fig" rid="pbio.2006405.g002">Fig 2</xref>), thereby producing a searchlight map for near versus far classification for each cue. This suggested that substantial parts of the visual cortex contained fMRI signals that could support reliable stimulus classification. For the motion cue, we found activity that supported reliable classification mostly in dorsal regions of the visual cortex (V2d, V3d, V3A), parietal areas, lateral intraparietal area (LIP) and anterior intraparietal area (AIP), and area MT and its satellites. Relatively weak classification was found in ventral areas of early visual cortex. We also noted, to a lesser extent, meaningful results in frontal areas (frontal eye field [FEF]) and 46. Classification accuracies were higher overall for the disparity cue and, in addition to the dorsal visual areas (unlike motion, also including the dorsal prelunate area [DP]), we observed ample significant classification in ventral areas (V2v, V3v, V4) and the parietal cortex (caudal intraparietal area [CIP], LIP, posterior intraparietal area [PIP]). Classification accuracies were higher still for the combined cue stimulus (disparity and motion). Searchlight maps were consistent across subjects (<xref ref-type="supplementary-material" rid="pbio.2006405.s001">S1A–S1C</xref> and <xref ref-type="supplementary-material" rid="pbio.2006405.s002">S2A–S2C</xref> Figs). Additionally, we computed a searchlight map for the incongruent stimuli in which reliable classification covered similar areas as the congruent condition (<xref ref-type="supplementary-material" rid="pbio.2006405.s003">S3 Fig</xref>). A voxel-versus-voxel comparison of the congruent and incongruent maps was not sensitive enough to observe significant differences.</p>
<fig id="pbio.2006405.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Whole-brain searchlight analyses for disparity, motion, and congruent conditions in monkey.</title>
<p>Flat maps showing the left and right monkey cortex. Borders between areas are defined by retinotopic mapping and indicated by the white dotted lines. Sulci/gyri are coded in dark/light gray. Results of a searchlight classifier analysis that moved iteratively throughout the entire volume of cortex, discriminating between near and far depth positions (group data, <italic>N</italic> = 2), are presented. The color code represents the <italic>t</italic> value of the classification accuracies obtained for depths defined by (A) disparity, (B) relative motion, and (C) the congruent combination of disparity and motion. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. CIP, caudal intraparietal area; DP, dorsal prelunate area; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area; V1, primary visual cortex; V2d, dorsal secondary visual area; V2v, ventral secondary visual area; V3A, visual area 3A; V3d, dorsal visual area 3; V3v, ventral visual areas 3; V4, visual area 4; V4A, visual area 4A; V4t, transitional visual area 4.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g002" xlink:type="simple"/>
</fig>
<p>Similar to monkeys, a wide range of areas in human visual cortex were involved in processing stimuli depicting depth (<xref ref-type="fig" rid="pbio.2006405.g003">Fig 3</xref>). Human dorsal areas V3d and V3B/KO consistently showed highest reliable decoding across the three conditions. Primary and secondary visual cortex (V1, V2) and ventral areas (V3v, V4) also supported significant classification accuracies, yet were particularly strong when motion-defined depth was presented. In addition, weaker although significant discriminability was also found in dorsal areas V3A and V7. However, whereas our monkey data pointed to MT as an important cortical locus for depth processing, the presumed conglomerate of homologous areas in humans, the MT+ complex [<xref ref-type="bibr" rid="pbio.2006405.ref019">19</xref>], showed only partial and fragmentary significant classification across conditions and hemispheres. Similar fragmented response patterns were found in the lateral occipital (LO) cortex. Moreover, in contrast to monkeys, humans exhibited more activity for motion than for disparity. Surprisingly, whereas disparity compared to motion-based classification was qualitatively more pronounced in ventral visual areas in monkeys, the opposite was found in humans.</p>
<fig id="pbio.2006405.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Whole-brain searchlight analyses for disparity, motion, and congruent conditions in human.</title>
<p>Flat maps showing the left and right human cortex. Data are from Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]. Same conventions as in <xref ref-type="fig" rid="pbio.2006405.g002">Fig 2</xref>. The color code represents the <italic>t</italic> value of the classification accuracies obtained for depths defined by (A) disparity, (B) relative motion, and (C) the congruent combination of disparity and motion (group data, <italic>N</italic> = 20). The map for the incongruent condition is shown in <xref ref-type="supplementary-material" rid="pbio.2006405.s004">S4 Fig</xref>. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. hMT+, human middle temporal area; LO, lateral occipital area; V3B/KO, area V3B, kinetic occipital area.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g003" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Region of interest–based multivoxel patterns analysis</title>
<p>To better assess depth processing in the monkey brain, we adapted methods from a previous human study [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>] and confined the fMRI responses to independently defined regions of interest (ROIs; see <xref ref-type="sec" rid="sec017">Materials and methods</xref> for definitions). We then trained a machine learning classifier (SVM) to distinguish between near and far voxel activation patterns for each depth cue and across ROIs. For each area, we calculated the performance of the classifier in decoding depth from an independent data set using a leave-one-out cross-validation approach.</p>
<p>Consistent with the searchlight analyses described above, depth defined by congruent stimuli was reliably decoded in most of the ROIs (<xref ref-type="fig" rid="pbio.2006405.g004">Fig 4A</xref>), but performance varied across areas. This widespread sensitivity to different cues throughout visual cortex does not indicate explicit encoding of depth. The SVM may decode low-level image features, rather than depth per se. Discrimination performance was higher across the three conditions in early visual areas (V1, V2), ventral area V4A, dorsal V3, and MT (and its satellites). Although most of the areas showed lower sensitivity for both single cues compared with the concurrent stimulus, motion classification was particularly poor (&lt;56%) in the more dorsal regions (V3A, DP, PIP, and CIP).</p>
<fig id="pbio.2006405.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Classification performances and quadratic summation test.</title>
<p>(A) Prediction performance (accuracy and sensitivity) for near versus far discrimination in different ROIs and for different conditions. The red lines illustrate performance expected from the quadratic summation of prediction sensitivities for the marginal cues. Error bars, SEM. (B) Results as an integration index. A value of zero indicates the minimum bound for fusion (the prediction based on quadratic summation). Data are presented as notched distribution plots. The center of the “bowtie” represents the median, the greenish area depicts 68% confidence values, and the upper and lower error bars 95% confidence intervals. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. CIP, caudal intraparietal area; DP, dorsal prelunate area; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area; ROI, region of interest.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>fMRI quadratic summation</title>
<p>Our main interest was not to compare classification accuracies of single and combined cue conditions across regions (because these are influenced by a range of factors besides neural activity, such as our ability to measure fMRI activity with the same sensitivity in different anatomical locations—which basically holds for all fMRI studies comparing signals across areas) but to evaluate the relative performance within each area of the condition in which both disparity and motion concurrently signalled depth.</p>
<p>We compared prediction accuracies for the congruent stimulus relative to a minimum bound for fusion prediction based on the quadratic summation of decoding accuracies for the single cues (see Figs <xref ref-type="fig" rid="pbio.2006405.g001">1</xref> and <xref ref-type="fig" rid="pbio.2006405.g004">4A</xref>). We found that fMRI responses were higher than quadratic summation (<italic>P</italic> &lt; 0.01) in areas of the MT cluster (fundus superior temporal [FST], medial superior temporal [MST], and MT) and ventral V3. We quantified the extent of integration across areas using a bootstrapped index (<xref ref-type="sec" rid="sec017">Materials and methods</xref>, <xref ref-type="disp-formula" rid="pbio.2006405.e001">Eq 1</xref>). Values close to zero correspond to the performance expected if information from disparity and motion are collocated but processed independently. A higher value would indicate that a fusion mechanism may be present (see <xref ref-type="fig" rid="pbio.2006405.g001">Fig 1C</xref>). We found that the integration index in mid-level areas MST and MT significantly exceeded zero (Bonferroni correction for multiple comparisons, <italic>P</italic> &lt; 0.01) (<xref ref-type="fig" rid="pbio.2006405.g004">Fig 4B</xref>). In addition, we found integration indices above zero (uncorrected threshold) in areas FST and V3v (<xref ref-type="table" rid="pbio.2006405.t001">Table 1</xref>) (for human results, see <xref ref-type="supplementary-material" rid="pbio.2006405.s005">S5 Fig</xref> and Ban and colleagues, 2012). Our findings in the MT complex are consistent with decoding improvements for the congruent condition as a result of the fusion of disparity and motion cues (see <xref ref-type="fig" rid="pbio.2006405.g001">Fig 1</xref>)—although this test alone cannot rule out independences.</p>
<table-wrap id="pbio.2006405.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.t001</object-id>
<label>Table 1</label> <caption><title>Significance tests for the integration index, congruency, and transfer index.</title></caption>
<alternatives>
<graphic id="pbio.2006405.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Area</th>
<th align="center" colspan="3"><italic>P</italic> value</th>
</tr>
<tr>
<th align="center">Integration index above zero</th>
<th align="center">Congruent versus incongruent</th>
<th align="center">Transfer index from chance</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">V1</td>
<td align="char" char=".">0.4166</td>
<td align="char" char=".">0.1323</td>
<td align="char" char=".">0.0014</td>
</tr>
<tr>
<td align="left">V2</td>
<td align="char" char=".">0.9957</td>
<td align="char" char=".">0.5755</td>
<td align="char" char=".">0.0034</td>
</tr>
<tr>
<td align="left">V3v</td>
<td align="char" char=".">0.0027</td>
<td align="char" char=".">0.0119</td>
<td align="char" char=".">0.0901</td>
</tr>
<tr>
<td align="left">V4</td>
<td align="char" char=".">0.5305</td>
<td align="char" char=".">0.9448</td>
<td align="char" char=".">0.7851</td>
</tr>
<tr>
<td align="left">V4A</td>
<td align="char" char=".">0.4949</td>
<td align="char" char=".">0.3559</td>
<td align="char" char=".">0.0177</td>
</tr>
<tr>
<td align="left">OT</td>
<td align="char" char=".">0.8483</td>
<td align="char" char=".">0.9244</td>
<td align="char" char=".">0.9750</td>
</tr>
<tr>
<td align="left">PIT</td>
<td align="char" char=".">0.1708</td>
<td align="char" char=".">0.0066</td>
<td align="char" char=".">0.9999</td>
</tr>
<tr>
<td align="left">V3d</td>
<td align="char" char=".">0.4482</td>
<td align="char" char=".">0.1485</td>
<td align="char" char="."><bold>0.0001</bold></td>
</tr>
<tr>
<td align="left">V3A</td>
<td align="char" char=".">0.4701</td>
<td align="char" char=".">0.9824</td>
<td align="char" char=".">0.8874</td>
</tr>
<tr>
<td align="left">DP</td>
<td align="char" char=".">0.7433</td>
<td align="char" char=".">0.9786</td>
<td align="char" char=".">0.9993</td>
</tr>
<tr>
<td align="left">MT</td>
<td align="char" char="."><bold>0.0001</bold></td>
<td align="char" char="."><bold>0.0001</bold></td>
<td align="char" char="."><bold>0.0001</bold></td>
</tr>
<tr>
<td align="left">MST</td>
<td align="char" char="."><bold>0.0002</bold></td>
<td align="char" char=".">0.1077</td>
<td align="char" char=".">0.7006</td>
</tr>
<tr>
<td align="left">FST</td>
<td align="char" char=".">0.0011</td>
<td align="char" char=".">0.0213</td>
<td align="char" char=".">0.9999</td>
</tr>
<tr>
<td align="left">V4t</td>
<td align="char" char=".">0.0734</td>
<td align="char" char=".">0.9758</td>
<td align="char" char=".">0.1672</td>
</tr>
<tr>
<td align="left">PIP</td>
<td align="char" char=".">0.1053</td>
<td align="char" char=".">0.6139</td>
<td align="char" char=".">0.9999</td>
</tr>
<tr>
<td align="left">CIP</td>
<td align="char" char=".">0.0867</td>
<td align="char" char=".">0.8908</td>
<td align="char" char=".">0.3893</td>
</tr>
<tr>
<td align="left">LIP</td>
<td align="char" char=".">0.5229</td>
<td align="char" char=".">0.6474</td>
<td align="char" char=".">0.3355</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Probabilities associated with obtaining (i) a value of zero for the fMRI integration index, (ii) zero difference between classification performances for congruent and incongruent conditions, and (iii) no difference between the value of the transfer index compared with random performance. These <italic>P</italic> values are calculated using bootstrapped resampling with 10,000 samples. Bold indicates Bonferroni-corrected significance (<italic>P</italic> &lt; 0.01). MT is the only area passing all tests at Bonferroni-corrected level.</p></fn>
<fn id="t001fn002"><p>Abbreviations: CIP, caudal intraparietal area; DP, dorsal prelunate area; fMRI, functional MRI; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec005">
<title>Congruent versus incongruent cues</title>
<p>We added the incongruent condition to perform a second test to assess cue integration mechanisms. Both cues, motion and disparity, were presented simultaneously, but, in this case, depicting opposite depths (one cue signalled “near” and the other “far”). If the representation of depth defined by the two stimulus dimensions (disparity and motion) is independent, this conflicting situation should have no effect on classification performance. Thus, the performance of a machine learning classifier in distinguishing between voxel patterns should be similar for both congruent and incongruent conditions and comparable to the quadratic sum of the component cues. In contrast to this expectation, if fusion is present, the discrimination performance should be lower when motion and disparity conflict. Under strict fusion, performance would diminish below that of either component cue, whereas in robust fusion, performance would revert to the level of one of the two components.</p>
<p>Area MT exhibited a significant drop in performance for the incongruent condition in comparison to the congruent stimulus (<xref ref-type="fig" rid="pbio.2006405.g005">Fig 5A</xref>). However, classification accuracy was still slightly higher than for single cue conditions. The visual posterior inferotemporal area (PIT) also showed lower prediction accuracy for the incongruent stimulus, although this was not significant after Bonferroni correction. These findings suggest that MT may house a mixed population that contains both units tuned to independent and to fused cues. This may help support a robust fusion process that prevents viewers from becoming completely insensitive to conflicting cues.</p>
<fig id="pbio.2006405.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Congruency and transfer test.</title>
<p>(A) Prediction accuracy for near versus far classification when cues are congruent or incongruent in different ROIs. The horizontal line at 0.5 corresponds to chance performance. Error bars, SEM; *<italic>P</italic> &lt; 0.01 uncorrected; **<italic>P</italic> &lt; 0.01 Bonferroni corrected. (B) Prediction accuracy for the cross-cue transfer analysis in different regions. Classification performances are shown when data were trained and tested with the same cue (within-cue, dark purple), trained with one cue and tested with the other (cross-cue, cyan), and for randomly permuted data (light purple). Error bars, SEM. (C) Data shown as a transfer index. A value of 100% would indicate that prediction accuracies were equivalent for within- and between-cue testing. Distribution plots show the median; cyan area and error bars represent the 68% and 95% confidence intervals, respectively. Purple dotted horizontal lines depict a bootstrapped chance baseline based on the upper 95th percentile for transfer obtained with randomly permuted data. *<italic>P</italic> &lt; 0.01 uncorrected; **<italic>P</italic> &lt; 0.01 Bonferroni corrected. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. CIP, caudal intraparietal area; DP, dorsal prelunate area; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area; ROI, region of interest.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Transfer test</title>
<p>To assess for brain areas involved in a generalized depth representation, we performed a cross-cue transfer test. In particular, we evaluated whether activity patterns evoked by one depth cue provide information about the other. We trained a machine learning classifier to discriminate depth configurations using one cue (for example, disparity), and tested classifier’s prediction using depth responses elicited by the other cue (for example, motion).</p>
<p>For each ROI, we compared the average performance of the classifier in predicting the near versus far stimulus configuration within cues (trained and tested with the same cue) and across cues (trained with one cue and tested with the other) (<xref ref-type="fig" rid="pbio.2006405.g005">Fig 5B</xref>). To quantify the transfer of information between cues we calculated a bootstrapped index (<xref ref-type="sec" rid="sec017">Materials and methods</xref>, <xref ref-type="disp-formula" rid="pbio.2006405.e002">Eq 2</xref>). A value of one would indicate that prediction accuracy is equal for both within- and between-cue classification, meaning that there is 100% transfer of information. To provide a baseline for the transfer that arises by chance, we conducted the transfer test on randomly permuted data. We calculated random transfer performance 1,000 times for each ROI, and we chose the 95th percentile of the resulting distribution of transfer indices as the significance threshold.</p>
<p>We found transfer indices above the baseline in areas V1, V2, V3d, and MT, although only V3d and MT exceeded the threshold significantly (Bonferroni corrected, <italic>P</italic> &lt; 0.01) (<xref ref-type="table" rid="pbio.2006405.t001">Table 1</xref>). Transfer performance of MT was around 55% of that obtained when trained and tested on the same cue (<xref ref-type="fig" rid="pbio.2006405.g005">Fig 5C</xref>). These results suggest that area MT may play a key role in a more generic representation of depth in the monkey. The same test in humans highlighted dorsal areas V3d and V3B/KO, but did not yield significant results in hMT+ (<xref ref-type="supplementary-material" rid="pbio.2006405.s006">S6 Fig</xref>).</p>
</sec>
<sec id="sec007">
<title>Composition of the neuronal population explaining cue-fusion results</title>
<p>So far, we considered scenarios in which neuronal population responses relate either to fusion or independence mechanisms. However, even in areas where we found evidence for depth cue integration (monkey MT and human V3B/KO), it is unlikely that we sampled voxels that respond exclusively to fused signals. To evaluate how different population mixtures might affect decoding results, we used simulations in which we systematically varied the composition of the neuronal population and compared the simulation results with the empirical data (<xref ref-type="supplementary-material" rid="pbio.2006405.s007">S7 Fig</xref>), exactly as in Ban and colleagues, 2012 (see their <xref ref-type="fig" rid="pbio.2006405.g006">Fig 6</xref> and <xref ref-type="sec" rid="sec017">Methods</xref> section). Whereas in humans the estimated number of fusion units ranges between 50% and 70% in V3B/KO, our simulations suggested that approximately 35% of the neural population in monkey MT might be tuned to fusion. This difference in neuronal composition across species might explain the relative differences in classification performance between human V3B/KO (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6A</xref> of Ban and colleagues, 2012) and monkey MT (<xref ref-type="supplementary-material" rid="pbio.2006405.s007">S7B Fig</xref>). In particular, whereas classification performance for the incongruent condition is comparable to that of the single cues in human V3B/KO, the lower percentage of units that contribute to fusion in monkey MT might have caused higher performance for the incongruent condition compared with single cues. Despite these differences, the crucial point is whether the sensitivity for the incongruent condition exceeds the quadratic summation of the single cues. Our statistical tests showed that sensitivity for the incongruent condition was not significantly greater than the quadratic summation in monkey MT nor in human V3B/KO (<xref ref-type="supplementary-material" rid="pbio.2006405.s010">S10 Fig</xref>). In sum, our analyses showed that the ratio of fusion-tuned units in monkey MT is sufficiently high to observe significant differences between the classification performances for congruent and incongruent conditions.</p>
<fig id="pbio.2006405.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Flat maps for integration and transfer tests based on the searchlight analyses.</title>
<p>Integration and transfer test maps for monkeys (A, B) and humans (C, D), calculated from the results of group searchlight classifier analyses. Color code represents the <italic>P</italic> values obtained from the bootstrap distribution of the integration and transfer indices in monkey and human. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. CIP, caudal intraparietal area; DP, dorsal prelunate area; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Searchlight MVPA: Integration and transfer tests</title>
<p>In addition to performing our ROI analysis, we used the searchlight multivoxel pattern analysis (MVPA) approach to confirm that we did not miss important areas involved in cue integration in monkeys (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6A</xref>). We first used the integration index and confirmed our ROI-based analysis of area MT as well as revealing responses in CIP and a confined region in the dorsal part of V2 and V3. Searchlight analysis of the human brain (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6C</xref>) pointed to area V3B/KO as the only consistent locus for cue integration. Second, we used a searchlight analysis of the cross-cue transfer test. We found that areas V3d and MT supported significant classification of near-far patterns across cues (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6B</xref>). In addition to these two areas, V3A also exhibited high transfer indices. For humans, the highest transfer indices were found around the dorsal areas V3d, V3A, and V3B (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6D</xref>).</p>
<p>To highlight the implication of areas across the five tests performed (selectivity for disparity, motion and congruent stimuli, and integration and transfer indices), we computed a probabilistic summary map. This color coded map summarizes each voxel that reached significance in each of the five tests (<xref ref-type="fig" rid="pbio.2006405.g007">Fig 7</xref>). Taken together, these results suggest that monkey area MT and human V3B/KO are the prime candidate regions for fusing motion and disparity cues in the monkey and human brains, respectively.</p>
<fig id="pbio.2006405.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Cue integration summary map in monkey and human.</title>
<p>Summary maps highlighting the implication of the different areas in monkeys (A) and humans (B) across all the analyses performed: sensitivity for disparity, motion and congruent stimuli, and integration and transfer indices. Color code indicates each voxel that reached significance (monkey, <italic>P</italic> &lt; 0.01; human, <italic>P</italic> &lt; 0.05) in each of the five tests, ranging from 1 (one test passed) to 5 (five tests passed). The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. CIP, caudal intraparietal area; DP, dorsal prelunate area; FST, fundus of the superior temporal sulcus area; LIP, lateral intraparietal area; MST, medial superior temporal sulcus area; MT, middle temporal area; OT, occipitotemporal area; PIP, posterior intraparietal area; PIT, posterior inferotemporal area.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Depth discrimination task</title>
<p>To assess whether monkeys perceived the depth stimuli in a similar manner as humans (Ban and colleagues, 2012), we performed a depth discrimination task for each of the four conditions in two rhesus macaques. Two planes with different depths were sequentially presented, and monkeys had to indicate whether the second stimulus (target depth) was nearer or farther compared with the first stimulus (reference depth) by making saccadic movements to one of two dots on the left and right sides of the screen. Differences between the reference and target depth stimuli ranged between 0 and 6.3 arcmin. Both monkeys were able to discriminate between the two depth planes (<italic>P</italic> &lt; 0.01) for all conditions at depth differences higher than 1.8 arcmin (<xref ref-type="fig" rid="pbio.2006405.g008">Fig 8A</xref>). Remarkably, one of the monkeys was able to classify between stimuli even for the smallest tested depth difference (0.3 arcmin) when the congruent condition was presented. In general, when depths were discriminable, monkeys showed higher sensitivity (<italic>d’</italic>) to the congruent stimulus compared with the other conditions. Sensitivity to the relative motion condition was slightly lower than for disparity in both monkeys, while discrimination for the incongruent condition was comparable to that of the single cues. Overall, when pooling the performances across all depth levels and subjects, classification accuracies were above chance for all stimuli and significantly higher for the congruent condition than for the three other conditions (<xref ref-type="fig" rid="pbio.2006405.g008">Fig 8B</xref>). Moreover, we calculated sensitivity based on just noticeable difference (j.n.d.) thresholds [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]. As in humans, we found that monkeys were most sensitive when disparity and motion concurrently signalled depth differences, and they were least sensitive for relative motion–related differences (<xref ref-type="fig" rid="pbio.2006405.g008">Fig 8C</xref>). We used sensitivities to single cue conditions to calculate their quadratic summation. In line with fusion and the previous human results [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>], performance for congruent cues exceeded both the quadratic summation and that of the incongruent cues. In summary, these results suggest similar depth perception across conditions between monkeys and humans, with increased performance for the congruent condition and significantly lower discrimination for relative motion–defined depths compared with depths defined by disparity.</p>
<fig id="pbio.2006405.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2006405.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Depth discrimination task.</title>
<p>We assessed whether monkeys were able to discriminate different depth levels using the four stimulus conditions of the main experiment. (A) We show sensitivities for depth differences between two sequentially presented planes for monkeys B and T. Both monkeys were able to discriminate between depths for all conditions when the reference and target planes differed by more than 1.8 arcmin in depth. Particularly, monkey B performed excellently and was able to classify between congruent stimuli even for the finest depth difference used (0.3 arcmin). In general, when depths were discriminable, monkeys showed highest sensitivity to the congruent stimulus and lower sensitivity for motion than disparity. Discrimination for the incongruent condition was comparable to that of the single cues. (B) Overall discrimination accuracy across depth levels and monkeys. (C) Sensitivity calculated based on j.n.d. thresholds. As in humans, monkeys were most sensitive when disparity and motion concurrently signalled depth differences, and they were least sensitive for relative motion–related differences. Error bars show bootstrapped 95% confidence intervals; significance was set to <italic>P</italic> &lt; 0.01. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. j.n.d., just noticeable difference.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Eye movement analyses</title>
<p>We analyzed horizontal eye movements from our subjects to assess possible differences between the stimulus conditions. First, we observed that the distribution of eye positions during each condition (averaged across blocks) was centered within a 0.25° window surrounding the fixation point (0°) for all eight conditions (<xref ref-type="supplementary-material" rid="pbio.2006405.s011">S11A Fig</xref>). Second, for each condition, we computed the mean eye position within each presentation block. No significant differences were observed across conditions in eye positions (Kruskal–Wallis test, <italic>P</italic> = 0.1315) (<xref ref-type="supplementary-material" rid="pbio.2006405.s011">S11B Fig</xref>). Hence, differences in fixation are an unlikely explanation of our findings. These results are in line with previous human data using identical stimuli [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], also showing that eye positions were not different across conditions, and unlikely to explain the observed differences in fMRI signals.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>Interacting with objects is contingent on a reliable estimation of the 3D structure of the world around us. The primate brain exploits a range of sources of sensory information for that purpose. Amongst others, cues such as binocular disparity and relative motion have shown to be important for humans and monkeys to perceive depth [<xref ref-type="bibr" rid="pbio.2006405.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref021">21</xref>]. Little is known, however, about how these two sources—and others—are combined in the visual cortex to support reliable judgments of depth.</p>
<p>We considered two scenarios under which depth might be processed within the primate visual cortex: a fusion mechanism, in which cues are combined to estimate depth by reducing variance, and independent processing of separate cues, in which different configurations of multivariate distributions (bivariate in our study) can be discriminated using the optimal decision boundary. Our findings in monkey area MT, together with those discovered in human area V3B/KO [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], suggest that a fusion mechanism may take place in dorsal visual areas of the primate brain.</p>
<p>Here, we assessed fMRI responses in monkey visual cortex to near and far depth planes defined by binocular disparity, relative motion, and their combination (both congruent and incongruent). We found that much of the visual cortex is implicated in depth cue processing, although performance varied across regions and conditions (<xref ref-type="fig" rid="pbio.2006405.g002">Fig 2</xref>). Integration and transfer index maps (<xref ref-type="fig" rid="pbio.2006405.g006">Fig 6</xref>) highlighted the importance of dorsal visual areas (V2d, V3d, V3A), CIP, and area MT and its satellites in the integration of binocular cues.</p>
<p>We then performed a ROI-based MVPA on fMRI signals and assessed results using three criteria. First, we tested for regions where performance for congruent cue stimuli exceeded the quadratic summation of the marginal cues’ accuracies and hence surpassed the performance expected if depth representations from disparity and motion were collocated but independent. Second, to test whether this improvement was specific to the congruent condition, we contrasted performance when the two cues signalled either the same or the opposite depth. Third, we assessed whether regions supported transfer of depth information between cues.</p>
<p>Our results revealed that MT in the monkey was the only region that met all three criteria. We note that, besides MT, the MST area performed above zero for the ROI-based integration index test, but it did not meet the other two criteria for cue integration. The searchlight maps for the integration index also pointed to CIP and confined subcompartments of V2d and V3d. It is reasonable to think that the increased decoding performance in these areas for the congruent cues is a consequence of collocated representations of disparity and motion information. Consequently, a lack of sensitivity of our analyses for the single cue stimuli in these regions resulted in low decoding performance. Besides MT, area V3d was the only region that passed the transfer test, but V3d did not pass the other two tests. It may be that our tests were not sensitive enough to reveal conclusive evidence for fusion in this area. Decoding accuracy for the disparity condition was very high (&gt;90%) in V3d, and responses for the congruent condition may have been near the ceiling, limiting the capability to exceed the quadratic summation level (<xref ref-type="fig" rid="pbio.2006405.g004">Fig 4</xref>).</p>
<p>In summary, these findings suggest that area MT plays a key role in the integration of different depth cues in the monkey cortex. Moreover, other extrastriate visual areas (such as V3d) might be related to intermediate depth representation, thus supporting fusion computations in addition to independent representation of depth cues.</p>
<sec id="sec012">
<title>Depth processing in the primate cortex</title>
<p>Depth representation from individual cues (binocular disparity, motion, texture, shape, shading, etc.) has been widely studied; however, surprisingly little is known about how the primate brain integrates the information from different depth cues. Previous fMRI studies in humans reported sensitivity to binocular disparity at multiple levels of the visual hierarchy, from early visual cortex to parietal and temporal areas [<xref ref-type="bibr" rid="pbio.2006405.ref022">22</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref029">29</xref>]. Likewise, large portions of human extrastriate visual cortex are activated by 3D structure from motion [<xref ref-type="bibr" rid="pbio.2006405.ref030">30</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref033">33</xref>]. Nevertheless, this extensive sensitivity for both cues does not necessarily imply depth selectivity: for instance, it might represent low-level image features, local disparities, or speed of movement (although this was controlled for in some experiments [<xref ref-type="bibr" rid="pbio.2006405.ref005">5</xref>]).</p>
<p>Depth processing has also been investigated in macaques by measuring fMRI responses to both binocular and monocular cues [<xref ref-type="bibr" rid="pbio.2006405.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref034">34</xref>–<xref ref-type="bibr" rid="pbio.2006405.ref039">39</xref>]. Similar to humans, these studies suggested a widely distributed network for 3D representations, implicating monkey occipital, temporal, and parietal cortices. However, the human intraparietal sulcus (IPS) has shown much stronger sensitivity to motion-defined 3D than the corresponding monkey region, suggesting that some visuospatial processing areas might not be present in the macaque cortex [<xref ref-type="bibr" rid="pbio.2006405.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref040">40</xref>] or that similar functions in both species rely on different regions [<xref ref-type="bibr" rid="pbio.2006405.ref015">15</xref>].</p>
<p>Behavioral experiments have been performed to compare cue integration in human and nonhuman primates (macaques). Notably, Schiller and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref021">21</xref>] investigated how disparity, motion parallax, and shading depth cues were perceived and integrated. They showed that both species effectively utilized these cues to perceive depth. Moreover, they found that performance was significantly increased when all three cues were presented together, indicating that these separate cues are integrated at yet unknown sites in the primate brain. In addition, consistent with our behavioral data, previous work reported less effective depth discrimination for motion than for disparity in primates [<xref ref-type="bibr" rid="pbio.2006405.ref041">41</xref>]. In line with these findings, results from our depth discrimination task in monkeys using disparity and relative motion, together with those previously obtained in adult and juvenile humans [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>], confirmed the role of these cues in depth perception and that depth cue integration takes place in the primate brain. Overall, similar behavioral results in both species also indicate that monkeys are an excellent animal model for the study of the underlying neural mechanisms of depth perception.</p>
</sec>
<sec id="sec013">
<title>MT is likely to contain a mixed population of units that support cue fusion</title>
<p>Neurophysiological evidence in nonhuman primates suggests sensitivity to binocular disparity throughout visual cortex [<xref ref-type="bibr" rid="pbio.2006405.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref042">42</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref043">43</xref>]. Recordings in monkey V2 and MT have shown neurons selective to different disparities and clustered organization according to their disparity preference [<xref ref-type="bibr" rid="pbio.2006405.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref044">44</xref>]. Furthermore, monkey MT has also been related to depth representation defined by motion [<xref ref-type="bibr" rid="pbio.2006405.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref006">6</xref>]. A recent study [<xref ref-type="bibr" rid="pbio.2006405.ref003">3</xref>] showed that single MT neurons have depth-sign–tuned responses for both binocular disparity and motion cues. Specifically, they observed that 68 cells (approximately 51%) preferred either disparity or motion alone, while the remaining cells showed significant selectivity for both conditions. Among these neurons selective for both cues, 37 cells (approximately 28% of the population) had the same depth-sign preference (congruent cells), and 29 cells (approximately 21% of the population) had opposite depth-sign preference (incongruent cells). Thus, disparity tuning at the single cell level did not always match across cues, supporting the existence of congruent and opposite depth-sign–tuned cells. Moreover, responses of congruent neurons to the congruent combination of binocular disparity and motion cues showed an increased selectivity compared with that from single cues, but no enhancement was found in the incongruent cells (when disparity and motion were presented congruently). In <xref ref-type="fig" rid="pbio.2006405.g001">Fig 1C</xref> we depicted the two ideal scenarios for fusion and independence; however, in light of previous findings, it is unrealistic to expect such a pure neuronal assembly, but a hybrid arrangement instead. On one hand, if a sizeable neuronal subpopulation contributes to fusion, we should be able to detect a sensitivity increase caused by the reduced variance, (only) when the congruent stimulus is presented. At the same time, in a mixed population, separate portions of neurons respond independently to each of the single cues, and their aggregate neuronal response patterns will also be more discriminable when cues are presented together. Moreover, the increase in discriminability by these independent assemblies should equally affect the incongruent condition, because the improvement corresponds to the quadratic sum of the discriminabilities of the marginal distributions. Critically, the contribution of the fusion mechanism will elicit a significant sensitivity improvement that will exceed the quadratic summation of the single cues for the congruent condition, but not for the incongruent stimulus. The combination of both effects can be seen in our results in monkey MT (<xref ref-type="supplementary-material" rid="pbio.2006405.s010">S10 Fig</xref>, left), where (i) a fusion-tuned subpopulation contributes with a substantial enhancement that exceeds the performance of the incongruent condition and the quadratic summation of the individual cues; and (ii) separate single cue–tuned neural subpopulations contribute with higher responses for the incongruent condition compared with single cues, although not higher than their quadratic summation. Altogether, the findings of Nadler and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref003">3</xref>] are in agreement with our results, because we obtained higher classification performance when the two cues are presented congruently (<xref ref-type="supplementary-material" rid="pbio.2006405.s010">S10 Fig</xref>). To the best of our knowledge, responses of congruent cells to the incongruent condition and responses of incongruent cells to both congruent and incongruent conditions were not tested in Nadler and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref003">3</xref>], nor elsewhere [<xref ref-type="bibr" rid="pbio.2006405.ref045">45</xref>]. These findings support MT as a potential candidate for integrating depth cues, although the role of opposite cells is currently not understood. Additional electrophysiological studies, in which both congruent and conflicting combinations of disparity and motion cues are tested, are required to further our understanding of cue integration mechanisms at the single cell level in MT.</p>
<p>At a mechanistic level, it is important to note that contributions made by either “congruent” or “incongruent” neurons, when estimating environmental properties from the combination of signals, are unknown. Recently, however, we proposed a model whereby both congruent and incongruent responses contribute to estimating the most likely depth of the scene (at a population level) [<xref ref-type="bibr" rid="pbio.2006405.ref046">46</xref>]. In particular, a population “best guess” estimate is computed by combing the outputs of congruent neurons that drive excitation and incongruent neurons that drive inhibition. This process allows robust perceptual estimates—accounting for improved performance when signals are consistent, and robust reversion to one of the signals under conflict. This model explains why many neurons are incongruent, and how the use of such neurons supports robust perceptual estimations by the brain. Our fMRI logic is based on separating “independence” versus “fusion.” It is not specific to the way in which fusion is implemented, and, as corroborated by the new model, it is likely to involve the contributions of both “congruent” and “incongruent” neurons. A given stimulus will evoke activity within the population of fusion of neurons (some will appear “congruent” and others “incongruent,” while, in reality, all encode information about the likelihood of the viewed stimulus). Thus, there is nothing special about the response that will be evoked by a consistent or inconsistent stimulus in terms of the types of neurons that they will stimulate. However, there is a difference in the statistical evidence in favor of a particular depth interpretation: this will be higher when cues are consistent. The present results invite future studies on the neural implementation of the new model [<xref ref-type="bibr" rid="pbio.2006405.ref046">46</xref>] at the single cell level in monkey area MT.</p>
</sec>
<sec id="sec014">
<title>Human V3B/KO versus macaque MT</title>
<p>Recent tests for cue fusion revealed V3B/KO as the main cortical locus in the human cortex for the integration of depth defined by binocular disparity and motion cues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>]. Complementary fMRI studies that used texture [<xref ref-type="bibr" rid="pbio.2006405.ref047">47</xref>], shading [<xref ref-type="bibr" rid="pbio.2006405.ref048">48</xref>], and gloss [<xref ref-type="bibr" rid="pbio.2006405.ref049">49</xref>] supported the capability of human area V3B/KO to integrate qualitatively different depth cues. However, no evidence of integration was found in human MT+. Considering that no conclusive evidence yet exists concerning the existence of a macaque homologue of human V3B/KO, one could argue that cue fusion for motion and disparity in monkey occurs in a region more caudally relative to MT, in a location that corresponds at least topographically with V3B/KO. In this respect, it is also worth mentioning that the parcellation of dorsal extrastriate visual cortex of the monkey is highly complex and that the exact areal definitions are highly disputed. Nevertheless, our results suggest an areal difference across primate species for depth cue integration and a potential homology between human V3B/KO and monkey MT for this specific functionality.</p>
<p>Then, can we claim that human MT+ is not an exact homologue to monkey MT? Of course, it is unreasonable to expect that 100% identical areas can be found in two different species, as ecological pressure must have triggered (slightly) different functional properties in individual species. There is evidence that human MT+ and monkey MT might share a large range of functions. For example, when we previously showed monkeys and humans identical videos and correlated the “free-viewing” fMRI signals from independently identified monkey MT with all signals from all voxels of the human cortex, we found significant correlations not only in human area MT+ but also in dorsal areas of the visual cortex. However, when seeding in human MT+, the correlations we found were surprisingly well confined to area MT in the monkey [<xref ref-type="bibr" rid="pbio.2006405.ref015">15</xref>]. These entirely data-driven results suggest that MT shares a number of functional properties across species, but not all. While human MT+ is functionally more closely related to MT than other areas in the monkey cortex, monkey MT might be carrying some functionalities that through evolutionary pressure are distributed across several regions of the human visual cortex (<xref ref-type="supplementary-material" rid="pbio.2006405.s008">S8 Fig</xref>). Thus, according to our results and those of Ban and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], human MT+ cannot be considered an exact homologue of monkey MT, at least with regard to depth cue integration. Response patterns across species differed substantially in MT (see <xref ref-type="supplementary-material" rid="pbio.2006405.s009">S9A Fig</xref>). Although responses for disparity, relative motion, and their combination are discriminable in both human MT+ and monkey MT (indicating that there is not a lack of responsivity in MT of both species), human MT+ did not show significant increases for the congruent condition compared with the quadratic summation of the single cues, nor a significant transfer of depth information across cues—which are required to support fusion. Moreover, when comparing the relative performances of MT across species, we observed that monkey MT exceeded human MT+ significantly (<xref ref-type="supplementary-material" rid="pbio.2006405.s009">S9B Fig</xref>). Considering these and previous findings, monkey MT share depth cue fusion processes more with human V3B/KO, compared with human MT. However, this specific correspondence does not rule out other functional correspondences between human and monkey MT.</p>
<p>CIP has also been implicated in processing depth from different cues (disparity and texture) and integration [<xref ref-type="bibr" rid="pbio.2006405.ref050">50</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref051">51</xref>]. In our study, we observed higher classification accuracy for the congruent condition compared with the quadratic summation of the single cues and the incongruent condition in area CIP; however, results were not significant (<xref ref-type="fig" rid="pbio.2006405.g004">Fig 4</xref>). In addition, near-far discriminability for the motion-defined depth condition dropped to chance level in area CIP, as well as in the rest of parietal regions except for LIP. Accordingly, human V7, the putative corresponding area of monkey CIP [<xref ref-type="bibr" rid="pbio.2006405.ref052">52</xref>], did not show selectivity for depth cues integration [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref047">47</xref>].</p>
</sec>
<sec id="sec015">
<title>What is the contribution of relative motion and speed?</title>
<p>To depict depth structure from motion, we used a display in which near and far conditions were defined by differences in speed between a target (center) and reference plane (surround). On one hand, the reference plane always moved horizontally (and sinusoidally, from left to right and right to left) with a constant speed. Importantly, the direction of the reference plane movement was unambiguous due to a static reference background surrounding the reference plane (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1</xref>). When the centrally presented target plane moved faster than the reference plane, it appeared to protrude towards the viewer (near percept). The opposite effect (far percept) was established when the target plane moved slower compared with the reference. Because there is no ambiguity on the direction of the reference plane movement, the differential movement velocity disambiguated the sign of depth in our stimuli [<xref ref-type="bibr" rid="pbio.2006405.ref053">53</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref054">54</xref>]. Moreover, the occlusion of dot patterns at the edge between target and reference planes may also contribute to solve depth-sign ambiguity [<xref ref-type="bibr" rid="pbio.2006405.ref055">55</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref056">56</xref>]. It has been shown that accretion–deletion in the presence of relative motion provides sufficient information to disambiguate depth sign. However, accretion–deletion alone is unable to provide any depth perception [<xref ref-type="bibr" rid="pbio.2006405.ref056">56</xref>]. Results from our depth discrimination task in monkeys (<xref ref-type="fig" rid="pbio.2006405.g008">Fig 8</xref>), together with those previously reported in humans [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], suggest that both human and nonhuman primates are capable to discriminate between near versus far depth positions on the basis of relative motion defined in this study.</p>
<p>As in any fMRI experiment, responses from voxels reflect a complex mixture of neural signals (originating from thousands of neurons), most of them even unknown. It is therefore natural to ask whether the fMRI decoding performance that we measure reflects differences in the perceived depth, as opposed to simpler differences in the speed of motion. In particular, it is well known that MT contains neurons that respond differentially to different speeds of motion [<xref ref-type="bibr" rid="pbio.2006405.ref057">57</xref>]. Our data contain a number of lines of evidence suggesting that speed per se is unlikely to be responsible for the results that we report. First, the congruent and incongruent stimulus conditions both contained stimuli that differed in disparity-defined depth and the speed of motion. An explanation for our data premised on motion speed alone would not predict the differential decoding performance we find for congruent versus incongruent stimuli in monkey MT. Specifically, the speed of motion for the congruent condition, near disparity and near motion (NN), is matched to that of the incongruent condition, far disparity and near motion (FN), and the speed in the congruent condition, far disparity and far motion (FF), is matched with that of the incongruent condition, near disparity and far motion (NF). Thus, if discrimination relied on speed, the classification of congruent conditions (NN versus FF) and incongruent conditions (NF versus FN) should be similar. Our results in monkeys (MT) and humans (V3B/KO), however, showed differences in performance between congruent and incongruent conditions. This suggests that there is something particular about the difference in speed, in that it produces an impression of depth compatible with the depth signal provided by the disparity cue. Second, if the classification were a mere consequence of speed differences, we would not have expected to see transfer of decoding performance between the different cue conditions (<xref ref-type="fig" rid="pbio.2006405.g005">Fig 5</xref>). It might be possible to explain this finding based on an association between speed and disparity preference in the underlying selectivities of individual neurons. However, although MT neurons with strong speed tuning also tend to have strong disparity tuning, there is no evidence for a correlation between the preferred magnitude of disparity and velocity [<xref ref-type="bibr" rid="pbio.2006405.ref058">58</xref>]. Thus, there is little evidence to suggest that retinal speed signals are confounded with the disparity sign of the stimuli. Although we are aware that fMRI responses contain a complex mixture of neural signals with different properties, such as speed preferences, we believe that the tests performed in our experiment are sufficiently robust to overcome such difficulty. Together, this suggests that the results we report in monkey MT and human V3B/KO are likely to be due to the impression of depth evoked by different speeds of motion, rather than speed.</p>
</sec>
<sec id="sec016">
<title>Conclusion</title>
<p>We found that fMRI responses are more discriminable when the two cues signal depth concurrently, and that depth information provided by one cue might be diagnostic of depth indicated by the other. We revealed that monkey area MT shows fMRI signals consistent with a fusion mechanism of independent depth cues. These results may reconcile the human imaging data with previous monkey electrophysiological studies implicating area MT in depth perception based on motion and binocular disparity signals. Our findings, together with those obtained in humans, provide evidence for a fusion mechanism for depth perception in the dorsal stream of primates. The fusion of depth cues, however, appears to be computed in different areas in humans (V3B/KO) and monkeys (MT). Therefore, it is tempting to speculate that human V3B/KO may have been part of the MT cluster in an ancestor of monkeys and humans, which has drifted in a caudo-dorsal direction during human evolution.</p>
</sec>
</sec>
<sec id="sec017" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec018">
<title>Subjects</title>
<p>Four rhesus monkeys (<italic>Macaca mulatta</italic>, two female), 4–6 years old, weighing between 4 and 7 kg, participated in the experiments (two underwent fMRI and the other two performed the behavioral task). Animal care and experimental procedures met the Belgian and European guidelines and were approved by the ethical committee of the KU Leuven Medical School (Protocols P103/2008 and P022/2014). Animals were born in captivity and were pair- or group-housed (two to five animals per group; cage size at least 16–32 m<sup>3</sup>) with cage enrichment (toys, foraging devices), outside views, and natural day-night cycles (throughout the year, supplemented with an artificial 12/12-hour light/dark cycle) at the primate facility of the KU Leuven Medical School. They were daily fed with standard primate chow supplemented with bread, nuts, raisins, prunes, and fruits. The animals received their water supply either during the experiments or in the cages before and after the experiments.</p>
<p>Monkeys had previous experience performing behavioral tasks and were prepared for fMRI sessions. Prior to scanning, monkeys that underwent fMRI were trained daily (2–4 weeks) to perform a passive fixation task while in a sphinx position with their head rigidly fixed in a plastic primate chair. For the discrimination task, subjects were trained to answer using saccadic eye movements to left or right targets. Details concerning head-post surgery and training procedures have been previously described [<xref ref-type="bibr" rid="pbio.2006405.ref059">59</xref>]. During the experimental period, access to water was restricted, but animals were allowed to drink until fully satiated during the daily training and scanning sessions.</p>
<p>The human data were the same as those presented in Ban and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], although additional data analyses were performed on this data set (see below). Because the basic cue-fusion results observed in human V3B/KO described in Ban and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>] have already been replicated using exactly the same stimuli [<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>] and with entirely different stimuli [<xref ref-type="bibr" rid="pbio.2006405.ref047">47</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref048">48</xref>], we deemed it unnecessary to replicate them in the present study. Despite our efforts to perform an identical experiment, there are always unavoidable differences between monkey and human fMRI experiments, which are summarized in <xref ref-type="supplementary-material" rid="pbio.2006405.s013">S1 Table</xref>. One difference is that humans performed a subjective assessment of eye vergence [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], while monkeys received liquid reward to maintain fixation to a fixation point. However, it is rather unlikely that differences in the fixation task greatly impacted the sensory-driven activations. In particular, previous experiments showed that activity patterns can be similar during anesthesia versus awake fixation experiments [<xref ref-type="bibr" rid="pbio.2006405.ref060">60</xref>].</p>
</sec>
<sec id="sec019">
<title>Stimuli</title>
<p>Stereoscopic presentation and display parameters were matched as closely as possible to those used in the previous human study [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>] and will be only summarized briefly. For a detailed table listing every difference between the monkey and the human study, see <xref ref-type="supplementary-material" rid="pbio.2006405.s013">S1 Table</xref>.</p>
<p>Stimuli consisted of two planes, the reference plane and the target plane, defined by random patterns of dots (0.15° size, 15 dots/deg<sup>2</sup> density). The target plane was represented by a square (10 × 10°) located in the center of the screen, superimposed on the rectangular reference plane (18 × 14°). In addition, this random dot region was surrounded by a static grid (43 × 32°) of black and white squares that provided an unambiguous background (permanently present on the screen on top of a gray flat surface). At the center of the screen, a fixation square (0.5° × 0.5°) was presented within a circular mask (1° diameter). The reference plane remained the same across all conditions, with no stereoscopic structure and a constant motion profile. The stereoscopic structure of the dots and the motion of the target plane were altered to depict depth relative to the reference plane. We defined eight stimuli and four different conditions: depth defined (i) by disparity, (ii) by motion, (iii) by congruent disparity and motion, and (iv) by incongruent disparity and motion.</p>
<p>Disparity-defined stimuli were rendered as red-cyan anaglyphs. To create the random dot stereograms (RDSs), we first measured the distance between the centers of the two eyes of our subject to adjust the dot displacements between the left- and right-eye images to the interpupillary distance of each monkey. The luminance of the red dots through the red filter was 11 cd/m<sup>2</sup> and 0.2 cd/m<sup>2</sup> through the cyan filter, and the luminance of cyan dots was 10.1 cd/m<sup>2</sup> through the cyan filter and 0.0 cd/m<sup>2</sup> through the red filter. For the disparity-alone condition, the target plane was given a horizontal binocular disparity of ±9 arcmin to signal near or far depth relative to the reference plane that presented no binocular disparity structure. Both target and reference planes moved rigidly (same speed and phase) with a sinusoidal horizontal movement of 0.9° amplitude and 1-second period (i.e., in 1 second, the planes covered a displacement of 0.9° from left to right and from right to left).</p>
<p>To depict depth from relative motion alone, the target plane moved horizontally with different amplitudes (1.32° for near and 0.29° for far) relative to the reference plane that kept the same movement amplitude (0.9°). Both planes moved in phase and followed a sinusoidal velocity profile with a 1-second period (<xref ref-type="fig" rid="pbio.2006405.g001">Fig 1</xref>). The direction of the movement was always obvious due to a static background surrounding the moving reference plane (see above). The relative motion of the target plane with respect to the reference plane gave rise to a pattern of deletion and accretion of the reference (near stimuli) or target (far stimuli) dots, as the two planes translated back and forth across the screen. Because there is no ambiguity about the direction of the reference plane movement due to the static background, the speed difference between planes supports the disambiguation of depth in our stimuli (i.e., when the target moves faster than the reference, it always appears to be protruding toward the viewer) [<xref ref-type="bibr" rid="pbio.2006405.ref053">53</xref>].</p>
<p>The near/far speeds of the relative motion stimuli were defined based on the psychophysical experiments [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], during which human observers were asked to judge the impression of depth perceived from relative motion differences of the target and reference planes (the reference plane speed was fixed) and depth perceived from the binocular disparities. We took the psychophysical matching point between the disparity and relative motion stimuli for performing the main fMRI experiments. Even when the eye and/or head movements are restricted, as in the present study and that of Ban and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], the depth sign from relative motion was not ambiguous for our stimuli. Our introspection of having viewed these stimuli many times, and the reports of the human observers, indicate that our stimuli were effective in depicting near versus far depth positions on the basis of relative motion.</p>
<p>For stimuli concurrently depicting disparity and motion-defined depth, the target plane had a disparity of ±9 arcmin and a movement amplitude of either 1.32° or 0.29°. In the congruent condition both disparity and motion signalled the same depth (near-near or far-far). For the incongruent case, disparity and motion simultaneously provided opposing information (near-far or far-near).</p>
</sec>
<sec id="sec020">
<title>Experimental setup and design of the fMRI experiment</title>
<p>Monkeys were placed within the bore of the magnet in sphinx position inside a plastic primate chair using a physical head restraint. Images were projected (Barco 6300 LCD projector) on a translucent screen located at a distance of 57 cm from the monkey. Subjects had to fixate passively on a square presented in the center of the screen. Eye position was monitored at 120 Hz using a pupil-corneal reflection tracking system (Iscan). To encourage monkeys to maintain fixation and remain quiet, liquid reward was delivered through a plastic tube located just inside their mouths. Stimuli were presented to the monkeys through colored filters (red-cyan anaglyph goggles, as in Durand and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref034">34</xref>] and Tsao and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref038">38</xref>]) placed in front of the eyes. Before each scanning session, a contrast agent, MION, was injected into the femoral/saphenous vein (6–11 mg/kg) to improve the contrast-to-noise ratio [<xref ref-type="bibr" rid="pbio.2006405.ref059">59</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref061">61</xref>].</p>
<p>We presented the stimuli in blocks of 16 seconds. In each block, stimuli were picked randomly from a set of 24 example stimuli (per subject) that differed in the random placement of dots making up the stereogram. Individual stimuli were presented for 1 second, followed by a 1-second fixation period. Three blocks of each stimulus type were randomly presented during an individual run (24 stimulus blocks), and the scan started and ended with a 16-second fixation interval that served as baseline. Each scan lasted 416 seconds, during which 208 volumes were acquired.</p>
</sec>
<sec id="sec021">
<title>fMRI data acquisition</title>
<p>Monkey data were acquired with a 3T MR Siemens Trio scanner with an AC88-insert head gradient. Functional images were collected using a gradient-echo T2*-weighted echo-planar imaging sequence (repetition time [TR] = 2,000 ms, echo time [TE] = 17 ms, 52 slices, voxel size = 1 mm isotropic, flip angle = 75°). Monkeys were scanned with a custom-built, eight-channel, implanted phased-array receive coil [<xref ref-type="bibr" rid="pbio.2006405.ref062">62</xref>] and a saddle-shaped, radial transmit-only surface coil.</p>
<p>To provide an anatomical reference for the functional scans, high-resolution T1-weighted images were acquired for each monkey during a separate session under ketamine-xylazine anesthesia, using a single radial transmit-receive surface coil and a MP-RAGE sequence (TR = 2,200 ms, TE = 4.05 ms, flip angle = 13°, 208 slices, voxel size = 0.4 mm isotropic). During the session, 12–15 whole-brain volumes were obtained and averaged to improve signal-to-noise ratio.</p>
</sec>
<sec id="sec022">
<title>fMRI preprocessing</title>
<p>Functional volumes were reconstructed online using Siemens GRAPPA image reconstruction. We preprocessed the fMRI data using custom Matlab (MathWorks) scripts and the SPM5 software package (Wellcome Department of Cognitive Neurology, London, UK; <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>). We only analyzed runs in which monkeys performed more than 97% of fixation within a 2 × 2 deg. fixation window. Temporal preprocessing was applied to correct for linear trends and spin-excitation history effects caused by head motion. Spatial preprocessing consisted of motion correction and rigid coregistration to the individual anatomical template. To compensate for echo-planar distortions and intersession variance, functional images were matched to the anatomy, applying nonlinear warping in JIP (<ext-link ext-link-type="uri" xlink:href="http://www.nitrc.org/projects/jip" xlink:type="simple">www.nitrc.org/projects/jip</ext-link> [<xref ref-type="bibr" rid="pbio.2006405.ref063">63</xref>]). No spatial smoothing was performed on the data for the analysis.</p>
<p>For each monkey, we used retinotopic mapping procedures to define the ROIs in the occipital cortex, IT, and posterior parietal cortex. Retinotopic organization in occipital visual areas and IT cortex (V1, V2, V3v, V3d, V4, V4A, V4t, OT, MT, MST, FST, and PIT) of both subjects was found to be highly consistent with previous reports [<xref ref-type="bibr" rid="pbio.2006405.ref064">64</xref>]. We used individual retinotopic maps along with previous parcellation schemes [<xref ref-type="bibr" rid="pbio.2006405.ref065">65</xref>] to delineate V3A, DP area, and posterior parietal cortex areas (PIP, CIP, and LIP). The anatomical labels were drawn on the inflated cortical surface of each monkey using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu" xlink:type="simple">http://surfer.nmr.mgh.harvard.edu</ext-link>) and then projected into the volume space. To ensure that distinct ROIs were not overlapping, we performed an automatic correction using customized Matlab code.</p>
<p>Preprocessing of the fMRI responses and ROI definitions in human are described in Ban and colleagues [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]</p>
</sec>
<sec id="sec023">
<title>ROI-based multivoxel pattern analysis</title>
<p>We first performed a <italic>t</italic> test to contrast the response to all stimulus conditions versus the fixation baseline across all the runs. Within each ROI, we selected gray matter voxels from both hemispheres and sorted them by their <italic>t</italic> statistic. We then selected the top 150 voxels from each ROI. For smaller cortical areas with less than 150 voxels, we selected all the voxels with a <italic>t</italic> value &gt; 0 [<xref ref-type="bibr" rid="pbio.2006405.ref025">25</xref>].</p>
<p>We normalized (<italic>z</italic>-score) each voxel’s time course separately for each experimental run to minimize baseline differences across runs. The data vectors for the multivariate analysis were generated by shifting the fMRI time series by two volumes, to account for the hemodynamic response delay. We then averaged all data points within each block to obtain the voxel patterns. To control for the possibility that classification accuracy was due to a univariate baseline difference, we normalized each pattern vector by subtracting the mean voxel amplitude.</p>
<p>We used a linear SVM (LibSVM) to classify between fMRI response patterns evoked by near versus far stimulus presentations for each condition separately and performed an <italic>n</italic>-fold leave-one-run-out cross-validation, in which data from all runs but one were used as training patterns (24 patterns per run, 6 per condition) and data from the remaining run were used as test patterns. Then, the mean accuracy across cross-validations was used.</p>
<p>To quantify differences in SVM prediction accuracies between combined-cue conditions and the minimum bound prediction, we calculated an fMRI integration index (<italic>ϕ</italic>):
<disp-formula id="pbio.2006405.e001">
<alternatives>
<graphic id="pbio.2006405.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006405.e001" xlink:type="simple"/>
<mml:math display="block" id="M1"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mrow/><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mrow/><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>d</italic>′<sub><italic>D</italic>+<italic>M</italic></sub> is the classifier’s performance in the congruent condition, and <italic>d</italic>′<sub><italic>D</italic></sub> and <italic>d</italic>′<sub><italic>M</italic></sub> are performances for single cue conditions in <italic>d</italic>-prime units, calculated with the following formula:
<disp-formula id="pbio.2006405.e002">
<alternatives>
<graphic id="pbio.2006405.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006405.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mi>d</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>erfinv</italic> is the inverse error function and <italic>p</italic> is the proportion of correct predictions.</p>
<p>To conduct the transfer test analysis, we first used a recursive feature elimination (RFE) method [<xref ref-type="bibr" rid="pbio.2006405.ref066">66</xref>] to select the voxels in each ROI with the highest discriminative power. Then, we used a standard SVM to compute within- and between-cue prediction accuracies. To assess the relationship between transfer classification performance and the mean performance for single cues, we calculated a transfer index (T):
<disp-formula id="pbio.2006405.e003">
<alternatives>
<graphic id="pbio.2006405.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2006405.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>d</italic>′<sub><italic>T</italic></sub> is the transfer classification performance, and <italic>d</italic>′<sub><italic>D</italic></sub> and <italic>d</italic>′<sub><italic>M</italic></sub> are the performances for single cue conditions. Statistical significance of the results was evaluated using bootstrapped resampling with 10,000 samples.</p>
</sec>
<sec id="sec024">
<title>Searchlight analysis</title>
<p>We performed a searchlight classification analysis [<xref ref-type="bibr" rid="pbio.2006405.ref067">67</xref>] in volume space for each monkey by selecting spherical volumes of cortical voxels within a 3-mm radius, moving voxel-wise through the entire cortical volume. Prior to the analysis, a mask was applied to exclude voxels outside the cortex. We ran three SVM classification analyses (disparity, motion, and congruent and incongruent conditions) discriminating between near-far patterns. We then calculated the fMRI integration index at each voxel location (<xref ref-type="disp-formula" rid="pbio.2006405.e001">Eq 1</xref>). To assess transfer classification accuracies, we conducted between-cue classification analysis. Finally, we computed group maps for each condition and projected the result onto a representative cortical surface (inherently causing some smoothing). This analysis confirmed that all the relevant voxels in the classification analyses were captured by the ROIs previously defined.</p>
<p>In addition to the ROI-based analysis from Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>], we ran the searchlight (8-mm radius) analysis in humans following the same procedures as described for the monkey. We then computed groups maps and projected the results onto a flattened cortical surface of the human brain.</p>
</sec>
<sec id="sec025">
<title>Depth discrimination task experiment</title>
<p>As in the fMRI experiments, random dot patterns depicting depth from binocular disparity, relative motion, and the combination of both (congruent and incongruent) were presented to two monkeys. Because the monkeys of the fMRI experiment were not available anymore, two different animals (Monkey B and Monkey T), were trained on this task. Monkeys were placed in sphinx position in front of an LCD screen (57-cm distance) and viewed the stimuli through colored filters (red-cyan). In each trial, two stimuli with a slight depth difference between them were sequentially shown, and subjects indicated which of the two planes was farther by making a saccade to one of two targets (1° dots at 10 deg eccentricity) located on the left and right sides of the screen. The first stimulus (1-second duration, reference depth) systematically showed the same depth as the far stimuli during the fMRI experiment (9 arcmin for disparity and/or 0.29° movement amplitude), whereas the second plane (1 second, target depth) was presented at different levels of depth compared with the reference plane (9 ± 0–6.3 arcmin for disparity and 0.29 ± 0–0.18° for motion). For the case of the incongruent condition, disparity far and motion near were depicted, as in the previous human study [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2006405.ref011">11</xref>]. Each condition was presented in alternate runs, which consisted of 360 trials (15 trials per 12 levels for near or far depths) in which different depth levels of the same condition were randomly presented. A total of 140 runs were acquired.</p>
</sec>
<sec id="sec026">
<title>Eye movement analysis</title>
<p>Eye positions of one eye were monitored in both monkeys (Iscan, 120 Hz) while they performed the passive fixation task during fMRI sessions. To assess possible differences in eye positions between conditions, we analyzed horizontal eye movements in detail—which are most relevant for depth-defining stimuli. First, for each block (16 seconds), we measured eye position and calculated the average eye position for each stimulus across trials. The mean eye traces were used to compute the distribution of the eye position for each condition. Second, the mean eye position within each presentation block (across the 16 seconds of trial duration) was computed. Then, fixation per condition and 95% confidence intervals were calculated. A Kruskal–Wallis test was used to calculate significant differences between conditions.</p>
</sec>
</sec>
<sec id="sec027">
<title>Supporting information</title>
<supplementary-material id="pbio.2006405.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Individual maps for monkey N.</title>
<p>Flat maps showing the left and right cortex in monkey N. The borders between areas are delineated by the white dotted lines. Sulci/gyri are coded in dark/light gray. Superimposed on the maps are the results of the classification performances obtained for depths defined by (A) disparity, (B) relative motion, and (C) the congruent combination of disparity and motion. The color code represents the <italic>t</italic> value of the classification accuracies obtained for each condition. Maps (D) and (E) show the results of the integration and transfer tests based on the searchlight analyses. Color code represents the <italic>P</italic> values obtained from the bootstrap distribution of the integration and transfer indices. (F) Integration summary map. Color code indicates each voxel that reached significance in each of the five tests, ranging from 1 (one test passed) to 5 (five tests passed).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Individual maps for monkey D.</title>
<p>Flat maps showing the left and right visual ROIs in monkey D. Same conventions as in <xref ref-type="supplementary-material" rid="pbio.2006405.s001">S1 Fig</xref> ROI, region of interest.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Searchlight map for the incongruent condition in monkeys.</title>
<p>Flat map showing the cortex of the left and right hemisphere of the monkey. Results of a searchlight classifier analysis that moved iteratively throughout the entire volume of cortex, discriminating between near and far depth positions for the incongruent stimulus (group data, <italic>N</italic> = 2). The color code represents the <italic>t</italic> value of the classification accuracies. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Searchlight map for the incongruent condition in humans.</title>
<p>Flat maps showing the left and right human cortex. Same conventions as in <xref ref-type="fig" rid="pbio.2006405.g003">Fig 3</xref>. The color code represents the <italic>t</italic> value (<italic>N</italic> = 11) of the classification accuracies for the incongruent stimulus. Data are from Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Integration index in humans.</title>
<p>Results for the quadratic summation test shown as an integration index. A value of zero indicates the minimum bound for fusion (the prediction based on quadratic summation). Data are presented as notched distribution plots. The center of the “bowtie” represents the median, the greenish area depicts 68% confidence values, and the upper and lower error bars 95% confidence intervals. *<italic>P</italic> &lt; 0.05 Bonferroni corrected. Data from Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>].</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Transfer test in humans.</title>
<p>Transfer index across regions. A value of 100% would indicate that prediction accuracies were equivalent for within- and between-cue testing. Distribution plots show the median; cyan area and error bars represent the 68% and 95% confidence intervals, respectively. Purple dotted horizontal lines depict a bootstrapped chance baseline based on the upper 95th percentile for transfer obtained with randomly permuted data. *<italic>P</italic> &lt; 0.05 Bonferroni corrected. Data from Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>fMRI decoding data from monkey MT and results from simulations.</title>
<p>We explored the composition of the neuronal population, comparing our simulation results to our empirical data. To evaluate how a population mixture might affect decoding results, we used simulations to vary systematically the composition of the neuronal population, following exactly the same procedures as our previous study (see <xref ref-type="fig" rid="pbio.2006405.g006">Fig 6</xref> and <xref ref-type="sec" rid="sec017">Methods</xref> section in Ban and colleagues, 2012 [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>]). (A) Simulation results show decoding performance of a simulated population of voxels for different compositions of neuronal populations. (B) Real fMRI decoding data from monkey MT. (C) The χ<sup>2</sup> statistic was used to identify the closest fit between empirical and simulated data from a range of population mixtures. According to the simulations, around 35% neurons were found to be tuned as fusion neurons in MT. Error bars, SEM. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. fMRI, functional MRI; MT, middle temporal area.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Intra- and interspecies activity correlation maps for MT.</title>
<p>In a previous study, we showed monkeys and humans identical videos and correlated the “free-viewing” fMRI signals from independently identified monkey MT with all signals from all voxels of the human cortex, and vice versa. When seeding in monkey MT, we found significant correlations not only in human area MT+, but also in dorsal areas of the visual cortex. However, when seeding in human MT+, the correlations we found were surprisingly well confined to area MT in the monkey. These results suggest that MT shares a number of functional properties across species, but not all. While human MT+ is functionally more closely related to MT than other areas in the monkey cortex, monkey MT might be carrying multiple functionalities that are distributed across several regions of the human visual cortex. fMRI, functional MRI; MT, middle temporal area. <italic>Figure was adapted from Mantini and colleagues</italic>, <italic>2012</italic>, <italic>Supplementary Figure 8</italic> [<xref ref-type="bibr" rid="pbio.2006405.ref015">15</xref>].</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Comparison of response patterns in monkey MT and human MT.</title>
<p>(A) Response patterns in MT differed substantially across species. Here, we show the prediction accuracy for near versus far classification across conditions in monkey area MT (mMT) and human MT+ (hMT). Responses for disparity, relative motion, and their combination are discriminable in both hMT [<xref ref-type="bibr" rid="pbio.2006405.ref010">10</xref>] and mMT, indicating that we have enough sensitivity in MT of both species. While cross-cue classification was significant in mMT, discriminability between depths was not significant in hMT, suggesting no transfer of depth information across cues (disparity and motion) in this area in humans. The horizontal line at 0.5 corresponds to chance performance. Error bars, SEM. (B) To assess the difference across response patterns, we compared the relative performance of mMT and hMT+ under three different conditions. Note that we are not comparing absolute but relative activity levels. First, we show the increase of sensitivity for the congruent condition relative to the quadratic summation of the single cues (integration) in both areas. Results in MT across species were significantly different (<italic>P</italic> &lt; 0.01; Bayes factor [BF] in favor of the hypothesis of a difference between mMT and hMT = 16.75). Second, we compared the performance of the congruent condition with the incongruent condition between species. mMT exceeded hMT substantially (<italic>P</italic> &lt; 0.01; BF = 3.06). Third, we compared the cross-cue transfer of depth information between cues (disparity and motion) in both species. While performance was comparable to the permuted chance baseline for hMT (indicating no transfer of depth information), cross-cue transfer accuracy in monkey MT was significantly higher (<italic>P</italic> &lt; 0.01; BF &gt; 10<sup>3</sup>). Error bars, SEM. Statistical significance of the results was evaluated using bootstrapped resampling with 10,000 samples. BF analysis was based on Dienes 2008 (“Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference.” Palgrave-Macmillan). The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. BF, Bayes factor; hMT, human MT+; mMT, monkey area MT; MT, middle temporal area.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Performance of the incongruent condition in monkey MT and human V3B/KO.</title>
<p>Performance of the incongruent condition compared with the congruent condition and the quadratic summation of the single cues. We observed differences in the performance of the incongruent condition relative to the single cues (that might be explained by differences in the composition of the neural population in the ROI); however, the crucial point is whether the sensitivity for the incongruent condition exceeds the quadratic summation of the single cues. Our statistical tests showed that sensitivity for the incongruent condition was not significantly greater (n.s.g.) than the quadratic summation in monkey MT nor in human V3B/KO. Error bars, SEM; *<italic>P</italic> &lt; 0.01. Statistical significance of the results was evaluated using bootstrapped resampling with 10,000 samples. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. MT, middle temporal area; n.s.g., not significantly greater; ROI, region of interest; V3B/KO, area V3B, kinetic occipital area.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>Eye movement analysis.</title>
<p>We assessed possible differences in eye position between conditions. (A) We show the distribution of the eye positions during stimulus presentation (16 seconds) for each condition averaged across trials. Eye positions were centered within a 0.25° window surrounding the fixation point (0°) for all eight conditions. (B) We show eye positions for each condition. No significant differences were observed across conditions (Kruskal–Wallis test, <italic>P</italic> = 0.1315). Hence, differences in eye position are an unlikely explanation of our findings. Error bars show 95% confidence intervals; significance was set to <italic>P</italic> &lt; 0.01. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s012" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Classification performance across areas.</title>
<p>Classification accuracies for near versus far discrimination in different ROIs and for different conditions. Error bars show 95% confidence intervals. The underlying data for the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.6pm117m" xlink:type="simple">https://doi.org/10.5061/dryad.6pm117m</ext-link>. ROI, region of interest.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2006405.s013" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.2006405.s013" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Differences between human and monkey experiment design.</title>
<p>Differences regarding the task and fMRI acquisition between human and monkey experiments. fMRI, functional MRI.</p>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors thank A. Coeman, C. Fransen, P. Kayenbergh, I. Puttemans, C. Ulens, S. De Pril, A. Hermans, G. Meulemans, W. Depuydt, S. Verstraeten, and M. Depaep for technical and administrative support.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>2D</term>
<def><p>two-dimensional</p></def>
</def-item>
<def-item><term>3D</term>
<def><p>three-dimensional</p></def>
</def-item>
<def-item><term>AIP</term>
<def><p>anterior intraparietal area</p></def>
</def-item>
<def-item><term>CIP</term>
<def><p>caudal intraparietal area</p></def>
</def-item>
<def-item><term>DP</term>
<def><p>dorsal prelunate area</p></def>
</def-item>
<def-item><term>fMRI</term>
<def><p>functional MRI</p></def>
</def-item>
<def-item><term>FST</term>
<def><p>fundus of the superior temporal sulcus area</p></def>
</def-item>
<def-item><term>IPS</term>
<def><p>intraparietal sulcus</p></def>
</def-item>
<def-item><term>j.n.d.</term>
<def><p>just noticeable difference</p></def>
</def-item>
<def-item><term>LIP</term>
<def><p>lateral intraparietal area</p></def>
</def-item>
<def-item><term>LO</term>
<def><p>lateral occipital</p></def>
</def-item>
<def-item><term>MION</term>
<def><p>monocrystalline iron oxide nanoparticle</p></def>
</def-item>
<def-item><term>MST</term>
<def><p>medial superior temporal sulcus area</p></def>
</def-item>
<def-item><term>MT</term>
<def><p>middle temporal area</p></def>
</def-item>
<def-item><term>OT</term>
<def><p>occipitotemporal area</p></def>
</def-item>
<def-item><term>PIP</term>
<def><p>posterior intraparietal area</p></def>
</def-item>
<def-item><term>PIT</term>
<def><p>posterior inferotemporal area</p></def>
</def-item>
<def-item><term>RDS</term>
<def><p>random dot stereogram</p></def>
</def-item>
<def-item><term>RFE</term>
<def><p>recursive feature elimination</p></def>
</def-item>
<def-item><term>ROI</term>
<def><p>region of interest</p></def>
</def-item>
<def-item><term>SVM</term>
<def><p>support vector machine</p></def>
</def-item>
<def-item><term>TE</term>
<def><p>echo time</p></def>
</def-item>
<def-item><term>TR</term>
<def><p>repetition time</p></def>
</def-item>
<def-item><term>V1</term>
<def><p>primary visual cortex</p></def>
</def-item>
<def-item><term>V2</term>
<def><p>secondary visual cortex</p></def>
</def-item>
<def-item><term>V2d</term>
<def><p>dorsal secondary visual area</p></def>
</def-item>
<def-item><term>V2v</term>
<def><p>ventral secondary visual area</p></def>
</def-item>
<def-item><term>V3A</term>
<def><p>visual area 3A</p></def>
</def-item>
<def-item><term>V3B/KO</term>
<def><p>area V3B, kinetic occipital area</p></def>
</def-item>
<def-item><term>V3d</term>
<def><p>dorsal visual area 3</p></def>
</def-item>
<def-item><term>V3v</term>
<def><p>ventral visual areas 3</p></def>
</def-item>
<def-item><term>V4</term>
<def><p>visual area 4</p></def>
</def-item>
<def-item><term>V4A</term>
<def><p>visual area 4A</p></def>
</def-item>
<def-item><term>V4t</term>
<def><p>transitional visual area 4</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2006405.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>The Extraction of 3D Shape in the Visual System of Human and Nonhuman Primates</article-title>. <source>Annu Rev Neurosci</source>. <year>2011</year>;<volume>34</volume>(<issue>1</issue>):<fpage>361</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>The Human Brain in Depth: How We See in 3D</article-title>. <source>Annu Rev Vis Sci</source>. <year>2016</year>;<volume>2</volume>(<issue>1</issue>):annurev-vision-111815-114605.</mixed-citation></ref>
<ref id="pbio.2006405.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadler</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Barbash</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Shimpi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>. <article-title>Joint representation of depth from motion parallax and binocular disparity cues in macaque area MT</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>(<issue>35</issue>):<fpage>14061</fpage>–<lpage>74</lpage>, <fpage>14074a</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0251-13.2013" xlink:type="simple">10.1523/JNEUROSCI.0251-13.2013</ext-link></comment> <object-id pub-id-type="pmid">23986242</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadler</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>. <article-title>A neural representation of depth from motion parallax in macaque visual cortex</article-title>. <source>Nature</source>. <year>2008</year>;<volume>452</volume>(<issue>7187</issue>):<fpage>642</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06814" xlink:type="simple">10.1038/nature06814</ext-link></comment> <object-id pub-id-type="pmid">18344979</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Peuskens</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Denys</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sunaert</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <etal>et al</etal>. <article-title>Extracting 3D from motion: differences in human and monkey intraparietal cortex</article-title>. <source>Science</source>. <year>2002</year>;<volume>298</volume>(<issue>5592</issue>):<fpage>413</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1073574" xlink:type="simple">10.1126/science.1073574</ext-link></comment> <object-id pub-id-type="pmid">12376701</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xiao</surname> <given-names>D-K</given-names></name>, <name name-style="western"><surname>Marcar</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Raiguel</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Selectivity of Macaque MT/V5 Neurons for Surface Orientation in Depth Specified by Motion</article-title>. <source>Eur J Neurosci</source>. <year>1997</year>;<volume>9</volume>(<issue>5</issue>):<fpage>956</fpage>–<lpage>64</lpage>. <object-id pub-id-type="pmid">9182948</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bradley</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>Encoding of three-dimensional structure-from-motion by primate area MT neurons</article-title>. <source>Nature</source>. <year>1998</year>;<volume>392</volume>(<issue>6677</issue>):<fpage>714</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/33688" xlink:type="simple">10.1038/33688</ext-link></comment> <object-id pub-id-type="pmid">9565031</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Cumming</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Cortical area MT and the perception of stereoscopic depth</article-title>. <source>Nature</source>. <year>1998</year>;<volume>394</volume>(<issue>August</issue>):<fpage>677</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Organization of disparity-selective neurons in macaque area MT</article-title>. <source>J Neurosci</source>. <year>1999</year>;<volume>19</volume>(<issue>4</issue>):<fpage>1398</fpage>–<lpage>415</lpage>. <object-id pub-id-type="pmid">9952417</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Preston</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Meeson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>The integration of motion and disparity cues to depth in dorsal visual cortex</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>(<issue>4</issue>):<fpage>636</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3046" xlink:type="simple">10.1038/nn.3046</ext-link></comment> <object-id pub-id-type="pmid">22327475</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dekker</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>van der Velde</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Nardini</surname> <given-names>M</given-names></name>. <article-title>Late Development of Cue Integration Is Linked to Sensory Fusion in Cortex</article-title>. <source>Curr Biol</source>. <year>2015</year>;<volume>25</volume>(<issue>21</issue>):<fpage>2856</fpage>–<lpage>61</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2015.09.043" xlink:type="simple">10.1016/j.cub.2015.09.043</ext-link></comment> <object-id pub-id-type="pmid">26480841</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Greenlee</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Kraemer</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Rgen Hennig</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hennig</surname> <given-names>J</given-names></name>. <article-title>The Processing of First- and Second-Order Motion in Human Visual Cortex Assessed by Functional Magnetic Resonance Imaging (fMRI)</article-title>. <source>J Neurosci</source>. <year>1998</year>;<volume>18</volume>(<issue>10</issue>).</mixed-citation></ref>
<ref id="pbio.2006405.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dupont</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>De Bruyn</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vandenberghe</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rosier</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Michiels</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Marchal</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>The kinetic occipital region in human visual cortex</article-title>. <source>Cereb Cortex</source>. <year>1997</year>;<volume>7</volume>(<issue>3</issue>):<fpage>283</fpage>–<lpage>92</lpage>. <object-id pub-id-type="pmid">9143447</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>. <article-title>Comparative mapping of higher visual areas in monkeys and humans</article-title>. <source>Trends Cogn Sci</source>. <year>2004</year>;<volume>8</volume>(<issue>7</issue>):<fpage>315</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2004.05.009" xlink:type="simple">10.1016/j.tics.2004.05.009</ext-link></comment> <object-id pub-id-type="pmid">15242691</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mantini</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Betti</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Perrucci</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Romani</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Corbetta</surname> <given-names>M</given-names></name>, <etal>et al</etal>. SUPP. <article-title>Interspecies activity correlations reveal functional correspondence between monkey and human brain areas</article-title>. <source>Nat Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>3</issue>):<fpage>277</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.1868" xlink:type="simple">10.1038/nmeth.1868</ext-link></comment> <object-id pub-id-type="pmid">22306809</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mantini</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Corbetta</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Romani</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>. <article-title>Data-driven analysis of analogous brain networks in monkeys and humans during natural vision</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>63</volume>(<issue>3</issue>):<fpage>1107</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.08.042" xlink:type="simple">10.1016/j.neuroimage.2012.08.042</ext-link></comment> <object-id pub-id-type="pmid">22992489</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Monkey Cortex through fMRI Glasses</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>83</volume>(<issue>3</issue>):<fpage>533</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.07.015" xlink:type="simple">10.1016/j.neuron.2014.07.015</ext-link></comment> <object-id pub-id-type="pmid">25102559</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakahara</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hayashi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Konishi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Miyashita</surname> <given-names>Y</given-names></name>. <article-title>Functional MRI of Macaque Monkeys Performing a Cognitive Set-Shifting Task</article-title>. <source>Science (80-)</source>. <year>2002</year>;<volume>295</volume>(<issue>5559</issue>).</mixed-citation></ref>
<ref id="pbio.2006405.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeYoe</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Carman</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Glickman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wieser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>1996</year>;<volume>93</volume>(<issue>6</issue>):<fpage>2382</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">8637882</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cumming</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>. <article-title>The Physiology of Stereopsis</article-title>. <source>Annu Rev Neurosci</source>. <year>2001</year>;<volume>24</volume>(<issue>1</issue>):<fpage>203</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schiller</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Slocum</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Jao</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Weiner</surname> <given-names>VS</given-names></name>. <article-title>The integration of disparity, shading and motion parallax cues for depth perception in humans and monkeys</article-title>. <source>Brain Res</source>. <year>2011</year>;<volume>1377</volume>:<fpage>67</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2011.01.003" xlink:type="simple">10.1016/j.brainres.2011.01.003</ext-link></comment> <object-id pub-id-type="pmid">21219887</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Backus</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Fleet</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Parker</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Human Cortical Activity Correlates With Stereoscopic Depth Perception</article-title>. <source>J Neurophysiol</source>. <year>2001</year>;<volume>86</volume>(<issue>4</issue>):<fpage>2054</fpage> LP-2068. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.2001.86.4.2054" xlink:type="simple">10.1152/jn.2001.86.4.2054</ext-link></comment> <object-id pub-id-type="pmid">11600661</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Canon</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Dahmen</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Kourtzi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Neural correlates of disparity-defined shape discrimination in the human brain</article-title>. <source>J Neurophysiol</source>. <year>2007</year>;<volume>97</volume>(<issue>2</issue>):<fpage>1553</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01074.2006" xlink:type="simple">10.1152/jn.01074.2006</ext-link></comment> <object-id pub-id-type="pmid">17151220</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cumming</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Parker</surname> <given-names>AJ</given-names></name>. <article-title>Responses of primary visual cortical neurons to binocular disparity without depth perception</article-title>. <source>Nature</source>. <year>1997</year>;<volume>389</volume>(<issue>6648</issue>):<fpage>280</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/38487" xlink:type="simple">10.1038/38487</ext-link></comment> <object-id pub-id-type="pmid">9305841</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Preston</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kourtzi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Multivoxel pattern selectivity for perceptually relevant binocular disparities in the human brain</article-title>. <source>J Neurosci</source>. <year>2008</year>;<volume>28</volume>(<issue>44</issue>):<fpage>11315</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2728-08.2008" xlink:type="simple">10.1523/JNEUROSCI.2728-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18971473</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gonzalez</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Perez</surname> <given-names>R</given-names></name>. <article-title>Neural mechanisms underlying stereoscopic vision</article-title>. <source>Prog Neurobiol</source>. <year>1998</year>;<volume>55</volume>(<issue>3</issue>):<fpage>191</fpage>–<lpage>224</lpage>. <object-id pub-id-type="pmid">9643554</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neri</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bridge</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Heeger</surname> <given-names>DJ</given-names></name>. <article-title>Stereoscopic processing of absolute and relative disparity in human visual cortex</article-title>. <source>J Neurophysiol</source>. <year>2004</year>;<volume>92</volume>(<issue>3</issue>):<fpage>1880</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01042.2003" xlink:type="simple">10.1152/jn.01042.2003</ext-link></comment> <object-id pub-id-type="pmid">15331652</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rokers</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Cormack</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>. <article-title>Disparity-and velocity-based signals for three- dimensional motion perception in human MT+</article-title>. <source>Nat Neurosci</source>. <year>2009</year>;<volume>12</volume>.</mixed-citation></ref>
<ref id="pbio.2006405.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Georgieva</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Peeters</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kolster</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>The Processing of Three-Dimensional Shape from Disparity in the Human Brain</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>3</issue>).</mixed-citation></ref>
<ref id="pbio.2006405.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murray</surname> <given-names>SO</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Woods</surname> <given-names>DL</given-names></name>. <article-title>Processing shape, motion and three-dimensional shape-from-motion in the human cortex</article-title>. <source>Cereb Cortex</source>. <year>2003</year>;<volume>13</volume>(<issue>5</issue>):<fpage>508</fpage>–<lpage>16</lpage>. <object-id pub-id-type="pmid">12679297</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Sunaert</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Van Hecke</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marchal</surname> <given-names>G</given-names></name>. <article-title>Human cortical regions involved in extracting depth from motion</article-title>. <source>Neuron</source>. <year>1999</year>;<volume>24</volume>(<issue>4</issue>):<fpage>929</fpage>–<lpage>40</lpage>. <object-id pub-id-type="pmid">10624956</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paradis</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Cornilleau-Pérès</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Droulez</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Van De Moortele</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Lobel</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Berthoz</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Visual perception of motion and 3-D structure from motion: an fMRI study</article-title>. <source>Cereb Cortex</source>. <year>2000</year>;<volume>10</volume>(<issue>8</issue>):<fpage>772</fpage>–<lpage>83</lpage>. <object-id pub-id-type="pmid">10920049</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peuskens</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Claeys</surname> <given-names>KG</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Van Hecke</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Attention to 3-D Shape, 3-D Motion, and Texture in 3-D Structure from Motion Displays</article-title>. <source>J Cogn Neurosci</source>. <year>2004</year>;<volume>16</volume>(<issue>4</issue>):<fpage>665</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892904323057371" xlink:type="simple">10.1162/089892904323057371</ext-link></comment> <object-id pub-id-type="pmid">15165355</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durand</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Nelissen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Joly</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Wardak</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>JF</given-names></name>, <etal>et al</etal>. <article-title>Anterior regions of monkey parietal cortex process visual 3D shape</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>100</volume>(<issue>2</issue>):<fpage>130</fpage>–<lpage>4</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durand</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Peeters</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Norman</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Parietal regions processing visual 3D shape extracted from disparity</article-title>. <source>Neuroimage</source>. <year>2009</year>;<volume>46</volume>(<issue>4</issue>):<fpage>1114</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.03.023" xlink:type="simple">10.1016/j.neuroimage.2009.03.023</ext-link></comment> <object-id pub-id-type="pmid">19303937</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Peuskens</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Denys</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nelissen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Sunaert</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Similarities and differences in motion processing between the human and macaque brain: Evidence from fMRI</article-title>. <source>Neuropsychologia</source>. <year>2003</year>;<volume>41</volume>(<issue>13</issue>):<fpage>1757</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">14527539</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sereno</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Trinath</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Augath</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>. <article-title>Three-Dimensional Shape Representation in Monkey Cortex</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>33</volume>(<issue>4</issue>):<fpage>635</fpage>–<lpage>52</lpage>. <object-id pub-id-type="pmid">11856536</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Sasaki</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Knutsen</surname> <given-names>T a.</given-names></name>, <name name-style="western"><surname>Mandeville</surname> <given-names>JB</given-names></name>, <etal>et al</etal>. <article-title>Stereopsis activates V3A and caudal intraparietal areas in macaques and humans</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>39</volume>(<issue>3</issue>):<fpage>555</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">12895427</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verhoef</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Bohon</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Conway</surname> <given-names>BR</given-names></name>. <article-title>Functional Architecture for Disparity in Macaque Inferior Temporal Cortex and Its Relationship to the Architecture for Faces, Color, Scenes, and Visual Field</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>17</issue>):<fpage>6952</fpage>–<lpage>68</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5079-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5079-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25926470</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Janssen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>. <article-title>Extracting 3D structure from disparity</article-title>. <source>Trends Neurosci</source>. <year>2006</year>;<volume>29</volume>(<issue>8</issue>):<fpage>466</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2006.06.012" xlink:type="simple">10.1016/j.tins.2006.06.012</ext-link></comment> <object-id pub-id-type="pmid">16842865</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>PH</given-names></name>. <article-title>Behavioral assessment of motion parallax and stereopsis as depth cues in rhesus monkeys</article-title>. <source>Vision Res</source>. <year>2002</year>;<volume>42</volume>(<issue>16</issue>):<fpage>1953</fpage>–<lpage>61</lpage>. <object-id pub-id-type="pmid">12160568</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>AJ</given-names></name>. <article-title>Binocular depth perception and the cerebral cortex</article-title>. <source>Nat Rev Neurosci</source>. <year>2007</year>;<volume>8</volume>(<issue>5</issue>):<fpage>379</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2131" xlink:type="simple">10.1038/nrn2131</ext-link></comment> <object-id pub-id-type="pmid">17453018</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srivastava</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>De Maziere</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Janssen</surname> <given-names>P</given-names></name>. <article-title>A Distinct Representation of Three-Dimensional Shape in Macaque Anterior Intraparietal Area: Fast, Metric, and Coarse</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>34</issue>):<fpage>10613</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.6016-08.2009" xlink:type="simple">10.1523/JNEUROSCI.6016-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19710314</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>HD</given-names></name>, <name name-style="western"><surname>Roe</surname> <given-names>AW</given-names></name>. <article-title>A Map for Horizontal Disparity in Monkey V2</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>58</volume>(<issue>3</issue>):<fpage>442</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.02.032" xlink:type="simple">10.1016/j.neuron.2008.02.032</ext-link></comment> <object-id pub-id-type="pmid">18466753</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>. <article-title>The neural basis of depth perception from motion parallax</article-title>. <source>Philos Trans R Soc B Biol Sci</source>. <year>2016</year>;<volume>371</volume>(<issue>1697</issue>):<fpage>20150256</fpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rideaux</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Proscription supports robust perceptual integration by suppression in human visual cortex</article-title>. <source>Nat Commun</source>. <year>2018</year> <month>Apr</month> <day>17</day>;<volume>9</volume>(<issue>1</issue>):<fpage>1502</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-018-03400-y" xlink:type="simple">10.1038/s41467-018-03400-y</ext-link></comment> <object-id pub-id-type="pmid">29666361</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murphy</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Integration of texture and disparity cues to surface slant in dorsal visual cortex</article-title>. <source>J Neurophysiol</source>. <year>2013</year>;<volume>110</volume>(<issue>1</issue>):<fpage>190</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01055.2012" xlink:type="simple">10.1152/jn.01055.2012</ext-link></comment> <object-id pub-id-type="pmid">23576705</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dövencioğlu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schofield</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Perceptual Integration for Qualitatively Different 3-D Cues in the Human Brain</article-title>. <source>Psychologist</source>. <year>2013</year>;<volume>26</volume>(<issue>3</issue>):<fpage>194</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Di Luca</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Muryy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fleming</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Welchman</surname> <given-names>AE</given-names></name>. <article-title>Differential processing of binocular and monocular gloss cues in human visual cortex</article-title>. <source>J Neurophysiol</source>. <year>2016</year>;(<issue>February</issue>):jn.00829.2015.</mixed-citation></ref>
<ref id="pbio.2006405.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenberg</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Angelaki</surname> <given-names>DE</given-names></name>. <article-title>Reliability-dependent contributions of visual orientation cues in parietal cortex</article-title>. <source>Proc Natl Acad Sci</source>. <year>2014</year>;<volume>111</volume>(<issue>50</issue>):<fpage>18043</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1421131111" xlink:type="simple">10.1073/pnas.1421131111</ext-link></comment> <object-id pub-id-type="pmid">25427796</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsutsui</surname> <given-names>K-I</given-names></name>, <name name-style="western"><surname>Sakata</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Naganuma</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Taira</surname> <given-names>M</given-names></name>. <article-title>Neural correlates for perception of 3D surface orientation from texture gradient</article-title>. <source>Science (80-)</source>. <year>2002</year>;<volume>298</volume>(<issue>5592</issue>):<fpage>409</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Claeys</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Nelissen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Smans</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sunaert</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Todd</surname> <given-names>JT</given-names></name>, <etal>et al</etal>. <article-title>Mapping the parietal cortex of human and non-human primates</article-title>. <source>Neuropsychologia</source>. <year>2006</year>;<volume>44</volume>(<issue>13</issue>):<fpage>2647</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2005.11.001" xlink:type="simple">10.1016/j.neuropsychologia.2005.11.001</ext-link></comment> <object-id pub-id-type="pmid">16343560</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schiller</surname> <given-names>PH</given-names></name>. <article-title>The effect of overall stimulus velocity on motion parallax</article-title>. <source>Vis Neurosci</source>. <year>2008</year>;<volume>25</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0952523808080012" xlink:type="simple">10.1017/S0952523808080012</ext-link></comment> <object-id pub-id-type="pmid">18282306</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Howard I, Rogers B. Seeing in depth, volume 2: Depth perception. Ontario, Canada: I. Porteous; 2002.</mixed-citation></ref>
<ref id="pbio.2006405.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ono</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rogers</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Ohmi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ono</surname> <given-names>ME</given-names></name>. <article-title>Dynamic occlusion and motion parallax in depth perception</article-title>. <source>Perception</source>. <year>1988</year>;<volume>17</volume>(<issue>2</issue>):<fpage>255</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p170255" xlink:type="simple">10.1068/p170255</ext-link></comment> <object-id pub-id-type="pmid">3226867</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yoonessi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>CL</given-names></name>. <source>Depth perception from dynamic occlusion in motion parallax: Roles of expansion-compression versus accretion-deletion</source>. <year>2013</year>;<volume>13</volume>:<fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation></ref>
<ref id="pbio.2006405.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lagae</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Raiguel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>GA</given-names></name>. <article-title>Speed and direction selectivity of macaque middle temporal neurons</article-title>. <source>J Neurophysiol</source>. <year>1993</year>;<volume>69</volume>(<issue>1</issue>):<fpage>19</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1993.69.1.19" xlink:type="simple">10.1152/jn.1993.69.1.19</ext-link></comment> <object-id pub-id-type="pmid">8433131</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Uka</surname> <given-names>T</given-names></name>. <article-title>Coding of Horizontal Disparity and Velocity by MT Neurons in the Alert Macaque</article-title>. <source>J Neurophysiol</source>. <year>2003</year>;</mixed-citation></ref>
<ref id="pbio.2006405.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mandeville</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Nelissen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Van Hecke</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rosen</surname> <given-names>BR</given-names></name>, <etal>et al</etal>. <article-title>Visual motion processing investigated using contrast agent-enhanced fMRI in awake behaving monkeys</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>32</volume>(<issue>4</issue>):<fpage>565</fpage>–<lpage>77</lpage>. <object-id pub-id-type="pmid">11719199</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Premereur</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Van Dromme</surname> <given-names>ICL</given-names></name>, <name name-style="western"><surname>Romero</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Janssen</surname> <given-names>P</given-names></name>. <article-title>Effective Connectivity of Depth-Structure–Selective Patches in the Lateral Bank of the Macaque Intraparietal Sulcus</article-title>. <source>PLoS Biol</source>. <year>2015</year>;<volume>13</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002072" xlink:type="simple">10.1371/journal.pbio.1002072</ext-link></comment> <object-id pub-id-type="pmid">25689048</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leite</surname> <given-names>FP</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sasaki</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wald</surname> <given-names>LL</given-names></name>, <etal>et al</etal>. <article-title>Repeated fMRI using iron oxide contrast agent in awake, behaving macaques at 3 Tesla</article-title>. <source>Neuroimage</source>. <year>2002</year>;<volume>16</volume>(<issue>2</issue>):<fpage>283</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/nimg.2002.1110" xlink:type="simple">10.1006/nimg.2002.1110</ext-link></comment> <object-id pub-id-type="pmid">12030817</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Janssens</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Keil</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Farivar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>McNab</surname> <given-names>J a.</given-names></name>, <name name-style="western"><surname>Polimeni</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Gerits</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>An implanted 8-channel array coil for high-resolution macaque MRI at 3T</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>62</volume>(<issue>3</issue>):<fpage>1529</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.05.028" xlink:type="simple">10.1016/j.neuroimage.2012.05.028</ext-link></comment> <object-id pub-id-type="pmid">22609793</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mandeville</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>J-K</given-names></name>, <name name-style="western"><surname>Jarraya</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Rosen</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Jenkins</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>. <article-title>fMRI of Cocaine Self-Administration in Macaques Reveals Functional Inhibition of Basal Ganglia</article-title>. <source>Neuropsychopharmacology</source>. <year>2011</year>;<volume>36</volume>(<issue>6</issue>):<fpage>1187</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/npp.2011.1" xlink:type="simple">10.1038/npp.2011.1</ext-link></comment> <object-id pub-id-type="pmid">21307843</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Janssens</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Popivanov</surname> <given-names>ID</given-names></name>, <name name-style="western"><surname>Vanduffel</surname> <given-names>W</given-names></name>. <article-title>Probabilistic and single-subject retinotopic maps reveal the topographic organization of face patches in the macaque cortex</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>31</issue>):<fpage>10156</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2914-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2914-13.2013</ext-link></comment> <object-id pub-id-type="pmid">25080579</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arcaro</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Pinsk</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>. <article-title>Visuotopic organization of macaque posterior parietal cortex: a functional magnetic resonance imaging study</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>6</issue>):<fpage>2064</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3334-10.2011" xlink:type="simple">10.1523/JNEUROSCI.3334-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21307244</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Martino</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Valente</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Staeren</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ashburner</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goebel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Formisano</surname> <given-names>E</given-names></name>. <article-title>Combining multivariate voxel selection and support vector machines for mapping and classification of fMRI spatial patterns</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>43</volume>(<issue>1</issue>):<fpage>44</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2008.06.037" xlink:type="simple">10.1016/j.neuroimage.2008.06.037</ext-link></comment> <object-id pub-id-type="pmid">18672070</object-id></mixed-citation></ref>
<ref id="pbio.2006405.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Goebel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>P</given-names></name>. <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci</source>. <year>2006</year>;<volume>103</volume>(<issue>10</issue>):<fpage>3863</fpage>–<lpage>3868</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0600244103" xlink:type="simple">10.1073/pnas.0600244103</ext-link></comment> <object-id pub-id-type="pmid">16537458</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>