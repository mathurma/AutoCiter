<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00442</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005034</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Social psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Social psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Game theory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Game theory</subject><subj-group><subject>Prisoner's dilemma</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Game theory</subject><subj-group><subject>Public goods game</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Recreation</subject><subj-group><subject>Games</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Scale-free networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Reinforcement Learning Explains Conditional Cooperation and Its Moody Cousin</article-title>
<alt-title alt-title-type="running-head">Reinforcement Learning Explains Conditional Cooperation and Its Moody Cousin</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ezaki</surname> <given-names>Takahiro</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Horita</surname> <given-names>Yutaka</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4103-0591</contrib-id>
<name name-style="western">
<surname>Takezawa</surname> <given-names>Masanori</given-names></name>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Masuda</surname> <given-names>Naoki</given-names></name>
<xref ref-type="aff" rid="aff007"><sup>7</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Research Center for Advanced Science and Technology, The University of Tokyo, Meguro-ku, Tokyo, Japan</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Japan Society for the Promotion of Science, Kojimachi, Chiyoda-ku, Tokyo, Japan</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>National Institute of Informatics, Hitotsubashi, Chiyoda-ku, Tokyo, Japan</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>JST, ERATO, Kawarabayashi Large Graph Project, c/o Global Research Center for Big Data Mathematics, NII, Chiyoda-ku, Tokyo, Japan</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Behavioral Science, Hokkaido University, Kita-ku, Sapporo, Japan</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Center for Experimental Research in Social Sciences, Hokkaido University, Kita-ku, Sapporo, Japan</addr-line>
</aff>
<aff id="aff007">
<label>7</label>
<addr-line>Department of Engineering Mathematics, University of Bristol, Clifton, Bristol, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Komarova</surname> <given-names>Natalia L.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of California, Irvine, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: NM. Performed the experiments: TE. Analyzed the data: TE. Wrote the paper: TE YH MT NM.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">naoki.masuda@bristol.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>7</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>20</day>
<month>7</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>7</issue>
<elocation-id>e1005034</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>3</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>27</day>
<month>6</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Ezaki et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005034"/>
<abstract>
<p>Direct reciprocity, or repeated interaction, is a main mechanism to sustain cooperation under social dilemmas involving two individuals. For larger groups and networks, which are probably more relevant to understanding and engineering our society, experiments employing repeated multiplayer social dilemma games have suggested that humans often show conditional cooperation behavior and its moody variant. Mechanisms underlying these behaviors largely remain unclear. Here we provide a proximate account for this behavior by showing that individuals adopting a type of reinforcement learning, called aspiration learning, phenomenologically behave as conditional cooperator. By definition, individuals are satisfied if and only if the obtained payoff is larger than a fixed aspiration level. They reinforce actions that have resulted in satisfactory outcomes and anti-reinforce those yielding unsatisfactory outcomes. The results obtained in the present study are general in that they explain extant experimental results obtained for both so-called moody and non-moody conditional cooperation, prisoner’s dilemma and public goods games, and well-mixed groups and networks. Different from the previous theory, individuals are assumed to have no access to information about what other individuals are doing such that they cannot explicitly use conditional cooperation rules. In this sense, myopic aspiration learning in which the unconditional propensity of cooperation is modulated in every discrete time step explains conditional behavior of humans. Aspiration learners showing (moody) conditional cooperation obeyed a noisy GRIM-like strategy. This is different from the Pavlov, a reinforcement learning strategy promoting mutual cooperation in two-player situations.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Laboratory experiments using human participants have shown that, in groups or contact networks, humans often behave as conditional cooperator or its moody variant. Although conditional cooperation in dyadic interaction is well understood, mechanisms underlying these behaviors in group or networks beyond a pair of individuals largely remain unclear. In this study, we show that players adopting a type of reinforcement learning exhibit these conditional cooperation behaviors. The results are general in the sense that the model explains experimental results to date obtained in various situations. It explains moody conditional cooperation, which is a recently discovered behavioral trait of humans, in addition to traditional conditional cooperation. It also explains experimental results obtained with both the prisoner’s dilemma and public goods games and with different population structure. Crucially, our model assumes that individuals do not have access to information about what other individuals are doing such that they cannot explicitly condition their behavior on how many others have previously cooperated. Thus, our results provide a proximate and unified understanding of these experimentally observed patterns.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by: Japan Society for the Promotion of Science (Grant No. 13J05086 to TE and 15K13111 and 25285176 to MT). <ext-link ext-link-type="uri" xlink:href="https://www.jsps.go.jp/english/index.html" xlink:type="simple">https://www.jsps.go.jp/english/index.html</ext-link>; and Japan Science and Technology Agency, Exploratory Research for Advanced Technology, Kawarabayashi Large Graph Project to MT and NM. <ext-link ext-link-type="uri" xlink:href="http://www.jst.go.jp/erato/kawarabayashi/" xlink:type="simple">http://www.jst.go.jp/erato/kawarabayashi/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="13"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Humans very often cooperate with each other when free-riding on others’ efforts is ostensibly lucrative. Among various mechanisms enabling cooperation in social dilemma situations, direct reciprocity, i.e., repeated interaction between a pair of individuals, is widespread. If individuals will repeatedly interact, they are motivated to keep on cooperation because no cooperation would invite retaliation by the peer in the succeeding interactions [<xref ref-type="bibr" rid="pcbi.1005034.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref002">2</xref>]. Past theoretical research using the two-player prisoner’s dilemma game (PDG) identified tit-for-tat (TFT) [<xref ref-type="bibr" rid="pcbi.1005034.ref002">2</xref>], generous TFT [<xref ref-type="bibr" rid="pcbi.1005034.ref003">3</xref>], a win-stay lose-shift strategy often called Pavlov [<xref ref-type="bibr" rid="pcbi.1005034.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref006">6</xref>] as representative strong competitors in the repeated two-player PDG.</p>
<p>Direct reciprocity in larger groups corresponds to the individual’s action rule collectively called the conditional cooperation (CC), a multiplayer variant of TFT. By definition, an individual employing CC would cooperate if a large amount of cooperation has been made by other group members. In the present study, we study a reinforcement learning model. Depending on the parameter values, the outcome of the learning process shows CC patterns and their variant that have been observed in behavioral experiments.</p>
<p>In fact, the following evidence suggests that the concept and relevance of CC are much more nuanced than in the case of dyadic interactions, calling for examinations. First, early theoretical studies have concluded that CC in the multiplayer PDG is unstable as the group size increases [<xref ref-type="bibr" rid="pcbi.1005034.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref008">8</xref>]. In addition, CC assumed in these and follow-up studies is a threshold behavior (i.e., players cooperate when the number of peers that have cooperated the last time exceeds a prescribed threshold). However, CC patterns and their variants observed in the extant experiments are gradual rather than a threshold behavior [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref017">17</xref>].</p>
<p>Second, the public goods game (PGG) models social dilemmas occurring in a group beyond a pair of individuals. In the repeated PGG, CC has been observed in laboratory experiments with human participants [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref017">17</xref>] and in real society [<xref ref-type="bibr" rid="pcbi.1005034.ref018">18</xref>]. By definition, an individual adopting CC increases the amount of cooperation when others have made large contributions the last time. CC in the repeated PGG with two or more players is theoretically stable under some conditions [<xref ref-type="bibr" rid="pcbi.1005034.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref024">24</xref>]. However, these conditions are not generous and how they connect to the experimental results is not clear. This situation contrasts to that of the two-player, discrete-action PDG mentioned previously, where conditions under which direct reciprocity occurs and strategies enabling them are well characterized [<xref ref-type="bibr" rid="pcbi.1005034.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref005">5</xref>].</p>
<p>Third, recent experiments have discovered that humans show moody conditional cooperation (MCC) behavior in the repeated PDG game played on contact networks [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>]. MCC is defined as follows. MCC is the same as CC if the player has cooperated the last time. If the player has defected the last time, a player adopting MCC decides on the action without taking into account what the neighbors in the contact network have done previously. In this sense, the player’s action rule is moody. The genesis of MCC is not well understood. First, evolutionary dynamics do not promote MCC behavior [<xref ref-type="bibr" rid="pcbi.1005034.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]. Second, non-evolutionary numerical simulations assuming MCC do not intend to explain why MCC emerges or is stable [<xref ref-type="bibr" rid="pcbi.1005034.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref026">26</xref>]. Third, a numerical study employing reinforcement learning [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>] has MCC behavior built in into the model in the sense that MCC occurs whenever cooperation is sustained (see <xref ref-type="sec" rid="sec007">Discussion</xref> for more).</p>
<p>In this article, we provide an account for experimentally observed CC and MCC patterns using a family of reinforcement learning called the aspiration learning [<xref ref-type="bibr" rid="pcbi.1005034.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref036">36</xref>]. In reinforcement learning, players satisfice themselves rather than maximize the payoff in the sense that a player increases and decreases the likelihood of the behavior that has yielded a large and small reward, respectively. In aspiration learning, players are satisfied if and only if the obtained payoff is larger than a threshold. Because the probability to select the behavior, such as cooperation, is dynamically updated in every discrete time step, aspiration learning is different from a conditional strategy in general.</p>
<p>Our main conclusion that reinforcement learning explains CC and MCC resembles that of a previous study [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]. However, the present study is radically different from Ref. [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>] in the following aspects. First, as stated above, MCC behavior is an assumed mode of the model proposed in Ref. [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]. In the present model, players myopically adjust the unconditional probability of cooperation depending on the previous action and reward, as in previous aspiration learning models [<xref ref-type="bibr" rid="pcbi.1005034.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>]. Second, the present model is also simpler, even without assuming players to be aware of the amount of cooperation carried out nearby or to explicitly implement conditional strategies.</p>
<sec id="sec002">
<title>Model</title>
<p>We place a player obeying the reinforcement learning rule on each node of the square lattice with 10 × 10 nodes with periodic boundary conditions. However, the following results do not require particular network structure (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). Each player is involved in the two-player PDG against each of the four neighbors on the network. The game is also interpreted as a PGG played in the group composed of the player and all neighbors submitting binary decisions [<xref ref-type="bibr" rid="pcbi.1005034.ref040">40</xref>]. The game is repeated over <italic>t</italic><sub>max</sub> rounds. We set <italic>t</italic><sub>max</sub> = 25 unless otherwise stated.</p>
<p>Each player selects either to cooperate (C) or defect (D) in each round (<xref ref-type="fig" rid="pcbi.1005034.g001">Fig 1A</xref>). The submitted action (i.e., C or D) is used consistently against all the neighbors. In other words, a player is not allowed to cooperate with one neighbor and defect against another neighbor in the same round. If both players in a pair cooperate, both players gain payoff <italic>R</italic> = 3. If both defect, both gain <italic>P</italic> = 1. If a player cooperates and the other player defects, the defector exploits the cooperator such that the cooperator and defector gain <italic>S</italic> = 0 and <italic>T</italic> = 5, respectively.</p>
<fig id="pcbi.1005034.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005034.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Behavior of the aspiration learner in the repeated PD game.</title>
<p>(A) Payoff matrix. The payoff values for the row player are shown. (B) Concept of satisficing in the aspiration-based reinforcement learning model. The payoff values shown on the horizontal axis are those for the focal player. (C) Relationship between the aspiration level, <italic>A</italic>, and the approximate (un)conditional strategy, given the payoff matrix shown in (A).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.g001" xlink:type="simple"/>
</fig>
<p>Each player is assumed to update the intended probability to cooperate, <italic>p</italic><sub><italic>t</italic></sub>, according to the Bush-Mosteller (BM) model of reinforcement learning [<xref ref-type="bibr" rid="pcbi.1005034.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>] as follows:
<disp-formula id="pcbi.1005034.e001"><alternatives><graphic id="pcbi.1005034.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfenced close="" open="{" separators=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>C</mml:mtext> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>C</mml:mtext> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>D</mml:mtext> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>D</mml:mtext> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>a</italic><sub><italic>t</italic>−1</sub> is the action in the (<italic>t</italic>− 1)th round, and <italic>s</italic><sub><italic>t</italic>−1</sub> is the stimulus that drives learning (−1 &lt; <italic>s</italic><sub><italic>t</italic>−1</sub> &lt; 1). The current action is reinforced and suppressed if <italic>s</italic><sub><italic>t</italic>−1</sub> &gt; 0 and <italic>s</italic><sub><italic>t</italic>−1</sub> &lt; 0, respectively. For example, the first line on the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1005034.e001">Eq (1)</xref> states that the player increases the probability to cooperate if it has cooperated and been satisfied in the previous round. The multiplicative factor (1 − <italic>p</italic><sub><italic>t</italic>−1</sub>) is imposed to respect the constraint <italic>p</italic><sub><italic>t</italic></sub> &lt; 1.</p>
<p>The stimulus is defined by
<disp-formula id="pcbi.1005034.e002"><alternatives><graphic id="pcbi.1005034.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close="]" open=" [" separators=""><mml:mi>β</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>r</italic><sub><italic>t</italic>−1</sub> is the payoff to the player in round <italic>t</italic> − 1, averaged over the four neighboring players, <italic>A</italic> is the aspiration level, and <italic>β</italic>(&gt; 0) controls the sensitivity of <italic>s</italic><sub><italic>t</italic>−1</sub> to <italic>r</italic><sub><italic>t</italic>−1</sub> − <italic>A</italic> [<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>]. The player is satisfied and dissatisfied if <italic>r</italic><sub><italic>t</italic>−1</sub> − <italic>A</italic> &gt; 0 (i.e., <italic>s</italic><sub><italic>t</italic>−1</sub> &gt; 0) and <italic>r</italic><sub><italic>t</italic>−1</sub> − <italic>A</italic> &lt; 0 (i.e., <italic>s</italic><sub><italic>t</italic>−1</sub> &lt; 0), respectively (<xref ref-type="fig" rid="pcbi.1005034.g001">Fig 1B</xref>). The so-called Pavlov strategy corresponds to <italic>β</italic> = ∞ and <italic>P</italic> &lt; <italic>A</italic> &lt; <italic>R</italic> [<xref ref-type="bibr" rid="pcbi.1005034.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref005">5</xref>] (<xref ref-type="fig" rid="pcbi.1005034.g001">Fig 1C</xref>). The so-called GRIM strategy, which starts with cooperation and turns into permanent defection (if without noise) once the player is defected [<xref ref-type="bibr" rid="pcbi.1005034.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref041">41</xref>], corresponds to <italic>β</italic> = ∞ and <italic>S</italic> &lt; <italic>A</italic> &lt; <italic>R</italic> [<xref ref-type="bibr" rid="pcbi.1005034.ref038">38</xref>]. When <italic>β</italic> &lt; ∞, which we assume, the behavior realized by the BM model is not an exact conditional strategy such as Pavlov or GRIM, but an approximate one. Unlike some previous studies in which <italic>A</italic> adaptively changes over time [<xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>], we assume that <italic>A</italic> is fixed.</p>
<p>In each round, each player is assumed to misimplement the decision with probability <italic>ϵ</italic> [<xref ref-type="bibr" rid="pcbi.1005034.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>]. Therefore, the actual probability to cooperate in round <italic>t</italic> is given by <inline-formula id="pcbi.1005034.e003"><alternatives><graphic id="pcbi.1005034.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>≡</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. We set <italic>ϵ</italic> = 0.2 and the initial probability of cooperation <italic>p</italic><sub>1</sub> = 0.5 unless otherwise stated.</p>
</sec>
</sec>
<sec id="sec003" sec-type="results">
<title>Results</title>
<sec id="sec004">
<title>Prisoner’s dilemma game</title>
<p>For <italic>A</italic> = 0.5 and <italic>A</italic> = 1.5, the realized probability of cooperation, <inline-formula id="pcbi.1005034.e005"><alternatives><graphic id="pcbi.1005034.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, averaged over the players and simulations is shown in <xref ref-type="fig" rid="pcbi.1005034.g002">Fig 2A</xref> up to 100 rounds. Due to a relatively large initial probability of cooperation, <italic>p</italic><sub>1</sub> = 0.5, <inline-formula id="pcbi.1005034.e006"><alternatives><graphic id="pcbi.1005034.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> drops within the first ≈20 rounds and stays at the same level afterwards for both <italic>A</italic> values. This pattern is roughly consistent with behavioral results obtained in laboratory experiments [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref042">42</xref>].</p>
<fig id="pcbi.1005034.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005034.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Probability of cooperation in the repeated PDG game on the square lattice having 10 × 10 nodes.</title>
<p>(A) Mean time courses of the actual probability of cooperation, <inline-formula id="pcbi.1005034.e004"><alternatives><graphic id="pcbi.1005034.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>. The lines represent the actual probability of cooperation averaged over the 10<sup>2</sup> players and 10<sup>3</sup> simulations. We set <italic>β</italic> = 0.2 and <italic>A</italic> = 0.5. The shaded regions represent the error bar calculated as one standard deviation. (B) Probability of cooperation for various values of the sensitivity of the stimulus to the reward, <italic>β</italic>, and the aspiration level, <italic>A</italic>. The shown values are averages over the 10<sup>2</sup> players, the first <italic>t</italic><sub>max</sub> = 25 rounds, and 10<sup>3</sup> simulations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.g002" xlink:type="simple"/>
</fig>
<p>For a range of the two main parameters, the sensitivity of the stimulus to the reward (i.e., <italic>β</italic>) and the aspiration level setting the satisfaction threshold for players (i.e., <italic>A</italic>), <inline-formula id="pcbi.1005034.e007"><alternatives><graphic id="pcbi.1005034.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> averaged over the first 25 rounds is shown in <xref ref-type="fig" rid="pcbi.1005034.g002">Fig 2B</xref>. The figure indicates that cooperation is frequent when <italic>β</italic> is large, which is consistent with the previous results [<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>], and when <italic>A</italic> is less than ≈1. The probability of cooperation is also relatively large when <italic>A</italic> is larger than ≈2. In this situation, defection leads to an unsatisfactory outcome unless at least two out of the four neighbors cooperate (<xref ref-type="fig" rid="pcbi.1005034.g001">Fig 1B</xref>). Because this does not happen often, a player would frequently switch between defection and cooperation, leading to <inline-formula id="pcbi.1005034.e008"><alternatives><graphic id="pcbi.1005034.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>The results shown in <xref ref-type="fig" rid="pcbi.1005034.g002">Fig 2B</xref> were largely unchanged when we varied <italic>t</italic><sub>max</sub> and <italic>ϵ</italic> (Fig B in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>).</p>
<p>The probability of cooperation, <inline-formula id="pcbi.1005034.e009"><alternatives><graphic id="pcbi.1005034.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, is plotted against the fraction of cooperating neighbors in the previous round, denoted by <italic>f</italic><sub>C</sub>, for <italic>A</italic> = 0.5 and two values of <italic>β</italic> in <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3A and 3B</xref>. The results not conditioned on the action of the player in the previous round are shown by the circles. The player is more likely to cooperate when more neighbors cooperate, consistent with CC patterns reported in experiments with the PDG on the square lattice [<xref ref-type="bibr" rid="pcbi.1005034.ref042">42</xref>]. CC is particularly pronounced at a large value of <italic>β</italic> (<xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3B</xref> as compared to <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3A</xref>).</p>
<fig id="pcbi.1005034.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005034.g003</object-id>
<label>Fig 3</label>
<caption>
<title>CC and MCC in the repeated PDG on the square lattice.</title>
<p>The actual probability of cooperation, <inline-formula id="pcbi.1005034.e010"><alternatives><graphic id="pcbi.1005034.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, is plotted against the fraction of cooperative neighbors in the previous round, <italic>f</italic><sub>C</sub>. The error bars represent the mean ± standard deviation calculated on the basis of all players, <italic>t</italic><sub>max</sub> = 25 rounds, and 10<sup>3</sup> simulations. The circles represent the results not conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub>. The triangles and the squares represent the results conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = C and <italic>a</italic><sub><italic>t</italic>−1</sub> = D, respectively. We set (A) <italic>β</italic> = 0.1 and <italic>A</italic> = 0.5, (B) <italic>β</italic> = 0.4 and <italic>A</italic> = 0.5, (C) <italic>β</italic> = 0.4 and <italic>A</italic> = 2.0, and (D) <italic>β</italic> = 0.4 and <italic>A</italic> = −1.0.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.g003" xlink:type="simple"/>
</fig>
<p>The relationship between <inline-formula id="pcbi.1005034.e011"><alternatives><graphic id="pcbi.1005034.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and <italic>f</italic><sub>C</sub> conditioned on the last action of the focal player, denoted by <italic>a</italic><sub><italic>t</italic>−1</sub>, is shown by the triangles and squares. We observe clear MCC patterns, particularly for a large <italic>β</italic>. In other words, players that have previously cooperated (i.e., <italic>a</italic><sub><italic>t</italic>−1</sub> = C) show CC, whereas the probability of cooperation stays constant or mildly decreases as <italic>f</italic><sub>C</sub> increases when the player has previously defected (i.e., <italic>a</italic><sub><italic>t</italic>−1</sub> = D). These MCC patterns are consistent with the extant experimental results [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>].</p>
<p>In the experiments, MCC has also been observed for different population structure such as the scale-free network [<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>] and a dynamically changing network [<xref ref-type="bibr" rid="pcbi.1005034.ref014">14</xref>]. We carried out numerical simulations on the regular random graph (i.e., random graph in which all nodes have the same degree, or the number of neighbors) with degree four and the well-mixed group of five players in which each player had four partners. The results remained qualitatively the same as those for the square lattice, suggesting robustness of the present numerical results with respect to the network structure (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). Spatial or network reciprocity is not needed for the present model to show MCC patterns.</p>
<p>A different aspiration level, <italic>A</italic>, produces different patterns. CC and MCC patterns are lost when we set <italic>A</italic> = 2 (<xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3C</xref>), with which the dependence of <inline-formula id="pcbi.1005034.e012"><alternatives><graphic id="pcbi.1005034.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> on <italic>f</italic><sub>C</sub> is small, and <inline-formula id="pcbi.1005034.e013"><alternatives><graphic id="pcbi.1005034.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> when no neighbor has cooperated in the previous round (i.e., <italic>f</italic><sub>C</sub> = 0) is larger for <italic>a</italic><sub><italic>t</italic>−1</sub> = D (squares in <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3C</xref>) than for <italic>a</italic><sub><italic>t</italic>−1</sub> = C (triangles). The latter pattern in particular contradicts the previous behavioral results [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>]. CC and MCC patterns are mostly lost for <italic>A</italic> = −1 as well (<xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3D</xref>). With <italic>A</italic> = −1, the BM player is satisfied by any outcome such that any action is reinforced except for the action implementation error. Therefore, the behavior is insensitive to the reward, or to <italic>f</italic><sub>C</sub>.</p>
<p>To assess the robustness of the results, we scanned a region in the <italic>β</italic> − <italic>A</italic> parameter space. For each combination of <italic>β</italic> and <italic>A</italic> values, we performed linear least-square fits to the relationship between the mean <inline-formula id="pcbi.1005034.e015"><alternatives><graphic id="pcbi.1005034.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and <italic>f</italic><sub>C</sub>, estimating <inline-formula id="pcbi.1005034.e016"><alternatives><graphic id="pcbi.1005034.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi mathvariant="normal">C</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1005034.g004">Fig 4A</xref>). CC is supported if the obtained slope <italic>α</italic><sub>1</sub> is positive when unconditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> (circles in <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3</xref>). MCC is supported if <italic>α</italic><sub>1</sub> is positive when <italic>a</italic><sub><italic>t</italic>−1</sub> = C (triangles in <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3</xref>) and negative or close to zero when <italic>a</italic><sub><italic>t</italic>−1</sub> = D (squares in <xref ref-type="fig" rid="pcbi.1005034.g003">Fig 3</xref>). Intercept <italic>α</italic><sub>2</sub> is equal to the value of <inline-formula id="pcbi.1005034.e017"><alternatives><graphic id="pcbi.1005034.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula> when no neighbor has cooperated in the previous round. The behavioral results suggest that <italic>α</italic><sub>2</sub> is larger when conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = C than on <italic>a</italic><sub><italic>t</italic>−1</sub> = D [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>].</p>
<fig id="pcbi.1005034.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005034.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Search of CC and MCC patterns in the repeated PDG on the square lattice.</title>
<p>(A) Schematic of the linear fit, <inline-formula id="pcbi.1005034.e014"><alternatives><graphic id="pcbi.1005034.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi mathvariant="normal">C</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. (B) Slope <italic>α</italic><sub>1</sub> of the linear fit when not conditioned on the focal player’s previous action, <italic>a</italic><sub><italic>t</italic>−1</sub>. (C) <italic>α</italic><sub>1</sub> when conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = C. (D) <italic>α</italic><sub>1</sub> when conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = D. (E) Difference between the intercept, <italic>α</italic><sub>2</sub>, obtained from the linear fit conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = C and that conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = D. For each combination of the <italic>β</italic> and <italic>A</italic> values, a linear fit was obtained by the least-squares method on the basis of the 10<sup>2</sup> players, <italic>t</italic><sub>max</sub> = 25 rounds, and 10<sup>3</sup> simulations, yielding 2.5 × 10<sup>6</sup> samples in total.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.g004" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1005034.g004">Fig 4B</xref> indicates that the slope <italic>α</italic><sub>1</sub> unconditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> is positive, producing CC, when <italic>A</italic> ≤ 1 and <italic>β</italic> is larger than ≈0.25. However, <italic>α</italic><sub>1</sub> is less positive when <italic>A</italic> is extremely small, i.e., smaller than ≈0. When conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = C, <italic>α</italic><sub>1</sub> is positive, consistent with the MCC patterns, except when <italic>β</italic> is larger than ≈0.5 and <italic>A</italic> is smaller than ≈0 (<xref ref-type="fig" rid="pcbi.1005034.g004">Fig 4C</xref>). When conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> = D, <italic>α</italic><sub>1</sub> is close to zero when <italic>A</italic> ≤ 1 and substantially negative when <italic>A</italic> ≥ 1 (<xref ref-type="fig" rid="pcbi.1005034.g004">Fig 4D</xref>). The difference in the value of <italic>α</italic><sub>2</sub>, the intercept of the linear fit, between the cases <italic>a</italic><sub><italic>t</italic>−1</sub> = C and <italic>a</italic><sub><italic>t</italic>−1</sub> = D is shown in <xref ref-type="fig" rid="pcbi.1005034.g004">Fig 4E</xref>. The figure indicates that this value is non-negative, consistent with the experimental results, only when <italic>A</italic> &lt; 1. To conclude, CC and MCC patterns consistent with the behavioral results are produced when 0 &lt; <italic>A</italic> &lt; 1 and <italic>β</italic> is not too small. We also confirmed that a different implementation of the BM model [<xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>] produced CC and MCC patterns when <italic>A</italic> &lt; 1 (Fig C in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>).</p>
<p>The BM model with <italic>P</italic> &lt; <italic>A</italic> &lt; <italic>R</italic>, i.e., 1 &lt; <italic>A</italic> &lt; 3, corresponds to the Pavlov strategy, which is a strong competitor and facilitator of cooperation in the repeated PDG [<xref ref-type="bibr" rid="pcbi.1005034.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref005">5</xref>]. Our results do not indicate that the Pavlov strategy explains CC and MCC patterns. In fact, the BM model with <italic>S</italic> &lt; <italic>A</italic> &lt; <italic>P</italic> (i.e., 0 &lt; <italic>A</italic> &lt; 1), which is a noisy GRIM-like reinforcement learning, robustly produces CC and MCC patterns. It should be noted that, a noisy GRIM strategy without reinforcement learning components does not produce CC and MCC patterns (Fig D in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). This result suggests an active role of reinforcement learning rather than merely conditional strategies such as the noisy GRIM.</p>
</sec>
<sec id="sec005">
<title>Public goods game</title>
<p>CC behavior has been commonly observed for humans engaged in the repeated PGG in which participants make a graded amount of contribution [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref045">45</xref>]. It should be noted that the player’s action is binary in the PDG. In accordance with the setting of previous experiments [<xref ref-type="bibr" rid="pcbi.1005034.ref010">10</xref>], we consider the following repeated PGG in this section. We assume that four players form a group and repeatedly play the game. In each round, each player receives one monetary unit and determines the amount of contribution to a common pool, denoted by <italic>a</italic><sub><italic>t</italic></sub> ∈ [0, 1]. The sum of the contribution over the four players is multiplied by 1.6 and equally redistributed to them. Therefore, the payoff to a player is equal to <inline-formula id="pcbi.1005034.e018"><alternatives><graphic id="pcbi.1005034.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>4</mml:mn> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>3</mml:mn></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1005034.e019"><alternatives><graphic id="pcbi.1005034.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the contribution by the <italic>j</italic>th other group member in round <italic>t</italic>. The Nash equilibrium is given by no contribution by anybody, i.e., <inline-formula id="pcbi.1005034.e020"><alternatives><graphic id="pcbi.1005034.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (1 ≤ <italic>j</italic> ≤ 3).</p>
<p>We simulated the repeated PGG in which players implemented a variant of the BM model (see <xref ref-type="sec" rid="sec008">Materials and Methods</xref>). Crucially, we introduced a threshold contribution value <italic>X</italic> above which the action was regarded to be cooperative. In other words, an amount of contribution <italic>a</italic><sub><italic>t</italic></sub> ≥ <italic>X</italic> and <italic>a</italic><sub><italic>t</italic></sub> &lt; <italic>X</italic> are defined to be cooperation and defection, respectively. Binarization of the action is necessary for determining the behavior to be reinforced and that to be anti-reinforced.</p>
<p>In <xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5</xref>, the contribution by a player, <italic>a</italic><sub><italic>t</italic></sub>, averaged over the players, rounds, and simulations is plotted against the average contribution by the other group members, which is again denoted by <italic>f</italic><sub>C</sub>(0 ≤ <italic>f</italic><sub>C</sub> ≤ 1). We observe CC behavior for this parameter set when <italic>X</italic> = 0.3 and 0.4 (circles in <xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5A and 5B</xref>, respectively). CC patterns are weak for <italic>X</italic> = 0.5 (<xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5C</xref>). The average contribution by a player as a function of <italic>f</italic><sub>C</sub> and the action of the focal player in the previous round is shown by the triangles and squares in <xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5A–5C</xref>. We find MCC patterns. CC and MCC shown in <xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5</xref> are robustly observed if <italic>β</italic> is larger than ≈0.2, <italic>A</italic> ≤ 1, and 0.1 ≤ <italic>X</italic> ≤ 0.4 (<xref ref-type="fig" rid="pcbi.1005034.g005">Fig 5D–5G</xref> and Fig E in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>).</p>
<fig id="pcbi.1005034.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005034.g005</object-id>
<label>Fig 5</label>
<caption>
<title>CC and MCC patterns in the repeated PGG in a group of four players.</title>
<p>(A)–(C) Contribution by a player (i.e., <italic>a</italic><sub><italic>t</italic></sub>) conditioned on the average contribution by the other group members in the previous round (i.e., <italic>f</italic><sub>C</sub>). We set <italic>β</italic> = 0.4 and <italic>A</italic> = 0.9. (A) <italic>X</italic> = 0.3, (B) <italic>X</italic> = 0.4, and (B) <italic>X</italic> = 0.5. The circles represent the results not conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub>. The triangles and the squares represent the results conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> ≥ <italic>X</italic> and <italic>a</italic><sub><italic>t</italic>−1</sub> &lt; <italic>X</italic>, respectively. (D) Slope <italic>α</italic><sub>1</sub> of the linear fit, <italic>a</italic><sub><italic>t</italic></sub> ≈ <italic>α</italic><sub>1</sub> <italic>f</italic><sub>C</sub> + <italic>α</italic><sub>2</sub>, when not conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub>. (E) <italic>α</italic><sub>1</sub> when conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> ≥ <italic>X</italic>. (F) <italic>α</italic><sub>1</sub> when conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> &lt; <italic>X</italic>. (G) Difference between <italic>α</italic><sub>2</sub> obtained from the linear fit conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> ≥ <italic>X</italic> and that conditioned on <italic>a</italic><sub><italic>t</italic>−1</sub> &lt; <italic>X</italic>. The mean and standard deviation in (A)–(C) and the linear fit used in (D)–(G) were calculated on the basis of the four players, <italic>t</italic><sub>max</sub> = 25 rounds, and 2.5 × 10<sup>4</sup> simulations, yielding 2.5 × 10<sup>6</sup> samples in total.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.g005" xlink:type="simple"/>
</fig>
<p>Directional learning is a reinforcement learning rule often applied to behavioral data in the PGG [<xref ref-type="bibr" rid="pcbi.1005034.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref047">47</xref>] and the PDG [<xref ref-type="bibr" rid="pcbi.1005034.ref048">48</xref>]. By definition, a directional learner keeps increasing (decreasing) the contribution if an increase (decrease) in the contribution in the previous round has yielded a large reward. In a broad parameter region, we did not find CC or MCC behavior with players obeying the directional learning rule (Fig F in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). The present BM model is simpler and more accurate in explaining the experimental results in terms of CC and MCC patterns than directional learning is.</p>
</sec>
<sec id="sec006">
<title>Presence of free riders</title>
<p>So far, we have assumed that all players are aspiration learners. Empirically, strategies depend on individuals in the repeated PDG [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>] and PGG [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref018">18</xref>]. In particular, a substantial portion of participants in the repeated PGG, varying between 2.5% and 33% depending on experiments, is free rider, i.e., unconditional defector [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref050">50</xref>]. Therefore, we performed simulations when BM players and unconditional defectors were mixed. We found that the CC and MCC patterns measured for the learning players did not considerably alter in both PDG and PGG when up to half the players were assumed to be unconditional defectors (Fig G in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>).</p>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>We have provided compelling numerical evidence that the BM model, a relatively simple aspiration-based reinforcement learning model that has been employed in various decision making tasks [<xref ref-type="bibr" rid="pcbi.1005034.ref027">27</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>], explains CC and MCC patterns. On one hand, aspiration learning has offered a proximate mechanism for cooperation [<xref ref-type="bibr" rid="pcbi.1005034.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>]. On the other hand, conditional cooperation in the repeated PGG [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref043">43</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref045">45</xref>] and its moody variant in the repeated PDG on networks [<xref ref-type="bibr" rid="pcbi.1005034.ref013">13</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>] have been consistently observed. Here we provided a connection between aspiration learning and conditional cooperation. Our choice of the parameter values including the number of rounds, the size of the group or neighborhood, and the payoff values, supports the comparison of the present numerical data with the results of behavioral experiments.</p>
<p>We are not the first to provide this link. Cimini and Sánchez have shown that MCC emerges from a BM model [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]. The current results significantly depart from theirs and are fundamentally new as follows.</p>
<p>First, MCC is built in into their model in the sense that every outcome except for a population of unconditional defectors implies MCC patterns. In their model, the linear relationship <italic>p</italic><sub><italic>t</italic></sub> = <italic>α</italic><sub>1</sub> <italic>f</italic><sub>C</sub> + <italic>α</italic><sub>2</sub> after the focal player’s cooperation, where <italic>p</italic><sub><italic>t</italic></sub> is the probability of cooperation and <italic>f</italic><sub>C</sub> is the fraction of cooperation in the neighborhood in the previous round, adaptively changes according to the BM model dynamics. In fact, <italic>α</italic><sub>1</sub> and <italic>α</italic><sub>2</sub> are simultaneously updated under a constraint and take a common value after a transient (<xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>), consistent with their numerical results (Fig 2 in [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]). This relationship yields <italic>p</italic><sub><italic>t</italic></sub> = <italic>α</italic><sub>1</sub>(<italic>f</italic><sub>C</sub> + 1), implying MCC whenever <italic>α</italic><sub>1</sub> &gt; 0. When <italic>α</italic><sub>1</sub> = 0, we obtain <italic>p</italic><sub><italic>t</italic></sub> = 0, i.e., unconditional defection. In contrast, players in our model directly adapt the unconditional probability of cooperation without knowing <italic>f</italic><sub>C</sub> such that there is no room for players to explicitly learn the MCC rule. Therefore, our approach is inherently bottom-up.</p>
<p>Second, our model is cognitively less taxing than the Cimini-Sánchez model. In their model, a player refers to <italic>f</italic><sub>C</sub> and updates the action rule based on its own actions in the last two rounds. Depending on the action that the player has submitted in the second last round, the parameters in one of the two subrules ((<italic>p</italic>, <italic>r</italic>) or <italic>q</italic> in [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>]) are updated. In contrast, as already mentioned, players do not refer to <italic>f</italic><sub>C</sub> in our model. They only refer to their own reward and action in the previous round. A player simply increases or decreases the unconditional probability of cooperation in the next round depending on the amount of satisfaction, as assumed in the previous experimental [<xref ref-type="bibr" rid="pcbi.1005034.ref028">28</xref>] and theoretical [<xref ref-type="bibr" rid="pcbi.1005034.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref039">39</xref>] studies applying aspiration-based reinforcement learning models to social dilemma games.</p>
<p>In Ref. [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>], the Pavlov rather than GRIM rule produced MCC patterns. Our results were the opposite. With Pavlov, CC behavior is lost in our simulations (Figs <xref ref-type="fig" rid="pcbi.1005034.g004">4B</xref> and <xref ref-type="fig" rid="pcbi.1005034.g005">5D</xref>). In addition, a Pavlov player cooperates more often after it has defected than cooperated in the last round (Figs <xref ref-type="fig" rid="pcbi.1005034.g004">4E</xref> and <xref ref-type="fig" rid="pcbi.1005034.g005">5G</xref>), qualitatively contradicting the experimental results. This inconsistency with Pavlov persists even if we use the Macy-Flache reinforcement learning model as in [<xref ref-type="bibr" rid="pcbi.1005034.ref025">25</xref>] (Fig C in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). MCC is intuitively associated with GRIM, not Pavlov, for the following reason. Consider the two-person PDG for simplicity and a player obeying MCC. The player and has obtained payoff <italic>R</italic> (by mutual cooperation; <italic>f</italic><sub>C</sub> = 1), the player would cooperate in the next round. If the same MCC player has obtained payoff <italic>S</italic> (by the player’s unilateral cooperation; <italic>f</italic><sub>C</sub> = 0), the player would defect in the next round. If the player has obtained payoff <italic>P</italic> or <italic>T</italic> (by the player’s defection, i.e., <italic>a</italic><sub><italic>t</italic>−1</sub> = D), the player would next submit <italic>a</italic><sub><italic>t</italic></sub> (= C or D) independently of the previously obtained payoff (i.e., <italic>P</italic> or <italic>T</italic>). If <italic>a</italic><sub><italic>t</italic></sub> = C, the player has flipped the action because <italic>a</italic><sub><italic>t</italic>−1</sub> = D. This MCC behavior is not realizable by the aspiration learning because it requires <italic>S</italic>, <italic>P</italic>, <italic>T</italic> &lt; <italic>A</italic> &lt; <italic>R</italic>, which contradicts the payoff of the PDG, i.e., <italic>S</italic> &lt; <italic>P</italic> &lt; <italic>R</italic> &lt; <italic>T</italic>. If <italic>a</italic><sub><italic>t</italic></sub> = D, the player has not flipped the action. This MCC behavior is realizable by a value of <italic>A</italic> verifying <italic>S</italic> &lt; <italic>A</italic> &lt; <italic>R</italic>, <italic>P</italic>, <italic>T</italic>, which is the GRIM.</p>
<p>The GRIM is not exploited by an unconditional defector. In contrast, the Pavlov is exploited by an unconditional defector every other round because Pavlov players flip between cooperation and defection. In experiments, a substantial fraction of participants unconditionally defects [<xref ref-type="bibr" rid="pcbi.1005034.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1005034.ref050">50</xref>]. The parameters of the aspiration learning may have evolved such that humans behave like noisy GRIM to protect themselves against exploitation by unconditional defectors. It should be noted that the mere GRIM strategy, corresponding to <italic>β</italic> = ∞ and <italic>S</italic> &lt; <italic>A</italic> &lt; <italic>P</italic> in our model, does not produce MCC patterns (Fig D in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). Therefore, an involvement of reinforcement learning seems to be crucial in explaining the behavioral results, at least within the framework of the present model.</p>
<p>Our numerical results indicated MCC in the PGG. Past laboratory experiments using the PGG focused on CC, not MCC, to the best of our knowledge. As pointed out in previous literature [<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>], examining the possibility of MCC patterns in the repeated PGG with experimental data warrants future research. Conversely, applying the BM model and examining the relevance of noisy GRIM in the existing and new experimental data may be fruitful exercises.</p>
<p>The results were insensitive to the population structure (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1005034.s001">S1 Text</xref>). This is in a stark contrast with a range of results in evolutionary games on networks, which generally say that the population structure is a major determinant of evolutionary game dynamics, in particular, the frequency of cooperation [<xref ref-type="bibr" rid="pcbi.1005034.ref051">51</xref>–<xref ref-type="bibr" rid="pcbi.1005034.ref053">53</xref>]. The discrepancy suggests that, under social dilemma games in laboratory experiments, humans may behave differently from the assumptions of evolutionary dynamics. In fact, regular lattices [<xref ref-type="bibr" rid="pcbi.1005034.ref054">54</xref>] and scale-free networks [<xref ref-type="bibr" rid="pcbi.1005034.ref016">16</xref>] do not enhance cooperation in behavioral experiments, which is contrary to the prediction of the evolutionary game theory. In addition, human strategy updating can considerably deviate from those corresponding to major evolutionary rules [<xref ref-type="bibr" rid="pcbi.1005034.ref042">42</xref>]. Aspiration learning provides an attractive alternative to evolutionary rules in approximating human behavior in social dilemma situations and beyond.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec009">
<title>BM model for the PGG</title>
<p>Unlike in the PDG, the action is continuous in the PGG such that the behavior to be reinforced or anti-reinforced is not obvious. Therefore, we modify the BM model for the PDG in the following two aspects. First, we define <italic>p</italic><sub><italic>t</italic></sub> as the expected contribution that the player makes in round <italic>t</italic>. We draw the actual contribution <italic>a</italic><sub><italic>t</italic></sub> from the truncated Gaussian distribution whose mean and standard deviation are equal to <italic>p</italic><sub><italic>t</italic></sub> and 0.2, respectively. If <italic>a</italic><sub><italic>t</italic></sub> falls outside the interval [0, 1], we discard it and redraw <italic>a</italic><sub><italic>t</italic></sub> until it falls within [0, 1]. Second, we introduce a threshold contribution value <italic>X</italic>, distinct from <italic>A</italic>, used for regarding the action to be either cooperative or defective.</p>
<p>We update <italic>p</italic><sub><italic>t</italic></sub> as follows:
<disp-formula id="pcbi.1005034.e021"><alternatives><graphic id="pcbi.1005034.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005034.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfenced close="" open="{" separators=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>X</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>X</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mi>X</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mi>X</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
For example, the first line on the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1005034.e021">Eq (3)</xref> states that, if the player has made a large contribution (hence regarded to be C) and it has been rewarding, the player will increase the expected contribution in the next round. The stimulus, <italic>s</italic><sub><italic>t</italic>−1</sub>, is defined by <xref ref-type="disp-formula" rid="pcbi.1005034.e002">Eq (2)</xref>.</p>
<p>In the numerical simulations, we draw the initial condition, <italic>p</italic><sub>1</sub>, from the uniform density on [0, 1], independently for different players.</p>
</sec>
</sec>
<sec id="sec010">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005034.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005034.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supporting Information for: Reinforcement Learning Explains Conditional Cooperation and Its Moody Cousin.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We acknowledge Hisashi Ohtsuki and Shinsuke Suzuki for valuable comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005034.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Trivers</surname> <given-names>RL</given-names></name>. <article-title>The evolution of reciprocal altruism</article-title>. <source>Q Rev Biol</source>. <year>1971</year>;<volume>46</volume>:<fpage>35</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1086/406755" xlink:type="simple">10.1086/406755</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Axelrod</surname> <given-names>R</given-names></name>. <source>The Evolution of Cooperation</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Basic Books</publisher-name>; <year>1984</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nowak</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>May</surname> <given-names>RM</given-names></name>. <article-title>Evolutionary games and spatial chaos</article-title>. <source>Nature</source>. <year>1992</year>;<volume>359</volume>:<fpage>826</fpage>–<lpage>829</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/359826a0" xlink:type="simple">10.1038/359826a0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kraines</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kraines</surname> <given-names>V</given-names></name>. <article-title>Learning to cooperate with Pavlov: An adaptive strategy for the iterated prisoner’s dilemma with noise</article-title>. <source>Theory Decis</source>. <year>1993</year>;<volume>35</volume>:<fpage>107</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF01074955" xlink:type="simple">10.1007/BF01074955</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nowak</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Sigmund</surname> <given-names>K</given-names></name>. <article-title>A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner’s Dilemma game</article-title>. <source>Nature</source>. <year>1993</year>;<volume>364</volume>:<fpage>56</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/364056a0" xlink:type="simple">10.1038/364056a0</ext-link></comment> <object-id pub-id-type="pmid">8316296</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nowak</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Sigmund</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>El-Sedy</surname> <given-names>E</given-names></name>. <article-title>Automata, repeated games and noise</article-title>. <source>J Math Biol</source>. <year>1995</year>;<volume>33</volume>:<fpage>703</fpage>–<lpage>722</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00184645" xlink:type="simple">10.1007/BF00184645</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boyd</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Richerson</surname> <given-names>PJ</given-names></name>. <article-title>The evolution of reciprocity in sizable groups</article-title>. <source>J Theor Biol</source>. <year>1988</year>;<volume>132</volume>:<fpage>337</fpage>–<lpage>356</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0022-5193(88)80219-4" xlink:type="simple">10.1016/S0022-5193(88)80219-4</ext-link></comment> <object-id pub-id-type="pmid">3226132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Joshi</surname> <given-names>NV</given-names></name>. <article-title>Evolution of cooperation by reciprocation within structured demes</article-title>. <source>J Genet</source>. <year>1987</year>;<volume>66</volume>:<fpage>69</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02934456" xlink:type="simple">10.1007/BF02934456</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keser</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>van Winden</surname> <given-names>F</given-names></name>. <article-title>Conditional cooperation and voluntary contributions to public goods</article-title>. <source>Scand J Econ</source>. <year>2000</year>;<volume>102</volume>:<fpage>23</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9442.00182" xlink:type="simple">10.1111/1467-9442.00182</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fischbacher</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Gächter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fehr</surname> <given-names>E</given-names></name>. <article-title>Are people conditionally cooperative? Evidence from a public goods experiment</article-title>. <source>Econ Lett</source>. <year>2001</year>;<volume>71</volume>:<fpage>397</fpage>–<lpage>404</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0165-1765(01)00394-9" xlink:type="simple">10.1016/S0165-1765(01)00394-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fehr</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Fischbacher</surname> <given-names>U</given-names></name>. <article-title>Social norms and human cooperation</article-title>. <source>Trends Cogn Sci</source>. <year>2004</year>;<volume>8</volume>:<fpage>185</fpage>–<lpage>190</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2004.02.007" xlink:type="simple">10.1016/j.tics.2004.02.007</ext-link></comment> <object-id pub-id-type="pmid">15050515</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref012">
<label>12</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gächter</surname> <given-names>S</given-names></name>. <chapter-title>Conditional cooperation: Behavioral regularities from the lab and the field and their policy implications</chapter-title>. In: <name name-style="western"><surname>Frey</surname> <given-names>BS</given-names></name>, <name name-style="western"><surname>Stutzer</surname> <given-names>A</given-names></name>, editors. <source>Economics and Psychology: A Promising New Cross-disciplinary Field</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2007</year>. pp. <fpage>19</fpage>–<lpage>50</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grujić</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fosco</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Araujo</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cuesta</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Sánchez</surname> <given-names>A</given-names></name>. <article-title>Social experiments in the mesoscale: Humans playing a spatial prisoner’s dilemma</article-title>. <source>PLOS ONE</source>. <year>2010</year>;<volume>5</volume>:<fpage>e13749</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0013749" xlink:type="simple">10.1371/journal.pone.0013749</ext-link></comment> <object-id pub-id-type="pmid">21103058</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grujić</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Röhl</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Semmann</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Milinski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Traulsen</surname> <given-names>A</given-names></name>. <article-title>Consistent strategy updating in spatial and non-spatial behavioral experiments does not promote cooperation in social networks</article-title>. <source>PLOS ONE</source>. <year>2012</year>;<volume>7</volume>:<fpage>e47718</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0047718" xlink:type="simple">10.1371/journal.pone.0047718</ext-link></comment> <object-id pub-id-type="pmid">23185242</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grujić</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>A comparative analysis of spatial Prisoner’s Dilemma experiments: Conditional cooperation and payoff irrelevance</article-title>. <source>Sci Rep</source>. <year>2014</year>;<volume>4</volume>:<fpage>4615</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep04615" xlink:type="simple">10.1038/srep04615</ext-link></comment> <object-id pub-id-type="pmid">24722557</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gracia-Lázaro</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Heterogeneous networks do not promote cooperation when humans play a Prisoner’s Dilemma</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>;<volume>109</volume>:<fpage>12922</fpage>–<lpage>12926</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1206681109" xlink:type="simple">10.1073/pnas.1206681109</ext-link></comment> <object-id pub-id-type="pmid">22773811</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fowler</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Christakis</surname> <given-names>NA</given-names></name>. <article-title>Cooperative behavior cascades in human social networks</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2010</year>;<volume>107</volume>:<fpage>5334</fpage>–<lpage>5338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0913149107" xlink:type="simple">10.1073/pnas.0913149107</ext-link></comment> <object-id pub-id-type="pmid">20212120</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rustagi</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Engel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kosfeld</surname> <given-names>M</given-names></name>. <article-title>Conditional cooperation and costly monitoring explain success in forest commons management</article-title>. <source>Science</source>. <year>2010</year>;<volume>330</volume>:<fpage>961</fpage>–<lpage>965</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1193649" xlink:type="simple">10.1126/science.1193649</ext-link></comment> <object-id pub-id-type="pmid">21071668</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wahl</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Nowak</surname> <given-names>MA</given-names></name>. <article-title>The continuous prisoner’s dilemma: I. Linear reactive strategies</article-title>. <source>J Theor Biol</source>. <year>1999</year>;<volume>200</volume>:<fpage>307</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/jtbi.1999.0996" xlink:type="simple">10.1006/jtbi.1999.0996</ext-link></comment> <object-id pub-id-type="pmid">10527720</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doebeli</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hauert</surname> <given-names>C</given-names></name>. <article-title>Models of cooperation based on the Prisoner’s Dilemma and the Snowdrift game</article-title>. <source>Ecol Lett</source>. <year>2005</year>;<volume>8</volume>:<fpage>748</fpage>–<lpage>766</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1461-0248.2005.00773.x" xlink:type="simple">10.1111/j.1461-0248.2005.00773.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>André</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Day</surname> <given-names>T</given-names></name>. <article-title>Perfect reciprocity is the only evolutionarily stable strategy in the continuous iterated prisoner’s dilemma</article-title>. <source>J Theor Biol</source>. <year>2007</year>;<volume>247</volume>:<fpage>11</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jtbi.2007.02.007" xlink:type="simple">10.1016/j.jtbi.2007.02.007</ext-link></comment> <object-id pub-id-type="pmid">17397874</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Le</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Boyd</surname> <given-names>R</given-names></name>. <article-title>Evolutionary dynamics of the continuous iterated Prisoner’s Dilemma</article-title>. <source>J Theor Biol</source>. <year>2007</year>;<volume>245</volume>:<fpage>258</fpage>–<lpage>267</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jtbi.2006.09.016" xlink:type="simple">10.1016/j.jtbi.2006.09.016</ext-link></comment> <object-id pub-id-type="pmid">17125798</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Takezawa</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Price</surname> <given-names>ME</given-names></name>. <article-title>Revisiting “The revolution of reciprocity in sizable groups”: Continuous reciprocity in the repeated <italic>n</italic>-person prisoner’s dilemma</article-title>. <source>J Theor Biol</source>. <year>2010</year>;<volume>264</volume>:<fpage>188</fpage>–<lpage>196</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jtbi.2010.01.028" xlink:type="simple">10.1016/j.jtbi.2010.01.028</ext-link></comment> <object-id pub-id-type="pmid">20144622</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guttman</surname> <given-names>JM</given-names></name>. <article-title>On the evolution of conditional cooperation</article-title>. <source>Eur J Polit Econ</source>. <year>2013</year>;<volume>30</volume>:<fpage>15</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ejpoleco.2012.11.003" xlink:type="simple">10.1016/j.ejpoleco.2012.11.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cimini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sánchez</surname> <given-names>A</given-names></name>. <article-title>Learning dynamics explains human behaviour in Prisoner’s Dilemma on networks</article-title>. <source>J R Soc Interface</source>. <year>2014</year>;<volume>11</volume>:<fpage>20131186</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsif.2013.1186" xlink:type="simple">10.1098/rsif.2013.1186</ext-link></comment> <object-id pub-id-type="pmid">24554577</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gracia-Lázaro</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Cuesta</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Sánchez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moreno</surname> <given-names>Y</given-names></name>. <article-title>Human behavior in Prisoner’s Dilemma experiments suppresses network reciprocity</article-title>. <source>Sci Rep</source>. <year>2012</year>;<volume>2</volume>:<fpage>325</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep00325" xlink:type="simple">10.1038/srep00325</ext-link></comment> <object-id pub-id-type="pmid">22439103</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref027">
<label>27</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bush</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Mosteller</surname> <given-names>F</given-names></name>. <source>Stochastic Models for Learning</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1955</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref028">
<label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rapoport</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chammab</surname> <given-names>AM</given-names></name>. <source>Prisoner’s Dilemma: A Study in Conflict and Cooperation</source>. <publisher-loc>Ann Arbor</publisher-loc>: <publisher-name>University of Michigan Press</publisher-name>; <year>1965</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macy</surname> <given-names>MW</given-names></name>. <article-title>Learning to cooperate: Stochastic and tacit collusion in social exchange</article-title>. <source>Am J Sociol</source>. <year>1991</year>;<volume>97</volume>:<fpage>808</fpage>–<lpage>843</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1086/229821" xlink:type="simple">10.1086/229821</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Fudenberg</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Levine</surname> <given-names>DK</given-names></name>. <source>The Theory of Learning in Games</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bendor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mookherjee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ray</surname> <given-names>D</given-names></name>. <article-title>Aspiration-based reinforcement learning in repeated interaction games: An overview</article-title>. <source>Int Game Theory Rev</source>. <year>2001</year>;<volume>3</volume>:<fpage>159</fpage>–<lpage>174</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0219198901000348" xlink:type="simple">10.1142/S0219198901000348</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macy</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Flache</surname> <given-names>A</given-names></name>. <article-title>Learning dynamics in social dilemmas</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2002</year>;<volume>99</volume>:<fpage>7229</fpage>–<lpage>7236</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.092080099" xlink:type="simple">10.1073/pnas.092080099</ext-link></comment> <object-id pub-id-type="pmid">12011402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bendor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Diermeier</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ting</surname> <given-names>M</given-names></name>. <article-title>A behavioral model of turnout</article-title>. <source>Am Polit Sci Rev</source>. <year>2003</year>;<volume>97</volume>:<fpage>261</fpage>–<lpage>280</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0003055403000662" xlink:type="simple">10.1017/S0003055403000662</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref034">
<label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Duffy</surname> <given-names>J</given-names></name>. <chapter-title>Agent-based models and human subject experiments</chapter-title>. In: <name name-style="western"><surname>Tesfatsion</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Judd</surname> <given-names>KL</given-names></name>, editors. <source>Handbook of Computational Economics</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>North-Holland</publisher-name>; <year>2006</year>. pp. <fpage>949</fpage>–<lpage>1011</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fowler</surname> <given-names>JH</given-names></name>. <article-title>Habitual voting and behavioral turnout</article-title>. <source>J Polit</source>. <year>2006</year>;<volume>68</volume>:<fpage>335</fpage>–<lpage>344</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1468-2508.2006.00410.x" xlink:type="simple">10.1111/j.1468-2508.2006.00410.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rische</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Komarova</surname> <given-names>NL</given-names></name>. <article-title>Regularization of languages by adults and children: A mathematical framework</article-title>. <source>Cogn Psychol</source>. <year>2016</year>;<volume>84</volume>:<fpage>1</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cogpsych.2015.10.001" xlink:type="simple">10.1016/j.cogpsych.2015.10.001</ext-link></comment> <object-id pub-id-type="pmid">26580218</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Karandikar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mookherjee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ray</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Vega-Redondo</surname> <given-names>F</given-names></name>. <article-title>Evolving aspirations and cooperation</article-title>. <source>J Econ Theory</source>. <year>1998</year>;<volume>80</volume>:<fpage>292</fpage>–<lpage>331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/jeth.1997.2379" xlink:type="simple">10.1006/jeth.1997.2379</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Posch</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pichler</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sigmund</surname> <given-names>K</given-names></name>. <article-title>The efficiency of adapting aspiration levels</article-title>. <source>Proc R Soc B</source>. <year>1999</year>;<volume>266</volume>:<fpage>1427</fpage>–<lpage>1435</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1999.0797" xlink:type="simple">10.1098/rspb.1999.0797</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Masuda</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nakamura</surname> <given-names>M</given-names></name>. <article-title>Numerical analysis of a reinforcement learning model with the dynamic aspiration level in the iterated Prisoner’s dilemma</article-title>. <source>J Theor Biol</source>. <year>2011</year>;<volume>278</volume>:<fpage>55</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jtbi.2011.03.005" xlink:type="simple">10.1016/j.jtbi.2011.03.005</ext-link></comment> <object-id pub-id-type="pmid">21397610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pacheco</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Santos</surname> <given-names>FC</given-names></name>, <name name-style="western"><surname>Souza</surname> <given-names>MO</given-names></name>, <name name-style="western"><surname>Skyrms</surname> <given-names>B</given-names></name>. <article-title>Evolutionary dynamics of collective action in <italic>N</italic>-person stag hunt dilemmas</article-title>. <source>Proc R Soc B</source>. <year>2009</year>;<volume>276</volume>:<fpage>315</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.2008.1126" xlink:type="simple">10.1098/rspb.2008.1126</ext-link></comment> <object-id pub-id-type="pmid">18812288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>JW</given-names></name>. <article-title>Non-cooperative equilibrium for supergames</article-title>. <source>Rev Econ Stud</source>. <year>1971</year>;<volume>38</volume>:<fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2296617" xlink:type="simple">10.2307/2296617</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Traulsen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Semmann</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sommerfeld</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Krambeck</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Milinski</surname> <given-names>M</given-names></name>. <article-title>Human strategy updating in evolutionary games</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2010</year>;<volume>107</volume>:<fpage>2962</fpage>–<lpage>2966</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0912515107" xlink:type="simple">10.1073/pnas.0912515107</ext-link></comment> <object-id pub-id-type="pmid">20142470</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kurzban</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Houser</surname> <given-names>D</given-names></name>. <article-title>Experiments investigating cooperative types in humans: A complement to evolutionary theory and simulations</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2005</year>;<volume>102</volume>:<fpage>1803</fpage>–<lpage>1807</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0408759102" xlink:type="simple">10.1073/pnas.0408759102</ext-link></comment> <object-id pub-id-type="pmid">15665099</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Herrmann</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Thöni</surname> <given-names>C</given-names></name>. <article-title>Measuring conditional cooperation: A replication study in Russia</article-title>. <source>Exp Econ</source>. <year>2009</year>;<volume>12</volume>:<fpage>87</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10683-008-9197-1" xlink:type="simple">10.1007/s10683-008-9197-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chaudhuri</surname> <given-names>A</given-names></name>. <article-title>Sustaining cooperation in laboratory public goods experiments: A selective survey of the literature</article-title>. <source>Exp Econ</source>. <year>2011</year>;<volume>14</volume>:<fpage>47</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10683-010-9257-1" xlink:type="simple">10.1007/s10683-010-9257-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Burton-Chellew</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Nax</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>West</surname> <given-names>SA</given-names></name>. <article-title>Payoff-based learning explains the decline in cooperation in public goods games</article-title>. <source>Proc R Soc B</source>. <year>2015</year>;<volume>282</volume>:<fpage>20142678</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.2014.2678" xlink:type="simple">10.1098/rspb.2014.2678</ext-link></comment> <object-id pub-id-type="pmid">25589609</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nax</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Perc</surname> <given-names>M</given-names></name>. <article-title>Directional learning and the provisioning of public goods</article-title>. <source>Sci Rep</source>. <year>2015</year>;<volume>5</volume>:<fpage>8010</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep08010" xlink:type="simple">10.1038/srep08010</ext-link></comment> <object-id pub-id-type="pmid">25619192</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Selten</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Stoecker</surname> <given-names>R</given-names></name>. <article-title>End behavior in sequences of finite prisoner’s dilemma supergames: A learning theory approach</article-title>. <source>J Econ Behav Organ</source>. <year>1986</year>;<volume>7</volume>:<fpage>47</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0167-2681(86)90021-1" xlink:type="simple">10.1016/0167-2681(86)90021-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kurzban</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Houser</surname> <given-names>D</given-names></name>. <article-title>Individual differences in cooperation in a circular public goods game</article-title>. <source>Eur J Pers</source>. <year>2001</year>;<volume>15</volume>:<fpage>37</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/per.420" xlink:type="simple">10.1002/per.420</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fischbacher</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Gächter</surname> <given-names>S</given-names></name>. <article-title>Social preference, beliefs and the dynamics of free riding in public goods experiments</article-title>. <source>Am Econ Rev</source>. <year>2010</year>;<volume>100</volume>:<fpage>541</fpage>–<lpage>556</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1257/aer.100.1.541" xlink:type="simple">10.1257/aer.100.1.541</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref051">
<label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Nowak</surname> <given-names>MA</given-names></name>. <source>Evolutionary Dynamics</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005034.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Szabó</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Fáth</surname> <given-names>G</given-names></name>. <article-title>Evolutionary games on graphs</article-title>. <source>Phys Rep</source>. <year>2007</year>;<volume>446</volume>:<fpage>97</fpage>–<lpage>216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.physrep.2007.04.004" xlink:type="simple">10.1016/j.physrep.2007.04.004</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perc</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gómez-Gardeñes</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Szolnoki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Floría</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Moreno</surname> <given-names>Y</given-names></name>. <article-title>Evolutionary dynamics of group interactions on structured populations: A review</article-title>. <source>J R Soc Interface</source>. <year>2013</year>;<volume>10</volume>:<fpage>20120997</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsif.2012.0997" xlink:type="simple">10.1098/rsif.2012.0997</ext-link></comment> <object-id pub-id-type="pmid">23303223</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005034.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kirchkamp</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nagel</surname> <given-names>R</given-names></name>. <article-title>Naive learning and cooperation in network experiments</article-title>. <source>Games Econ Behav</source>. <year>2007</year>;<volume>58</volume>:<fpage>269</fpage>–<lpage>292</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.geb.2006.04.002" xlink:type="simple">10.1016/j.geb.2006.04.002</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>