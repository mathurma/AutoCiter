<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-00692</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005137</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neuronal plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Excitatory postsynaptic potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Excitatory postsynaptic potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Unsupervised Learning in an Ensemble of Spiking Neural Networks Mediated by ITDP</article-title>
<alt-title alt-title-type="running-head">Ensemble Learning Neural Network by ITDP</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0883-7150</contrib-id>
<name name-style="western">
<surname>Shim</surname> <given-names>Yoonsik</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5503-0467</contrib-id>
<name name-style="western">
<surname>Philippides</surname> <given-names>Andrew</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Staras</surname> <given-names>Kevin</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5164-4004</contrib-id>
<name name-style="western">
<surname>Husbands</surname> <given-names>Phil</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Centre for Computational Neuroscience and Robotics, University of Sussex, Falmer, Brighton, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Neuroscience, School of Life Sciences, University of Sussex, Falmer, Brighton, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bush</surname> <given-names>Daniel</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item><p><bold>Conceptualization:</bold> YS PH AP.</p></list-item>
<list-item><p><bold>Formal analysis:</bold> YS PH.</p></list-item>
<list-item><p><bold>Funding acquisition:</bold> PH KS.</p></list-item>
<list-item><p><bold>Investigation:</bold> YS.</p></list-item>
<list-item><p><bold>Methodology:</bold> YS PH AP KS.</p></list-item>
<list-item><p><bold>Project administration:</bold> PH.</p></list-item>
<list-item><p><bold>Software:</bold> YS PH.</p></list-item>
<list-item><p><bold>Supervision:</bold> PH AP KS.</p></list-item>
<list-item><p><bold>Visualization:</bold> YS.</p></list-item>
<list-item><p><bold>Writing – original draft:</bold> YS PH.</p></list-item>
<list-item><p><bold>Writing – review &amp; editing:</bold> YS AP KS PH.</p></list-item>
</list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">y.s.shim@sussex.ac.uk</email> (YS); <email xlink:type="simple">philh@sussex.ac.uk</email> (PH)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>19</day>
<month>10</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>10</issue>
<elocation-id>e1005137</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>4</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>9</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Shim et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005137"/>
<abstract>
<p>We propose a biologically plausible architecture for unsupervised ensemble learning in a population of spiking neural network classifiers. A mixture of experts type organisation is shown to be effective, with the individual classifier outputs combined via a gating network whose operation is driven by input timing dependent plasticity (ITDP). The ITDP gating mechanism is based on recent experimental findings. An abstract, analytically tractable model of the ITDP driven ensemble architecture is derived from a logical model based on the probabilities of neural firing events. A detailed analysis of this model provides insights that allow it to be extended into a full, biologically plausible, computational implementation of the architecture which is demonstrated on a visual classification task. The extended model makes use of a style of spiking network, first introduced as a model of cortical microcircuits, that is capable of Bayesian inference, effectively performing expectation maximization. The unsupervised ensemble learning mechanism, based around such spiking expectation maximization (SEM) networks whose combined outputs are mediated by ITDP, is shown to perform the visual classification task well and to generalize to unseen data. The combined ensemble performance is significantly better than that of the individual classifiers, validating the ensemble architecture and learning mechanisms. The properties of the full model are analysed in the light of extensive experiments with the classification task, including an investigation into the influence of different input feature selection schemes and a comparison with a hierarchical STDP based ensemble architecture.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Ensemble effects appear to be common in the nervous system. That is, there are many examples of where groups of neurons, or groups of neural circuits, act together to give better performance than is possible from a single neuron or single neural circuit. For instance, there is evidence that ensembles of spatially distinct neural circuits are involved in some classification tasks. Several authors have suggested that architectures for ensemble learning similar to those developed in machine learning and artificial intelligence might be active in the brain, coordinating the activity of populations of classifier circuits. However, to date it has not been clear what kinds of biologically plausible mechanism might underpin such a scheme. Our model shows how such an architecture can be successfully constructed though the use of the rather understudied mechanism of input timing dependent plasticity (ITDP) as a way of coordinating and guiding the activity of a population of model cortical microcircuits. The model is successfully demonstrated on a visual classification task (recognizing hand written integers).</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Project INSIGHT</institution>
</funding-source>
<award-id>308943</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0883-7150</contrib-id>
<name name-style="western">
<surname>Shim</surname> <given-names>Yoonsik</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded by The European Union as part of EU ICT FET FP7 project INSIGHT: Darwinian Neurodynamics (<ext-link ext-link-type="uri" xlink:href="http://insightproject.eu/" xlink:type="simple">http://insightproject.eu/</ext-link>), grant agreement number 308943. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="16"/>
<table-count count="0"/>
<page-count count="41"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>There is growing evidence that many brain mechanisms involved in perception and learning make use of ensemble effects, whereby groups of neurons, or groups of neural circuits, act together to improve performance. At the lowest level of neuronal organisation it appears that the collective activity of groups of neurons is used to overcome the unreliable, stochastic nature of single neuron firing during the learning of motor skills [<xref ref-type="bibr" rid="pcbi.1005137.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref002">2</xref>]. There are also many examples at higher levels of organisation. For instance Li et al. (2008) [<xref ref-type="bibr" rid="pcbi.1005137.ref003">3</xref>] used a combination of functional magnetic resonance imaging and olfactory psychophysics to show that initially indistinguishable odours become discriminable after aversive conditioning, and that during the learning process there were clear, spatially diverse ensemble activity patterns across the primary olfactory (piriform) cortex and in the orbitofrontal cortex. They hypothesized that in this case fear conditioning recruits functionally distinct networks from across the cortex which act in concert to maximize adaptive behaviour. Many others have suggested that the integration of information from multiple sensory modalities and different areas of the cortex, in complex recognition or other cognitive tasks, may involve ensemble learning mechanisms [<xref ref-type="bibr" rid="pcbi.1005137.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref009">9</xref>]. For instance, the influential ‘functional constancy’, or ‘metamodal’, theory of cortical operation [<xref ref-type="bibr" rid="pcbi.1005137.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref011">11</xref>] suggests coordinated action of multiple areas during learning and cognitive processing [<xref ref-type="bibr" rid="pcbi.1005137.ref006">6</xref>]. The hypothesis is that different cortical areas have a core functional, or information processing, specialization, and this is maintained following the loss of a sense, but with a shift in preferred input sensory modality. According to the theory, the relative weights of different sensory input modalities (e.g., vision, touch, hearing) within an area are related to how useful the information in that modality is for the area’s core function (e.g. motion detection, object recognition etc). Information from the different areas is presumably integrated and coordinated by some kind of ensemble mechanisms, especially during periods of adjustment after the loss of a sensory modality (e.g. through blindness) [<xref ref-type="bibr" rid="pcbi.1005137.ref006">6</xref>]. Indeed, these kinds of observations have led to an argument that ensembles of neurons, rather than single neurons, should be viewed as the basic functional unit of the central nervous system [<xref ref-type="bibr" rid="pcbi.1005137.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref015">15</xref>].</p>
<p>The examples above are reminiscent of the kinds of effects seen in both cooperative and competitive ensemble methods known to be effective in machine learning [<xref ref-type="bibr" rid="pcbi.1005137.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref020">20</xref>]. Hence a number of researchers have implemented ensemble models that attempt to reflect aspects of the biology while borrowing ideas and methods from machine learning. These include low-level models concentrating on the oscillatory properties of neuron ensembles, showing how synchronisation dynamics between ensembles can underpin supervised and unsupervised adaptation in a variety of scenarios [<xref ref-type="bibr" rid="pcbi.1005137.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref023">23</xref>], and higher-level models proposing information processing architectures that can be used to coordinate and organise learning in ensembles in the brain [<xref ref-type="bibr" rid="pcbi.1005137.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref006">6</xref>]. In the latter category, mixture of experts (MoE) type architectures [<xref ref-type="bibr" rid="pcbi.1005137.ref024">24</xref>] have been proposed as an interesting candidate for ensemble learning in the cortex and other areas. In particular Bock and Fine (2014) [<xref ref-type="bibr" rid="pcbi.1005137.ref006">6</xref>] have argued that a MoE architecture is a very good fit to the functional constancy theory of cortical operation.</p>
<p>In the artificial neural network literature, ensemble learning on a classification task typically involves multiple continuous value (i.e. on-spiking) artificial neural networks (classifiers) acting in parallel on the same stimuli (pattern to classify), or on different aspects, or modes, of the same overall stimuli. A combined classification from the multiple classifiers, e.g. by majority vote, very often gives better, more reliable performance than that of a single classifier [<xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref020">20</xref>]. The MoE ensemble learning architecture makes use of the input stimuli not only to train the individual classifiers (experts) but also to control the mechanism that combines the outputs of the individual experts into an overall classification. In the classic MoE architecture [<xref ref-type="bibr" rid="pcbi.1005137.ref024">24</xref>], the individual classification outputs of the experts are non-linearly combined via a single gating network which also receives the same input stimuli as the experts (<xref ref-type="fig" rid="pcbi.1005137.g001">Fig 1</xref>). One of the attractions of this architecture is its tendency to cluster input-output patterns into natural groupings, such that each expert can concentrate on a different sub-region of input space (or a different set of sub-problems or ‘tasks’). The gating network tends to guide adaptation in the individual classifiers such that the task space is divided up so as to reduce interference.</p>
<fig id="pcbi.1005137.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The standard MoE architecture.</title>
<p>The outputs (classifications) from the classifier networks are fed into an output unit which combines them according to some simple rule. The gating network weights the individual classifier outputs before they enter the final output unit, and thus guides learning of the overall combined classification. The classifiers and gating networks receive the same input data. See text for further details.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g001" xlink:type="simple"/>
</fig>
<p>The suggestions of MoE type architectures at play in the brain are intriguing but to date there have been no detailed, implementation-level, proposals for biologically plausible, unsupervised, spike-based architectures that exhibits such ensemble learning effects. In this paper, for the first time, we put forward a detailed hypothesis of how experimentally observed neural mechanisms of plasticity can be combined to give an effective and biologically plausible ensemble learning architecture. We demonstrate such an architecture through the computational implementation of a model of unsupervised learning in an ensemble of spiking networks.</p>
<p>One key problem to overcome was how the outputs of multiple networks/areas/‘experts’ could be combined via a non-linear gating mechanism in a biologically plausible way. We propose that a mechanism based on input timing dependent plasticity (ITDP) provides a solution. ITDP, a form of heterosynaptic plasticity activated by correlations between different presynaptic pathways [<xref ref-type="bibr" rid="pcbi.1005137.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>], is a rather understudied mechanisms of plasticity but it has been shown to occur in the cortex [<xref ref-type="bibr" rid="pcbi.1005137.ref027">27</xref>], the cortico-amygdala regions [<xref ref-type="bibr" rid="pcbi.1005137.ref028">28</xref>] involved in the odour discrimination task mentioned earlier [<xref ref-type="bibr" rid="pcbi.1005137.ref003">3</xref>], as well as in the hippocampus [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>]. We argue that it is a good candidate for the kind of coordination needed in biological ensemble learning mechanisms, particularly as it has recently been shown to involve exactly the kind of gating plasticity mechanisms that would be required in our hypothesized architecture [<xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>].</p>
<p>Nessler et al. (2013) [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] recently proposed a spiking model of cortical microcircuits that are able to perform Bayesian inference. They model the soft winner-take-all (WTA) circuits, involving pyramidal neurons inhibiting each other via interneurons, which have been shown to be a common motif of cortical microcircuits [<xref ref-type="bibr" rid="pcbi.1005137.ref031">31</xref>]. A combination of spike timing dependent plasticity (STDP) and activity-dependent changes in the excitability of neurons is able to induce Bayesian information processing in these circuits such that they are able to perform expectation maximisation (EM). The circuits are thus referred to as SEM networks (spiking EM) [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>]. Our ensemble architecture makes use of such SEM networks as the individual ensemble units (classifiers).</p>
<sec id="sec002">
<title>Mixture of Experts</title>
<p>The standard MoE architecture [<xref ref-type="bibr" rid="pcbi.1005137.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref032">32</xref>] used in machine learning is shown in <xref ref-type="fig" rid="pcbi.1005137.g001">Fig 1</xref>. The outputs of an ensemble of <italic>N</italic> classifiers feed into a final decision unit whose output is the combined classification. A separate gating network, with <italic>N</italic> outputs, weights the individual classifier outputs, typically by multiplying them by the corresponding gating output (<xref ref-type="fig" rid="pcbi.1005137.g001">Fig 1</xref>). The final decision unit uses a simple rule (often some variation of the highest weighted classification from the ensemble classifiers) to generate the final classification. The classifiers and the gating network are typically feedforward nets which are trained by a gradient descent algorithm in a supervised manner. In the standard setup the classifiers in the ensemble and the gating network all receive the same input data. The classifiers and the combining mechanism, via the gating network, adapt together, with the gating mechanism helping to ‘guide’ learning. This often leads to some degree of specialization among the ensemble with different classifiers performing better in different areas of the input space. Extensions can include more explicit variation among the classifiers by providing them with different inputs (e.g. different sub samples, or features, of some overall input vector). Techniques such as this can encourage diversity among the classifiers which is generally a good thing in terms of performance [<xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>]. In general, ensemble methods, such as MoE, have been shown to outperform single classifier methods in many circumstances. The combined performance of an ensemble of relatively simple, cheap classifiers is often much better than that of the individual classifiers themselves [<xref ref-type="bibr" rid="pcbi.1005137.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref020">20</xref>].</p>
<p>Our model of ensemble learning in biologically plausible spiking neural networks does not attempt to slavishly follow the methods and structure of the standard MoE architecture, but instead adapts some of the basic underlying principles to produce a MoE like system which can operate according to biologically plausible mechanisms which are based on empirical findings.</p>
</sec>
<sec id="sec003">
<title>Input Timing Dependent Synaptic Plasticity</title>
<p>The term input timing dependent plasticity (ITDP) was first coined in [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] where it was empirically demonstrated in the hippocampus. It is a form of heterosynaptic plasticity—where the activity of a particular neuron leads to changes in the strength of synaptic connections between <italic>another</italic> pair of neurons, rather than its own connections. Classical Hebbian plasticity involves correlations between pre- and post- synaptic activity, specifically activity in the presynaptic cell is causally related to activity in the postsynaptic cell [<xref ref-type="bibr" rid="pcbi.1005137.ref033">33</xref>]. By contrast, ITDP involves synaptic plasticity which is induced by correlations between two presynaptic pathways. Dudman et al. (2007) [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] observed that stimulation of distal perforant path (PP) inputs to hippocampal CA1 pyramidal neurons induced long-term potentiation at the CA1 proximal Schaffer collateral (SC) synapses when the two inputs were paired at a precise interval. The neural system is illustrated in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref> left. Plasticity at the synapse (SC) between neurons CA3 and CA1 is induced when there is a precise interval between stimulations from CA3 and from the distal (PP) perforant pathway from neuron EC in the entorhinal cortex (see timing curve, <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref> left). More recently, Basu et al. (2016) [<xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>] have extended these findings by investigating the role of additional long-range inhibitory projections (LRIPs) from EC to CA1, the function of which were largely unknown. They showed that the LRIPs have a powerful gating role, by disinhibiting intrahippocampal information flow. This enables the induction of plasticity when cortical and hippocampal inputs arrive at CA1 pyramidal neurons with a precise 20ms interval.</p>
<fig id="pcbi.1005137.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Experimentally observed ITDP behaviour (left) (after [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>]), and its simplifications (right) used in this paper.</title>
<p>The original ITDP behaviour is modelled either by a Gaussian (for spiking neural network) or a pulse (for logical voter network) functions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g002" xlink:type="simple"/>
</fig>
<p>Humeau et al. (2003) [<xref ref-type="bibr" rid="pcbi.1005137.ref025">25</xref>] observed a very similar form of heterosynaptic plasticity in the mammalian lateral amygdala. Specifically, simultaneous activation of converging cortical and thalamic afferents induced plasticity. More recently ITDP has been demonstrated in the cortex [<xref ref-type="bibr" rid="pcbi.1005137.ref027">27</xref>] and in the cortico-amygdala regions [<xref ref-type="bibr" rid="pcbi.1005137.ref028">28</xref>]. Another study [<xref ref-type="bibr" rid="pcbi.1005137.ref034">34</xref>] predicted the function of the vestibule-occular reflex gain adaptation by modeling heterosynaptic spike-timing dependent depression from the interaction between vestibular and floccular inputs converging on the medial vestibular nucleus in the cerebellum. Dong et al. (2008) [<xref ref-type="bibr" rid="pcbi.1005137.ref035">35</xref>] also reported a related kind of heterosynaptic plasticity operating in the hippocampus, but on different pathways from those studied by Dudman et al (2007) [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] and Basu et al. (2016) [<xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>]. Thus this, as yet little studied, form of plasticity appears to exist in many of the main brain regions associated with learning and the coordination of information from multiple sensory/internal pathways.</p>
<p>In the above example of ITDP acting in the hippocampus (<xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>), the role of neuron EC in enabling ITDP driven plasticity at synapse SC is somewhat reminiscent of the action of the gating neurons in the MoE architecture outlined in the previous section, especially when we take into account the new findings that the EC to CA1 inhibitory projections do indeed enable a gating mechanism [<xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>]. Moreover, distal projection from the entorhinal cortex to the CA1 region are topographic [<xref ref-type="bibr" rid="pcbi.1005137.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref037">37</xref>] and the enhancement of excitatory postsynaptic potentials (EPSP) is specific to the paired pathway [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>], indicating that only the ITDP synapse which is paired with the distal signal is potentiated. These facts suggest the possibility of specific targeted pathways enabling ‘instructor’ signals. In addition, the EPSP from the distal input is attenuated [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>], meaning that the ‘instructor’ signal would not directly influence any final network output, rather it indirectly influences through ‘instructions’ that enable plasticity. These properties are exactly those needed to operate a biologically plausible spiking MoE type architecture. This led us to the development of such an architecture using an ensemble of spiking networks with ITDP-activating distal connections playing a kind of gating role which allows coordinated learning in the ensemble (these connections are a slight abstraction of the PP and LRIP connections rolled into one, to provide a temporally precise mechanism). This system is described over the following sections and embodies our biologically founded hypothesis of a potential role for ITDP in coordinating ensemble learning.</p>
<p>First a tractable analytic model of the biologically plausible ITDP driven spiking ensemble architecture and its attendant MoE type mechanisms is developed. Derived from a logical model based on the probabilities of neural firing events, this gives insights into the system’s performance and stability. With this knowledge in hand, the analytic model is extended into a full, biologically plausible, computational implementation of the architecture which is demonstrated on a visual classification task (identifying hand written characters). The unsupervised ensemble learning mechanism is shown to perform the task well, with the combined ensemble performance being significantly better than that of the individual classifiers. The properties of the full model are analysed in the light of extensive experiments with the classification task, including an investigation into the influence of different input feature selection schemes and a comparison with a hierarchical STDP-only based ensemble architecture.</p>
</sec>
</sec>
<sec id="sec004" sec-type="results">
<title>Results</title>
<sec id="sec005">
<title>An Analytic Model of a Voter Ensemble Network with ITDP</title>
<p>This section describes the analytic formulation of ITDP driven spiking ensemble learning using probability metrics. The development of such an analytic/logical model serves two purposes: to demonstrate and better understand the mechanisms of spike-based ensemble learning, particularly the coordination of classifier outputs through ITDP, and as the basis of a fast, simplified model which can be used to provide unsupervised learning in an ensemble of arbitrary base classifiers. Later in the paper we extend the proposed model to a more biologically plausible spiking neural network ensemble learning architecture.</p>
<sec id="sec006">
<title>Three neuron ITDP</title>
<p>We developed a tractable model based on the hippocampal system in which Dudman et al. (2007) [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] first demonstrated ITDP empirically. Consider three simplified binary ‘neurons’ which ‘fire’ an event (spike) according to their firing probabilities. The first neuron <italic>k</italic> represents a target neuron which corresponds to the hippocampal CA1 pyramidal cell (<xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>), the second neuron <italic>m</italic> represents a CA3 neuron which projects a fast Schaffer collateral (SC) synapse to the proximal dendrite of <italic>k</italic>, and the last neuron <italic>g</italic> represents a neuron from the entorhinal cortex that projects a distal (PP) synapse via a perforant pathway to the CA1 cell. <italic>g</italic> is modelled as a gating neuron.</p>
<p>For analytical tractability, we first consider a discrete-time based system as an extremely simplified case. We assume output of the system is clocked, where all neurons always give their decisions synchronously by either firing or being silent at every tick. The distal firing delay (20ms) of biological ITDP is eliminated by ignoring the effects of hippocampal trisynaptic transmission delay and the deformation of distal excitatory postsynaptic potentials (EPSPs) due to dendritic propagation. Thus the potentiation of the ITDP synapse occurs only when the two presynaptic neurons fire together at any given time instance. This plasticity rule can be conceptually illustrated by simplifying the original experimental ITDP curve as a pulse-like function (Logical ITDP model in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>), where we can regard the ITDP operation as a logical process which is modelled as “(<italic>m</italic>, <italic>g</italic>) fire together, (<italic>m</italic>, <italic>k</italic>) wire together” in a heterosynaptic way. A model using a Gaussian simplification which takes the proximal-distal spike interval into account (Simplified ITDP model in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>) will be used later for a more detailed, biologically plausible neural network model, where each presynaptic neuron fire a burst of spikes as an output event thus having a range of different spike-timings between two presynaptic neurons. For the time being we concentrate on the logical model which allows us to examine some important intrinsic properties of learning in a spiking ensemble. From this logical simplification, we can express the probabilities of the possible joint events of two presynaptic neurons with independent Bernoulli random variables <italic>m</italic> and <italic>g</italic> at any discrete time instance as:
<disp-formula id="pcbi.1005137.e001"><alternatives><graphic id="pcbi.1005137.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>∧</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula> <disp-formula id="pcbi.1005137.e002"><alternatives><graphic id="pcbi.1005137.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>∨</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula> <disp-formula id="pcbi.1005137.e003"><alternatives><graphic id="pcbi.1005137.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>¬</mml:mo> <mml:mi>m</mml:mi> <mml:mo>∧</mml:mo> <mml:mo>¬</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>∨</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>We assume <italic>m</italic> and <italic>g</italic> to be independent in this simplified illustrative model in line with the (hippocampal) biological case where the input signals for neurons <italic>m</italic> and <italic>g</italic> are assumed to be uncorrelated. This is because whereas <italic>g</italic> receives direct sensory information from EC, <italic>m</italic> receives highly processed information of the same sensory signal through a tri-synaptic path, so the inputs for the two neurons can essentially be assumed to be independent. In the full ensemble models developed later, this assumption holds, to a good level of approximation, as the input vectors for each ensemble classifier are distinct measurements of the raw input data through the use of different feature subsets for each classifier. This issue is discussed further in Methods.</p>
<p>The synaptic weight <italic>w</italic> in this logical model is potentiated by ITDP when both <italic>m</italic> and <italic>g</italic> fire. In order to prevent the unbounded growth of weight strength, we employed the synaptic learning rule from [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], such that the synapse is potentiated by an amount which is inversely exponentially dependant on its weight, whereas it is depressed by a constant amount if only one neuron <italic>m</italic> or <italic>g</italic> fires. If neither of the presynaptic neurons fire, no ITDP is triggered. This self-dependent rule is not intended to model the slight amount of LTD which was originally shown in the outer region of the peak potentiation of the experimental ITDP curve shown by [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] (see <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref> left). Rather, it provides a local mechanism for synaptic normalisation where multiple proximal synapses from a number of <italic>m</italic> neurons compete for the synaptic resources without the unbounded growth of synaptic weights. Also it has been shown that the kind of inversely exponential weight dependency rule used here closely reproduced the pre-post pairing frequency dependent STDP behaviour of biological synapses [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] when used to model STDP. It is expected that this correspondence will also be valid for other types of timing-dependent plasticities such as ITDP. Thus, using this rule, the weight change by ITDP in our logical model is triggered when either one of <italic>m</italic> or <italic>g</italic> or both fire. The change of the weight Δ<italic>w</italic> from neuron <italic>m</italic> to the postsynaptic neuron <italic>f</italic> can be written as:
<disp-formula id="pcbi.1005137.e004"><alternatives><graphic id="pcbi.1005137.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mo>Δ</mml:mo><mml:mi>w</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext>  </mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∧</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext>  </mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∨</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∧</mml:mo><mml:mo>¬</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>∧</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mtext>  </mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise </mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>a</italic> ≥ 1 is a constant which shifts the weight to a positive value. It is evident that the sum of all three probabilities is 1 according to Eqs <xref ref-type="disp-formula" rid="pcbi.1005137.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1005137.e003">3</xref>. From Eqs <xref ref-type="disp-formula" rid="pcbi.1005137.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1005137.e004">4</xref>, we derived the expected value of the weight <italic>w</italic> at equilibrium under constant presynaptic firing probabilities to give the expression in <xref ref-type="disp-formula" rid="pcbi.1005137.e005">Eq 5</xref> (see <xref ref-type="sec" rid="sec015">Methods</xref> for details).
<disp-formula id="pcbi.1005137.e005"><alternatives><graphic id="pcbi.1005137.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mtext>E</mml:mtext> <mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>w</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>Now we have the expected value of <italic>w</italic> at equilibrium expressed in terms of the two probabilities <italic>p</italic>(<italic>m</italic>) and <italic>p</italic>(<italic>g</italic>). It can be seen that the weight converges to the difference of two log probabilities of the events (<italic>m</italic> = 1 and <italic>g</italic> = 1) and (<italic>m</italic> = 1 or <italic>g</italic> = 1) with a shift of log(<italic>a</italic>).</p>
</sec>
</sec>
<sec id="sec007">
<title>Unsupervised Learning in a Spiking Voter Ensemble Network</title>
<p>Next we built an extended logical model for learning the weighted combination of a population (ensemble) of spiking neuronal voters (classifiers) using the simplified ITDP model described earlier. A voter was assumed to have a set of output neurons (one for each class) each of which fires an event (spike) according to its firing probability distribution. The voter follows the mechanism of stochastic winner-takes-all (sWTA), where only a single neuron can fire for any presented input data. The firing probabilities of the neurons in a voter sum up to unity and these probabilities are determined by the input presented to the voter. Therefore, a voter generates a stochastic decision (casts a vote representing the classification) by firing a spike from one of its output neurons whenever an input pattern is presented to the voter. The input pattern shown to the voter can be any neurally coded information (such as an image, sound, or tactile information) which is to be classified by the voter. A pattern given to the voter is regarded as being labeled as belonging to a certain class (c), where the number of existing classes is assumed to be initially known. However, it is unnecessary to relate the absolute value of the class label to the specific neuron index, since any voter neuron can represent an arbitrary data class by firing dominantly. In this abstract model, which was primarily motivated as a vehicle to test the efficacy of ITDP driven coordination of ensemble member outputs, the individual ensemble classifiers were assumed to be fully trained in advance using an arbitrary set of input data. Their tables of firing probabilities (as in <xref ref-type="fig" rid="pcbi.1005137.g003">Fig 3</xref>) effectively represent the posterior probabilities of each class for a given input vector.</p>
<fig id="pcbi.1005137.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g003</object-id>
<label>Fig 3</label>
<caption>
<title>A voter and the voter ensemble network (<italic>N</italic><sub><italic>C</italic></sub> = 4).</title>
<p>(Left) A voter and the predefined firing probabilities of each voter neuron for a set of virtual input samples <italic>X</italic> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>M</italic></sub>}. (Right) The voter ensemble network. The weight <inline-formula id="pcbi.1005137.e006"><alternatives><graphic id="pcbi.1005137.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> represents the weight of connection from the <italic>i</italic>th neuron of the <italic>j</italic>th voter to the <italic>k</italic>th neuron of the final voter.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g003" xlink:type="simple"/>
</fig>
<p>Using the simplified voter model, we can build an analytically tractable voter ensemble network capable of learning the spike-based weighted combination of the individual voters. In other words, learn to combine the individual votes by weighting them appropriately so as to give a better overall classification. The ensemble system consists of three subsystems similar to those in the MoE architecture: an ensemble of voters, a final voter which receives the decisions from the ensemble and combines them to give the final classification output, and a gating voter which guides ITDP between the ensemble and the final voter (<xref ref-type="fig" rid="pcbi.1005137.g003">Fig 3</xref> right). The neurons of all voters in the ensemble project connections to all the neurons in the final voter (c.f. proximal projections from CA3 in the hippocampal case), whereas the gating voter projects topographic (one to one) distal connections to the final voter (<xref ref-type="fig" rid="pcbi.1005137.g003">Fig 3</xref> right, c.f. distal topographic projections from EC in the hippocampal case). Every ensemble voter and the gating voter take their own representation vectors derived either from the same input pattern or from different patterns from distinct input subsets (e.g. different regions of an image). The spikes from the gating voter passing through the topographic distal connection are assumed to have no significant contribution to the final voter output (except indirectly through guiding ITDP). This is because, following the biological data, in our model long range EPSP propagation from distal synapses to the soma is significantly attenuated and therefore has little influence on evoking postsynaptic action potentials [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>].</p>
<p>The gating voter guides ITDP via its topographic projections, which selectively enhance the connection strengths from the ensemble voter neurons representing the same class to one of the final voter neurons (the gating voter’s topographic counterpart) regardless of the ensemble neuron indices. Therefore, the system produces the ‘unsupervised’ weighted combination of ensemble outputs by learning the ITDP weights to reflect the long term co-firing statistics of the ensemble and the gating voter so that the most coherent neuronal paths for a specific class are converged to one of the final voter neurons.</p>
<p>We derived the following analytic solution (<xref ref-type="disp-formula" rid="pcbi.1005137.e007">Eq 6</xref>) for the values of the weights of the ITDP synapses projecting from the voter ensemble to the final voter (<xref ref-type="fig" rid="pcbi.1005137.g003">Fig 3</xref>) under equilibrium (i.e. when they have converged after learning). See <xref ref-type="sec" rid="sec015">Methods</xref> for details of the derivation.
<disp-formula id="pcbi.1005137.e007"><alternatives><graphic id="pcbi.1005137.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>Where <inline-formula id="pcbi.1005137.e008"><alternatives><graphic id="pcbi.1005137.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the firing probability of the <italic>i</italic>th neuron of the <italic>j</italic>th ensemble voter for input sample <italic>x</italic><sub><italic>l</italic></sub>, <inline-formula id="pcbi.1005137.e009"><alternatives><graphic id="pcbi.1005137.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the weight from <inline-formula id="pcbi.1005137.e010"><alternatives><graphic id="pcbi.1005137.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> to the <italic>k</italic>th neuron (<italic>f</italic><sub><italic>k</italic></sub>) of the final voter, and <italic>p</italic>(<italic>g</italic><sub><italic>k</italic></sub>|<italic>x</italic><sub><italic>l</italic></sub>) is the firing probability of the corresponding gating voter neuron which projects to <italic>f</italic><sub><italic>k</italic></sub>.</p>
<p>We also derived an analytic solution for the expected firing probability of a final voter neuron under the presentation of the samples belonging to a particular class as given in <xref ref-type="disp-formula" rid="pcbi.1005137.e011">Eq 7</xref> (see <xref ref-type="sec" rid="sec015">Methods</xref> for derivation).
<disp-formula id="pcbi.1005137.e011"><alternatives><graphic id="pcbi.1005137.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>q</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>S</mml:mi></mml:msub></mml:munderover> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <italic>p</italic>(<italic>f</italic><sub><italic>k</italic></sub>|<italic>c</italic>) is the firing probability of a final voter neuron at <italic>q</italic>th ensemble state <italic>s</italic><sub><italic>q</italic></sub> under presentation of the samples from class <italic>c</italic>, <italic>u</italic><sub><italic>k</italic></sub>(<italic>q</italic>) is the weighted sum of spikes from the ensemble in state <italic>s</italic><sub><italic>q</italic></sub> arriving at the postsynaptic neuron <italic>k</italic>, and <italic>N</italic><sub><italic>C</italic></sub> is the number of classes (see <xref ref-type="sec" rid="sec015">Methods</xref> for full explanation of all terms). This gives the analytic solution of the final voter firing probabilities as a function of joint probabilities of ensemble voter firings under each class presentation. The addition of these expression now gives us a complete analytic spiking ensemble model.</p>
<sec id="sec008">
<title>Validation of analytic solutions by numerical simulation</title>
<p>In order to see if the ensemble architecture performs as expected and to validate the analytic solutions of the voter ensemble network, we compared its results, as derived in the previous section, with a numerical simulation that simply iterated through all the underlying equations of the same model. This validation was deemed worthwhile because the simplified analytical model is based on Bernoulli random variables that simulate per sample firing events. The numerical simulation of the model allowed us to check that the long-term trends and statistics matched those predicted by the analytical solutions. Full details can be found in <xref ref-type="supplementary-material" rid="pcbi.1005137.s001">S1 Text</xref>.</p>
<p>The simple iterative numerical simulation—using abstract input data—did indeed produce very close agreement with the analytic solutions, validating our analytic formulation of expected weight values, and demonstrated that the system performs very well under appropriate parameter settings. By defining a number of parameters that easily allowed us to design a range of differently performing ensembles, the simple numerical simulation also allowed various insights into the overall learning dynamics and the dependence on key factors (ensemble size, gating voter performance, ensemble voter performances). The performance of classifiers (voters) was measured using normalised conditional entropy (NCE) [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], which is suitable for measuring the performance of a multi-class discrimination task where the explicit relation between the neuronal index and the corresponding class is unavailable. NCE has a value in the range 0 ≤ NCE ≤ 0.5, with lower conditional entropy indicating that each neuron fires more predominantly for one class, hence giving better performance—this measure will be used throughout the remainder of this paper (see <xref ref-type="sec" rid="sec015">Methods</xref> for the details of the simulation procedure and the NCE calculation, see <xref ref-type="supplementary-material" rid="pcbi.1005137.s001">S1 Text</xref> for full details of the simple numerical simulation results).</p>
<p>One key insight confirmed by the simple numerical simulation was that, as long as there is sufficient guidance from the gating voter, the decisions from the better performing ensemble neurons influence the final voter output more by developing relatively stronger weights than the other neurons. Thus the spike from one strongly weighted synaptic projection can overwhelm several other weakly weighted ‘wrong’ decisions. Such dynamics achieved successful learning of the weighted vote, based on the history of ensemble behaviour (exactly the behaviour we desire in this kind of ensemble learning). More specifically, the simulation of the simplified spiking ensemble system showed that the gating voter and at least one ensemble voter must have positive discriminability (NCE&lt;0.5) in order to properly learn to perform weighted voting. That is, the gating voter, and at least one ensemble member, must have at least reasonable—but not necessarily great—performance on the classification task for the overall ensemble performance to be very good.</p>
<p>These validation tests showed that the logical model of a spiking voter ensemble system and its analytic solutions are capable of performing efficient spike-based weighted voting, driven by ITDP, and gave us important insights into how that is achieved. They also demonstrated how the seemingly complex network of interactions between stochastic processes within a population of voters can be effectively described by a series of probability metrics. In the next section we report on results from a computational model based on this tractable logical model which was significantly extended to encompass more biologically realistic spiking neural networks, with ensemble members having their own inherent plasticity. This system was demonstrated on a practical classification task with real data.</p>
</sec>
</sec>
<sec id="sec009">
<title>Ensemble of ITDP Mediated Spiking Expectation Maximization Neural Networks</title>
<p>The logical voter ensemble model described in the previous section showed that the computational characteristics of ITDP provide a novel functionality which can be used to coordinate multiple neural classifiers such that they perform spike based online ensemble learning. This form of ensemble learning simultaneously solves both the weighted vote and combining problems of arbitrarily ordered decisions from individual classifiers in an unsupervised manner. After this validation of the overall ensemble scheme, we next investigated an extended neural architecture for combined learning in an ensemble of biologically plausible spiking neural network classifiers using ITDP. The overall scheme is based on the initial simplified model, but the components are now significantly extended. Instead of assuming the individual classifiers are pre-trained, they are fully implemented as spiking networks with their own inherent plasticity. Individual classifier and overall ensemble learning dynamics occur simultaneously. The individual classifiers in the ensemble are implemented as Spiking Expectation Maximisation (SEM) neural network which have been shown to perform spike based Bayesian inference [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], an ability that is often cited as an important mechanism for perception [<xref ref-type="bibr" rid="pcbi.1005137.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref040">40</xref>] in which hidden causes (e.g. the categories of objects) underlying noisy and potentially ambiguous sensory inputs have to be inferred.</p>
<p>A body of experimental data proposes that the brain can be viewed as using principles of Bayesian inference for processing sensory information in order to solve cognitive tasks such as reasoning and for producing adequate sensorimotor responses [<xref ref-type="bibr" rid="pcbi.1005137.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref042">42</xref>]. Learning using Bayesian inference updates the probability estimate for a hypothesis (a posterior probability distribution for hidden causes) as additional evidence is acquired. Recently, a spike-based neuronal implementation of Bayesian processing has been proposed by Nessler et al. [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref044">44</xref>] as a model of common cortical microcircuits. Their feedforward network architecture implements Bayesian computations using population-coded input neurons and a soft winner takes all (WTA) output layer, in which internal generative models are represented implicitly through the synaptic weights to be learnt, and the inference for the probability of hidden causes is carried out by integrating such weighted inputs and competing for firing in a WTA circuit. The synaptic learning uses a spike-timing dependent plasticity (STDP) rule which has been shown to effectively implement Maximum Likelihood Estimation (MLE) allowing the network to emulate the Expectation Maximization (EM) algorithm. The behaviour of such networks was validated by a rigorous mathematical formulation which explains its relation to the EM algorithm [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>].</p>
<p>Our reimplementation and extension of Nessler’s [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] model forms the basis of our classifiers and is well-suited for integration into our spike-based ensemble system. Viewing the SEM model as a unit cortical microcircuit for solving classification tasks, we can naturally build an integrated ITDP-based ensemble architecture as an extension of the logical ITDP ensemble model described earlier. <xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref> shows the two layer feedforward neural architecture for the SEM-ITDP ensemble system. The first layer consists of an ensemble of SEM networks and a gating SEM, which share the presynaptic input neurons encoding the input data. Reflecting the often non-uniform, and specifically targeted, convergent receptive fields of cortical neurons involved in perceptual processing [<xref ref-type="bibr" rid="pcbi.1005137.ref045">45</xref>], each WTA circuit receives a projection from a subset of input neurons (representing e.g. a specific retinal area), which enables learning for different ‘feature’ subsets of the input data. All synapses in the ensemble layer are subjected to STDP learning. Following Nessler et al. (2013) [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] and others, in order to demonstrate and test the operation of the system, binarized MNIST handwritten digit images [<xref ref-type="bibr" rid="pcbi.1005137.ref046">46</xref>] were used as input data for classification, where the ON/OFF state of each pixel is encoded by two input neurons. The MNIST dataset is a large database of handwritten digits covering a wide range of writing styles, making it a challenging problem. The output from the ensemble layer is fed to the final WTA circuit via ITDP synapses which are driven by the more biologically plausible ITDP curve shown in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>. The following sections will describe in detail the model SEM circuit and the ITDP dynamics, followed by an investigation into how the SEM-ITDP ensemble system applied to image classification performed simultaneous realtime learning of both the individual classifier networks and the ITDP layer in parallel.</p>
<fig id="pcbi.1005137.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g004</object-id>
<label>Fig 4</label>
<caption>
<title>SEM-ITDP ensemble network architecture.</title>
<p>The STDP connections, which projects from the selected input neurons to each WTA circuit, together with the WTA circuits constitute the SEM ensemble. The ITDP connections have the same connectivity as the logical ITDP model. All of the ensemble, gating and final output networks use the same SEM circuit model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g004" xlink:type="simple"/>
</fig>
<sec id="sec010">
<title>SEM neural network model</title>
<p>Let us first revisit a single SEM neural network model [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] for spike based unsupervised classification. The SEM network is a single layer spiking neural network in which the neurons in the output layer receive all-to-all connections projected from a set of inputs. The output neurons are grouped as a WTA circuit which is subjected to lateral inhibition, modelled as a common (global) inhibitory signal which is in turn based on the activity of the neurons. A WTA circuit consists of <italic>K</italic> stochastically firing neurons. The firing of each neuron <italic>z</italic><sub><italic>k</italic></sub> is modelled as an inhomogeneous Poisson process with instantaneous firing rate <italic>r</italic><sub><italic>k</italic></sub>(<italic>t</italic>),
<disp-formula id="pcbi.1005137.e012"><alternatives><graphic id="pcbi.1005137.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula> <disp-formula id="pcbi.1005137.e013"><alternatives><graphic id="pcbi.1005137.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>v</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1005137.e014"><alternatives><graphic id="pcbi.1005137.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mtext>inh</mml:mtext></mml:msub> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mtext>inh</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mtext>inh</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>inh</mml:mtext></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>u</italic><sub><italic>k</italic></sub>(<italic>t</italic>) is a membrane potential which sums up the EPSPs from all presynaptic input neurons (<italic>y</italic><sub><italic>i</italic></sub> (<italic>i</italic> = 1, …, <italic>n</italic>)) multiplied by the respective synaptic weight <italic>w</italic><sub><italic>ki</italic></sub>. The variable <italic>w</italic><sub><italic>k</italic>0</sub> represents neuronal excitability, and <italic>I</italic>(<italic>t</italic>) is the input from the global inhibitory signal to the WTA circuit. <italic>v</italic>(<italic>t</italic>) is an additional stochastic perturbation by a Ornstein-Uhlenbeck process which emulates the background neural activity using a kind of simulated Brownian dynamics that decorrelates the WTA firing rate from that of the input firing rate in order to prevent mislearning [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref047">47</xref>]. The EPSP evoked by the <italic>i</italic>th input neuron is modelled as a double exponential curve which has both fast rising (<italic>τ</italic><sub><italic>f</italic></sub>) and slow decaying (<italic>τ</italic><sub><italic>s</italic></sub>) time constants. At each time instance, EPSP amplitudes are summed over all presynaptic spike times (<italic>t</italic><sub><italic>p</italic></sub>) to become <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) for the <italic>i</italic>th input at time <italic>t</italic>.
<disp-formula id="pcbi.1005137.e015"><alternatives><graphic id="pcbi.1005137.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mtext>EPSP</mml:mtext></mml:msub> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>p</mml:mi></mml:msub></mml:munder> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>e</mml:mi> <mml:mfrac><mml:mrow><mml:msub><mml:mi>t</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mfrac><mml:mrow><mml:msub><mml:mi>t</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi></mml:mrow> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>f</mml:mi></mml:msub></mml:mfrac></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula></p>
<p>The scaling factor <italic>A</italic><sub>EPSP</sub> is set as a function of the two time constants in order to ensure that the peak value of an EPSP is 1. Whenever one of the neurons in the WTA circuit fires at <italic>t</italic><sub><italic>f</italic></sub>, <italic>I</italic>(<italic>t</italic>) adds a strong negative pulse (amplitude of <italic>A</italic><sub>inh</sub>) to the membrane potential of all <italic>z</italic> neurons, which exponentially decays back to its resting value (<italic>O</italic><sub>inh</sub>) with a time constant (<italic>τ</italic><sub>inh</sub>). Therefore, <italic>I</italic>(<italic>t</italic>) determines the overall firing rate of WTA circuits as well as controlling the refractory period of a fired neuron.</p>
<p>Input evidence <italic>x</italic><sub><italic>j</italic></sub> for a feature <italic>j</italic> of observed data is encoded as a group of neuronal activations <italic>y</italic><sub><italic>i</italic></sub>. If the set of possible values of <italic>x</italic><sub><italic>j</italic></sub> consists of <italic>m</italic> values <italic>G</italic><sub><italic>j</italic></sub> = [<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, …, <italic>v</italic><sub><italic>m</italic></sub>], the input <italic>x</italic><sub><italic>j</italic></sub> is encoded using <italic>m</italic> input neurons. Therefore, if input data is given as a <italic>N</italic> (<italic>j</italic> = 1, …, <italic>N</italic>) dimensional vector, the total number of input neurons is <italic>mN</italic>. For further details of the Bayesian processing dynamics of the SEM networks see the Methods section.</p>
<p>The rules for STDP driven synapse plasticity between the input layer and the SEM classifiers, ITDP driven plasticity on final output network synapses (as in <xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>), and neuronal excitability plasticity, are all explained in the Methods section. In this extended version of the model, ITDP follows the biologically realistic plasticity curve shown in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref> middle (Simplified ITDP curve).</p>
</sec>
<sec id="sec011">
<title>Experiments with ensembles of SEM networks</title>
<p>In this section we present results from running the full biologically plausible SEM ensemble architecture on a real visual classification task (as depicted in <xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>). We show that the ensemble learning architecture successfully performed the task and operated as expected from the earlier experiments with the more abstract logical ensemble model (on which it is based). Weights in the STDP and ITDP connection layers smoothly converged to allow robust and accurate classification. The overall ensemble performance was significantly better than the individual SEM classifier performances. The initial experiments used a random (input) feature selection scheme.</p>
<p>The SEM ensemble architecture was tested on an unsupervised classification task involving recognizing MNIST handwritten digits [<xref ref-type="bibr" rid="pcbi.1005137.ref046">46</xref>]. Each piece of input data was a greyscale image having 28×28 = 784 pixels. The class labels of all data were unknown to the ensemble system, so both the learning and combining aspects of the ensemble are unsupervised. All images were binarized by setting all pixels with intensity greater than 200 (max 255) to 1, and 0 otherwise. The dimension of the binary image was reduced by abandoning less occupied pixels by preprocessing over the entire images in the dataset (pixels being ‘on’ in less than 3% of the total image presentation were disabled) [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>].</p>
<p>In contrast to the logical voter model experiments, where output was manually designed to produce stochastic decisions, the outputs of individual SEM networks using a real dataset tend to produce the same decision error for the specific input data. Promoting diversity between individual classifier outputs is a prerequisite for improving ensemble quality in the machine learning literature [<xref ref-type="bibr" rid="pcbi.1005137.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref049">49</xref>], and ensemble feature selection has been shown to be an essential step for constructing an ensemble of accurate and diverse base classifiers. The features of an image in biological visual processing generally implies the neurally extracted stimuli which represent the elementary visual information of a scene (such as spot lights, oriented bars, and colors), and they need to be learnt through the layers of a neural pipeline [<xref ref-type="bibr" rid="pcbi.1005137.ref050">50</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref052">52</xref>] which is beyond the scope of this work. For the sake of simplicity, we used a raw pixel as the basic feature which could be selected as an informative subset of the input data space. It has been shown that specific forms of weight decay or regularization provide a mechanism for biologically plausible Bayesian feature selection [<xref ref-type="bibr" rid="pcbi.1005137.ref053">53</xref>–<xref ref-type="bibr" rid="pcbi.1005137.ref055">55</xref>]. In our ensemble system, selective projections from the input layer to the ensemble WTAs effectively implemented pixel/feature selection in this regard. Each ensemble layer SEM network learnt over a distinct subregion of images by neurally implementing ensemble feature selection, where each ensemble WTA circuit received the projection from a selected subset of input neurons such that the all-to-all connectivity from a pair of input neurons <italic>m</italic> and <italic>m</italic> + 1 to the WTA neurons was enabled if the pixel <italic>m</italic> was selected as a feature. A quarter of the total number of pixels were selected for each ensemble member by the feature selection schemes used (described later).</p>
<p>The gating network used either full (i.e. the whole image) or partial features for testing supervised or unsupervised gating of ITDP learning. In order for both the partial-featured ensemble network and the full-featured gating network to receive input from the same number of input neurons, the images were supersampled to 56×56 pixels. This is because the output of our WTA circuit is a train of spikes (typically bursting at a few tens of Hz) during input presentation, and different numbers of input neurons may result in different numbers of spikes in an output burst. For ITDP learning, it is logically compatible with biological ITDP <italic>in vitro</italic> (both distal and proximal neurons fire a single spike) to make all the ensemble WTAs and the gating WTA fire the same number of spikes per burst. The image supersampling replicated a pixel to four identical pixels (all four pixels indicate the same feature), so the set of all features for the gating WTA was represented by a quarter of the pixels of the supersampled image. A quarter of pixels were selected for each ensemble WTA as its feature subset using some selection scheme (see later). Thus the same number of input pixels was achieved both for the ensemble and gating WTAs. Another way of thinking about this process is that the pixels selected for an ensemble WTA were replicated in order to match their number to the size of the original (not supersampled) image.</p>
<p>We conducted an initial experiment using four classes of images (digits 0, 1, 2, and 3) each of which had 700 samples (2800 images in total). The original 784 pixels were reduced to 347 by dimensionality reduction, followed by supersampling them to <italic>m</italic> = 1388, hence there were <italic>N</italic><sub><italic>I</italic></sub> = 2<italic>m</italic> = 2776 input neurons in the input layer and <italic>K</italic> = 4 output neurons in each WTA circuit. The number of synapses is proportional to <italic>N</italic><sub><italic>E</italic></sub> as each ensemble WTA receive the same number of inputs in order to give an output burst of regular numbered spikes which behaves as similar as possible to the (earlier tractable) logical voter ensemble model (which had been shown to perform well). Given an ensemble size <italic>N</italic><sub><italic>E</italic></sub>, the system has <italic>KN</italic><sub><italic>I</italic></sub>(<italic>N</italic><sub><italic>E</italic></sub> + 1)/4 STDP synapses in the first layer, <italic>K</italic><sup>2</sup> <italic>N</italic><sub><italic>E</italic></sub> ITDP synapses in the second layer, and <italic>N</italic><sub><italic>I</italic></sub> + <italic>K</italic>(<italic>N</italic><sub><italic>E</italic></sub> + 2) Poissonian neurons. The effect of increased synapses in the second layer was compensated by adjusting the inhibition level of the final WTA circuit (See Methods). We initially used random feature selection, where a quarter of pixels were randomly selected for each ensemble member and for the gating network, and the corresponding input neurons projected STDP synapses to their target WTA circuit. The input was fed to the network by successively presenting an image from one class for a certain duration (<italic>T</italic><sub><italic>present</italic></sub>), followed by a resting period (<italic>T</italic><sub><italic>rest</italic></sub>) where none of the input neurons fire, in order to avoid overlap of EPSPs from input spikes caused by different input images. Full numerical details of the experimental setting can be found in Methods (subsection SEM-ITDP experiments).</p>
<p>An example of the ensemble classification learning task with random feature selection is shown in <xref ref-type="fig" rid="pcbi.1005137.g005">Fig 5</xref>. One of the images in a class was presented for 40ms followed by another 40ms of resting period. Different images generated from four classes were presented successively in a repeating order. Approximately a few tens of seconds after starting the simulation, the output neurons of all WTA circuits began to fire a series of ordered bursts almost exclusively to one of the hidden classes of each presented image. The allocation of output neuron indices firing for a specific class arbitrarily emerged in all of the ensemble layer WTAs by unsupervised learning, whereas the neuron indices between the gating network and final network were matched by ITDP guidance. Technically, the system is not completely unsupervised because the number of classes is provided, even if the class labels are not; however, blinding the class labels makes the task challenging for the system which has to discriminate distinct hidden causes in a self-organised manner. It can be seen from the figure that, after a period of learning, the network outputs produce consistent firing patterns, each output spiking exclusively for a single class of input data.</p>
<fig id="pcbi.1005137.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Spike trains from the SEM ensemble network with <italic>N</italic><sub><italic>E</italic></sub> = 5 and random feature selection.</title>
<p>(Left) Plot shows the input neuron spikes from eight image presentations from different classes (digits) which are depicted in different colors (black: 0, red: 1, green: 2, blue: 4). (Right) Two graphs show the output spikes of ensemble, gating, and final WTA neurons before and after learning. The colors of the spikes represent which class is being presented as input. After learning the network outputs produce consistent firing patterns, each output spiking exclusively for a single class.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g005" xlink:type="simple"/>
</fig>
<p>After learning, the presynaptic weight maps for each output neuron of an ensemble layer WTA circuit clearly represent four different hidden causes, which are shown by depicting the difference between ON and OFF weights for each pixel (<xref ref-type="fig" rid="pcbi.1005137.g006">Fig 6A and 6B</xref>). Once one of the WTA neurons fires for one class more than the others, its presynaptic STDP weights are adjusted such that either the ON or OFF weights for corresponding pixels are enhanced by STDP to reflect the target class. Thus the output neuron comes to fire more when an image from the same class is presented again. <xref ref-type="fig" rid="pcbi.1005137.g006">Fig 6C</xref> shows the emergence of typical ITDP guided weight learning on the connections between the ensemble layer and the final net (<xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>). Over the learning period weight values become segregated into groups which depend on the frequency of the co-firing of the ensemble and the gating neurons. In most cases, the highest-valued group consisted of projections which formed topographical (but not necessarily using the same index) connections between the neurons of each ensemble WTA and the final output WTA neurons, which meant that the connections carrying the signal for the same class were most enhanced and converged to the corresponding final WTA neurons. Therefore it can be seen that the process for combining ensemble outputs, controlled by ITDP learning, functioned similarly to the learning of a spike-based majority vote system where only topographic connections having identical weights exist between each ensemble WTA and the final WTA. Despite the system having no information about the class labels in ensemble WTA neurons, the gating WTA (which is also unsupervised) could selectively recruit and assign the ensemble output to converge to one of the final layer neurons based on the history of the ensemble output. Clearly, the fully extended ensemble architecture performs as expected.</p>
<fig id="pcbi.1005137.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g006</object-id>
<label>Fig 6</label>
<caption>
<title>An example of the STDP weight maps of a SEM classifier after learning (A, B) and the time evolution of ITDP weights (C).</title>
<p>Each weight map represents the presynaptic weight values that project to each of four WTA neurons (which each fire dominantly for one of the classes). The grey area shows pixels disabled by preprocessing, and each colored pixel represent the difference of the weights from the two input neurons for the corresponding pixel (white pixels represent unselected features). So as to use all features, a quarter of pixels are evenly selected from the supersampled image in order to use all pixels of the original data.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g006" xlink:type="simple"/>
</fig>
<p>The classification performance of the network was represented by calculating the normalised conditional entropy (NCE) as in Eqs <xref ref-type="disp-formula" rid="pcbi.1005137.e050">26</xref>–<xref ref-type="disp-formula" rid="pcbi.1005137.e052">28</xref>. Low conditional entropy indicates that each output neuron fires predominantly for inputs from one class, hence representing high classification performance. In order to observe the continuous change of network performance over time, the conditional entropy was calculated within a moving time window of 2800 image presentations (the total number of data) which is approximately 224 seconds in simulation time. In most cases, the conditional entropies of all WTA circuits were converged after approximately a couple of rounds of total data presentation (after 448 sec). While the visual observation of spike bursting in the output WTA after learning seemed to show less salient differences than expected, the traces of normalized conditional entropy showed that the final WTA outperformed the individual ensemble WTAs in nearly all cases. <xref ref-type="fig" rid="pcbi.1005137.g007">Fig 7</xref> shows three particular examples of different gating WTA performances of: (A) better than the ensemble average, (B) similar to the ensemble average, (C) worse than the ensemble average. It is interesting to note that the performance of the gating WTA, which actually guides the whole ensemble, does not have to have the best performance in order for the overall performance of the ensemble to be better than that of the individual classifiers.</p>
<fig id="pcbi.1005137.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Examples of ensemble behaviours (<italic>N</italic><sub><italic>E</italic></sub> = 9) for different gating network performances ((A) better than, (B) similar to, (C) worse than the ensemble average).</title>
<p>All the ensemble and the gating WTAs used random feature selection. The colors represent the NCEs of the final network (red), the gating network (blue), the ensemble networks (grey) and their average (black). Vertical lines indicate the time span of the total data presentation, where input data are sequentially presented for multiple rounds in order to see long term convergence. The NCE value at time <italic>t</italic> is calculated by counting the class-dependent spikes within the past finite time window of [<italic>T</italic><sub><italic>p</italic></sub>, <italic>t</italic>] (<italic>T</italic><sub><italic>p</italic></sub> &lt; <italic>t</italic>). In order to prevent a sudden change in the NCE plots due to the exclusion of the early system output (which are immature resulting in high NCE values) from the time window, <italic>T</italic><sub><italic>p</italic></sub> was dynamically changed for faster burn-out of those initial values as: <italic>T</italic><sub><italic>p</italic></sub> = <italic>t</italic>(1−<italic>d</italic>/4<italic>D</italic>) where <italic>d</italic> = <italic>t</italic> when <italic>t</italic> &lt; 2<italic>D</italic> and <italic>d</italic> = 2<italic>D</italic> otherwise, <italic>D</italic> = 224sec is the duration of one round of dataset presentation. See Methods for details of the NCE calculations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g007" xlink:type="simple"/>
</fig>
<p>As well as supporting the theoretical model of the logical voter ensemble presented earlier, these initial experiments demonstrated that the ensemble architecture for a population of spiking networks successfully extended into a more biologically realistic implementation in which the individual classifiers and the combining mechanism all operated and learned in parallel.</p>
</sec>
<sec id="sec012">
<title>SEM ensemble learning with different feature selection schemes</title>
<p>The initial experiments described in the previous section used a simple random input feature selection scheme. In order to investigate the influence of feature selection on learning in the SEM ensemble, a detailed set of experiments were carried out to compare a number of different feature selection heuristics. This section presents the results of those experiments. The basic experimental setup and the visual classification task were the same as in the previous section. In each of the new feature selection schemes, pixel subsets were stochastically selected from controlled probability distributions. Ensemble behaviour was compared across the controlled feature distributions and the random selection scheme in terms of the relationship between performance and ensemble diversity.</p>
<p>For the controlled feature subsets, two Gaussian distribution schemes were tested, being systematically investigated for various ensemble sizes <italic>N</italic><sub><italic>E</italic></sub>. These schemes are reminiscent of basic biological topographic sensory receptive fields/features [<xref ref-type="bibr" rid="pcbi.1005137.ref045">45</xref>]. In order to promote diversity of input patterns for ensemble members, each distribution was designed to enable pixels to be drawn from different regions of the image, and for each ensemble WTA to receive projections from different input neurons, corresponding to the selected pixel subsets. Hence each of the SEM classifiers in the ensemble received its inputs from a different region of the image as defined by the distributions. The first method selects pixels by sampling from <italic>N</italic><sub><italic>E</italic></sub> normal 2D Gaussian distributions (i.e. with identity covariance matrices) with different means (mean positions are distributed evenly on the image)—one for each ensemble member. The second Gaussian method uses the same number of stretched Gaussian distributions (the selected pixel group forms a thick bar on the image) all having the same mean at the centre of the image but with varying orientations (which differ by <italic>π</italic>/<italic>N</italic><sub><italic>E</italic></sub> rad)—see Figs <xref ref-type="fig" rid="pcbi.1005137.g008">8</xref> and <xref ref-type="fig" rid="pcbi.1005137.g009">9</xref> for illustrative descriptions of each selection scheme and their resultant visual regions. See Methods section for further details of the schemes.</p>
<fig id="pcbi.1005137.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Illustrative images for controlled feature assignment for SEM ensemble networks.</title>
<p>White regions indicate available pixels (active region) as defined by preprocessing, and the Gaussian means for the normal Gaussian selection scheme are evenly placed inside such regions by random placement procedure (See Methods for details of the actual Gaussian mean placement). The number of stretched Gaussian features used increases linearly with ensemble size (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). The diameters of red circles and ovals roughly represent the full width at a tenth of maximum (FWTM) for each principal direction (the length of an oval is shown far shorter than it actual is for the sake of visualization—long ovals are used to ensure they form roughly uniform bars in the region of available pixels). In all cases, exactly 1/4 of pixels from the available (white) region are stochastically selected (without replacement) for each ensemble network according to each distribution function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g008" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005137.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Examples of STDP weight maps from different feature selection schemes when <italic>N</italic><sub><italic>E</italic></sub> = 5.</title>
<p>The weight maps for the ensemble WTA neurons which represent the digit 1 after learning are shown.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g009" xlink:type="simple"/>
</fig>
<p>Since the SEM network implements spike-based stochastic EM [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], its solution is only guaranteed to converge to one of the local maxima, dependent on both the initial conditions and the stochastic firing process, which means that the system behaviour can vary to some extent between repeated trials. Thus it was necessary to set some criterion for the comparison of the system behaviours under different feature assignment schemes. The most obvious approach would be to compare them at their peak performances when all ensemble WTAs and the gating WTA are at their global maxima. If all the ensemble and gating WTAs produce their maximum performances, the final result will be also be at the maximum. However, it is hard to manually search all WTAs for maximum performances (which is another optimisation problem), thus we first observed the performance of the system under different conditions only when the performance of the full-featured gating network (i.e. using input from all features as in <xref ref-type="fig" rid="pcbi.1005137.g006">Fig 6</xref>) had reached a level close to its best possible value. Later, more reliable, comparisons were performed by using statistics from a number of repeated trials using supervised output from the gating network by giving the true class labels without learning (thus forcing identical gating network behaviour over trials).</p>
<p>The ensemble system using three different feature selection scheme (random, normal Gaussian, stretched Gaussian—see <xref ref-type="fig" rid="pcbi.1005137.g008">Fig 8</xref>) was investigated with eight different ensemble sizes <italic>N</italic><sub><italic>E</italic></sub> = {5, 7, 9, 11, 13, 16, 20, 25}. <xref ref-type="fig" rid="pcbi.1005137.g010">Fig 10</xref> shows an example of the performances of all WTA outputs after running two rounds of input presentations. In order to minimise the influence of different gating network performances on the comparison of final performances, in the runs summarized in the figure only the results from similar gating network performances were plotted. The gating network always used the same full set of features, and the results from the reasonably high gating network performances (NCE≈0.26) were found by manually repeating several tens of runs with different initial weights. The results show that the ensemble systems using the Gaussian feature selection schemes both outperform, to a similar degree, random selection and that the final performances increase (i.e. NCE decreases) with ensemble size in all cases.</p>
<fig id="pcbi.1005137.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g010</object-id>
<label>Fig 10</label>
<caption>
<title>All WTA performances vs. ensemble sizes for different feature selection schemes.</title>
<p>Results having similar gating network performances are depicted by manually finding the ‘best’ gating network performances at around NCE≈0.26. All NCE values were taken at the end of simulations which were run for two rounds of input presentations (<italic>t</italic> = 448 sec). Colors represent: ensemble networks (grey), gating network (blue), and the final output network (red).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g010" xlink:type="simple"/>
</fig>
<p>Further trends in system behaviour were investigated in more detail by averaging over repeated simulations using a supervised gating network whose neurons output the true class (the <italic>i</italic>th neuron fires when the image from class <italic>i</italic> is presented), thus taking variability in the unsupervised gating network out of the equation. At the beginning of each image presentation (<italic>t</italic><sub>0</sub>), a supervised gating neuron output was manually given as a train of three spikes, with an interspike interval of 15ms starting from <italic>t</italic><sub>0</sub> + 5ms. By alleviating issues of variability, analysis of repeated simulations with the supervised gating network allowed better insight into the dependency of system behaviour on the feature selection schemes and ensemble size. The mean positions of the Normal Gaussian features were randomly ‘jittered’ about (within constraints, see <xref ref-type="sec" rid="sec015">Methods</xref> for details) between simulations so as to eliminate any dependence on exact pre-determined positions. We also measured the diversity of ensemble members in order to investigate its influence on the final performance. Although various measurements for the diversity of classifier ensembles have been proposed, there is no globally accepted theory on the relationship between the ensemble diversity and performance, and only a few studies have conducted a comparative analysis of different diversity measures for classifier ensembles [<xref ref-type="bibr" rid="pcbi.1005137.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref057">57</xref>]. Among them, we chose an entropy based diversity measure [<xref ref-type="bibr" rid="pcbi.1005137.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref059">59</xref>] because it is a none-pairwise method (hence less computationally intensive) and has been shown to have strong correlation with ensemble accuracy across various combining methods and different benchmark datasets.</p>
<p>After learning has converged, the diversity of an ensemble of size <italic>N</italic><sub><italic>E</italic></sub> for <italic>N</italic><sub><italic>C</italic></sub> classes is calculated over the total input presentations as:
<disp-formula id="pcbi.1005137.e016"><alternatives><graphic id="pcbi.1005137.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtext>Div</mml:mtext><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>M</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>P</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:msubsup> <mml:msub><mml:mtext>log</mml:mtext> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msub> <mml:msubsup><mml:mi>P</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula> <disp-formula id="pcbi.1005137.e017"><alternatives><graphic id="pcbi.1005137.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:munderover> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msubsup> <mml:msubsup><mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>M</italic> is the number of input data, and <inline-formula id="pcbi.1005137.e018"><alternatives><graphic id="pcbi.1005137.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> represents the proportion of ensemble members which assign <italic>d</italic><sub><italic>l</italic></sub> to the instructed class <italic>k</italic> given by the gating network. While the original diversity metric simply counts the number of classifiers giving the same decision, the SEM network output consists of multiple spikes which can have originated from different output neurons within the time window of the image presentation. Thus <inline-formula id="pcbi.1005137.e019"><alternatives><graphic id="pcbi.1005137.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msubsup><mml:mi>P</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is calculated from a soft decision, where <inline-formula id="pcbi.1005137.e020"><alternatives><graphic id="pcbi.1005137.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi> <mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the number of spikes from the neuron of the <italic>j</italic>th ensemble network which represents the <italic>k</italic>th class under the presentation of input data <italic>d</italic><sub><italic>l</italic></sub>. Identifying which ensemble network neuron represents the <italic>k</italic>th class is done by counting the total number of spikes from each neuron when the <italic>k</italic>th gating network neuron fires and assigning the neuron which fires most.</p>
<p>The result from repeated simulations (<xref ref-type="fig" rid="pcbi.1005137.g011">Fig 11</xref>) showed that the normal Gaussian selection scheme provided the best performances even if the average ensemble performance (<inline-formula id="pcbi.1005137.e021"><alternatives><graphic id="pcbi.1005137.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msub><mml:mi>E</mml:mi> <mml:mtext>esb</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:mfrac> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mtext>NCE</mml:mtext> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>) was the worst. As expected, the ensemble diversities showed an inverse relationship with the final network NCE, indicating its crucial role in the combined performance. It can be inferred that while the two Gaussian feature schemes try to select pixel subsets explicitly from different regions of the image, the normal Gaussian scheme generally has more superimposed pixels between subsets than the other. This results in higher redundancy among the output of ensemble members, and hence higher diversity. Preliminary ‘feature jitter’ experiments with higher degrees of noise made it clear that the normal Gaussian scheme works best when the features are reasonably evenly spread over the active region of the image with decent separation between the means—in other words a set of evenly spread reasonably independent features. This fits in with insights on how the architecture works (good performance is encouraged by not too much correlation between individual ensemble member inputs, and a good level of diversity in the ensemble—appropriately used the normal Gaussian features are a straightforward way of achieving this). While performance gets better as the ensemble size increases, diversity roughly increases with ensemble size, indicating a greater chance of disagreement in outputs between ensemble members as the population size increases. Krogh and Vedelsby (1995) [<xref ref-type="bibr" rid="pcbi.1005137.ref060">60</xref>] have shown that the combination performance (final error <italic>E</italic>) of a regression ensemble is linearly related to the average error (<inline-formula id="pcbi.1005137.e022"><alternatives><graphic id="pcbi.1005137.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mover><mml:mi>E</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) and the ambiguity (<inline-formula id="pcbi.1005137.e023"><alternatives><graphic id="pcbi.1005137.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mover><mml:mi>A</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) of the individual ensemble members as follows: <inline-formula id="pcbi.1005137.e024"><alternatives><graphic id="pcbi.1005137.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>=</mml:mo> <mml:mover><mml:mi>E</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mover><mml:mi>A</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, where each term corresponds to the final network performance, average ensemble performance, and diversity in our system. We can expect a similar linear relationships between these quantities and indeed <xref ref-type="fig" rid="pcbi.1005137.g011">Fig 11E</xref> shows the linear relationship between the final network performance and <italic>E</italic><sub>esb</sub>−Div.</p>
<fig id="pcbi.1005137.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Statistics of ensemble performances and diversities for different feature selection schemes and ensemble sizes.</title>
<p>Each point in the graphs (A-D) is the averaged value of 50 simulations, and the error bars represent standard deviations. <italic>E</italic><sub>esb</sub> in (C, D, E) represents the average NCE of ensemble members at each simulation, Div (B, D, E) is diversity. (E) Final network NCE vs. the difference of diversity and average ensemble NCE. The background dots (grey, orange, light blue) represent every individual simulation from all three feature selection schemes (random, normal Gaussian, stretched Gaussian respectively) and eight ensemble sizes (3×8×50 = 1200 runs), and the larger dots are the average values of each of 50 repeated simulations (same colors as A-D).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g011" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1005137.g012">Fig 12A-12C</xref> shows that the trained ensemble generalized well to unseen data. Its performance on unseen classification data compared very well with its performance on the training data. In common with all the other earlier results, the figure shows the NCE entropy measure for performance because of the unsupervised nature of the task, where the ‘correct’ associations between input data and most active output neuron are not known in advance. Individual classifier performances are shown in grey, and the overall ensemble (output layer) performance is shown in red. An alternative is to measure the classification error rate in the test phase in relation to the associations between the class of the input data and the output neuron firing rates made during the training phase. In terms of this classification error rate, the trained ensemble typically generalizes to unseen data with an error of 15% or less. The best prediction performances were found using the normal Gaussian selection scheme (<xref ref-type="fig" rid="pcbi.1005137.g012">Fig 12D–12F</xref>), which resulted in an error rate of 10% or less. It can be seen that not only the ensemble size but also its diversity in the training phase influences the performances on the unseen test set, where the generalization performances of ensembles having greater diversities can outperform those with larger ensemble sizes (ex. <italic>N</italic><sub><italic>E</italic></sub> = 13). Similar trends relating diversities and average test error rates of ensemble members, indicate that networks in the more diverse ensembles are more likely to disagree with each other because of a greater number of misclassifications. However, their combined output eventually yields a better generalization performance on unseen data, indicating that ensemble diversity is more important for the final result than the individual classifier performances.</p>
<fig id="pcbi.1005137.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g012</object-id>
<label>Fig 12</label>
<caption>
<title/>
<p>(A, B, C) Training and test performances demonstrating generalization to unseen data (<italic>N</italic><sub><italic>E</italic></sub> = 5). The testing phase starts at iteration 448 by freezing the weights and by replacing the input samples by the test set which was not shown to the system during the learning phase. (D) Test set error rates of the final output unit, (E) the average ensemble error rates, and (F) the training phase diversities (same as in <xref ref-type="fig" rid="pcbi.1005137.g011">Fig 11</xref>) over different ensemble sizes using the normal Gaussian selection scheme on the integer recognition problem. Each data point was plotted by averaging 50 runs, where the error bar shows the standard deviations. NCE calculations as in <xref ref-type="fig" rid="pcbi.1005137.g007">Fig 7</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g012" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec013">
<title>Comparison with an STDP Only Ensemble</title>
<p>Although the starting points for the ITDP based ensemble architecture proposed in this paper were the earlier hypotheses about MoE type architectures operating in the brain [<xref ref-type="bibr" rid="pcbi.1005137.ref006">6</xref>], and the realization that the circuits involved in ITDP studied in [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>] had exactly the properties required for an ITDP driven gating mechanism that could control ensemble learning, an alternative hypotheses involves a hierarchical STDP only architecture. A multi-layered STDP system where the final layer learns to coordinate the decisions of an earlier layer of classifiers might also provide a mechanisms for effective ensemble learning.</p>
<p>The SEM neural network classifiers realize expectation maximization by learning the co-firing statistics of pre and postsynaptic neurons via STDP. The neurons of the input layer represents discrete-valued multidimensional data (ex. digital pixel image) using a spike-coded vector, where the value of each dimension is expressed by a group of exclusively firing neurons representing its corresponding states. Since the spike output of a WTA ensemble similarly can be regarded as the binary-coded multidimensional input data for the final layer (ex. <italic>N</italic><sub><italic>E</italic></sub> dimensional data where the value of each dimension has <italic>N</italic><sub><italic>C</italic></sub> states), this naturally leads to the possibility that the latent variable (hypothesis) of a given ensemble state can be inferred by the final WTA network using STDP learning instead of ITDP. One difference between the ensemble WTA layer and the input layer during the presentation of input data is that the firing probabilities of WTA neurons are not exclusive for a given input sample (more than one neuron can have a non-zero firing probability), while the population code used in the input layer neurons always have all-or-none firing rates, which means that the state of the given input data is represented stochastically in the WTA layer. Although, as a form of interference, this might inherently affect the behavior of a SEM network, previous work [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>] indicates that it should still be able to deal with incomplete or missing data.</p>
<p>Possible applications of multi-layered SEM microcircuit were suggested in [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], and a further study [<xref ref-type="bibr" rid="pcbi.1005137.ref061">61</xref>] has shown the power of recurrent networks of multiple SEM circuits when used as a neural reservoir for performing classification, memorization, and recall of spatiotemporal patterns. These insights suggest an STDP only implementation of the MoE type architecture presented earlier might be viable. Hence we conducted a preliminary investigation of using STDP to learn the second layer connection weights (i.e. connections between the ensemble and final layer, <xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>), making a comparison of the use of STDP and ITDP in that part of the ensemble classifier system.</p>
<p>The learning of the second layer of weights by STDP was done straightforwardly by applying the same learning rule as in the first layer connections (between the input and ensemble layers). All other settings and parameters were exactly the same as the original system. In order to avoid the influence of the inevitable trial-to-trial variance of the presynaptic SEM ensemble when the two learning rules are tested separately, the original ensemble network architecture was expanded by having two final (parallel) WTA circuits which both receive connections from the same ensemble WTAs, but are subject to different synaptic learning rules (one for STDP and the other for ITDP). This setup, where the learning rules are tested in parallel, ensures that both final layer WTAs receive exactly the same inputs, so that any differences in their final performances depend only on the different synaptic learning rules. For the repeated simulations with the normal Gaussian feature selection scheme, the same initial mean positions were used without the random mean placement ‘jittering’. This is because the purpose of the current experiment is to compare the two plasticity methods under as identical conditions as possible, and we know from the earlier experiments with the ITDP ensemble that the performance and trends of the fixed normal Gaussians was very close to the average of the randomly jittered placements. These procedures enables a well-defined, unbiased comparison between the two learning rules.</p>
<p>The connections from the gating WTA to the ITDP final layer operate exactly as in the experiments described earlier (i.e. as genuine gating connections involved in the heterosynaptic plasticity process). For comparability, the STDP final layer also receives projections from the gating WTA, but they of course operate very differently—they are just like the connections to the final network from any of the ensemble networks. Therefore in the STDP case the gating WTA does not have an actual gating function but effectively operates as an extra ensemble member. The corresponding synaptic weights are learnt by STDP in just the same way as for all other ensemble WTA projections to the STDP final layer neurons. This use of an additional ensemble member is potentially advantageous for the STDP final network in terms of the amount of information used.</p>
<p>The results of multiple runs of the expanded comparative architecture on the MNIST handwritten digits recognition task with random feature selection are illustrated in <xref ref-type="fig" rid="pcbi.1005137.g013">Fig 13</xref>. It is clear from these initial tests that the STDP version compares favourably with the ITDP version, although is generally not as good. The performance of the STDP final WTA over repeated trials shows that on many runs it outperforms most of the ensemble WTAs (i.e. ensemble learning is successful in this version of the architecture). Although the STDP net is capable of bringing improved classification from the SEM ensemble, its performance variance over repeated trials is higher than the ITDP net, indicating less robustness against the various ensemble conditions. However, while the ITDP net is dependent on the gating WTA performance (as we know from earlier experiments—<xref ref-type="fig" rid="pcbi.1005137.g007">Fig 7</xref>), no single presynaptic WTA circuit strongly influences the STDP net performance. The result of repeated runs sorted by the gating WTA performance (<xref ref-type="fig" rid="pcbi.1005137.g013">Fig 13B</xref>) indeed shows this dependency of the ITDP net, and the STDP net outperforms the ITDP net in the region where the gating WTA performances are the worst. However, as was shown with earlier experiments, it is relatively easy to find good initial gating network settings, and it might not be unreasonable to assume these would be partially or fully hardwired in by evolution in an ITDP ensemble. The dependence of ITDP on (a reasonable) gating signal may be disadvantageous in terms of the performance consistency in this type of neural system in isolation, and without any biases in initial settings, but on the other hand, the gating mechanism (which after all is the very essence of the ITDP system) can act as an effective and compact interface for providing higher control when connected to other neural modules. For example, the supervising signal could be directly provided via a gating network from the higher nervous system, or the gating signal could be continuously updated by reward-based learning performed by an external neural system such as the basal ganglia. Also it is possible that multiple ITDP ensemble modules could be connected such that the final output of one module is fed to the gating signal of other modules (similar to the multilayered STDP SEM networks), achieving successive improvements of system performance as information is passed through modules.</p>
<fig id="pcbi.1005137.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Training performances of the expanded STDP/ITDP networks (using random feature selection on the MNIST handwritten digits classification task as in earlier experiments).</title>
<p>Each color represents, red: ITDP final WTA, green: STDP final WTA, blue: gating WTA, grey/black: ensemble WTAs and their average. (A, B) An example of time courses of performances and the final performances from 50 repeated trials using unsupervised gating WTA. The individual trials were sorted by gating WTA performances in ascending order. (C, D) Simulations using the automatic selection of gating WTA. The vertical lines with arrowheads in C indicate where the switching of gating WTA occurs (see text for further details).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g013" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pcbi.1005137.g013">Fig 13C and 13D</xref> show the performances using a high performing ensemble WTA as the gating WTA which is automatically selected during the early simulation period. The gating WTA was continuously updated during the first round of dataset presentation (0 &lt; <italic>t</italic> &lt; 224) by assigning one of the ensemble WTAs as the gating WTA whenever the current gating WTA is outranked by it. This procedure was used, rather than assigning previously found good (ITDP) gating network settings, in an attempt not to potentially bias proceeding against STDP by using a network known to be good for ITDP. When the gating WTA is replaced by the selected ensemble WTA, the indices of its neurons representing corresponding classes also changes. Thus the entire set of ITDP weights are automatically re-learnt to new values, which causes the transition in the NCE value of the final WTA until re-stabilization (the hills in the red line in <xref ref-type="fig" rid="pcbi.1005137.g013">Fig 13C</xref>).</p>
<p>Indeed, we can see from the results of the more detailed set of comparative experiments shown in <xref ref-type="fig" rid="pcbi.1005137.g014">Fig 14</xref> that given a qualified gating signal of the kind describe above (i.e. from a gating network that performs classification reasonably well), the ITDP final net consistently and significantly outperformed the STDP final net over a wide range of conditions (feature selection scheme, ensemble size) in both training and testing. This was the case even though the STDP net uses one more presynaptic WTA circuit ensemble member, which can be seen to confer an advantage (first two columns in <xref ref-type="fig" rid="pcbi.1005137.g014">Fig 14</xref>). Clearly, if the gating network was used only in the ITDP case, and the main ensemble was the same size under both conditions, then the ITDP version’s margin of superiority would be increased further.</p>
<fig id="pcbi.1005137.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g014</object-id>
<label>Fig 14</label>
<caption>
<title>Average performances of STDP and ITDP ensembles over 50 trials on the MNIST handwritten digits task using selected/supervised gating WTAs for different feature selection schemes and ensemble sizes (<italic>N</italic><sub><italic>E</italic></sub> = 5, 9, 16, 25).</title>
<p>The training and test phases were run for three and two rounds of dataset presentation respectively. The error bars represent the standard deviations of the performances from corresponding repeated runs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g014" xlink:type="simple"/>
</fig>
<p>It is interesting to note that the overall trends of the final performances of both methods are similar to each other over the repeated trials in the region of good performance gating WTAs (the ups and downs of the red and green plots over the trials in <xref ref-type="fig" rid="pcbi.1005137.g013">Fig 13B and 13D</xref> follow each other quite closely). There is also a similar dependency of the average performances on the ensemble sizes (<xref ref-type="fig" rid="pcbi.1005137.g014">Fig 14</xref>), which suggests that there might be some shared underlying mechanisms in both combining methods. In the STDP ensemble, the synapses carrying the presynaptic spikes onto the postsynaptic neurons get enhanced after a few milliseconds of neural firing. Since all the WTA neurons fire highly synchronous bursts of spikes during every input presentation (the behavior is similar to the clocked output of the abstract voter ensemble model), in most cases the last spike of the final WTA burst triggering STDP follows right after the end of the presynaptic bursts. This leads to the synaptic potentiation by STDP reflecting all the presynaptic bursts. Considering the plasticity curves of STDP and ITDP in our model are of a similar type with a few tens of milliseconds of time shift, both plasticities can be generally thought as enhancing the synaptic weight if two neuron co-fire around the peak of the curve, and depressing it otherwise. This insight leads to the hypothesis that the final WTA in the STDP network acts functionally quite similarly to the gating WTA in the ITDP network. Among the presynaptic ensemble WTA neurons, the better performing neurons (those which fire only under the presentation of a specific class) will fire more spikes than the worse performing neurons. This is because the neurons of each ensemble WTA typically fire highly regular burst of 3-4 spikes in total. The best performing neurons in the ensemble layer fires all 4 spikes for its corresponding class and remains silent for the other classes. In the poorer performing WTA neurons, more than two neurons will fire 1-2 spikes each, resulting in the dispersion of spikes. Thus, over the course of STDP weight updates, the weights from the better performing presynaptic WTA neurons will get more potentiation (by summing EPSPs from all 4 presynaptic spikes) than the connection weight from the more poorly performing neurons (which typically carry only 1-2 spikes). This leads us to infer that the best performing presynaptic WTA neurons under each class presentation generally influence the final WTA most as learning proceeds (through the Hebbian STDP process). This autonomously drives the final WTA towards better performance through increased correlated activity with the ensemble, effectively making it a good ‘gating’ WTA (or at least ‘guiding’ WTA). This ‘guiding’ results in better performance of the combined ensemble output in an analogous way to the explicit gating signals in the ITDP ensemble mechanism. Of course the STDP version requires correlated pre- and post-synaptic firing from the start in order to gain traction, whereas the more direct ITDP version does not require post-synaptic firing. Although this STDP ‘gating signal’ may result in positive feedback of the final WTA behavior, inputs from the other presynaptic neurons always interfere with it, preventing an indefinite increase of system performance. The effect of supervised gating signals shown in <xref ref-type="fig" rid="pcbi.1005137.g014">Fig 14</xref> indeed shows the difference between the two mechanisms: the STDP final net has increased performances driven by the supervised signal from one of the presynaptic WTAs during the training phase, but its performance drop is much larger than for the ITDP final net in the test phase after the supervised signal is removed. In particular, the odd dependence of the STDP net on ensemble size in the stretched Gaussian selection case (where performance decreases with ensemble size in the training phase, instead of increasing as in all other cases, and the discrepancy with the test phase is particularly marked: <xref ref-type="fig" rid="pcbi.1005137.g014">Fig 14</xref> bottom of 3rd column) indicates the possibility of a negative effect of the supervised signal when the ensemble size is small, where the training result can be deceptive because of the large influence of the supervising signal on the final WTA relative to the inputs from the rest of the presynaptic WTAs. By contrast the explicit gating signal in the ITDP system is more stable and less prone to such effects, providing better overall performance.</p>
</sec>
</sec>
<sec id="sec014" sec-type="conclusions">
<title>Discussion</title>
<p>The main aim of this paper was to explore a hypothesized role for ITDP in the coordination of ensemble learning, and in so doing present a biologically plausible architecture, with attendant mechanisms, capable of producing unsupervised ensemble learning in a population of spiking neural networks. We believe this has been achieved through the development of an MoE type architecture built around SEM networks whose outputs are combined via an ITDP based mechanism. While the architecture was successfully demonstrated on a visual classification task, and performed well, our central concern in this paper was not to try and absolutely maximize its performance (although of course we have striven for good performance). There are various methods and tricks from the machine learning literature on ensemble learning that could be employed in order to increase performance a little, but a detailed investigation of such extensions is outside the scope of the current paper, making it far too long, and some would involve data manipulation that would move the system away from the realms of biological plausibility, which would detract from our main aims. However, one interesting direction for future work related to this involves using different input data subsets for each ensemble member. This can increase diversity in the ensemble which has been shown to boost performance in many circumstances [<xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref049">49</xref>], a finding that seems to carry over to our spiking ensemble system according to the observations on diversity described in the previous section. Preliminary experiments were carried out in which each SEM classifier was fed its own distinct and separate dataset (all classifiers were fed in parallel, with an expanded, separate set of input neurons for each classifier, rather than them all using the same ones as in the setup described earlier in this paper). A significant increase in the overall ensemble performance after training was observed as shown in <xref ref-type="fig" rid="pcbi.1005137.g015">Fig 15</xref>. Further work needs to be done to investigate the generalization of these results and to analyse differences in learning dynamics for the ensemble system with single (one set for all classifier) and multiple (different sets for each classifier) input data sets. The issue of how such multiple input data sets might impinge on biological plausibility must also be examined. A related area of further study is in applying the architecture to multiple sensory modes, with data from different sensory modes feeding into separate ensemble networks. Some of the biological evidence for ensemble learning, as discussed in the Introduction section, appears to involve the combination of multiple modes. Although we have tested the architecture using a single sensory mode, there is no reason why it cannot be extended to handle multiple modes.</p>
<fig id="pcbi.1005137.g015" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g015</object-id>
<label>Fig 15</label>
<caption>
<title>Training performances of ensemble networks using different datasets for each ensemble member (<italic>N</italic><sub><italic>E</italic></sub> = 5).</title>
<p>Individual classifier performances are shown in grey, and the overall ensemble (output layer) performance is shown in red. Results are for various input feature selection schemes on the handwritten integers problem as in the previous section.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g015" xlink:type="simple"/>
</fig>
<p>While our SEM ensemble model mimics the general MoE architecture, the overall process is not identical to that used in the classic MoE system [<xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref024">24</xref>]. A key difference is that the operation of the SEM gating WTA on the ensemble outputs is not based on immediate training input but is accumulated by slow additive synaptic plasticity over a relatively long time scale, whereas the standard MoE gating network instantaneously changes the strength of ensemble efferents for each input vector. Therefore our spiking system is not as adept at directly and quickly filtering out the wrong output from the ensemble WTAs when an output neuron in the ensemble fires for multiple classes. In this case the false spikes are also passed to the final layer through the enhanced connections. However, because such a neuron has a higher probability of firing for multiple classes, it dissipates its synaptic resource over multiple efferent connections, resulting in lower synaptic weights than in the case of a neuron which fires predominantly for one class. Hence the neuron that fires for multiple classes has less chance of winning at the final output layer WTA. Similarly, false spikes from the gating WTA will result in less chance of enhancing the corresponding target set of ITDP weights because of timing mismatch. In this way our spiking ensemble system can effectively filter out these false classifications, but using different learning dynamics from the classical system. However, if a large number of ensemble WTAs fire equally wrongly for the same class, the final output develops a higher chance of generating the wrong output. The standard architecture can of course suffer from the same problem [<xref ref-type="bibr" rid="pcbi.1005137.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref049">49</xref>]. This can happen, for instance, when two input images are hard to discriminate (such as the digits 3 and 8), even if the input subfeatures are randomly selected. Therefore the system is not entirely free from the feature selection problem as experienced in other ensemble learning methods. This limitation meant that in such circumstances simulations using high ensemble sizes did not significantly improve the overall performance (<xref ref-type="fig" rid="pcbi.1005137.g011">Fig 11</xref>), indicating a lack of ensemble diversity. Preliminary experiments indicated that by using an evolutionary search algorithm to evolve individual feature selection schemes for each ensemble member, diversity is increased, alleviating this problem greatly and significantly increasing performance. This is reminiscent of individually evolved receptive fields/input ‘features’ for spatially separated networks in the cortex and other areas. Future work will explore this issue more thoroughly. An interesting extension is the possibility of a form of evolutionary search being neuronally integrated into the current architecture [<xref ref-type="bibr" rid="pcbi.1005137.ref062">62</xref>] so that feature selection is performed in parallel with the other plastic processes, becoming part of the overall adaptation.</p>
<p>The empirical work on which we base our ITDP model [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>] was conducted <italic>in vitro</italic>. While this was of course because of the technical difficulty of conducting such research <italic>in vivo</italic>, it should be noted that work by Dong et al. (2008) [<xref ref-type="bibr" rid="pcbi.1005137.ref035">35</xref>] suggests that in some circumstances there can be activity dependent differences in the dynamics of heterosynaptic plasticity operating <italic>in vivo</italic>. While Dong et al. were looking at heterosynaptic plasticity in the hippocampus, they were not studying ITDP as defined in [<xref ref-type="bibr" rid="pcbi.1005137.ref026">26</xref>] and they were observing quite different neural pathways from Dudman et al. (specifically, Dong’s system involved Schaffer and commissural pathways, crucially without the different proximal and distal projections onto CA1 found in Dudman’s system, from EC and CA3 respectively—instead the two CA1 inputs are both from CA3). However, Dong et al. (2008) [<xref ref-type="bibr" rid="pcbi.1005137.ref035">35</xref>] made the interesting finding that in the system they were studying, <italic>in vivo</italic>, coincident activity of converging afferent pathways tended to switch the pathways to be LTP only or LTP/LTD <italic>depending</italic> on the activity states of the hippocampus [<xref ref-type="bibr" rid="pcbi.1005137.ref035">35</xref>]. If such findings extended to the system we have based our learning rule on, then of course our hypothesis would have to be revised. We are working under the assumption that the behaviour is stable <italic>in vivo</italic>. Recently Basu et al. (2016) [<xref ref-type="bibr" rid="pcbi.1005137.ref029">29</xref>] have provided some indirect evidence that the ITDP behaviour of the particular circuits we are basing our functional model on does hold <italic>in vivo</italic>. They cite studies of the temporal relation of oscillatory activity in the entorhinal cortex and the hippocampus <italic>in vivo</italic> that suggest that the disinhibitory gating mechanism enabled by the LRIPs may indeed be engaged during spatial behavior [<xref ref-type="bibr" rid="pcbi.1005137.ref063">63</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref064">64</xref>] and associational learning [<xref ref-type="bibr" rid="pcbi.1005137.ref065">65</xref>]. For example, during running and memory tasks, fast gamma oscillations (100Hz) arising from EC are observed in CA1 and precede the slow gamma oscillations (50Hz) in CA1, which are thought to reflect the CA3 pyramidal neuron input [<xref ref-type="bibr" rid="pcbi.1005137.ref063">63</xref>]. Crucially, EC-CA1 gamma activity and CA3-CA1 gamma activity display a 90° phase offset during theta frequency oscillations (8 to 9Hz) [<xref ref-type="bibr" rid="pcbi.1005137.ref063">63</xref>] which is consistent with a 20-25ms time delay. However, since any ensemble learning of the kind we have presented here would be part of a wider ‘cognitive architecture’, it is interesting to speculate that some activity dependent influence on the dynamics of such learning might occur in the bigger picture (e.g. moderating or switching on/off ensemble learning in certain conditions).</p>
<p>For reasons discussed earlier in this paper, ITDP seems a very good candidate for involvement in a biological mechanism ideal for combining ensemble member outputs, but it was naturally interesting to also attempt an all STDP implementation. Although we had imagined interference effects would compromise its learning ability, this version of the architecture performed surprisingly well. When the gating network performed relatively poorly, the STDP version compared very favourably with the ITDP version. However, with good (or at least reasonably) performing gating networks the ITDP version was significantly better over all conditions. This highlighted the dependence of the ITDP architecture on a gating network that achieves reasonable performance in agreement with the similar findings from the initial more abstract (voter) model. This shows that there is a small price to pay for the advantage the ITDP process confers, namely that it strengthens connections without a need for the corresponding final output neuron to be firing, thus providing a strong guiding function. The various methods for reducing this reliance (or at least ensuring the gating performance is always reasonable) that were outlined in the previous section will be the subject of future work. Preliminary analysis, as discussed in the previous section, suggests that there are some very interesting parallels between the ways the successful ITDP and STDP architectures operated, notably that the best performing ensemble WTA neurons in the STDP version had a guiding role functionally similar to that of the gating network in the ITDP version. While the differences and commonalities between ITDP and STDP dynamics in combining ensemble classifiers were briefly discussed in relation to the preliminary experiments, a more thorough comparative analysis of the effects of various conditions on both learning schemes will be addressed in the future work. Certainly the ITDP vs STDP work undertaken so far suggests that STDP-only architectures are another plausible hypothesis for ensemble learning in populations of biological spiking networks.</p>
<p>Lateral inhibition in the SEM networks—which provides the competition mechanism in the WTA circuits—is modeled as a common global signal that depends on the activity of the neurons in the network [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>]. This effectively models a form of strong uniform local lateral inhibition as widely experimentally observed in the cortex [<xref ref-type="bibr" rid="pcbi.1005137.ref066">66</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref067">67</xref>]. This inhibition mechanism is a core part of the SEM network dynamics and reflects the fact that they are small locally organised networks. We assume multiple such networks act as the ensemble members in our architecture. However, it might be possible to model the ensemble layer by a bigger single group of neurons which inhibit each other according to a ‘Mexican hat’ type function. Since with this form of inhibition (which is also commonly assumed [<xref ref-type="bibr" rid="pcbi.1005137.ref068">68</xref>]) the effect drops off with distance, with strong interaction among nearby neurons, a set of overlapping networks could emerge that function similarly to a smoothed version of multiple WTA circuits.</p>
<p>Dealing with arbitrary (unknown) numbers of classes with our ITDP ensemble architecture in a general unsupervised manner is a challenging future direction, although an individual SEM network with a sufficient number of output neurons has been shown to perform unsupervised clustering of a given dataset to some extent [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>]. It might be possible to employ a higher control to vary the number of classes in a supervised way as shown in [<xref ref-type="bibr" rid="pcbi.1005137.ref072">72</xref>]. More preferably, the smoothed version of a lateral inhibition mechanism using the Mexican hat topology may be capable of dealing with unknown numbers of classes in a more biologically plausible way by incorporating more sophisticated synaptic and neuromodulatory mechanisms.</p>
<p>The novel architecture presented here demonstrates for the first time how unsupervised (or indeed any form of) ensemble learning can be neurally implemented with populations of spiking networks. Our results show that the ensemble performs better than individual networks. The lack of diversity within the population, which sometimes becomes apparent, will be tackled in the next phase of work as outlined above. It is also possible that the relative strength of the ensemble method, in terms of efficiency of learning, might change when reducing the time spent on learning in the SEM networks (i.e. there may be an interesting resource/performance trade-off). This issue will also be investigated.</p>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec016">
<title>Analytic Ensemble Model</title>
<sec id="sec017">
<title>Derivation of expression for synaptic weights under the influence of ITDP</title>
<p>From Eqs <xref ref-type="disp-formula" rid="pcbi.1005137.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1005137.e004">4</xref>, we derived the expected value of the weight <italic>w</italic> at equilibrium under constant presynaptic firing probabilities to give the expression in <xref ref-type="disp-formula" rid="pcbi.1005137.e005">Eq 5</xref> as follows.
<disp-formula id="pcbi.1005137.e025"><alternatives><graphic id="pcbi.1005137.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>w</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>w</mml:mi></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>¬</mml:mo> <mml:mi>m</mml:mi> <mml:mo>∧</mml:mo> <mml:mo>¬</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>Solving for <italic>w</italic> progresses thus:
<disp-formula id="pcbi.1005137.e026"><alternatives><graphic id="pcbi.1005137.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>a</mml:mi></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula></p>
<p>Taking logs on both sides of <xref ref-type="disp-formula" rid="pcbi.1005137.e026">Eq 15</xref> to pull out <italic>w</italic> gives
<disp-formula id="pcbi.1005137.e027"><alternatives><graphic id="pcbi.1005137.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mtext>E</mml:mtext> <mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>w</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>This gives the expected value of <italic>w</italic> at equilibrium expressed in terms of the two probabilities <italic>p</italic>(<italic>m</italic>) and <italic>p</italic>(<italic>g</italic>).</p>
</sec>
<sec id="sec018">
<title>Analytic solution of ITDP weights</title>
<p>In practice, a voter in our analytic, abstract ensemble model emulates an abstract classifier which is assumed to have been fully trained in advance using an arbitrary set of input data. A typical expression of the statistical decision follows the Bayesian formalism, where the firing probability of each voter neuron <italic>m</italic><sub><italic>i</italic></sub> represents the posterior probability <italic>p</italic>(<italic>m</italic><sub><italic>i</italic></sub>|<bold>x</bold>) of the corresponding class label for a given input vector. The input vectors for each ensemble voter are distinct measurements of the raw input data (e.g. determined by using different feature subsets for each voter). A voter outputs a decision with probability one (∑<sub><italic>i</italic></sub> <italic>p</italic>(<italic>m</italic><sub><italic>i</italic></sub>|<bold>x</bold>) = 1) by exclusively firing one of its neurons according to sWTA mechanism. We assume that the input measurements for different voters ensure the ideal diversity of the ensemble so that the decision outputs of voters are independent of each other. We set the number of neurons in a voter to the number of possible decisions (classes) <italic>N</italic><sub><italic>C</italic></sub>; the firing probabilities of the neurons for the presented sample comprises a probability vector. The probability vectors of a voter are defined differently for each sample, comprising <italic>M</italic> probability vectors of size <italic>N</italic><sub><italic>C</italic></sub> where <italic>M</italic> is the number of data samples and <italic>N</italic><sub><italic>C</italic></sub> is the number of existing classes (equal to the number of voter output neurons). The statistics of probability vectors for each pattern class are designed differently in order to emulate the classification capability of voters which is assumed to be fully learnt in advance.</p>
<p>The analytic solution for ITDP learning for the ensemble system is similar to the previous three node formulation, as each connection weight of the ensemble network is estimated by assuming zero expected value of weight change once equilibrium has been reached. Recall the weight update Eqs <xref ref-type="disp-formula" rid="pcbi.1005137.e004">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1005137.e025">14</xref>, which are now written as the sum of the weight changes made from each presented input sample. Consider the probability of sample presentations for <italic>x</italic><sub><italic>l</italic></sub> during ITDP learning as <italic>p</italic>(<italic>x</italic><sub><italic>l</italic></sub>), where <inline-formula id="pcbi.1005137.e028"><alternatives><graphic id="pcbi.1005137.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The expected change of individual weights by ITDP can be written as the sum of all long term weight changes occurring at each sample presentation in the same way as in <xref ref-type="disp-formula" rid="pcbi.1005137.e025">Eq 14</xref>,
<disp-formula id="pcbi.1005137.e029"><alternatives><graphic id="pcbi.1005137.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>p</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>}</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <inline-formula id="pcbi.1005137.e030"><alternatives><graphic id="pcbi.1005137.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the firing probability of the <italic>i</italic>th neuron of the <italic>j</italic>th ensemble voter for input sample <italic>x</italic><sub><italic>l</italic></sub>, <inline-formula id="pcbi.1005137.e031"><alternatives><graphic id="pcbi.1005137.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the weight from <inline-formula id="pcbi.1005137.e032"><alternatives><graphic id="pcbi.1005137.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> to the <italic>k</italic>th neuron (<italic>f</italic><sub><italic>k</italic></sub>) of the final voter, and <italic>p</italic>(<italic>g</italic><sub><italic>k</italic></sub>|<italic>x</italic><sub><italic>l</italic></sub>) is the firing probability of the corresponding gating voter neuron which projects to <italic>f</italic><sub><italic>k</italic></sub>. Assuming the constant probability of every sample presentation and solving for <inline-formula id="pcbi.1005137.e033"><alternatives><graphic id="pcbi.1005137.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> at its equilibrium gives the following analytic solution of weight convergence:
<disp-formula id="pcbi.1005137.e034"><alternatives><graphic id="pcbi.1005137.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mtext>log</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:msubsup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>g</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where the constant probability of sample presentation <italic>p</italic>(<italic>x</italic><sub><italic>l</italic></sub>) = 1/<italic>M</italic> has been eliminated from the equation.</p>
</sec>
<sec id="sec019">
<title>Analytic solutions of final voter firing probabilities</title>
<p>While it is sufficient to express the behaviours of the ensemble voters and the gating voter using pre-determined firing probabilities for the purpose of obtaining weight convergence, the firing probabilities of neurons in the final voter are calculated by integrating the ‘EPSP’s from all presynaptic spikes. Taking the stochastic winner-takes-all Poissonian spiking formulation [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], the firing probability of neuron <italic>k</italic> of the final voter at a discrete time <italic>t</italic> is written as:
<disp-formula id="pcbi.1005137.e035"><alternatives><graphic id="pcbi.1005137.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula> <disp-formula id="pcbi.1005137.e036"><alternatives><graphic id="pcbi.1005137.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>s</mml:mi></mml:munder> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
where <italic>u</italic><sub><italic>k</italic></sub>(<italic>t</italic>) is the EPSP of the final voter neuron <italic>k</italic> at time <italic>t</italic>, <italic>N</italic><sub><italic>E</italic></sub> is the ensemble size, and <inline-formula id="pcbi.1005137.e037"><alternatives><graphic id="pcbi.1005137.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the time of the <italic>s</italic>’th spike output by neuron <inline-formula id="pcbi.1005137.e038"><alternatives><graphic id="pcbi.1005137.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. The EPSP response kernel <italic>ϵ</italic>(<italic>t</italic>) could be simply modelled as a rectangular function which integrates all the past spikes within a finite time window, or we could use exponential decay to smoothly decrease the potential. However, for the sake of computational convenience for understanding the analytic solution of long term final voter behaviour, we only integrate the instantaneous presynaptic spikes, which is equivalent to using a unit impulse function for <italic>ϵ</italic>(<italic>t</italic>), where all the spiking events are clocked at every discrete time instance as assumed in the voter ensemble system. The average values of the final voter probabilities can be calculated by solving the expected values of time-varying final voter probabilities themselves. At each discrete time <italic>t</italic>, the state of the ensemble is always defined by the firing of <italic>N</italic><sub><italic>E</italic></sub> neurons from the ensemble voters (one of the <italic>N</italic><sub><italic>C</italic></sub> neurons fires in each voter), resulting in <inline-formula id="pcbi.1005137.e039"><alternatives><graphic id="pcbi.1005137.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>N</mml:mi> <mml:mrow><mml:mi>C</mml:mi></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> possible states of the ensemble. Given a set of ensemble firing states <italic>S</italic> = {<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>, …, <italic>s</italic><sub><italic>N</italic><sub><italic>S</italic></sub></sub>}, let us define an index function <italic>R</italic>(<italic>q</italic>, <italic>j</italic>) which gives the index of the firing neuron of the voter <italic>j</italic> at the ensemble state <italic>s</italic><sub><italic>q</italic></sub>. The function can be defined to return <italic>d</italic> + 1 where <italic>d</italic> is the <italic>j</italic>’th digit of the <italic>N</italic><sub><italic>C</italic></sub>-ary number which is equivalent to decimal number <italic>q</italic>. For example, if <italic>N</italic><sub><italic>C</italic></sub> = 4 and <italic>N</italic><sub><italic>E</italic></sub> = 3, then <italic>R</italic>(25, 2) = 2 + 1 = 3 (25 is 121 as a quaternary number). Using this index function, the probability of the occurrence of the state <italic>s</italic><sub><italic>q</italic></sub> under the presentation of sample <italic>x</italic><sub><italic>l</italic></sub> can be written very succinctly as a joint probability of neurons firing:
<disp-formula id="pcbi.1005137.e040"><alternatives><graphic id="pcbi.1005137.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>The weighted sum of spikes from the ensemble in state <italic>s</italic><sub><italic>q</italic></sub> arriving at the postsynaptic neuron <italic>k</italic> is
<disp-formula id="pcbi.1005137.e041"><alternatives><graphic id="pcbi.1005137.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>E</mml:mi></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula></p>
<p>The probability of a final voter neuron <italic>p</italic>(<italic>f</italic><sub><italic>k</italic></sub>|<italic>x</italic><sub><italic>l</italic></sub>) at ensemble state <italic>q</italic> is then calculated as in <xref ref-type="disp-formula" rid="pcbi.1005137.e035">Eq 19</xref>. Now we can calculate the expected probability of a final voter neuron under the presentation of sample <italic>x</italic><sub><italic>l</italic></sub> as:
<disp-formula id="pcbi.1005137.e042"><alternatives><graphic id="pcbi.1005137.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>q</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>S</mml:mi></mml:msub></mml:munderover> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula></p>
<p>The expected firing probability of the final net neuron <italic>k</italic> under the presentation of the samples from class <italic>c</italic> can be written as follows by the law of total probability:
<disp-formula id="pcbi.1005137.e043"><alternatives><graphic id="pcbi.1005137.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula> <disp-formula id="pcbi.1005137.e044"><alternatives><graphic id="pcbi.1005137.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>M</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:munderover> <mml:mtext>E</mml:mtext> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>This gives the <xref ref-type="disp-formula" rid="pcbi.1005137.e011">Eq 7</xref>, the expected (long term) firing probability of final net neuron <italic>k</italic> under the presentation of class <italic>c</italic>.</p>
</sec>
<sec id="sec020">
<title>Simulation of voter ensemble network</title>
<p>The detailed methods for the iterated simulation of the simple analytic spiking ensemble system are as follows.</p>
<p>During the learning phase, the input classes for the ensemble and gating voters were equally presented by turn, which led to the same presentation probability of every input class <italic>p</italic>(<italic>c</italic>) = 1/<italic>N</italic><sub><italic>C</italic></sub>. Consider the input dataset as being divided into <italic>N</italic><sub><italic>C</italic></sub> subsets belonging to each class; <italic>X</italic><sub><italic>c</italic></sub> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>n</italic></sub>, …, <italic>x</italic><sub><italic>M</italic><sub><italic>c</italic></sub></sub>} where <italic>c</italic> = 1, 2, …, <italic>N</italic><sub><italic>C</italic></sub>. The following steps were performed at each timestep <italic>t</italic> = (1, 2, …, <italic>T</italic>) with the learning rate <italic>η</italic> = 0.001 and the shift constant <italic>a</italic> = <italic>e</italic><sup>5</sup>.</p>
<list list-type="bullet">
<list-item>
<p>Present a sample <italic>x</italic><sub><italic>n</italic></sub> from the subset <italic>X</italic><sub><italic>c</italic></sub>, where <italic>n</italic> = {(<italic>t</italic> − 1) div <italic>N</italic><sub><italic>C</italic></sub>} + 1 and <italic>c</italic> = {(<italic>t</italic> − 1)mod<italic>N</italic><sub><italic>C</italic></sub>} + 1.</p>
</list-item>
<list-item>
<p>All ensemble voters and gating voter fire according to their firing probabilities <inline-formula id="pcbi.1005137.e045"><alternatives><graphic id="pcbi.1005137.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic>p</italic>(<italic>g</italic><sub><italic>k</italic></sub>|<italic>x</italic><sub><italic>n</italic></sub>).</p>
</list-item>
<list-item>
<p>All weights are updated by ITDP as <italic>w</italic> ← <italic>w</italic> + <italic>ηΔw</italic>. For every weight <inline-formula id="pcbi.1005137.e046"><alternatives><graphic id="pcbi.1005137.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1005137.e047"><alternatives><graphic id="pcbi.1005137.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow> <mml:mi>j</mml:mi></mml:msubsup></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, if both the ensemble neuron <inline-formula id="pcbi.1005137.e048"><alternatives><graphic id="pcbi.1005137.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and the gating neuron <italic>g</italic><sub><italic>k</italic></sub> fire. If only one of those two fires, decrease the weight by -1. If neither of them fires, do nothing.</p>
</list-item>
</list>
<p>The measuring phase was run for every <italic>X</italic><sub><italic>c</italic></sub>, each for the duration of <italic>T</italic><sub><italic>m</italic></sub> = <italic>T</italic>/<italic>N</italic><sub><italic>C</italic></sub>, in order to see how well one of the neurons in the final voter fired exclusively for each class. The measuring phase for each class presentation proceeded as follows:</p>
<list list-type="bullet">
<list-item>
<p>All ensemble voters and the gating voter fire according to their firing probabilities <inline-formula id="pcbi.1005137.e049"><alternatives><graphic id="pcbi.1005137.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic>p</italic>(<italic>g</italic><sub><italic>k</italic></sub>|<italic>x</italic><sub><italic>n</italic></sub>).</p>
</list-item>
<list-item>
<p>Each final voter neuron fires after calculating its firing probabilities according to the weighted integration of all presynaptic spikes as in <xref ref-type="disp-formula" rid="pcbi.1005137.e035">Eq 19</xref>.</p>
</list-item>
</list>
<p>In order to compare the final voter output from the measuring phase with the analytic solution, we calculate all the momentary probabilities of each final voter neuron during simulation and check their averages with <xref ref-type="disp-formula" rid="pcbi.1005137.e011">Eq 7</xref>.</p>
</sec>
<sec id="sec021">
<title>Performance measure</title>
<p>The NCE of a voter is calculated over the input set as:
<disp-formula id="pcbi.1005137.e050"><alternatives><graphic id="pcbi.1005137.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>NCE</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>,</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula>
where <italic>C</italic> = {<italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>, …, <italic>c</italic><sub><italic>N</italic><sub><italic>C</italic></sub></sub>} is the class of presented inputs, and <italic>F</italic> = {<italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub>, …, <italic>f</italic><sub><italic>N</italic><sub><italic>C</italic></sub></sub>} denotes the discrete random variable defined by the firing probabilities of the voter neurons <italic>f</italic><sub><italic>i</italic></sub> for each input class, and <italic>H</italic> is the standard entropy function. NCE can be expressed in terms of the joint probability distribution <italic>P</italic>(<italic>C</italic>, <italic>F</italic>), which has <italic>N</italic><sub><italic>C</italic></sub>×<italic>N</italic><sub><italic>C</italic></sub> elements, as follows:
<disp-formula id="pcbi.1005137.e051"><alternatives><graphic id="pcbi.1005137.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>|</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:msubsup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula> <disp-formula id="pcbi.1005137.e052"><alternatives><graphic id="pcbi.1005137.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e052" xlink:type="simple"/><mml:math display="block" id="M52"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>,</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>N</mml:mi> <mml:mi>C</mml:mi></mml:msub></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>log</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula>
where we can analytically calculate <italic>p</italic>(<italic>c</italic><sub><italic>n</italic></sub>, <italic>f</italic><sub><italic>i</italic></sub>) from a probability table defined as in <xref ref-type="fig" rid="pcbi.1005137.g003">Fig 3</xref> or it can be measured from a numerical simulation by counting all the spikes over the simulation.</p>
</sec>
</sec>
<sec id="sec022">
<title>SEM Network Ensemble Learning</title>
<p>The detailed methods for the full SEM-ITDP ensemble architecture are as follows.</p>
<sec id="sec023">
<title>Bayesian dynamics</title>
<p>According to the formulation given in [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], the overall network dynamics can be explained in terms of spike-based Bayesian computation. The combined firing activity of all <italic>z</italic> neurons in a WTA circuit can be expressed as the sum of <italic>K</italic> independent Poisson processes, which represents an inhomogeneous Poisson process of the WTA circuit with rate:
<disp-formula id="pcbi.1005137.e053"><alternatives><graphic id="pcbi.1005137.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula></p>
<p>In an infinitesimal time interval [<italic>t</italic>, <italic>t</italic> + <italic>dt</italic>], the firing probabilities of a WTA circuit and its neurons are <italic>R</italic>(<italic>t</italic>)<italic>dt</italic> and <italic>r</italic><sub><italic>k</italic></sub>(<italic>t</italic>)<italic>dt</italic> respectively. Thus if we observe a neural spike in a WTA circuit within this time interval, the conditional probability that this spike originated from neuron <italic>z</italic><sub><italic>k</italic></sub> is expressed as
<disp-formula id="pcbi.1005137.e054"><alternatives><graphic id="pcbi.1005137.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>q</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>R</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula></p>
<p>Thus a firing event of <italic>z</italic><sub><italic>k</italic></sub> can be thought as a sample drawn from the conditional probability <italic>q</italic><sub><italic>k</italic></sub>(<italic>t</italic>) which is equivalent to the posterior distribution of hidden cause <italic>k</italic>, given the evidence represented by the input neuron activation vector <bold>y</bold>(<italic>t</italic>) = {<italic>y</italic><sub>1</sub>(<italic>t</italic>), <italic>y</italic><sub>2</sub>(<italic>t</italic>), …, <italic>y</italic><sub><italic>n</italic></sub>(<italic>t</italic>)} under the network weights <bold>w</bold>. By following Bayes’ rule, we can identify the network dynamics as a posterior probability which is expressed using prior and likelihood distributions as:
<disp-formula id="pcbi.1005137.e055"><alternatives><graphic id="pcbi.1005137.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>q</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:msup> <mml:mo>·</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>⇒</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">w</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>The input neurons encode the actual observable input variables <italic>x</italic><sub><italic>j</italic></sub>s with a population code in order to assess different combinations of input neuron spiking states for every possible input vector <bold>x</bold> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>N</italic></sub>} from the raw input data to be classified. The state of an input variable <italic>x</italic> is encoded using a group of input neurons, where only one neuron in the group can fire at each time instance to represent the instantaneous value of <italic>x</italic>(<italic>t</italic>). Therefore, together with the total prior probabilities ∑<italic>p</italic>(<italic>k</italic>|<bold>w</bold>) = 1, the Bayesian computation of the network shown in <xref ref-type="disp-formula" rid="pcbi.1005137.e055">Eq 31</xref> operates under the constraints,
<disp-formula id="pcbi.1005137.e056"><alternatives><graphic id="pcbi.1005137.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mo>∀</mml:mo> <mml:mi>k</mml:mi> <mml:mo>:</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula>
where <italic>G</italic><sub><italic>j</italic></sub> represents a set of all possible values that an instantaneous input evidence <italic>x</italic><sub><italic>j</italic></sub> can have, which is also equivalent to the discretized value of each input variable in the continuous case. This means that an input evidence <italic>x</italic><sub><italic>j</italic></sub> for a feature <italic>j</italic> of observed data is encoded as a group of neuronal activations <italic>y</italic><sub><italic>i</italic></sub>. If the set of possible value of <italic>x</italic><sub><italic>j</italic></sub> consists of <italic>m</italic> values <italic>G</italic><sub><italic>j</italic></sub> = [<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, …, <italic>v</italic><sub><italic>m</italic></sub>], the input <italic>x</italic><sub><italic>j</italic></sub> is encoded using <italic>m</italic> input neurons. Therefore, if input data is given as a <italic>N</italic> (<italic>j</italic> = 1, …, <italic>N</italic>) dimensional vector, the total number of input neurons is <italic>mN</italic>.</p>
</sec>
<sec id="sec024">
<title>Synaptic and neuronal plasticities</title>
<p>Synaptic plasticity in the STDP connections (<xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>) captures both biological plausibility and the computational requirement for Bayesian inference. The LTP part of the STDP curve used follows the shape of EPSPs at the synapses [<xref ref-type="bibr" rid="pcbi.1005137.ref030">30</xref>], which is similar to biological STDP, in that the backpropagating action potential from a postsynaptic neuron interacts with the presynaptic EPSP arriving at the synapse. The magnitude of the weight update depends on the inverse exponential of the synaptic weight itself to prevent unbounded weight growth. Let us denote the connection weight from the <italic>i</italic>’th input neuron to the <italic>k</italic>’th ensemble layer neuron as <italic>w</italic><sub><italic>ki</italic></sub>, where now <italic>k</italic> indicates the index for the entire layer of ensemble neurons (except the gating network), not the index within each WTA circuit. The synaptic update at time <italic>t</italic> is given by:
<disp-formula id="pcbi.1005137.e057"><alternatives><graphic id="pcbi.1005137.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>c</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula>
where <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is the sum of EPSPs evoked by all presynaptic spikes as in <xref ref-type="disp-formula" rid="pcbi.1005137.e015">Eq 11</xref>, and <italic>c</italic> (which is set to <italic>e</italic><sup>5</sup> throughout the experiment) is a constant which determines the upper bound of synaptic weights. The LTD part was set to decrease by a constant amount of 1. Given the EPSP caused by presynaptic spikes, synaptic update occurs only at the moment of a postsynaptic neuron firing, with a certain learning rate. This plasticity rule can exhibit the stimulus frequency dependent behaviour of biological synapses which has been observed in biological STDP experiments [<xref ref-type="bibr" rid="pcbi.1005137.ref069">69</xref>], where the shape of the plasticity curve (including the traditional hyperbolic curve of the phenomenological model [<xref ref-type="bibr" rid="pcbi.1005137.ref070">70</xref>, <xref ref-type="bibr" rid="pcbi.1005137.ref071">71</xref>]) depends on the repetition frequency of the delayed pairing of pre and postsynaptic stimulations.</p>
<p>In contrast to the logical ITDP model, the SEM ITDP ensemble uses the more biologically realistic ITDP plasticity curve shown in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref> middle (Simplified ITDP curve). The continuous ITDP curve also serves for dealing with the irregular spike trains output by the presynaptic SEM networks, whereas the logical ITDP ensemble model concurrently fires a single spike from each voter. The ITDP plasticity curve is defined as a function of the time difference between two input stimuli using a Gaussian factor. As in the logical ITDP model, the peak LTP at an input time delay of -20ms (where distal input precedes proximal input by 20ms) in biological ITDP is ignored for computational convenience, by assuming that the axon converging on the proximal synapse already has 20ms of conduction delay. Thus the plasticity curve was shifted to have its peak value at zero delay. The change of the ITDP synapse from the <italic>k</italic>th ensemble layer neuron to the <italic>f</italic>th neuron of the final output WTA (see <xref ref-type="fig" rid="pcbi.1005137.g004">Fig 4</xref>) can be written as:
<disp-formula id="pcbi.1005137.e058"><alternatives><graphic id="pcbi.1005137.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>c</mml:mi> <mml:mo>·</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula> <disp-formula id="pcbi.1005137.e059"><alternatives><graphic id="pcbi.1005137.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>f</mml:mi> <mml:mi>G</mml:mi></mml:msubsup></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:munder> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mi>f</mml:mi> <mml:mi>G</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula> <disp-formula id="pcbi.1005137.e060"><alternatives><graphic id="pcbi.1005137.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula>
where <italic>h</italic><sub><italic>fk</italic></sub>(<italic>t</italic>) is the sum of all synaptic potentiations evoked by the spike time differences between the proximal (from ensemble neurons, <italic>s</italic><sub><italic>k</italic></sub>) and distal (from gating neurons <inline-formula id="pcbi.1005137.e061"><alternatives><graphic id="pcbi.1005137.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msubsup><mml:mi>s</mml:mi> <mml:mi>f</mml:mi> <mml:mi>G</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>) inputs, calculated by the Gaussian function <italic>g</italic>(<italic>x</italic>). The proximal weight <italic>w</italic><sub><italic>fk</italic></sub> is updated whenever either of the two presynaptic neurons spike. In the same way as the STDP update rule, the ITDP synaptic change is regulated by an inverse exponential dependence on the weight itself and a constant synaptic depression of 1, which results in the simplified ITDP curve shown in <xref ref-type="fig" rid="pcbi.1005137.g002">Fig 2</xref>. The variance of <italic>g</italic>(<italic>x</italic>) was set to <italic>σ</italic><sup>2</sup> = 1.5×10<sup>−4</sup>, where the <italic>x</italic> axis represents the spike time difference in seconds.</p>
<p>The self-excitability of the WTA output neurons is modelled in a way that is directly analogous to the plasticity of the synaptic weights. Recalling the membrane potential <italic>u</italic>(<italic>t</italic>) of a SEM circuit neuron in <xref ref-type="disp-formula" rid="pcbi.1005137.e013">Eq 9</xref>, the excitability <italic>w</italic><sub><italic>k</italic>0</sub> of neuron <italic>z</italic><sub><italic>k</italic></sub> is increased whenever it fires (<italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = 1) as a function of the inverse exponential of <italic>w</italic><sub><italic>k</italic>0</sub> and is decreased by 1 if not firing (<italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = 0).
<disp-formula id="pcbi.1005137.e062"><alternatives><graphic id="pcbi.1005137.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e062" xlink:type="simple"/><mml:math display="block" id="M62"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula></p>
<p>The update of <italic>w</italic><sub><italic>k</italic>0</sub> is circuit-spike triggered, which means that the excitabilities of all neurons in the WTA circuit are updated if one of the neurons fires. Therefore the value of <italic>w</italic><sub><italic>k</italic>0</sub> converges to satisfy the normalization constraint as a prior probability which is necessary for the above mentioned Bayesian computation.</p>
<p>All the plasticities of STDP, ITDP, and neuronal excitability described above are updated at their corresponding trigger conditions by <italic>w</italic> ← <italic>w</italic> + <italic>η</italic>Δ<italic>w</italic>. The learning rates (<italic>η</italic>) of every individual synapse and excitability are adaptively changed by a variance tracking rule [<xref ref-type="bibr" rid="pcbi.1005137.ref043">43</xref>] as:
<disp-formula id="pcbi.1005137.e063"><alternatives><graphic id="pcbi.1005137.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e063" xlink:type="simple"/><mml:math display="block" id="M63"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>η</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>e</mml:mi> <mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula> <disp-formula id="pcbi.1005137.e064"><alternatives><graphic id="pcbi.1005137.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e064" xlink:type="simple"/><mml:math display="block" id="M64"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>←</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>w</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula> <disp-formula id="pcbi.1005137.e065"><alternatives><graphic id="pcbi.1005137.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e065" xlink:type="simple"/><mml:math display="block" id="M65"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>←</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>η</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>w</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(40)</label></disp-formula>
where <italic>m</italic><sub>1</sub> and <italic>m</italic><sub>2</sub> are the first and the second moments of the corresponding learning variable. <italic>μ</italic> is a constant which is globally set to 0.01. The learning rate and moments are updated together whenever the update of a learning variable is triggered.</p>
</sec>
<sec id="sec025">
<title>SEM-ITDP experiments</title>
<p>The core (common) numerical details of the SEM-ITDP experiments are as follows: <italic>T</italic><sub><italic>present</italic></sub> = 40<italic>ms</italic>, <italic>T</italic><sub><italic>rest</italic></sub> = 40<italic>ms</italic>. During input presentations, one of the two input neurons that encode a pixel state fires with a constant rate of 40Hz.</p>
<p>The common network parameters (used in all experiments) were as follows; <italic>A</italic><sub>inh</sub> = 3000, <italic>O</italic><sub>inh</sub> = −550, <italic>τ</italic><sub>inh</sub> = 0.005sec for neuronal inhibition, <italic>τ</italic><sub><italic>s</italic></sub> = 0.015sec, <italic>τ</italic><sub><italic>f</italic></sub> = 0.001sec, <italic>A</italic><sub>EPSP</sub> = {<italic>τ</italic><sub><italic>s</italic></sub>/(<italic>τ</italic><sub><italic>s</italic></sub>−<italic>τ</italic><sub><italic>f</italic></sub>)}(<italic>τ</italic><sub><italic>s</italic></sub>/<italic>τ</italic><sub><italic>f</italic></sub>)<sup><italic>τ</italic><sub><italic>f</italic></sub>/(<italic>τ</italic><sub><italic>s</italic></sub>−<italic>τ</italic><sub><italic>f</italic></sub>)</sup> for the EPSP kernel. Due to the smaller number of afferent connections to the final WTA than the ensemble layer WTAs, its global inhibition level was shifted (increasing output by giving less inhibition) by an amount proportional to the ensemble size <italic>N</italic><sub><italic>E</italic></sub> (i.e. related to the number of presynaptic neurons) in order to match the output intensity to those of the ensemble neurons. The inhibition level of the final WTA circuit was set as <inline-formula id="pcbi.1005137.e066"><alternatives><graphic id="pcbi.1005137.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mtext>inh</mml:mtext> <mml:mtext>f</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>3000</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005137.e067"><alternatives><graphic id="pcbi.1005137.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:msubsup><mml:mi>O</mml:mi> <mml:mtext>inh</mml:mtext> <mml:mtext>f</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>550</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, with the level of shift <italic>I</italic><sub><italic>s</italic></sub> = 560 − 4<italic>N</italic><sub><italic>E</italic></sub>.</p>
</sec>
<sec id="sec026">
<title>Feature selection using Gaussian distributions</title>
<p>The normal Gaussian selection scheme worked by sampling pixels from 2D normal distributions with different means. The distribution function for the <italic>i</italic>th ensemble network was:
<disp-formula id="pcbi.1005137.e068"><alternatives><graphic id="pcbi.1005137.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>1</mml:mn> <mml:mo>×</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula>
with the variance <italic>σ</italic><sup>2</sup> = 49. Different means (<inline-formula id="pcbi.1005137.e069"><alternatives><graphic id="pcbi.1005137.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) for each ensemble WTA were located evenly on the active region of image to cover every region. Although the Gaussian means for each ensemble WTA can be evenly placed simply by using a regular lattice of different sizes on the image, their locations were stochastically generated by a simple optimization procedure in order to reduce any potential bias from a single specific formation of the means on the image (the random elements are also more biologically relevant). In order to reduce the computation time for the optimization, the mean positions were jittered by a small amount around the manually placed initial positions under a certain constraint (<xref ref-type="fig" rid="pcbi.1005137.g016">Fig 16</xref>). The initial mean positions were properly designed to be evenly scattered across the image for each ensemble size in order to prevent any biased placement of the generated mean positions. A simple iterative procedure for the random Gaussian mean placement is as follows:</p>
<list list-type="order"><list-item><p>Given the set of initial mean points (<inline-formula id="pcbi.1005137.e070"><alternatives><graphic id="pcbi.1005137.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>U</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>), <italic>i</italic> = {1, 2, …, <italic>N</italic><sub><italic>E</italic></sub>}, every mean point (<inline-formula id="pcbi.1005137.e071"><alternatives><graphic id="pcbi.1005137.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) is drawn by randomly jittering the corresponding initial point as: <inline-formula id="pcbi.1005137.e072"><alternatives><graphic id="pcbi.1005137.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>U</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005137.e073"><alternatives><graphic id="pcbi.1005137.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>U</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where Δ<italic>μ</italic><sub><italic>x</italic></sub> and Δ<italic>μ</italic><sub><italic>y</italic></sub> are randomly drawn in the range [−<italic>ϵ</italic>, <italic>ϵ</italic>].</p></list-item> <list-item><p>Repeat 1 until every mean point (<inline-formula id="pcbi.1005137.e074"><alternatives><graphic id="pcbi.1005137.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>x</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>y</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) is inside the inner region (which is surrounded by the green pixels as in <xref ref-type="fig" rid="pcbi.1005137.g016">Fig 16</xref>), where the minimum distance <italic>d</italic><sub><italic>min</italic></sub> between all pairs of mean points satisfies <italic>d</italic><sub><italic>min</italic></sub> &gt; <italic>δ</italic>.</p></list-item></list>
<fig id="pcbi.1005137.g016" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005137.g016</object-id>
<label>Fig 16</label>
<caption>
<title>Examples of random Gaussian mean placements for different <italic>N</italic><sub><italic>E</italic></sub> from the manually designed initial points (black points).</title>
<p>The red pixels represent the outer border of the active region of the image, and the yellow pixels represent a forbidden region which is 3 pixels thick. The jittered mean points were restricted to be placed inside the inner region (including the green pixels) which is surrounded by the inner border (green).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.g016" xlink:type="simple"/>
</fig>
<p>The parameters <italic>ϵ</italic> and <italic>δ</italic> are set for each ensemble sizes <italic>N</italic><sub><italic>E</italic></sub> = {5, 7, 9, 11, 13, 16, 20, 25} as: (<italic>ϵ</italic>, <italic>δ</italic>) = {(9, 14), (7, 10), (5, 9), (5, 7.5), (5, 7), (5, 5.5), (5, 4.5), (3, 4.2)}, which were found to allow the optimization process to execute in a reasonable time while producing reasonably evenly distributed mean points.</p>
<p>The stretched Gaussian distribution selected pixel subsets to form a bar shape (at different orientations) as shown in <xref ref-type="fig" rid="pcbi.1005137.g009">Fig 9</xref> bottom. The probability density function for stretched Gaussian distribution was:
<disp-formula id="pcbi.1005137.e075"><alternatives><graphic id="pcbi.1005137.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e075" xlink:type="simple"/><mml:math display="block" id="M75"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msubsup><mml:mo>Σ</mml:mo> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula>
where <bold>x</bold> = (<italic>x</italic>, <italic>y</italic>) is a random vector (mean at the origin), and Σ<sub><italic>i</italic></sub> is the covariance matrix (symmetric, positive definite) for the <italic>i</italic>th ensemble member. Each element of the inverse covariance matrix is written as:
<disp-formula id="pcbi.1005137.e076"><alternatives><graphic id="pcbi.1005137.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e076" xlink:type="simple"/><mml:math display="block" id="M76"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mo>Σ</mml:mo> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>=</mml:mo> <mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mi>a</mml:mi></mml:mtd> <mml:mtd><mml:mi>b</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>b</mml:mi></mml:mtd> <mml:mtd><mml:mi>c</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula> <disp-formula id="pcbi.1005137.e077"><alternatives><graphic id="pcbi.1005137.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mtext>cos</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mtext>sin</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mspace width="2.em"/><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext> <mml:mn>2</mml:mn> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>4</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext> <mml:mn>2</mml:mn> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>4</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mspace width="2.em"/><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mtext>sin</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:msup><mml:mtext>cos</mml:mtext> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(44)</label></disp-formula></p>
<p>The variances for the ellipsoids were set to <inline-formula id="pcbi.1005137.e078"><alternatives><graphic id="pcbi.1005137.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1005137.e079"><alternatives><graphic id="pcbi.1005137.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005137.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>625</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> identically for all ensemble members (i.e. the pixels practically form a bar shape), except its orientation is rotated by <italic>θ</italic><sub><italic>i</italic></sub> rad. Starting from <italic>θ</italic><sub>0</sub> = 0, the orientations are incremented by <italic>π</italic>/<italic>N</italic><sub><italic>E</italic></sub> for each successive distribution (i.e. <italic>θ</italic><sub><italic>i</italic></sub> = <italic>i</italic>.<italic>π</italic>/<italic>N</italic><sub><italic>E</italic></sub>).</p>
</sec>
</sec>
</sec>
<sec id="sec027">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1005137.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005137.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Text and figures giving full details of the methods and results of a validation of the analytic solutions for the initial abstract/simplified ensemble learning model (the voter ensemble model) by numerical simulation.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We acknowledge useful discussions with members of the INSIGHT consortium, particularly Chrisantha Fernando and Eors Szathmary. We also thank Dan Bush for helpful discussions relating to ITDP. We also thanks Wolfgang Maass and Michael Pfeiffer for access to SEM network implementation details.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005137.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laubach</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wessberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>M</given-names></name>. <article-title>Cortical ensemble activity increasingly predicts behaviour outcomes during learning of a motor task</article-title>. <source>Nature</source>. <year>2000</year>;<volume>405</volume>:<fpage>567</fpage>–<lpage>571</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35014604" xlink:type="simple">10.1038/35014604</ext-link></comment> <object-id pub-id-type="pmid">10850715</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cohen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>M</given-names></name>. <article-title>Reduction of Single-Neuron Firing Uncertainty by Cortical Ensembles during Motor Skill Learning</article-title>. <source>Journal of Neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>14</issue>):<fpage>3574</fpage>–<lpage>3582</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5361-03.2004" xlink:type="simple">10.1523/JNEUROSCI.5361-03.2004</ext-link></comment> <object-id pub-id-type="pmid">15071105</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Parrish</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gottfried</surname> <given-names>J</given-names></name>. <article-title>Aversive Learning Enhances Perceptual and Cortical Discrimination of Indiscriminable Odor Cues</article-title>. <source>Science</source>. <year>2008</year>;<volume>319</volume>:<fpage>1842</fpage>–<lpage>1845</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1152837" xlink:type="simple">10.1126/science.1152837</ext-link></comment> <object-id pub-id-type="pmid">18369149</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>Biologically Based Computational Models of High-Level Cognition</article-title>. <source>Science</source>. <year>2006</year>;<volume>314</volume>:<fpage>91</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1127242" xlink:type="simple">10.1126/science.1127242</ext-link></comment> <object-id pub-id-type="pmid">17023651</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref005">
<label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <chapter-title>Modeling integration and dissociation in brain and cognitive development</chapter-title>. In: <name name-style="western"><surname>Munakata</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MH</given-names></name>, editors. <source>Processes of Change in Brain and Cognitive Development: Attention and Performance XXI</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2006</year>. p. <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bock</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Fine</surname> <given-names>I</given-names></name>. <article-title>Anatomical and Functional Plasticity in Early Blind Individuals and the Mixture of Experts Architecture</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2014</year>;<volume>8</volume>(<issue>971</issue>):Article 971. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnhum.2014.00971" xlink:type="simple">10.3389/fnhum.2014.00971</ext-link></comment> <object-id pub-id-type="pmid">25566016</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kopell</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Whittington</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kramer</surname> <given-names>MA</given-names></name>. <article-title>Neuronal assembly dynamics in the beta1 frequency range permits short-term memory</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2011</year>;<volume>108</volume>(<issue>9</issue>):<fpage>3779</fpage>–<lpage>3784</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1019676108" xlink:type="simple">10.1073/pnas.1019676108</ext-link></comment> <object-id pub-id-type="pmid">21321198</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Karmos</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mehta</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Ulbert</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>. <article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title>. <source>Science</source>. <year>2008</year>;<volume>320</volume>(<issue>5872</issue>):<fpage>110</fpage>–<lpage>113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1154735" xlink:type="simple">10.1126/science.1154735</ext-link></comment> <object-id pub-id-type="pmid">18388295</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Varela</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lachaux</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rodriguez</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Martinerie</surname> <given-names>J</given-names></name>. <article-title>The brainweb: Phase synchronization and large-scale integration</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2001</year>;<volume>2</volume>(<issue>4</issue>):<fpage>229</fpage>–<lpage>239</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35067550" xlink:type="simple">10.1038/35067550</ext-link></comment> <object-id pub-id-type="pmid">11283746</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pascual-Leone</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hamilton</surname> <given-names>R</given-names></name>. <article-title>The metamodal organization of the brain</article-title>. <source>Progress in Brain Research</source>. <year>2001</year>;<volume>134</volume>:<fpage>427</fpage>–<lpage>445</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0079-6123(01)34028-1" xlink:type="simple">10.1016/S0079-6123(01)34028-1</ext-link></comment> <object-id pub-id-type="pmid">11702559</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pascual-Leone</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Amedi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fregni</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Merabet</surname> <given-names>LB</given-names></name>. <article-title>The plastic human brain cortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>2005</year>;<volume>28</volume>:<fpage>377</fpage>–<lpage>401</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.27.070203.144216" xlink:type="simple">10.1146/annurev.neuro.27.070203.144216</ext-link></comment> <object-id pub-id-type="pmid">16022601</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>. <article-title>Coding and transmission of information by neural ensembles</article-title>. <source>Trends in Neurosciences</source>. <year>2004</year>;<volume>27</volume>(<issue>4</issue>):<fpage>225</fpage>–<lpage>230</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2004.02.006" xlink:type="simple">10.1016/j.tins.2004.02.006</ext-link></comment> <object-id pub-id-type="pmid">15046882</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nicolelis</surname> <given-names>MAL</given-names></name>, <name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>. <article-title>Principles of neural ensemble physiology underlying the operation of brain-machine interfaces</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2009</year>;<volume>10</volume>(<issue>7</issue>):<fpage>530</fpage>–<lpage>540</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2653" xlink:type="simple">10.1038/nrn2653</ext-link></comment> <object-id pub-id-type="pmid">19543222</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moioli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Husbands</surname> <given-names>P</given-names></name>. <article-title>Neuronal Assembly Dynamics in Supervised and Unsupervised Learning Scenarios</article-title>. <source>Neural Computation</source>. <year>2013</year>;<volume>25</volume>:<fpage>2934</fpage>–<lpage>2975</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00502" xlink:type="simple">10.1162/NECO_a_00502</ext-link></comment> <object-id pub-id-type="pmid">23895050</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>From the neuron doctrine to neural networks</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2015</year>;<volume>16</volume>:<fpage>487</fpage>–<lpage>497</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3962" xlink:type="simple">10.1038/nrn3962</ext-link></comment> <object-id pub-id-type="pmid">26152865</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Opitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Maclin</surname> <given-names>R</given-names></name>. <article-title>Popular ensemble methods: An empirical study</article-title>. <source>Journal of Artificial Intelligence Research</source>. <year>1999</year>;<volume>11</volume>:<fpage>169</fpage>–<lpage>198</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Xin</surname> <given-names>Y</given-names></name>. <article-title>Simultaneous training of negatively correlated neural networks in an ensemble</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</source>. <year>1999</year>;<volume>29</volume>(<issue>6</issue>):<fpage>716</fpage>–<lpage>725</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/3477.809027" xlink:type="simple">10.1109/3477.809027</ext-link></comment> <object-id pub-id-type="pmid">18252352</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yuksel</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Gader</surname> <given-names>PD</given-names></name>. <article-title>Twenty years of mixture of experts</article-title>. <source>IEEE Transactions on Neural Networks and Learning Systems</source>. <year>2012</year>;<volume>23</volume>:<fpage>1177</fpage>–<lpage>1193</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNNLS.2012.2200299" xlink:type="simple">10.1109/TNNLS.2012.2200299</ext-link></comment> <object-id pub-id-type="pmid">24807516</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Dietterich TG. Ensemble Methods in Machine Learning. In: Proceedings of the First International Workshop on Multiple Classifier Systems. MCS’00. London, UK, UK: Springer-Verlag; 2000. p. 1–15.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Polikar</surname> <given-names>R</given-names></name>. <article-title>Ensemble learning</article-title>. <source>Scholarpedia</source>. <year>2009</year>;<volume>4</volume>(<issue>1</issue>):<fpage>2776</fpage>. revision #91224. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.4249/scholarpedia.2776" xlink:type="simple">10.4249/scholarpedia.2776</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref021">
<label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name>. <source>Dynamical systems in neuroscience: The geometry of excitability and bursting</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2007</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moioli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Vargas</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Husbands</surname> <given-names>P</given-names></name>. <article-title>Synchronisation effects on the behavioural performance and information dynamics of a simulated minimally cognitive robotic agent</article-title>. <source>Biological Cybernetics</source>. <year>2012</year>;<volume>106</volume>(<issue>6–7</issue>):<fpage>407</fpage>–<lpage>427</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-012-0507-5" xlink:type="simple">10.1007/s00422-012-0507-5</ext-link></comment> <object-id pub-id-type="pmid">22810898</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Santos</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Barandiaran</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Husbands</surname> <given-names>P</given-names></name>. <article-title>Synchrony and phase relation dynamics underlying sensorimotor coordination</article-title>. <source>Adaptive Behavior</source>. <year>2012</year>;<volume>20</volume>(<issue>5</issue>):<fpage>321</fpage>–<lpage>336</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1059712312451859" xlink:type="simple">10.1177/1059712312451859</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jacobs</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Nowlan</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>Adaptive mixtures of local experts</article-title>. <source>Neural Computation</source>. <year>1991</year>;<volume>3</volume>(<issue>1</issue>):<fpage>125</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1991.3.1.79" xlink:type="simple">10.1162/neco.1991.3.1.79</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Humeau</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shaban</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bissière</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lüthi</surname> <given-names>A</given-names></name>. <article-title>Presynaptic induction of heterosynaptic associative plasticity in the mammalian brain</article-title>. <source>Nature</source>. <year>2003</year>;<volume>426</volume>:<fpage>841</fpage>–<lpage>845</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature02194" xlink:type="simple">10.1038/nature02194</ext-link></comment> <object-id pub-id-type="pmid">14685239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dudman</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Tsay</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Siegelbaum</surname> <given-names>A</given-names></name>. <article-title>A role for synaptic inputs at distal dendrites: Instructive signals for hippocampal long-term plasticity</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>:<fpage>866</fpage>–<lpage>879</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.10.020" xlink:type="simple">10.1016/j.neuron.2007.10.020</ext-link></comment> <object-id pub-id-type="pmid">18054862</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mehaffey</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Doupe</surname> <given-names>AJ</given-names></name>. <article-title>Naturalistic stimulation drives opposing heterosynaptic plasticity at two inputs to songbird cortex</article-title>. <source>Nature Neuroscience</source>. <year>2015</year>;<volume>18</volume>:<fpage>1272</fpage>–<lpage>1280</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4078" xlink:type="simple">10.1038/nn.4078</ext-link></comment> <object-id pub-id-type="pmid">26237364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cho</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Bayazitov</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Meloni</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Myers</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Carlezon</surname> <given-names>WA</given-names> <suffix>Jr</suffix> </name>, <name name-style="western"><surname>Zakharenko</surname> <given-names>SS</given-names></name>, <etal>et al</etal>. <article-title>Coactivation of thalamic and cortical pathways induces input timing-dependent plasticity in amygdala</article-title>. <source>Nature Neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>1</issue>):<fpage>113</fpage>–<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2993" xlink:type="simple">10.1038/nn.2993</ext-link></comment> <object-id pub-id-type="pmid">22158512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Basu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Jeffrey</surname> <given-names>DZ</given-names></name>, <name name-style="western"><surname>Stephanie</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Frederick</surname> <given-names>LH</given-names></name>, <name name-style="western"><surname>Boris</surname> <given-names>VZ</given-names></name>, <name name-style="western"><surname>Losonczy</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Gating of hippocampal activity, plasticity, and memory by entorhinal cortex long-range inhibition</article-title>. <source>Science</source>. <year>2016</year>;<volume>351</volume>(<fpage>6269</fpage>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.aaa5694" xlink:type="simple">10.1126/science.aaa5694</ext-link></comment> <object-id pub-id-type="pmid">26744409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</article-title>. <source>PLOS Computational Biology</source>. <year>2013</year>;<volume>9</volume>(<issue>4</issue>):<fpage>e1003037</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003037" xlink:type="simple">10.1371/journal.pcbi.1003037</ext-link></comment> <object-id pub-id-type="pmid">23633941</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>KA</given-names></name>. <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annual Review of Neuroscience</source>. <year>2004</year>;<volume>27</volume>:<fpage>419</fpage>–<lpage>451</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.27.070203.144152" xlink:type="simple">10.1146/annurev.neuro.27.070203.144152</ext-link></comment> <object-id pub-id-type="pmid">15217339</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref032">
<label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hampshire</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Waibel</surname> <given-names>A</given-names></name>. <source>The meta-pi network: Building Distributed Knowledge Representations for Robust Pattern Recognition</source>. <publisher-loc>PA</publisher-loc>: <publisher-name>Carnegie Mellon University</publisher-name>; <year>1989</year>. CMU-CS-89-166. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.142911" xlink:type="simple">10.1109/34.142911</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref033">
<label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hebb</surname> <given-names>DO</given-names></name>. <source>The Organization of Behavior: A Neuropsychological Theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1949</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Menzies</surname> <given-names>JRW</given-names></name>, <name name-style="western"><surname>Porrill</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dutia</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>P</given-names></name>. <article-title>Synaptic plasticity in medial vestibular nucleus neurons: Comparison with computational requirements of VOR adaptation</article-title>. <source>PLoS ONE</source>. <year>2010</year>;<volume>5</volume>(<issue>10</issue>):<fpage>e13182</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0013182" xlink:type="simple">10.1371/journal.pone.0013182</ext-link></comment> <object-id pub-id-type="pmid">20957149</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dong</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Han</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>L</given-names></name>. <article-title>Coincident Activity of Converging Pathways Enables Simultaneous Long-Term Potentiation and Long-Term Depression in Hippocampal CA1 Network In Vivo</article-title>. <source>PLoS ONE</source>. <year>2008</year>;<volume>3</volume>(<issue>8</issue>):<fpage>e2848</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0002848" xlink:type="simple">10.1371/journal.pone.0002848</ext-link></comment> <object-id pub-id-type="pmid">18682723</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tamamaki</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nojyo</surname> <given-names>Y</given-names></name>. <article-title>Preservation of Topography in the Connections Between the Subiculum, Field CAl, and the Entorhinal Cortex in Rats</article-title>. <source>Journal of Comparative Neurology</source>. <year>1995</year>;<volume>353</volume>:<fpage>379</fpage>–<lpage>390</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.903530306" xlink:type="simple">10.1002/cne.903530306</ext-link></comment> <object-id pub-id-type="pmid">7538515</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Honda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Umitsu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ishizuka</surname> <given-names>N</given-names></name>. <article-title>Topographic projections of perforant path from entorhinal area to CA1 and subiculum in the rat</article-title>. <source>Neuroscience Research</source>. <year>2000</year>;<volume>24</volume>:<fpage>S101</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kersten</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Yuille</surname> <given-names>A</given-names></name>. <article-title>Bayesian models of object perception</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2003</year>;<volume>13</volume>:<fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0959-4388(03)00042-4" xlink:type="simple">10.1016/S0959-4388(03)00042-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref039">
<label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Knill</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Richards</surname> <given-names>W</given-names></name>. <source>Perception as Bayesian Inference</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>1996</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/CBO9780511984037" xlink:type="simple">10.1017/CBO9780511984037</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fiser</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname> <given-names>M</given-names></name>. <article-title>Statistically optimal perception and learning: from behavior to neural representation</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2010</year>;<volume>14</volume>:<fpage>119</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.01.003" xlink:type="simple">10.1016/j.tics.2010.01.003</ext-link></comment> <object-id pub-id-type="pmid">20153683</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref041">
<label>41</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <source>Probabilistic Models of the Brain</source>. <publisher-name>MIT Press</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref042">
<label>42</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>RPN</given-names></name>. <source>Bayesian Brain: Probabilistic Approaches to Neural Coding</source>. <publisher-name>MIT-Press</publisher-name>; <year>2007</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7551/mitpress/9780262042383.001.0001" xlink:type="simple">10.7551/mitpress/9780262042383.001.0001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Hebbian learning of Bayes optimal decisions</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2008</year>;<volume>21</volume>:<fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nessler</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pfeiffer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>STDP enables spiking neurons to detect hidden causes of their inputs</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2009</year>;<volume>22</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref045">
<label>45</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kandel</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Jessell</surname> <given-names>TM</given-names></name>. <source>Principles of Neural Science</source>. <edition>4th ed</edition>. <publisher-name>McGraw Hill</publisher-name>; <year>2000</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref046">
<label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">LeCun Y, Cortes C, Burges CJC. The MNIST database of handwritten digits; 2009. Accessed: 2016-01-30. <ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/" xlink:type="simple">http://yann.lecun.com/exdb/mnist/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Destexhe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rudolph</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fellous</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Fluctuating synaptic conductances recreate in vivo-like activity in neocortical neurons</article-title>. <source>Neuroscience</source>. <year>2001</year>;<volume>107</volume>:<fpage>13</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0306-4522(01)00344-X" xlink:type="simple">10.1016/S0306-4522(01)00344-X</ext-link></comment> <object-id pub-id-type="pmid">11744242</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sollich</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>A</given-names></name>. <article-title>Learning with ensembles: How overfitting can be useful</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>1996</year>;<volume>8</volume>:<fpage>190</fpage>–<lpage>196</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kuncheva</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Whitaker</surname> <given-names>C</given-names></name>. <article-title>Measures of diversity in classifier ensembles</article-title>. <source>Machine Learning</source>. <year>2003</year>;<volume>51</volume>:<fpage>181</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1022859003006" xlink:type="simple">10.1023/A:1022859003006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee H, Grosse R, Ranganath R, Ng AY. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In: Proceedings of the 26th Annual International Conference on Machine Learning; 2009. p. 609-616.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref051">
<label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep Convolutional Neural Networks. In: The Twenty-sixth Annual Conference on Neural Information Processing Systems (NIPS 2012). Lake Tahoe, Nevada; 2012. p. 1–9.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref052">
<label>52</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>. <chapter-title>Deep Learning of Representations</chapter-title>. In: <name name-style="western"><surname>Bianchini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maggini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jain</surname> <given-names>LC</given-names></name>, editors. <source>Handbook on Neural Information Processing</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2013</year>. p. <fpage>1</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-642-36657-4_1" xlink:type="simple">10.1007/978-3-642-36657-4_1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>MacKay</surname> <given-names>DJ</given-names></name>. <article-title>A practical Bayesian framework for backpropagation networks</article-title>. <source>Neural Computation</source>. <year>1992</year>;<volume>4</volume>:<fpage>448</fpage>–<lpage>472</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1992.4.3.448" xlink:type="simple">10.1162/neco.1992.4.3.448</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Williams</surname> <given-names>PM</given-names></name>. <article-title>Bayesian regularization and pruning using a Laplace prior</article-title>. <source>Neural Computation</source>. <year>1995</year>;<volume>7</volume>:<fpage>117</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1995.7.1.117" xlink:type="simple">10.1162/neco.1995.7.1.117</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Connor</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hollensen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Krigolson</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Trappenberg</surname> <given-names>T</given-names></name>. <article-title>A biological mechanism for Bayesian feature selection: Weight decay and raising the LASSO</article-title>. <source>Neural Networks</source>. <year>2015</year>;<volume>67</volume>:<fpage>121</fpage>–<lpage>130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2015.03.005" xlink:type="simple">10.1016/j.neunet.2015.03.005</ext-link></comment> <object-id pub-id-type="pmid">25897512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref056">
<label>56</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Tsymbal</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pechenizkiy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>P</given-names></name>. <chapter-title>Diversity in Random Subspacing Ensembles</chapter-title>. In: <name name-style="western"><surname>Kambayashi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Mohania</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wöß</surname> <given-names>W</given-names></name>, editors. <source>Data Warehousing and Knowledge Discovery. vol. 3181 of Lecture Notes in Computer Science</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2004</year>. p. <fpage>309</fpage>–<lpage>319</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-540-30076-2_31" xlink:type="simple">http://dx.doi.org/10.1007/978-3-540-30076-2_31</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tang</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Suganthan</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>X</given-names></name>. <article-title>An analysis of diversity measures</article-title>. <source>Machine Learning</source>. <year>2006</year>;<volume>65</volume>(<issue>1</issue>):<fpage>247</fpage>–<lpage>271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10994-006-9449-2" xlink:type="simple">10.1007/s10994-006-9449-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref058">
<label>58</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cunningham</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Carney</surname> <given-names>J</given-names></name>. Diversity versus Quality in Classification Ensembles Based on Feature <chapter-title>Selection</chapter-title>. In: <source>Machine Learning: ECML 2000. vol. 1810 of Lecture Notes in Computer Science</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2000</year>. p. <fpage>109</fpage>–<lpage>116</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref059">
<label>59</label>
<mixed-citation publication-type="other" xlink:type="simple">Mikami A, Kudo M, Nakamura A. Diversity Measures and Margin Criteria in Multi-class Majority Vote Ensemble. In: Schwenker F, Roli F, Kittler J, editors. Multiple Classifier Systems: 12th International Workshop, MCS 2015. Cham: Springer International Publishing; 2015. p. 27–37.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Krogh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vedelsby</surname> <given-names>J</given-names></name>. <article-title>Neural Network Ensembles, Cross Validation and Active Learning</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>1995</year>;<volume>7</volume>:<fpage>231</fpage>–<lpage>238</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005137.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Klampfl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>. <article-title>Emergence of dynamic memory traces in cortical microcircuit models through STDP</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>28</issue>):<fpage>11515</fpage>–<lpage>11529</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5044-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5044-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23843522</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fernando</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Szathmáry</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Husbands</surname> <given-names>P</given-names></name>. <article-title>Selectionist and evolutionary approaches to brain function: A critical appraisal</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2012</year>;<volume>6</volume>(<issue>24</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2012.00024" xlink:type="simple">10.3389/fncom.2012.00024</ext-link></comment> <object-id pub-id-type="pmid">22557963</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schomburg</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Fernández-Ruiz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mizuseki</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Berényi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Anastassiou</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>Theta phase segregation of input-specific gamma patterns in entorhinal-hippocampal networks</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>84</volume>(<issue>2</issue>):<fpage>470</fpage>–<lpage>485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.08.051" xlink:type="simple">10.1016/j.neuron.2014.08.051</ext-link></comment> <object-id pub-id-type="pmid">25263753</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chrobak</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Lörincz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Physiological patterns in the hippocampo-entorhinal cortex system</article-title>. <source>Hippocampus</source>. <year>2000</year>;<volume>10</volume>(<issue>4</issue>):<fpage>457</fpage>–<lpage>465</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/1098-1063(2000)10:4%3C457::AID-HIPO12%3E3.0.CO;2-Z" xlink:type="simple">10.1002/1098-1063(2000)10:4%3C457::AID-HIPO12%3E3.0.CO;2-Z</ext-link></comment> <object-id pub-id-type="pmid">10985285</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Igarashi</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Colgin</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>. <article-title>Coordination of entorhinal-hippocampal ensemble activity during associative learning</article-title>. <source>Nature</source>. <year>2014</year>;<volume>510</volume>:<fpage>143</fpage>–<lpage>147</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature13162" xlink:type="simple">10.1038/nature13162</ext-link></comment> <object-id pub-id-type="pmid">24739966</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Berens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Keliris</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Tolias</surname> <given-names>AS</given-names></name>. <article-title>Decorrelated neuronal firing in cortical microcircuits</article-title>. <source>Science</source>. <year>2010</year>;<volume>327</volume>(<issue>5965</issue>):<fpage>584</fpage>–<lpage>587</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1179867" xlink:type="simple">10.1126/science.1179867</ext-link></comment> <object-id pub-id-type="pmid">20110506</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fino</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>Dense inhibitory connectivity in neocortex</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>6</issue>):<fpage>1188</fpage>–<lpage>1203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.025" xlink:type="simple">10.1016/j.neuron.2011.02.025</ext-link></comment> <object-id pub-id-type="pmid">21435562</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Chaos and synchrony in a model of a hypercolumn in visual cortex</article-title>. <source>Journal of Computational Neuroscience</source>. <year>1996</year>;<volume>3</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00158335" xlink:type="simple">10.1007/BF00158335</ext-link></comment> <object-id pub-id-type="pmid">8717487</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>. <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>32</volume>:<fpage>1149</fpage>–<lpage>1164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(01)00542-6" xlink:type="simple">10.1016/S0896-6273(01)00542-6</ext-link></comment> <object-id pub-id-type="pmid">11754844</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lübke</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Frotscher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name>. <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>(<issue>5297</issue>):<fpage>213</fpage>–<lpage>215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.275.5297.213" xlink:type="simple">10.1126/science.275.5297.213</ext-link></comment> <object-id pub-id-type="pmid">8985014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poo</surname> <given-names>M</given-names></name>. <article-title>Synaptic modification by correlated activity: Hebb’s postulate revisited</article-title>. <source>Annual Review of Neuroscience</source>. <year>2001</year>;<volume>24</volume>:<fpage>139</fpage>–<lpage>166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.24.1.139" xlink:type="simple">10.1146/annurev.neuro.24.1.139</ext-link></comment> <object-id pub-id-type="pmid">11283308</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005137.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wysoskia</surname> <given-names>SG</given-names></name>, <name name-style="western"><surname>Benuskovaa</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kasabova</surname> <given-names>N</given-names></name>. <article-title>Evolving spiking neural networks for audiovisual information processing</article-title>. <source>Neural Networks</source>. <year>2010</year>;<volume>23</volume>:<fpage>819</fpage>–<lpage>835</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2010.04.009" xlink:type="simple">10.1016/j.neunet.2010.04.009</ext-link></comment> <object-id pub-id-type="pmid">20510579</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>