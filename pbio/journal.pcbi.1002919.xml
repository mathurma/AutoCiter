<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-01549</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002919</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computer Science</subject>
          <subject>Physics</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Optimal Properties of Analog Perceptrons with Excitatory Weights</article-title>
        <alt-title alt-title-type="running-head">Optimal Properties of Analog Perceptrons</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Clopath</surname>
            <given-names>Claudia</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Brunel</surname>
            <given-names>Nicolas</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Laboratory of Neurophysics and Physiology, CNRS and Université Paris Descartes, Paris, France</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Centre for Theoretical Neuroscience, Columbia University, New York, New York, United States of America</addr-line>
      </aff>
      <aff id="aff3">
        <label>3</label>
        <addr-line>Departments of Statistics and Neurobiology, University of Chicago, Chicago, Illinois, United States of America</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>Indiana University, United States of America</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">cc3450@columbia.edu</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: NB. Performed the experiments: CC NB. Analyzed the data: CC NB. Contributed reagents/materials/analysis tools: CC NB. Wrote the paper: CC NB.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>2</month>
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>21</day>
        <month>2</month>
        <year>2013</year>
      </pub-date>
      <volume>9</volume>
      <issue>2</issue>
      <elocation-id>e1002919</elocation-id>
      <history>
        <date date-type="received">
          <day>28</day>
          <month>9</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>12</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <copyright-holder>Clopath and Brunel</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>The cerebellum is a brain structure which has been traditionally devoted to supervised learning. According to this theory, plasticity at the Parallel Fiber (PF) to Purkinje Cell (PC) synapses is guided by the Climbing fibers (CF), which encode an ‘error signal’. Purkinje cells have thus been modeled as perceptrons, learning input/output binary associations. At maximal capacity, a perceptron with excitatory weights expresses a large fraction of zero-weight synapses, in agreement with experimental findings. However, numerous experiments indicate that the firing rate of Purkinje cells varies in an analog, not binary, manner. In this paper, we study the perceptron with analog inputs and outputs. We show that the optimal input has a sparse binary distribution, in good agreement with the burst firing of the Granule cells. In addition, we show that the weight distribution consists of a large fraction of silent synapses, as in previously studied binary perceptron models, and as seen experimentally.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Learning properties of neuronal networks have been extensively studied using methods from statistical physics. However, most of these studies ignore a fundamental constraint in networks of real neurons: synapses are either excitatory or inhibitory, and cannot change sign during learning. Here, we characterize the optimal storage properties of an analog perceptron with excitatory synapses, as a simplified model for cerebellar Purkinje cells. The information storage capacity is shown to be optimized when inputs have a sparse binary distribution, while the weight distribution at maximal capacity consists of a large amount of zero-weight synapses. Both features are in agreement with electrophysiological data.</p>
      </abstract>
      <funding-group>
        <funding-statement>This work has been supported by the Agence Nationale de la Recherche, grant ANR-08-SYSC-005 and by the Swiss National Science Foundation, grant PA00P3_139703. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="6"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Purkinje cells (PCs) are the only outputs of the cerebellar cortex, a brain structure involved in motor learning. They receive a very large number (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e001" xlink:type="simple"/></inline-formula>150,000) of excitatory synaptic inputs from Granule Cells (GCs) through parallel fibers (PFs), and a single very strong input from the inferior olive through climbing fibers (CFs).</p>
      <p>Single PCs have long been considered as a neurobiological implementation of a perceptron <xref ref-type="bibr" rid="pcbi.1002919-Marr1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Albus1">[2]</xref>, the simplest feedforward network endowed with supervised learning <xref ref-type="bibr" rid="pcbi.1002919-Rosenblatt1">[3]</xref>, since CFs are thought to provide PCs with an error signal <xref ref-type="bibr" rid="pcbi.1002919-Soetedjo1">[4]</xref>. A perceptron learns associations between input patterns and a binary output that are imposed to it. Learning is due to synaptic modifications, under the control of an error signal. The learning capabilities of perceptrons have been extensively studied for unbiased <xref ref-type="bibr" rid="pcbi.1002919-Cover1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Gardner1">[6]</xref> as well as biased patterns <xref ref-type="bibr" rid="pcbi.1002919-Gardner1">[6]</xref>, and for unconstrained synapses <xref ref-type="bibr" rid="pcbi.1002919-Cover1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Gardner1">[6]</xref>. In real neurons, synapses are either excitatory (glutamatergic synapses), or inhibitory (GABAergic synapses), depending on the identity of the pre-synaptic neurons (except during early development, when GABAergic synapses are initially excitatory and then become inhibitory). A multitude of experiments characterizing synaptic plasticity have shown that the strength, but not the sign, of a synapse can be modified by patterns of neuronal activity. This has led to the study of perceptrons with sign-constrained weights <xref ref-type="bibr" rid="pcbi.1002919-Amit1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Kanter1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Nadal1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>. In particular, Brunel et al. <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref> showed that when synaptic weights are constrained to be excitatory (positive or zero), a perceptron at maximal capacity has a distribution of synaptic weights with two components: a finite fraction of zero-weight (‘silent’) synapses; and a truncated Gaussian distribution for the rest of the synapses. They further showed that this distribution is in striking agreement with experimental data <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>.</p>
      <p>Numerous experiments show however that in the course of specific motor tasks, the firing rate of Purkinje cell varies in an analog, not binary, fashion <xref ref-type="bibr" rid="pcbi.1002919-Barmack1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Ke1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Thier1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Thach1">[14]</xref>. We therefore set out to investigate the capacity and distribution of synaptic weights of a perceptron storing associations between analog inputs and outputs. More precisely, each input or output unit can take an analog value drawn from a distribution with a given mean and variance. We show that the optimal input distribution matches the firing pattern of the Granule cells, and weight distribution at maximal capacity reproduces the experimental Parallel Fiber to Purkinje cell synaptic weight distribution.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>The analog perceptron</title>
        <p>The perceptron consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e002" xlink:type="simple"/></inline-formula> inputs and one output. Both inputs and outputs take continuous values. We require this perceptron to learn a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e003" xlink:type="simple"/></inline-formula> prescribed random input-output associations, where the inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e004" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e005" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e006" xlink:type="simple"/></inline-formula>) are drawn randomly and independently from a distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e007" xlink:type="simple"/></inline-formula>, with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e008" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e009" xlink:type="simple"/></inline-formula> while the target outputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e010" xlink:type="simple"/></inline-formula> are drawn randomly and independently from a distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e011" xlink:type="simple"/></inline-formula> with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e012" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e013" xlink:type="simple"/></inline-formula>. Note that since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e014" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e015" xlink:type="simple"/></inline-formula> represent firing rates of input and output cells, respectively, they must be non-negative quantities. In particular, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e016" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e017" xlink:type="simple"/></inline-formula> represent the mean firing rates of granule/Purkinje cell, respectively. The output of the perceptron when a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e018" xlink:type="simple"/></inline-formula> is presented in input is given by<disp-formula id="pcbi.1002919.e019"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e019" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e020" xlink:type="simple"/></inline-formula> is a monotonically increasing static transfer function (f-I curve), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e021" xlink:type="simple"/></inline-formula> are the synaptic weights from input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e022" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e023" xlink:type="simple"/></inline-formula> represents inhibitory inputs that cancel the leading order term in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e024" xlink:type="simple"/></inline-formula> so that the argument of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e025" xlink:type="simple"/></inline-formula> is of order 1. In Purkinje cells, these inhibitory inputs are provided by interneurons of the molecular layer. The goal of perceptron learning is to find a set of synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e026" xlink:type="simple"/></inline-formula> for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e027" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e028" xlink:type="simple"/></inline-formula>.</p>
        <p>We focus for simplicity on a linear transfer function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e029" xlink:type="simple"/></inline-formula>, but our results can be applied to arbitrary invertible transfer functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e030" xlink:type="simple"/></inline-formula>. Indeed, the problem of learning associations (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e031" xlink:type="simple"/></inline-formula>) in a perceptron with an arbitrary invertible transfer function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e032" xlink:type="simple"/></inline-formula> is equivalent to the problem of learning (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e033" xlink:type="simple"/></inline-formula>) in a linear perceptron. All the results derived in this paper can then be applied to a perceptron with transfer function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e034" xlink:type="simple"/></inline-formula>, except that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e035" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e036" xlink:type="simple"/></inline-formula> are now defined to be the two first moments of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e037" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2b">
        <title>Storage capacity</title>
        <p>In the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e038" xlink:type="simple"/></inline-formula> limit the probability of finding a set of weights that satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e039" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e040" xlink:type="simple"/></inline-formula> is expected to be 1 if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e041" xlink:type="simple"/></inline-formula> is below a critical value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e042" xlink:type="simple"/></inline-formula>, while it is 0 when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e043" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002919-Hertz1">[15]</xref>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e044" xlink:type="simple"/></inline-formula> is therefore the number of associations that can be learned per synapse, and is commonly used as a measure of storage capacity.</p>
        <p>This storage capacity can be computed analytically using the replica method (see <xref ref-type="sec" rid="s4">Methods</xref>) <xref ref-type="bibr" rid="pcbi.1002919-Gardner1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Gutfreund1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Kohler1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Hertz1">[15]</xref>. The capacity is given by<disp-formula id="pcbi.1002919.e045"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e045" xlink:type="simple"/><label>(2)</label></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e046" xlink:type="simple"/></inline-formula> is given by the equation<disp-formula id="pcbi.1002919.e047"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e047" xlink:type="simple"/><label>(3)</label></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e048" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e049" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e050" xlink:type="simple"/></inline-formula> depends on the statistics of the associations as<disp-formula id="pcbi.1002919.e051"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e051" xlink:type="simple"/><label>(4)</label></disp-formula></p>
        <p>Therefore, the maximal capacity only depends on a single parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e052" xlink:type="simple"/></inline-formula>, which is a function of the statistics of the patterns that need to be learned. This dependence is shown in <xref ref-type="fig" rid="pcbi-1002919-g001">Fig. 1A</xref>. It shows that the capacity is exactly equal to 0.5 when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e053" xlink:type="simple"/></inline-formula>, while it decreases monotonically as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e054" xlink:type="simple"/></inline-formula> increases.</p>
        <fig id="pcbi-1002919-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002919.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>A. Maximal capacity as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e055" xlink:type="simple"/></inline-formula>.</title>
            <p><bold>B</bold>. Mean squared error between the output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e056" xlink:type="simple"/></inline-formula> and the target output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e057" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e058" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e059" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e060" xlink:type="simple"/></inline-formula>). Red: analytical calculation, <xref ref-type="disp-formula" rid="pcbi.1002919.e166">Eq. (17)</xref>; Blue, numerical simulations (with parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e061" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e062" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e063" xlink:type="simple"/></inline-formula>, simulation length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e064" xlink:type="simple"/></inline-formula>, average over 20 trails, error bars: standard deviation).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002919.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>If the number of patterns to be learned exceeds the maximal capacity, the mean squared error becomes strictly positive. It can also be computed using the replica method (see <xref ref-type="sec" rid="s4">Methods</xref>, <xref ref-type="disp-formula" rid="pcbi.1002919.e166">Eq. (17)</xref>). Unsurprisingly, it increases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e065" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1002919-g001">Fig. 1B</xref> which shows the result of the analytical calculation, as well as numerical simulations. If uncorrelated noise is added to the perceptron, the total mean squared error is the sum of the error without noise (<xref ref-type="disp-formula" rid="pcbi.1002919.e166">Eq. 17</xref>) and the variance of the uncorrelated noise.</p>
        <p>In the simulations, inputs and outputs are drawn from an exponential distribution. The weight update at each presentation is the standard perceptron one, i.e.<disp-formula id="pcbi.1002919.e066"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e066" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e067" xlink:type="simple"/></inline-formula> is the learning rate. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e068" xlink:type="simple"/></inline-formula> is set to zero if application of the update leads to a negative weight. This corresponds to a gradient descent of a cost function proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e069" xlink:type="simple"/></inline-formula>, in the closed orthant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e070" xlink:type="simple"/></inline-formula>.</p>
        <p>This learning rule is in qualitative agreement with experimental data on synaptic plasticity in GC to PC synapses <xref ref-type="bibr" rid="pcbi.1002919-Hansel1">[18]</xref>, . In Purkinje cells, the error signal is thought to be conveyed by climbing fiber (CF) activation. Two protocols have been shown to be effective in eliciting long-term plasticity. Pairing GC with and CF activation leads to Long-Term Depression (LTD) of the synapse, while Long-Term Potentiation (LTP) is induced by stimulating the GC alone (see <xref ref-type="fig" rid="pcbi-1002919-g003">Fig. 3AB</xref> of <xref ref-type="bibr" rid="pcbi.1002919-Jorntell1">[19]</xref> for details). Writing climbing fiber activation as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e071" xlink:type="simple"/></inline-formula>, we see that <xref ref-type="disp-formula" rid="pcbi.1002919.e066">Eq. (5)</xref> is recovered if one chooses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e072" xlink:type="simple"/></inline-formula>, which captures the two experimental protocols described above.</p>
      </sec>
      <sec id="s2c">
        <title>Distribution of synaptic weights</title>
        <p>The distribution of synaptic weights at maximal capacity can also be computed using the replica method (see <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref> for details of the calculation). It turns out that the distribution obeys exactly the same equation as in the binary perceptron, i.e.<disp-formula id="pcbi.1002919.e073"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e073" xlink:type="simple"/><label>(6)</label></disp-formula>where<disp-formula id="pcbi.1002919.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e074" xlink:type="simple"/><label>(7)</label></disp-formula>and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e075" xlink:type="simple"/></inline-formula> is the average synaptic weight. In particular the fraction of zero weight synapses is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e076" xlink:type="simple"/></inline-formula>. Interestingly, there is a very simple relationship between capacity and fraction of silent synapses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e077" xlink:type="simple"/></inline-formula>, that holds for any value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e078" xlink:type="simple"/></inline-formula>. The fraction of silent synapses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e079" xlink:type="simple"/></inline-formula> is shown as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e080" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002919-g002">Fig. 2A</xref>. It shows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e081" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e082" xlink:type="simple"/></inline-formula>, and increases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e083" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002919-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002919.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>A. Fraction of silent synapses at maximal capacity as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e084" xlink:type="simple"/></inline-formula>.</title>
            <p><bold>B</bold>. Distribution of synaptic weights for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e085" xlink:type="simple"/></inline-formula>, at maximal capacity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e086" xlink:type="simple"/></inline-formula>). Red: analytical calculation, <xref ref-type="disp-formula" rid="pcbi.1002919.e073">Eq. (6)</xref>; Blue, numerical simulations (with parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e087" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e088" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e089" xlink:type="simple"/></inline-formula>, simulation length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e090" xlink:type="simple"/></inline-formula>). C. Fraction of silent synapses as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e091" xlink:type="simple"/></inline-formula>, beyond the maximal capacity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e092" xlink:type="simple"/></inline-formula>), for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e093" xlink:type="simple"/></inline-formula> (red: analytical calculation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e094" xlink:type="simple"/></inline-formula>); blue: numerical simulations, with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e095" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e096" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e097" xlink:type="simple"/></inline-formula>, simulation length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e098" xlink:type="simple"/></inline-formula>, average over 10 trails, error bars: standard deviation).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002919.g002" position="float" xlink:type="simple"/>
        </fig>
        <p>The full distribution of weights is shown in <xref ref-type="fig" rid="pcbi-1002919-g002">Fig. 2B</xref>, together with the results of a numerical simulation (see parameters in the caption of <xref ref-type="fig" rid="pcbi-1002919-g002">Fig. 2B</xref>). The theoretical distribution of synaptic weights is in good agreement with experimental measurements of the efficacy of a large set of GC to PC synapses, using paired recordings in vitro (see Fig. 6A of <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref> for details) <xref ref-type="bibr" rid="pcbi.1002919-Harvey1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Isope1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>.</p>
        <p>Above maximal capacity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e099" xlink:type="simple"/></inline-formula>, the distribution of synaptic weights is still given by <xref ref-type="disp-formula" rid="pcbi.1002919.e073">Eq. (6)</xref>, but the fraction of zero weight synapses decreases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e100" xlink:type="simple"/></inline-formula>, and goes to zero in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e101" xlink:type="simple"/></inline-formula> limit (see <xref ref-type="fig" rid="pcbi-1002919-g002">Fig. 2C</xref>). In that limit the distribution becomes increasingly close to a Gaussian distribution peaked around a positive value, with a width that tends to zero in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e102" xlink:type="simple"/></inline-formula> limit.</p>
      </sec>
      <sec id="s2d">
        <title>Statistics of inputs and outputs maximizing storage capacity</title>
        <p>To maximize storage capacity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e103" xlink:type="simple"/></inline-formula> should be as small as possible. We first ask which distribution of inputs maximize capacity. From <xref ref-type="disp-formula" rid="pcbi.1002919.e051">Eq. (4)</xref>, it is clear that to maximize capacity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e104" xlink:type="simple"/></inline-formula> should be as small as possible, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e105" xlink:type="simple"/></inline-formula> should be as large as possible. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e106" xlink:type="simple"/></inline-formula> is a distribution of firing rates, it must be bounded between 0 and a maximal firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e107" xlink:type="simple"/></inline-formula>. The distribution of a bounded variable that maximizes the variance with a fixed mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e108" xlink:type="simple"/></inline-formula> is a binary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e109" xlink:type="simple"/></inline-formula>. Thus, we predict that to optimize capacity, patterns of activity in the Granule cell layer should be sparse (to ensure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e110" xlink:type="simple"/></inline-formula> is small), but active cells should be active close to their maximal firing rates. Interestingly, this is in striking agreement with available data <xref ref-type="bibr" rid="pcbi.1002919-Chadderton1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Jorntell1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Rancz1">[23]</xref> showing that (i) Granule cells have very sparse activity in vivo (average firing rates of 0.5 Hz <xref ref-type="bibr" rid="pcbi.1002919-Chadderton1">[22]</xref>) (ii) they can respond with brief, high frequency bursts of action potentials to sensory inputs (with an average frequency of 77 Hz within the burst, and maximal frequencies up to 250 Hz, see e.g. <xref ref-type="fig" rid="pcbi-1002919-g003">Fig. 3</xref> of <xref ref-type="bibr" rid="pcbi.1002919-Chadderton1">[22]</xref>).</p>
        <p>The next question is which distribution of output firing rates optimizes the capacity. <xref ref-type="disp-formula" rid="pcbi.1002919.e051">Eq. (4)</xref> makes it clear the capacity is optimized for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e111" xlink:type="simple"/></inline-formula>. In this limit however, all input patterns lead to exactly the same output, and the Purkinje cell output contains no information on which input was presented. This is of course not a desirable outcome, and suggests the capacity is not the correct measure to maximize in this case. We therefore turn to the Shannon mutual information between the Purkinje cell output and its inputs as a more suitable measure. In the presence of additive Gaussian noise of zero mean and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e112" xlink:type="simple"/></inline-formula>, this is simply the mutual information of a Gaussian channel with a signal-to-noise ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e113" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e114" xlink:type="simple"/></inline-formula> bits per pattern (see e.g. <xref ref-type="bibr" rid="pcbi.1002919-Cover2">[24]</xref>). The total information in bits per synapse is therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e115" xlink:type="simple"/></inline-formula>. The information is zero when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e116" xlink:type="simple"/></inline-formula>, and reaches a maximum for a finite value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e117" xlink:type="simple"/></inline-formula>, which depends on both the noise standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e118" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e119" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002919-g003">Fig. 3A</xref> shows the information as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e120" xlink:type="simple"/></inline-formula>, for different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e121" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e122" xlink:type="simple"/></inline-formula>. It shows that the optimal value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e123" xlink:type="simple"/></inline-formula> increases approximately linearly with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e124" xlink:type="simple"/></inline-formula> for large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e125" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002919-g003">Fig. 3B</xref>).</p>
        <fig id="pcbi-1002919-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002919.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>A. Information as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e126" xlink:type="simple"/></inline-formula> for different levels of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e127" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e128" xlink:type="simple"/></inline-formula>.</title>
            <p>B. Optimal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e129" xlink:type="simple"/></inline-formula> (green line, right y-axis) and information (blue line, left y-axis) as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e130" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002919.g003" position="float" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>In this paper, we have considered an analog firing rate model for a Purkinje cell with plastic excitatory weights, and derived both its maximal capacity and the distribution of weights at maximal capacity. We showed that the capacity is of the same order as in a binary perceptron model.</p>
      <p>The distribution of synaptic weights of the analog perceptron is composed at maximal capacity of two parts: a large fraction (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e131" xlink:type="simple"/></inline-formula>) of silent synapses and a truncated Gaussian. It has exactly the same shape as in several other models: a standard binary perceptron <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>, and a bistable perceptron <xref ref-type="bibr" rid="pcbi.1002919-Clopath1">[25]</xref>. This distribution is in quantitative agreement with a combination of electron microscopy and electrophysiological data in adult rat slices <xref ref-type="bibr" rid="pcbi.1002919-Harvey1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Isope1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>. Furthermore, a gradient descent learning rule leading to maximal capacity bears strong similarities with synaptic plasticity experiments: LTD when PF and CF are coactivated, LTP when PF fires alone (i.e. CF below baseline, thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e132" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1002919-Hansel1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Jorntell1">[19]</xref>.</p>
      <p>We found that in order to maximize the capacity, the input variance should be as large as possible. We argue that GCs in vivo are close to such an optimal distribution, since they fire high-frequency bursts at very low rates <xref ref-type="bibr" rid="pcbi.1002919-Chadderton1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Jorntell1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Rancz1">[23]</xref>. Furthermore, GC bursts have been found in some experiments to be critical to induce plasticity in PF to PC synapse <xref ref-type="bibr" rid="pcbi.1002919-Bidoret1">[26]</xref>. Indeed, no plasticity is induced in those protocols with a single GC spike. Secondly, lower variance in the output also increases the capacity, but at a cost of losing information contained in the output, in the presence of noise. For a given variance of the noise, there is an optimal variance of the output that maximizes the information contained in the output.</p>
      <p>The model we have studied here is essentially equivalent to the ADALINE (Adaptive Linear Neuron) model <xref ref-type="bibr" rid="pcbi.1002919-Widrow1">[27]</xref>, whose storage capacity, in the absence of constraints on synaptic weights, is equal to 1. The result can be easily intuitively understood by the fact that when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e133" xlink:type="simple"/></inline-formula>, there are exactly N linear equations to solve, <xref ref-type="disp-formula" rid="pcbi.1002919.e019">Eq. 1</xref>, with N unknowns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e134" xlink:type="simple"/></inline-formula> (see e.g. <xref ref-type="bibr" rid="pcbi.1002919-Hertz1">[15]</xref>). We have shown here that the constraints that all synaptic weights should be positive or zero leads to a capacity which is decreased by a factor 2 or more, depending on the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e135" xlink:type="simple"/></inline-formula>. This decrease in capacity is similar to what is observed in the standard perceptron with excitatory synapses <xref ref-type="bibr" rid="pcbi.1002919-Amit1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Kanter1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Nadal1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>. Note that learning associations with constrained weights is similar conceptually to non-negative matrix factorization <xref ref-type="bibr" rid="pcbi.1002919-Lee1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Lee2">[29]</xref>. Generalizations of such models in the temporal domain (the so-called adaptive filter models) have been proposed to describe learning in the cerebellar cortex <xref ref-type="bibr" rid="pcbi.1002919-Fujita1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Dean1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Porrill1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002919-Lepora1">[33]</xref>. It would be of interest to investigate capacity and distribution of synaptic weights of such models.</p>
      <p>In this paper, we have focused on a single plasticity site, the GC to PC synapse. Many other sites of plasticity are known to exist in the cerebellum <xref ref-type="bibr" rid="pcbi.1002919-Hansel1">[18]</xref>. Future studies are needed to clarify the impact of these additional sites of plasticity on the learning capabilities of this structure.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Calculation of the storage capacity</title>
        <p>The replica method involves calculating the average logarithm of the volume of the space of weights satisfying all constraints given by <xref ref-type="disp-formula" rid="pcbi.1002919.e019">Eq. (1)</xref> <xref ref-type="bibr" rid="pcbi.1002919-Gardner1">[6]</xref>. To compute the average logarithm, one uses the replica trick: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e136" xlink:type="simple"/></inline-formula> replicas of the system are introduced, one computes<disp-formula id="pcbi.1002919.e137"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e137" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e138" xlink:type="simple"/></inline-formula> represents an average over the patterns, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e139" xlink:type="simple"/></inline-formula> is a replica index. This calculation is done using a standard procedure. After introducing integral representations for the delta functions, one averages over the distribution of the patterns. One then introduces order parameters<disp-formula id="pcbi.1002919.e140"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e140" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula id="pcbi.1002919.e141"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e141" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula id="pcbi.1002919.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e142" xlink:type="simple"/><label>(10)</label></disp-formula>together with conjugate parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e143" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e144" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e145" xlink:type="simple"/></inline-formula>. We then use a replica-symmetric ansatz (all the order parameters are taken to be independent of replica index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e146" xlink:type="simple"/></inline-formula>), perform the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e147" xlink:type="simple"/></inline-formula> and obtain<disp-formula id="pcbi.1002919.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e148" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pcbi.1002919.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e149" xlink:type="simple"/><label>(12)</label></disp-formula>where in the Equation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e150" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1002919.e149">Eq. (12)</xref>), the two first lines are identical to the binary perceptron with excitatory weights <xref ref-type="bibr" rid="pcbi.1002919-Brunel1">[10]</xref>, while the last line is specific to the analog perceptron.</p>
        <p>In the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e151" xlink:type="simple"/></inline-formula> limit, the integral in <xref ref-type="disp-formula" rid="pcbi.1002919.e148">Eq. (11)</xref> is dominated by the extremum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e152" xlink:type="simple"/></inline-formula>. The typical values of all order parameters are then obtained by the resulting saddle point equations, setting the derivatives of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e153" xlink:type="simple"/></inline-formula> with respect to all order parameters to zero. The maximal capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e154" xlink:type="simple"/></inline-formula> is obtained in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e155" xlink:type="simple"/></inline-formula>, for which the volume vanishes. This limit yields <xref ref-type="disp-formula" rid="pcbi.1002919.e045">Eqs. (2</xref>,<xref ref-type="disp-formula" rid="pcbi.1002919.e051">4</xref>).</p>
      </sec>
      <sec id="s4b">
        <title>Calculation of the mean squared error</title>
        <p>Following <xref ref-type="bibr" rid="pcbi.1002919-Gardner2">[34]</xref>, we introduce a cost function which is given by the sum of the squared error for all patterns,<disp-formula id="pcbi.1002919.e156"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e156" xlink:type="simple"/><label>(13)</label></disp-formula>and compute its minimum over the space of weights. This is done introducing a partition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e157" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1002919.e158"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e158" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e159" xlink:type="simple"/></inline-formula> is an inverse temperature, and computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e160" xlink:type="simple"/></inline-formula> using the replica method. The mean squared error is then given by<disp-formula id="pcbi.1002919.e161"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e161" xlink:type="simple"/><label>(15)</label></disp-formula></p>
        <p>To perform this calculation, a new parameter has to be introduced,<disp-formula id="pcbi.1002919.e162"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e162" xlink:type="simple"/><label>(16)</label></disp-formula>which will remain finite when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e163" xlink:type="simple"/></inline-formula> in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e164" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e165" xlink:type="simple"/></inline-formula>. The mean squared error is then given by<disp-formula id="pcbi.1002919.e166"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e166" xlink:type="simple"/><label>(17)</label></disp-formula>where<disp-formula id="pcbi.1002919.e167"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e167" xlink:type="simple"/><label>(18)</label></disp-formula><disp-formula id="pcbi.1002919.e168"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e168" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pcbi.1002919.e169"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002919.e169" xlink:type="simple"/><label>(20)</label></disp-formula>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e170" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e171" xlink:type="simple"/></inline-formula> diverges to infinity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002919.e172" xlink:type="simple"/></inline-formula>, and <xref ref-type="disp-formula" rid="pcbi.1002919.e168">Eqs. (19</xref>,<xref ref-type="disp-formula" rid="pcbi.1002919.e169">20</xref>) reduce to <xref ref-type="disp-formula" rid="pcbi.1002919.e045">Eqs. (2</xref>,<xref ref-type="disp-formula" rid="pcbi.1002919.e047">3</xref>).</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Boris Barbour, Mariano Casado, Vincent Hakim, Clément Léna, and Jean-Pierre Nadal for fruitful discussions and helpful comments on the manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002919-Marr1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name> (<year>1969</year>) <article-title>A theory of cerebellar cortex</article-title>. <source>J Physiol (Lond)</source> <volume>202</volume>: <fpage>437</fpage>–<lpage>470</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Albus1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Albus</surname><given-names>J</given-names></name> (<year>1971</year>) <article-title>A theory of cerebellar function</article-title>. <source>J Mathematical Biosciences</source> <volume>10</volume>: <fpage>25</fpage>–<lpage>61</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Rosenblatt1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenblatt</surname><given-names>F</given-names></name> (<year>1958</year>) <article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title>. <source>Psych Review</source> <volume>65</volume>: <fpage>386</fpage>–<lpage>408</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Soetedjo1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soetedjo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kojima</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Fuchs</surname><given-names>AF</given-names></name> (<year>2008</year>) <article-title>Complex spike activity in the oculomotor vermis of the cerebellum: a vectorial error signal for saccade motor learning?</article-title> <source>J Neurophysiol</source> <volume>100</volume>: <fpage>1949</fpage>–<lpage>1966</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Cover1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cover</surname><given-names>T</given-names></name> (<year>1965</year>) <article-title>Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</article-title>. <source>IEEE Trans Electron Comput</source> <volume>14</volume>: <fpage>326</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Gardner1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name> (<year>1988</year>) <article-title>The phase space of interactions in neural network models</article-title>. <source>J Phys A</source> <volume>21</volume>: <fpage>257</fpage>–<lpage>270</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Amit1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wong</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Campbell</surname><given-names>C</given-names></name> (<year>1989</year>) <article-title>Perceptron learning with sign-constrained weights</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>22</volume>: <fpage>2039</fpage>–<lpage>2045</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Kanter1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanter</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Eisenstein</surname><given-names>E</given-names></name> (<year>1990</year>) <article-title>On the capacity per synapse</article-title>. <source>J Phys A: Math Gen</source> <volume>23</volume>: <fpage>L93i</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Nadal1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name> (<year>1990</year>) <article-title>On the storage capacity with sign-constrained synaptic couplings</article-title>. <source>Network: Comput Neural Syst</source> <fpage>463</fpage>–<lpage>466</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Brunel1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Isope</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name> (<year>2004</year>) <article-title>Optimal information storage and the distribution of synaptic weights: Perceptron versus purkinje cell</article-title>. <source>Neuron</source> <volume>43</volume>: <fpage>745</fpage>–<lpage>757</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Barmack1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barmack</surname><given-names>NH</given-names></name>, <name name-style="western"><surname>Yakhnitsa</surname><given-names>V</given-names></name> (<year>2008</year>) <article-title>Functions of interneurons in mouse cerebellum</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>1140</fpage>–<lpage>1152</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Ke1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ke</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Raymond</surname><given-names>JL</given-names></name> (<year>2009</year>) <article-title>Elimination of climbing fiber instructive signals during motor learning</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>1171</fpage>–<lpage>1179</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Thier1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thier</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dicke</surname><given-names>PW</given-names></name>, <name name-style="western"><surname>Haas</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Barash</surname><given-names>S</given-names></name> (<year>2000</year>) <article-title>Encoding of movement time by populations of cerebellar Purkinje cells</article-title>. <source>Nature</source> <volume>405</volume>: <fpage>72</fpage>–<lpage>76</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Thach1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thach</surname><given-names>WT</given-names></name> (<year>1968</year>) <article-title>Discharge of Purkinje and cerebellar nuclear neurons during rapidly alternating arm movements in the monkey</article-title>. <source>J Neurophysiol</source> <volume>31</volume>: <fpage>785</fpage>–<lpage>797</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Hertz1">
        <label>15</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hertz J, Krogh A, Palmer RG (1991) Introduction to the Theory of Neural Computation. Redwood City CA: Addison-Wesley.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Gutfreund1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Stein</surname><given-names>Y</given-names></name> (<year>2613–2630</year>) <article-title>Capacity of neural networks with discrete synaptic couplings</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>23</volume>: <fpage>1990</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Kohler1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Widmaier</surname><given-names>D</given-names></name> (<year>1991</year>) <article-title>Sign-constrained linear learning and diluting in neural networks</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>24</volume>: <fpage>L495</fpage>–<lpage>L502</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Hansel1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>D'Angelo</surname><given-names>E</given-names></name> (<year>2001</year>) <article-title>Beyond parallel fiber LTD: the diversity of synaptic and non-synaptic plasticity in the cerebellum</article-title>. <source>Nat Neurosci</source> <volume>4</volume>: <fpage>467</fpage>–<lpage>475</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Jorntell1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jorntell</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hansel</surname><given-names>C</given-names></name> (<year>2006</year>) <article-title>Synaptic memories upside down: bidirectional plasticity at cerebellar parallel fiber-Purkinje cell synapses</article-title>. <source>Neuron</source> <volume>52</volume>: <fpage>227</fpage>–<lpage>238</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Harvey1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Napper</surname><given-names>RM</given-names></name> (<year>1988</year>) <article-title>Quantitative study of granule and Purkinje cells in the cerebellar cortex of the rat</article-title>. <source>J Comp Neurol</source> <volume>274</volume>: <fpage>151</fpage>–<lpage>157</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Isope1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isope</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name> (<year>2002</year>) <article-title>Properties of unitary Granule cell to Purkinje cell synapses in adult rat cerebellar slices</article-title>. <source>J Neurosci</source> <volume>22</volume>: <fpage>9668</fpage>–<lpage>9678</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Chadderton1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chadderton</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Margrie</surname><given-names>TW</given-names></name>, <name name-style="western"><surname>Hausser</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Integration of quanta in cerebellar granule cells during sensory processing</article-title>. <source>Nature</source> <volume>428</volume>: <fpage>856</fpage>–<lpage>860</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Rancz1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rancz</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Ishikawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Duguid</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Chadderton</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Mahon</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>High-fidelity transmission of sensory information by single cerebellar mossy fibre boutons</article-title>. <source>Nature</source> <volume>450</volume>: <fpage>1245</fpage>–<lpage>1248</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Cover2">
        <label>24</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cover T, Thomas J (1991) Elements of Information Theory. New York: Wiley.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Clopath1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Storage of correlated patterns in standard and bistable purkinje cell models</article-title>. <source>Plos Comp Biol</source> <volume>8</volume>: <fpage>e1002448</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Bidoret1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bidoret</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ayon</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Casado</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Presynaptic NR2A-containing NMDA receptors implement a high-pass filter synaptic plasticity rule</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>106</volume>: <fpage>14126</fpage>–<lpage>14131</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Widrow1">
        <label>27</label>
        <mixed-citation publication-type="other" xlink:type="simple">Widrow B, Hoff ME (1960) Adaptive switching circuits. In: 1960 IRE WESCON Convention Record. New York: IRE. pp. 96–104.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Lee1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>1999</year>) <article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source> <volume>401</volume>: <fpage>788</fpage>–<lpage>791</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Lee2">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2001</year>) <article-title>Algorithms for non-negative matrix factorizatio</article-title>. <source>Adv Neural Info Proc Syst</source> <volume>13</volume>: <fpage>556</fpage>–<lpage>562</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Fujita1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fujita</surname><given-names>M</given-names></name> (<year>1982</year>) <article-title>Simulation of adaptive modification of the vestibulo-ocular reflex with an adaptive filter model of the cerebellum</article-title>. <source>Biol Cybern</source> <volume>45</volume>: <fpage>207</fpage>–<lpage>214</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Dean1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dean</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stone</surname><given-names>JV</given-names></name> (<year>2002</year>) <article-title>Decorrelation control by the cerebellum achieves oculomotor plant compensation in simulated vestibulo-ocular reflex</article-title>. <source>Proc Biol Sci</source> <volume>269</volume>: <fpage>1895</fpage>–<lpage>1904</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Porrill1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dean</surname><given-names>P</given-names></name> (<year>2007</year>) <article-title>Cerebellar motor learning: when is cortical plasticity not enough?</article-title> <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>1935</fpage>–<lpage>1950</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Lepora1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lepora</surname><given-names>NF</given-names></name>, <name name-style="western"><surname>Porrill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yeo</surname><given-names>CH</given-names></name>, <name name-style="western"><surname>Dean</surname><given-names>P</given-names></name> (<year>2010</year>) <article-title>Sensory prediction or motor control? Application of marr-albus type models of cerebellar function to classical conditioning</article-title>. <source>Front Comput Neurosci</source> <volume>4</volume>: <fpage>140</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002919-Gardner2">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Derrida</surname><given-names>B</given-names></name> (<year>1988</year>) <article-title>Optimal storage properties of neural network models</article-title>. <source>J Phys A:Gen</source> <volume>21</volume>: <fpage>271</fpage>–<lpage>284</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>