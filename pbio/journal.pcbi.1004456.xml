<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-02254</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004456</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Explaining the Timing of Natural Scene Understanding with a Computational Model of Perceptual Categorization</article-title>
<alt-title alt-title-type="running-head">Principles of Natural Scene Categorization</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Sofer</surname> <given-names>Imri</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Crouzet</surname> <given-names>Sébastien M.</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Serre</surname> <given-names>Thomas</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Cognitive, Linguistic &amp; Psychological Sciences Department, Brown Institute for Brain Science, Brown University, Providence, Rhode Island, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname> <given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Technische Universitat Chemnitz, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: IS SMC TS. Performed the experiments: IS. Analyzed the data: IS. Wrote the paper: IS SMC TS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">thomas_serre@brown.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>9</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>3</day>
<month>9</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>9</issue>
<elocation-id>e1004456</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>12</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>7</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Sofer et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004456" xlink:type="simple"/>
<abstract>
<p>Observers can rapidly perform a variety of visual tasks such as categorizing a scene as open, as outdoor, or as a beach. Although we know that different tasks are typically associated with systematic differences in behavioral responses, to date, little is known about the underlying mechanisms. Here, we implemented a single integrated paradigm that links perceptual processes with categorization processes. Using a large image database of natural scenes, we trained machine-learning classifiers to derive quantitative measures of task-specific perceptual discriminability based on the distance between individual images and different categorization boundaries. We showed that the resulting discriminability measure accurately predicts variations in behavioral responses across categorization tasks and stimulus sets. We further used the model to design an experiment, which challenged previous interpretations of the so-called “superordinate advantage.” Overall, our study suggests that observed differences in behavioral responses across rapid categorization tasks reflect natural variations in perceptual discriminability.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The speed of sight has fascinated scientists and philosophers for centuries. In the blink of an eye, observers can rapidly and effortlessly perform a variety of categorization tasks such as categorizing a scene as open, as natural, or as a beach. The past decade of work has shown that there exist systematic differences in behavioral responses across different categorization tasks: For instance, participants appear to be faster and more accurate at categorizing a scene as outdoor (i.e., superordinate level) compared to categorizing a scene as a beach (i.e., basic level). Here, we describe a computational model combined with human psychophysics experiments, which help shed light on the underlying mechanisms. Using a large natural scene database, we trained machine learning algorithms for different categorization tasks and showed that it is possible to derive confidence measures that accurately predict variations in participants’ behavioral responses across categorization tasks and stimulus sets. Using the computational model to sample stimuli for a human experiment, we demonstrated that it is possible to reverse the superordinate advantage, rendering human observers superordinate categorization slower and less accurate than basic categorization—effectively challenging previous interpretations of the phenomenon. The study further offers a vivid example on how computational models can help summarize and organize existing experimental data as well as plan and interpret new experiments.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by the National Science Foundation (NSF) early career award [grant number IIS-1252951 to TS]. Additional support was provided by the Defense Advanced Research Projects Agency (DARPA) young faculty award [grant number YFA N66001-14-1-4037 to TS], the Office of Naval Research (ONR) grant [grant number N000141110743 to TS], the Brown Institute for Brain Sciences (BIBS), the Center for Vision Research (CVR), and the Center for Computation and Visualization (CCV). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Categorization is perhaps one of our most critical visual functions as it allowed our ancestors to distinguish friend from foe and the edible from the inedible. Observers can rapidly extract meaning from brief presentations of complex visual scenes [<xref ref-type="bibr" rid="pcbi.1004456.ref001">1</xref>]—far exceeding the best existing engineered artificial systems [<xref ref-type="bibr" rid="pcbi.1004456.ref002">2</xref>].</p>
<p>Observers can reliably perform a variety of categorization tasks [<xref ref-type="bibr" rid="pcbi.1004456.ref003">3</xref>] such as categorizing a scene as open, as outdoor, or as a beach. However, it has also been shown that there exist systematic differences in participants’ behavioral responses across categorization tasks. In particular, categorizing a scene as open or navigable (i.e., attribute level) necessitates shorter presentation times than categorizing a scene as a lake or a beach (i.e., basic level, see [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>]). (Note that our definition of basic-levelness follows the common usage in vision science (see [<xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004456.ref013">13</xref>]) and reflects a logical [<xref ref-type="bibr" rid="pcbi.1004456.ref014">14</xref>] rather than functional definition of the basic level.) Similarly, participants appear to be faster and more accurate when categorizing a scene as outdoor (i.e., superordinate level) compared to categorizing a scene at a basic level [<xref ref-type="bibr" rid="pcbi.1004456.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref013">13</xref>]. A very recent study further suggests that subordinate scene categorization is less sensitive and slower than basic level categorization [<xref ref-type="bibr" rid="pcbi.1004456.ref015">15</xref>].</p>
<p>Beyond the categorization of natural scenes, there exist systematic differences in behavioral responses for object categories across taxonomic levels with observers’ subordinate-level categorization (e.g., pigeons vs. other birds) being slower and less accurate than basic-level categorization (e.g., birds vs. non-birds, see [<xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>]. Similarly, basic-level categorization (e.g., birds vs. dogs) has been shown to be slower than superordinate categorization (e.g., animals vs. non-animals, see [<xref ref-type="bibr" rid="pcbi.1004456.ref016">16</xref>]). Participants tend to be faster and more accurate at categorizing faces at the superordinate level (i.e. categorizing faces vs. non-faces) compared with categorizing faces at the familiarity level (famous vs. non-famous, see [<xref ref-type="bibr" rid="pcbi.1004456.ref006">6</xref>]). However, for both familiar faces and other individually-known familiar objects, categorization at the subordinate level is faster than at the basic level [<xref ref-type="bibr" rid="pcbi.1004456.ref017">17</xref>]. Similarly, there exist systematic differences in behavioral responses for different social inference tasks [<xref ref-type="bibr" rid="pcbi.1004456.ref012">12</xref>]: For instance, categorization at the level of intentionality is faster than categorization at the level of belief and personality.</p>
<p>Such systematic behavioral differences across categorization tasks are often taken as suggestive evidence for an underlying hierarchical organization of categorization processes with some categorization tasks taking precedence over others [<xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004456.ref013">13</xref>], but see also [<xref ref-type="bibr" rid="pcbi.1004456.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1004456.ref020">20</xref>]. Overall, the past decade of research on visual categorization has produced a significant and rapidly increasing amount of data and, while systematic differences across categorization tasks have been well-characterized to date, little is known about the underlying mechanisms.</p>
<p>In this study, we describe a computational model to account for variations in participants’ behavioral responses (both accuracy and reaction time) across tasks and stimuli for the rapid categorization of natural scenes. Previous work has proceeded along two seemingly parallel paths (see [<xref ref-type="bibr" rid="pcbi.1004456.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref022">22</xref>] for discussions) with a nearly exclusive focus on modeling either visual representations (see [<xref ref-type="bibr" rid="pcbi.1004456.ref023">23</xref>] for review) or categorization and decision-making (see [<xref ref-type="bibr" rid="pcbi.1004456.ref024">24</xref>] for review). Here, we implemented a single integrated paradigm that links perception with categorization processes.</p>
<p>Formally, visual categorization corresponds to the process of associating visual stimuli <bold>x</bold><sub><italic>i</italic> = 1…<italic>m</italic></sub> to category labels <italic>y</italic><sub><italic>i</italic> = 1…<italic>m</italic></sub> to form (<bold>x</bold><sub><italic>i</italic></sub>,<italic>y</italic><sub><italic>i</italic></sub>) exemplar-label pairs. <bold>x</bold><sub><italic>i</italic></sub> may be parametrized by a feature vector in a <italic>N</italic>-dimensional perceptual space <inline-formula id="pcbi.1004456.e001"><alternatives><graphic id="pcbi.1004456.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e001"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mi>N</mml:mi></mml:msubsup> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <xref ref-type="fig" rid="pcbi.1004456.g001">Fig 1A</xref> illustrates such a feature space for an hypothetical population of <italic>N</italic> = 2 feature detectors (in practice, we expect <italic>N</italic> to be much larger). Learning to categorize visual stimuli requires learning a categorization boundary that best represents the relation between input images <bold>x<sub><italic>i</italic></sub></bold> and their corresponding category labels <italic>y</italic><sub><italic>i</italic></sub>. Once a categorization boundary had been learned, the classification of a stimulus depends on its position relative to the categorization boundary: One side of the categorization boundary will be associated with a target set of stimuli while the other side will be associated with the distractor set. An illustration for hypothetical decision boundaries corresponding to different taxonomic levels is shown in <xref ref-type="fig" rid="pcbi.1004456.g001">Fig 1B–1D</xref>. According to this computational framework, different categorization tasks correspond to different decision boundaries, which carve the same perceptual space, an idea that has motivated the development of most existing computational models of perceptual categorization (see [<xref ref-type="bibr" rid="pcbi.1004456.ref022">22</xref>] for review).</p>
<fig id="pcbi.1004456.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Principles of scene categorization.</title>
<p>(A) Perceptual space: Visual features are first extracted from individual images, which can then be represented as datapoints in an <italic>N</italic>-dimensional space. (B–D) Categorization boundaries: The model assumes that different categorization tasks carve up the same perceptual space and correspond to different categorization boundaries (shown for hypothetical tasks: Superordinate level—‘natural’ vs. ‘man-made’ in (B), basic level—‘beach’ vs. ‘forest’ in (C) and scene attribute—‘easy’ vs. ‘hard’ to navigate in (D).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g001"/>
</fig>
<p>We used a rudimentary visual representation based on the “gist” algorithm [<xref ref-type="bibr" rid="pcbi.1004456.ref025">25</xref>] but other visual representations are possible (see [<xref ref-type="bibr" rid="pcbi.1004456.ref023">23</xref>] for review; see also <xref ref-type="sec" rid="sec026">Discussion</xref>). We further used a large image database [<xref ref-type="bibr" rid="pcbi.1004456.ref026">26</xref>] to train and test machine learning classifiers (regularized logistic regression) and estimate the decision boundaries associated with many different scene categorization tasks. A task-dependent measure of perceptual discriminability can then be derived for a particular categorization task by considering the distance between individual stimuli and the categorization boundary (<xref ref-type="fig" rid="pcbi.1004456.g002">Fig 2A</xref>). The basic intuition for this measure is that, for a particular categorization task, images that are closer to the categorization boundary will be harder to categorize than those that are further away leading to behavioral responses that are slower and less accurate. Furthermore, these values can be aggregated to yield estimates of accuracy for arbitrary sets of target and distractor stimuli (<xref ref-type="fig" rid="pcbi.1004456.g002">Fig 2B</xref>).</p>
<fig id="pcbi.1004456.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Computing discriminability values.</title>
<p>(A) For an individual image and a specific categorization task (e.g., task 1), discriminability values are derived from the model by considering the distance <italic>d</italic><sub>1</sub> between the image and the categorization boundary associated with task 1. Here we tested the hypothesis that for a given stimulus and task, discriminability values drive participants’ average categorization accuracy and reaction times. (B) Discriminability values can also be computed for arbitrary sets of target (green) and distractor (brown) images. The normalized distance between these two distributions will determine how easy or difficult the task, as a whole, will be for human participants.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g002"/>
</fig>
<p>The goal of the present study was to test the hypothesis that the perceptual discriminability of individual stimuli for a particular task is one of the main factors driving behavioral responses. While this hypothesis is built-in for many categorization models (see [<xref ref-type="bibr" rid="pcbi.1004456.ref024">24</xref>] for review), it had so far only been tested with simple artificial stimuli where participants were trained to learn a new object category parametrized by two dimensions (e.g., [<xref ref-type="bibr" rid="pcbi.1004456.ref027">27</xref>], but see also [<xref ref-type="bibr" rid="pcbi.1004456.ref028">28</xref>] for alternative models.) However, this hypothesis has not yet been tested for well-learned, natural categories.</p>
<p>We first found that model-derived discriminability values predicted well behavioral responses for different categorization tasks as reported in two published studies [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>]. In addition, in experiment 1, we were further able to show that the model accurately predicted variations in accuracy and reaction time at the level of individual stimuli within the context of a scene categorization task. We then used the model to test the hypothesis that the so-called “superordinate advantage” [<xref ref-type="bibr" rid="pcbi.1004456.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref013">13</xref>], whereby superordinate categorization is faster and more accurate than basic categorization, may reflect the greater perceptual discriminability of scenes at the superordinate vs. basic level. Consistent with this hypothesis, we first found that the model was consistent with the reported results of a published study on the superordinate advantage [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>]. In experiment 2, we further showed that it is possible to use model-derived discriminability values to sample stimuli and to effectively reverse the superordinate advantage, making participants superordinate categorization slower and less accurate than basic categorization, thus offering a possible perceptual explanation of the phenomenon.</p>
<p>Overall, our results provide a computational-level explanation for the systematic variations in rapid categorization behavioral responses across taxonomic levels, suggesting that these differences may simply reflect natural variations in perceptual discriminability. Our study thus challenges several existing theories of visual processing and offers a vivid example of how computational models can help summarize existing data as well as plan and interpret novel experiments.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>General methods</title>
<sec id="sec004">
<title>Ethics statement</title>
<p>The protocol was approved by Brown IRB [protocol #1002000135] and was carried out in accordance with the provisions of the World Medical Association Declaration of Helsinki. All participants reported having normal or corrected-to-normal vision and gave written informed consent.</p>
</sec>
<sec id="sec005">
<title>Stimulus pre-processing</title>
<p>Images were converted to grayscale, cropped to a squared image, and then rescaled to 256×256 pixels. To minimize low-level brightness differences between targets and distractors, stimuli for each individual session were set to a constant mean brightness value (equal for all images in the corresponding session).</p>
</sec>
<sec id="sec006">
<title>Computational model</title>
<p>The visual representation used here, called “gist” is relatively low-level [25, <xref ref-type="supplementary-material" rid="pcbi.1004456.s003">S1 Fig</xref>]. It was chosen for its simplicity in the absence of any strong evidence that a more complex visual representation would lead to significantly different model predictions (<xref ref-type="supplementary-material" rid="pcbi.1004456.s004">S2 Fig</xref>). Briefly, the image was first convolved with a bank of 32 Gabor filters (4 different scales in 8 orientations). The resulting convolution maps were then averaged separately in individual cells on a 4 × 4 grid covering the whole image. This yielded a 512-dimensional feature vector. Matlab source code for the “gist” is available online (see [<xref ref-type="bibr" rid="pcbi.1004456.ref025">25</xref>] for details).</p>
<p>Categorization boundaries were learned from natural scenes using a logistic regression classifier with L2 regularization. Software was implemented in Python using the scikit-learn [<xref ref-type="bibr" rid="pcbi.1004456.ref029">29</xref>] and the liblinear library [<xref ref-type="bibr" rid="pcbi.1004456.ref030">30</xref>]. Comparable levels of accuracy and qualitatively similar patterns of results were obtained with other types of classifiers (e.g., SVMs) as well as more complex kernels (see <xref ref-type="supplementary-material" rid="pcbi.1004456.s001">S1 Text</xref>). Categorization tasks were modeled as binary categorization tasks except in two of the comparisons with published results [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>], which were modeled using a one-vs-all multi-class classification approach.</p>
<p>Classifiers were trained and tested using cross-validation techniques whereby images were split into disjoint training and test sets multiple times at random (with replacement). The number of images sampled had to be varied across experiments because we tried to use the maximal number of samples available while creating datasets containing a balanced number of positive and negative samples. Except when noted, training and test data were split using an 80–20% training-test split. All hyper-parameters were optimized on the training set using a 5-fold cross-validation procedure.</p>
<p>Discriminability values were estimated by computing the average (test) classification error for each image in the dataset over multiple splits of the training/test data. This measure is simpler to compute than estimating the average distance between an image and the categorization boundary across all training/test data splits and, in practice, we found these two measures to agree closely.</p>
<p>Except when noted, the model accuracy was computed as the average rate of correct (test) classification over all random splits of the training/test data (<italic>N</italic> = 100, unless specified otherwise).</p>
</sec>
<sec id="sec007">
<title>Apparatus and procedure</title>
<p>Participants sat in a dimly lit room. They were instructed to sit with their back leaning against the chair so as to maintain a viewing distance of approximately 75 cm to the CRT monitor (800 × 600 pixels, refresh rate of 140 Hz). Stimulus presentation was controlled using Matlab and the PsychToolbox [<xref ref-type="bibr" rid="pcbi.1004456.ref031">31</xref>] on a Mac Pro. Behavioral responses were collected using two handheld thumb button switches connected to a response time box [<xref ref-type="bibr" rid="pcbi.1004456.ref032">32</xref>].</p>
<p>On each trial, the experiment ran as follows: On a black background (1) a fixation cross appeared for a variable time (1,100–1,600 ms); (2) a stimulus (10° × 10°) was presented for a single frame (7 ms). The order of image presentations was randomized. Participants were instructed to answer as fast and as accurately as possible by pressing the button in their strong hand if they saw a target, and the other button if they saw a distractor. Participants were forced to respond within 500 ms (a sound was played and a message displayed in the absence of a response past the response deadline). At the end of each block, participants received feedback about their accuracy. An illustration of the experimental paradigm is shown in <xref ref-type="fig" rid="pcbi.1004456.g003">Fig 3A</xref>.</p>
<fig id="pcbi.1004456.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Experiment 1: Model discriminability values predicted participants mean accuracy and RTs.</title>
<p>(A). Overview of the experimental design: Each trial began with a fixation cross followed by the subsequent brief presentation of an image (7 ms). Participants were required to respond within 500 ms. (B) Representative scenes sampled at five distinct discriminability value levels for a natural vs. man-made categorization task. Note that the original stimuli used could not be shown because of copyright and were replaced instead by visually similar images found on Flickr under the creative common. (C) Average results across all participants: Accuracy (percentage of correct responses, blue) and mean reaction time (RT) for correct responses (red) as a function of discriminability values as predicted by the model. Curves correspond to a GLMM fit and error bars to the standard deviation of the mean. (D) Results for individual participants.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g003"/>
</fig>
</sec>
<sec id="sec008">
<title>Sample size and stopping criterion</title>
<p>Here, we applied a Bayesian analysis of results (see below). Thus, there was no need to predetermine a stopping rule or sample size, as the analysis does not depend on the researchers’ intentions [<xref ref-type="bibr" rid="pcbi.1004456.ref033">33</xref>].</p>
</sec>
</sec>
<sec id="sec009">
<title>Predicting behavioral categorization based on discriminability: Existing literature and experiment 1</title>
<sec id="sec010">
<title>Model initial validation</title>
<p>We initially validated the model by postdicting behavioral categorization results reported in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>]. Because of the relatively small size of the image dataset, the proportion of training images relative to test images had to be increased to reproduce the results described in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>] (96/4% training/test split). Because of the nature of the task in [<xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>], the accuracy measure <italic>acc</italic><sub><italic>i</italic>,<italic>j</italic></sub> for discriminating between category <italic>i</italic> and <italic>j</italic> was computed as 1 minus the fraction of images from category <italic>i</italic> classified as category <italic>j</italic> and vice-versa.</p>
</sec>
<sec id="sec011">
<title>Using the model to sample stimuli</title>
<p>The model was trained to discriminate between ‘natural’ and ‘man-made’ scenes using the SUN database [<xref ref-type="bibr" rid="pcbi.1004456.ref026">26</xref>], which is currently the largest available scene database with nearly 400 basic level categories for a total of approximately 100,000 images. We selected the following man-made categories: ‘skyscraper,’ ‘highway,’ ‘street,’ ‘tower,’ ‘alley,’ ‘apartment building,’ and ‘amphitheater’ and the following natural categories ‘beach,’ ‘desert,’ ‘cultivated field,’ ‘coral reef,’ ‘iceberg,’ ‘forest’ (by combining ‘broad leaf tree,’ ‘needle leaf tree’ and ‘rain forest’), ‘mountain’ (by combining ‘mountain,’ ‘snowy mountain,’ ‘coast,’ and ‘cliff’).</p>
<p>Decision values were estimated for each individual image using the procedure described in <italic>General methods</italic>. We binned stimuli according to their associated discriminability values (5 bins for each superordinate category: 0.2, 0.4, 0.6, 0.8, and 1.0) and sampled 96 images for each bin. Each sampled image was inspected manually. If the category label for the image was found to be ambiguous (e.g., a house in a prairie may yield some ambiguity in the corresponding class label because a house is man-made but a prairie is natural), a stimulus was re-sampled from the same bin. For each level, the resulting mean discriminability values for the chosen images was computed to ensure that it remained close to the target discriminability values.</p>
</sec>
<sec id="sec012">
<title>Experimental paradigm</title>
<p>A total of 8 participants completed the experiment (6 males, 2 females; mean age 21 years, range 20–26; all right-handed). All participants reported having normal or corrected-to-normal vision and gave written informed consent.</p>
<p>The experiment followed the experimental design described in <italic>General methods</italic>. We used a within-subject factorial design: 2 categories (man-made vs natural) × 5 discriminability values (0.2, 0.4, 0.6, 0.8, and 1.0) derived from the model. Participants first viewed 20 natural and 20 man-made images randomly selected from the target and distractor image sets (4 from each discriminability value). Participants subsequently completed 16 blocks total. In each block, 6 images from each condition were presented in a random order leading to 60 images in each block and 960 trials in total.</p>
</sec>
<sec id="sec013">
<title>Analysis of results</title>
<p>Signal detection theory can be formulated as a special instance of a generalized linear model to estimate experimental effects on participants [<xref ref-type="bibr" rid="pcbi.1004456.ref034">34</xref>]. It can also be extended to the population level using generalized linear mixed effect models [<xref ref-type="bibr" rid="pcbi.1004456.ref035">35</xref>], thus providing a very powerful and efficient estimation technique. In addition, mixed effect models are equivalent to Bayesian hierarchical models with an uninformative prior [<xref ref-type="bibr" rid="pcbi.1004456.ref036">36</xref>]. Therefore, this analysis did not suffer from the common drawbacks associated with null hypothesis significance testing [<xref ref-type="bibr" rid="pcbi.1004456.ref033">33</xref>].</p>
<p>The response <italic>y</italic> of each participant was modeled as:
<disp-formula id="pcbi.1004456.e002"><alternatives><graphic id="pcbi.1004456.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mi>r</mml:mi> <mml:mi>o</mml:mi> <mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:msup><mml:mi>t</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>probit</italic><sup>−1</sup> denotes the cumulative distribution function of the standard normal distribution, <italic>β</italic><sub><italic>bias</italic></sub> the response bias of the participant, and <italic>β</italic><sub><italic>sens</italic></sub> the participant’s sensitivity for categorizing ‘natural’ vs. ‘man-made’ images for a middle discriminability target value (discriminability value = 0.6). <italic>β</italic><sub><italic>slope</italic></sub> corresponds to the change in sensitivity associated with a change in discriminability value, and it is the parameter of interest for this analysis. For each trial, the participant’s response was set to 1 for ‘man-made’ responses and, and 0 for ‘natural’ responses. <italic>x</italic><sub><italic>sens</italic></sub> was set to 0.5 for man-made trials, and −0.5 for natural trials. <italic>x</italic><sub><italic>slope</italic></sub>, which codes for the discriminability value, was set to (−2,1,0,1,2) for natural images with corresponding discriminability values (1,0.8,0.6,0.4,0.2). For the man-made category, a reversed coding scheme was used.</p>
<p>The RT at trial <italic>i</italic> for each subject was modeled in a similar manner:
<disp-formula id="pcbi.1004456.e003"><alternatives><graphic id="pcbi.1004456.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi> <mml:mi>T</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
with <italic>x</italic><sub><italic>slope</italic></sub> coded as (−2,1,0,1,2) for images with associated discriminability values (1,0.8,0.6,0.4,0.2). P-values and confidence intervals for all experiments and model analyses were estimated using <italic>n</italic> = 10,000 Monte Carlo samples. P-values referred to a two-tailed test.</p>
</sec>
</sec>
<sec id="sec014">
<title>Using the model to reverse the superordinate advantage: Existing literature and experiment 2</title>
<sec id="sec015">
<title>Model initial validation</title>
<p>We initially validated the model by postdicting behavioral categorization results reported in [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>] (experiment 1 and 2), using the procedure described in General Methods. The model was trained and tested using the 8-scene database [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>] (which constitutes a superset of the manually sampled subset used in [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>]).</p>
</sec>
<sec id="sec016">
<title>Using the model to sample stimuli</title>
<p>For this experiment, we used the computational model to create sets of natural and man-made stimuli. First, sets of stimuli were obtained by considering combinations of 3 basic categories from a larger set of natural (‘beach,’ ‘desert,’ ‘cultivated field,’ ‘coral reef,’ and ‘iceberg’) and man-made (‘skyscraper,’ ‘highway,’ ‘street,’ ‘tower,’ ‘alley,’ ‘apartment building,’ and ‘amphitheater’) categories. The computational model was then tested on all possible combinations of natural and man-made sets for categorization at the superordinate level (man-made / target vs. natural / distractor categorization) and at the basic level (forest / target vs. natural / distractor categorization) as done in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>]. The same set of natural stimuli was used as distractor for both categorization tasks.</p>
<p>We subsequently chose two sets of natural and two sets of man-made stimuli to create two experimental conditions: a <italic>superordinate advantage</italic> condition for which the model predicted high perceptual discriminability for superordinate-level categorization but low discriminability for basic-level categorization and a <italic>basic advantage</italic> condition for which the model predicted the opposite trend (low perceptual discriminability for superordinate-level categorization and high discriminability for basic-level categorization). This yielded the following category combinations for the superordinate advantage condition: ‘beach,’ ‘cultivated field,’ and ‘coral reef’ for natural categories and ‘alley,’ ‘street,’ and ‘skyscraper’ for man-made stimuli and the following category combinations for the basic advantage condition: ‘beach,’ ‘iceberg,’ and ‘desert’ for the natural categories while the man-made categories were ‘alley,’ ‘amphitheater,’ and ‘highway.’</p>
<p>For each experimental condition, 168 images were randomly sampled from both target and distractor categories. All sampled images were inspected visually and images for which the associated class label was deemed ambiguous were replaced by a randomly sampled image. To generate predictions for individual tasks, we re-trained the classifiers using the cross-validation procedure described in <italic>General methods</italic>.</p>
</sec>
<sec id="sec017">
<title>Experimental paradigm</title>
<p>A total of 24 participants completed the experiment (8 males, 16 females; mean age 24 years, range 18–25; all right-handed). All participants reported having normal or corrected-to-normal vision and gave written informed consent.</p>
<p>The experiment started with a practice block for an unrelated rapid categorization task (animal vs. non-animal) to familiarize participants with the experimental paradigm. The experiment began after participants correctly categorized 75% of the images in a single practice block. In addition, participants were allowed to browse through the stimulus set used in the session before the main experiment to familiarize themselves with the task.</p>
<p>We used a mixed design: 2 conditions (superordinate advantage and basic advantage) × 3 target categories (forest, mountain, and man-made). Half of the participants were assigned to the superordinate advantage condition and half to the basic advantage condition. Three tasks were tested: one superordinate (man-made vs. natural) and two basic categorization tasks (forest vs. natural and mountain vs. natural). Each participant completed 18 blocks (6 blocks for each task, 56 stimuli per block, 336 stimuli per task for a total of 1,008 trials). The order of the blocks was counterbalanced across participants. Each target image appeared only once for the entire experiment while each distractor appeared 3 times (once for each task). In each block, targets and distractors appeared with an equal probability. The target category was indicated at the beginning of each block with a written instruction on the screen together with 16 random exemplar images (8 targets and 8 distractors).</p>
</sec>
<sec id="sec018">
<title>Analysis of results</title>
<p>The original experiment included three tasks: 1 superordinate and 2 basic-level (forest and mountain) categorization tasks. The superordinate and the forest categorization tasks were the main factors tested in the experiment, and the mountain task was introduced to collect additional data. However, we observed a pervasive influence of a speed-accuracy tradeoff (SAT) for the mountain task: Participants appeared to be using a different SAT criterion (they were either more accurate and slower or less accurate and faster) and behavioral responses for this task could not simply be compared to behavioral responses for the other two tasks. This result did not conflict with our main hypothesis that the superordinate advantage can be reversed and the task was simply excluded from further analysis.</p>
<p>The behavioral response <italic>y</italic> of each participant was modeled as:
<disp-formula id="pcbi.1004456.e004"><alternatives><graphic id="pcbi.1004456.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mi>r</mml:mi> <mml:mi>o</mml:mi> <mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:msup><mml:mi>t</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>probit</italic><sup>−1</sup>, <italic>β</italic><sub><italic>bias</italic></sub> and <italic>β</italic><sub><italic>sens</italic></sub> were defined as in experiment 1, and <italic>β</italic><sub><italic>cont</italic></sub> corresponded to the change in sensitivity between the superordinate and the basic-level categorization task. For this analysis, <italic>β</italic><sub><italic>cont</italic></sub> was the key parameter of interest. This formulation is similar to a two-factor anova, where <italic>β</italic><sub><italic>bias</italic></sub> represents the first main effect, <italic>β</italic><sub><italic>sens</italic></sub> the second main effect, and <italic>β</italic><sub><italic>cont</italic></sub> the interaction.</p>
<p>As for experiment 1, for each trial, <italic>y</italic> was set to 1 if the participant pressed the target button, and 0 otherwise (non response trials were omitted). <italic>x</italic><sub><italic>sens</italic></sub> was set to 0.5 for target trials and −0.5 for distractor trials. <italic>x</italic><sub><italic>cont</italic></sub> was set to 0.25 for superordinate/target trials and for basic/distractor trials, and it was set to −0.25 for superordinate/distractor trials and for basic/target trials. All parameters were set as random effects to allow them to vary for each individual participant. The same model was fitted to each condition separately, and from each, one can derive the sensitivity for the two tasks in that condition:
<disp-formula id="pcbi.1004456.e005"><alternatives><graphic id="pcbi.1004456.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtext>Basic:</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula> <disp-formula id="pcbi.1004456.e006"><alternatives><graphic id="pcbi.1004456.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtext>Superordinate:</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
<p>The model used for RTs was similar, albeit simpler, since we only used correct trials. For each individual trial and subject, the RT was modeled as:
<disp-formula id="pcbi.1004456.e007"><alternatives><graphic id="pcbi.1004456.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004456.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi> <mml:mi>T</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>b</mml:mi> <mml:mi>i</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>β</italic><sub>0</sub> denotes the mean RT, <italic>β</italic><sub><italic>bias</italic></sub> the response bias, and <italic>β</italic><sub><italic>cont</italic></sub> the change in RT between the two tasks. <italic>x</italic><sub><italic>bias</italic></sub> was set to 0.5 for target trials −0.5 for distractor trials. <italic>x</italic><sub><italic>cont</italic></sub> was set to 0.5 for the superordinate-level categorization task and to -0.5 for the basic-level categorization task. Monte-Carlo samples (<italic>n</italic> = 10,000) were used to estimate p-values and confidence intervals for all experiments and analyses. P-values refer to two-tailed test.</p>
</sec>
</sec>
</sec>
<sec id="sec019" sec-type="results">
<title>Results</title>
<sec id="sec020">
<title>Predicting behavioral categorization based on discriminability: Existing literature and experiment 1</title>
<sec id="sec021">
<title>Model initial validation</title>
<p>As an initial validation of the model, we considered two representative rapid scene categorization studies [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>] to compare the model’s predicted perceptual discriminability for different categorization tasks (across taxonomic levels) against human behavioral responses. For both studies, we trained the computational model using the stimuli set from the original experiments, and assessed the model’s discriminability for the same tasks. We then compared the model discriminability scores against human behavioral responses (as reported in the original studies).</p>
<p>In [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>], the authors used a staircase procedure to estimate the presentation duration needed for participants to reach a fixed level of accuracy for fourteen distinct scene categorization tasks [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>]. These tasks were based on either scene attributes (concealment, depth, naturalness, navigability, openness, temperature, and transience) or basic level category membership (desert, field, forest, lake, mountain, ocean, and river). We took participants’ presentation time thresholds as reported in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>] and compared them to the model-predicted perceptual discriminability. As expected we found them to be negatively correlated (Spearman correlation; <italic>r</italic>(12) = −0.57,<italic>p</italic> = 0.03; <xref ref-type="fig" rid="pcbi.1004456.g004">Fig 4A</xref>).</p>
<fig id="pcbi.1004456.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Comparison between the model predicted task discriminability against human accuracy on multiple scene categorization tasks based on two representative studies.</title>
<p>(A) Negative correlation between the model predicted task discriminability and participants’ presentation time threshold [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>]. (B) Positive correlation between the model predicted task discriminability and participants sensitivity [<xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>] (a small jitter was added to the display in (B) to improve visualization).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g004"/>
</fig>
<p>In [<xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>], the authors looked at the rate at which participants classify two masked images, which belong to different categories, as belonging to the same category. This rate was used to define the perceptual similarity between any two categories. The authors tested all possible pairs of 15 categories, which resulted in 105 pairs of categories overall. We correlated the human sensitivity scores reported in [<xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>] for individual tasks against the discriminability predicted by the model for the same tasks (Spearman correlation; <italic>r</italic>(103) = 0.64,<italic>p</italic> &lt; 10<sup>−4</sup>; <xref ref-type="fig" rid="pcbi.1004456.g004">Fig 4B</xref>).</p>
<p>Overall, discriminability values derived from the computational model appeared sufficient to account for observed participants’ variations in behavioral responses for a relatively large and disparate number of tasks across experiments. Beyond this initial model validation, we will next show that it is possible to use the model to sample stimulus sets in order to systematically manipulate participants’ behavioral responses.</p>
</sec>
<sec id="sec022">
<title>Experiment 1</title>
<p>We assessed the accuracy and RTs from human participants using a rapid man-made vs. natural scene categorization paradigm. Images were sampled using discriminability values derived from the model. Sample images for each level of discriminability are shown in <xref ref-type="fig" rid="pcbi.1004456.g003">Fig 3B</xref>. On average, participants answered correctly on 83.0% of the trials (±2.4%). Trials for which participants failed to answer before the deadline were excluded from further analysis (5% of the total number of trials). The mean RT for correct responses was 372 ms (±7 ms), and is comparable to previously published results [<xref ref-type="bibr" rid="pcbi.1004456.ref007">7</xref>].</p>
<p>The model predicted a monotonic increase in accuracy and corresponding monotonic decrease in reaction time as a function of the stimulus discriminability values on either side of the categorization boundary. We thus fitted one generalized linear mixed effect model (GLMM) to behavioral responses to estimate the change in the rate of correct responses as a function of discriminability values and one separate GLMM to RTs (Methods). Decision values were found to have a significant effect at the group level for both accuracy (<italic>β</italic><sub><italic>slope</italic></sub> = 0.14, 95% confidence interval <italic>CI</italic> = [0.10,0.17], <italic>p</italic> &lt; 10<sup>−4</sup>) and RT (<italic>β</italic><sub><italic>slope</italic></sub> = 3.92,<italic>CI</italic> = [2.85,5.02],<italic>p</italic> &lt; 10<sup>−4</sup>). Results are shown in <xref ref-type="fig" rid="pcbi.1004456.g003">Fig 3C</xref>. These group-level results also held for individual participants as shown in <xref ref-type="fig" rid="pcbi.1004456.g003">Fig 3D</xref> (<italic>p</italic> &lt; 10<sup>−3</sup> for all participants).</p>
<p>These results validate the model key hypothesis that, for a given categorization task, variations in behavioral responses across stimuli are accounted for by the predicted stimulus’ perceptual discriminability for that particular task. Could natural variations in task discriminability thus also account for systematic variations in behavioral responses found across categorization tasks—including differences reported across taxonomic levels as exemplified by the “superordinate advantage”?</p>
</sec>
</sec>
<sec id="sec023">
<title>Using the model to reverse the superordinate advantage: Existing literature and experiment 2</title>
<sec id="sec024">
<title>Model initial validation</title>
<p>A re-drawing of <xref ref-type="fig" rid="pcbi.1004456.g004">Fig 4</xref> with the addition of color labels to indicate the taxonomic levels of the different tasks used in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>] makes it clear that behavioral differences between taxonomic levels (attribute vs. basic level in <xref ref-type="fig" rid="pcbi.1004456.g005">Fig 5A</xref> or basic vs. superordinate level in <xref ref-type="fig" rid="pcbi.1004456.g005">Fig 5B</xref>) can be also explained by differences in perceptual discriminability. That is, the perceptual discriminability, as postdicted by the model for the attribute and superordinate categorization tasks used in [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>] and [<xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>] respectively, tend to be higher than for the corresponding basic categorization tasks. In addition, the model correctly postdicted the presentation threshold for the ‘forest’ (basic) category (which appeared to be faster than most attributes) or the ‘transience’ (attribute) category (which was comparable in speed to several basic level categories)—two categories that would be considered as outliers under a level-of-categorization interpretation.</p>
<fig id="pcbi.1004456.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Re-drawing of <xref ref-type="fig" rid="pcbi.1004456.g004">Fig 4</xref> with labels indicating the taxonomic level of individual categorization tasks.</title>
<p>(A) Attribute-level categories are labeled in blue and basic-level categories in red. (B) Basic-level categories are labeled in blue and superordinate-level categories in red.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g005"/>
</fig>
<p>These initial results suggest that the superordinate advantage could simply reflect natural variations in discriminability between different target and distractor sets. To explicitly test this hypothesis, we used data by [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>] and found that the model postdicted a higher perceptual discriminability for their superordinate-level vs. basic-level categorization tasks and a lower discriminability for categorization between two basic categories that belong to the same superordinate class (e.g., both natural) compared to categorization between two basic categories that belong to different superordinate classes.</p>
<p>In [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>], participants were tested on different categorization tasks using a backwards masking paradigm. In a first experiment, one group of participants performed a superordinate-level categorization task while another group performed a basic level categorization task. Consistent with participants’ behavioral responses [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>], the model correctly postdicted a higher perceptual discriminability for superordinate vs. basic categorization as measured by the difference in sensitivity (<italic>A</italic>′) between superordinate and basic categorization (Human: <italic>M</italic> = 0.05±0.02; Model: <italic>M</italic> = 0.03±0.01) (see [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>] for details). In a second experiment, participants had to discriminate between two basic categories that either belonged to the same or different superordinate categories. Again, consistent with participants’ behavioral responses [<xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>], the model correctly postdicted a lower discriminability for categorization between two basic categories that belong to the same superordinate class (e.g., both natural) compared to categorization between two basic categories that belong to different superordinate classes. This effect was measured using the difference in sensitivity between the “same” task and the “different” task (Human: <italic>M</italic> = 0.14±0.06; Model: <italic>M</italic> = 0.05±0.01). Next, we demonstrate the contribution of perceptual discriminability to the superordinate advantage more directly by showing that it was possible to sample stimuli based on model-derived discriminability values to reverse the superordinate advantage—rendering a superordinate categorization task harder for human participants compared to a basic level categorization task.</p>
</sec>
<sec id="sec025">
<title>Experiment 2</title>
<p>We sampled stimuli from the SUN database using model discriminability values to yield either high discriminability for superordinate categorization but low discriminability for basic categorization to try to replicate the superordinate advantage (“superordinate advantage” condition) and a low discriminability for superordinate categorization and a high discriminability for basic categorization to try to reverse the superordinate advantage (“basic advantage” condition; <xref ref-type="fig" rid="pcbi.1004456.g006">Fig 6A</xref>). In each condition, participants had to perform both a superordinate (man-made vs. natural) and a basic categorization task (forest vs. natural). The only difference between the two conditions was the set of target and distractor stimuli used, which were both sampled from the SUN image dataset as in Experiment 1 (<xref ref-type="fig" rid="pcbi.1004456.g006">Fig 6B</xref>).</p>
<fig id="pcbi.1004456.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Experiment 2: Reversing the superordinate advantage.</title>
<p>(A). Model discriminability values were used to sample stimulus sets to yield a high discriminability for superordinate categorization and a low discriminability for basic categorization to try to replicate the superordinate advantage (“superordinate advantage” condition) as well as a low discriminability for superordinate categorization and a high discriminability for basic categorization to try to reverse the superordinate advantage (“basic advantage” condition). (B) Representative images used in the experiment. Note that the original stimuli could not be shown because of copyright issues. Instead, shown are visually similar images from Flickr with a Creative Common licence. (C) Experimental results: The model correctly predicted higher accuracy and lower mean RTs for the superordinate vs. basic categorization task in the superordinate advantage condition and the opposite trend in the basic advantage condition.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g006"/>
</fig>
<p>Both the man-made and the natural superordinate categories consisted of images from three basic categories (<xref ref-type="fig" rid="pcbi.1004456.g006">Fig 6B</xref>). However, across conditions, different basic categories were chosen. This was done by running a large number of model simulations. Overall, for the superordinate task, we used all possible combinations of 3 man-made basic categories against all possible combinations of 3 natural basic categories (<xref ref-type="fig" rid="pcbi.1004456.g007">Fig 7</xref>). We simulated the basic categorization task by categorizing the forest category against all combinations of three natural categories. For each condition, we then obtained categories that maximized the difference between the superordinate and the basic tasks (see <xref ref-type="sec" rid="sec002">Methods</xref> for details).</p>
<fig id="pcbi.1004456.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004456.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Reversal category picking framework.</title>
<p>We created many different image datasets to train and test the model on both a basic level categorization task (forest vs. natural stimuli) and a superordinate categorization task (man-made vs. natural stimuli). This was done by considering all possible combinations of 3 basic categories from a larger set of natural categories and all possible combinations of 3 basic categories from a larger set of man-made categories. We computed discriminability values for all the corresponding categorization tasks and chose natural and man-made combination sets of stimuli to create 2 experimental conditions: (1) A <italic>superordinate advantage</italic> condition for which the model predicted high perceptual discriminability for superordinate-level categorization but low discriminability for basic-level categorization (blue line). The combination set included ‘beach,’ ‘cultivated field,’ and ‘coral reef’ for natural categories and ‘alley,’ ‘street,’ and ‘skyscraper’ for man-made stimuli. (2) A <italic>basic advantage</italic> condition for which the model predicted the opposite trend (low perceptual discriminability for superordinate-level categorization and high discriminability for basic-level categorization, red line). The combination set included: ‘beach,’ ‘iceberg,’ and ‘desert’ for the natural category while the man-made category included ‘alley,’ ‘amphitheater,’ and ‘highway.’</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.g007"/>
</fig>
<p>As in Experiment 1, we used a GLMM to analyze participants’ sensitivity and mean RT for correct responses (<xref ref-type="fig" rid="pcbi.1004456.g006">Fig 6C</xref>). In the <italic>superordinate advantage</italic> condition, the average sensitivity was 2.31 (±0.19) for the superordinate task and 1.44 (±0.12) for the basic task. The within-subject difference in sensitivity was large and significant (<italic>β</italic><sub><italic>cont</italic></sub> = 0.87,<italic>CI</italic> = [0.64,1.11], <italic>p</italic> &lt; 10<sup>−4</sup>). Mean RTs were 356 ms (±6 ms) for the superordinate task and 364 ms (±6 ms) for the basic task. The within-subject difference in mean RT was significant as well (<italic>β</italic><sub><italic>cont</italic></sub> = 8.36,<italic>CI</italic> = [0.11,16.63], <italic>p</italic> = 0.050).</p>
<p>The opposite pattern was observed in the <italic>basic advantage</italic> condition. The average sensitivity was 1.85 (±0.13) in the superordinate task, and 2.28 (±0.12) in the basic task. The within-subject difference in sensitivity was smaller than the other group but still highly significant (<italic>β</italic><sub><italic>cont</italic></sub> = 0.43,<italic>CI</italic> = [0.24,0.63], <italic>p</italic> &lt; 10<sup>−4</sup>). The mean RT was 376 ms (±5 ms) for the superordinate task, and 357 ms (±6 ms) for the basic task. The within-subject difference in mean RT was again large and significant (<italic>β</italic><sub><italic>cont</italic></sub> = 19.14,<italic>CI</italic> = [12.01,26.5], <italic>p</italic> &lt; 10<sup>−4</sup>).</p>
</sec>
</sec>
</sec>
<sec id="sec026" sec-type="conclusions">
<title>Discussion</title>
<p>We have described an integrated paradigm that links perceptual processes with categorization processes. We used a large natural scene database to train and test machine learning classifiers in order to derive task-dependent perceptual discriminability values for individual images based on their distance to different categorization boundaries. We showed that the resulting model is consistent with a host of published results [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>]. In addition, we also designed two experiments to demonstrate that it is possible to use the model to sample stimuli in order to manipulate participants’ behavioral responses (both accuracy and reaction times).</p>
<p>In experiment 1, we showed that sampling stimuli with increasing discriminability values (i.e. with increasing distance to the category boundary) yields behavioral responses that are increasingly fast and accurate. This suggests that the perceptual discriminability of individual stimuli for a particular task is one of the main factors driving behavioral responses.</p>
<p>A few recent studies have hinted at the contribution of perceptual discriminability to categorization using isolated objects [<xref ref-type="bibr" rid="pcbi.1004456.ref037">37</xref>], objects in clutter [<xref ref-type="bibr" rid="pcbi.1004456.ref023">23</xref>] and scenes [<xref ref-type="bibr" rid="pcbi.1004456.ref038">38</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref039">39</xref>]. It has been shown that the perceptual dissimilarity between categories directly affects the speed of superordinate-level vs. basic-level categorization in pigeons [<xref ref-type="bibr" rid="pcbi.1004456.ref040">40</xref>]. Early work on scene and face processing already hinted at this contribution by showing, for instance, that the stimulus content across spatial scales affects scene categorization performance [<xref ref-type="bibr" rid="pcbi.1004456.ref041">41</xref>]. Subsequent work has also shown that the manipulation of the phase and amplitude spectra of an image affects behavioral responses during scene superordinate categorization [<xref ref-type="bibr" rid="pcbi.1004456.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref043">43</xref>]. More recently, it has been shown that a low-level perceptual similarity measure based on stimulus contrast predicts the ease of categorization judgments for both artificial stimuli [<xref ref-type="bibr" rid="pcbi.1004456.ref044">44</xref>] and natural scenes [<xref ref-type="bibr" rid="pcbi.1004456.ref045">45</xref>]. Our study further demonstrates that it is possible to use modern machine learning tools and computer vision databases to predict human behavioral responses for many categorization tasks across taxonomic levels.</p>
<p>In experiment 2, we further showed that it is possible to use the model to sample stimuli in order to reverse the “superordinate advantage” rendering participants’ superordinate categorization arbitrarily slower and less accurate than basic categorization. Previous work has shown that it is possible to manipulate level-of-categorization effects by controlling the similarity between face stimuli [<xref ref-type="bibr" rid="pcbi.1004456.ref046">46</xref>] and the typicality of objects [<xref ref-type="bibr" rid="pcbi.1004456.ref047">47</xref>]. Here, we used the model to sample stimuli based on computed discriminability values, possibly making a superordinate categorization task harder compared to a basic level categorization task simply by sampling the right stimuli.</p>
<p>Our results suggest that the superordinate advantage is at least in part driven by the perceptual discriminability of target and distractor stimulus sets. Simply put, superordinate-level categorization tasks tend to be easier than basic-level categorization tasks leading to observers’ behavioral responses that are faster and more accurate. This is consistent with the somewhat higher accuracy of both connectionist models [<xref ref-type="bibr" rid="pcbi.1004456.ref048">48</xref>] and modern computer vision systems for categorization at the superordinate vs. basic level [<xref ref-type="bibr" rid="pcbi.1004456.ref049">49</xref>] and is consistent with the fact that children learn to categorize natural object categories at the superordinate level first [<xref ref-type="bibr" rid="pcbi.1004456.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref050">50</xref>].</p>
<p>Our results are consistent with the differentiation theory [<xref ref-type="bibr" rid="pcbi.1004456.ref051">51</xref>] and the Parallel Distributed Processing (PDP) theory [<xref ref-type="bibr" rid="pcbi.1004456.ref052">52</xref>] in that level-of-categorization effects as reported in multiple studies [<xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1004456.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1004456.ref013">13</xref>] arise, not because of privileged processing at particular taxonomic levels, but because of differences in perceptual discriminability across tasks. In addition, this perceptual explanation rules out an interpretation of level-of-categorization effects based on the “global-to-specific” theory of categorization, whereby categorization at more global (coarser) categorization stages need to be completed before categorization at more specific (finer) levels can begin. Hence, one would expect a basic advantage over subordinate categorization (e.g., detection preceding identification [<xref ref-type="bibr" rid="pcbi.1004456.ref053">53</xref>]) as well as a superordinate and attribute advantage over basic and subordinate categorizations [<xref ref-type="bibr" rid="pcbi.1004456.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref011">11</xref>]. Our results demonstrate that observed differences in timing across categorization tasks do not necessarily reflect the fact that some categorization tasks take precedence over others (see also [<xref ref-type="bibr" rid="pcbi.1004456.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref020">20</xref>]).</p>
<p>While our results point to perceptual discriminability as playing a fundamental role in level-of-categorization effects, additional memory-related factors such as typicality are likely to affect rapid categorization. More generally, a complete model should also take into account known semantic contributions to visual categorization. One proposal is that mental representations of categories across taxonomic levels occupy nodes in a semantic network [<xref ref-type="bibr" rid="pcbi.1004456.ref054">54</xref>]. The rapid perceptual categorization mechanisms studied here may determine which nodes get activated first before activation spreads to other nodes enabling the slower retrieval of information at other levels of categorization [<xref ref-type="bibr" rid="pcbi.1004456.ref052">52</xref>].</p>
<p>The present study also has implications for models of category learning and models on the development of visual expertise. It is known that experts can override the supremacy of one level of categorization found in novices with their own level of expertise (e.g. the subordinate level becomes faster for bird experts that are over-trained at the subordinate level, the basic-level becomes faster for Chinese character experts that are over-trained in discriminating characters at the basic level (irrespective of font and writing style, see [<xref ref-type="bibr" rid="pcbi.1004456.ref022">22</xref>] for review). One simple explanation consistent with our results is that practice for a task leads to long-term perceptual learning that increases the discriminability between targets and distractors, making participants faster and more accurate.</p>
<p>Despite its ability to account for behavioral responses, the proposed model remains relatively simple. We used a rudimentary visual representation based on the “gist” algorithm [<xref ref-type="bibr" rid="pcbi.1004456.ref025">25</xref>] and off-the-shelf machine learning classifiers (see [<xref ref-type="bibr" rid="pcbi.1004456.ref039">39</xref>] for a similar model used to explain the scene categorization advantage when scenes contained consistent vs. inconsistent objects). However, the fact that a relatively simple (V1-like) model of feature computation, seems sufficient to account for behavioral responses does not necessary imply that rapid scene categorization is based on low-level visual processing. We have tested alternative visual representations based on common features used in computer vision and found all these models to be relatively correlated. This could possibly reflect a limitation inherent to the ever limited size of natural image databases [<xref ref-type="bibr" rid="pcbi.1004456.ref055">55</xref>] as well as possible inherent biases such as photographers selecting vantage points [<xref ref-type="bibr" rid="pcbi.1004456.ref056">56</xref>]. Note that such image bias is quite different from the “natural bias” reported here in terms of differences in perceptual discriminability across categorization tasks, which is likely to reflect physical properties of our visual environment as opposed to biases in the image dataset per se.</p>
<p>In addition, while the superordinate advantage has been described for other classes of stimuli beyond scenes such as animals [<xref ref-type="bibr" rid="pcbi.1004456.ref016">16</xref>] or faces [<xref ref-type="bibr" rid="pcbi.1004456.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004456.ref006">6</xref>], we have here only considered the relevance of the model for scene categorization. The use of a similar framework for other type of classifications would be likely to require more elaborated visual representations. In theory, it should be relatively straightforward to test additional perceptual representations—possibly reflecting higher level visual processes (see [<xref ref-type="bibr" rid="pcbi.1004456.ref023">23</xref>] for a review).</p>
<p>A possible neural correlate for decision boundaries includes neurons with category-like tuning found throughout the cortex such as within the ventral stream, the prefrontal cortex (PFC) and the parietal cortex [<xref ref-type="bibr" rid="pcbi.1004456.ref057">57</xref>] and/or attentional processes that would differentially modulate individual feature dimensions according to their task diagnosticity [<xref ref-type="bibr" rid="pcbi.1004456.ref058">58</xref>]. Perceptual spaces in practice tend to be more flexible than assumed in the model as novel features can be learned (i.e., the meaning of some of the dimensions may change and/or dimensions may be added as a result of learning and plasticity) and perceptual spaces can be reshaped by task history and other cognitive factors [<xref ref-type="bibr" rid="pcbi.1004456.ref059">59</xref>]. Alternative categorization algorithms to the proposed decision boundary have been described based on either the distance to category prototypes [<xref ref-type="bibr" rid="pcbi.1004456.ref060">60</xref>] or the distance to individual exemplars [<xref ref-type="bibr" rid="pcbi.1004456.ref061">61</xref>]. The proposed discriminability measures based on the distance between stimuli and decision boundaries could be easily extended to distances to exemplars or prototypes [<xref ref-type="bibr" rid="pcbi.1004456.ref062">62</xref>]. While it is expected that a better model of the categorization process should improve the fit to behavioral data, it is unlikely to change any of our conclusions, since categorization models tend to produce similar behavioral predictions.</p>
<p>Overall, our study provides a computational level explanation for systematic variations found in behavioral responses for rapid categorization tasks across taxonomic levels, challenging several existing theories of visual processing and suggesting, instead, that observed differences in behavioral responses may simply reflect natural variations in perceptual discriminability.</p>
</sec>
<sec id="sec027">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004456.s001" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.s001" mimetype="application/pdf" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supplementary materials and methods including details on the comparison between different types of visual descriptors and classifiers.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004456.s002" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.s002" mimetype="application/zip" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Supplementary file containing all image stimuli used and corresponding behavioral responses from human participants.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004456.s003" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.s003" mimetype="image/tiff" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Sketch of the gist visual representation used.</title>
<p>The response of a battery of filters at multiple orientations and spatial frequencies is first computed for an individual image. These filter responses are then spatially pooled to yield a 512-dimensional (gist) feature vector.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004456.s004" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004456.s004" mimetype="image/tiff" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Correlation between visual representations.</title>
<p>Simple visual representations like the gist tend to be relatively correlated with more complex ones including state-of-the-art visual descriptors from computer vision (see text for detail). This is true when correlating both the predicted class labels for individual train-test splits (A) and discriminability values computed across all train-test splits (B).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Dr. Michelle Greene (Stanford) and Prof. Thomas Palmeri (Vanderbilt University) for their useful feedback on the manuscript. We would also like to thank Sahar Shahamatdar for her contribution during the early stages of this work. Earlier versions of this work appeared as Abstract #36.401 presented at the 13th annual meeting of the Vision Science Society, 2013 (Understanding the nature of the visual representations underlying rapid categorization tasks by Imri Sofer, Kwang Ryeol Lee, Pachaya Sailamul, Sebastien Crouzet &amp; Thomas Serre) and Abstract #55.12 presented at the 14th annual meeting of the Vision Science Society, 2014 (A simple rapid categorization model accounts for variations in behavioral responses across rapid scene categorization tasks by Thomas Serre, Imri Sofer &amp; Sebastien Crouzet).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004456.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Biederman</surname> <given-names>I</given-names></name> (<year>1972</year>) <article-title>Perceiving real-world scenes</article-title>. <source>Science (80)</source> <volume>177</volume>: <fpage>77</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.177.4043.77" xlink:type="simple">10.1126/science.177.4043.77</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fleuret</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Dubout</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wampler</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Comparing machines and humans on a visual categorization test</article-title>. <source>Proc Natl Acad Sci</source> <volume>108</volume>: <fpage>17621</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1109168108" xlink:type="simple">10.1073/pnas.1109168108</ext-link></comment> <object-id pub-id-type="pmid">22006295</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tversky</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hemenway</surname> <given-names>K</given-names></name> (<year>1983</year>) <article-title>Categories of environmental scenes</article-title>. <source>Cogn Psychol</source> <volume>15</volume>(<issue>1</issue>): <fpage>121</fpage>–<lpage>149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0285(83)90006-3" xlink:type="simple">10.1016/0010-0285(83)90006-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Greene</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>The briefest of glances: the time course of natural scene understanding</article-title>. <source>Psychol Sci</source> <volume>20</volume>: <fpage>464</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9280.2009.02316.x" xlink:type="simple">10.1111/j.1467-9280.2009.02316.x</ext-link></comment> <object-id pub-id-type="pmid">19399976</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Grill-Spector</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name> (<year>2005</year>) <article-title>Visual Recognition As Soon as You know it is there, you know what it is</article-title>. <source>Psychol Sci</source> <volume>16</volume>: <fpage>152</fpage>–<lpage>160</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.0956-7976.2005.00796.x" xlink:type="simple">10.1111/j.0956-7976.2005.00796.x</ext-link></comment> <object-id pub-id-type="pmid">15686582</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barragan-Jason</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lachat</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Barbeau</surname> <given-names>EJ</given-names></name> (<year>2012</year>) <article-title>How Fast is Famous Face Recognition?</article-title> <source>Front Psychol</source> <volume>3</volume>: <fpage>454</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2012.00454" xlink:type="simple">10.3389/fpsyg.2012.00454</ext-link></comment> <object-id pub-id-type="pmid">23162503</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Joubert</surname> <given-names>OR</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>Ga</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name> (<year>2007</year>) <article-title>Processing scene context: fast categorization and object interference</article-title>. <source>Vision Res</source> <volume>47</volume>: <fpage>3286</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2007.09.013" xlink:type="simple">10.1016/j.visres.2007.09.013</ext-link></comment> <object-id pub-id-type="pmid">17967472</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bowers</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>KW</given-names></name> (<year>2008</year>) <article-title>Detecting objects is easier than categorizing them</article-title>. <source>Q J Exp Psychol</source> <volume>61</volume>: <fpage>552</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/17470210701798290" xlink:type="simple">10.1080/17470210701798290</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Loschky</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>Larson</surname> <given-names>AM</given-names></name> (<year>2010</year>) <article-title>The natural/man-made distinction is made before basic-level distinctions in scene gist processing</article-title>. <source>Vis cogn</source> <volume>18</volume>: <fpage>513</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/13506280902937606" xlink:type="simple">10.1080/13506280902937606</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mack</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Palmeri</surname> <given-names>TJ</given-names></name> (<year>2010</year>) <article-title>Decoupling object detection and categorization</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>36</volume>: <fpage>1067</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0020254" xlink:type="simple">10.1037/a0020254</ext-link></comment> <object-id pub-id-type="pmid">20731505</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kadar</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ben-Shahar</surname> <given-names>O</given-names></name> (<year>2012</year>) <article-title>A perceptual paradigm and psychophysical evidence for hierarchy in scene gist processing</article-title>. <source>J Vis</source> <volume>12</volume>(<issue>13</issue>):<fpage>16</fpage>, <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/12.13.16" xlink:type="simple">10.1167/12.13.16</ext-link></comment> <object-id pub-id-type="pmid">23255732</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Malle</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Holbrook</surname> <given-names>J</given-names></name> (<year>2012</year>) <article-title>Is there a hierarchy of social inferences? The likelihood and speed of inferring intentionality, mind, and personality</article-title>. <source>J Pers Soc Psychol</source> <volume>102</volume>: <fpage>661</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0026790" xlink:type="simple">10.1037/a0026790</ext-link></comment> <object-id pub-id-type="pmid">22309029</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Prass</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Grimsen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>König</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fahle</surname> <given-names>M</given-names></name> (<year>2013</year>) <article-title>Ultra rapid object categorization: effects of level, animacy and context</article-title>. <source>PLoS One</source> <volume>8</volume>: <fpage>e68051</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0068051" xlink:type="simple">10.1371/journal.pone.0068051</ext-link></comment> <object-id pub-id-type="pmid">23840810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gosselin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>P</given-names></name> (<year>2001</year>) <article-title>Why do we slip to the basic level? Computational constraints and their implementation</article-title>. <source>Psychol Rev</source> <volume>108</volume>(<issue>4</issue>): <fpage>735</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.108.4.735" xlink:type="simple">10.1037/0033-295X.108.4.735</ext-link></comment> <object-id pub-id-type="pmid">11699115</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Malcolm</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Nuthmann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name> (<year>2014</year>) <article-title>Beyond gist: strategic and incremental information accumulation for scene categorization</article-title>. <source>Psychol Sci</source> <volume>25</volume>: <fpage>1087</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797614522816" xlink:type="simple">10.1177/0956797614522816</ext-link></comment> <object-id pub-id-type="pmid">24604146</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Macé</surname> <given-names>MJM</given-names></name>, <name name-style="western"><surname>Joubert</surname> <given-names>OR</given-names></name>, <name name-style="western"><surname>Nespoulous</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name> (<year>2009</year>) <article-title>The time-course of visual categorizations: you spot the animal faster than the bird</article-title>. <source>PLoS One</source> <volume>4</volume>: <fpage>e5927</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0005927" xlink:type="simple">10.1371/journal.pone.0005927</ext-link></comment> <object-id pub-id-type="pmid">19536292</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name> (<year>2001</year>) <article-title>The entry point of face recognition: evidence for face expertise</article-title>. <source>J Exp Psychol Gen</source> <volume>130</volume>: <fpage>534</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.130.3.534" xlink:type="simple">10.1037/0096-3445.130.3.534</ext-link></comment> <object-id pub-id-type="pmid">11561926</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>D’Lauro</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Curran</surname> <given-names>T</given-names></name> (<year>2008</year>) <article-title>The preferred level of face categorization depends on discriminability</article-title>. <source>Psychon Bull Rev</source> <volume>15</volume>: <fpage>623</fpage>–<lpage>629</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/PBR.15.3.623" xlink:type="simple">10.3758/PBR.15.3.623</ext-link></comment> <object-id pub-id-type="pmid">18567265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mack</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Palmeri</surname> <given-names>TJ</given-names></name> (<year>2011</year>) <article-title>The Timing of Visual Object Categorization</article-title>. <source>Front Psychol</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2011.00165" xlink:type="simple">10.3389/fpsyg.2011.00165</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vanrullen</surname> <given-names>R</given-names></name> (<year>2011</year>) <article-title>Four common conceptual fallacies in mapping the time course of recognition</article-title>. <source>Front Psychol</source> <volume>2</volume>: <fpage>365</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2011.00365" xlink:type="simple">10.3389/fpsyg.2011.00365</ext-link></comment> <object-id pub-id-type="pmid">22162973</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schyns</surname> <given-names>P</given-names></name> (<year>1998</year>) <article-title>The development of features in object concepts</article-title>. <source>Behav Brain</source> <volume>21</volume>: <fpage>1</fpage>–<lpage>54</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Richler</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Palmeri</surname> <given-names>TJ</given-names></name> (<year>2014</year>) <article-title>Visual category learning</article-title>. <source>Wiley Interdiscip Rev Cogn Sci</source> <volume>5</volume>: <fpage>75</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/wcs.1268" xlink:type="simple">10.1002/wcs.1268</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Crouzet</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name> (<year>2011</year>) <article-title>What are the Visual Features Underlying Rapid Object Recognition?</article-title> <source>Front Psychol</source> <volume>2</volume>: <fpage>326</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2011.00326" xlink:type="simple">10.3389/fpsyg.2011.00326</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name> (<year>1992</year>) <source>Multidimensional models of perception and cognition</source>. <publisher-loc>Hillsdale, New Jersey</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates, Inc</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name> (<year>2001</year>) <article-title>Modeling the shape of the scene: A holistic representation of the spatial envelope</article-title>. <source>Int J Comput Vis</source> <volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1011139631724" xlink:type="simple">10.1023/A:1011139631724</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Xiao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hays</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ehinger</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname> <given-names>A</given-names></name> (<year>2010</year>) <source>SUN database: Large-scale scene recognition from abbey to zoo</source>. <publisher-loc>In</publisher-loc>: <publisher-name>Comput. Vis. Pattern Recognit</publisher-name>. pp. <fpage>3485</fpage>–<lpage>3492</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Maddox</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Prinzmetal</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Ivry</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name> (<year>1994</year>) <article-title>A probabilistic multidimensional model of location information</article-title>. <source>Psychol Res</source> <volume>56</volume>: <fpage>66</fpage>–<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00419713" xlink:type="simple">10.1007/BF00419713</ext-link></comment> <object-id pub-id-type="pmid">8153245</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nosofsky</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Palmeri</surname> <given-names>T</given-names></name> (<year>1997</year>) <article-title>Comparing exemplar-retrieval and decision-bound models of speed perceptual classification</article-title>. <source>Percept Psychophys</source> <volume>59</volume> (<issue>7</issue>): <fpage>1027</fpage>–<lpage>1048</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03205518" xlink:type="simple">10.3758/BF03205518</ext-link></comment> <object-id pub-id-type="pmid">9360476</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pedregosa</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Varoquaux</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Gramfort</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Michel</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Thirion</surname> <given-names>B</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Mach Learn Res</source> <volume>12</volume>: <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fan</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XR</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>CJ</given-names></name> (<year>2008</year>) <article-title>LIBLINEAR: A Library for Large Linear Classification</article-title>. <source>J Mach Learn Res</source> <volume>9</volume>: <fpage>1871</fpage>–<lpage>1874</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>. <volume>10</volume>(<issue>4</issue>): <fpage>433</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1163/156856897X00357" xlink:type="simple">10.1163/156856897X00357</ext-link></comment> <object-id pub-id-type="pmid">9176952</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Kleiner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>ZL</given-names></name> (<year>2010</year>) <article-title>RTbox: a device for highly accurate response time measurements</article-title>. <source>Behav Res Methods</source> <volume>42</volume>: <fpage>212</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BRM.42.1.212" xlink:type="simple">10.3758/BRM.42.1.212</ext-link></comment> <object-id pub-id-type="pmid">20160301</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name> (<year>2010</year>) <article-title>Bayesian data analysis</article-title>. <source>Wiley Interdiscip Rev Cogn Sci</source> <volume>1</volume>: <fpage>658</fpage>–<lpage>676</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/wcs.72" xlink:type="simple">10.1002/wcs.72</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>DeCarlo</surname> <given-names>LT</given-names></name> (<year>1998</year>) <article-title>Signal detection theory and generalized linear models</article-title>. <source>Psychol Methods</source> <volume>3</volume>: <fpage>186</fpage>–<lpage>205</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/1082-989X.3.2.186" xlink:type="simple">10.1037/1082-989X.3.2.186</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Moscatelli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mezzetti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lacquaniti</surname> <given-names>F</given-names></name> (<year>2012</year>) <article-title>Modeling psychophysical data at the population-level: the generalized linear mixed model</article-title>. <source>J Vis</source> <volume>12</volume>(<issue>11</issue>): <fpage>26</fpage>, <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/12.11.26" xlink:type="simple">10.1167/12.11.26</ext-link></comment> <object-id pub-id-type="pmid">23104819</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>J</given-names></name> (<year>2007</year>) <source>Data analysis using regression and multilevel/hierarchical models</source>. <fpage>1</fpage>–<lpage>651</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mohan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Arun</surname> <given-names>SP</given-names></name> (<year>2012</year>) <article-title>Similarity relations in visual search predict rapid visual categorization</article-title>. <source>J Vis</source> <volume>12</volume>: <fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/12.11.19" xlink:type="simple">10.1167/12.11.19</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Renninger</surname> <given-names>LW</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>J</given-names></name> (<year>2004</year>) <article-title>When is scene identification just texture recognition?</article-title> <source>Vision Res</source> <volume>44</volume>: <fpage>2301</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2004.04.006" xlink:type="simple">10.1016/j.visres.2004.04.006</ext-link></comment> <object-id pub-id-type="pmid">15208015</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mack</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Palmeri</surname> <given-names>TJ</given-names></name> (<year>2010</year>) <article-title>Modeling categorization of scenes containing consistent versus inconsistent objects</article-title>. <source>J Vis</source> <volume>10</volume>(<issue>11</issue>): <fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/10.3.11" xlink:type="simple">10.1167/10.3.11</ext-link></comment> <object-id pub-id-type="pmid">20377288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lazareva</surname> <given-names>OF</given-names></name>, <name name-style="western"><surname>Soto</surname> <given-names>Fa</given-names></name>, <name name-style="western"><surname>Wasserman</surname> <given-names>Ea</given-names></name> (<year>2010</year>) <article-title>Effect of between-category similarity on basic level superiority in pigeons</article-title>. <source>Behav Processes</source> <volume>85</volume>: <fpage>236</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.beproc.2010.06.014" xlink:type="simple">10.1016/j.beproc.2010.06.014</ext-link></comment> <object-id pub-id-type="pmid">20600696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schyns</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name> (<year>1997</year>) <article-title>Flexible, diagnosticity-driven, rather than fixed, perceptually determined scale selection in scene and face recognition</article-title>. <source>Perception</source> <volume>26</volume>: <fpage>1027</fpage>–<lpage>1038</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1068/p261027" xlink:type="simple">10.1068/p261027</ext-link></comment> <object-id pub-id-type="pmid">9509161</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gaspar</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name> (<year>2009</year>) <article-title>How do amplitude spectra influence rapid animal detection?</article-title> <source>Vision Res</source> <volume>49</volume>: <fpage>3001</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2009.09.021" xlink:type="simple">10.1016/j.visres.2009.09.021</ext-link></comment> <object-id pub-id-type="pmid">19818804</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Joubert</surname> <given-names>OR</given-names></name>, <name name-style="western"><surname>Rousselet</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name> (<year>2009</year>) <article-title>Rapid visual categorization of natural scene contexts with equalized amplitude spectrum and increasing phase noise</article-title>. <source>J Vis</source> <volume>9</volume>(<issue>2</issue>): <fpage>1</fpage>–<lpage>16</lpage>. <object-id pub-id-type="pmid">19271872</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name> (<year>2012</year>) <article-title>Spatially pooled contrast responses predict neural and perceptual similarity of naturalistic image categories</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>: <fpage>e1002726</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002726" xlink:type="simple">10.1371/journal.pcbi.1002726</ext-link></comment> <object-id pub-id-type="pmid">23093921</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Groen</surname> <given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Prins</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname> <given-names>HS</given-names></name> (<year>2013</year>) <article-title>From image statistics to scene gist: evoked neural activity reveals transition from low-level natural image structure to scene category</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>18814</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3128-13.2013" xlink:type="simple">10.1523/JNEUROSCI.3128-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24285888</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>D’Lauro</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Curran</surname> <given-names>T</given-names></name> (<year>2008</year>) <article-title>The preferred level of face categorization depends on discriminability</article-title>. <source>Psychon Bull Rev</source> <volume>15</volume>: <fpage>623</fpage>–<lpage>629</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/PBR.15.3.623" xlink:type="simple">10.3758/PBR.15.3.623</ext-link></comment> <object-id pub-id-type="pmid">18567265</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Murphy</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Brownell</surname> <given-names>HH</given-names></name> (<year>1985</year>) <article-title>Category differentiation in object recognition: typicality constraints on the basic category advantage</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>11</volume>: <fpage>70</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0278-7393.11.1.70" xlink:type="simple">10.1037/0278-7393.11.1.70</ext-link></comment> <object-id pub-id-type="pmid">3156953</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Quinn</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MH</given-names></name> (<year>2000</year>) <article-title>Global-Before-Basic Object Categorization in Connectionist Networks and 2-Month-Old Infants</article-title>. <source>Infancy</source> <volume>1</volume>: <fpage>31</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/S15327078IN0101_04" xlink:type="simple">10.1207/S15327078IN0101_04</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Berg</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name> (<year>2010</year>) <chapter-title>What does classifying more than 10,000 image categories tell us?</chapter-title> In: <source>Proc. 11th Eur. Conf. Comput. Vis</source>. <publisher-name>Springer-Verlag</publisher-name>, pp. <fpage>71</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mandler</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>McDonough</surname> <given-names>L</given-names></name> (<year>1993</year>) <article-title>Concept formation in infancy</article-title>. <source>Cogn Dev</source> <volume>8</volume>: <fpage>291</fpage>–<lpage>318</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0885-2014(93)80003-C" xlink:type="simple">10.1016/S0885-2014(93)80003-C</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Murphy</surname> <given-names>G</given-names></name> (<year>2002</year>) <source>The big book of concepts</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004456.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rogers</surname> <given-names>TT</given-names></name>, <name name-style="western"><surname>Patterson</surname> <given-names>K</given-names></name> (<year>2007</year>) <article-title>Object categorization: reversals and explanations of the basic-level advantage</article-title>. <source>J Exp Psychol Gen</source> <volume>136</volume>: <fpage>451</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.136.3.451" xlink:type="simple">10.1037/0096-3445.136.3.451</ext-link></comment> <object-id pub-id-type="pmid">17696693</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Liu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name> (<year>2002</year>) <article-title>Stages of processing in face perception: an MEG study</article-title>. <source>Nat Neurosci</source> <volume>5</volume>: <fpage>910</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn909" xlink:type="simple">10.1038/nn909</ext-link></comment> <object-id pub-id-type="pmid">12195430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jolicoeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gluck</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Kosslyn</surname> <given-names>SM</given-names></name> (<year>1984</year>) <article-title>Pictures and names: Making the connection</article-title>. <source>Cogn Psychol</source> <volume>16</volume>(<issue>2</issue>): <fpage>243</fpage>–<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0010-0285(84)90009-4" xlink:type="simple">10.1016/0010-0285(84)90009-4</ext-link></comment> <object-id pub-id-type="pmid">6734136</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Pinto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Doukhan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name> (<year>2009</year>) <article-title>A high-throughput screening approach to discovering good forms of biologically inspired visual representation</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000579</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000579" xlink:type="simple">10.1371/journal.pcbi.1000579</ext-link></comment> <object-id pub-id-type="pmid">19956750</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Drewes</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rosas</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gegenfurtner</surname> <given-names>KR</given-names></name> (<year>2010</year>) <article-title>Animal detection in natural scenes: Critical features revisited</article-title>. <source>J Vis</source> <volume>10</volume>: <fpage>1</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/10.4.6" xlink:type="simple">10.1167/10.4.6</ext-link></comment> <object-id pub-id-type="pmid">20465326</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Seger</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name> (<year>2010</year>) <article-title>Category learning in the brain</article-title>. <source>Annu Rev Neurosci</source> <volume>33</volume>: <fpage>203</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.051508.135546" xlink:type="simple">10.1146/annurev.neuro.051508.135546</ext-link></comment> <object-id pub-id-type="pmid">20572771</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Çukur</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Nishimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Huth</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name> (<year>2013</year>) <article-title>Attention during natural vision warps semantic representation across the human brain</article-title>. <source>Nat Neurosci</source> <volume>16</volume>: <fpage>763</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3381" xlink:type="simple">10.1038/nn.3381</ext-link></comment> <object-id pub-id-type="pmid">23603707</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Schyns</surname> <given-names>PG</given-names></name> (<year>1997</year>) <article-title>Categories and percepts: a bi-directionnal framework for categorization</article-title>. <source>Trends Cogn Sci</source> <volume>1</volume>: <fpage>183</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1364-6613(97)01056-5" xlink:type="simple">10.1016/S1364-6613(97)01056-5</ext-link></comment> <object-id pub-id-type="pmid">21223900</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Posner</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Keele</surname> <given-names>SW</given-names></name> (<year>1968</year>) <article-title>On the genesis of abstract ideas</article-title>. <source>J Exp Psych</source> <volume>77</volume>(<issue>3</issue>): <fpage>353</fpage>–<lpage>363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0025953" xlink:type="simple">10.1037/h0025953</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nosofsky</surname> <given-names>RM</given-names></name> (<year>1986</year>) <article-title>Attention, similarity, and the identification categorization relationship</article-title>. <source>J Exp Psychol Gen</source> <volume>115</volume>: <fpage>39</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.115.1.39" xlink:type="simple">10.1037/0096-3445.115.1.39</ext-link></comment> <object-id pub-id-type="pmid">2937873</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004456.ref062">
<label>62</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jäkel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name> (<year>2008</year>) <article-title>Generalization and similarity in exemplar models of categorization: Insights from machine learning</article-title>. <source>Psychon Bull Rev</source> <volume>15</volume>: <fpage>256</fpage>–<lpage>271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/PBR.15.2.256" xlink:type="simple">10.3758/PBR.15.2.256</ext-link></comment> <object-id pub-id-type="pmid">18488638</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>