<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
	<front>
		<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="flc">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
				<publisher-name>Public Library of Science</publisher-name>
				<publisher-loc>San Francisco, USA</publisher-loc>
			</publisher></journal-meta>
		<article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0020118</article-id><article-id pub-id-type="publisher-id">06-PLCB-RA-0047R4</article-id><article-id pub-id-type="sici">plcb-02-09-05</article-id><article-categories>
				<subj-group subj-group-type="heading">
					<subject>Research Article</subject>
				</subj-group>
				<subj-group subj-group-type="Discipline">
					<subject>Computational Biology</subject>
					<subject>Molecular Biology</subject>
					<subject>Computational Biology/Systems Biology</subject>
				</subj-group>
				<subj-group subj-group-type="System Taxonomy">
					<subject>Mammals</subject>
					<subject>None</subject>
				</subj-group>
			</article-categories><title-group><article-title>Imitating Manual Curation of Text-Mined Facts in Biomedicine</article-title><alt-title alt-title-type="running-head">AI Curation in Biomedicine</alt-title></title-group><contrib-group>
				<contrib contrib-type="author" xlink:type="simple">
					<name name-style="western">
						<surname>Rodriguez-Esteban</surname>
						<given-names>Raul</given-names>
					</name>
					<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
					<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
				</contrib>
				<contrib contrib-type="author" xlink:type="simple">
					<name name-style="western">
						<surname>Iossifov</surname>
						<given-names>Ivan</given-names>
					</name>
					<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
					<xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
				</contrib>
				<contrib contrib-type="author" xlink:type="simple">
					<name name-style="western">
						<surname>Rzhetsky</surname>
						<given-names>Andrey</given-names>
					</name>
					<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
					<xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
					<xref ref-type="aff" rid="aff4"><sup>4</sup></xref>
					<xref ref-type="aff" rid="aff5"><sup>5</sup></xref>
					<xref ref-type="corresp" rid="cor1"><sup>*</sup></xref>
				</contrib>
			</contrib-group><aff id="aff1">
				<label>1</label><addr-line> Department of Electrical Engineering, Columbia University, New York, New York, United States of America
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line> Center for Computational Biology and Bioinformatics, Joint Centers for Systems Biology, Columbia University, New York, New York, United States of America
			</addr-line></aff><aff id="aff3">
				<label>3</label><addr-line> Department of Biomedical Informatics, Columbia University Medical Center, Columbia University, New York, New York, United States of America
			</addr-line></aff><aff id="aff4">
				<label>4</label><addr-line> Judith P. Sulzberger, MD Columbia Genome Center, Columbia University Medical Center, Columbia University, New York, New York, United States of America
			</addr-line></aff><aff id="aff5">
				<label>5</label><addr-line> Department of Biological Sciences, Columbia University, New York, New York, United States of America
			</addr-line></aff><contrib-group>
				<contrib contrib-type="editor" xlink:type="simple">
					<name name-style="western">
						<surname>Valencia</surname>
						<given-names>Alfonso</given-names>
					</name>
					<role>Editor</role><xref ref-type="aff" rid="edit1"/>
				</contrib>
			</contrib-group><aff id="edit1">Centro Nacional de Biotecnologia, Spain</aff><author-notes>
			<fn fn-type="con" id="ack1">
				<p>RRE and AR conceived and designed the experiments. RRE performed the experiments. RRE, II, and AR analyzed the data. II contributed reagents/materials/analysis tools. AR wrote the paper.</p>
			</fn>				<corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">ar345@columbia.edu</email></corresp>
			<fn fn-type="conflict" id="ack3">
				<p> The authors have declared that no competing interests exist.</p>
			</fn></author-notes><pub-date pub-type="ppub">
				<month>9</month>
				<year>2006</year>
			</pub-date><pub-date pub-type="epub">
				<day>8</day>
				<month>9</month>
				<year>2006</year>
			</pub-date><pub-date pub-type="epreprint">
				<day>27</day>
				<month>7</month>
				<year>2006</year>
			</pub-date><volume>2</volume><issue>9</issue><elocation-id>e118</elocation-id><history>
				<date date-type="received">
					<day>10</day>
					<month>2</month>
					<year>2006</year>
				</date>
				<date date-type="accepted">
					<day>27</day>
					<month>7</month>
					<year>2006</year>
				</date>
			</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2006</copyright-year><copyright-holder>Rodriguez-Esteban et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
				<p>Text-mining algorithms make mistakes in extracting facts from natural-language texts. In biomedical applications, which rely on use of text-mined data, it is critical to assess the quality (the probability that the message is correctly extracted) of individual facts—to resolve data conflicts and inconsistencies. Using a large set of almost 100,000 manually produced evaluations (most facts were independently reviewed more than once, producing independent evaluations), we implemented and tested a collection of algorithms that mimic human evaluation of facts provided by an automated information-extraction system. The performance of our best automated classifiers closely approached that of our human evaluators (ROC score close to 0.95). Our hypothesis is that, were we to use a larger number of human experts to evaluate any given sentence, we could implement an artificial-intelligence curator that would perform the classification job at least as accurately as an average individual human evaluator. We illustrated our analysis by visualizing the predicted accuracy of the text-mined relations involving the term <italic>cocaine</italic>.</p>
			</abstract><abstract abstract-type="synopsis">
				<title>Synopsis</title>
				<p>Current automated approaches for extracting biologically important facts from scientific articles are imperfect: while being capable of efficient, fast, and inexpensive analysis of enormous quantities of scientific prose, they make errors. To emulate the human experts evaluating the quality of the automatically extracted facts, we have developed an artificial intelligence program (“a robotic curator”) that closely approaches human experts in the quality of distinguishing the correctly extracted facts from the incorrectly extracted ones.</p>
			</abstract><funding-group><funding-statement>This study was supported by grants from the National Institutes of Health (GM61372 and U54 CA121852-01A1), the National Science Foundation (supplement to EIA-0121687), the Cure Autism Now Foundation, and the Defense Advanced Research Projects Agency (FA8750-04-2–0123).</funding-statement></funding-group><counts>
				<page-count count="14"/>
			</counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group>
				<custom-meta>
					<meta-name>citation</meta-name>
					<meta-value>Rodriguez-Esteban R, Iossifov I, Rzhetsky A (2006) Imitating manual curation of text-mined facts in biomedicine. PLoS Comput Biol 2(9): e118. DOI: <ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0020118" xlink:type="simple">10.1371/journal.pcbi.0020118</ext-link></meta-value>
				</custom-meta>
			</custom-meta-group></article-meta>
	</front>
	<body>
		<sec id="s1">
			<title>Introduction</title>
			<disp-quote>
				<p>… he will throughly purge his floor, and gather his wheat into the garner; but he will burn up the chaff with unquenchable fire. —Matthew 3:12 [<xref ref-type="bibr" rid="pcbi-0020118-b001">1</xref>]</p>
			</disp-quote>
			<p><named-content content-type="genus-species" xlink:type="simple">Information extraction</named-content> uses computer-aided methods to recover and structure meaning that is locked in natural-language texts. The assertions uncovered in this way are amenable to computational processing that approximates human reasoning. In the special case of biomedical applications, the texts are represented by books and research articles, and the extracted meaning comprises diverse classes of facts, such as relations between molecules, cells, anatomical structures, and maladies.</p>
			<p>Unfortunately, the current tools of information extraction produce imperfect, noisy results. Although even imperfect results are useful, it is highly desirable for most applications to have the ability to rank the text-derived facts by the confidence in the quality of their extraction (as we did for relations involving <italic>cocaine</italic>, see <xref ref-type="fig" rid="pcbi-0020118-g001">Figure 1</xref>). We focus on automatically extracted statements about molecular interactions, such as <italic>small molecule A binds protein B</italic>, <italic>protein B activates gene C</italic>, or <italic>protein D phosphorylates small molecule E</italic>. (In the following description we refer to phrases that represent biological entities—such as <italic>small molecule A</italic>, <italic>protein B</italic>, and <italic>gene C—</italic>as <italic>terms</italic>, and to biological relations between these entities—such as <italic>activate</italic> or <italic>phosphorylate—</italic>as <italic>relations</italic> or <italic>verbs</italic>.)</p>
			<fig id="pcbi-0020118-g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g001</object-id>
				<label>Figure 1</label>
				<caption>
					<title><italic>Cocaine: </italic>The Predicted Accuracy of Individual Text-Mined Facts Involving Semantic Relation <italic>Stimulate</italic></title>
					<p>Each directed arc from an entity <italic>A</italic> to an entity <italic>B</italic> in this figure should be interpreted as a statement <italic>“A stimulates B”,</italic> where, for example, <italic>A</italic> is <italic>cocaine</italic> and <italic>B</italic> is <italic>progesterone</italic>. The predicted accuracy of individual statements is indicated both in color and in width of the corresponding arc. Note that, for example, the relation between <italic>cocaine</italic> and <italic>progesterone</italic> was derived from multiple sentences, and different instances of extraction output had markedly different accuracy. Altogether we collected 3,910 individual facts involving <italic>cocaine</italic>. Because the same fact can be repeated in different sentences, only 1,820 facts out of 3,910 were unique. The facts cover 80 distinct semantic relations, out of which <italic>stimulate</italic> is just one example.</p>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g001" xlink:type="simple"/>
			</fig>
			<p>Several earlier studies have examined aspects of evaluating the quality of text-mined facts. For example, Sekimizu et al. and Ono et al. attempted to attribute different confidence values to different verbs that are associated with extracted relations, such as <italic>activate, regulate,</italic> and <italic>inhibit</italic> [<xref ref-type="bibr" rid="pcbi-0020118-b002">2</xref>,<xref ref-type="bibr" rid="pcbi-0020118-b003">3</xref>]. Thomas et al. proposed to attach a quality value to each extracted statement about molecular interactions [<xref ref-type="bibr" rid="pcbi-0020118-b004">4</xref>], although the researchers did not implement the suggested scoring system in practice. In an independent study [<xref ref-type="bibr" rid="pcbi-0020118-b005">5</xref>], Blaschke and Valencia used word-distances between biological terms in a given sentence as an indicator of the precision of extracted facts. In our present analysis we applied several machine-learning techniques to a large training set of 98,679 manually evaluated examples (pairs of extracted facts and corresponding sentences) to design a tool that mimics the work of a human curator who manually cleans the output of an information-extraction program.</p>
			<sec id="s1a">
				<title>Approach</title>
				<p>Our goal is to design a tool that can be used with any information-extraction system developed for molecular biology. In this study, our training data came from the GeneWays project (specifically, GeneWays 6.0 database [<xref ref-type="bibr" rid="pcbi-0020118-b006">6</xref>,<xref ref-type="bibr" rid="pcbi-0020118-b007">7</xref>]), and thus our approach is biased toward relationships that are captured by that specific system (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 1</xref>). We believe that the spectrum of relationships represented in the GeneWays ontology is sufficiently broad that our results will prove useful for other information-extraction projects.</p>
				<p>Our approach followed the path of supervised machine-learning. First, we generated a large training set of facts that were originally gathered by our information-extraction system, and then manually labeled as “correct” or “incorrect” by a team of human curators. Second, we used a battery of machine-learning tools to imitate computationally the work of the human evaluators. Third, we split the training set into ten parts, so that we could evaluate the significance of performance differences among the several competing machine-learning approaches.</p>
			</sec>
		</sec>
		<sec id="s2">
			<title>Methods</title>
			<sec id="s2a">
				<title>Training data</title>
				<p>With the help of a text-annotation company, ForScience, we generated a training set of approximately 45,000 multiply annotated unique facts, or almost 100,000 independent evaluations. These facts were originally extracted by the GeneWays pipeline, then were annotated by biology-savvy doctoral-level curators as “correct” or “incorrect,” referring to quality of information extraction. Examples of automatically extracted relations, sentences corresponding to each relation, and the labels provided by three evaluators are shown in <xref ref-type="table" rid="pcbi-0020118-t001">Table 1</xref>.</p>
				<table-wrap content-type="2col" id="pcbi-0020118-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t001</object-id><label>Table 1</label><caption>
						<p>A Sample of Sentences That Were Used as an Input to Automated Information Extraction, Biological Relations Extracted from These Sentences, and the Corresponding Evaluations</p>
					</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t001" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb1col1" align="left" charoff="0" char=""/><col id="tb1col2" align="left" charoff="0" char=""/><col id="tb1col3" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left"><hr/>Sentence &lsqb;Source&rsqb;</td><td><hr/>Extracted Relation</td><td><hr/>Evaluation (Confidence)</td></tr></thead><tbody><tr><td><bold>NIK</bold> <italic>binds</italic> to <bold>Nck</bold> in cultured cells &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b022">22</xref>&rsqb;</td><td>nik <italic>bind</italic> nck</td><td>Correct (high)</td></tr><tr><td>One is that <bold>presenilin</bold> is <italic>required for</italic> the proper trafficking of <bold>Notch</bold> and APP to their proteases, which may reside in an intracellular compartment &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b023">23</xref>&rsqb;</td><td>presenilin <italic>required for</italic> notch</td><td>Correct (high)</td></tr><tr><td>Serine 732 <italic>phosphorylation</italic> of <bold>FAK</bold> by <bold>Cdk5</bold> is important for microtubule organization, nuclear movement, and neuronal migration &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b024">24</xref>&rsqb;</td><td>cdk5 <italic>phosphorylate</italic> fak</td><td>Correct (high)</td></tr><tr><td>Histogram quantifying the percent of <bold>Arr2</bold> <italic>bound</italic> to <bold>rhodopsin</bold>-containing membranes after treatment with blue light (B) or blue light followed by orange light (BO) &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b025">25</xref>&rsqb;</td><td>arr2 <italic>bind</italic> rhodopsin</td><td>Correct (low)</td></tr><tr><td>It is now generally accepted that a shift from monomer to dimer and <bold>cadherin</bold> clustering <italic>activates</italic> classic <bold>cadherins</bold> at the surface into an adhesively competent conformation &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b026">26</xref>&rsqb;</td><td>cadherin <italic>activate</italic> cadherins</td><td>Correct (low)</td></tr><tr><td><italic>Binding</italic> of G to <bold>CSP</bold> was four times greater than binding to <bold>syntaxin</bold> &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b027">27</xref>&rsqb;</td><td>csp <italic>bind</italic> syntaxin</td><td>Incorrect (low)</td></tr><tr><td>Treatment with NEM applied with <bold>cGMP</bold> made <italic>activation</italic> by <bold>cAMP</bold> more favorable by about 2.5 kcal/mol &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b028">28</xref>&rsqb;</td><td>camp <italic>activate</italic> cgmp</td><td>Incorrect (low)</td></tr><tr><td>This matrix is likely to consist of <bold>actin</bold> filaments, as similar filaments can be <italic>induced</italic> by <bold>actin</bold>-stabilizing toxins (O. S. et al., unpublished data) &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b029">29</xref>&rsqb;</td><td>actin <italic>induce</italic> actin</td><td>Incorrect (high)</td></tr><tr><td>A ligand-gated <italic>association</italic> between <bold>cytoplasmic domains</bold> of <bold>UNC5</bold> and DCC family receptors converts netrin-induced growth cone attraction to repulsion &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b030">30</xref>&rsqb;</td><td>cytoplasmic domains <italic>associate</italic> unc5</td><td>Incorrect (high)</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt101"><p>Column 1, sample sentences input to automated information extraction.</p></fn><fn id="nt102"><p>Column 2, relations extracted either correctly or incorrectly.</p></fn><fn id="nt103"><p>Column 3, evaluations by three human experts.</p></fn><fn id="nt104"><p>A high-confidence label corresponds to a perfect agreement among all experts; a low-confidence label indicates that one of the experts disagreed with the other two. Clearly, automated information extraction can be associated with a loss of detail of meaning, as in the case of the <italic>cadherin activates cadherins</italic> example (row 5).</p></fn></table-wrap-foot> --></table-wrap>
				<p>Each extracted fact was evaluated by one, two, or three different curators. The complete evaluation set comprised 98,679 individual evaluations performed by four different people, so most of the statement–sentence pairs were evaluated multiple times, with each person evaluating a given pair at most once. In total, 13,502 statement–sentence pairs were evaluated by just one person, 10,457 by two people, 21,421 by three people, and 57 by all four people. Examples of both high inter-annotator agreement and low-agreement sentences are shown in <xref ref-type="table" rid="pcbi-0020118-t001">Table 1</xref>.</p>
				<p>The statements in the training dataset were grouped into chunks; each chunk was associated with a specific biological project, such as analysis of interactions in <named-content content-type="genus-species" xlink:type="simple">Drosophila melanogaster</named-content>. Pairwise agreement between evaluators was high (92%) in most chunks (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 2</xref>), with the exception of a chunk of 5,271 relations where agreement was only 74%. These relatively low-agreement evaluations were not included in the training data for our analysis (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 3</xref>).</p>
				<p>To facilitate evaluation, we developed a Sentence Evaluation Tool implemented in Java programming language by Mitzi Morris and Ivan Iossifov. This tool presented to an evaluator a set of annotation choices regarding each extracted fact; the choices are listed in <xref ref-type="table" rid="pcbi-0020118-t002">Table 2</xref>. The tool also presented in a single window the fact itself and the sentence it was derived from. In the case that a broader context was required for the judgement, the evaluator had a choice to retrieve the complete journal article containing this sentence by clicking a single button on the program interface.</p>
				<table-wrap content-type="1col" id="pcbi-0020118-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t002</object-id><label>Table 2</label><caption>
						<p>List of Annotation Choices Available to the Evaluators</p>
					</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t002" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb2col1" align="left" charoff="0" char=""/><col id="tb2col2" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left"><hr/>Level of Annotation</td><td><hr/>Choices</td></tr></thead><tbody><tr><td><bold>Term level</bold></td><td>Upstream term is a junk substance</td></tr><tr><td></td><td>Action is incorrect biologically</td></tr><tr><td></td><td>Downstream term is a junk substance</td></tr><tr><td><bold>Relation level</bold></td><td>Correctly extracted</td></tr><tr><td></td><td>Sentence is hypothesis, not fact</td></tr><tr><td></td><td>Unable to decide</td></tr><tr><td></td><td>Incorrectly extracted</td></tr><tr><td></td><td>Incorrect upstream</td></tr><tr><td></td><td>Incorrect downstream</td></tr><tr><td></td><td>Incorrect action type</td></tr><tr><td></td><td>Missing or extra negation</td></tr><tr><td></td><td>Wrong action direction</td></tr><tr><td></td><td>Sentence does not support the action</td></tr><tr><td><bold>Sentence level</bold></td><td>Wrong sentence boundary</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt201"><p>The term &ldquo;action&rdquo; refers to the type of the extracted relation. For example, in the statement <italic>A binds B,</italic> &ldquo;binds&rdquo; is the <italic>action</italic>, &ldquo;A&rdquo; is the <italic>upstream term,</italic> and &ldquo;B&rdquo; is the <italic>downstream term</italic>. Action direction is defined as <italic>upstream to downstream,</italic> and &ldquo;junk substance&rdquo; is an obviously incorrectly identified term/entity.</p></fn></table-wrap-foot> --></table-wrap>
				<p>For the convenience of representing the results of manual evaluation, we computed an evaluation score for each statement as follows. Each sentence–statement score was computed as a sum of the scores assigned by individual evaluators; for each evaluator, −1 was added if the expert believed that the presented information was extracted incorrectly, and +1 was added if he or she believed that the extraction was correct. For a set of three experts, this method permitted four possible scores: 3(1,1,1), 1(1,1,−1), −1(1,−1, −1), and −3. Similarly, for just two experts, the possible scores are 2(1,1), 0(1,−1), and −2(−1,−1) (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 4</xref>).</p>
			</sec>
			<sec id="s2b">
				<title>Computational Methods</title>
				<sec id="s2b1">
					<title>Machine-learning algorithms: General framework.</title>
					<p>The objects that we want to classify, the fact–sentence pairs, have complex properties. We want to place each of the objects into one of two classes, <italic>correct</italic> or <italic>incorrect</italic>. In the training data, each extracted fact is matched to a unique sentence from which it was extracted, even though multiple sentences can express the same fact and a single sentence can contain multiple facts. The <italic>i</italic><sup>th</sup> object (the <italic>i</italic><sup>th</sup> fact–sentence pair) comes with a set of known features or properties that we encode into a feature vector, <bold>F</bold><italic><sub>i</sub></italic>:
						<disp-formula id="pcbi-0020118-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
					<p>In the following description we use <italic>C</italic> to indicate the random variable that represents class (with possible values <italic>c<sub>correct</sub></italic> and <italic>c<sub>incorrect</sub></italic>), and <italic>F</italic> to represent a 1 × <italic>n</italic> random vector of feature values (also often called <italic>attributes</italic>), such that <italic>F<sub>j</sub></italic> is the <italic>j</italic><sup>th</sup> element of <italic>F</italic>. For example, for fact <italic>p53 activates JAK,</italic> feature <italic>F</italic><sub>1</sub> would have value 1 because the upstream term <italic>p53</italic> is found in a dictionary derived from the GenBank database [<xref ref-type="bibr" rid="pcbi-0020118-b008">8</xref>]; otherwise, it would have value 0.</p>
				</sec>
				<sec id="s2b2">
					<title>Full Bayesian inference.</title>
					<p>The full Bayesian classifier assigns the <italic>i</italic><sup>th</sup> object to the <italic>k</italic><sup>th</sup> class if the posterior probability <italic>P</italic>(<italic>C</italic> = <italic>c<sub>k</sub></italic> | <italic>F</italic> = <bold>F</bold><italic><sub>i</sub></italic>) is greater for the <italic>k</italic><sup>th</sup> class than for any alternative class. This posterior probability is computed in the following way (a restated version of Bayes' theorem).
						<disp-formula id="pcbi-0020118-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&times;</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
					<p>In the real-life applications, we estimate probability <italic>P</italic>(<italic>F</italic> = <bold>F</bold><italic><sub>i</sub></italic> | <italic>C</italic> = <italic>c<sub>k</sub></italic>) from the training data as a ratio of the number of objects that belong to the class <italic>c<sub>k</sub> and</italic> have the same set of feature values as specified by the vector <bold>F</bold><italic><sub>i</sub></italic> to the total number of objects in class <italic>c<sub>k</sub></italic> in the training data.</p>
					<p>In other words, we estimate the conditional probability for every possible value of the feature vector <italic>F</italic> for every value of class <italic>C</italic>. Assuming that all features can be discretized, we have to estimate
						<disp-formula id="pcbi-0020118-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&times;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&times;</mml:mo><mml:mo>&hellip;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&times;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math> --></disp-formula>parameters, where <italic>v<sub>i</sub></italic> is the number of discrete values observed for the <italic>i</italic><sup>th</sup> feature and <italic>m</italic> is the number of classes.
					</p>
					<p>Clearly, even for a space of only 20 binary features (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 5</xref>), the number of parameters that we would need to estimate is (2<sup>20</sup> −1) × 2 = 2,097,150, which exceeds several times the number of datapoints in our training set.</p>
				</sec>
				<sec id="s2b3">
					<title>Naïve Bayes classifier.</title>
					<p>The most affordable approximation to the full Bayesian analysis is the Naïve Bayes classifier. It is based on the assumption of conditional independence of features:
						<disp-formula id="pcbi-0020118-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020118-e005"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e005" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:malignmark/><mml:mo>&times;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&hellip;</mml:mo></mml:mrow></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020118-e006"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e006" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:malignmark/><mml:mo>&times;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
					<p>Obviously, we can estimate <italic>P</italic>(<italic>F<sub>j</sub></italic> = <italic>f<sub>i,j</sub></italic> | <italic>C</italic> = <italic>c<sub>k</sub></italic>)s reasonably well with a relatively small set of training data, but the assumption of conditional independence (<xref ref-type="disp-formula" rid="pcbi-0020118-e004">Equation 4</xref>) comes at a price: the Naïve Bayes classifier is usually markedly less successful in its job than are its more sophisticated relatives (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 6</xref>).</p>
					<p>In an application with <italic>m</italic> classes and <italic>n</italic> features (given that the <italic>i</italic><sup>th</sup> feature has <italic>v<sub>i</sub></italic> admissible discrete values), a Naïve Bayes algorithm requires estimation of <italic>m</italic> × Σ<italic><sub>i=1</sub></italic><sub>,<italic>n</italic></sub> (<italic>v<sub>i</sub></italic> − 1) parameters (which value, in our case, is equal to 4,208).</p>
				</sec>
			</sec>
			<sec id="s2c">
				<title>Middle Ground between the Full and Naïve Bayes: Clustered Bayes</title>
				<p>We can find an intermediate ground between the full and Naïve Bayes classifiers by assuming that features in the random vector <italic>F</italic> are arranged into groups or clusters, such that all features within the same cluster are dependent on one another (conditionally on the class), and all features from different classes are conditionally independent. That is, we can assume that the feature random vector (<italic>F</italic>) and the observed feature vector for the <italic>i</italic><sup>th</sup> object (<bold>F</bold><italic><sub>i</sub></italic>) can be partitioned into subvectors:
					<disp-formula id="pcbi-0020118-e007"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e007" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>F</mml:mi><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>&Phi;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&Phi;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&Phi;</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>and</mml:mtext></mml:mrow></mml:math> --></disp-formula>
					<disp-formula id="pcbi-0020118-e008"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e008" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>respectively, where Φ<italic><sub>j</sub></italic> is the <italic>j</italic><sup>th</sup> cluster of features; <bold>f</bold><italic><sub>i,j</sub></italic> is the set of values for this cluster with respect to the <italic>i</italic><sup>th</sup> object, and <italic>M</italic> is the total number of clusters of features.
				</p>
				<p>The Clustered Bayes classifier is based on the following assumption about conditional independence of <italic>clusters</italic> of features:
					<disp-formula id="pcbi-0020118-e009"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e009" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&Phi;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math> --></disp-formula>
					<disp-formula id="pcbi-0020118-e010"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e010" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:malignmark/><mml:mo>&times;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&Phi;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&hellip;</mml:mo></mml:mrow></mml:math> --></disp-formula>
					<disp-formula id="pcbi-0020118-e011"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e011" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:malignmark/><mml:mo>&times;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&Phi;</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
				</p>
				<p>We tested two versions of the Clustered Bayes classifier: one version used all 68 features (Clustered Bayes 68) with a coarser discretization of feature values; another version used a subset of 44 features (Clustered Bayes 44) but allowed for more discrete values for each continuous-valued feature, see legend to <xref ref-type="fig" rid="pcbi-0020118-g002">Figure 2</xref>.</p>
				<fig id="pcbi-0020118-g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g002</object-id>
					<label>Figure 2</label>
					<caption>
						<title>The Correlation Matrix for the Features Used by the Classification Algorithms</title>
						<p>The half-matrix below the diagonal was derived from analysis of the whole GeneWays 6.0 database; the half-matrix above the diagonal represents a correlation matrix estimated from only the manually annotated dataset. The white dotted lines outline clusters of features, suggested by analysis of the annotated dataset; we used these clusters in implementation of the Clustered Bayes classifier. We used two versions of the Clustered Bayes classifier: with all 68 features (Clustered Bayes 68), and with a subset of only 44 features but a higher number of discrete values allowed for nonbinary features (Clustered Bayes 44). The Clustered Bayes 44 classifier did not use features 1, 6, 7, 8, 9, 12, 27, 28, 31, 34, 37, 40, 42, 47, 48, 49, 52, 54, 55, 60, 62, 63, and 65.</p>
					</caption>
					<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g002" xlink:type="simple"/>
				</fig>
			</sec>
			<sec id="s2d">
				<title>Linear and Quadratic Discriminants</title>
				<p>Another method that can be viewed as an approximation to full Bayesian analysis is Discriminant Analysis invented by Sir Ronald A. Fisher [<xref ref-type="bibr" rid="pcbi-0020118-b009">9</xref>]. This method requires no assumption about conditional independence of features; instead, it assumes that the conditional probability <italic>P</italic> (<italic>F</italic> = <bold>F</bold><italic><sub>i</sub></italic> | <italic>C</italic> = <italic>c<sub>k</sub></italic>) is a multivariate normal distribution.
					<disp-formula id="pcbi-0020118-e012"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e012" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mstyle scriptlevel='+1'><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&mu;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>&prime;</mml:mo></mml:mrow><mml:msubsup><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>V</mml:mi></mml:mstyle><mml:mi>k</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&mu;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>&pi;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>V</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where <italic>n</italic> is the total number of features/variables in the class-specific multivariate distributions. The method has two variations. The first, <italic>Linear Discriminant Analysis,</italic> assumes that different classes have different mean values for features (vectors <italic>μ<sub>k</sub></italic>), but the same variance–covariance matrix, <bold>V</bold> = <bold>V</bold><italic><sub>k</sub></italic> for all <italic>k</italic> (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 7</xref>). In the second variation, <italic>Quadratic Discriminant Analysis</italic> (QDA), the assumption of the common variance–covariance matrix for all classes, is relaxed, such that every class is assumed to have a distinct variance–covariance matrix, <bold>V</bold><italic><sub>k</sub></italic> (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 8</xref>).
				</p>
				<p>In this study we present results for QDA; the difference from the linear discriminant analysis was insignificant for our data (unpublished data). In terms of the number of parameters to estimate, QDA uses only two symmetrical class-specific covariance matrices and the two class-specific mean vectors. For 68 features the method requires estimation of 2 × (68 × 69)/2 + 2 × 68 = 4,828 parameters.</p>
				<sec id="s2d1">
					<title>Maximum-entropy method.</title>
					<p>The current version of the maximum-entropy method was formulated by E. T. Jaynes [<xref ref-type="bibr" rid="pcbi-0020118-b010">10</xref>,<xref ref-type="bibr" rid="pcbi-0020118-b011">11</xref>]; the method can be traced to earlier work by J. Willard Gibbs. The idea behind the approach is as follows. Imagine that we need to estimate a probability distribution from an incomplete or small dataset—this problem is the same as that of estimating the probability of the class given the feature vector <italic>P</italic> (<italic>C</italic> = <italic>c<sub>k</sub></italic> | <italic>F</italic> = <bold>F</bold><italic><sub>i</sub></italic>), from a relatively small training set. Although we have no hope of estimating the distribution completely, we can estimate with sufficient reliability the first (and, potentially, the second) moments of the distribution. Then, we can try to find a probability distribution that has the same moments as our unknown distribution and the highest possible Shannon's entropy—the intuition behind this approach being that the maximum-entropy distribution will minimize unnecessary assumptions about the unknown distribution. The maximum-entropy distribution with constraints imposed by the first-order feature moments alone (the mean values of features) is known to have the form of an exponential distribution [<xref ref-type="bibr" rid="pcbi-0020118-b012">12</xref>]:
						<disp-formula id="pcbi-0020118-e013"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e013" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&lambda;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle='true'><mml:msubsup><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&lambda;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>and the maximum-entropy distribution for the case when both the first-order and the second-order moments of the unknown distribution are fixed has the form of a multidimensional normal distribution [<xref ref-type="bibr" rid="pcbi-0020118-b012">12</xref>]. The conditional distribution that we are trying to estimate can be written in the following exponential form:
						<disp-formula id="pcbi-0020118-e014"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e014" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mo>&equals;</mml:mo><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>F</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&lambda;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&equals;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&nu;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle='true'><mml:msubsup><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&lambda;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&equals;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&nu;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math> --></disp-formula>
					</p>
					<p>Parameters <italic>λ<sub>i,k</sub></italic> and <italic>v<sub>x,y,k</sub></italic> are <italic>k</italic>-class–specific weights of individual features and feature pairs, respectively, and in principle can be expressed in terms of the first and second moments of the distributions. The values of parameters in <xref ref-type="disp-formula" rid="pcbi-0020118-e009">Equations 9</xref>and <xref ref-type="disp-formula" rid="pcbi-0020118-e010">10</xref> are estimated by maximizing the product of probabilities for the individual training examples.</p>
					<p>We tested two versions of the maximum-entropy classifier. MaxEnt 1 uses only information about the first moments of features in the training data (<xref ref-type="disp-formula" rid="pcbi-0020118-e009">Equation 9</xref>); MaxEnt 2 uses the set of all individual features and the products of feature pairs (<xref ref-type="disp-formula" rid="pcbi-0020118-e010">Equation 10</xref>). To select the most informative pairs of features, we used a mutual information approach, as described in the subsection dealing with classification features.</p>
					<p>For two classes <italic>(correct</italic> and <italic>incorrect)</italic> and 68 features, MaxEnt 1 requires estimation of 136 parameters. In contrast, MaxEnt 2 requires estimation of 4,828 parameters: weight parameters for all first moments for two classes, plus weights for the second moments for two classes. MaxEnt 2-v is a version of MaxEnt 2 classifier where the squared values of features are not used, so that the classifier requires estimation of only 4,692 weight parameters.</p>
				</sec>
			</sec>
			<sec id="s2e">
				<title>Feed-Forward Neural Network</title>
				<p>A typical feed-forward artificial neural network is a directed acyclic graph organized into three (or more) layers. In our case, we chose a three-layered network, with a set of nodes of the <italic>input layer</italic>, {<italic>x<sub>i</sub></italic>}<italic><sub>i=</sub></italic><sub>1,…,<italic>N<sub>x</sub></italic></sub>, nodes of the <italic>hidden layer</italic>, {<italic>y<sub>j</sub></italic>}<italic><sub>j=</sub></italic><sub>1,…,<italic>N<sub>y</sub></italic></sub>, and a single node representing the <italic>output layer</italic>, <italic>z</italic><sub>1</sub>, see <xref ref-type="fig" rid="pcbi-0020118-g003">Figure 3</xref>. The number of input nodes, <italic>N<sub>x,</sub></italic> is determined by the number of features used in the analysis (68 in our case). The number of hidden nodes, <italic>N<sub>y,</sub></italic> determines both the network's expressive power and its ability to generalize. Too small a number of hidden nodes makes a simplistic network that cannot learn from complex data. Too large a number makes a network that tends to overtrain—that works perfectly on the training data, but poorly on new data. We experimented with different values of <italic>N<sub>y</sub></italic> and settled on <italic>N<sub>y</sub></italic> = 10.</p>
				<fig id="pcbi-0020118-g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g003</object-id>
					<label>Figure 3</label>
					<caption>
						<title>A Hypothetical Three-Layered Feed-Forward Neural Network</title>
						<p>We used a similar network with 68 input units (one unit per classification feature) and ten hidden-layer units.</p>
					</caption>
					<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g003" xlink:type="simple"/>
				</fig>
				<p>The values of the input nodes, {<italic>x<sub>i</sub></italic>}<italic><sub>i=</sub></italic><sub>1,…,N<sub><italic>x</italic></sub>,</sub> are feature values of the object that we need to classify. The value of each node, <italic>y<sub>j</sub>,</italic> in the hidden layer is determined in the following way:
					<disp-formula id="pcbi-0020118-e015"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e015" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where <italic>F</italic>(<italic>x</italic>) is a hyperbolic tangent function that creates an S-shaped curve:
					<disp-formula id="pcbi-0020118-e016"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e016" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msup><mml:mo>&minus;</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msup><mml:mo>&plus;</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>and {<italic>w<sub>j,k</sub></italic>} are weight parameters. Finally, the value of the output node <italic>z</italic><sub>1</sub> is determined as a linear combination of the values of all hidden nodes:
					<disp-formula id="pcbi-0020118-e017"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e017" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where {<italic>a<sub>k</sub></italic>} are additional weight parameters. We trained our network, using a back-propagation algorithm [<xref ref-type="bibr" rid="pcbi-0020118-b013">13</xref>], to distinguish two classes, <italic>correct</italic> and <italic>incorrect,</italic> where positive values of <italic>z</italic><sub>1</sub> corresponded to the class <italic>correct</italic>. The feed-forward neural network that we used in our analysis can be thought of as a model with <italic>N<sub>x</sub></italic> × <italic>N<sub>y</sub></italic> + <italic>N<sub>y</sub></italic> parameters (690 in our case).
				</p>
			</sec>
			<sec id="s2f">
				<title>Support Vector Machines</title>
				<p>The Support Vector Machines (SVM, [<xref ref-type="bibr" rid="pcbi-0020118-b014">14</xref>,<xref ref-type="bibr" rid="pcbi-0020118-b015">15</xref>]) algorithm solves a binary classification problem by dividing two sets of data geometrically, by finding a hyperplane that separates the two classes of objects in the training data in an optimum way (maximizing the margin between the two classes).</p>
				<p>The SVM is a <italic>kernel</italic>-based algorithm, where the kernel is an inner product of two feature vectors (function/transformation of the original data). In this study, we used three of the most popular kernels: the linear, polynomial, and Rbf (radial basis function) kernels. The linear kernel <italic>K</italic><sup>L</sup> (<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>) = 〈<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>〉 is simply the inner product of the two input feature vectors; an SVM with the linear kernel searches for a class-separating hyperplane in the original space of the data. Using a polynomial kernel, <inline-formula id="pcbi-0020118-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex001" xlink:type="simple"/></inline-formula>
					, is equivalent to transforming the data into a higher-dimensional space and searching for a separating plane there (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 9</xref>). Finally, using an Rbf kernel, <inline-formula id="pcbi-0020118-ex002"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex002" xlink:type="simple"/></inline-formula>
					, corresponds to finding a separating hyperplane in an infinite-dimensional space.
				</p>
				<p>In the most real-world cases the two classes cannot be separated perfectly by a hyperplane, and some classification errors are unavoidable. SVM algorithms use the <italic>C</italic>-parameter to control the error rate during the training phase (if the error is not constrained, the margin of every hyperplane can be extended infinitely). In this study, we used the default values for the <italic>C</italic>-parameter suggested by the SVM Light tool. <xref ref-type="table" rid="pcbi-0020118-t003">Table 3</xref> lists the SVM models and <italic>C</italic>-parameter values that we used in this study.</p>
				<table-wrap content-type="1col" id="pcbi-0020118-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t003</object-id><label>Table 3</label><caption>
						<p>Parameter Values Used for Various SVM Classifiers in This Study</p>
					</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t003" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb3col1" align="left" charoff="0" char=""/><col id="tb3col2" align="left" charoff="0" char=""/><col id="tb3col3" align="left" charoff="0" char=""/><col id="tb3col4" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left"><hr/>Model</td><td><hr/>Kernel</td><td><hr/>Kernel Parameter</td><td><hr/><italic>C</italic>-Parameter</td></tr></thead><tbody><tr><td><italic>SVM</italic> (OSU SVM)</td><td>Linear</td><td></td><td>1</td></tr><tr><td><italic>SVM-t0</italic> (SVM Light)</td><td>Linear</td><td></td><td>1</td></tr><tr><td><italic>SVM-t1-d2</italic></td><td>Polynomial</td><td><italic>d</italic> &equals; 2</td><td>0.3333</td></tr><tr><td><italic>SVM-t1-d3</italic></td><td>Polynomial</td><td><italic>d</italic> &equals; 3</td><td>0.1429</td></tr><tr><td><italic>SVM-t2-g0.5</italic></td><td>Rbf</td><td><italic>g</italic> &equals; 0.5</td><td>1.2707</td></tr><tr><td><italic>SVM-t2-g1</italic></td><td>Rbf</td><td><italic>g</italic> &equals; 1</td><td>0.7910</td></tr><tr><td><italic>SVM-t2-g2</italic></td><td>Rbf</td><td><italic>g</italic> &equals; 2</td><td>0.5783</td></tr></tbody></table> --><!-- --></table-wrap>
				<p>The output of an SVM analysis is not probabilistic, but there are tools to convert an SVM classification output into “posterior probabilities” (see chapter by J. Platt in [<xref ref-type="bibr" rid="pcbi-0020118-b016">16</xref>]). (A similar comment is applicable to the artificial neural network.)</p>
				<p>The number of support vectors used by the SVM classifier depends on the size and properties of the training dataset. The average number of (1 × 68-dimensional) support vectors used in ten cross-validation experiments was 12,757.5, 11,994.4, 12,092, 12,289.9, 12,679.7, and 14,163.8, for SVM, SVM-t1-d2, SVM-t1-d3, SVM-t2-g0.5, SVM-t2-g1, and SVM-t2-g2 classifiers, respectively. The total number of data-derived values (which we loosely call “parameters”) used by the SVM in our cross-validation experiments was therefore, on average, between 827,614 and 880,270 for various SVM versions.</p>
				<sec id="s2f1">
					<title>Meta-method.</title>
					<p>We implemented the meta-classifier on the basis of the SVM algorithm (linear kernel with <italic>C =</italic> 1) applied to predictions (converted into probabilities that the object belongs to the class <italic>correct</italic>) provided by the individual “simple” classifiers. The meta-method used 1,445 support vectors (1 × 7-dimensional), in addition to combined parameters of the seven individual classifiers used as input to the meta-classifier.</p>
				</sec>
				<sec id="s2f2">
					<title>Implementation.</title>
					<p>A summary of the sources of software used in our study is shown in <xref ref-type="table" rid="pcbi-0020118-t004">Table 4</xref>.</p>
					<table-wrap content-type="1col" id="pcbi-0020118-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t004</object-id><label>Table 4</label><caption>
							<p>Machine Learning Methods Used in This Study and Their Implementations</p>
						</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t004" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb4col1" align="left" charoff="0" char=""/><col id="tb4col2" align="left" charoff="0" char=""/><col id="tb4col3" align="left" charoff="0" char=""/><col id="tb4col4" align="char" charoff="0" char="."/></colgroup><thead><tr><td align="left"><hr/>Method</td><td><hr/>Implementation</td><td><hr/>URL</td><td><hr/>Number of Parameters</td></tr></thead><tbody><tr><td>Na&iuml;ve Bayes</td><td>This study, WEKA</td><td><ext-link ext-link-type="uri" xlink:href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</ext-link></td><td>4,208</td></tr><tr><td>Clustered Bayes 68</td><td>This study</td><td>N/A</td><td>276,432</td></tr><tr><td>Clustered Bayes 44</td><td>This study</td><td>N/A</td><td>361,270</td></tr><tr><td>Discriminant analysis</td><td>This study</td><td>N/A</td><td>4,828</td></tr><tr><td>SVM</td><td>OSU SVM toolbox for Matlab</td><td><ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/svm">http://sourceforge.net/projects/svm</ext-link></td><td>827,614</td></tr><tr><td>SVM-t<sup>a</sup></td><td>SVM light &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b031">31</xref>&rsqb;</td><td><ext-link ext-link-type="uri" xlink:href="http://svmlight.joachims.org/">http://svmlight.joachims.org/</ext-link></td><td align="left">&thinsp;827,614 to 880,270</td></tr><tr><td>Neural network</td><td>Neural network toolbox for Matlab</td><td>N/A</td><td>690</td></tr><tr><td>MaxEnt 1</td><td>Maximum entropy modeling toolkit for Python and C&plus;&plus;</td><td><ext-link ext-link-type="uri" xlink:href="http://homepages.inf.ed.ac.uk">http://homepages.inf.ed.ac.uk</ext-link> /s0450736/maxent_toolkit.html</td><td>136</td></tr><tr><td>MaxEnt 2</td><td>Same as the MaxEnt 1</td><td>same as the MaxEnt 1</td><td>4,828</td></tr><tr><td>MaxEnt 2-v</td><td>Same as the MaxEnt 1</td><td>same as the MaxEnt 1</td><td>4,692</td></tr><tr><td>Meta-classifier</td><td>OSU SVM toolbox for Matlab</td><td><ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/svm">http://sourceforge.net/projects/svm</ext-link></td><td>&gt;11,560</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt401"><p><sup>a</sup>Wildcard that can stand for t2-g0.5 and several other variations.</p></fn></table-wrap-foot> --></table-wrap>
				</sec>
			</sec>
			<sec id="s2g">
				<title>Features Used in Our Analysis</title>
				<p>We selected 68 individual features covering a range of characteristics that could help in the classification (see <xref ref-type="table" rid="pcbi-0020118-t005">Table 5</xref>). To capture the flow of information in a molecular interaction graph (the edge direction) in each extracted relation we identified an “upstream term” (corresponding to the graph node with the outgoing directed edge) and a “downstream term” (the node with the incoming directed edge): for example, in the phrase <italic>“JAK</italic> phosphorylates <italic>p53</italic>,” <italic>JAK</italic> is the upstream term, and <italic>p53</italic> is the downstream term. Features in the group <italic>keywords</italic> represent a list of tokens that may signal that the sentence is hypothetical, interrogative, negative, or that there is a confusion in the relation extraction (e.g., the particle “by” in passive-voice sentences). We eventually abandoned <italic>keywords</italic> as we found them to be uninformative features, but they are still listed for the sake of completeness.</p>
				<table-wrap content-type="2col" id="pcbi-0020118-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t005</object-id><label>Table 5</label><caption>
						<p>List of the Features That We Used in the Present Study</p>
					</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t005" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb5col1" align="left" charoff="0" char=""/><col id="tb5col2" align="left" charoff="0" char=""/><col id="tb5col3" align="left" charoff="0" char=""/><col id="tb5col4" align="char" charoff="0" char="."/></colgroup><thead><tr><td align="left"><hr/>Group of Features</td><td><hr/>Feature(s)</td><td><hr/>Values</td><td><hr/>Number of Features</td></tr></thead><tbody><tr><td><bold>Dictionary look-ups</bold></td><td>&lcub;Upstream, downstream&rcub; term can be found in &lcub;GeneBank, NCBI taxonomy, LocusLink, SwissProt, FlyBase, drug list, disease list, Specialist Lexicon, Bacteria, English Dictionary&rcub;</td><td>Binary</td><td>20</td></tr><tr><td><bold>Word metrics</bold></td><td>Length of the sentence (word count)</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Distance between the upstream and the downstream term</td><td>Integer</td><td>1</td></tr><tr><td></td><td>Minimum non-negative word distance between the upstream and the downstream term</td><td>Non-negative Integer</td><td>1</td></tr><tr><td></td><td>Distance between the upstream term and the action</td><td>Integer</td><td>1</td></tr><tr><td></td><td>Distance between the downstream term and the action</td><td>Integer</td><td>1</td></tr><tr><td><bold>Previous scores</bold></td><td>Average score of relationships with the same &lcub;upstream term, downstream term, action&rcub;</td><td>Real</td><td>3</td></tr><tr><td></td><td>Count of evaluated relationships with the same &lcub;upstream term, downstream term, action&rcub;</td><td>Positive integer</td><td>3</td></tr><tr><td></td><td>Total count of relationships with the same &lcub;upstream term, downstream term, action&rcub;</td><td>Positive integer</td><td>3</td></tr><tr><td></td><td>Average score of relationships that share the same pair of upstream and downstream terms</td><td>Real</td><td>1</td></tr><tr><td></td><td>Total count of evaluated relationships that share the same pair of upstream and downstream terms</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Total count of relationships with both the same upstream and downstream terms</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Number of relations extracted from the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Number of evaluated relations extracted from the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Average score of relations from the same sentence</td><td>Real</td><td>1</td></tr><tr><td></td><td>Number of relations sharing upstream term in same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Number of evaluated relations sharing upstream term in the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Average score of relations sharing upstream term in same sentence</td><td>Real</td><td>1</td></tr><tr><td></td><td>Relations sharing downstream term in the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Evaluated relations sharing downstream term in the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Average score of relations sharing downstream term in the same sentence</td><td>Real</td><td>1</td></tr><tr><td></td><td>Number of relations sharing same action in the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Number of evaluated relations sharing action in the same sentence</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Average score of relations sharing action in the same sentence</td><td>Real</td><td>1</td></tr><tr><td><bold>Punctuation</bold></td><td>Number of &lcub;periods, commas, semicolons, colons&rcub; in the sentence</td><td>Non-negative integer</td><td>4</td></tr><tr><td></td><td>Number of &lcub;periods, commas, semicolons, colons&rcub; between upstream and downstream terms</td><td>Non-negative integer</td><td>4</td></tr><tr><td><bold>Terms</bold></td><td>Semantic subclass category of the &lcub;upstream, downstream&rcub; term</td><td>Integer</td><td>2</td></tr><tr><td></td><td>Probability that the &lcub;upstream, downstream&rcub; term has been correctly recognized</td><td>Real</td><td>2</td></tr><tr><td></td><td>Probability that the &lcub;upstream, downstream&rcub; term has been correctly mapped</td><td>Real</td><td>2</td></tr><tr><td><bold>Part-of-speech tags</bold></td><td>&lcub;Upstream, downstream&rcub; term is a noun phrase</td><td>Binary</td><td>2</td></tr><tr><td></td><td>Action is a verb</td><td>Binary</td><td>1</td></tr><tr><td><bold>Other</bold></td><td>Relationship is negative</td><td>Binary</td><td>1</td></tr><tr><td></td><td>Action index</td><td>Positive integer</td><td>1</td></tr><tr><td></td><td>Keyword is present</td><td>Binary</td><td align="left">(Not used)</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt501"><p><named-content content-type="genus-species">Dictionary lookups</named-content> are binary features indicating absence or presence of a term in a specific dictionary.</p></fn><fn id="nt502"><p><named-content content-type="genus-species">Previous scores</named-content> are the average scores that a term or an action has in other relations evaluated. <italic>Term-recognition probabilities</italic> are generated by the GeneWays pipeline and reflect the likelihood that a term had been correctly recognized and mapped.</p></fn><fn id="nt503"><p><italic>Sharing of the same action</italic> (verb) by two different facts within the same sentence occurs in phrases such as <italic>A and B were shown to phosphorylate C</italic>. In this example, two individual relations, <italic>A phosphorylates C</italic> and <italic>B phosphorylates C,</italic> share the same verb, <italic>phosphorylate</italic>.</p></fn><fn id="nt504"><p><named-content content-type="genus-species">Semantic categories</named-content> are entities (semantic classes) in the GeneWays ontology (e.g., <italic>gene, protein, geneorprotein</italic>).</p></fn><fn id="nt505"><p><italic>Part-of-speech tags</italic> were generated by the Maximum Entropy tagger, MXPOST &lsqb;<xref ref-type="bibr" rid="pcbi-0020118-b032">32</xref>&rsqb;.</p></fn></table-wrap-foot> --></table-wrap>
				<p>To represent the second-order features (pairs of features), we defined a new feature as a product of the normalized values of two features. We obtained the normalized values of features by subtracting the mean value from each feature value, then dividing the result by the standard deviation for this feature.</p>
				<p>After a number of feature-selection experiments for the MaxEnt 2 method, we settled on using <italic>all</italic> second-order features.</p>
			</sec>
			<sec id="s2h">
				<title>Separating Data into Training and Testing: Cross-Validation</title>
				<p>To evaluate the success of our classifiers we used a 10-fold cross-validation approach, where we used <inline-formula id="pcbi-0020118-ex003"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex003" xlink:type="simple"/></inline-formula>
					 of data for training and <inline-formula id="pcbi-0020118-ex004"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex004" xlink:type="simple"/></inline-formula>
					 for testing. More precisely, given a partition of the manually evaluated data into ten equal portions, we created ten different pairs of training–test subsets, so that ten distinct testing sets put together covered the whole collection of the manually evaluated sentences. We then used ten training–test-set pairs to compare all algorithms.
				</p>
			</sec>
			<sec id="s2i">
				<title>Comparison of Methods: ROC Scores</title>
				<p>To quantify and compare success of the various classification methods, we used receiver operating characteristic (ROC) scores, also called <italic>areas under ROC curve</italic> [<xref ref-type="bibr" rid="pcbi-0020118-b017">17</xref>].</p>
				<p>An ROC score is computed in the following way. All test-set predictions of a particular classification method are ordered by the decreasing quality score provided by this method; for example, in the case of the Clustered Bayes algorithm, the quality score is the posterior probability that the test object belongs to the class <italic>correct</italic>. The ranked list is then converted into binary predictions by applying a decision threshold, <italic>T:</italic> all test objects with a quality score above <italic>T</italic> are classified as <italic>correct,</italic> and all test objects with lower-than-threshold scores are classified as <italic>incorrect</italic>. The ROC score is then computed by plotting the proportion of true-positive predictions (in the test set we know both the correct label and the quality score of each object) against false-positive predictions for the whole spectrum of possible values of <italic>T,</italic> then integrating the area under the curve obtained in this way, see <xref ref-type="fig" rid="pcbi-0020118-g004">Figure 4</xref>.</p>
				<fig id="pcbi-0020118-g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g004</object-id>
					<label>Figure 4</label>
					<caption>
						<title>ROC Curves for the Classification Methods Used in the Present Study</title>
						<p>We show only the linear-kernel SVM and the Clustered Bayes 44 ROC curves to avoid excessive data clutter.</p>
					</caption>
					<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g004" xlink:type="simple"/>
				</fig>
				<p>The ROC score is an estimate of the probability that the classifier under scrutiny will label correctly a pair of statements, one of which is from the class <italic>correct</italic> and one from the class <italic>incorrect</italic> [<xref ref-type="bibr" rid="pcbi-0020118-b017">17</xref>]. A completely random classifier therefore would have an ROC score of 0.5, whereas a hypothetical perfect classifier would have an ROC score of 1. It is also possible to design a classifier that performs less accurately than would one that is completely random; in this case the ROC score is less than 0.5, which indicates that we can improve the accuracy of the classifier by simply reversing all predictions.</p>
			</sec>
		</sec>
		<sec id="s3">
			<title>Results</title>
			<p>The raw extracted facts produced by our system are noisy. Although many relation types are extracted with accuracy above 80%, and even above 90% (see <xref ref-type="fig" rid="pcbi-0020118-g005">Figure 5</xref>), there are particularly noisy verbs/relations that bring the average accuracy of the “raw” data to about 65% (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 10</xref>). Therefore, additional purification of text-mining output, either computational or manual, is indeed important.</p>
			<fig id="pcbi-0020118-g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g005</object-id>
				<label>Figure 5</label>
				<caption>
					<title>Accuracy of the Raw (Noncurated) Extracted Relations in the GeneWays 6.0 Database</title>
					<p>The accuracy was computed by averaging over all individual specific information extraction examples manually evaluated by the human curators. The plot compactly represents both the per-relation accuracy of the extraction process (indicated with the length of the corresponding bar) and the abundance of the corresponding relations in the database (represented by the bar color). There are relations extracted with a high precision; there are also many noisy relationships. The database accuracy was markedly increased by the automated curation outlined in this study (see <xref ref-type="fig" rid="pcbi-0020118-g009">Figure 9</xref>).</p>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g005" xlink:type="simple"/>
			</fig>
			<p>The classification problem of separating correctly and incorrectly extracted facts appears to belong to a class of easier problems. Even the simplest Naïve Bayes method had an average ROC score of 0.84, which more sophisticated approaches surpassed to reach almost 0.95. Judging by the average ROC score, the quality of prediction increased in the following order of methods: Clustered Bayes 68, Naïve Bayes, MaxEnt 1, Clustered Bayes 44, QDA, artificial neural network, SVMs, and MaxEnt 2/MaxEnt 2-v (see <xref ref-type="table" rid="pcbi-0020118-t006">Table 6</xref>). The Meta-method was always slightly more accurate than MaxEnt 2, as explained in the legend to <xref ref-type="table" rid="pcbi-0020118-t006">Table 6</xref> and as shown in <xref ref-type="fig" rid="pcbi-0020118-g004">Figure 4</xref>.</p>
			<table-wrap content-type="1col" id="pcbi-0020118-t006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t006</object-id><label>Table 6</label><caption>
					<p>ROC Scores for Methods Used in This Study, with Error Bars Calculated in 10-Fold Cross-Validation</p>
				</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t006" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb6col1" align="left" charoff="0" char=""/><col id="tb6col2" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left"><hr/>Method</td><td><hr/>ROC Score &plusmn; 2<italic>&sigma;</italic></td></tr></thead><tbody><tr><td>Clustered Bayes 68</td><td>0.8115 &plusmn; 0.0679</td></tr><tr><td>Na&iuml;ve Bayes</td><td>0.8409 &plusmn; 0.0543</td></tr><tr><td>MaxEnt 1</td><td>0.8647 &plusmn; 0.0412</td></tr><tr><td>Clustered Bayes 44</td><td>0.8751 &plusmn; 0.0414</td></tr><tr><td>QDA</td><td>0.8826 &plusmn; 0.0445</td></tr><tr><td>SVM-t0</td><td>0.9203 &plusmn; 0.0317</td></tr><tr><td>SVM</td><td>0.9222 &plusmn; 0.0299</td></tr><tr><td>Neural network</td><td>0.9236 &plusmn; 0.0314</td></tr><tr><td>SVM-t1-d2</td><td>0.9277 &plusmn; 0.0285</td></tr><tr><td>SVM-t2-g2</td><td>0.9280 &plusmn; 0.0285</td></tr><tr><td>SVM-t1-d3</td><td>0.9281 &plusmn; 0.0280</td></tr><tr><td>SVM-t2-g1</td><td>0.9286 &plusmn; 0.0283</td></tr><tr><td>SVM-t2-g0.5</td><td>0.9287 &plusmn; 0.0285</td></tr><tr><td>MaxEnt 2</td><td>0.9480 &plusmn; 0.0178</td></tr><tr><td>MaxEnt 2-v</td><td>0.9492 &plusmn; 0.0156</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt601"><p>The Meta-method is much more expensive computationally than the rest of the methods, so we evaluated it using a smaller dataset and the corresponding results are not directly comparable with those for the other methods. The Meta-method outperformed other methods listed in this table when trained on the same data (unpublished data).</p></fn></table-wrap-foot> --></table-wrap>
			<p><xref ref-type="table" rid="pcbi-0020118-t006">Table 6</xref> provides a somewhat misleading impression that MaxEnt 2 and MaxEnt 2-v are <italic>not</italic> significantly more accurate than their closest competitors (the SVM family), because of the overlapping confidence intervals. However, when we trace the performance of all classifiers in individual cross-validation experiments (see <xref ref-type="fig" rid="pcbi-0020118-g006">Figure 6</xref>) it becomes clear that MaxEnt 2 and MaxEnt 2-v outperformed their rivals in every cross-validation experiment. The SVM and artificial neural network methods performed essentially identically, and were always more accurate than three other methods: QDA, Clustered Bayes 44, and MaxEnt 1. Finally, the performance of the Clustered Bayes 68 and the Naïve Bayes methods was reliably the least accurate of all methods studied.</p>
			<fig id="pcbi-0020118-g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g006</object-id>
				<label>Figure 6</label>
				<caption>
					<title>Ranks of All Classification Methods Used in This Study in Ten Cross-Validation Experiments</title>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g006" xlink:type="simple"/>
			</fig>
			<p>It is a matter of both academic curiosity and of practical importance to know how the performance of our artificial intelligence curator compares with that of humans. If we define the <italic>correct</italic> answer as a majority-vote of the three human evaluators (see <xref ref-type="table" rid="pcbi-0020118-t006">Table 6</xref>), the average accuracy of MaxEnt 2 is slightly lower than, but statistically indistinguishable from, humans (at the 99% level of significance, see <xref ref-type="table" rid="pcbi-0020118-t006">Table 6</xref>; capital letters “A,” “L,” “S,” and “M” hide the real names of the human evaluators). If, however, in the spirit of Turing's test of machine intelligence [<xref ref-type="bibr" rid="pcbi-0020118-b018">18</xref>], we treat the MaxEnt 2 algorithm on an equal footing with the human evaluators, compute the average over predictions of all four anonymous evaluators, and compare the quality of the performance of each evaluator with regard to the average, MaxEnt 2 always performs slightly more accurately (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 11</xref>) than one of the human evaluators (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 12</xref>). (In all cases we compared performance of the algorithm on data that was not used for its training; see <xref ref-type="table" rid="pcbi-0020118-t006">Tables 6</xref> and <xref ref-type="table" rid="pcbi-0020118-t007">7</xref>.)</p>
			<table-wrap content-type="1col" id="pcbi-0020118-t007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t007</object-id><label>Table 7</label><caption>
					<p>Comparison of the Performance of Human Evaluators and of the MaxEnt 2 Algorithm</p>
				</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t007" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb7col1" align="left" charoff="0" char=""/><col id="tb7col2" align="left" charoff="0" char=""/><col id="tb7col3" align="char" charoff="0" char="."/><col id="tb7col4" align="left" charoff="0" char=""/><col id="tb7col5" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" colspan="2"><hr/>Evaluator</td><td><hr/>Correct</td><td><hr/>Incorrect</td><td><hr/>Accuracy &lsqb;99&percnt; CI&rsqb;</td></tr></thead><tbody><tr><td><bold>Batch A</bold></td><td>A</td><td>10,981</td><td>208 (11,189)</td><td>0.981410 &lsqb;0.978014 0.984628&rsqb;</td></tr><tr><td></td><td>L</td><td>10,547</td><td>642 (11,189)</td><td>0.942622 &lsqb;0.936902 0.948253&rsqb;</td></tr><tr><td></td><td>M</td><td>10,867</td><td>322 (11,189)</td><td>0.971222 &lsqb;0.967111 0.975244&rsqb;</td></tr><tr><td></td><td>MaxEnt 2</td><td>10,537</td><td>652 (11,189)</td><td>0.941728 &lsqb;0.935919 0.947359&rsqb;</td></tr><tr><td><bold>Batch B</bold></td><td>A</td><td>9,796</td><td>430 (10,226)</td><td>0.957950 &lsqb;0.952767 0.962938&rsqb;</td></tr><tr><td></td><td>M</td><td>9,898</td><td>328 (10,226)</td><td>0.967925 &lsqb;0.963329 0.972325&rsqb;</td></tr><tr><td></td><td>S</td><td>9,501</td><td>725 (10,226)</td><td>0.929102 &lsqb;0.922453 0.935556&rsqb;</td></tr><tr><td></td><td>MaxEnt 2</td><td>9,379</td><td>847 (10,226)</td><td>0.917172 &lsqb;0.910033 0.924115&rsqb;</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt701"><p>Column 1 lists all evaluators (four human evaluators, &ldquo;A&rdquo;, &ldquo;L&rdquo;, &ldquo;M&rdquo;, and &ldquo;S&rdquo;, and the MaxEnt 2 classifier).</p></fn><fn id="nt702"><p>Column 2 gives the number of correct answers (with respect to the gold standard) produced by each evaluator.</p></fn><fn id="nt703"><p>Column 3 shows the number of incorrect answers for each evaluator out of the total number of examples (in parentheses).</p></fn><fn id="nt704"><p>Column 4 shows the accuracy and the 99&percnt; confidence interval for the accuracy value.</p></fn><fn id="nt705"><p>The gold standard was defined as the majority among three human evaluators (examples with uncertain votes were not considered, so each evaluator's vote was either strictly negative or strictly positive).</p></fn><fn id="nt706"><p>Batches A and B were evaluated by different sets of human evaluators.</p></fn><fn id="nt707"><p>We computed the binomial confidence intervals at the <italic>&alpha;</italic>-level of significance (<italic>&alpha;</italic> &times; 100&percnt; CI) by identifying a pair of parameter values that separate areas of approximately <mml:math display='inline'><mml:mrow><mml:mstyle scriptlevel='+1'><mml:mfrac><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>&alpha;</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math> at each distribution tail.</p></fn></table-wrap-foot> --></table-wrap>
			<p>The features that we used in our analysis are obviously not all equally important. To elucidate the relative importance of the individual features and of feature pairs, we computed the mutual information between all pairs of features and the class variable (see <xref ref-type="fig" rid="pcbi-0020118-g007">Figure 7</xref>). The mutual information of class variable <italic>C,</italic> and a pair of feature variables (<italic>F<sub>i</sub>,F<sub>j</sub></italic>), is defined in the following way (e.g., see [<xref ref-type="bibr" rid="pcbi-0020118-b019">19</xref>]).
				<disp-formula id="pcbi-0020118-e018"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e018" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:malignmark/><mml:mo>&equals;</mml:mo></mml:mrow></mml:math> --></disp-formula>
				<disp-formula id="pcbi-0020118-e019"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e019" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&plus;</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&minus;</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where function <italic>H</italic>(<italic>P</italic>[<italic>x</italic>]) is Claude E. Shannon's entropy of distribution <italic>P</italic>(<italic>x</italic>) (see p. 14 of [<xref ref-type="bibr" rid="pcbi-0020118-b020">20</xref>]), defined in the following way:
				<disp-formula id="pcbi-0020118-e020"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020118.e020" xlink:type="simple"/><!-- <mml:math display='block'><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mstyle displaystyle='true'><mml:munder><mml:mo>&sum;</mml:mo><mml:mi>x</mml:mi></mml:munder></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:mi>P</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>log</mml:mtext><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math> --></disp-formula>where summation is done over all admissible values of <italic>x</italic>. <xref ref-type="fig" rid="pcbi-0020118-g007">Figure 7</xref> shows that the most informative standalone features, as expected, are those that are derived from the human evaluations of the quality of extraction of individual relations and terms (such as the average quality scores), and features reflecting properties of the sentence that was used to extract the corresponding fact. In addition, some dictionary-related features, such as finding a term in LocusLink, are fairly informative. Some features, however, become informative only in combination with other features. For example, the minimum positive distance between two terms in a sentence is not very informative by itself, but becomes fairly useful in combination with other features, such as the number of commas in the sentence or the length of the sentence (see <xref ref-type="fig" rid="pcbi-0020118-g007">Figure 7</xref>). Similarly, while finding a term in GenBank does not help the classifier by itself, the feature becomes informative in combination with syntactic properties of the sentence and statistics about the manually evaluated data.
			</p>
			<fig id="pcbi-0020118-g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g007</object-id>
				<label>Figure 7</label>
				<caption>
					<title>Comparison of a Correlation Matrix for the Features (Colored Half of the Matrix) Computed Using Only the Annotated Set of Data and a Matrix of Mutual Information between All Feature Pairs and the Statement Class (Correct or Incorrect)</title>
					<p>The plot indicates that a significant amount of information critical for classification is encoded in pairs of weakly correlated features. The white dotted lines outline clusters of features, suggested by analysis of the annotated dataset; we used these clusters in implementation of the Clustered Bayes classifier.</p>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g007" xlink:type="simple"/>
			</fig>
			<p>Assignment of facts to classes <italic>correct</italic> and <italic>incorrect</italic> by evaluators is subject to random errors: facts that were seen by many evaluators would be assigned to the appropriate class with higher probability than facts that were seen by only one evaluator. This introduction of noise directly affects the estimate of the accuracy of an artificial intelligence curator: if the gold standard is noisy, the apparent accuracy of the algorithm compared with the gold standard is lower than the real accuracy. Indeed, the three-evaluator gold standard (see <xref ref-type="table" rid="pcbi-0020118-t008">Table 8</xref>) indicated that the actual optimum accuracy of the MaxEnt 2 classifier is higher than 88%. (The 88% accuracy estimate came from comparison of MaxEnt 2 predictions with the whole set of annotated facts, half of which were seen by only one or two evaluators; see <xref ref-type="fig" rid="pcbi-0020118-g008">Figure 8</xref>.) When MaxEnt 2 was compared with the three-human gold standard, the estimated accuracy was about 91% (see <xref ref-type="table" rid="pcbi-0020118-t008">Table 8</xref>).</p>
			<table-wrap content-type="1col" id="pcbi-0020118-t008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.t008</object-id><label>Table 8</label><caption>
					<p>Comparison of Human Evaluators and a Program That Mimicked Their Work</p>
				</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.t008" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb8col1" align="left" charoff="0" char=""/><col id="tb8col2" align="left" charoff="0" char=""/><col id="tb8col3" align="char" charoff="0" char="."/><col id="tb8col4" align="left" charoff="0" char=""/><col id="tb8col5" align="left" charoff="0" char=""/><col id="tb8col6" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" colspan="2"><hr/>Evaluator</td><td><hr/>Correct</td><td><hr/>Incorrect</td><td><hr/>Accuracy</td><td><hr/>&lsqb;99&percnt; CI&rsqb;</td></tr></thead><tbody><tr><td><bold>Batch A</bold></td><td>A</td><td>10,700</td><td>182 (10,882)</td><td>0.983275</td><td>&lsqb;0.980059 0.986400&rsqb;</td></tr><tr><td></td><td>L</td><td>10,452</td><td>430 (10,882)</td><td>0.960485</td><td>&lsqb;0.955615 0.965172&rsqb;</td></tr><tr><td></td><td>M</td><td>10,629</td><td>253 (10,882)</td><td>0.976751</td><td>&lsqb;0.972983 0.980426&rsqb;</td></tr><tr><td></td><td>MaxEnt 2</td><td>10,537</td><td>345 (10,882)</td><td>0.968296</td><td>&lsqb;0.963885 0.972523&rsqb;</td></tr><tr><td><bold>Batch B</bold></td><td>A</td><td>9,499</td><td>363 (9,862)</td><td>0.963192</td><td>&lsqb;0.958223 0.967958&rsqb;</td></tr><tr><td></td><td>M</td><td>9,636</td><td>226 (9,862)</td><td>0.977084</td><td>&lsqb;0.973130 0.980836&rsqb;</td></tr><tr><td></td><td>S</td><td>9,332</td><td>530 (9,862)</td><td>0.946258</td><td>&lsqb;0.940276 0.952038&rsqb;</td></tr><tr><td></td><td>MaxEnt 2</td><td>9,379</td><td>483 (9,862)</td><td>0.951024</td><td>&lsqb;0.945346 0.956500&rsqb;</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt801"><p>Column 1lists all evaluators (four human evaluators, &ldquo;A&rdquo;, &ldquo;L&rdquo;, &ldquo;M&rdquo;, and &ldquo;S&rdquo;, and the MaxEnt 2 classifier).</p></fn><fn id="nt802"><p>Column 2 gives the number of correct answers (with respect to the gold standard) produced by each evaluator.</p></fn><fn id="nt803"><p>Column 3 shows the number of incorrect answers for each evaluator out of the total number of examples (in parentheses). Examples with tied scores (i.e., two positive and two negative votes) were not considered for the gold standard.</p></fn><fn id="nt804"><p>Column 4 shows the accuracy and the 99&percnt; confidence interval for the accuracy value.</p></fn><fn id="nt805"><p>The gold standard was defined as the majority among three human evaluators <italic>and</italic> the MaxEnt 2 algorithm. We did not include evaluation ties (two positive and two negative evaluations for the same statement&ndash;sentence pair) into the gold standard, which explains the difference in the number of the statement&ndash;sentence pairs used in the three-evaluator-gold-standard and four-evaluator-gold-standard experiments. The even (two by two) evaluator splits are clearly uninformative in assessing the relative performance of our evaluators because all four evaluators get an equal penalty for each tie case.</p></fn><fn id="nt806"><p>Batches A and B were evaluated by different sets of human evaluators. We computed the binomial confidence intervals at the <italic>&alpha;</italic>-level of significance (<italic>&alpha;</italic> &times; 100&percnt; CI) by identifying a pair of parameter values that separate areas of approximately <mml:math display='inline'><mml:mrow><mml:mstyle scriptlevel='+1'><mml:mfrac><mml:mrow><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math> at each distribution tail.</p></fn></table-wrap-foot> --></table-wrap>
		</sec>
		<sec id="s4">
			<title>Discussion</title>
			<p>As evidenced by <xref ref-type="fig" rid="pcbi-0020118-g005">Figures 5</xref> and <xref ref-type="fig" rid="pcbi-0020118-g009">9</xref>, the results of our study are directly applicable to analysis of large text-mined databases of molecular interactions: we can identify sets of molecular interactions with any predefined level of precision (see <xref ref-type="fig" rid="pcbi-0020118-g008">Figure 8</xref>). For example, we can request from a database all interactions with extraction precision 95% or greater, which would result in the case of the GeneWays 6.0 database in recall of 77.9% (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 13</xref>). However, we are not forced to discard the unrequested lower-than-threshold-precision interactions, as we must the chaff from the wheat in the epigraph to this article. Intuitively, even weakly supported facts (i.e., those on which there is not full agreement) can be useful in interpreting experimental results, and may gain additional support when studied in conjunction with other related facts (see <xref ref-type="fig" rid="pcbi-0020118-g001">Figure 1</xref> for examples of weakly supported yet useful facts, such as <italic>cocaine stimulates prolactin,</italic> with a low extraction confidence, but biologically plausible; the accuracy predictions were computed using the MaxEnt 2 method). We envision that, in the near future, we will have computational approaches, such as probabilistic logic, that allow us to use weakly supported facts for building a reliable model of molecular interactions from unreliable facts (paraphrasing John von Neumann's “synthesis of reliable organisms from unreliable components” [<xref ref-type="bibr" rid="pcbi-0020118-b021">21</xref>]).</p>
			<fig id="pcbi-0020118-g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g008</object-id>
				<label>Figure 8</label>
				<caption>
					<title>Values of Precision, Recall, and Accuracy of the MaxEnt 2 Classifier Plotted against the Corresponding Log-Scores Provided by the Classifier</title>
					<p>Precision is defined as<inline-formula id="pcbi-0020118-ex005"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex005" xlink:type="simple"/></inline-formula>
						 , recall is defined as<inline-formula id="pcbi-0020118-ex006"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex006" xlink:type="simple"/></inline-formula>
						 , and accuracy is defined as<inline-formula id="pcbi-0020118-ex007"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020118.ex007" xlink:type="simple"/></inline-formula>
						 . The optimum accuracy was close to 88%, and attained a score threshold slightly above 0. We can improve precision at the expense of accuracy. For example, by setting the threshold score to 0.6702, we can bring the overall database precision to 95%, which would correspond to a recall of 77.91% and to an overall accuracy of 84.18%.
					</p>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g008" xlink:type="simple"/>
			</fig>
			<p>Experiments with any standalone set of data generate results insufficient to allow us to draw conclusions about the general performance of different classifiers. Nevertheless, we can speculate about the reasons for the observed differences in performance of the methods when applied to our data. The modest performance of the Naïve Bayes classifier is unsurprising: we know that many pairs of features used in our analysis are highly or weakly correlated (see <xref ref-type="fig" rid="pcbi-0020118-g002">Figures 2</xref> and <xref ref-type="fig" rid="pcbi-0020118-g007">7</xref>). The actual feature dependencies violate the method's major assumption about the conditional independence of features. MaxEnt 1, which performed significantly more accurately than the Naïve Bayes in our experiments, but was not as efficient as other methods, takes into account only the class-specific mean values of features; it does not incorporate parameters to reflect dependencies between individual features. This deficiency of MaxEnt 1 is compensated by MaxEnt 2, which has an additional set of parameters for pairs of features leading to a markedly improved performance (<xref ref-type="supplementary-material" rid="pcbi-0020118-sd001">Text S1 Note 14</xref>).</p>
			<fig id="pcbi-0020118-g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.0020118.g009</object-id>
				<label>Figure 9</label>
				<caption>
					<title>Accuracy and Abundance of the Extracted and Automatically Curated Relations</title>
					<p>This plot represents both the per-relation accuracy after both information extraction and automated curation were done. Accuracy is indicated with the length of the relation-specific bars, while the abundance of the corresponding relations in the manually curated dataset is represented by color. Here, the MaxEnt 2 method was used for the automated curation. The results shown correspond to a score-based decision threshold set to zero; that is, all negative-score predictions were treated as “incorrect.” An increase in the score-based decision boundary can raise the precision of the output at the expense of a decrease in the recall (see <xref ref-type="fig" rid="pcbi-0020118-g008">Figure 8</xref>).</p>
				</caption>
				<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.g009" xlink:type="simple"/>
			</fig>
			<p>Our explanation for the superior performance of the MaxEnt 2 algorithm with respect to the remainder of the algorithms in the study batch is that MaxEnt 2 requires the least parameter tweaking in comparison with other methods of similar complexity. Performance of the Clustered Bayes method is highly sensitive to the definition of feature clusters and to the way we discretize the feature values—essentially presenting the problem of selecting an optimal model from an extensive set of rival models, each model defined by a specific set of feature clusters. Our initial intuition was that a reasonable choice of clusters can become clear from analysis of an estimated feature-correlation matrix. We originally expected that more highly correlated parameters would belong to the same cluster. However, the correlation matrices estimated from the complete GeneWays 6.0 database and from a subset of annotated facts turned out to be rather different—see <xref ref-type="fig" rid="pcbi-0020118-g002">Figure 2</xref>—suggesting that we could group features differently. In addition, analysis of mutual information between the class of a statement and pairs of features (see <xref ref-type="fig" rid="pcbi-0020118-g007">Figure 7</xref>) indicated that the most informative pairs of features are often only weakly correlated. It is quite likely that the optimum choice of feature clusters in the Clustered Bayes method would lead to a classifier performance accuracy significantly higher than that of MaxEnt 2 in our study, but the road to this improved classifier lies through a searching an astronomically large space of alternative models.</p>
			<p>Similar to optimizing the Clustered Bayes algorithm through model selection, we can experiment with various kernel functions in the SVM algorithm, and can try alternative designs of the artificial neural network. These optimization experiments are likely to be computationally expensive, but are almost certain to improve the prediction quality. Furthermore, there are bound to exist additional useful classification features waiting to be discovered in future analyses. Finally, we speculate that we can improve the quality of the classifier by increasing the number of human evaluators who annotate each datapoint in the training set. This would allow us to improve the gold standard itself, and could lead to development of a computer program that performs the curation job consistently at least as accurately as an average human evaluator.</p>
		</sec>
		<sec id="s5">
			<title>Supporting Information</title>
			<supplementary-material id="pcbi-0020118-sd001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020118.sd001" xlink:type="simple">
				<label>Text S1</label>
				<caption>
					<title>Supplementary Text</title>
					<p>(47 KB PDF)</p>
				</caption>
			</supplementary-material>
		</sec>
	</body>
	<back>
		<ack>
			<p>The authors are grateful to Mr. Murat Cokol and to Ms. Lyn Dupré Oppenheim for comments on the earlier version of the manuscript, and to Mr. Marc Hadfield and to Ms. Mitzi Morris for programming assistance.</p>
		</ack>
		
		<glossary>
			<title>Abbreviations</title>
			<def-list>
				<def-item>
					<term>QDA</term>
					<def>
						<p>Quadratic Discriminant Analysis</p>
					</def>
				</def-item>
				<def-item>
					<term>Rbf</term>
					<def>
						<p>radial basis function</p>
					</def>
				</def-item>
				<def-item>
					<term><italic>ROC</italic></term>
					<def>
						<p>receiver operating charecteristic</p>
					</def>
				</def-item>
				<def-item>
					<term>SVM</term>
					<def>
						<p>support vector machines</p>
					</def>
				</def-item>
			</def-list>
		</glossary>
		<ref-list>
			<title>References</title>
			<ref id="pcbi-0020118-b001">
<label>1</label>
				<element-citation publication-type="other" xlink:type="simple">
					<collab xlink:type="simple">21st Century King James Bible Publishers</collab>
					<year>1994</year>
					<source>The Holy Bible: 21st Century King James Version: Containing the Old Testament and the New Testament</source>
					<publisher-loc>Gary (South Dakota)</publisher-loc>
					<publisher-name>21st Century King James Bible Publishers</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">1888</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b002">
<label>2</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Sekimizu</surname>
							<given-names>T</given-names>
						</name>
						<name name-style="western">
							<surname>Park</surname>
							<given-names>HS</given-names>
						</name>
						<name name-style="western">
							<surname>Tsujii</surname>
							<given-names>J</given-names>
						</name>
					</person-group>
					<year>1998</year>
					<article-title>Identifying the interaction between genes and gene products based on frequently seen verbs in MEDLINE abstracts.</article-title>
					<source>Genome Inform Ser Workshop Genome Inform</source>
					<volume>9</volume>
					<fpage>62</fpage>
					<lpage>71</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b003">
<label>3</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Ono</surname>
							<given-names>T</given-names>
						</name>
						<name name-style="western">
							<surname>Hishigaki</surname>
							<given-names>H</given-names>
						</name>
						<name name-style="western">
							<surname>Tanigami</surname>
							<given-names>A</given-names>
						</name>
						<name name-style="western">
							<surname>Takagi</surname>
							<given-names>T</given-names>
						</name>
					</person-group>
					<year>2001</year>
					<article-title>Automated extraction of information on protein–protein interactions from the biological literature.</article-title>
					<source>Bioinformatics</source>
					<volume>17</volume>
					<fpage>155</fpage>
					<lpage>161</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b004">
<label>4</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Thomas</surname>
							<given-names>J</given-names>
						</name>
						<name name-style="western">
							<surname>Milward</surname>
							<given-names>D</given-names>
						</name>
						<name name-style="western">
							<surname>Ouzounis</surname>
							<given-names>C</given-names>
						</name>
						<name name-style="western">
							<surname>Pulman</surname>
							<given-names>S</given-names>
						</name>
						<name name-style="western">
							<surname>Carroll</surname>
							<given-names>M</given-names>
						</name>
					</person-group>
					<year>2000</year>
					<article-title>Automatic extraction of protein interactions from scientific abstracts.</article-title>
					<source>Pac Symp Biocomput</source>
					<volume>2000</volume>
					<fpage>541</fpage>
					<lpage>552</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b005">
<label>5</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Blaschke</surname>
							<given-names>C</given-names>
						</name>
						<name name-style="western">
							<surname>Valencia</surname>
							<given-names>A</given-names>
						</name>
					</person-group>
					<year>2001</year>
					<article-title>The potential use of SUISEKI as a protein interaction discovery tool.</article-title>
					<source>Genome Inform Ser Workshop Genome Inform</source>
					<volume>12</volume>
					<fpage>123</fpage>
					<lpage>134</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b006">
<label>6</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Friedman</surname>
							<given-names>C</given-names>
						</name>
						<name name-style="western">
							<surname>Kra</surname>
							<given-names>P</given-names>
						</name>
						<name name-style="western">
							<surname>Yu</surname>
							<given-names>H</given-names>
						</name>
						<name name-style="western">
							<surname>Krauthammer</surname>
							<given-names>M</given-names>
						</name>
						<name name-style="western">
							<surname>Rzhetsky</surname>
							<given-names>A</given-names>
						</name>
					</person-group>
					<year>2001</year>
					<article-title>GENIES: A natural language processing system for the extraction of molecular pathways from journal articles.</article-title>
					<source>Bioinformatics</source>
					<volume>17</volume>
					<issue>(Supplement 1)</issue>
					<fpage>S74</fpage>
					<lpage>S82</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b007">
<label>7</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Rzhetsky</surname>
							<given-names>A</given-names>
						</name>
						<name name-style="western">
							<surname>Iossifov</surname>
							<given-names>I</given-names>
						</name>
						<name name-style="western">
							<surname>Koike</surname>
							<given-names>T</given-names>
						</name>
						<name name-style="western">
							<surname>Krauthammer</surname>
							<given-names>M</given-names>
						</name>
						<name name-style="western">
							<surname>Kra</surname>
							<given-names>P</given-names>
						</name><etal/>
					</person-group>
					<year>2004</year>
					<article-title>GeneWays: A system for extracting, analyzing, visualizing, and integrating molecular pathway data.</article-title>
					<source>J Biomed Inform</source>
					<volume>37</volume>
					<fpage>43</fpage>
					<lpage>53</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b008">
<label>8</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Benson</surname>
							<given-names>DA</given-names>
						</name>
						<name name-style="western">
							<surname>Karsch-Mizrachi</surname>
							<given-names>I</given-names>
						</name>
						<name name-style="western">
							<surname>Lipman</surname>
							<given-names>DJ</given-names>
						</name>
						<name name-style="western">
							<surname>Ostell</surname>
							<given-names>J</given-names>
						</name>
						<name name-style="western">
							<surname>Wheeler</surname>
							<given-names>DL</given-names>
						</name>
					</person-group>
					<year>2005</year>
					<article-title>GenBank.</article-title>
					<source>Nucleic Acids Res</source>
					<volume>33</volume>
					<fpage>D34</fpage>
					<lpage>D38</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b009">
<label>9</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Fisher</surname>
							<given-names>RA</given-names>
						</name>
					</person-group>
					<year>1936</year>
					<article-title>The use of multiple measurements in taxonomic problems.</article-title>
					<source>Ann Eugenic</source>
					<volume>7</volume>
					<fpage>179</fpage>
					<lpage>188</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b010">
<label>10</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Jaynes</surname>
							<given-names>ET</given-names>
						</name>
					</person-group>
					<year>1957</year>
					<article-title>Information theory and statistical mechanics.</article-title>
					<source>Phys Rev</source>
					<volume>106</volume>
					<fpage>620</fpage>
					<lpage>630</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b011">
<label>11</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Jaynes</surname>
							<given-names>ET</given-names>
						</name>
						<name name-style="western">
							<surname>Bretthorst</surname>
							<given-names>GL</given-names>
						</name>
					</person-group>
					<year>2003</year>
					<source>Probability theory: The logic of science</source>
					<publisher-loc>Cambridge (United Kingdom)/New York</publisher-loc>
					<publisher-name>Cambridge University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">758</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b012">
<label>12</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Cover</surname>
							<given-names>TM</given-names>
						</name>
						<name name-style="western">
							<surname>Thomas</surname>
							<given-names>JA</given-names>
						</name>
					</person-group>
					<year>2005</year>
					<source>Elements of information theory. 2nd edition</source>
					<publisher-loc>Hoboken</publisher-loc>
					<publisher-name>Wiley</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">748</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b013">
<label>13</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Chauvin</surname>
							<given-names>Y</given-names>
						</name>
						<name name-style="western">
							<surname>Rumelhart</surname>
							<given-names>DE</given-names>
						</name>
					</person-group>
					<year>1995</year>
					<source>Backpropagation: Theory, architectures, and applications. Developments in connectionist theory</source>
					<publisher-loc>Hillsdale (New Jersey)</publisher-loc>
					<publisher-name>Erlbaum</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">568</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b014">
<label>14</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Vapnik</surname>
							<given-names>V</given-names>
						</name>
					</person-group>
					<year>1995</year>
					<source>The nature of statistical learning theory. Statistics, Computer Science, Psychology</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Springer</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">314</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b015">
<label>15</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Cristianini</surname>
							<given-names>N</given-names>
						</name>
						<name name-style="western">
							<surname>Shawe-Taylor</surname>
							<given-names>J</given-names>
						</name>
					</person-group>
					<year>2000</year>
					<source>An introduction to support vector machines: And other kernel-based learning methods</source>
					<publisher-loc>Cambridge (United Kingdom)/New York</publisher-loc>
					<publisher-name>Cambridge University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">189</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b016">
<label>16</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="editor">
						<name name-style="western">
							<surname>Bartlett</surname>
							<given-names>PJ</given-names>
						</name>
						<name name-style="western">
							<surname>Schölklopf</surname>
							<given-names>B</given-names>
						</name>
						<name name-style="western">
							<surname>Schuurmans</surname>
							<given-names>D</given-names>
						</name>
						<name name-style="western">
							<surname>Smola</surname>
							<given-names>AJ</given-names>
						</name>
					</person-group>
					<year>2000</year>
					<source>Advances in large margin classifiers</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">422</size>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b017">
<label>17</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Hanley</surname>
							<given-names>JA</given-names>
						</name>
						<name name-style="western">
							<surname>McNeil</surname>
							<given-names>BJ</given-names>
						</name>
					</person-group>
					<year>1982</year>
					<article-title>The meaning and use of the area under a receiver operating characteristic (ROC) curve.</article-title>
					<source>Radiology</source>
					<volume>143</volume>
					<fpage>29</fpage>
					<lpage>36</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b018">
<label>18</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Turing</surname>
							<given-names>A</given-names>
						</name>
					</person-group>
					<year>1950</year>
					<article-title>Computing machinery and intelligence.</article-title>
					<source>Mind</source>
					<volume>59</volume>
					<fpage>433</fpage>
					<lpage>560</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b019">
<label>19</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Church</surname>
							<given-names>KW</given-names>
						</name>
						<name name-style="western">
							<surname>Hanks</surname>
							<given-names>P</given-names>
						</name>
					</person-group>
					<year>1989</year>
					<article-title>Word association norms, mutual information, and lexicography.</article-title>
					<source>Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics</source>
					<conf-date>10–13</conf-date>
					<conf-date>June 1986;</conf-date>
					<conf-loc>Morristown, New Jersey, United States.</conf-loc>
					<comment>pp.</comment>
					<fpage>76</fpage>
					<lpage>83</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b020">
<label>20</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Shannon</surname>
							<given-names>CE</given-names>
						</name>
						<name name-style="western">
							<surname>Weaver</surname>
							<given-names>W</given-names>
						</name>
					</person-group>
					<year>1949,1963</year>
					<source>The mathematical theory of communication</source>
					<publisher-loc>Urbana</publisher-loc>
					<publisher-name>University of Illinois Press</publisher-name>
					<fpage>144</fpage>
					<comment>p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b021">
<label>21</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>von Neumann</surname>
							<given-names>J</given-names>
						</name>
					</person-group>
					<year>1956</year>
					<article-title>Probabilistic logics and the synthesis of reliable organisms from unreliable components.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor">
						<name name-style="western">
							<surname>Shannon</surname>
							<given-names>CE</given-names>
						</name>
						<name name-style="western">
							<surname>McCarthy</surname>
							<given-names>J</given-names>
						</name>
					</person-group>
					<source>Automata Studies</source>
					<publisher-loc>Princeton</publisher-loc>
					<publisher-name>Princeton University Press</publisher-name>
					<comment>pp.</comment>
					<fpage>43</fpage>
					<lpage>98</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b022">
<label>22</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Ruan</surname>
							<given-names>W</given-names>
						</name>
						<name name-style="western">
							<surname>Pang</surname>
							<given-names>P</given-names>
						</name>
						<name name-style="western">
							<surname>Rao</surname>
							<given-names>Y</given-names>
						</name>
					</person-group>
					<year>1999</year>
					<article-title>The sh2/sh3 adaptor protein dock interacts with the ste20-like kinase misshapen in controlling growth cone motility.</article-title>
					<source>Neuron</source>
					<volume>24</volume>
					<fpage>595</fpage>
					<lpage>605</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b023">
<label>23</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Chan</surname>
							<given-names>YM</given-names>
						</name>
						<name name-style="western">
							<surname>Jan</surname>
							<given-names>YN</given-names>
						</name>
					</person-group>
					<year>1999</year>
					<article-title>Presenilins, processing of beta-amyloid precursor protein, and notch signaling.</article-title>
					<source>Neuron</source>
					<volume>23</volume>
					<fpage>201</fpage>
					<lpage>204</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b024">
<label>24</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Niethammer</surname>
							<given-names>M</given-names>
						</name>
						<name name-style="western">
							<surname>Smith</surname>
							<given-names>DS</given-names>
						</name>
						<name name-style="western">
							<surname>Ayala</surname>
							<given-names>R</given-names>
						</name>
						<name name-style="western">
							<surname>Peng</surname>
							<given-names>J</given-names>
						</name>
						<name name-style="western">
							<surname>Ko</surname>
							<given-names>J</given-names>
						</name><etal/>
					</person-group>
					<year>2000</year>
					<article-title>Nudel is a novel cdk5 substrate that associates with lis1 and cytoplasmic dynein.</article-title>
					<source>Neuron</source>
					<volume>28</volume>
					<fpage>697</fpage>
					<lpage>711</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b025">
<label>25</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Alloway</surname>
							<given-names>PG</given-names>
						</name>
						<name name-style="western">
							<surname>Howard</surname>
							<given-names>L</given-names>
						</name>
						<name name-style="western">
							<surname>Dolph</surname>
							<given-names>PJ</given-names>
						</name>
					</person-group>
					<year>2000</year>
					<article-title>The formation of stable rhodopsin-arrestin complexes induces apoptosis and photoreceptor cell degeneration.</article-title>
					<source>Neuron</source>
					<volume>28</volume>
					<fpage>129</fpage>
					<lpage>138</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b026">
<label>26</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Tanaka</surname>
							<given-names>H</given-names>
						</name>
						<name name-style="western">
							<surname>Shan</surname>
							<given-names>W</given-names>
						</name>
						<name name-style="western">
							<surname>Phillips</surname>
							<given-names>GR</given-names>
						</name>
						<name name-style="western">
							<surname>Arndt</surname>
							<given-names>K</given-names>
						</name>
						<name name-style="western">
							<surname>Bozdagi</surname>
							<given-names>O</given-names>
						</name><etal/>
					</person-group>
					<year>2000</year>
					<article-title>Molecular modification of n-cadherin in response to synaptic activity.</article-title>
					<source>Neuron</source>
					<volume>25</volume>
					<fpage>93</fpage>
					<lpage>107</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b027">
<label>27</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Magga</surname>
							<given-names>JM</given-names>
						</name>
						<name name-style="western">
							<surname>Jarvis</surname>
							<given-names>SE</given-names>
						</name>
						<name name-style="western">
							<surname>Arnot</surname>
							<given-names>MI</given-names>
						</name>
						<name name-style="western">
							<surname>Zamponi</surname>
							<given-names>GW</given-names>
						</name>
						<name name-style="western">
							<surname>Braun</surname>
							<given-names>JE</given-names>
						</name>
					</person-group>
					<year>2000</year>
					<article-title>Cysteine string protein regulates g protein modulation of n-type calcium channels.</article-title>
					<source>Neuron</source>
					<volume>28</volume>
					<fpage>195</fpage>
					<lpage>204</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b028">
<label>28</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Gordon</surname>
							<given-names>SE</given-names>
						</name>
						<name name-style="western">
							<surname>Varnum</surname>
							<given-names>MD</given-names>
						</name>
						<name name-style="western">
							<surname>Zagotta</surname>
							<given-names>WN</given-names>
						</name>
					</person-group>
					<year>1997</year>
					<article-title>Direct interaction between amino- and carboxyl-terminal domains of cyclic nucleotide-gated channels.</article-title>
					<source>Neuron</source>
					<volume>19</volume>
					<fpage>431</fpage>
					<lpage>441</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b029">
<label>29</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Gad</surname>
							<given-names>H</given-names>
						</name>
						<name name-style="western">
							<surname>Ringstad</surname>
							<given-names>N</given-names>
						</name>
						<name name-style="western">
							<surname>Low</surname>
							<given-names>P</given-names>
						</name>
						<name name-style="western">
							<surname>Kjaerulff</surname>
							<given-names>O</given-names>
						</name>
						<name name-style="western">
							<surname>Gustafsson</surname>
							<given-names>J</given-names>
						</name><etal/>
					</person-group>
					<year>2000</year>
					<article-title>Fission and uncoating of synaptic clathrin-coated vesicles are perturbed by disruption of interactions with the sh3 domain of endophilin.</article-title>
					<source>Neuron</source>
					<volume>27</volume>
					<fpage>301</fpage>
					<lpage>312</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b030">
<label>30</label>
				<element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Van Vactor</surname>
							<given-names>D</given-names>
						</name>
						<name name-style="western">
							<surname>Flanagan</surname>
							<given-names>JG</given-names>
						</name>
					</person-group>
					<year>1999</year>
					<article-title>The middle and the end: Slit brings guidance and branching together in axon pathway selection.</article-title>
					<source>Neuron</source>
					<volume>22</volume>
					<fpage>649</fpage>
					<lpage>652</lpage>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b031">
<label>31</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Joachims</surname>
							<given-names>T</given-names>
						</name>
					</person-group>
					<year>1998</year>
					<article-title>Making large-scale support vector machine learning practical.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor">
						<name name-style="western">
							<surname>Schölkopf</surname>
							<given-names>B</given-names>
						</name>
						<name name-style="western">
							<surname>Burges</surname>
							<given-names>C</given-names>
						</name>
						<name name-style="western">
							<surname>Smola</surname>
							<given-names>A</given-names>
						</name>
					</person-group>
					<source>Advances in kernel methods: Support vector machines</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<comment>392 p.</comment>
				</element-citation>
			</ref>
			<ref id="pcbi-0020118-b032">
<label>32</label>
				<element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author">
						<name name-style="western">
							<surname>Ratnaparkhi</surname>
							<given-names>A</given-names>
						</name>
					</person-group>
					<year>1996</year>
					<article-title>A maximum entropy part-of-speech tagger.</article-title>
					<comment>In:</comment>
					<source>Empirical methods in natural language processing</source>
					<publisher-loc>Philadelphia</publisher-loc>
					<publisher-name>University of Pennsylvania</publisher-name>
					<comment>pp.</comment>
					<fpage>491</fpage>
					<lpage>497</lpage>
				</element-citation>
			</ref>
		</ref-list>
	</back>
</article>