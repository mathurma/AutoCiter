<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01441</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006041</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological processes</subject><subj-group><subject>Sleep</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological processes</subject><subj-group><subject>Sleep</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Management engineering</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject><subj-group><subject>Decision tree learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject><subj-group><subject>Decision tree learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Decision tree learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Management engineering</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Brain-state invariant thalamo-cortical coordination revealed by non-linear encoders</article-title>
<alt-title alt-title-type="running-head">Using Machine Learning to study in vivo neuronal data</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2450-7397</contrib-id>
<name name-style="western">
<surname>Viejo</surname> <given-names>Guillaume</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cortier</surname> <given-names>Thomas</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Peyrache</surname> <given-names>Adrien</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Montreal Neurological Institute, McGill University, 3801 University Street, Montreal, QC H3A 2B4, Canada</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>École Normale Supérieure, 45 Rue d’Ulm, 75005 Paris, France</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Luczak</surname> <given-names>Artur</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Univ of Lethbridge, CANADA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">adrien.peyrache@mcgill.ca</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>3</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>3</issue>
<elocation-id>e1006041</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>8</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Viejo et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006041"/>
<abstract>
<p>Understanding how neurons cooperate to integrate sensory inputs and guide behavior is a fundamental problem in neuroscience. A large body of methods have been developed to study neuronal firing at the single cell and population levels, generally seeking interpretability as well as predictivity. However, these methods are usually confronted with the lack of ground-truth necessary to validate the approach. Here, using neuronal data from the head-direction (HD) system, we present evidence demonstrating how gradient boosted trees, a non-linear and supervised Machine Learning tool, can learn the relationship between behavioral parameters and neuronal responses with high accuracy by optimizing the information rate. Interestingly, and unlike other classes of Machine Learning methods, the intrinsic structure of the trees can be interpreted in relation to behavior (e.g. to recover the tuning curves) or to study how neurons cooperate with their peers in the network. We show how the method, unlike linear analysis, reveals that the coordination in thalamo-cortical circuits is qualitatively the same during wakefulness and sleep, indicating a brain-state independent feed-forward circuit. Machine Learning tools thus open new avenues for benchmarking model-based characterization of spike trains.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The thalamus is a brain structure that relays sensory information to the cortex and mediates cortico-cortical interaction. Unraveling the dialogue between the thalamus and the cortex is thus a central question in neuroscience, with direct implications on our understanding of how the brain operates at the macro scale and of the neuronal basis of brain disorders that possibly result from impaired thalamo-cortical networks, such as absent epilepsy and schizophrenia. Methods that are classically used to study the coordination between neuronal populations are usually sensitive to the ongoing global dynamics of the networks, in particular desynchronized (wakefulness and REM sleep) and synchronized (non-REM sleep) states. They thus fail to capture the underlying temporal coordination. By analyzing recordings of thalamic and cortical neuronal populations of the HD system in freely moving mice during exploration and sleep, we show how a general non-linear encoder captures a brain-state independent temporal coordination where the thalamic neurons leading their cortical targets by 20-50ms in all brain states. This study thus demonstrates how methods that do not assume any models of neuronal activity may be used to reveal important aspects of neuronal dynamics and coordination between brain regions.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Canadian Research Chair</institution>
</funding-source>
<award-id>245716</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Peyrache</surname> <given-names>Adrien</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>Canadian Foundation for Innovation</institution>
</funding-source>
<award-id>35476</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Peyrache</surname> <given-names>Adrien</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Canadian Research Chair 245716 to AP, <ext-link ext-link-type="uri" xlink:href="http://www.chairs-chaires.gc.ca/home-accueil-eng.aspx" xlink:type="simple">http://www.chairs-chaires.gc.ca/home-accueil-eng.aspx</ext-link>, and Canadian Foundation for Innovation grant 35476 to AP, <ext-link ext-link-type="uri" xlink:href="https://www.innovation.ca/" xlink:type="simple">https://www.innovation.ca/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="25"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-04-03</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Neuronal recordings that are analyzed in this report are available for download (<ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/thalamus/th-1" xlink:type="simple">https://crcns.org/data-sets/thalamus/th-1</ext-link>). Code is available online in a raw form and as a Jupyter notebook to present some of the analyses (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/PeyracheLab/NeuroBoostedTrees" xlink:type="simple">http://www.github.com/PeyracheLab/NeuroBoostedTrees</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Investigating how the brain operates at the neuronal level is usually addressed by the specification of neuronal responses to an experimentally measurable variable or by the quantification of the temporal coordination of neuronal ensembles [<xref ref-type="bibr" rid="pcbi.1006041.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref002">2</xref>]. Using various methods, the responses of single neurons can be characterized by the tuning curves based on a single measurement (i.e. average firing rate as a function of the observed value) [<xref ref-type="bibr" rid="pcbi.1006041.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref005">5</xref>], with generalized linear models accounting for the coding of multiple features [<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref007">7</xref>], biophysical models of spike train generation [<xref ref-type="bibr" rid="pcbi.1006041.ref008">8</xref>] or information measures and reverse reconstruction [<xref ref-type="bibr" rid="pcbi.1006041.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref009">9</xref>].</p>
<p>The coding of information in the brain relies on the coordinated firing of neuronal population [<xref ref-type="bibr" rid="pcbi.1006041.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref012">12</xref>]. The development of dense electrode arrays [<xref ref-type="bibr" rid="pcbi.1006041.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref014">14</xref>] and imaging techniques [<xref ref-type="bibr" rid="pcbi.1006041.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref016">16</xref>] in awake animals now allows monitoring of the activity of large ensembles of neurons and to address fundamental questions about neuronal network coordination. Neuronal interactions, in relation to behavior or internal parameters (e.g. brain states), are evaluated by the statistical dependencies of spike trains, the most widely used method being linear cross-correlations [<xref ref-type="bibr" rid="pcbi.1006041.ref017">17</xref>]. These linear measures can be generalized to population correlation with tools such as Principal Component Analysis (PCA) [<xref ref-type="bibr" rid="pcbi.1006041.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref019">19</xref>] and Independent Component Analysis [<xref ref-type="bibr" rid="pcbi.1006041.ref020">20</xref>]. Generalized linear models were used to build predictions of single spike trains as a function of the peer network activity [<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>] and to provide a full statistical description of spatio-temporal neuronal responses and correlations [<xref ref-type="bibr" rid="pcbi.1006041.ref021">21</xref>]. Methods from graph theory offer ways to compare interactions at the network level across experimental conditions [<xref ref-type="bibr" rid="pcbi.1006041.ref022">22</xref>]. Finally, among the large body of available tools, evaluating neuronal coupling by fitting spiking activity to Ising models has provided key insights into the nature of neuronal coordination in a population [<xref ref-type="bibr" rid="pcbi.1006041.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref024">24</xref>].</p>
<p>The majority of the methods enumerated above rely on a set of assumptions regarding the statistics of the data or the biophysics of neuronal spiking, among others, while seeking explanatory power. To assert the validity of a particular approach, the usual procedure is to divide the data set into a <italic>training set</italic>, used to fit the model parameters, and a <italic>test set</italic>, on which the likelihood of the model is evaluated. However, this method, called cross-validation, does not rule out the possibility that a particular fit of the model parameters, even when leading to high likelihood, corresponds to the wrong model. For example, the omission of a key feature in the model may attribute erroneous contribution to the set of chosen variables. These limitations arise from the lack of ground-truth data that in the most complex (and, therefore, interesting) cases represent an unreachable goal.</p>
<p>This lack of ground-truth data when performing data analysis is particularly unavoidable in neuroscience [<xref ref-type="bibr" rid="pcbi.1006041.ref025">25</xref>]. It has thus become necessary to establish standard, model-free methods that, even if they do not contribute to our understanding of the data, set levels of performance that may be used to benchmark model-based approaches [<xref ref-type="bibr" rid="pcbi.1006041.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref027">27</xref>]. Machine Learning provides a large array of techniques to classify datasets that have demonstrated high level of performance in fields ranging from image processing to astrophysics [<xref ref-type="bibr" rid="pcbi.1006041.ref028">28</xref>]. Using a supervised classifier, so-called <italic>gradient boosting</italic> [<xref ref-type="bibr" rid="pcbi.1006041.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref027">27</xref>], we show how this method can determine an encoding model for predicting population spike trains knowing the stimulus input. We also show, in line with recently published work [<xref ref-type="bibr" rid="pcbi.1006041.ref027">27</xref>], how gradient boosted trees (XGB) can also be used as a very efficient decoding model that is retrieving the stimulus likelihood knowing the spiking activity of a population of neurons. Finally, we demonstrate how it generates a very accurate encoding model for predicting a population spike train conditioned on another, anatomically projected, set of neuronal activity [<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>].</p>
<p>We tested the validity of the approach on data from the head-direction (HD) system [<xref ref-type="bibr" rid="pcbi.1006041.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>], a sensory pathway whose member neurons, the so-called <italic>HD cells</italic>, emit spike trains that can be explained with high accuracy simply by the direction of the head of the animal in the horizontal plane. Decision trees maximized their branching in input ranges where Fisher Information was maximal. We then determined the optimal parameters of the method for our data set. Finally, we applied this method to simultaneously recorded neurons in the thalamo-cortical network of the HD system, namely in the antero-dorsal nucleus of the thalamus (ADn) and the Post-subiculum (PoSub). We demonstrate that non-linear encoders such as boosted gradients, but not linear analysis, reveal that thalamic neurons lead cortical neurons in a brain-state independent manner.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Gradient boosted trees</title>
<p>Machine Learning literature defines boosting as the combination of many weak classifiers with limited prediction performances in order to build a stronger classifier. The first boosting algorithm is AdaBoost (Adaptive Boosting) [<xref ref-type="bibr" rid="pcbi.1006041.ref031">31</xref>] which trains weak learners using a distribution of weight over the training set. This distribution of weight is updated after the convergence of a weak learner in order for the next weak learner to focus on the difficult examples i.e. the points that are hard to classify.</p>
<p>Boosting algorithms come in different flavors for the type of learners or the updating of the weights [<xref ref-type="bibr" rid="pcbi.1006041.ref032">32</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref033">33</xref>]. Here we focused on the boosting using the decision tree model as the weak learner. The goal of the gradient boosted trees algorithm is to determine the optimal successive partition of features space in order to assign a weight or a label to a subset of the training examples. This algorithm is thus equivalent to <italic>decision trees</italic> in which input features are optimally segmented to determine a desired output. The problem is now to apply this reasoning to predict the spiking of neurons based on behavioral features and, conversely, to decode behavioral feature from a population of neurons coding for an internal representation of this feature. Lastly, this algorithms can be useful to predict the spike train of a given neuron from the spiking activity of an upstream neuronal population.</p>
<p>Practically, we first defined the training set [(<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>),…,(<italic>x</italic><sub><italic>m</italic></sub>, <italic>y</italic><sub><italic>m</italic></sub>)] where <italic>x</italic><sub><italic>i</italic></sub> ∈ <italic>R</italic><sup><italic>d</italic></sup> is the i-th training example with <italic>d</italic> different features and <italic>y</italic><sub><italic>i</italic></sub> is the target value. In this study, we focus on two different types of features: (1) behavioral features, in particular the HD and position of the animals and (2) spiking activity of neuronal ensembles. The goal of the learner reduces to how to make an accurate prediction <inline-formula id="pcbi.1006041.e001"><alternatives><graphic id="pcbi.1006041.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> given <italic>x</italic><sub><italic>i</italic></sub> and the correct value <italic>y</italic><sub><italic>i</italic></sub>. A target value <italic>y</italic><sub><italic>i</italic></sub> for a given training example <italic>x</italic><sub><italic>i</italic></sub> is a spike count over a finite time bin for one neuron. Assuming neuronal spiking follows an inhomogeneous Poisson distribution, we thus defined the prediction of the model as:
<disp-formula id="pcbi.1006041.e002"><alternatives><graphic id="pcbi.1006041.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>k</mml:mi> <mml:mo>|</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mo>λ</mml:mo> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>!</mml:mo></mml:mrow></mml:mfrac> <mml:msup><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
for a given intensity parameter λ = λ(<italic>x</italic><sub><italic>i</italic></sub>), the single parameter of a Poisson distribution. We defined <inline-formula id="pcbi.1006041.e003"><alternatives><graphic id="pcbi.1006041.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> for each training example as the prediction of the learning algorithm. This value corresponds to the mean of the predicted Poisson distribution.</p>
<p>The measure of the performance of the model is made through an objective function <italic>O</italic>(<italic>θ</italic>) = <italic>L</italic>(<italic>θ</italic>) + Ω(<italic>θ</italic>) that sums the training loss <italic>L</italic> and the regularization term (penalty for complexity) Ω. The training loss to be minimized is then defined as the negative log-likelihood over the full set:
<disp-formula id="pcbi.1006041.e004"><alternatives><graphic id="pcbi.1006041.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mo>-</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
also known as the Poisson loss.</p>
<p>For the regularization term Ω, the complexity of the tree set was defined as
<disp-formula id="pcbi.1006041.e005"><alternatives><graphic id="pcbi.1006041.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:mo>Ω</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:mi>T</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mover accent="true"><mml:mo>λ</mml:mo> <mml:mo>¯</mml:mo></mml:mover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>T</italic> is the total number of leaves and <italic>w</italic><sub><italic>j</italic></sub> the score of leaf <italic>j</italic>. <italic>γ</italic> and <inline-formula id="pcbi.1006041.e006"><alternatives><graphic id="pcbi.1006041.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mo>λ</mml:mo> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are two free parameters weighting the contribution of the two previous items in the objective function. For the sake of comparison with a related study [<xref ref-type="bibr" rid="pcbi.1006041.ref027">27</xref>], we used the same values: <italic>γ</italic> = 0.4 and <inline-formula id="pcbi.1006041.e007"><alternatives><graphic id="pcbi.1006041.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mover accent="true"><mml:mo>λ</mml:mo> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. However, in the following section detailing the methods, we keep these two parameters as variables.</p>
<p>To minimize the objective function, the learning algorithm must find the optimal set of split values and the optimal set of leaf values for each tree. An efficient strategy is thus to optimize trees sequentially i.e. the input of a tree is the output of the previous tree. After optimizing the <italic>t</italic> − 1 trees, the prediction at tree <italic>t</italic> is <inline-formula id="pcbi.1006041.e008"><alternatives><graphic id="pcbi.1006041.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> <italic>f</italic><sub><italic>t</italic></sub> the function that maps the <italic>x</italic><sub><italic>i</italic></sub> example onto the right leaf through the succession of tree partition.</p>
<p>By taking advantage of the fact that the same score is assigned to all the input data that fall into the same leaf, the objective function can be transformed from a sum over the training set to a sum over the leaves set:
<disp-formula id="pcbi.1006041.e009"><alternatives><graphic id="pcbi.1006041.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:msup><mml:mi>O</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>≈</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mi>t</mml:mi></mml:msup></mml:munderover> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:munder> <mml:msub><mml:mi>g</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>w</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:msubsup><mml:mi>I</mml:mi> <mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:munder> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pcbi.1006041.e010"><alternatives><graphic id="pcbi.1006041.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:mo>+</mml:mo> <mml:msup><mml:mrow><mml:mi>γ</mml:mi> <mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
The index function <italic>I</italic><sub><italic>j</italic></sub> = {<italic>i</italic>|<italic>f</italic>(<italic>x</italic><sub><italic>i</italic></sub>) = <italic>w</italic><sub><italic>j</italic></sub>} maps each training point <italic>x</italic><sub><italic>i</italic></sub> to the corresponding leaf <italic>j</italic> while <italic>g</italic><sub><italic>i</italic></sub> and <italic>h</italic><sub><italic>i</italic></sub> are respectively the first order and second order derivatives of the loss function. In the case of Poisson regression, the <italic>g</italic><sub><italic>i</italic></sub> and <italic>h</italic><sub><italic>i</italic></sub> are defined as:
<disp-formula id="pcbi.1006041.e011"><alternatives><graphic id="pcbi.1006041.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:msub><mml:mi>g</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:msup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula> <disp-formula id="pcbi.1006041.e012"><alternatives><graphic id="pcbi.1006041.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:msup></mml:math></alternatives> <label>(7)</label></disp-formula>
Finally, the sum of <italic>w</italic><sub><italic>j</italic></sub> and <inline-formula id="pcbi.1006041.e013"><alternatives><graphic id="pcbi.1006041.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1006041.e010">Eq 5</xref> is quadratic, which allows us to compute the optimal <inline-formula id="pcbi.1006041.e014"><alternatives><graphic id="pcbi.1006041.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and the corresponding best objective value
<disp-formula id="pcbi.1006041.e015"><alternatives><graphic id="pcbi.1006041.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:msup><mml:mi>O</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:munderover> <mml:mfrac><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi>γ</mml:mi> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>v</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
with <inline-formula id="pcbi.1006041.e016"><alternatives><graphic id="pcbi.1006041.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:msub><mml:mi>g</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006041.e017"><alternatives><graphic id="pcbi.1006041.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:msub><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>The best tree structure is then found by sequentially splitting the features space, with each splitting position corresponding to the maximum gain:
<disp-formula id="pcbi.1006041.e018"><alternatives><graphic id="pcbi.1006041.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:mi>G</mml:mi> <mml:mi>a</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>L</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>R</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mi>R</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>L</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow> <mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mi>R</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>L</mml:mi></mml:msub> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>γ</mml:mi></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
The gain for one split is a measure of fit improvement. It is the difference between the scores of the new leaves (subscripts <italic>R,L</italic>: right and left leaves, respectively) after the split and the score of the previous leaf. Details of the derivative steps and full explanations of the algorithm can be found in [<xref ref-type="bibr" rid="pcbi.1006041.ref034">34</xref>].</p>
<p>An example of the gradient boosted trees algorithm is shown in <xref ref-type="fig" rid="pcbi.1006041.g001">Fig 1</xref> for a non-linear tuning curve (blue curves Y). For each tree sequentially optimized (1,2 and 10 shown), the algorithm splits the tuning curve at different positions (X0, X1, X2, X3,…) and assigns a leaf score between each split. By iterating this procedure, the predicted firing rate (black curves <inline-formula id="pcbi.1006041.e019"><alternatives><graphic id="pcbi.1006041.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mover accent="true"><mml:mi>Y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) progressively converges to the actual firing rate.</p>
<fig id="pcbi.1006041.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Predicting the firing rate of a cell with gradient boosted trees.</title>
<p>Each row corresponds to the learning of one tree by the algorithm. The tuning curve is sequentially split as shown on the left figures (vertical lines; blue line displays the actual tuning curve and black lines correspond to the prediction). Thus, intervals between each pair of splits are assigned a different target value. The first two trees are shown on the right and the exact values of each leaf are indicated in the square boxes. Note that the predicted firing rates are the sum over all the leaves (i.e. the value of a single leaf can not be directly interpreted.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Scoring function</title>
<p>To estimate the quality of a model, we used the pseudo-<italic>R</italic><sup>2</sup> score:
<disp-formula id="pcbi.1006041.e020"><alternatives><graphic id="pcbi.1006041.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mi>p</mml:mi> <mml:msup><mml:mi>R</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mtext>log</mml:mtext> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mtext>log</mml:mtext> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mtext>log</mml:mtext> <mml:mi>y</mml:mi> <mml:mo>-</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mtext>log</mml:mtext> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
with <italic>y</italic> the target firing rate, <inline-formula id="pcbi.1006041.e021"><alternatives><graphic id="pcbi.1006041.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> the prediction, <inline-formula id="pcbi.1006041.e022"><alternatives><graphic id="pcbi.1006041.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> the mean firing rate [<xref ref-type="bibr" rid="pcbi.1006041.ref035">35</xref>]. A value of 1 indicates a perfect model that reproduces entirely the dataset while a value of 0 indicates a model that is no better than the average value of the training set.</p>
<p>To compute the pseudo-<italic>R</italic><sup>2</sup> score, the data set was divided into a training set and a test set, a procedure known as cross-validation, that prevents the model from over-fitting the training set. For all the predictions of firing rates, we used an 8-fold cross-validation, i.e the training set was divided into 8 discontinuous partitions with each one serving successively as the testing set. For each spiking activity predicted for one neuron, this procedure yields eight <italic>pR</italic><sup>2</sup> that were averaged. This mean <italic>pR</italic><sup>2</sup> served as a measure of performance of different techniques that were tested.</p>
</sec>
<sec id="sec005">
<title>Model comparison</title>
<p>In the present manuscript, we compare the prediction performance of XGB with three other methods. To this end, we computed the pseudo-<italic>R</italic><sup>2</sup> obtained with each method in an 8-fold cross-validation procedure. First, we tested a linear regression model between the animal’s HD and the binned spike trains. However, this method necessarily fails as the relation between the HD (an angular value) and the number of spikes emitted by HD cells is, in general, not linear. Therefore, we next linearized the HD by projecting the HD angular values on the first six harmonics of 2<italic>pi</italic> (called the 6<sup><italic>th</italic></sup> order kernel in <xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2B</xref>) and performed a linear regression with binned spike trains. Thus, a training point <italic>x</italic><sub><italic>i</italic></sub> corresponding to the direction <italic>θ</italic><sub><italic>i</italic></sub> is defined as a 12-dimensional input vector: <italic>x</italic><sub><italic>i</italic></sub> = […, <italic>cos</italic>(<italic>kθ</italic><sub><italic>i</italic></sub>), <italic>sin</italic>(<italic>kθ</italic><sub><italic>i</italic></sub>),…] for <italic>k</italic> in [<xref ref-type="bibr" rid="pcbi.1006041.ref001">1</xref>,…,<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>]. Finally, we tested a ‘model-based’ method: the tuning curve of a given HD neuron was computed from the training set and then used to predict the firing rate of the neuron in the test set.</p>
<fig id="pcbi.1006041.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Comparing gradient boosted trees (XGB) with classical methods.</title>
<p><bold>A</bold> Using the angle as the input feature (red), the Machine Learning algorithm is trained to minimize the error in predicting the firing rate of one HD neuron over time (yellow, spiking activity below) during the training phase. For each angular position in the test set, the algorithm predicts a firing rate (blue curve). The score of the algorithm measures how close the prediction is to the real value. <bold>B</bold> Using an 8-fold cross-validation, XGB was compared to model-based tuning curves (MB) with 60 bins, a linear regression model and a linear regression model with preprocessing of the features i.e the first six harmonics of the angular direction of the head were used instead of the raw angle. Recordings from ADn and PoSub were used to benchmark each model. <bold>C</bold> To find the optimal number of trees and the optimal depth of XGB, a grid-search was performed for each neuron using the Bayesian Information Criterion (BIC). <bold>D</bold> Distribution of the set of optimal parameters for all neurons. Overall, a maximum number of 100 trees with a depth of 5 was used to learn and predict spiking activity as in <bold>A</bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Fisher Information</title>
<p>Fisher Information (FI) is directly related to the variance of the most optimal decoder and can be computed, under the assumption of a Poisson Process, directly from the tuning curve [<xref ref-type="bibr" rid="pcbi.1006041.ref036">36</xref>]. For recall, <italic>FI</italic>(<italic>x</italic>) = (<italic>df</italic>/<italic>dx</italic>)<sup>2</sup>/<italic>f</italic>(<italic>x</italic>) with f(x) the firing rate at position <italic>x</italic> of the input feature. In practice, the Fisher Information was reduced to the squared slope of the line fitted between three successive bins of the tuning curve divided by the firing rate of the middle bin.</p>
</sec>
<sec id="sec007">
<title>Dataset</title>
<p>Neuronal recordings that are analyzed in this report were described in a previously published paper [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>] and are available for download (<ext-link ext-link-type="uri" xlink:href="https://crcns.org/data-sets/thalamus/th-1/" xlink:type="simple">https://crcns.org/data-sets/thalamus/th-1/</ext-link>). Briefly, multi-site silicon probes (Buzsaki32 and Buzsaki64 from Neuronexus) were inserted over the antero-dorsal nucleus (ADn) of the thalamus in 7 mice. In three of these animals, a second probe was lowered to the post-subiculum (PoSub).</p>
<p>During the recording session, neurophysiological signals were acquired continuously at 20 kHz on a 256-channel Amplipex system (Szeged; 16-bit resolution, analog multiplexing). The wide-band signal was downsampled to 1.25 kHz and used as the local-field potential signal. To track the position of the animals in the open maze and in their home cage during rest epochs, two small light-emitting diodes (LEDs; 5-cm separation), mounted above the headstage, were recorded by a digital video camera at 30 frames per second. The LED locations were detected online and resampled at 39 Hz by the acquisition system. Spike sorting was performed semi-automatically, using KlustaKwik (<ext-link ext-link-type="uri" xlink:href="http://klustakwik.sourceforge.net/" xlink:type="simple">http://klustakwik.sourceforge.net/</ext-link>). This was followed by manual adjustment of the waveform clusters using the software Klusters (<ext-link ext-link-type="uri" xlink:href="http://neurosuite.sourceforge.net/" xlink:type="simple">http://neurosuite.sourceforge.net/</ext-link>).</p>
<p>In animals implanted over the antero-dorsal nucleus, the thalamic probe was lowered until the first thalamic units could be detected on at least 2-3 shanks. The thalamic probe was then lowered by 70-140 <italic>μ</italic>m at the end of each session. In the animals implanted in both the thalamus and in the post-subiculum, the subicular probe was moved everyday once large HD cell ensembles were recorded from the thalamus. Thereafter, the thalamic probes were left at the same position for as long as the quality of the recordings remained high. They were subsequently adjusted to optimize the yield of HD cells. To prevent statistical bias of neuron sampling, we discarded sessions from analysis that were separated by less than 3 days during which the thalamic probe was not moved.</p>
</sec>
<sec id="sec008">
<title>Data analysis</title>
<p>In all analyses, spike trains were binned in 25 ms bins and smoothed with a 125 ms kernel, unless stated otherwise. The only exception is for decoding which was performed with bins of 200 ms. The animal’s HD was calculated by the relative orientation of two LEDs (blue and red) located on top of the head (see [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>] for more details). The HD tuning curve of a neuron is the ratio between the histogram of spike counts as a function of HD (60 bins between 0 and 2<italic>π</italic>) and total time spent in each bin of HD. For a given angular bin <italic>ϕ</italic><sub><italic>i</italic></sub>, the average firing rate is thus:
<disp-formula id="pcbi.1006041.e023"><alternatives><graphic id="pcbi.1006041.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mfrac> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>t</mml:mi></mml:msub> <mml:msub><mml:mi>n</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>t</mml:mi></mml:msub> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>δ</italic>(<italic>ϕ</italic><sub><italic>i</italic></sub>, <italic>ϕ</italic><sub><italic>t</italic></sub>) = 1 if, at time <italic>t</italic>, the angular HD <italic>ϕ</italic><sub><italic>t</italic></sub> is equal to <italic>ϕ</italic><sub><italic>i</italic></sub> (<italic>δ</italic>(<italic>ϕ</italic><sub><italic>i</italic></sub>, <italic>ϕ</italic><sub><italic>t</italic></sub>) = 0 otherwise), <italic>n</italic><sub><italic>t</italic></sub> the number of spikes counted in the <italic>t</italic>th time bin and T = 25ms (the time bin duration).</p>
</sec>
<sec id="sec009">
<title>Bayesian decoding</title>
<p>The goal of Bayesian decoding in this study is to predict the HD of the animal given the spiking activity of recorded neurons. Let <bold>n</bold> = (<italic>n</italic><sub>1</sub>, <italic>n</italic><sub>2</sub>,…,<italic>n</italic><sub><italic>N</italic></sub>) be the numbers of spikes fired by the HD neurons within a given time window (200 ms) and Φ the set of possible angular direction between 0 and 2<italic>π</italic>. The algorithm computes the probability <italic>P</italic>(Φ|<bold>n</bold>) using the classical formula of conditional probability:
<disp-formula id="pcbi.1006041.e024"><alternatives><graphic id="pcbi.1006041.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>|</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
Assuming the statistical independence of HD neurons and the Poisson distributions of their spikes, the probability <italic>P</italic>(<bold>n</bold>|Φ) can be evaluated as:
<disp-formula id="pcbi.1006041.e025"><alternatives><graphic id="pcbi.1006041.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">n</mml:mi> <mml:mo>|</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>τ</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msup> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>!</mml:mo></mml:mrow></mml:mfrac> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:msup><mml:mi>p</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>τ</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
with <italic>τ</italic> the length of the time window and <italic>f</italic><sub><italic>i</italic></sub>(Φ) the average firing rate of cell <italic>i</italic> at position Φ. The full detail of the algorithm can be found in [<xref ref-type="bibr" rid="pcbi.1006041.ref037">37</xref>].</p>
<p>When using XGB for decoding the HD, we set the algorithm to do multiclass classification: the algorithm returns the predicted probabilities that population vector <italic>n</italic> (a vector of spike count of each neuron) belongs to each ‘class’ Φ = (<italic>ϕ</italic><sub>1</sub>, <italic>ϕ</italic><sub>2</sub>,…,<italic>ϕ</italic><sub><italic>k</italic></sub>), i.e. 60 bins of HD. Briefly, learning of the decoder is achieved by minimizing the so-called ‘logarithmic loss’ computed as <inline-formula id="pcbi.1006041.e026"><alternatives><graphic id="pcbi.1006041.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>g</mml:mi> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where <italic>N</italic> is the number of data points, <italic>K</italic> the number of classes (60 bins of HD in our case), <inline-formula id="pcbi.1006041.e027"><alternatives><graphic id="pcbi.1006041.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msub><mml:mi mathvariant="bold">n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> if the data point <bold>n</bold><sub><italic>i</italic></sub> is in class <italic>ϕ</italic><sub><italic>j</italic></sub> and 0 otherwise, and <italic>p</italic>(<bold>n</bold><sub><italic>i</italic></sub> ∈ <italic>ϕ</italic><sub><italic>j</italic></sub>) is the predicted probability that observation <bold>n</bold><sub><italic>i</italic></sub> is in class <italic>ϕ</italic><sub><italic>j</italic></sub>. Thus, a perfect classifier would have a null log loss (for each data point, there is one and only one class that has a probability <italic>p</italic> = 1 and that is correctly labeled, i.e. <italic>y</italic> = 1).</p>
</sec>
<sec id="sec010">
<title>Spiking network simulation</title>
<p>To attest the robustness of our analyses, the methods presented in this study were tested on an emulation of spiking neuronal ensembles using the Brian simulator [<xref ref-type="bibr" rid="pcbi.1006041.ref038">38</xref>]. The network is composed of two layers of Poisson spiking neurons (<italic>P</italic><sub><italic>ADn</italic></sub> and <italic>P</italic><sub><italic>PoSub</italic></sub>) and one layer of integrate-and-fire neurons (<italic>I</italic><sub><italic>PoSub</italic></sub>). Poisson spiking neurons were individually parameterized by angular tuning curves. We used the actual HD of an exploration session (20 min) to generate a time-array of firing rate per neuron, at every time step of the simulation.</p>
<p>Integrate-and-fire neurons follow a stochastic differential equation:
<disp-formula id="pcbi.1006041.e028"><alternatives><graphic id="pcbi.1006041.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>v</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>v</mml:mi> <mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
with the membrane time constant <italic>τ</italic> = 50<italic>ms</italic> for all simulations. We set the spiking voltage threshold <italic>v</italic> = 1 and after-spike reset to <italic>v</italic> = 0 and no refractory period.</p>
<p>The simulated integrate-and-fire neurons <italic>I</italic><sub><italic>PoSub</italic></sub>, emulating spiking activity of observed PoSub neurons, received two sets of inputs. First, an input mimicking their actual tuning curve, each <italic>I</italic><sub><italic>PoSub</italic></sub> neuron receiving a connection from one <italic>P</italic><sub><italic>PoSub</italic></sub> neuron with a weight of 0.9. In other words, each integrate-and-fire <italic>I</italic><sub><italic>PoSub</italic></sub> neuron had a unique mirror Poisson spiking neuron in the <italic>P</italic><sub><italic>PoSub</italic></sub> layer that provides major driving input depending on the angular HD. The second set of synapses to <italic>I</italic><sub><italic>PoSub</italic></sub> were from a population mimicking ADn neurons, <italic>P</italic><sub><italic>ADn</italic></sub>, with full connectivity (i.e. <italic>I</italic><sub><italic>PoSub</italic></sub> receives inputs from all <italic>P</italic><sub><italic>ADn</italic></sub> neurons). The weights of the connections from <italic>P</italic><sub><italic>ADn</italic></sub> units and a given <italic>I</italic><sub><italic>PoSub</italic></sub> neuron were parameterized by the angular distance between the preferred direction of the <italic>I</italic><sub><italic>ADn</italic></sub> and its pre-synaptic <italic>P</italic><sub><italic>ADn</italic></sub> neurons. More specifically, for two neurons <italic>i</italic> and <italic>j</italic> with respective preferred angular directions <italic>ϕ</italic><sub><italic>i</italic></sub> and <italic>ϕ</italic><sub><italic>j</italic></sub>, the synaptic weight is defined as:
<disp-formula id="pcbi.1006041.e029"><alternatives><graphic id="pcbi.1006041.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
with <italic>α</italic> = 0.1 and <italic>β</italic> = 10.</p>
</sec>
<sec id="sec011">
<title>Code availability</title>
<p>The analyses presented in this report were run on Matlab (Mathworks, 2017) and Python. Code is available online in a raw form and as a Jupyter notebook to present some of the analyses (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/PeyracheLab/NeuroBoostedTrees" xlink:type="simple">www.github.com/PeyracheLab/NeuroBoostedTrees</ext-link>). Gradient boosting was implemented with the XGBoost toolbox [<xref ref-type="bibr" rid="pcbi.1006041.ref034">34</xref>].</p>
</sec>
</sec>
<sec id="sec012" sec-type="results">
<title>Results</title>
<sec id="sec013">
<title>Gradient boosted trees predict firing rates with raw features</title>
<p>We applied gradient boosted trees (XGB) to the prediction of spike counts from HD neurons recorded in ADn and PoSub (see <xref ref-type="sec" rid="sec002">Methods</xref> and <xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2A</xref> for a full display of the training process). Since the HD signal is a well-characterized signal relative to the angular direction of the animal’s head, we compared the prediction of XGB with the output of the model-based (MB) tuning curve (that is, the firing rate expected from the HD of the animal knowing the tuning curve; see <xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2B</xref>). The comparison shows that XGB reaches the same level of performance as MB for both ADn and PoSub. We then tested a generalized linear regression model with raw HD values or a 6<sup><italic>th</italic></sup> order kernel. In the first case, the model learns only from the angular features <italic>θ</italic> ranging from 0 to 2<italic>π</italic>. In the second case, the model learns with all the k harmonics (<italic>cosθ</italic>, <italic>sinθ</italic>,…, <italic>coskθ</italic>, <italic>sinkθ</italic>). A 6th order projection was used as it can fit the typical width of a HD cell tuning curve (approximatively 60 degrees at half peak). Not surprisingly, the simple linear model showed negative or null performances for both anatomical structures, because the relationship between a raw angular value and a binned spike train is unlikely linear (<xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2B</xref>). Preprocessing of the angular feature (with the 6<sup><italic>th</italic></sup> order kernel) increased the performance to the same levels as XGB and MB.</p>
<p>In comparison with XGB, linear models and MB are straightforward models in terms of numbers of free parameters. We thus performed a grid-search to find the optimal number of trees and depth of each tree to find the best estimate of the performance, measured by the <italic>pseudo</italic>−<italic>R</italic><sup>2</sup> (see <xref ref-type="sec" rid="sec002">Methods</xref>). A Bayesian Information Criterion (BIC) score (<xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2C</xref>) was used to compare grid points. The BIC score was defined as <italic>BIC</italic>(|<italic>Trees</italic>|, <italic>Depth</italic>) = (|<italic>Trees</italic>| + <italic>Depth</italic>)<italic>log</italic>(<italic>n</italic>) − 2<italic>log</italic>(<italic>L</italic>) with <italic>n</italic> the number of time steps in the data training set and L the likelihood of the model. By penalizing more complex models using this approach, we found that 100 trees with a maximal depth of 5 were sufficient to predict spike trains for all neurons (<xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2C and 2D</xref>).</p>
</sec>
<sec id="sec014">
<title>Decoding of brain signals</title>
<p>Once the relationship between a behavioral feature and spiking activity has been learned, XGB can be used to decode the internal representation of this feature based on population spiking activity. We thus tested its performance on the decoding of the HD signal distributed over population of HD cells. To this end, spiking activity was binned in 200ms windows and XGB was trained and compared to a Bayesian decoding method, a technique widely used for such tasks [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref037">37</xref>], that predicts the probability of having a particular HD at each time step based on the instantaneous spike count in the population. For both algorithms, 60 angular bins were used to predict the HD. We parametrized the gradient boosted trees to use the multi-class log-loss that outputs a probability of being in a certain class or not (see <xref ref-type="sec" rid="sec002">Methods</xref>).</p>
<p>We decoded the HD signal in sessions that contained more than 7 neurons in both ADn and PoSub (n = 5 sessions, two animals). An example of 30 second decoding for XGB is shown in <xref ref-type="supplementary-material" rid="pcbi.1006041.s001">S1A Fig</xref>. Gradient boosted trees and Bayesian decoding show similar performances when using ADn activity as a feature while gradient boosted trees slightly outperforms Bayesian decoding for PoSub activity (<xref ref-type="supplementary-material" rid="pcbi.1006041.s001">S1B Fig</xref>). In addition, we observed that the decoding of the HD from ADn firing rate outperforms the decoding of the head direction using PoSub activity. This observation was consistent for both methods.</p>
</sec>
<sec id="sec015">
<title>Information content of the feature space is revealed by data splitting</title>
<p>Gradient boosting, as most Machine Learning tools, can be considered a black box that achieves high levels of performance while the particular details of the learning procedure remain unknown. However, it is possible to retrieve the thresholds at which trees split the data to predict the target output (as shown in <xref ref-type="fig" rid="pcbi.1006041.g001">Fig 1</xref>). In the case of HD cells, whose firing was directly predicted from the HD of the animals, splits concentrated on HD values where the tuning curves were the steepest (see examples of <xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3A</xref>). In fact, the density of splits is strongly correlated with the Fisher Information (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3B</xref>), a measure that is related, but not equal, to tuning curve steepness and that estimates the variance of an optimal decoder [<xref ref-type="bibr" rid="pcbi.1006041.ref036">36</xref>].</p>
<fig id="pcbi.1006041.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Segmentation of behavioral features to predict neuronal spiking.</title>
<p><bold>A</bold> Tuning-curve splitting for one neuron of the antero-dorsal nucleus (ADn) and one neuron of the post-subiculum (PoSub). Each vertical gray line is a split from the gradient boosted trees used to predict firing rate. Dashed black lines indicate Fisher Information (computed from the tuning curves). <bold>B</bold> Density of angular splits for ADn and PoSub for all the neurons, and average (thick line). Splits positions were realigned relative to the peak of the tuning curve. Horizontal dashed lines display chance levels. Insets show the distribution of correlation coefficients between Fisher Information and density of splits. <bold>C</bold> Using <italic>x</italic> and <italic>y</italic> coordinates of the animal in the environment as additional input features of the algorithm. Colored lines indicate spatial positions of splits along <italic>x</italic> and <italic>y</italic>. Gray lines indicate a short segment of the trajectory of the animal during the example session. <bold>D</bold> Density of splits for <italic>x</italic> and <italic>y</italic> position features for all neurons. The highest density is shown in black. <bold>E</bold> Left. proportion of splits for the three input features (head direction, x position and y position for ADn and PoSub. Right. Mean gain value for the three input features (head direction, x position and y position for ADn and PoSub) <bold>F</bold> Same as <bold>E</bold> for the gain value except that the firing rate for each neuron was generated from the angular tuning curve (left) or the spatial tuning curve (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g003" xlink:type="simple"/>
</fig>
<p>Many neurons of the brain’s navigation system exhibit correlates to more than one behavioral parameters, for example HD and place [<xref ref-type="bibr" rid="pcbi.1006041.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref041">41</xref>]. We thus predicted spike trains based on the three observed behavioral features, assuming they were independent: x and y positions of the animal randomly foraging in the environment, as well as the HD. We thus increased the feature space and dissected the resulting splitting distribution of the gradient boosted trees. In average, the density of splits along the (<italic>x</italic>, <italic>y</italic>) coordinates was the highest in the corner of the environment (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3C and 3D</xref>) where animals naturally spend a large amount of time. Analysis of the distribution of splits reveals that the HD feature was more segmented than the (<italic>x</italic>, <italic>y</italic>) coordinates for both ADn and PoSub (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3E</xref>, <italic>left</italic>), showing that HD neurons in both ADn and PoSub are primarily driven by HD. Nevertheless, we observed that the proportion of positive splits relative to angular splits was slightly higher for PoSub when compared to ADn.</p>
<p>One potential issue with this approach is that training a large number of trees overfits the learning procedure: it is optimal for decoding performance but not necessarily for the interpretability of the tree structure. To best explain the contribution of various features to the spiking activity, it is sometimes more suited to concentrate on the structure of a smaller number of trees, and examine the ‘gain’ of each feature when training the first trees. In fact, the average gain (see <xref ref-type="disp-formula" rid="pcbi.1006041.e018">Eq 9</xref>) for each feature decreases exponentially as the number of trees increases (<xref ref-type="supplementary-material" rid="pcbi.1006041.s002">S2 Fig</xref>). In addition, we found that random features were also more split as the number of trees increased (<xref ref-type="supplementary-material" rid="pcbi.1006041.s003">S3 Fig</xref>). For all these reasons, we restricted our analyses to the characteristic decay constant of the gain as a function of number of trees (<xref ref-type="supplementary-material" rid="pcbi.1006041.s002">S2 Fig</xref>), i.e. 30 trees with a depth of 2.</p>
<p>Shifting from split density to gain analysis, we thus demonstrate that the gain of spatial features (x and y position) was approximatively three times higher for PoSub neurons compared to ADn neurons (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3E</xref> <italic>right</italic>), in agreement with previous studies that employed model-based methods (i.e. that assumed various properties of spike trains and sampling of the feature space) [<xref ref-type="bibr" rid="pcbi.1006041.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref040">40</xref>]. To assess that the advantage of angular information over spatial information was not caused by a difference in the trajectories of the animals (i.e sub-sampling of some portions of the 2 dimensional space), we generated, for each neuron, artificial spike trains sampled from either the angular or spatial tuning curves. In the case of angular tuning curve sampling, we found qualitatively the same gains for PoSub and ADn neurons (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3F</xref>, <italic>left</italic>). When sampling the spatial tuning curves to generate artificial spike trains, gains for spatial features were higher than for HD, as expected (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3F</xref>, <italic>right</italic>). However, the difference with HD gains was small, and the gains were not different for ADn and PoSub neurons, indicating that the place fields of these two classes of neurons do not convey much spatial information. Thus, we concluded that XGB, when used appropriately, is an efficient method for determining the relative contribution of various features to a series of spike trains.</p>
</sec>
<sec id="sec016">
<title>Performances of peer-prediction</title>
<p>Brain functions arise from the communication of neurons with their peers in local and downstream networks. However, how these interactions take place remains largely unknown. With this question in mind, we thus applied XGB to neuronal peer-prediction, that is learning to estimate the spiking activity of one neuron as a function of the activity of a population of other, presumably anatomically-related neurons ([<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>]). For each session that contained at least 7 neurons in both ADn and PoSub, the model learned all possible group combinations (ADn to ADn, PoSub to ADn, PoSub to PoSub, ADn to PoSub). This learning was performed with no spike history, i.e. the bins used as features were synchronous to the bin predicted. For intra-group prediction, the target neuron was removed from the pool of feature neurons. Tested during wake, REM and non-REM sleep, we found that peer-prediction had the highest prediction score between ADn neurons and the lowest score between PoSub neurons (<xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4A</xref>). Inter-group predictions were similar. In all cases, scores during non-REM sleep were systematically lower than during wakefulness and REM, in agreement with previous analysis of peer-prediction in thalamo-cortical assemblies [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>].</p>
<fig id="pcbi.1006041.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Peer-prediction between ADn and PoSub.</title>
<p><bold>A</bold> Two conditions were tested: prediction between neurons of the same population (ADn⇒ADn and PoSub⇒PoSub) and prediction using neurons of the other population (PoSub⇒ADn and ADn⇒PoSub). Only sessions with at least 7 neurons in each population were included (2 animals). Peer-prediction was then tested during wake (plain bars), REM sleep (dashed bars) and non-REM sleep (crossed bars) episodes. <bold>B</bold> To rule out the possibility that the difference in scores resulted from uneven number of recorded neurons, the score were recomputed using an equal number of neurons in each population (i.e by randomly selecting neurons within the largest group). <bold>C</bold> Number of splits of one feature neuron given its angular distance with the target neuron. <bold>D</bold> Number of splits given the mean firing rate of the feature neuron. Despite firing rate differences, all features neurons contributed.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g004" xlink:type="simple"/>
</fig>
<p>An uneven number of feature neurons is a potential confound in peer-prediction analysis. The prediction process was thus repeated by equalizing the number of neurons in both structures and it yielded scores similar to the original analysis (<xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4B</xref>). The activity within ADn is therefore more predictable than in the PoSub.</p>
<p>To best capture the statistical dependencies between spikes trains, we focused on a gain analysis (i.e. from the branching structure resulting from learning on only 30 trees with a depth of 2) and we found that the angular distance was a weak predictor of the split density for both ADn and PoSub (<xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4C</xref>). In others words, gradient boosted trees tend to split preferentially, yet mildly, the instantaneous firing rate of feature neurons that have a preferred direction closer to the target neuron. More surprisingly, we found no correlation between the mean firing rate of neurons and the density of splits (<xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4D</xref>). Feature data from neurons with high firing rates are characterized by a wider range of values to be split, yet, this does not lead to increased splitting. Thus, all neurons contributed to the prediction of the activity of another neuron despite each idiosyncratic spiking activity.</p>
</sec>
<sec id="sec017">
<title>Peer-prediction reveals the directionality of information flow across brain structures</title>
<p>While the HD signal is aligned with the actual heading of the awake animal in the PoSub, the spiking of HD cells in the ADn are best explained by the future heading of the animal, by about 10-50 ms [<xref ref-type="bibr" rid="pcbi.1006041.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref043">43</xref>]. This finding suggests that the neuronal activity in the ADn should lead PoSub spiking at least during wakefulness, perhaps in all brain states. We thus tested the ability of XGB to reveal the temporal constraints of neuronal communication across brain areas compared to the classical cross-correlation of spike train pairs. One issue with linear cross-correlation analysis is that it is dominated by the slow dynamics of the underlying signal and, while the HD signal has comparable dynamics during wake and REM sleep, it is accelerated during non-REM sleep [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>]. During wakefulness and REM, cross-correlations do not reveal clear bias in the temporal organization of the ADn to PoSub communication. Furthermore, a Principal Component Analysis of the cross-correlograms reveals that, overall, cross-correlograms of thalamo-cortical pairs of neurons are rather good indicators of the ongoing brain state (<xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5A</xref>, <italic>left</italic>). Finally, the sign of the correlation between HD neurons depend, in all brain states, on the angular difference of their preferred direction [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>]. Thus, cross-correlograms are in average flat (<xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5A</xref>, <italic>right</italic>), and the overall effect can only be captured by the study of the variance of the cross-correlograms. The variance of the corr-correlograms shows a slight biases for negative latencies from ADn to PoSub (insets in <xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5A</xref>, <italic>right</italic>), but, again, the variance profile (and thus its resolution) depends on the brain states.</p>
<fig id="pcbi.1006041.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Temporal coordination between ADn and PoSub is preserved across brain states, as revealed by gradient boosted trees but not cross-correlation.</title>
<p><bold>A</bold> Cross-correlation between spike trains of ADn and PoSub neurons during the Wake, REM sleep and non-REM sleep (respectively in red, yellow and blue). <italic>Top left</italic>, examples of spiking activity for the three brain states; <italic>middle left</italic>, one-session example of an average cross-correlation between an ensemble of ADn neurons and one PoSub neuron; <italic>bottom left</italic> first two dimensions of a PCA performed on all cross-correlations, across all three brain states and neurons of PoSub. Colored circles are the best Gaussian fit for each state, showing that Wake and REM sleep yield qualitatively similar cross-correlations, but not non-REM sleep. <italic>Right</italic>, averaged (± s.d.) cross-correlation for each state. The insets shows the variance. <bold>B</bold> Gradient boosted tree prediction of PoSub firing from the activity of ADn neuron ensembles at successive past to future time steps during Wake, REM sleep and non-REM sleep. <italic>Top right</italic>, example instantaneous firing rates an ADn neuronal ensemble (shifted at various positive and negative lags) and a PoSub neuron; <italic>middle right</italic>, prediction gain for the example session; <italic>left</italic>, the gain of the algorithm was maximal around 25 ms before PoSub spikes (vertical black lines) for wake and both sleep stages; <italic>bottom right</italic> PCA of all resulting gain, across brain states. The best Gaussian fits of each state (as in panel A) are now overlapping.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g005" xlink:type="simple"/>
</fig>
<p>Can XGB reveal the temporal component of neuronal communication across brain areas? To investigate this question, XBG was run for peer-prediction of individual PoSub neurons from multiple copies of ADn population activity at various time-lags. In other words, the model learned the relationship between the firing rates of feature neurons from time <italic>t</italic> − <italic>T</italic> to <italic>t</italic> + <italic>T</italic> (in <xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4A</xref>, the model had access only to time <italic>t</italic>). A graphical explanation of this procedure is shown in <xref ref-type="supplementary-material" rid="pcbi.1006041.s004">S4 Fig</xref>. Using only raw, unsmoothed spike counts, we found that the gain (the number of splits multiplied by the mean gain) was maximal at -25 ms when predicting PoSub firing rate with ADn activity (<xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5B</xref>), in agreement with the anticipation delay of ADn HD neurons [<xref ref-type="bibr" rid="pcbi.1006041.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref043">43</xref>]. The distribution of transmission delays was only weakly dependent on brain states, suggesting a hard-wired, internally organized circuit (<xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5B</xref>) [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>].</p>
</sec>
<sec id="sec018">
<title>Robustness of gradient boosted trees to detect delay of transmission is asserted by a spiking network</title>
<p>To assess that gradient boosting can determine temporal shifts between spike trains of neurons <italic>in vivo</italic>, independent of brain-state dynamics (i.e. feature dynamics), we further tested the methods with smulations of spiking networks [<xref ref-type="bibr" rid="pcbi.1006041.ref038">38</xref>]. We first sought to replicate the temporal delay between ADn and PoSub shown in <xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6A</xref> (see <xref ref-type="sec" rid="sec002">Methods</xref>). To this end, we used HD tuning curves and the animal’s HD to generate series of spike trains in an artificial population of ADn and PoSub neurons. Those neurons are Poisson spiking neurons parameterized at each time step only by the instantaneous firing rate read from the angular tuning curves, thus referred to as T(ADn) and T(PoSub). We then modeled a population of PoSub integrate-and-fire neurons that receive one-to-one inputs with a fixed weight from T(PoSub) and multiple inputs from T(ADn) with synaptic weights inversely proportional to the angular distance (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6A</xref>). As shown in <xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6B</xref> for four neurons of each layer, the neurons of PoSub fired whenever the animal’s HD crossed their angular tuning curves. To demonstrate that PoSub neurons integrate information that is related to the tuning curves of T(ADn), we showed the cross-correlation between each pair of neurons from the two layers, sorted by their preferred angular direction (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6C</xref>). As with cross-correlations of pairs of real HD neurons [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>], pairs of HD neurons with overlapping tuning curves show positive correlations (i.e. peaks in the cross-correlgrams) and pairs of opposite preferred directions show negative correlation (i.e. dip in the cross-correlograms). As expected, the average cross-correlogram is flat (inset in <xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6D</xref>).</p>
<fig id="pcbi.1006041.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Spiking network simulations reveal the robustness of gradient boosted trees to detect transmission delays independent of feature dynamics.</title>
<p><bold>A</bold> The layer of PoSub integrate-and-fire neurons (red dots) receives one-to-one input from a mirrored layer of neurons which determines their primary angular tuning curves (right T(PoSub) in blue dots) and inputs from a layer of ADn neurons (left T(ADn) in blue dots) with full connectivity. The synaptic weight from T(ADn) to PoSub is proportional to the angular difference between the respective tuning curves of ADn neurons and PoSub neurons. <bold>B</bold> Simulation of 15 s of data. Top row, real HD value of one animal. Middle, raster of spiking activity of T(ADn) (top) and T(PoSub) (bottom). Bottom, membrane potential of the PoSub neurons. <bold>C</bold> Cross-correlograms between the spiking activity of 10 T(ADn) neurons and 10 PoSub neurons sorted according to the angular peak of their tuning curves. The angular difference between the preferred firing directions is color-coded (0 in red, <italic>π</italic> in blue). <bold>D</bold> Centered standard deviation of the cross-correlograms at normal (full green line) and accelerated angular speed (dashed green line). Synaptic transmission is set at 0 in these simulations. Black lines show the best exponential fits. <bold>E</bold> Same as <bold>D</bold>, but using XGB peer prediction of PoSub spiking activity from T(ADn) activity. Note that the distribution peaks at 0 ms for both angular speeds. <bold>F</bold> Characteristic time decays of the cross-correlogram exponential fits as a function of angular speed. <bold>G</bold> Full width at half maximum (FWHM) of cross-correlograms and XGB learning gain as a function of angular velocity. <bold>H</bold> XGB gains as a function of synaptic delays of transmission between T(ADn) and PoSub.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g006" xlink:type="simple"/>
</fig>
<p>To reproduce the observation that the temporal width of cross-correlations was smaller for non-REM sleep than for REM sleep and wake, we gradually changed the speed of the animal’s HD in input. As expected, the temporal width of cross-correlations was primarily driven by the feature dynamics as shown in <xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6D</xref> for an angular speed accelerated four times. When doing peer-prediction with XGB as in <xref ref-type="fig" rid="pcbi.1006041.g005">Fig 5B</xref>, we observed that the prediction of time lag remained qualitatively the same when the angular speed was accelerated four times (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6E</xref>). We quantified the decrease of temporal width in the cross-correlogram for four different speeds with an exponential decay fit (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6F</xref>) and the full width at half maximum (FWHM, <xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6G</xref>). In contrast, the FWHM resulting from XGB remained constant across conditions (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6G</xref>).</p>
<p>Could the model confirm that XBG accurately tracks synaptic delays? Even when synaptic transmission delay was set at 0 ms, the variance of the cross-correlograms was slightly shifted at negative time lags (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6D</xref>), unlike the XGB gain (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6E</xref>). This suggests that linear cross-correlations, but not XGB, is biased by the integration time constant of the post-synaptic neuron. In addition, we observed that changing the intrinsic transmission delay was fully captured by XGB (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6H</xref>). In conclusion, applying gradient boosted trees on neuronal ensembles reveals intrinsic temporal organization of the circuit, independent of the brain-state specific dynamics of the underlying features.</p>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>We show how non-linear encoders are versatile and useful tools to study neuronal data in relation to behavior and brain states. More specifically, we found using these methods that, in the HD system, the thalamus temporally leads the cortex during wakefulness and sleep, suggesting a bottom-up transmission of signal irrespective of the brain state.</p>
<p>While classical tools aim to provide interpretation of the data by investigating the predictability of a particular model of neuronal function, we show that gradient boosted trees [<xref ref-type="bibr" rid="pcbi.1006041.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref044">44</xref>], a supervised learning technique commonly used in various fields of data mining, equals, if not outperforms other classes of Machine Learning models [<xref ref-type="bibr" rid="pcbi.1006041.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref046">46</xref>]. This performance was achieved by a direct fit of raw behavioral or neuronal data to the targeted spike trains, with no explicit prior on a cell’s response (e.g. a tuning curve, or a model of mixed-selectivity to a set of variables). We report optimal parameters and detailed methods to study neuronal response and dynamics as a function of behavior or endogenous processes (e.g. the neuronal peer network). Furthermore, we show that the resulting tree structure, after learning of the data, can be itself analyzed to reveal important properties of the neuronal networks.</p>
<sec id="sec020">
<title>Learning neuronal firing in relation to behavioral data: Performance and optimal parameters</title>
<p>We first sought to validate the approach of learning a predictive model of spike trains from behavioral data with a decision tree learning algorithm that does not include a predefined model of the training set. To this end, we analyzed a dataset of HD cells [<xref ref-type="bibr" rid="pcbi.1006041.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>], whose firing in relation to behavior is among the best characterized signals in the mammalian nervous system. We demonstrated first that gradient boosted trees predicts the firing of the neurons with high accuracy by establishing a direct correspondence between the raw behavioral data (in this case the HD angular value alone) and the instantaneous spiking of the neurons (<xref ref-type="fig" rid="pcbi.1006041.g002">Fig 2</xref>). Using a Generalized Linear Model to regress the spiking activity of a neuron on raw behavioral data, such as the HD angular values, necessarily fails as this relationship is not generally linear. It is thus necessary to project the raw data on a set of orthogonal functions that linearize the inputs. Therefore, we used a basis of trigonometric functions up to the 6th order that can, in theory, capture the typical width of a HD cell tuning curve (approximatively 60 degree width). In this case, the prediction performance was similar to XGB. The same type of transformation has been applied previously, for example Zernicke’s polynomials for position values in a circular environment [<xref ref-type="bibr" rid="pcbi.1006041.ref047">47</xref>]. However, it is clear from these two examples that one major strength of XGB is to generalize prediction to all possible behavioral data (e.g. not depending on the particular shape of an environment for position data). Finally, the performance of XGB was similar to a model-based approach (i.e. prediction of the firing rate on test data based on the tuning curve of the training set). This is not surprising for a class of neurons whose spiking activity is explained so well by an experimentally tractable signal. However, in general, tuning curves for even well defined neuronal responses explain actual spike trains only partially and XGB may well capture previously undetermined sources of variance.</p>
<p>Although XGB can be viewed as a model-free technique that does not assume any particular statistics or generative model of the input data, the procedure still depends on a limited set of free parameters that need to be tuned for optimal performance. To facilitate the use of this classifier for future studies and assure reproducibility of analyses across laboratories, we systematically explored the parameter space for depth and number of trees for spike train prediction. When computing prediction performance (measured by the pseudo-<italic>R</italic><sup>2</sup>), we found that minima were well localized, for all neurons, using the BIC score that penalizes over-complex models. More specifically, we show how the use of multiple trees (approximatively 100), each limited in depth (typically five branching), was an optimal choice of parameters. Importantly, these optimal parameters did not seem to depend on a neuron’s intrinsic parameters (e.g. firing rates) and there was no obvious trade-off between tree depth and number of trees (the two optimal values were independently distributed across neurons).</p>
</sec>
<sec id="sec021">
<title>Interpreting the structure of the gradient boosted trees</title>
<p>While the structure of a multi-layered neural nets (or other forms of <italic>deep</italic> architecture) after learning the classification of a dataset is notoriously unanalyzable [<xref ref-type="bibr" rid="pcbi.1006041.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref050">50</xref>], we show how the branching of the decision tree may be highly informative on how input data are matched to their output targets. The density of splits (or branching) across the series of trees was maximal in the range of inputs where firing rates vary the most. This could be interpreted as a maximum data splitting around the maxima of Fisher Information which is, for a Poisson process, directly related to the change in firing rate as a function of stimulus value, that is when spike trains are most informative about the encoded signal [<xref ref-type="bibr" rid="pcbi.1006041.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref051">51</xref>]. Although the relationship between tree branching and Fisher Information is, in our study, purely empirical, it is interesting to show, again, that unraveling the tree structure allows the understanding of how the data are learned by gradient boosting.</p>
<p>In the case of neuronal peer-prediction, analyzing the structure of gradient boosted trees presents the advantage that all kinds of neuronal interactions (positive, negative, linear or monotonically non-linear) yield comparable estimates (when quantifying split density or gain). This is in contrast with classical correlation analyses of individual spike trains relative to a population of peers that may be hard to interpret in certain cases where these interactions are both negative and positive ([<xref ref-type="bibr" rid="pcbi.1006041.ref052">52</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref053">53</xref>]. As we show here, linear correlations are also directly affected by the ongoing brain dynamics. In addition, fitting spiking data to maximum entropy (i.e. Ising) models have revealed that linear correlations may not indicate the true coordination between spike trains [<xref ref-type="bibr" rid="pcbi.1006041.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref024">24</xref>]. The analysis of tree branching provides an estimate of the statistical dependencies between spike trains, independent of the underlying type of interaction and without assuming a particular transfer function for the target neuron [<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>]. The nature of neuronal coordination as observed from spike trains is still debated, for example in the hippocampus [<xref ref-type="bibr" rid="pcbi.1006041.ref054">54</xref>], and unbiased, model-free methods may be highly informative on the nature of the actual statistical dependencies between neurons.</p>
<p>We also report an optimal range of tree number that should be used for split analysis when regressing spike trains on behavioral features or the activity of other neurons (Figs <xref ref-type="fig" rid="pcbi.1006041.g003">3</xref>–<xref ref-type="fig" rid="pcbi.1006041.g005">5</xref>). ‘Learning gain’ decreases exponentially with the number of trees (<xref ref-type="fig" rid="pcbi.1006041.g007">Fig 7</xref>). Using less trees (typically 30 with a depth of 2) allows for estimation of how different features contribute to the output target, at the expense of prediction and decoding performance (which are best estimated with approximatively 100 trees with a depth of 5, see above). In contrast, fitting the data on too many trees leads to overfitting and should be avoided. Overall, readers interested in using this technique should bear in mind that meaningful information about the dataset can sometimes be overshadowed by high split density. In such cases, it is of best interest to reduce the number of trees and to ensure that the average gain for splits is large enough.</p>
<fig id="pcbi.1006041.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006041.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Split analysis and optimal data prediction lies within different ranges of tree numbers (for a fixed tree depth).</title>
<p>Thus, the use of gradient boosted trees requires a careful tuning of the parameters of the algorithm depending on the question (interpretability of the structure versus prediction and decoding of the signal).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec022">
<title>Measuring the contribution of multiple behavioral variables</title>
<p>A large class of neurons in the brain are modulated by several dimensions of incoming stimuli [<xref ref-type="bibr" rid="pcbi.1006041.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref055">55</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref057">57</xref>], a property referred to as mixed-selectivity. Untangling the different contributions is sometimes challenging and gradient boosted trees offer a rapid and unequivocal approach to address this issue [<xref ref-type="bibr" rid="pcbi.1006041.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref027">27</xref>]. In fact, there is no intrinsic limit to the dimensionality of the inputs that can be learned. To further test this technique, we regressed spike trains of HD cells on spatial position, as well as on HD data. In line with previous reports [<xref ref-type="bibr" rid="pcbi.1006041.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref040">40</xref>], the HD cells of the PoSub correlated also with spatial factors while in the ADn, neurons coded mostly for the HD (<xref ref-type="fig" rid="pcbi.1006041.g003">Fig 3</xref>). XGB thus enables to rapidly explore the correlates of spike trains to measurements of external or internal variables of the system.</p>
</sec>
<sec id="sec023">
<title>Prediction of feed-forward activation in a thalamo-cortical network in vivo</title>
<p>Investigation of neuronal dynamics does not always entail the regression of spiking data to variables of the experiments. Many studies have focused on the spatio-temporal coordination of neuronal networks <italic>in vivo</italic>, independent of any behaviorally-related processing ([<xref ref-type="bibr" rid="pcbi.1006041.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref059">59</xref>]. In fact, the characterization of signal transmission between brain areas remains one of the most complex challenges of neuroscience as it first requires the recording of such data <italic>in vivo</italic> as well as the establishment of a proper model of interaction to determine the parameters of spike transmission (e.g. conduction delay and post-synaptic integration time).</p>
<p>Here we used data from the HD thalamo-cortical network [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>] with simultaneous recording of PoSub and ADn. It allowed us to demonstrate a temporally-shifted relationship from ADn to PoSub. More precisely, we used gradient boosted trees to predict PoSub HD cell firing activity based on the ensemble spike trains of the HD cells of the ADn, at various time-lags between the two series of spike trains. PoSub spiking was mostly dependent on ADn activity in preceding time bins (in average 25ms), thus indicating a likely feed-forward pathway. First, this replicates the findings that the HD signal of ADn neurons precedes the actual HD by about 25 ms [<xref ref-type="bibr" rid="pcbi.1006041.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref060">60</xref>]. Second, the temporal asymmetry in the prediction of cortical spiking relative to thalamic activity was preserved during sleep, both during REM and non-REM, and it therefore indicates that this differential temporal coding likely emerges from intrinsic wiring and dynamics. This confirms anatomical studies, as well as examination of putative synaptic interaction between neurons in this pathway [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>].</p>
<p>The robustness of this approach was validated by the analysis of artificially generated spike trains, drawn from actual tuning curves and in which different input feature dynamics (in our case, angular head velocity, or ‘virtual’ angular speed during sleep), transmission delays, and integration time constant were explored. This study confirmed the results of <italic>in vivo</italic> data: unlike linear cross-correlations, gradient boosting reveals temporal organization of spiking irrespective of the dynamics of the inputs and accurately extract, in all conditions, a delay introduced between spike trains (<xref ref-type="fig" rid="pcbi.1006041.g006">Fig 6</xref>). Furthermore, while PoSub integration time constant alone results in temporarily shifted cross-correlograms between ADn and PoSub simulated spike trains, gradient boosting captures only the synaptic transmission delay.</p>
</sec>
<sec id="sec024">
<title>Gradient boosted trees match Bayesian decoding in performance</title>
<p>Neurons convey information about external parameters, and it should thus be possible to decode these signals from population activity. The best examples are the demonstrations that position can be estimated from ensembles of hippocampal place cells during exploration and ‘imagination’ of future paths [<xref ref-type="bibr" rid="pcbi.1006041.ref061">61</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref063">63</xref>] as well as the HD signal during wakefulness [<xref ref-type="bibr" rid="pcbi.1006041.ref064">64</xref>] and sleep [<xref ref-type="bibr" rid="pcbi.1006041.ref030">30</xref>]. Decoding of neuronal signals has also been widely studied in the context of brain machine interface [<xref ref-type="bibr" rid="pcbi.1006041.ref065">65</xref>].</p>
<p>Bayesian decoding is the tool of reference to estimate a signal from ensembles of neurons. In fact, it computes the probability distribution of a particular signal given the tuning curves of the neurons and the instantaneous spike counts in the neuronal population. This technique generally assumes that spike counts are drawn from Poisson processes and that neurons are independent from each other ([<xref ref-type="bibr" rid="pcbi.1006041.ref037">37</xref>]). Here we have compared the performance of Bayesian decoders and gradient boosted trees for decoding angular values based on the activity of either ADn or PoSub neuronal ensembles (<xref ref-type="supplementary-material" rid="pcbi.1006041.s001">S1 Fig</xref>). We found that gradient boosted trees matched Bayesian decoding when using ADn neurons but were slightly better with PoSub activity. As emphasized in this report (<xref ref-type="fig" rid="pcbi.1006041.g004">Fig 4E and 4F</xref>), PoSub activity does not encode only the HD but also spatial information about the location of the animal. In case of mixed-selectivity signals, a model-free technique such as gradient boosted trees is less impaired at predicting an external variable compared to the classical method of Bayesian decoding.</p>
</sec>
<sec id="sec025">
<title>Potential for neuroscience and future work</title>
<p>The potential of these methods to unravel the dynamics of biological neuronal networks is tremendous and will be the scope of further studies. For instance, tracking synaptic transmission in pairwise spike trains [<xref ref-type="bibr" rid="pcbi.1006041.ref066">66</xref>], uncoupling the phase-locking of neuronal spiking to concomitant and nested brain oscillations [<xref ref-type="bibr" rid="pcbi.1006041.ref067">67</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref068">68</xref>], and determining the nature of the coordination in neuronal populations in relation to behavior [<xref ref-type="bibr" rid="pcbi.1006041.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref054">54</xref>] are examples of the current challenges of data analysis in systems neuroscience. In addition, future improvements of brain-machine interface will require the development of reliable and robust tools to decode neuronal activity [<xref ref-type="bibr" rid="pcbi.1006041.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1006041.ref070">70</xref>].</p>
<p>In summary, gradient boosted trees methods are potentially helpful tools to explore a dataset and make a prediction on the underlying biological processes which, in turn, can be tested with more classical methods. They may also be used to decode signals for closed-loops experiments and brain-machine interface in animals or humans. Finally, these methods open avenues for the study of neuronal data, in general, as the branching of the tree structure can be analyzed as a ‘proxy’ of the biological system itself.</p>
</sec>
</sec>
<sec id="sec026">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006041.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Decoding of HD angle.</title>
<p><bold>A</bold> Example of decoding for XGB during 30 seconds of head rotation for both ADn and PoSub spiking activities. The black line shows the real angular HD. <bold>B</bold> For sessions with large groups of neurons (<italic>n</italic> ≥ 7) in ADn and PoSub, the HD of the animal was decoded based on spiking activity with the classical Bayesian decoding and gradient boosted trees (XGB) over 60 angular bins.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006041.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Learning gain per tree decreases with the number of trees (blue line).</title>
<p>This decay was well captured by an exponential fit (red line), from which an optimal number of trees of approximatively 30 trees is derived (intersect of the linear fit at origin with the x-axis). At this stage the mean gain per tree is approximately <inline-formula id="pcbi.1006041.e030"><alternatives><graphic id="pcbi.1006041.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006041.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>3</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula> of its initial value and most of the learning has already occurred.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006041.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Features carrying actual signal are preferentially split in the first trees, resulting in higher gain.</title>
<p>The graph illustrates the evolution of split density when learning the spike train of a HD neuron as a function of the number of trees for three features: the actual HD and two random vectors. Split density increased linearly and similarly with the number of trees in the asymptotic regime for all features. However, the increase was much higher for the HD at low tree numbers, a difference well captured by gain analysis. Note that, as the order of features in the algorithm may impact which are split first, we showed how the feature data were organized (random 1, angle and random 2).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006041.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006041.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Revealing temporal delay in peer-prediction.</title>
<p>Feature space is composed of multiple copies of the activity of the feature neuron (in this case, in the ADn) at various time-lags (blue curves) to learn the target spike train (PoSub, red curves). The relationship between the two spike trains shows maximal dependence at t-1, resulting in a high number of splits by the algorithm (yellow horizontal lines). Splitting was less effective for more independent firing at t and t-2. In this example, the relationship at t-1 is trivial (linear and positively correlated). However, the quantification of these interactions give comparable values for a large variety of interactions (e.g. positive, negative or monotonically non linear).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006041.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>. <source>Spikes: exploring the neural code</source>. <publisher-name>MIT press</publisher-name>; <year>1999</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>. <article-title>Neural signatures of cell assembly organization</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2005</year>;<volume>6</volume>(<issue>5</issue>):<fpage>399</fpage>–<lpage>407</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1669" xlink:type="simple">10.1038/nrn1669</ext-link></comment> <object-id pub-id-type="pmid">15861182</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname> <given-names>TN</given-names></name>. <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of physiology</source>. <year>1962</year>;<volume>160</volume>(<issue>1</issue>):<fpage>106</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1113/jphysiol.1962.sp006837" xlink:type="simple">10.1113/jphysiol.1962.sp006837</ext-link></comment> <object-id pub-id-type="pmid">14449617</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref004">
<label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>O’keefe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nadel</surname> <given-names>L</given-names></name>. <source>The hippocampus as a cognitive map</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Clarendon Press</publisher-name>; <year>1978</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taube</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>RU</given-names></name>, <name name-style="western"><surname>Ranck</surname> <given-names>JB</given-names></name>. <article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis</article-title>. <source>Journal of Neuroscience</source>. <year>1990</year>;<volume>10</volume>(<issue>2</issue>):<fpage>420</fpage>–<lpage>435</lpage>. <object-id pub-id-type="pmid">2303851</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Csicsvari</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hirase</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dragoi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Organization of cell assemblies in the hippocampus</article-title>. <source>Nature</source>. <year>2003</year>;<volume>424</volume>(<issue>6948</issue>):<fpage>552</fpage>–<lpage>556</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature01834" xlink:type="simple">10.1038/nature01834</ext-link></comment> <object-id pub-id-type="pmid">12891358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Truccolo</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Eden</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>Journal of neurophysiology</source>. <year>2005</year>;<volume>93</volume>(<issue>2</issue>):<fpage>1074</fpage>–<lpage>1089</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00697.2004" xlink:type="simple">10.1152/jn.00697.2004</ext-link></comment> <object-id pub-id-type="pmid">15356183</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Uzzell</surname> <given-names>VJ</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>. <article-title>Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model</article-title>. <source>Journal of Neuroscience</source>. <year>2005</year>;<volume>25</volume>(<issue>47</issue>):<fpage>11003</fpage>–<lpage>11013</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3305-05.2005" xlink:type="simple">10.1523/JNEUROSCI.3305-05.2005</ext-link></comment> <object-id pub-id-type="pmid">16306413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borst</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Information theory and neural coding</article-title>. <source>Nature neuroscience</source>. <year>1999</year>;<volume>2</volume>(<issue>11</issue>):<fpage>947</fpage>–<lpage>957</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14731" xlink:type="simple">10.1038/14731</ext-link></comment> <object-id pub-id-type="pmid">10526332</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>68</volume>(<issue>3</issue>):<fpage>362</fpage>–<lpage>385</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.09.023" xlink:type="simple">10.1016/j.neuron.2010.09.023</ext-link></comment> <object-id pub-id-type="pmid">21040841</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nature neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>9</issue>):<fpage>1170</fpage>–<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>. <article-title>From the neuron doctrine to neural networks</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2015</year>;<volume>16</volume>(<issue>8</issue>):<fpage>487</fpage>–<lpage>497</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3962" xlink:type="simple">10.1038/nrn3962</ext-link></comment> <object-id pub-id-type="pmid">26152865</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Large-scale recording of neuronal ensembles</article-title>. <source>Nature neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>5</issue>):<fpage>446</fpage>–<lpage>451</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1233" xlink:type="simple">10.1038/nn1233</ext-link></comment> <object-id pub-id-type="pmid">15114356</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jun</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Mitelut</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lai</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gratiy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Anastassiou</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>TD</given-names></name>. <article-title>Real-time spike sorting platform for high-density extracellular probes with ground-truth validation and drift correction</article-title>. <source>bioRxiv</source>. <year>2017</year>; p. <fpage>101030</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Wardill</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Pulver</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Renninger</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Baohan</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>. <year>2013</year>;<volume>499</volume>(<issue>7458</issue>):<fpage>295</fpage>–<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12354" xlink:type="simple">10.1038/nature12354</ext-link></comment> <object-id pub-id-type="pmid">23868258</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dombeck</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Khabbaz</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Collman</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Adelman</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name>. <article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>56</volume>(<issue>1</issue>):<fpage>43</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2007.08.003" xlink:type="simple">10.1016/j.neuron.2007.08.003</ext-link></comment> <object-id pub-id-type="pmid">17920014</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perkel</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Gerstein</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>GP</given-names></name>. <article-title>Neuronal spike trains and stochastic point processes: II. Simultaneous spike trains</article-title>. <source>Biophysical journal</source>. <year>1967</year>;<volume>7</volume>(<issue>4</issue>):<fpage>419</fpage>–<lpage>440</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3495(67)86597-4" xlink:type="simple">10.1016/S0006-3495(67)86597-4</ext-link></comment> <object-id pub-id-type="pmid">4292792</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chapin</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>. <article-title>Principal component analysis of neuronal ensemble activity reveals multidimensional somatosensory representations</article-title>. <source>Journal of neuroscience methods</source>. <year>1999</year>;<volume>94</volume>(<issue>1</issue>):<fpage>121</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0165-0270(99)00130-2" xlink:type="simple">10.1016/S0165-0270(99)00130-2</ext-link></comment> <object-id pub-id-type="pmid">10638820</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peyrache</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Benchenane</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wiener</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Battaglia</surname> <given-names>FP</given-names></name>. <article-title>Principal component analysis of ensemble recordings reveals cell assemblies at high temporal resolution</article-title>. <source>Journal of computational neuroscience</source>. <year>2010</year>;<volume>29</volume>(<issue>1-2</issue>):<fpage>309</fpage>–<lpage>325</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-009-0154-6" xlink:type="simple">10.1007/s10827-009-0154-6</ext-link></comment> <object-id pub-id-type="pmid">19529888</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lopes-dos Santos</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Ribeiro</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tort</surname> <given-names>AB</given-names></name>. <article-title>Detecting cell assemblies in large neuronal populations</article-title>. <source>Journal of neuroscience methods</source>. <year>2013</year>;<volume>220</volume>(<issue>2</issue>):<fpage>149</fpage>–<lpage>166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2013.04.010" xlink:type="simple">10.1016/j.jneumeth.2013.04.010</ext-link></comment> <object-id pub-id-type="pmid">23639919</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>. <article-title>Dynamical networks: finding, measuring, and tracking neural population activity using network science</article-title>. <source>bioRxiv</source>. <year>2017</year>; p. <fpage>115485</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schneidman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source>. <year>2006</year>;<volume>440</volume>(<issue>7087</issue>):<fpage>1007</fpage>–<lpage>1012</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature04701" xlink:type="simple">10.1038/nature04701</ext-link></comment> <object-id pub-id-type="pmid">16625187</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cocco</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Leibler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Monasson</surname> <given-names>R</given-names></name>. <article-title>Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>;<volume>106</volume>(<issue>33</issue>):<fpage>14058</fpage>–<lpage>14062</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0906705106" xlink:type="simple">10.1073/pnas.0906705106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SL</given-names></name>. <article-title>Improving data quality in neuronal population recordings</article-title>. <source>Nature Neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>9</issue>):<fpage>1165</fpage>–<lpage>1174</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4365" xlink:type="simple">10.1038/nn.4365</ext-link></comment> <object-id pub-id-type="pmid">27571195</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Truccolo</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JP</given-names></name>. <article-title>Nonparametric modeling of neural point processes via stochastic gradient boosting regression</article-title>. <source>Neural computation</source>. <year>2007</year>;<volume>19</volume>(<issue>3</issue>):<fpage>672</fpage>–<lpage>705</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2007.19.3.672" xlink:type="simple">10.1162/neco.2007.19.3.672</ext-link></comment> <object-id pub-id-type="pmid">17298229</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Benjamin</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Fernandes</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Tomlinson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ramkumar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>VerSteeg</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Modern machine learning far outperforms GLMs at predicting spikes</article-title>. <source>bioRxiv</source>. <year>2017</year>; p. <fpage>111450</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taube</surname> <given-names>JS</given-names></name>. <article-title>The head direction signal: origins and sensory-motor integration</article-title>. <source>Annu Rev Neurosci</source>. <year>2007</year>;<volume>30</volume>:<fpage>181</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.29.051605.112854" xlink:type="simple">10.1146/annurev.neuro.29.051605.112854</ext-link></comment> <object-id pub-id-type="pmid">17341158</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peyrache</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lacroix</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Petersen</surname> <given-names>PC</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Internally organized mechanisms of the head direction sense</article-title>. <source>Nature neuroscience</source>. <year>2015</year>;<volume>18</volume>(<issue>4</issue>):<fpage>569</fpage>–<lpage>575</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3968" xlink:type="simple">10.1038/nn.3968</ext-link></comment> <object-id pub-id-type="pmid">25730672</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to boosting. In: European conference on computational learning theory. Springer; 1995. p. 23–37.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref032">
<label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Schapire</surname> <given-names>RE</given-names></name>. <chapter-title>The boosting approach to machine learning: An overview</chapter-title>. In: <source>Nonlinear estimation and classification</source>. <publisher-name>Springer</publisher-name>; <year>2003</year>. p. <fpage>149</fpage>–<lpage>171</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref033">
<label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ferreira</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Figueiredo</surname> <given-names>MA</given-names></name>. <chapter-title>Boosting algorithms: A review of methods, theory, and applications</chapter-title>. In: <source>Ensemble Machine Learning</source>. <publisher-name>Springer</publisher-name>; <year>2012</year>. p. <fpage>35</fpage>–<lpage>85</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen T, Guestrin C. Xgboost: A scalable tree boosting system. In: Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM; 2016. p. 785–794.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cameron</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Windmeijer</surname> <given-names>FA</given-names></name>. <article-title>An R-squared measure of goodness of fit for some common nonlinear regression models</article-title>. <source>Journal of Econometrics</source>. <year>1997</year>;<volume>77</volume>(<issue>2</issue>):<fpage>329</fpage>–<lpage>342</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0304-4076(96)01818-0" xlink:type="simple">10.1016/S0304-4076(96)01818-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>Mutual information, Fisher information, and population coding</article-title>. <source>Neural computation</source>. <year>1998</year>;<volume>10</volume>(<issue>7</issue>):<fpage>1731</fpage>–<lpage>1757</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976698300017115" xlink:type="simple">10.1162/089976698300017115</ext-link></comment> <object-id pub-id-type="pmid">9744895</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ginzburg</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title>. <source>Journal of neurophysiology</source>. <year>1998</year>;<volume>79</volume>(<issue>2</issue>):<fpage>1017</fpage>–<lpage>1044</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.1998.79.2.1017" xlink:type="simple">10.1152/jn.1998.79.2.1017</ext-link></comment> <object-id pub-id-type="pmid">9463459</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goodman</surname> <given-names>DF</given-names></name>, <name name-style="western"><surname>Brette</surname> <given-names>R</given-names></name>. <article-title>The brian simulator</article-title>. <source>Frontiers in neuroscience</source>. <year>2009</year>;<volume>3</volume>(<issue>2</issue>):<fpage>192</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.01.026.2009" xlink:type="simple">10.3389/neuro.01.026.2009</ext-link></comment> <object-id pub-id-type="pmid">20011141</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cacucci</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lever</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wills</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Burgess</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>O’Keefe</surname> <given-names>J</given-names></name>. <article-title>Theta-modulated place-by-direction cells in the hippocampal formation in the rat</article-title>. <source>Journal of Neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>38</issue>):<fpage>8265</fpage>–<lpage>8277</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2635-04.2004" xlink:type="simple">10.1523/JNEUROSCI.2635-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15385610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peyrache</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schieferstein</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Transformation of the head-direction signal into a spatial code</article-title>. <source>Nature communications</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1752</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-01908-3" xlink:type="simple">10.1038/s41467-017-01908-3</ext-link></comment> <object-id pub-id-type="pmid">29170377</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sargolini</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fyhn</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hafting</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Witter</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>MB</given-names></name>, <etal>et al</etal>. <article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex</article-title>. <source>Science</source>. <year>2006</year>;<volume>312</volume>(<issue>5774</issue>):<fpage>758</fpage>–<lpage>762</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1125572" xlink:type="simple">10.1126/science.1125572</ext-link></comment> <object-id pub-id-type="pmid">16675704</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Blair</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Sharp</surname> <given-names>PE</given-names></name>. <article-title>Anticipatory head direction signals in anterior thalamus: evidence for a thalamocortical circuit that integrates angular head motion to compute head direction</article-title>. <source>Journal of Neuroscience</source>. <year>1995</year>;<volume>15</volume>(<issue>9</issue>):<fpage>6260</fpage>–<lpage>6270</lpage>. <object-id pub-id-type="pmid">7666208</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Taube</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>RU</given-names></name>. <article-title>Comparisons of head direction cell activity in the postsubiculum and anterior thalamus of freely moving rats</article-title>. <source>Hippocampus</source>. <year>1998</year>;<volume>8</volume>(<issue>2</issue>):<fpage>87</fpage>–<lpage>108</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/(SICI)1098-1063(1998)8:2%3C87::AID-HIPO1%3E3.0.CO;2-4" xlink:type="simple">10.1002/(SICI)1098-1063(1998)8:2%3C87::AID-HIPO1%3E3.0.CO;2-4</ext-link></comment> <object-id pub-id-type="pmid">9572715</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>JH</given-names></name>. <article-title>Greedy function approximation: a gradient boosting machine</article-title>. <source>Annals of statistics</source>. <year>2001</year>; p. <fpage>1189</fpage>–<lpage>1232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aos/1013203451" xlink:type="simple">10.1214/aos/1013203451</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Burges</surname> <given-names>CJ</given-names></name>. <article-title>From ranknet to lambdarank to lambdamart: An overview</article-title>. <source>Learning</source>. <year>2010</year>;<volume>11</volume>(<issue>23-581</issue>):<fpage>81</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref046">
<label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Li P. Robust logitboost and adaptive base class (abc) logitboost. arXiv preprint arXiv:12033491. 2012;.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Acharya</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Aghajan</surname> <given-names>ZM</given-names></name>, <name name-style="western"><surname>Vuong</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Mehta</surname> <given-names>MR</given-names></name>. <article-title>Causal influence of visual cues on hippocampal directional selectivity</article-title>. <source>Cell</source>. <year>2016</year>;<volume>164</volume>(<issue>1</issue>):<fpage>197</fpage>–<lpage>207</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2015.12.015" xlink:type="simple">10.1016/j.cell.2015.12.015</ext-link></comment> <object-id pub-id-type="pmid">26709045</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, et al. Intriguing properties of neural networks. arXiv preprint arXiv:13126199. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref049">
<label>49</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zeiler</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Fergus</surname> <given-names>R</given-names></name>. <chapter-title>Visualizing and understanding convolutional networks</chapter-title>. In: <source>European conference on computer vision</source>. <publisher-name>Springer</publisher-name>; <year>2014</year>. p. <fpage>818</fpage>–<lpage>833</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:13013781. <year>2013</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1006041.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Neural correlations, population coding and computation</article-title>. <source>Nature reviews neuroscience</source>. <year>2006</year>;<volume>7</volume>(<issue>5</issue>):<fpage>358</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1888" xlink:type="simple">10.1038/nrn1888</ext-link></comment> <object-id pub-id-type="pmid">16760916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Renart</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>De La Rocha</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bartho</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hollender</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Reyes</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The asynchronous state in cortical circuits</article-title>. <source>science</source>. <year>2010</year>;<volume>327</volume>(<issue>5965</issue>):<fpage>587</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1179850" xlink:type="simple">10.1126/science.1179850</ext-link></comment> <object-id pub-id-type="pmid">20110507</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Okun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Steinmetz</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Cossell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Iacaruso</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Ko</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Barthó</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Diverse coupling of neurons to populations in sensory cortex</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>511</fpage>–<lpage>515</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14273" xlink:type="simple">10.1038/nature14273</ext-link></comment> <object-id pub-id-type="pmid">25849776</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chadwick</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>van Rossum</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Nolan</surname> <given-names>MF</given-names></name>. <article-title>Independent theta phase coding accounts for CA1 population sequences and enables flexible remapping</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e03542</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.03542" xlink:type="simple">10.7554/eLife.03542</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <etal>et al</etal>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>(<issue>7451</issue>):<fpage>585</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Finkelstein</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Derdikman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Foerster</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>. <article-title>Three-dimensional head-direction coding in the bat brain</article-title>. <source>Nature</source>. <year>2015</year>;<volume>517</volume>(<issue>7533</issue>):<fpage>159</fpage>–<lpage>164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14031" xlink:type="simple">10.1038/nature14031</ext-link></comment> <object-id pub-id-type="pmid">25470055</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hardcastle</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Maheswaranathan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Giocomo</surname> <given-names>LM</given-names></name>. <article-title>A Multiplexed, Heterogeneous, and Adaptive Code for Navigation in Medial Entorhinal Cortex</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>94</volume>(<issue>2</issue>):<fpage>375</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.03.025" xlink:type="simple">10.1016/j.neuron.2017.03.025</ext-link></comment> <object-id pub-id-type="pmid">28392071</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peyrache</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dehghani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Eskandar</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Donoghue</surname> <given-names>JA</given-names></name>, <etal>et al</etal>. <article-title>Spatiotemporal dynamics of neocortical excitation and inhibition during human sleep</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>(<issue>5</issue>):<fpage>1731</fpage>–<lpage>1736</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1109895109" xlink:type="simple">10.1073/pnas.1109895109</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Luczak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Barthó</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>. <article-title>Sequential structure of neocortical spontaneous activity in vivo</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>1</issue>):<fpage>347</fpage>–<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0605643104" xlink:type="simple">10.1073/pnas.0605643104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goodridge</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Taube</surname> <given-names>JS</given-names></name>. <article-title>Interaction between the postsubiculum and anterior thalamus in the generation of head direction cell activity</article-title>. <source>Journal of Neuroscience</source>. <year>1997</year>;<volume>17</volume>(<issue>23</issue>):<fpage>9315</fpage>–<lpage>9330</lpage>. <object-id pub-id-type="pmid">9364077</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>BL</given-names></name>. <article-title>Dynamics of the hippocampal ensemble code for space</article-title>. <source>Science</source>. <year>1993</year>;<volume>261</volume>(<issue>5124</issue>):<fpage>1055</fpage>–<lpage>1058</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.8351520" xlink:type="simple">10.1126/science.8351520</ext-link></comment> <object-id pub-id-type="pmid">8351520</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Johnson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title>. <source>Journal of Neuroscience</source>. <year>2007</year>;<volume>27</volume>(<issue>45</issue>):<fpage>12176</fpage>–<lpage>12189</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3761-07.2007" xlink:type="simple">10.1523/JNEUROSCI.3761-07.2007</ext-link></comment> <object-id pub-id-type="pmid">17989284</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfeiffer</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name>. <article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title>. <source>Nature</source>. <year>2013</year>;<volume>497</volume>(<issue>7447</issue>):<fpage>74</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12112" xlink:type="simple">10.1038/nature12112</ext-link></comment> <object-id pub-id-type="pmid">23594744</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Johnson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Seeland</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name>. <article-title>Reconstruction of the postsubiculum head direction signal from neural ensembles</article-title>. <source>Hippocampus</source>. <year>2005</year>;<volume>15</volume>(<issue>1</issue>):<fpage>86</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/hipo.20033" xlink:type="simple">10.1002/hipo.20033</ext-link></comment> <object-id pub-id-type="pmid">15390162</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laubach</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wessberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>. <article-title>Cortical ensemble activity increasingly predicts behaviour outcomes during learning of a motor task</article-title>. <source>Nature</source>. <year>2000</year>;<volume>405</volume>(<issue>6786</issue>):<fpage>567</fpage>–<lpage>571</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35014604" xlink:type="simple">10.1038/35014604</ext-link></comment> <object-id pub-id-type="pmid">10850715</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barthó</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hirase</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Monconduit</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zugaro</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Characterization of neocortical principal cells and interneurons by network interactions and extracellular features</article-title>. <source>Journal of neurophysiology</source>. <year>2004</year>;<volume>92</volume>(<issue>1</issue>):<fpage>600</fpage>–<lpage>608</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01170.2003" xlink:type="simple">10.1152/jn.01170.2003</ext-link></comment> <object-id pub-id-type="pmid">15056678</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tort</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Komorowski</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Manns</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Kopell</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname> <given-names>H</given-names></name>. <article-title>Theta—gamma coupling increases during the learning of item—context associations</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2009</year>;<volume>106</volume>(<issue>49</issue>):<fpage>20942</fpage>–<lpage>20947</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0911331106" xlink:type="simple">10.1073/pnas.0911331106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Belluscio</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Mizuseki</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kempter</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Cross-frequency phase—phase coupling between theta and gamma oscillations in the hippocampus</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>2</issue>):<fpage>423</fpage>–<lpage>435</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4122-11.2012" xlink:type="simple">10.1523/JNEUROSCI.4122-11.2012</ext-link></comment> <object-id pub-id-type="pmid">22238079</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Musallam</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pesaran</surname> <given-names>B</given-names></name>. <article-title>Selecting the signals for a brain—machine interface</article-title>. <source>Current opinion in neurobiology</source>. <year>2004</year>;<volume>14</volume>(<issue>6</issue>):<fpage>720</fpage>–<lpage>726</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2004.10.005" xlink:type="simple">10.1016/j.conb.2004.10.005</ext-link></comment> <object-id pub-id-type="pmid">15582374</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006041.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lebedev</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name>. <article-title>Brain—machine interfaces: past, present and future</article-title>. <source>TRENDS in Neurosciences</source>. <year>2006</year>;<volume>29</volume>(<issue>9</issue>):<fpage>536</fpage>–<lpage>546</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2006.07.004" xlink:type="simple">10.1016/j.tins.2006.07.004</ext-link></comment> <object-id pub-id-type="pmid">16859758</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>