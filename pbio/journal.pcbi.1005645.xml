<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01967</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005645</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject><subj-group><subject>Hidden Markov models</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>Old World monkeys</subject><subj-group><subject>Macaque</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Neuron’s eye view: Inferring features of complex stimuli from neural responses</article-title>
<alt-title alt-title-type="running-head">Neuron’s eye view</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Chen</surname> <given-names>Xin</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Beck</surname> <given-names>Jeffrey M.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9876-7837</contrib-id>
<name name-style="western">
<surname>Pearson</surname> <given-names>John M.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Duke Institute for Brain Sciences, Duke University, Durham, North Carolina, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Center for Cognitive Neuroscience, Duke University, Durham, North Carolina, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Neurobiology, Duke University Medical Center, Durham, North Carolina, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Park</surname> <given-names>Il Memming</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Stony Brook University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> JP JB.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Supervision:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> XC JP.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> XC JB JP.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> XC JB JP.</p>
</list-item>
</list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">john.pearson@duke.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>8</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>21</day>
<month>8</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>8</issue>
<elocation-id>e1005645</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>12</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>6</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Chen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005645"/>
<abstract>
<p>Experiments that study neural encoding of stimuli at the level of individual neurons typically choose a small set of features present in the world—contrast and luminance for vision, pitch and intensity for sound—and assemble a stimulus set that systematically varies along these dimensions. Subsequent analysis of neural responses to these stimuli typically focuses on regression models, with experimenter-controlled features as predictors and spike counts or firing rates as responses. Unfortunately, this approach requires knowledge in advance about the relevant features coded by a given population of neurons. For domains as complex as social interaction or natural movement, however, the relevant feature space is poorly understood, and an arbitrary <italic>a priori</italic> choice of features may give rise to confirmation bias. Here, we present a Bayesian model for exploratory data analysis that is capable of automatically identifying the features present in unstructured stimuli based solely on neuronal responses. Our approach is unique within the class of latent state space models of neural activity in that it assumes that firing rates of neurons are sensitive to multiple discrete time-varying features tied to the <italic>stimulus</italic>, each of which has Markov (or semi-Markov) dynamics. That is, we are modeling neural activity as driven by multiple simultaneous stimulus features rather than intrinsic neural dynamics. We derive a fast variational Bayesian inference algorithm and show that it correctly recovers hidden features in synthetic data, as well as ground-truth stimulus features in a prototypical neural dataset. To demonstrate the utility of the algorithm, we also apply it to cluster neural responses and demonstrate successful recovery of features corresponding to monkeys and faces in the image set.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Many neuroscience experiments begin with a set of reduced stimuli designed to vary only along a small set of variables. Yet many phenomena of interest—natural movies, objects—are not easily parameterized by a small number of dimensions. Here, we develop a novel Bayesian model for clustering stimuli based solely on neural responses, allowing us to discover which latent features of complex stimuli actually drive neural activity. We demonstrate that this model allows us to recover key features of neural responses in a pair of well-studied paradigms.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000066</institution-id>
<institution>National Institute of Environmental Health Sciences</institution>
</institution-wrap>
</funding-source>
<award-id>K01ES025442</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9876-7837</contrib-id>
<name name-style="western">
<surname>Pearson</surname> <given-names>John</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work is supported by an NIH K01 career development award to JP (K01ES025442) through the Big Data to Knowledge Initiative, the National Institute for Neurological Disorders and Stroke (R21-NS-084176; JMP), and the NIH Common Fund. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-08-31</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Code and data in the manuscript are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/pearsonlab/spiketopics" xlink:type="simple">https://github.com/pearsonlab/spiketopics</ext-link>. Data from Roitman and Shadlen (2002) is publicly available at <ext-link ext-link-type="uri" xlink:href="https://www.shadlenlab.columbia.edu/resources/RoitmanDataCode.html" xlink:type="simple">https://www.shadlenlab.columbia.edu/resources/RoitmanDataCode.html</ext-link>. Access to data from McMahon et al. (2014) were provided by the authors of that work (David Leopold: <email xlink:type="simple">leopoldd@mail.nih.gov</email>) and are part of a planned forthcoming data release.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The question of how the brain encodes information from the natural world forms one of the primary areas of study within neuroscience. For many sensory systems, particularly vision and audition, the discovery that single neurons modulate their firing of action potentials in response to particular stimulus features has proven foundational for theories of sensory function. Indeed, neuronal responses to contrast, edges, and motion direction appear to form fundamental primitives on which higher-level visual abstractions are built. Nevertheless, many of these higher-level abstractions do not exist in a stimulus space with obvious axes. As a result, experimenters must choose <italic>a priori</italic> features of interest in constructing their stimulus sets, with the result that cells may appear weakly tuned due to misalignment of stimulus and neural axes.</p>
<p>For example, in vision, methods like reverse correlation have proven successful in elucidating response properties of some cell types, but such techniques rely on a well-behaved stimulus space and a highly constrained encoding model in order to achieve sufficient statistical power to perform inference [<xref ref-type="bibr" rid="pcbi.1005645.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005645.ref003">3</xref>]. However, natural stimuli are known to violate both criteria, generating patterns of neural activity that differ markedly from those observed in controlled experiments with limited stimulus complexity [<xref ref-type="bibr" rid="pcbi.1005645.ref003">3</xref>–<xref ref-type="bibr" rid="pcbi.1005645.ref005">5</xref>]. Information-based approaches have gone some way in addressing this challenge [<xref ref-type="bibr" rid="pcbi.1005645.ref004">4</xref>], but this approach assumes a metric structure on stimuli in order to perform optimization, and was recently shown to be strongly related to standard Poisson regression models [<xref ref-type="bibr" rid="pcbi.1005645.ref006">6</xref>].</p>
<p>More recently, Gallant and collaborators have tackled this problem in the context of fMRI, demonstrating that information present in the blood oxygen level-dependent (BOLD) signal is sufficient to classify and map the representation of natural movie stimuli across the brain [<xref ref-type="bibr" rid="pcbi.1005645.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005645.ref009">9</xref>]. These studies have used a number of modeling frameworks, from Latent Dirichlet Allocation for categorizing scene contents [<xref ref-type="bibr" rid="pcbi.1005645.ref009">9</xref>] to regularized linear regression [<xref ref-type="bibr" rid="pcbi.1005645.ref008">8</xref>] to sparse nonparametric models [<xref ref-type="bibr" rid="pcbi.1005645.ref007">7</xref>] in characterizing brain encoding of stimuli, but in each case, models were built on pre-labeled training data. Clearly, a method that could infer stimulus structure directly from neural data themselves could extend such work to less easily characterized stimulus sets like those depicting social interactions.</p>
<p>A rich body of previous work has addressed the problem of identifying low-dimensional latent dynamics underlying neural firing. Typically, these models assume a continuous latent state governed by a linear dynamical system [<xref ref-type="bibr" rid="pcbi.1005645.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1005645.ref017">17</xref>]. Using generalized linear models and latent linear dynamical systems as building blocks, these models have proven able to infer (functional) connectivity [<xref ref-type="bibr" rid="pcbi.1005645.ref010">10</xref>], estimate spike times from a calcium images [<xref ref-type="bibr" rid="pcbi.1005645.ref011">11</xref>], and identify subgroups of neurons that share response dynamics [<xref ref-type="bibr" rid="pcbi.1005645.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref017">17</xref>]. Inference in these models is generally performed via expectation maximization, though [<xref ref-type="bibr" rid="pcbi.1005645.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005645.ref019">19</xref>] also used a variational Bayesian approach. In each case, the focus has typically been on inferring the dynamics of intrinsic neural activity, perhaps conditioned on known covariates <bold>x</bold><sub><italic>t</italic></sub>. Our work is distinct, however, in focusing on inferring features within <italic>stimuli</italic> that drive repeatable patterns of firing across time and trials.</p>
<p>Our model sits at the intersection of these regression and latent variable approaches. We utilize a Poisson observation model that shares many of the same features as the commonly used generalized linear models for Poisson regression. We also assume that the latent features modulating neural activity are time-varying and Markov. However, we make 3 additional unique assumptions: First, we assume that the activity of each neuron is modulated by a combination of multiple independent latent features governed by Markov dynamics. (This can be extended to the semi-Markov case; see Supplementary Information). This allows for latents to evolve over multiple timescales with non-trivial duration distributions, much like the hand-labeled features in social interaction data sets. Second, we assume that these latents are tied to stimulus presentation. That is, when identical stimuli are presented, the <italic>same</italic> latents are also present. This allows us to selectively model the dynamics of latent features of the <italic>stimulus</italic> that drive neural activity, rather than intrinsic neural dynamics (e.g., variation within and across trials). Finally, we enforce a sparse hierarchical prior on modulation strength that effectively limits the number of latent features to which the population of neurons is selective. This allows for a parsimonious explanation of the firing rates of single units in terms of a small set of stimulus features. Finally, we perform full variational Bayesian inference on all model parameters and take advantage of conditional conjugacy to generate coordinate ascent update rules, nearly all of which are explicit. Combined with forward-backward inference for latent states, our algorithm is exceptionally fast, automatically implements Occam’s razor, and facilitates proper model comparisons using the variational lower bound.</p>
<p>However, as noted above, we are not the first to employ variational Bayesian methods to the problem of inferring latent firing rate states. Moreover, several other models have made use of the idea of discrete latent states and Markov models as explanations of neural dynamics [<xref ref-type="bibr" rid="pcbi.1005645.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref020">20</xref>]. Both of those methods used a Hidden Markov Model (HMM) to capture variability in neural firing in time and identify discrete modes or states of spiking that could be driven by both spike history and external covariates. In [<xref ref-type="bibr" rid="pcbi.1005645.ref019">19</xref>], this state space was assumed to be organized according to a binary tree, dramatically reducing model complexity. Our model differs from both of these in assuming that the states that govern firing are <italic>deterministic</italic> functions of stimuli, and that these states are a collection of discrete, independent stimulus features, not a single HMM. Thus, while previous models serve well to capture transitions between discrete states of neural activity, our model discovers statistically reliable patterns of activity that are consistent across repeated presentations of a given stimulus. By directly associating latent factors that drive firing with stimulus features, we thus achieve a means of (multiply) coding a given stimulus. That is, we focus on binary latent states as a means of labeling a finite number of overlapping stimulus features.</p>
<p>Most importantly, as we will show, the stimulus features found by our model are often <italic>interpretable</italic>. The choice to assign multiple discrete, independent tags to each stimulus results in a combinatorial code, with capacity exponential in the number of tags. This can, in principle, accommodate a hierarchical structure (as in [<xref ref-type="bibr" rid="pcbi.1005645.ref019">19</xref>]), but need not. Yet the ultimate goal of latent state models such as ours is to provide a low-dimensional <italic>description</italic> of neural responses, not simply a compression of them. In practice, experimentalists may perform an initial screening experiment by exposing an organism to a broad range of stimuli, with few fixed hypotheses about responsiveness. A given population of neurons may respond to only a few stimulus features, and features so inferred do not necessarily generalize to new brain structures, nor to stimuli outside the initial set. The value of our model, as with topic models and other latent space models, comes in identifying stimulus features that are readily interpretable: we expect our method will be most useful when the latent tags it identifies group stimuli into useful categories that generate hypotheses for future experiments.</p>
<p>In the sections below, we outline the mathematics behind our model, discuss the process of approximate Bayesian inference we use to infer stimulus features, and perform a series of validation experiments on both synthetic data and real data sets of spiking responses. In the latter, we have chosen datasets where the features that drive spiking are reasonably well understood. We train our model without using this information and then compare the inferred and experimenter-labeled features as a means of illuminating strengths and weaknesses of our model. We conclude by discussing possible extensions and applications to other domains.</p>
</sec>
<sec id="sec002">
<title>Model</title>
<sec id="sec003">
<title>Observation model</title>
<p>Consider a population of <italic>U</italic> spiking neurons or units exposed to a series of stimuli indexed by a discrete time index <italic>t</italic> ∈ {1 … <italic>T</italic>}. We assume that this time index is unique across all stimuli, such that a particular <italic>t</italic> represents a unique moment in a particular stimulus. In order to model repeated presentations of the same stimulus to the same neuron, we further assume that each neuron is exposed to a stimulus <italic>M</italic><sub><italic>tu</italic></sub> times, though we do not assume any relationship among <italic>M</italic><sub><italic>tu</italic></sub>. That is, we need not assume either that all neurons see each stimulus the same number of times, nor that each stimulus is seen by all neurons. It is thus typical, but not required, that <italic>M</italic><sub><italic>tu</italic></sub> be sparse, containing many 0s, as shown in <xref ref-type="fig" rid="pcbi.1005645.g001">Fig 1</xref>.</p>
<fig id="pcbi.1005645.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Observational model.</title>
<p>A: Stimuli are concatenated to form a single time series indexed by <italic>t</italic>. B: Individual experimental sessions draw from the available set of stimuli, with index <italic>m</italic> representing unique (time, unit) presentations. Example stimulus sequences for two experimental sessions are shown, with corresponding neuronal spike data. Note that the number of presentations of each stimulus can differ by unit, and that units need not be simultaneously recorded. Images copyright Geoff Gallice, (retrieved from Wikimedia Commons), kimumbert/Flickr and dvs/Flickr under CC-BY. Stim 23 image copyright J.M. Garg (used with permission).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g001" xlink:type="simple"/>
</fig>
<p>Each unique observation <italic>m</italic> in our data set consists of a spike count <italic>N</italic><sub><italic>m</italic></sub> for a particular (time, unit) pair (<italic>t</italic>(<italic>m</italic>), <italic>u</italic>(<italic>m</italic>)). We model these spike counts as arising from a Poisson distribution with rate Λ<sub><italic>tu</italic></sub> and observation-specific multiplicative overdispersion <italic>θ</italic><sub><italic>m</italic></sub>:
<disp-formula id="pcbi.1005645.e001"><alternatives><graphic id="pcbi.1005645.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:msub><mml:mi>N</mml:mi> <mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Pois</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mo>Λ</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mrow><mml:mtext>where</mml:mtext> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>θ</mml:mi> <mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
That is, for a given stimulus presentation, the spiking response is governed by the firing rate Λ (we set Δ<italic>t</italic> = 1 for convenience), specific to the stimulus and unit, along with a moment-by-moment noise in the unit’s gain, <italic>θ</italic><sub><italic>m</italic></sub>. We restrict these <italic>θ</italic><sub><italic>m</italic></sub> to follow a Gamma distribution with the same shape and rate parameters, since this results in an expected noise gain of 1. In practice, we model this noise as independent across observations, though it is possible to weaken this assumption, allowing for <italic>θ</italic><sub><italic>m</italic></sub> to be autocorrelated in time (see Supplementary Information). Note that both the unit and time are functions of the observation index <italic>m</italic>, and that the distribution of the overdispersion for each observation may be specific to the unit observed.</p>
</sec>
<sec id="sec004">
<title>Firing rate model</title>
<p>At each stimulus time <italic>t</italic>, we assume the existence of <italic>K</italic> binary latent states <italic>z</italic><sub><italic>tk</italic></sub> and <italic>R</italic> observed covariates <italic>x</italic><sub><italic>tr</italic></sub>. The binary latent states can be thought of as time-varying “tags” of each stimulus—for example, content labels for movie frames—and are modeled as Markov chains with initial state probabilities <italic>π</italic><sub><italic>k</italic></sub> and transition matrices <italic>A</italic><sub><italic>k</italic></sub>. The observed covariates, by contrast, are known to the experimenter and may include contrast, motion energy, or any other <italic>a priori</italic> variable of interest.</p>
<p>We further assume that each unit’s firing rate at a particular point in time can be modeled as arising from the product of three effects: (1) a baseline firing rate specific to each unit (λ<sub>0</sub>), (2) a product of responses to each latent state (λ<sub><italic>z</italic></sub>), and (3) a product of responses to each observed covariate (λ<sub><italic>x</italic></sub>):
<disp-formula id="pcbi.1005645.e002"><alternatives><graphic id="pcbi.1005645.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>Λ</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msup> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>R</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
Note that this is conceptually similar to the generalized linear model for firing rates (in which we model log Λ) with the identification <italic>β</italic> = log λ. However, by modeling the firing rate as a product and placing Gamma priors on the individual effects, we will be able to take advantage of closed-form variational updates resulting from conjugacy that avoid explicit optimization (see below). Note also, that because we assume the <italic>z</italic><sub><italic>tk</italic></sub> are binary, the second term in the product above simply represents the cumulative product of the gain effects for those features present in the stimulus at a given moment in time.</p>
<p>In addition, to enforce parsimony in our feature inference, we place sparse hierarchical priors with hyperparameters <italic>γ</italic> = (<italic>c</italic>, <italic>d</italic>) on the λ<sub><italic>z</italic></sub> terms:
<disp-formula id="pcbi.1005645.e003"><alternatives><graphic id="pcbi.1005645.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
That is, the population distribution for the responses to latent features is a gamma distribution, with parameters that are themselves gamma-distributed random variables. As a result, <inline-formula id="pcbi.1005645.e004"><alternatives><graphic id="pcbi.1005645.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mi>d</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and var[λ<sub><italic>u</italic></sub>] = (<italic>cd</italic><sup>2</sup>)<sup>−1</sup>, so in the special case of <italic>c</italic> large and <inline-formula id="pcbi.1005645.e005"><alternatives><graphic id="pcbi.1005645.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the prior for firing rate response to each latent feature will be strongly concentrated around gain 1 (no effect). As we show below, this particular choice results in a model that only infers features for which the data present strong evidence, controlling for spurious feature detection. In addition, this particular choice of priors leads to closed-form updates in our variational approximation. For the baseline terms, λ<sub>0<italic>u</italic></sub>, we use a non-sparse version of the same model; for the covariate responses, λ<sub><italic>xu</italic></sub>, we model the unit effects non-hierarchically, using independent Gamma priors for each unit.</p>
<p>Putting all this together, we then arrive at the full generative model:
<disp-formula id="pcbi.1005645.e006"><alternatives><graphic id="pcbi.1005645.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>|</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Λ</mml:mo> <mml:mo>|</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>|</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>|</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>π</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>π</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where
<disp-formula id="pcbi.1005645.e007"><alternatives><graphic id="pcbi.1005645.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>|</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
and
<disp-formula id="pcbi.1005645.e008"><alternatives><graphic id="pcbi.1005645.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
in conjunction with the definitions of <italic>p</italic>(<italic>N</italic>|Λ, <italic>θ</italic>) and Λ(λ, <italic>z</italic>, <italic>x</italic>) in Eqs <xref ref-type="disp-formula" rid="pcbi.1005645.e001">(1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005645.e002">(2)</xref>. The generative model for spike counts is illustrated in <xref ref-type="fig" rid="pcbi.1005645.g002">Fig 2</xref>.</p>
<fig id="pcbi.1005645.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Generative model for spike counts.</title>
<p>A: Counts are assumed Poisson-distributed, with firing rates Λ that depend on each unit’s baseline (λ<sub>0</sub>), as well as responses to both latent discrete states <italic>z</italic><sub><italic>t</italic></sub> (λ<sub><italic>z</italic></sub>) and observed covariates <italic>x</italic><sub><italic>t</italic></sub> (λ<sub><italic>x</italic></sub>) that change in time. <italic>γ</italic> nodes represent hyperparameters for the firing rate effects. <italic>θ</italic> is a multiplicative overdispersion term specific to each observation, distributed according to hyperparameters <italic>s</italic>. B: Spike counts <italic>N</italic> are observed for each of <italic>U</italic> units over stimulus time <italic>T</italic> for multiple presentations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Inference</title>
<p>Given a sequence of stimulus presentations (<italic>t</italic>(<italic>m</italic>), <italic>u</italic>(<italic>m</italic>)) and observed spike counts <italic>N</italic><sub><italic>m</italic></sub>, we want to infer both the model parameters Θ = (λ<sub>0</sub>, λ<sub><italic>z</italic></sub>, λ<sub><italic>x</italic></sub>, <italic>A</italic>, <italic>π</italic>, <italic>c</italic><sub>0</sub>, <italic>d</italic><sub>0</sub>, <italic>c</italic><sub><italic>z</italic></sub>, <italic>d</italic><sub><italic>z</italic></sub>, <italic>s</italic>) and latent variables <italic>Z</italic> = (<italic>z</italic><sub><italic>kt</italic></sub>, <italic>θ</italic><sub><italic>m</italic></sub>). That is, we wish to calculate the joint posterior density:
<disp-formula id="pcbi.1005645.e009"><alternatives><graphic id="pcbi.1005645.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>|</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∝</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>|</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
In general, calculating the normalization constant for this posterior is computationally intractable. Instead, we will use a variational approach, approximating <italic>p</italic>(Θ, <italic>Z</italic>|<italic>N</italic>) by a variational posterior <italic>q</italic>(<italic>Z</italic>, Θ) = <italic>q</italic><sub><italic>Z</italic></sub>(<italic>Z</italic>)<italic>q</italic><sub>Θ</sub>(Θ) that factorizes over parameters and latents but is nonetheless close to <italic>p</italic> as measured by the Kullback-Leibler divergence [<xref ref-type="bibr" rid="pcbi.1005645.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref022">22</xref>]. Equivalently, we wish to maximize the variational objective
<disp-formula id="pcbi.1005645.e010"><alternatives><graphic id="pcbi.1005645.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>≡</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>|</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>q</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>|</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">H</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>q</mml:mi> <mml:mo>Θ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">H</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>q</mml:mi> <mml:mi>Z</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
with <inline-formula id="pcbi.1005645.e011"><alternatives><graphic id="pcbi.1005645.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula> the entropy. We adopt the factorial HMM trick of [<xref ref-type="bibr" rid="pcbi.1005645.ref023">23</xref>], making the reasonable assumption that the posterior factorizes over each latent time series <italic>z</italic><sub>⋅<italic>k</italic></sub> and the overdispersion factor <italic>θ</italic><sub><italic>m</italic></sub>, as well as the rate parameters <italic>λ</italic><sub>⋅<italic>u</italic></sub> associated with each Markov process. This factorization results in a variational posterior of the form:
<disp-formula id="pcbi.1005645.e014"><alternatives><graphic id="pcbi.1005645.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>u</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>r</mml:mi></mml:munder> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>×</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo>∏</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
With this ansatz, the variational objective decomposes in a natural way, and choices are available for nearly all of the <italic>q</italic>s that lead to closed-form updates.</p>
</sec>
<sec id="sec006">
<title>Variational posterior</title>
<p>From Eqs <xref ref-type="disp-formula" rid="pcbi.1005645.e001">(1)</xref> and <xref ref-type="disp-formula" rid="pcbi.1005645.e002">(2)</xref> above, we can write the probability of the observed data <italic>N</italic> as
<disp-formula id="pcbi.1005645.e015"><alternatives><graphic id="pcbi.1005645.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∏</mml:mo></mml:mstyle><mml:mi>m</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mo>Λ</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mo>Λ</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:munder><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∏</mml:mo></mml:mstyle><mml:mi>k</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where again, <italic>m</italic> indexes observations of (<italic>t</italic>(<italic>m</italic>), <italic>u</italic>(<italic>m</italic>)) pairs, the portion in brackets is the Poisson likelihood for each bin count and the last two nontrivial terms represent the probability of the Markov sequence given by <italic>z</italic><sub><italic>tk</italic></sub>. From this, we can expand the log likelihood:
<disp-formula id="pcbi.1005645.e016"><alternatives><graphic id="pcbi.1005645.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>,</mml:mo> <mml:mi>z</mml:mi> <mml:mo>|</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>k</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:munder> <mml:mo>[</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:msub><mml:mo>Λ</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>(</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:msub> <mml:mo>+</mml:mo> <mml:mtext>constant,</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Given that <xref ref-type="disp-formula" rid="pcbi.1005645.e016">Eq (11)</xref> is of an exponential family form for <italic>θ</italic> and λ when conditioned on all other variables, free-form variational arguments [<xref ref-type="bibr" rid="pcbi.1005645.ref021">21</xref>] suggest variational posteriors:
<disp-formula id="pcbi.1005645.e017"><alternatives><graphic id="pcbi.1005645.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>u</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
<disp-formula id="pcbi.1005645.e018"><alternatives><graphic id="pcbi.1005645.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mi>u</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
<disp-formula id="pcbi.1005645.e019"><alternatives><graphic id="pcbi.1005645.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>u</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
For the first of these two, updates in terms of sufficient statistics involving expectations of <italic>γ</italic> = (<italic>c</italic>, <italic>d</italic>) are straightforward (see Supplementary Information). However, this relies on the fact that <italic>z</italic><sub><italic>t</italic></sub> ∈ {0, 1}. The observed covariates <italic>x</italic><sub><italic>t</italic></sub> follow no such restriction, which results in a transcendental equation for the <italic>β</italic><sub><italic>x</italic></sub> updates. In our implementation of the model, we solve this using an explicit BFGS optimization on each iteration. Moreover, we place non-hierarchical Gamma priors on these effects: λ<sub><italic>xur</italic></sub> ∼ Gamma(<italic>a</italic><sub><italic>xur</italic></sub>, <italic>b</italic><sub><italic>xur</italic></sub>).</p>
<p>As stated above, for the latent states and baselines, we assume hierarchical priors. This allows us to model each neuron’s firing rate response to a particular stimulus as being drawn from a population response to that same stimulus. We also assume that the moment-to-moment noise in firing rates, <italic>θ</italic><sub><italic>m</italic></sub>, follows a neuron-specific distribution. As a result of the form of this hierarchy given in <xref ref-type="disp-formula" rid="pcbi.1005645.e003">Eq (3)</xref>, the first piece in <xref ref-type="disp-formula" rid="pcbi.1005645.e010">Eq (8)</xref> contains multiple terms of the form
<disp-formula id="pcbi.1005645.e020"><alternatives><graphic id="pcbi.1005645.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>c</mml:mi> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:mi>d</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>c</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mi>c</mml:mi> <mml:mi>d</mml:mi> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mo>Γ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
In order to calculate the expectation, we make use of the following inequality [<xref ref-type="bibr" rid="pcbi.1005645.ref024">24</xref>]
<disp-formula id="pcbi.1005645.e021"><alternatives><graphic id="pcbi.1005645.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt> <mml:mo>≤</mml:mo> <mml:mfrac><mml:mrow><mml:mi>z</mml:mi> <mml:mo>!</mml:mo></mml:mrow> <mml:mrow><mml:msup><mml:mi>z</mml:mi> <mml:mrow><mml:mi>z</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>z</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>≤</mml:mo> <mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
to lower bound the negative gamma function and approximate the above as
<disp-formula id="pcbi.1005645.e022"><alternatives><graphic id="pcbi.1005645.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≥</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>c</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:mi>d</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>c</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mi>d</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mi>c</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
Clearly, the conditional probabilities for <italic>c</italic> and <italic>d</italic> are gamma in form, so that if we use priors <italic>c</italic> ∼ Gamma(<italic>a</italic><sub><italic>c</italic></sub>, <italic>b</italic><sub><italic>c</italic></sub>) and <italic>d</italic> ∼ Gamma(<italic>a</italic><sub><italic>d</italic></sub>, <italic>b</italic><sub><italic>d</italic></sub>) the posteriors have the form
<disp-formula id="pcbi.1005645.e023"><alternatives><graphic id="pcbi.1005645.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mi>c</mml:mi><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi>U</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mo>[</mml:mo> <mml:mi>d</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>d</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
<disp-formula id="pcbi.1005645.e024"><alternatives><graphic id="pcbi.1005645.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mi>d</mml:mi><mml:mrow><mml:mo>∼</mml:mo> <mml:mtext>Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>U</mml:mi> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>c</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>u</mml:mi></mml:munder> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>c</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>u</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
This basic form, with appropriate indices added, gives the update rules for the hyperparameter posteriors for λ<sub>0</sub> and λ<sub><italic>z</italic></sub>. For <italic>θ</italic>, we simply set <italic>c</italic> = <italic>s</italic><sub><italic>u</italic></sub> and <italic>d</italic> = 1.</p>
<p>For each latent variable <italic>z</italic>, the Markov Chain parameters <italic>π</italic><sub><italic>k</italic></sub> and <italic>A</italic><sub><italic>k</italic></sub>, together with the observation model <xref ref-type="disp-formula" rid="pcbi.1005645.e016">Eq (11)</xref> determine a Hidden Markov Model, for which inference can be performed efficiently via conjugate updates and the well-known forward-backward algorithm [<xref ref-type="bibr" rid="pcbi.1005645.ref025">25</xref>]. More explicitly, given <italic>π</italic>, <italic>A</italic>, and the emission probabilities for the observations, log <italic>p</italic>(<italic>N</italic>|<italic>z</italic>), the forward-backward algorithm returns the probabilities <italic>p</italic>(<italic>z</italic><sub><italic>t</italic></sub> = <italic>s</italic>) (posterior marginal), <italic>p</italic>(<italic>z</italic><sub><italic>t</italic>+1</sub> = <italic>s</italic>′, <italic>z</italic><sub><italic>t</italic></sub> = <italic>s</italic>) (two-slice marginal) and log <italic>Z</italic> (normalizing constant).</p>
<p>Our final algorithm is presented in Algorithm 1. Equation numbers reference posterior definitions in the text. Exact updates for the sufficient statistics are presented in Table 2 of <xref ref-type="supplementary-material" rid="pcbi.1005645.s001">S1 Text</xref>.</p>
<p><bold>Algorithm 1</bold> Iterative update for variational inference</p>
<p specific-use="line">1: <bold>procedure</bold> I<sc>terate</sc></p>
<p specific-use="line">2:  Update baselines λ<sub>0</sub>                  ▷ conjugate Gamma <xref ref-type="disp-formula" rid="pcbi.1005645.e017">Eq (12)</xref></p>
<p specific-use="line">3:  Update baseline hyperparameters <italic>γ</italic><sub>0</sub>       ▷ conjugate Gamma (Eqs <xref ref-type="disp-formula" rid="pcbi.1005645.e023">18</xref> and <xref ref-type="disp-formula" rid="pcbi.1005645.e024">19</xref>)</p>
<p specific-use="line">4:  <bold>for</bold> <italic>k</italic> = 1 … <italic>K</italic> <bold>do</bold></p>
<p specific-use="line">5:   Update firing rate effects λ<sub><italic>zk</italic></sub>              ▷ conjugate Gamma <xref ref-type="disp-formula" rid="pcbi.1005645.e018">Eq (13)</xref></p>
<p specific-use="line">6:   Update firing rate hyperparameters <italic>γ</italic><sub><italic>zk</italic></sub>      ▷ conjugate Gamma (Eqs <xref ref-type="disp-formula" rid="pcbi.1005645.e023">18</xref> and <xref ref-type="disp-formula" rid="pcbi.1005645.e024">19</xref>)</p>
<p specific-use="line">7:   Calculate expected log evidence <italic>η</italic><sub><italic>k</italic></sub>                    ▷ (S13)</p>
<p specific-use="line">8:   Update Markov chain parameters <inline-formula id="pcbi.1005645.e025"><alternatives><graphic id="pcbi.1005645.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>π</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>              ▷ (S11, S12)</p>
<p specific-use="line">9:   <italic>ξ</italic><sub><italic>k</italic></sub>, Ξ<sub><italic>k</italic></sub>, log <italic>Z</italic><sub><italic>k</italic></sub>← FORWARD-BACKWARD (<inline-formula id="pcbi.1005645.e026"><alternatives><graphic id="pcbi.1005645.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mi>η</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>π</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>)   ▷ [<xref ref-type="bibr" rid="pcbi.1005645.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref027">27</xref>]</p>
<p specific-use="line">10:   <bold>if</bold> semi-Markov <bold>then</bold></p>
<p specific-use="line">11:    Update duration distribution <italic>p</italic><sub><italic>k</italic></sub>(<italic>d</italic>|<italic>j</italic>)         ▷ BFGS optimization (S25)</p>
<p specific-use="line">12:   <bold>end if</bold></p>
<p specific-use="line">13:   Update cached <italic>F</italic>                           ▷ (S8)</p>
<p specific-use="line">14:  <bold>end for</bold></p>
<p specific-use="line">15:  Update covariate firing effects λ<sub><italic>x</italic></sub>       ▷ BFGS optimization (<xref ref-type="disp-formula" rid="pcbi.1005645.e019">Eq 14</xref>, S54, S55)</p>
<p specific-use="line">16:  Update cached <italic>G</italic>                            ▷ (S9)</p>
<p specific-use="line">17:  Update overdispersion <italic>θ</italic>            ▷ conjugate Gamma (Eqs <xref ref-type="disp-formula" rid="pcbi.1005645.e023">18</xref> and <xref ref-type="disp-formula" rid="pcbi.1005645.e024">19</xref>)</p>
<p specific-use="line">18: <bold>end procedure</bold></p>
</sec>
</sec>
<sec id="sec007" sec-type="results">
<title>Results</title>
<p>In this section, we report the results of three experiments illustrating the capabilities of our model. The first demonstrates the ability of our algorithm to recover ground truth latent features in a synthetic dataset with parameters similar to typical neural recording experiments. In the second and third, we use data from actual experiments in order to compare labels specified by experimenters with those recovered by our model. In each case, our model was only trained using stimulus identity, <italic>not</italic> experimenter labels, but nonetheless managed to recover key features that drove neural firing in the experiment. Code for all experiments and analysis is provided online (see Supplementary Information).</p>
<sec id="sec008">
<title>Synthetic data</title>
<p>We generated synthetic data from the model for <italic>U</italic> = 100 neurons for <italic>T</italic> = 10,000 time bins of <italic>dt</italic> = 0.0333<italic>s</italic> (≈ 6min of movies at 30 frames per second). Assumed firing rates and effect sizes were realistic for cortical neurons, with mean baseline rates of 10 spikes/s and firing rate effects given by a Gamma(1, 1) distribution for <italic>K</italic><sub>data</sub> = 3 latent features. In addition, we included <italic>R</italic> = 3 known covariates generated according to Markov dynamics. For this experiment, we assumed that each unit was presented only once with the stimulus time series, so that <italic>M</italic><sub><italic>tu</italic></sub> = 1. That is, we tested a case in which inference was driven primarily by variability in population responses across stimuli rather than pooling of data across repetitions of the same stimulus. Moreover, to test the model’s ability to parsimoniously infer features, we set <italic>K</italic> = 5. That is, we asked the model to recover more features than were present in the data. Finally, we placed hierarchical priors on neurons’ baseline firing rates and sparse hierarchical priors on firing rate effects of latent states. We used 10 random restarts and iterated over parameter updates until the fractional change in <inline-formula id="pcbi.1005645.e027"><alternatives><graphic id="pcbi.1005645.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mi mathvariant="script">L</mml:mi></mml:math></alternatives></inline-formula> dropped below 10<sup>−4</sup>.</p>
<p>As seen in <xref ref-type="fig" rid="pcbi.1005645.g003">Fig 3</xref>, the model correctly recovers only the features present in the original data. We quantified this by calculating the normalized mutual information <inline-formula id="pcbi.1005645.e012"><alternatives><graphic id="pcbi.1005645.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mover accent="true"><mml:mi>I</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≡</mml:mo> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msqrt><mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo> <mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, between the actual states and the inferred states, with <italic>H</italic>(<italic>Z</italic>) and <italic>I</italic> estimated by averaging the variational posteriors (both absolute and conditioned on observed states) across time. Note that superfluous features in the model have high posterior uncertainty for <italic>z</italic><sub><italic>k</italic></sub> and high posterior confidence for λ<sub><italic>zk</italic></sub> around 1 (no effect).</p>
<fig id="pcbi.1005645.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison of actual and inferred states of the synthetic data.</title>
<p>A: Ground truth binary latent features for a subset of stimulus times in the synthetic dataset. B: Recovered binary features for the same subset. Features have been reordered to facilitate comparisons with panel A. The unused features are in gray, indicating a high posterior uncertainty in the model. C: Population posterior distributions for inferred hyper parameters. Features 3 and 4 are effectively point masses around gain 1 (no population gain change in response to the feature), while features 1–3 approximate the Gamma(1, 1) data-generating model. D: Normalized mutual information between actual and inferred states.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Labeled neural data</title>
<p>We applied our model to a well-studied neural data set comprising single neuron recordings from macaque area LIP collected during the performance of a perceptual discrimination task [<xref ref-type="bibr" rid="pcbi.1005645.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref029">29</xref>]. In the experiment, stimuli consisted of randomly moving dots, some percentage of which moved coherently in either the preferred or anti-preferred direction of motion for each neuron. The animal’s task was to report the direction of motion. Thus, in addition to 5 coherence levels, each trial also varied based on whether the motion direction corresponded to the target in or out of the response field as depicted in <xref ref-type="fig" rid="pcbi.1005645.g004">Fig 4</xref>. (In the case of 0% coherence, the direction of motion was inherently ambiguous and coded according to the monkey’s eventual choice.) For our experiment, we only analyzed correct trials, on which the animal’s choice (target IN or OUT of response field) was synonymous with the direction of dot motion.</p>
<fig id="pcbi.1005645.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Comparison of actual and inferred states of the Roitman dataset.</title>
<p>A: Experimental design features. Each vertical block represents a single type of trial (combination of stimulus coherence and choice location). Features present on a particular trial are plotted in white, and duration of each feature within the stimulus presentation period is indicated by the width of the bar in the horizontal direction. B: Recovered features from the model. Note that model features 6–9 are unused and that Features 1 &amp; 2 closely track the In and Out features of the data, respectively. Shorter bars represent inferred features that lasted less than the full stimulus presentation period. C: Actual and predicted firing rates for the stimulus period. Note that the model infers stimulus categories from the data, including appropriate timing of differentiation between categories.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g004" xlink:type="simple"/>
</fig>
<p>We fit a model with <italic>K</italic> = 10 features and <italic>U</italic> = 27 units to neural responses from the 1-second stimulus presentation period of the task. Spike counts corresponded to bins of <italic>dt</italic> = 20ms. For this experiment, units were individually recorded, so each unit experienced a different number of presentations of each stimulus condition, implying a ragged observation matrix. As a result, this dataset tests the model’s ability to leverage shared task structure across multiple sessions of recording, demonstrating that simultaneously recorded units are not required for inference of latent states.</p>
<p>
<xref ref-type="fig" rid="pcbi.1005645.g004">Fig 4</xref> shows the experimental labels from the concatenated stimulus periods, along with labels inferred by our model. Once again, the model has left some features unused, but correctly discerned differences between stimuli in the unlabeled data. Even more importantly, though given the opportunity to infer ten distinct stimulus classes, the model has made use of only five. Moreover, the discovered features clearly recapitulate the factorial design of the experiment, with the two most prominent features, <italic>Z</italic><sub>1</sub> and <italic>Z</italic><sub>2</sub>, capturing complementary values of the variable with the largest effect in the experiment: whether or not the relevant target was inside our outside the receptive field of the recorded neuron. This difference can be observed in both the averaged experimental data and the predicted data from the model (see <xref ref-type="fig" rid="pcbi.1005645.g004">Fig 4C</xref>), where the largest differences are between the dotted and solid lines. Finally, we can ask whether the reconstructed firing rates are in quantitative agreement with the data estimates by calculating an RMS error for each curve in <xref ref-type="fig" rid="pcbi.1005645.g004">Fig 4C</xref>. That is, we calculate <inline-formula id="pcbi.1005645.e013"><alternatives><graphic id="pcbi.1005645.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005645.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msqrt><mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:msqrt></mml:math></alternatives></inline-formula> for each unit, where <italic>f</italic><sub><italic>i</italic></sub> is the inferred firing rate from the model, <italic>f</italic><sub><italic>a</italic></sub> is the mean firing rate estimated from data, and expectations are taken across time bins. For our model, these values range from 4% to 12% across coherence levels.</p>
<p>But the model also reproduces less obvious features: it correctly discriminates between two identical stimulus conditions (0% coherence) based on the monkey’s eventual decision (In vs Out). In addition, the model correctly captures the initial 200ms “dead time” during the stimulus period, in which firing rates remain at pre-stimulus baseline. (Note that the timing is locked to the stimulus and consistent across trials, not idiosyncratic to each trial as in [<xref ref-type="bibr" rid="pcbi.1005645.ref030">30</xref>].) Finally, the model resists detection of features with little support in the experimental data. For instance, while feature <italic>Z</italic><sub>4</sub> captures the large difference between 50% coherence and other stimuli, the model does not infer a difference between intermediate coherence levels that are indistinguishable in this particular dataset. That is, mismatches between ground truth labels and model-inferred features here reflect underlying ambiguities in the neural data, while the model’s inferred features correctly pick out those combinations of variables most responsible for differences in spiking across conditions.</p>
</sec>
<sec id="sec010">
<title>Visual category data</title>
<p>As a second test of our model, we applied our algorithm to a designed structured stimuli dataset comprising <italic>U</italic> = 56 neurons from macaque inferotemporal cortex [<xref ref-type="bibr" rid="pcbi.1005645.ref031">31</xref>]. These neurons were repeatedly presented with 96 stimuli comprising 8 categories (<italic>M</italic> = 1483 total trials, with each stimulus exposed between 12 to 19 times to each unit) comprising monkey faces, monkey bodies, whole monkeys, natural scenes, food, manmade objects, and patterns (<xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5A</xref>). Data consisted of spike time series, which we binned into a 300ms pre-stimulus baseline, a 300ms stimulus presentation period, and a 300ms post-stimulus period. Three trials were excluded because of the abnormal stimulus presentation period. To maximize interpretability of the results, we placed strong priors on the <italic>π</italic><sub><italic>k</italic></sub> to formalize the assumption that all features were off during the baseline period. We also modeled overdispersion with extremely weak priors to encourage the model to attribute fluctuations in firing to noise in preference to feature detection. We again fit <italic>K</italic> = 10 features with sparse hierarchical priors on population responses.</p>
<fig id="pcbi.1005645.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparison of actual and inferred states of the macaque dataset.</title>
<p>A: Experimenter-determined features for the IT data set. 96 stimuli comprising 8 categories were presented in 1483 trials, with each stimulus presented to each neuron ∼15 times. B: The inferred states from our model. Color represents the mean percent change in firing rate across the population in response to each feature. For clarity, features with mean population effects &lt;5% are not plotted. The model has inferred features corresponding to monkey close-ups, whole monkey photos, and most close-ups of monkey body parts. C: Actual and predicted spikes per second across all stimulus of neuron 089a. D. Actual and predicted spikes per second across all stimulus of neuron 100a. Error bars for data represent 95% credible intervals for firing rates inferred from observed data using a Poisson model with weak priors. Error bars on predictions are 95% credible intervals based on simulation from the approximate posterior for the plotted unit. Images copyright Geoff Gallice, kimubert/Flickr, dvs/Flickr, Julien Harneis, and Celtus/WikiMedia under CC-BY. Second and third monkey images copyright J.M. Garg (used with permission).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g005" xlink:type="simple"/>
</fig>
<p>The inferred categories based on binned population responses are shown in <xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5B</xref>. For clarity, in <xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5</xref>, we only show population mean effects with a &gt; 5% gain modulation sorted from the highest to the lowest, though the full set of inferred states can be found in <xref ref-type="fig" rid="pcbi.1005645.g006">Fig 6</xref>. Out of the original categories, our model successfully recovers three features clearly corresponding to categories involving monkeys (Features 0–2). These can be viewed additively, with Feature 0 exclusive to monkey face close-ups, Feature 1 any photo containing a monkey face, either near or far; and Feature 2 any image containing a monkey body part (including faces); but as we will argue, given the nature of the model, it may be better to view these as a “combinatorial” code, with monkey close-ups encoded as 0&amp;1&amp;2 (∼ 59.46% increase in firing), whole monkeys as 1&amp;2 (∼ 32.47% increase), and monkey body parts as 2 (∼ 7.62% increase). Of course, this is consistent with what was found in [<xref ref-type="bibr" rid="pcbi.1005645.ref031">31</xref>], though our model used no labels on the images. And our interpretation that these neurons are sensitive to close-ups and faraway face and body parts is consistent with findings by another study using different experimental settings [<xref ref-type="bibr" rid="pcbi.1005645.ref032">32</xref>].</p>
<fig id="pcbi.1005645.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005645.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Small features suggest additional neural hypotheses.</title>
<p>A: Zoomed-in view of <xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5A</xref>, focusing on the first 24 images. B: The feature combinations 0&amp;1&amp;2&amp;4 (Group 1) and 0&amp;1&amp;2 (Group 2) are distinguished by direct vs. indirect gaze. Only Stimulus 5, coded 0&amp;1&amp;2&amp;5, is missing from Group 2. Images are for illustration only. Stims 2, 4, and 10 correspond to images in the original data set; other images approximate stimuli for which publication permission could not be obtained. Images copyright jinterwas/Flickr (Stim 2), Geoff Gallice (Stim 4) under CC-BY. Stim 10 copyright J.M. Garg (used with permission). All others in the public domain.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.g006" xlink:type="simple"/>
</fig>
<p>Again, as noted above, our results in <xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5A and 5B</xref> indicate predicted population responses, derived from the hierarchical prior. As evidenced in <xref ref-type="fig" rid="pcbi.1005645.g005">Fig 5C and 5D</xref>, individual neuron effects could be much larger. These panels show data for two example units, along with the model’s prediction. Clearly, the model recapitulates the largest distinctions between images in the data, though the assumption that firing rates should be the same for all images with similar features fails to capture some variability in the results. Here, RMS errors range from 16% to 238% across units, with most units showing at least qualitative agreement from only a handful of presentations of each stimulus. Even so, uncertainties in the predicted firing rates are also in line with uncertainties from those of observed rates, indicating that our model is correctly accounting for trial-to-trial noise.</p>
<p>Finally, even the weaker, sparser features inferred by our model captured intriguing additional information. As shown in <xref ref-type="fig" rid="pcbi.1005645.g006">Fig 6</xref>, Feature 4, a feature only weakly present in the population as a whole (and thus ignored in <xref ref-type="fig" rid="pcbi.1005645.g006">Fig 6A</xref>), when combined with the stronger Features 0, 1, and 2, successfully distinguishes between the monkey close-ups with direct and averted gaze. (Stimulus 5, with averted gaze, is additionally tagged with Feature 5, which we view as an imperfect match.) Thus, despite the fact that Feature 4 is barely a 3.4% gain change over the population, it suggests a link between neural firing and gaze direction, one for which there happens to be ample evidence [<xref ref-type="bibr" rid="pcbi.1005645.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1005645.ref034">34</xref>]. Similarly, Feature 5, barely a 1.1% effect, correctly tags three of the four close-ups with rightward gaze (with one false positive). Clearly, neither of these results is dispositive in this particular dataset, but in the absence of hypotheses about the effect of head orientation and gaze on neuronal firing, these minor features might suggest hypotheses for future experiments.</p>
<p>An additional feature of our approach is that the generated labels provide a concise and fairly complete summary of the stimulus-related activity of all neural recordings, which can be observed by comparing the categorization performance of decoded neural activity to the categorization performance of the decoded features. Although our model is not a data compression method, it nonetheless preserves most of the information about image category contained in the <italic>N</italic> = 56 dimensional spike counts via a 10-dimensional binary code. That is, using a sparse logistic regression on two-bit and three-bit combinations of our features to predict stimulus category ties and outperforms, respectively a multinomial logistic regression on the raw spike counts (see Supplementary Information).</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>Here, we have proposed and implemented a method for learning features in stimuli via the responses of populations of spiking neurons. This work addresses a growing trend in systems neuroscience—the increasing use of rich and unstructured or structured stimulus sets—without requiring either expert labeling or a metric on the stimulus space. As such, we expect it to be of particular use in disciplines like social neuroscience, olfaction, and other areas in which the real world is complex and strong hypotheses about the forms of the neural code are lacking. By learning features of interest to neural populations directly from neural data, we stand to generate unexpected, more accurate (less biased) hypotheses regarding the neural representation of the external world.</p>
<p>Here, we have validated this method using structured, labeled stimuli more typical of neuroscience experiments, showing that our model is capable of parsimoniously and correctly inferring features in the low signal-to-noise regime of cortical activity, even in the case of independently recorded neurons. Furthermore, by employing a fully variational, Bayesian approach to inference, we gain three key advantages: First, we gain the advantages of Bayesianism in general: estimates of confidence in inferences, parsimony and regularization via priors, and the ability to do principled model comparison. Second, variational methods scale well to large datasets and can be easily parallelized when combining data from multiple recording sessions. Finally, variational methods are fast, in that they typically converge within only a few tens of iterations and in many case (such as ours) can be implemented using explicit coordinate update rules, eliminating the need to tune a learning rate.</p>
<p>Finally, even small features in our model recapitulated known physiological results regarding face encoding in single neurons. And while these features alone might not provide proof positive of, e.g., viewpoint tuning, similar findings would be valuable in generating hypotheses in cases where the stimulus space and its neural correlates remain poorly understood. Thus our model facilitates an iterative experimental process: subjects are first be exposed to large, heterogeneous data; stimuli are then tagged based on neural responses; and finally, features with the largest effects are used to refine the set until it most accurately represents those stimuli with the largest neural correlates. Combined with the modularity of this and similar approaches, such models provide a promising opportunity to “build out” additional features that will meet the challenges of the next generation of experimental data.</p>
</sec>
<sec id="sec012">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005645.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Mathematical details.</title>
<p>Derivation of ELBO and Inference.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005645.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.s002" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Inferred latents as classification features.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005645.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005645.s003" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Effects of bin size and dynamics.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank David McMahon and David Leopold for generously sharing the visual cateogry stimuli and neural data from [<xref ref-type="bibr" rid="pcbi.1005645.ref031">31</xref>] and for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005645.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steveninck DRV</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Real-Time Performance of a Movement-Sensitive Neuron in the Blowfly Visual System: Coding and Information Transfer in Short Spike Sequences</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source>. <year>1988</year>;<volume>234</volume>(<issue>1277</issue>):<fpage>379</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1988.0055" xlink:type="simple">10.1098/rspb.1988.0055</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>R</given-names></name>. <article-title>Reverse correlation in neurophysiology</article-title>. <source>Cognitive Science</source>. <year>2004</year>;<volume>28</volume>(<issue>2</issue>):<fpage>147</fpage>–<lpage>166</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/s15516709cog2802_2" xlink:type="simple">10.1207/s15516709cog2802_2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringach</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Shapley</surname> <given-names>R</given-names></name>. <article-title>Receptive field structure of neurons in monkey primary visual cortex revealed by stimulation with natural image sequences</article-title>. <source>Journal of vision</source>. <year>2002</year>;<volume>2</volume>(<issue>1</issue>):<fpage>2</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/2.1.2" xlink:type="simple">10.1167/2.1.2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sharpee</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rust</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title>. <source>Neural computation</source>. <year>2004</year>;<volume>16</volume>(<issue>2</issue>):<fpage>223</fpage>–<lpage>250</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976604322742010" xlink:type="simple">10.1162/089976604322742010</ext-link></comment> <object-id pub-id-type="pmid">15006095</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vinje</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source>Science</source>. <year>2000</year>;<volume>287</volume>(<issue>5456</issue>):<fpage>1273</fpage>–<lpage>1276</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.287.5456.1273" xlink:type="simple">10.1126/science.287.5456.1273</ext-link></comment> <object-id pub-id-type="pmid">10678835</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Williamson</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <source>The equivalence of information-theoretic and likelihood-based methods for neural dimensionality reduction</source>. <year>2013</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vu</surname> <given-names>VQ</given-names></name>, <name name-style="western"><surname>Ravikumar</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>B</given-names></name>. <article-title>Encoding and Decoding V1 fMRI Responses to Natural Images with Sparse Nonparametric Models</article-title>. <source>Ann Appl Stat</source>. <year>2011</year>;<volume>5</volume>(<issue>2B</issue>):<fpage>1159</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/11-AOAS476" xlink:type="simple">10.1214/11-AOAS476</ext-link></comment> <object-id pub-id-type="pmid">22523529</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huth</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Nishimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vu</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>(<issue>6</issue>):<fpage>1210</fpage>–<lpage>1224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.10.014" xlink:type="simple">10.1016/j.neuron.2012.10.014</ext-link></comment> <object-id pub-id-type="pmid">23259955</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stansbury</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Natural scene statistics account for the representation of scene categories in human visual cortex</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>5</issue>):<fpage>1025</fpage>–<lpage>1034</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.06.034" xlink:type="simple">10.1016/j.neuron.2013.06.034</ext-link></comment> <object-id pub-id-type="pmid">23932491</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>(<issue>7207</issue>):<fpage>995</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vogelstein</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Watson</surname> <given-names>BO</given-names></name>, <name name-style="western"><surname>Packer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Yuste</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jedynak</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Spike inference from calcium imaging using sequential Monte Carlo methods</article-title>. <source>Biophys J</source>. <year>2009</year>;<volume>97</volume>(<issue>2</issue>):<fpage>636</fpage>–<lpage>655</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bpj.2008.08.005" xlink:type="simple">10.1016/j.bpj.2008.08.005</ext-link></comment> <object-id pub-id-type="pmid">19619479</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Park</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>MLR</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Encoding and decoding in parietal cortex during sensorimotor decision-making</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>10</issue>):<fpage>1395</fpage>–<lpage>1403</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3800" xlink:type="simple">10.1038/nn.3800</ext-link></comment> <object-id pub-id-type="pmid">25174005</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">
Buesing L, Machado TA, Cunningham JP, Paninski L. Clustered factor analysis of multineuronal spike data. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ, editors. Advances in Neural Information Processing Systems 27. Curran Associates, Inc.; 2014. p. 3500–3508.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Archer E, Park IM, Buesing L, Cunningham J, Paninski L. Black box variational inference for state space models. 2015;.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">
Park M, Bohner G, Macke JH. Unlocking neural population non-stationarities using hierarchical dynamics models. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R, editors. Advances in Neural Information Processing Systems 28. Curran Associates, Inc.; 2015. p. 145–153.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref016">
<label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhao Y, Park IM. Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref017">
<label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Gao Y, Archer E, Paninski L, Cunningham JP. Linear dynamical neural population models through nonlinear embeddings. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">
Ulrich KR, Carlson DE, Lian W, Borg JS, Dzirasa K, Carin L. Analysis of Brain States from Multi-Region LFP Time-Series. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ, editors. Advances in Neural Information Processing Systems 27. Curran Associates, Inc.; 2014. p. 2483–2491.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">
Putzky P, Franzen F, Bassetto G, Macke JH. A Bayesian model for identifying hierarchically organised states in neural population activity. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ, editors. Advances in Neural Information Processing Systems 27. Curran Associates, Inc.; 2014. p. 3095–3103.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Escola</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fontanini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Katz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Hidden Markov models for the stimulus-response relationships of multistate neural systems</article-title>. <source>Neural Comput</source>. <year>2011</year>;<volume>23</volume>(<issue>5</issue>):<fpage>1071</fpage>–<lpage>1132</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00118" xlink:type="simple">10.1162/NECO_a_00118</ext-link></comment> <object-id pub-id-type="pmid">21299424</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wainwright</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>. <article-title>Graphical Models, Exponential Families, and Variational Inference</article-title>. <source>Found Trends Mach Learn</source>. <year>2008</year>;<volume>1</volume>(<issue>1-2</issue>):<fpage>1</fpage>–<lpage>305</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Blei DM, Kucukelbir A, McAuliffe JD. Variational Inference: A Review for Statisticians. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Jordan</surname> <given-names>MI</given-names></name>. <article-title>Factorial hidden Markov models</article-title>. <source>Machine learning</source>. <year>1997</year>;<volume>29</volume>(<issue>2-3)</issue>:<fpage>245</fpage>–<lpage>273</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1007425814087" xlink:type="simple">10.1023/A:1007425814087</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref024">
<label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Abramowitz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Stegun</surname> <given-names>IA</given-names></name>. <source>Handbook of mathematical functions: with formulas, graphs, and mathematical tables</source>. <volume>55</volume>. <publisher-name>Courier Corporation</publisher-name>; <year>1964</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref025">
<label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Beal</surname> <given-names>MJ</given-names></name>. <source>Variational algorithms for approximate Bayesian inference</source>. <publisher-name>University of London</publisher-name>; <year>2003</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref026">
<label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Murphy</surname> <given-names>KP</given-names></name>. <source>Machine learning: a probabilistic perspective</source>. <publisher-name>MIT press</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yu</surname> <given-names>SZ</given-names></name>, <name name-style="western"><surname>Kobayashi</surname> <given-names>H</given-names></name>. <article-title>Practical implementation of an efficient forward-backward algorithm for an explicit-duration hidden Markov model</article-title>. <source>Signal Processing, IEEE Transactions on</source>. <year>2006</year>;<volume>54</volume>(<issue>5</issue>):<fpage>1947</fpage>–<lpage>1951</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSP.2006.872540" xlink:type="simple">10.1109/TSP.2006.872540</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roitman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title>. <source>The Journal of neuroscience</source>. <year>2002</year>;<volume>22</volume>(<issue>21</issue>):<fpage>9475</fpage>–<lpage>9489</lpage>. <object-id pub-id-type="pmid">12417672</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Roitman Data and Code;. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.shadlenlab.columbia.edu/resources/RoitmanDataCode.html" xlink:type="simple">https://www.shadlenlab.columbia.edu/resources/RoitmanDataCode.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1005645.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Latimer</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Yates</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>MLR</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source>. <year>2015</year>;<volume>349</volume>(<issue>6244</issue>):<fpage>184</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.aaa4056" xlink:type="simple">10.1126/science.aaa4056</ext-link></comment> <object-id pub-id-type="pmid">26160947</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McMahon</surname> <given-names>DBT</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Bondar</surname> <given-names>IV</given-names></name>, <name name-style="western"><surname>Leopold</surname> <given-names>DA</given-names></name>. <article-title>Face-selective neurons maintain consistent visual responses across months</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>(<issue>22</issue>):<fpage>8251</fpage>–<lpage>8256</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1318331111" xlink:type="simple">10.1073/pnas.1318331111</ext-link></comment> <object-id pub-id-type="pmid">24799679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McMahon</surname> <given-names>DBT</given-names></name>, <name name-style="western"><surname>Russ</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Elnaiem</surname> <given-names>HD</given-names></name>, <name name-style="western"><surname>Kurnikova</surname> <given-names>AI</given-names></name>, <name name-style="western"><surname>Leopold</surname> <given-names>DA</given-names></name>. <article-title>Single-Unit Activity during Natural Vision: Diversity, Consistency, and Spatial Sensitivity among AF Face Patch Neurons</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>14</issue>):<fpage>5537</fpage>–<lpage>5548</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3825-14.2015" xlink:type="simple">10.1523/JNEUROSCI.3825-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25855170</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Perrett</surname> <given-names>DI</given-names></name>, <name name-style="western"><surname>Hietanen</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Oram</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Benson</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>. <article-title>Organization and Functions of Cells Responsive to Faces in the Temporal Cortex [and Discussion]</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>1992</year>;<volume>335</volume>(<issue>1273</issue>):<fpage>23</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.1992.0003" xlink:type="simple">10.1098/rstb.1992.0003</ext-link></comment> <object-id pub-id-type="pmid">1348133</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005645.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Functional Compartmentalization and Viewpoint Generalization Within the Macaque Face-Processing System</article-title>. <source>Science</source>. <year>2010</year>;<volume>330</volume>(<issue>6005</issue>):<fpage>845</fpage>–<lpage>851</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1194908" xlink:type="simple">10.1126/science.1194908</ext-link></comment> <object-id pub-id-type="pmid">21051642</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>