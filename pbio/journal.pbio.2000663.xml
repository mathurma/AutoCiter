<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2000663</article-id>
<article-id pub-id-type="publisher-id">pbio.2000663</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonology</subject><subj-group><subject>Syntax</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Neurolinguistics</subject><subj-group><subject>Sentence processing</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurolinguistics</subject><subj-group><subject>Sentence processing</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Neurolinguistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurolinguistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A mechanism for the cortical computation of hierarchical linguistic structure</article-title>
<alt-title alt-title-type="running-head">A mechanism for linguistic cortical computation</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3395-7234</contrib-id>
<name name-style="western">
<surname>Martin</surname>
<given-names>Andrea E.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Doumas</surname>
<given-names>Leonidas A. A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, School of Philosophy, Psychology, and Language Sciences, University of Edinburgh, Edinburgh, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Psychology of Language, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Poeppel</surname>
<given-names>David</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>New York University, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">andrea.martin@mpi.nl</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>2</day>
<month>3</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2017</year>
</pub-date>
<volume>15</volume>
<issue>3</issue>
<elocation-id>e2000663</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>7</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>2</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Martin, Doumas</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2000663"/>
<abstract>
<p>Biological systems often detect species-specific signals in the environment. In humans, speech and language are species-specific signals of fundamental biological importance. To detect the linguistic signal, human brains must form hierarchical representations from a sequence of perceptual inputs distributed in time. What mechanism underlies this ability? One hypothesis is that the brain repurposed an available neurobiological mechanism when hierarchical linguistic representation became an efficient solution to a computational problem posed to the organism. Under such an account, a single mechanism must have the capacity to perform multiple, functionally related computations, e.g., detect the linguistic signal <italic>and</italic> perform other cognitive functions, while, ideally, oscillating like the human brain. We show that a computational model of analogy, built for an entirely different purpose—learning relational reasoning—processes sentences, represents their meaning, and, crucially, exhibits oscillatory activation patterns resembling cortical signals elicited by the same stimuli. Such redundancy in the cortical and machine signals is indicative of formal and mechanistic alignment between representational structure building and “cortical” oscillations. By inductive inference, this synergy suggests that the cortical signal reflects structure generation, just as the machine signal does. A single mechanism—using time to encode information across a layered network—generates the kind of (de)compositional representational hierarchy that is crucial for human language and offers a mechanistic linking hypothesis between linguistic representation and cortical computation.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Human language is a fundamental biological signal with computational properties that differ from other perception-action systems: hierarchical relationships between sounds, words, phrases, and sentences and the unbounded ability to combine smaller units into larger ones, resulting in a "discrete infinity" of expressions. These properties have long made language hard to account for from a biological systems perspective and within models of cognition. We argue that a single computational mechanism—using time to encode hierarchy—can satisfy the computational requirements of language, in addition to those of other cognitive functions. We show that a well-supported neural network model of analogy oscillates like the human brain while processing sentences. Despite being built for an entirely different purpose (learning relational concepts), the model processes hierarchical representations of sentences and exhibits oscillatory patterns of activation that closely resemble the human cortical response to the same stimuli. From the model, we derive an explicit computational mechanism for how the human brain could convert perceptual features into hierarchical representations across multiple timescales, providing a linking hypothesis between linguistic and cortical computation. Our results suggest a formal and mechanistic alignment between representational structure building and cortical oscillations that has broad implications for discovering the computational first principles of cognition in the human brain.</p>
</abstract>
<funding-group>
<funding-statement>Economic and Social Research Council of the United Kingdom esrc.ac.uk (grant number ES/K009095/1). Grant to AEM. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="9"/>
<table-count count="0"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All files (data, stimuli, and code) are available in the Open Science Framework data repository (<ext-link ext-link-type="uri" xlink:href="https://osf.io/eb2vp/" xlink:type="simple">https://osf.io/eb2vp/</ext-link>) and on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/AlexDoumas/dingetal_sent" xlink:type="simple">https://github.com/AlexDoumas/dingetal_sent</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Detecting relevant signals in the environment is a crucial function in biological systems. For humans, language is a critical, if not the defining, species-specific environmental signal to detect. As such, it is not surprising that the human auditory system is specialised for speech processing [e.g., <xref ref-type="bibr" rid="pbio.2000663.ref001">1</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref002">2</xref>]. However, very little is known about the biological mechanisms that detect the “linguistic signal” within speech (i.e., words, phrases, sentences, meaning), apart from the fact that that cortical entrainment to the acoustic envelope of speech likely plays a fundamental role in spoken language comprehension [<xref ref-type="bibr" rid="pbio.2000663.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref004">4</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref005">5</xref>]. In this paper, we show that using time to encode information about the structural relationship between representations within the linguistic signal, or <italic>time-based binding</italic>, can generate the kinds of representations that can support human language within a layered neural network and produces oscillations that are highly similar to human cortical signals. By inductive inference, time-based binding, or a formal equivalent, can support language-related cortical computation.</p>
<p>In order to detect linguistic signals in the environment, the human brain requires a computational system that can generate hierarchical representations from the sequential perceptual input of speech or text. Generating abstract, higher-level representations like sentences from either sensory source is likely to rely on simultaneous tracking of multiple levels of the linguistic signal [e.g., <xref ref-type="bibr" rid="pbio.2000663.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref007">7</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref008">8</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref009">9</xref>]. However, operating over these multiple levels of signal must be a mechanism that can compute structured, discrete representations from unstructured continuous input in time—essentially, an operation that can form representations from feature sets, as in other areas of perception [<xref ref-type="bibr" rid="pbio.2000663.ref010">10</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref013">13</xref>]. However, complications arise because human language has computational properties that set it apart from other domains of cognition and perception, namely, representation of discrete infinity, arbitrary form-meaning correspondence, and learnability constraints [<xref ref-type="bibr" rid="pbio.2000663.ref012">12</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref014">14</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref015">15</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref016">16</xref>]. One important property is <italic>compositionality</italic>—when interpreting a simple sentence like <italic>Fun games waste time</italic>, being a proposition in human language requires that the representation of <italic>fun</italic> as a discrete word is maintained, even after computation of the phrases <italic>Fun games</italic>, <italic>Fun games waste</italic>, and the sentence <italic>Fun games waste time</italic>. To make things more complex, this stringent representational requirement holds below, and beyond, the word-level: both low-level (e.g., phonetic, syllabic, and orthographic) and high-level (e.g., phrases, sentences, event-structures, discourse) representations have discrete hierarchy in the face of compositionality. Linguistic computation in the human brain, therefore, must generate a representational hierarchy that can represent the compositional product of input representations while maintaining discrete input units [<xref ref-type="bibr" rid="pbio.2000663.ref017">17</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref018">18</xref>].</p>
<p>Such a representational hierarchy suggests that a first principle of linguistic computation is a form of relationality: the system must determine whether (or not) to relate and (then compose) input representations with one another. One way to achieve compositionality without sacrificing information is to represent the relationship between input and output representations explicitly [<xref ref-type="bibr" rid="pbio.2000663.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]. In a basic sense, this computational situation is akin to that which the system faces during analogical or relational reasoning, in which the relation between representations must be computed. For example, when the system must discriminate the conceptual propositions "John loves Mary" from "Mary loves John," it must discriminate sequences of identical input arguments that have very different consequents by virtue of their relational structure [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref021">21</xref>]. Thus, it appears that the encoding of relational structure is a powerful tool for a biological system that solves the problems that human cognition evolved in response to. Moreover, from a biological systems perspective, it is highly desirable for a single (cortical, computational) mechanism to have the capacity to compute over multiple domains, or to function as a subroutine in multiple cognitive functions [<xref ref-type="bibr" rid="pbio.2000663.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref023">23</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref024">24</xref>]. Below, we describe the computational challenge of (de)compositionality and describe a mechanism that can encode and preserve hierarchical relational structures from unstructured input within a layered neural network. Our approach contrasts with extant associative models of speech and language processing, cognition, and cortical microcircuits.</p>
<p>If you can understand the sentence <italic>Fun games waste time</italic>, you typically also know that the verb <italic>waste</italic> can combine with phrases other than <italic>fun games</italic> and nouns other than <italic>time</italic> without changing the meaning or syntactic function of <italic>waste</italic>. One way to achieve this functionality is to represent <italic>waste</italic> independently from <italic>fun games</italic> and <italic>time</italic> while also generating representations of the (grammatical) relations between the inputs <italic>fun</italic>, <italic>games</italic>, <italic>waste</italic>, and <italic>time</italic>. Those representations of grammatical relations can then be generatively applied to other inputs. A linear form of such a representation would be something like:
<disp-formula id="pbio.2000663.e001">
<alternatives>
<graphic id="pbio.2000663.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2000663.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>fun</mml:mtext></mml:mrow><mml:mrow><mml:mtext>adj</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>games</mml:mtext></mml:mrow><mml:mtext>n</mml:mtext></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mtext>NP</mml:mtext></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mtext>ADJP</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>waste</mml:mtext></mml:mrow><mml:mtext>v</mml:mtext></mml:msub><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mtext>time</mml:mtext></mml:mrow><mml:mtext>n</mml:mtext></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mtext>NP</mml:mtext></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mtext>VP</mml:mtext></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mtext>IP</mml:mtext></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where the most distal level of brackets codes the whole sentence representation, and another set of brackets codes the adjective, noun, and verb phrases. The curly brackets in <xref ref-type="disp-formula" rid="pbio.2000663.e001">Eq (1)</xref> indicate phrasal grouping in an approximation of formal linguistic notation, where IP = inflectional phrase or sentence, AdjP = adjective phrase, NP = noun phrase, VP = verb phrase, and adj = adjective, n = noun, v = verb. <italic>Fun games</italic> is an adjective phrase that contains the adjective <italic>fun</italic> and the noun phrase that contains the noun <italic>games</italic>. That adjective phrase is composed with a verb phrase, which, itself, is the product of combing the verb <italic>waste</italic> with the noun phrase containing the noun <italic>time</italic>.</p>
<p>In contrast, systems that do not encode relational structures in inputs, such as traditional recurrent neural networks (RNNs), would represent <italic>fun games waste time</italic> by creating conjunctive representation, such that {<italic>fun</italic>} and {<italic>games</italic>} becomes the holistic {<italic>fun games</italic>}, losing any internal distinction between {<italic>fun</italic>} and {<italic>games</italic>}. The end product representation would be something like {<italic>fun games waste time</italic>}, with no subunit of the sentence being independently represented. In such a system, {<italic>fun games waste time</italic>} would bear no relation to {<italic>games waste time</italic>} or {<italic>games are fun</italic>} nor to {<italic>long showers waste water</italic>} because <italic>waste</italic> is not represented independently [<xref ref-type="bibr" rid="pbio.2000663.ref025">25</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref026">26</xref>]. More generally, systems that use tensor products (the outer product of two vectors or matrices) to represent or bind stimuli together in the network suffer from the same problem in that once you bind the two things together in the network, you cannot separate them anymore. This pitfall stems from the fact that there are multiple solutions to decomposing a tensor into its input vectors—you can never know which solution is "the right one" for the original input vectors once you have multiplied them. That means you cannot compose and decompose representations in a tensor system without losing information, which is unsuitable for linguistic representation and for any neural or biological system that must be both compositional and hierarchical [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>].</p>
<p>To circumvent this problem, relational structures can be encoded via <italic>binding</italic> [<xref ref-type="bibr" rid="pbio.2000663.ref013">13</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref027">27</xref>]. We define binding for our purposes here as the representational state where two codes in the network are linked together for processing but where their representations are not defined by this particular instance of binding. Such a mechanism allows the system to both maintain independent representations of input units while also composing the same representations together during processing, as needed. In sum, our focus here is on the representation of sequences in which implicit ordinal and time-sensitive relationships matter, and, in fact, may signal the hierarchical relationships that have been compressed into that sequence. The basic idea is to encode the elements that are bound in lower layers of a hierarchy directly from the sequential input and then use slower dynamics to accumulate evidence for relations at higher levels of the hierarchy. This necessarily entails a memory of the ordinal relationships that, computationally, requires higher-level representations to integrate or bind lower-level representations over time—with more protracted activity. This temporal binding mandates an asynchrony of representation between hierarchical levels of representation in order to maintain distinct, separable representations despite binding.</p>
<sec id="sec002">
<title>Time-based binding in a layered neural network</title>
<p>Here we describe how representations that maintain relational structure can be composed and decomposed in a layered neural network. A simple computational mechanism, <italic>time-based binding</italic>, in which time is used to carry information about the relationships between representations in the input, is one way to achieve such an architecture. It is a truism and/or a biological principle of cortical organisation that "neurons that fire together, wire together" [<xref ref-type="bibr" rid="pbio.2000663.ref028">28</xref>]. As such, some time-based binding systems use synchrony of firing to link representations together in the network for processing [e.g., <xref ref-type="bibr" rid="pbio.2000663.ref029">29</xref>]. Conversely, neurons that do not fire in synchrony can stay independent, and the proximity in time between firings can be exploited to carry information. Discovery of Relations by Analogy (DORA; a symbolic-connectionist model of relational reasoning; the full computational specifics of the model can be found in Doumas et. al [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>], operating procedure available in Appendix A) exploits the synchrony principle in order to keep representations separable in the limit while binding them together for processing. This situation means that the system can be said to have <italic>variable-value independence</italic> when the representation of a given variable and its particular value at a moment in time are explicitly, independently represented [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref030">30</xref>]. A representation of a variable and its value must be able to function separately in any system performing a computation that requires relationality or (de)compositionality, such as during analogical reasoning, language processing, and likely many other higher-level cognition functions [<xref ref-type="bibr" rid="pbio.2000663.ref031">31</xref>]. In a system with variable-value independence, statistics about the association between representations can still play an important role, but those statistics are not the sole basis of the representational architecture. In other words, variable-value independence allows the system to represent a variable, its value, and also to compute statistics about their association without changing the core representations.</p>
<p>In brief, DORA's primary computational assumptions are (1) a neural network with layers of units, (2) lateral inhibition, (3) separate banks of units, (4) Hebbian learning, and (5) sensitivity to time. The layered structure of the network, combined with sensitivity to time as carrying information about the relations between the nodes in the layers of the network, is one solution to preserving the structure in (1). After learning (please see Appendix A in [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] and pp. 8–13, 16–21 in the main text), the network represents words, phrases, and sentences across layers of the network, such that words and brackets in (1) correspond to nodes on different layers of the network. Nodes on higher layers code for the composition (a phrase) of two sublayer nodes (words) and fire when either of the subnodes below it fire in time. Asynchrony of unit firing (in this case, of a node, but in theory, of a neural population or assembly, see [<xref ref-type="bibr" rid="pbio.2000663.ref032">32</xref>]) allows the network to bind representations together for processing on a higher layer while maintaining independent codes for the input representations on a lower layer of the network. The combination of layers and time-based binding via asynchrony is what allows the network to have (de)compositionality—representations of compositional product <italic>and</italic> the decomposed inputs on different layers of the network. To represent (1), the network encodes the adjective phrase {fun<sub>adj</sub>{games<sub>n</sub>}<sub>NP</sub>}<sub>ADJP</sub> over two layers of the network; on the lower layer, one node codes for the word {fun<sub>adj</sub>} and another for the word {games<sub>n</sub>}(see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1</xref>). On the next layer above that, the phrasal node will activate when the nodes {fun<sub>adj</sub>} and {games<sub>n</sub>} fire. These word nodes fire staggered in time, or at an asynchrony, and still activate the node that codes for the phrase {fun<sub>adj</sub>{games<sub>n</sub>}<sub>NP</sub>}<sub>ADJP</sub>. A similar configuration codes the phrasal binding between {waste<sub>v</sub>} and {time<sub>n</sub>} (a node that codes the verb phrase {waste<sub>v</sub>{time<sub>n</sub>}<sub>NP</sub>}<sub>VP</sub>). The relationality between the phrases {fun<sub>adj</sub>{games<sub>n</sub>}<sub>NP</sub>}<sub>ADJP</sub> and {waste<sub>v</sub>{time<sub>n</sub>}<sub>NP</sub>}<sub>VP</sub> is represented by encoding information about being an agent (e.g., "the waster") or a patient (e.g., "the wasted") in a two-argument predicate. One way to represent predicate argument relationships in a neural network is to code role information in a separate node from the particular argument that fills that role at a given processing moment. When the role slot in a predicate is represented separately from the given input, predicate–argument relationships can be generatively applied to any input that predicate is associated with in the dataset (see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1</xref>). These argument role nodes, which code for the role-filler binding relation, can be learned in the same way the word nodes are (see [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] for details). Lastly, a sentence node that represents the relation between {fun<sub>adj</sub>{games<sub>n</sub>}<sub>NP</sub>}<sub>ADJP</sub> and {waste<sub>v</sub>{time<sub>n</sub>}<sub>NP</sub>}<sub>VP</sub> as a sentence (a node that fires when the AdjP and VP units fire, and thus codes for the whole sentence {{fun<sub>adj</sub>{games<sub>n</sub>}<sub>NP</sub>}<sub>ADJP</sub>{waste<sub>v</sub>{time<sub>n</sub>}<sub>NP</sub>}<sub>VP</sub>}<sub>IP</sub>) is on the highest layer. The layered structure of the network is a core assumption and is necessary for time-based binding to function and for predicates to be represented in a connectionist network. We base the assumption that the system has layers on the broadest notion of cortical organisation; however, the representational codes for a given layer are learned from the input (see [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] for detailed explanation of the learning process, which itself is not central to the results reported here, nor to the theoretical claim made here). The main advantage of time-based binding is that it avoids the superposition problem that a system that only uses synchronous firing alone would face.</p>
<fig id="pbio.2000663.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g001</object-id>
<label>Fig 1</label>
<caption>
<title>A DORA representation of the proposition <italic>waste</italic> (games, time) during processing that illustrates how time-based binding works.</title>
<p>We use different shapes to represent units in different layers (ovals for Proposition node "P-units"/sentences, rectangles for Role-filler binding nodes or "RB units"/phrases, triangles and large circles for Propositional Object (PO) units/words and argument roles, and small circles for semantic units/features) for the purposes of clarity. Abbreviations "wtr." and "wtd." signify the role of <italic>waster</italic> and <italic>wasted</italic> in the proposition <italic>waste</italic>(games, time), respectively. In the model, these units are simply nodes in different layers of the network. Darker units denote when a unit is firing at a given time step (panels a–d in the Fig 1), which in this case corresponds to 250 msec/4 Hz. Please see page 18 for a detailed discussion of P-units, RB-units, and PO units.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g001" xlink:type="simple"/>
</fig>
<p>In sum, the essential difference between DORA and other connectionist models is that DORA binds representations using asynchrony of unit firing and thus can vary the level of representation at which the asynchrony is maintained. Slight asynchrony in unit firing leads to independent, discriminable sequences of representation across layers. Units on the next layer of the network then code for or fire when two or more subunits fire within a certain time of each other, which results in a hierarchical representation that can discriminate sequences with the same inputs and represents input values independently. In this way, DORA learns structured representations of relations from unstructured (holistic flat feature vector) inputs and is based on traditional connectionist computing principles (i.e., layers of interconnected nodes passing activation via weighted connections that are modified via Hebbian learning). However, in contrast with most connectionist networks, DORA effectively learns and implements hierarchical symbolic representations [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref033">33</xref>]. As summarised above, DORA does this by using time to encode relational information and uses comparisons of distribution of activity in time to subset units into function representations and to continue refining them throughout the learning process [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]. DORA accounts for numerous phenomena from relational learning, as well as its development (e.g., [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref034">34</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref035">35</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref036">36</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref037">37</xref>]).</p>
</sec>
<sec id="sec003">
<title>Tracking hierarchical representations in language processing</title>
<p>Although there are seldom clear physical boundaries in speech input that directly correspond to higher-level representations (i.e., phonemes, words, phrases, and sentences), we perceive and experience complex discrete representations in continuous input. A growing body of evidence suggests that this perception is based in the entrainment of cortical brain rhythms to speech [<xref ref-type="bibr" rid="pbio.2000663.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref005">5</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref007">7</xref>]. While such neuroimaging evidence is suggestive and its implications tantalising, there is a mechanistic gap between such cortical signals and the representational output of the system (as described by formal theories, or as might be formalised in any computational system). An adequate mechanistic linking hypothesis between linguistic and cortical computation would explain how the system goes from input of perceptual features to a hierarchy of structured representations and would describe <italic>how</italic> the computational mechanism gives rise to the observed cortical activation states. Furthermore, such a hypothesis would shed light on whether the observed cortical signal reflected “mere” tracking of hierarchical linguistic representations or whether it, in fact, reflects the online generation of hierarchical linguistic structure.</p>
<p>To carve the computational problem at its joints, we turn to Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>], who report cortical tracking of hierarchical linguistic structures (i.e., words, phrases, and sentences) in oscillatory data in both electrocorticography and magnetoencephalography recordings. Ding et al. presented auditory strings of synthesised speech in Mandarin Chinese and American English. Their stimuli were isochronous but manipulated the structural relationship between the syllables such that, in one condition, there was no meaningful relationship between the string of syllables/words (“<italic>walk egg nine house”</italic>), in a second condition, phrases were formed from adjacent syllables (<italic>“flat table angry birds”</italic>), and in a third condition, sentences emerged (<italic>“new plans gave hope”</italic>). An increase in power at frequencies in the oscillatory response on the timescale of syllabic/lexical rate (4 Hz), phrasal rate (2 Hz), and sentential rate (1 Hz) tracked the hierarchy of linguistic representation. Importantly, Ding et al. showed that the signal could not be attributed to entrainment to acoustic information, transitional probability, or word predictability [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]. Finding cortical entrainment at these slower oscillations (1 Hz, 2 Hz) suggests that cortical populations are entraining to abstract linguistic representations like phrases and sentences. This is remarkable because it is unclear what, if any physical instantiation of these higher-level stimuli are present in the speech signal—at least, there is no (known) set of acoustic cues in speech that reliably or diagnostically signal word, phrase, and sentence structures. That Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] found cortical entrainment to such higher-level linguistic structures suggests that the brain is, nonetheless, sensitive to the presence of these abstract linguistic representations.</p>
<p>Strikingly, DORA predicts such a representational pattern in oscillatory unit firing. DORA is a model of how information is represented—consequently, the simulations we present here are in no way intended as a fully articulated model of parsing. Furthermore, we want to emphasise that DORA represents a form of role-filler binding predicate calculus, in which expressions can be, but are not always, nor even usually, formally equivalent to the natural language expression. Future work is needed to derive representations in DORA with one-to-one correspondence to natural language; however, for the sentence stimuli tested herein, the difference between natural language form of the expression and the DORAese predicate logic does not bear on the theoretical conclusion that we draw from the results of our simulations. However, if the structure of information in DORA turned out to resemble how language appears to be represented in the human brain, that finding would indicate mechanistic synergy between the two computational systems.</p>
<p>We present simulations of sentence processing within a model of relational concept learning that provides a mechanistic explanation for how representational structure emerges from unstructured input. We show that the model not only represents sentences and exhibits oscillatory activation patterns that are strikingly similar to human cortical oscillatory brain activity during exposure to the same stimuli [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]. We further demonstrate that the model, as a result of processing the sentence stimuli, forms representations of sentence meaning that reliably discriminate between semantically composable grammatical sentences and syntactically intact but semantically non-composable sentences. Our results, though not a model of parsing nor of a cortical microcircuit, are an existence proof that using time to carry information (“time-based binding,” or generating explicit hierarchical representations by encoding structural relations in time) addresses two hard problems for cognition that have so far been investigated entirely separately: sentence processing and analogy (see [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] for human-level simulations of relational and analogical reasoning). In order to determine if the oscillatory pattern from Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] can arise from the serial presentation of a stimulus at 4 Hz alone, we ran the same simulations in a RNN. The RNN simulations test whether signals like the cortical oscillations observed by Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] can arise from a system without time-based binding or representational hierarchy, in which they might arise from seriality alone.</p>
</sec>
<sec id="sec004">
<title>The current study</title>
<p>DORA processed the English sentence stimuli from Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>], as well as three control conditions wherein either only syntactic relationships, but no compositional meaning, were present (<italic>Jabberwocky condition</italic>), no syntactic relationships existed between the words in the input (<italic>Word List condition</italic>), or only phrases existed (a version of the phrase-only condition from Ding et al., <italic>Phrases condition;</italic> please see [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] for the list of Grammatical Stimuli and <xref ref-type="supplementary-material" rid="pbio.2000663.s001">S1 Text</xref> for a list of our additional stimuli and <xref ref-type="fig" rid="pbio.2000663.g002">Fig 2</xref> for a schematic of Grammatical sentence representations in DORA's predicate calculus). We observed whether DORA represented the phrases or sentences correctly, and recorded the oscillatory pattern of unit firing in layers of DORA’s network during processing. Finally, in contrast to available brain data, we assessed the content of the representations DORA generated during processing of Grammatical and Jabberwocky sentences. We plotted the activation of existing predicates in memory in response to the hierarchical representations that parsing generated in those two conditions. To discount the hypothesis that the oscillatory pattern stems only from serialised processing, we repeated the four simulations in a RNN for comparison. Please see the <xref ref-type="sec" rid="sec007">Methods</xref> section for a description of the RNN that we trained. In brief, this was a standard RNN with one hidden layer that was trained on the same stimuli as in the DORA simulations.</p>
<fig id="pbio.2000663.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g002</object-id>
<label>Fig 2</label>
<caption>
<title>A representation of the sentence "Dry fur rubs skin" in Learning and Inference with Schemas and Analogies (LISA; [<xref ref-type="bibr" rid="pbio.2000663.ref021">21</xref>]) /DORAese predicate calculus.</title>
<p>We use different shapes to represent units in different layers (ovals for P-units/sentences, rectangles for RB units/phrases, triangles and large circles for PO units/words, and small circles for semantic units/features) for the purpose of clarity. In the model, these units are simply nodes in different layers of the network.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g002" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec005" sec-type="results">
<title>Results</title>
<p>The results of the simulations are presented in Figs <xref ref-type="fig" rid="pbio.2000663.g003">3</xref>, <xref ref-type="fig" rid="pbio.2000663.g004">4</xref> and <xref ref-type="fig" rid="pbio.2000663.g005">5</xref>. Interestingly, the oscillatory pattern of firing in the various layers of DORA during processing of sentences, closely mirrored the patterns observed by Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]. Specifically, just like the cortical signals, for Grammatical sentences, DORA showed an activation burst that lasted throughout the processing of the sentence (i.e., firing in the 1 Hz range), activation bursts at twice the rate of the whole sentence burst (i.e., firing in the 2 Hz range), aligned with phrase-level processing, and activation bursts at four times the rate of the whole sentence burst (i.e., firing in the 4 Hz range), corresponding to word-level processing. However, the activity in the Word List condition also resembled the human data, in which in both cases, there was only spiking at 4 Hz but not at slower frequencies, which indicates that larger constituents were neither tracked nor formed in this condition. The Jabberwocky condition, which has no analogue in the available human data, resulted in oscillations that resembled the Grammatical condition, suggesting that the model is sensitive to something akin to syntactic structure or category during processing of Jabberwocky. Finally, we had the model process the Phrases condition, which contained strings of phrases, but not sentences, and it showed a power increase at 2 Hz and 4 Hz, resembling the human data to a phrase-only condition in Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>].</p>
<fig id="pbio.2000663.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Grammatical sentences: DORA network power spectrum compared to human cortical oscillations.</title>
<p>The solid line represents cortical power while participants listened to four syllable/word sentences played over 1 s in Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]. Power increases are evident at the 1 Hz (sentence duration), 2 Hz (phrase duration), and 4 Hz (word duration) range. The dashed line depicts firing in DORA while processing the same sentences used in Ding et al. Units in DORA fire for the duration of the sentence, at intervals of half the length of the sentence and at intervals lasting a quarter of the length of the sentence. Data from the stimulation and the code to run it are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/eb2vp/" xlink:type="simple">https://osf.io/eb2vp/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexDoumas/dingetal_sent" xlink:type="simple">https://github.com/AlexDoumas/dingetal_sent</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g003" xlink:type="simple"/>
</fig>
<fig id="pbio.2000663.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g004</object-id>
<label>Fig 4</label>
<caption>
<title>DORA network power spectrum plot of the Word List, Phrases, and Jabberwocky conditions.</title>
<p>For Word List, an increase in firing only occurred at the 4 Hz range, corresponding to firing of nodes coding for words. Lack of firing at other frequencies indicates that no hierarchical representations were processed in the Word list condition. In the Phrases condition, there was an increase in power at 2 Hz and 4 Hz, indicating that units coding words and units coding phrases were active during the processing of this condition. No sentence units were active. In the Jabberwocky condition, there was an increase at 1, 2, and 4 Hz range, similar to the pattern seen for grammatical sentences, indicating that hierarchical representations were indeed activated. See <xref ref-type="fig" rid="pbio.2000663.g005">Fig 5</xref> for a comparison of activation across the propositions in the model's long-term memory between Jabberwocky and Grammatical sentences. Data from the simulations and the code to run them are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/eb2vp/" xlink:type="simple">https://osf.io/eb2vp/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexDoumas/dingetal_sent" xlink:type="simple">https://github.com/AlexDoumas/dingetal_sent</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g004" xlink:type="simple"/>
</fig>
<fig id="pbio.2000663.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Left Panel: Grammatical.</title>
<p>Plot of active propositions in memory across trials in the Grammatical condition. On the <italic>x</italic>-axis are P-Units, on the <italic>y</italic>-axis are processing iterations of the model, equivalent to trials or instances of processing a sentence during the simulation. The darker colour indicates more activation of existing propositional role-filler binding combinations in memory. Grammatical sentences resulted in stronger activation of extant propositions than Jabberwocky sentences did. Right Panel: Jabberwocky. The Jabberwocky condition did not activate as many single existing propositions in the model's memory as the Grammatical condition did, rather, activation was spread more broadly across memory, despite both conditions producing similar oscillations in DORA. Data from the simulations and the code to run them are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/eb2vp/" xlink:type="simple">https://osf.io/eb2vp/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexDoumas/dingetal_sent" xlink:type="simple">https://github.com/AlexDoumas/dingetal_sent</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g005" xlink:type="simple"/>
</fig>
<p>To assess the nature of the representations DORA generated during parsing, we plotted activation maps of the contents of DORA’s memory over 100 trials (instances of processing a sentence in the simulation) in the Grammatical and Jabberwocky conditions. Note that the oscillatory firing pattern from the Word List condition indicated that DORA did not compose role-filler bindings to compare with memory in that condition. The difference in activation of existing representations in DORA’s memory between the Grammatical condition (<xref ref-type="fig" rid="pbio.2000663.g003">Fig 3</xref>) and the Jabberwocky condition (<xref ref-type="fig" rid="pbio.2000663.g004">Fig 4</xref>) are illustrated below (<xref ref-type="fig" rid="pbio.2000663.g005">Fig 5</xref>). We plot only the activation of propositions that contain a word that was present in the stimuli in these particular trials (0–100). The darker bars indicate that the hierarchical representations formed during processing more strongly activated representations that were already in the semantic memory of the model. Jabberwocky sentences activated fewer existing token units and activated these units to a lesser extent than the Grammatical sentences did, suggesting the model is generating novel, syntactically licensed representations that are "representationally unusual," comparable to the human experience of reading syntactically intact but semantically anomalous Jabberwocky sentences. We performed a <italic>t</italic>-test comparing the number of units above threshold across the 100 runs in the Grammatical and Jabberwocky conditions and found that more units were active while processing a Grammatical sentence (mean units active = 70.78) than a Jabberwocky sentence (mean units active = 31.95); <italic>t</italic> = 58.312, <italic>p</italic> &lt; 2.2e−16.</p>
<p>We then repeated the four simulations in an RNN. In order to determine the activation level of the RRNs at each frequency, we attempted to identify units in the trained RRNs output layer that were active above a threshold of 0.7 consistently at 1 Hz, 2 Hz, 3 Hz, and 4 Hz across all sentences in each condition (<xref ref-type="fig" rid="pbio.2000663.g006">Fig 6</xref>). Activation in the hidden layer corresponds to representations of the statistical patterns that the network learns, so finding patterns here would indicate that the RNN learned hierarchical linguistic structures from the input. For the Grammatical condition, zero units in the hidden layer were active at the 1 Hz, 2 Hz, and 3 Hz rates; five units (out of 50 hidden units in the network) were active above the threshold at the 4 Hz rate. For the Jabberwocky condition, zero units were active at the 1 Hz, 2 Hz, and 3 Hz rates; nine units were active above the threshold at the 4 Hz rate. For the Word List condition, zero units were active at the 1 Hz, 2 Hz, and 3 Hz rates; six units were active above the threshold at the 4 Hz rate. For the Phrase condition, one unit was active at the 2 Hz rate; five hidden units out of 30 nodes were active above the threshold at the 4 Hz rate. <xref ref-type="fig" rid="pbio.2000663.g006">Fig 6</xref> shows the proportion of units (ranging from 0–0.2 proportion of units) active in the recurrent layer at each rate in each condition. We not that the RNN achieved perfect performance, that is, it learned the sentences such that for any input word, the RNN could tell you the next <italic>n</italic> words with 100% accuracy. The difference between the RNN and DORA is that, in doing so, the RNN did not do anything in a way that resembles what humans do, that is, it neither formed (symbolic) hierarchical representations, nor oscillated, nor oscillated in a way that resembles the cortical signals to the same stimuli.</p>
<fig id="pbio.2000663.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Proportion of units (range: 0–0.2) active above threshold (0.7) in the recurrent layer of the RNNs.</title>
<p>Note: total number of units in the recurrent layer <italic>n</italic> = 50 for all conditions except for Phrases, in which <italic>n</italic> = 30. On average, between 5–9 units out of 50 in the hidden layer activated at 4 Hz. There was no evidence of activity at 1 Hz or 2 Hz, and hence no evidence for coding or tracking of linguistic structures in the RNN. Data from the simulations and the code to run the simulations are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/eb2vp/" xlink:type="simple">https://osf.io/eb2vp/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexDoumas/dingetal_sent" xlink:type="simple">https://github.com/AlexDoumas/dingetal_sent</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006" sec-type="conclusions">
<title>Discussion</title>
<p>We have presented simulations from a computational model that learns and generates hierarchical structure from an unstructured string of lexical input. The model, DORA [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>], was built for a completely different purpose (learning relational concepts to perform analogical reasoning) but achieved the current outcome for sentence processing without any formal or structural changes from its original state. DORA learns and generates structured, symbolic representations using time-based binding in a layered neural network [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]; it is this computational architecture that enables the model to process sentences, to form hierarchical representations in general, and is what gives rise to the oscillatory pattern that resembled cortical oscillations. DORA is a model of how information is represented in the human mind, and perhaps more speculatively, how <italic>information</italic> might be macroscopically represented in cortical networks. It is not a model of parsing or cortical microcircuits. Nonetheless, when processing sentences like <italic>fun games waste time</italic>, the English sentence stimuli from Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>], the oscillatory firing pattern of units in various layers of DORA closely resembled the cortical oscillations observed by Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]—an activation burst lasting throughout the processing of the sentence (1 Hz range), activation bursts at phrase-level processing, or twice the rate of the whole sentence burst (2 Hz range), and activation bursts at word-level processing, or around four times the rate of the whole sentence burst (4 Hz range). Furthermore, the representations that the model generated during parsing reliably reflected whether the sentence was grammatical, jabberwocky, a list of phrases, or a list of words. This result indicates that DORA's architecture generates representations that are both semantically rich and structurally sensitive, two properties that are essential to human language. In contrast, a traditional connectionist network (an RNN) failed to show an oscillatory pattern that resembled the cortical signal, but a few nodes coded words at 4 Hz. Thus, there was no evidence of the RNN representing hierarchical linguistic structures. This contrast showed that seriality of processing alone is insufficient to produce the oscillatory pattern observed in humans and in DORA.</p>
<p>Our results naturally beg the converse question, as to whether any system with representational hierarchy could produce the oscillatory pattern of activation that [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] and DORA show. For example, could natural language processing (NLP) parsers, which feature representational hierarchy and were developed to specifically parse natural language in a machine, produce oscillations and the pattern seen in [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] and in our simulations? In principle, any system of hierarchy that is (de)compositional has the representational ingredients to encode units that could be fired in a sequence. However, we would argue that any given representational hierarchy could only produce oscillations <italic>if it were combined with time-based binding</italic>, which, as far as we know, no NLP system features. In DORA, time-based binding <italic>is</italic> the oscillation of activity throughout the network, which is part of the reason why the RNN did not show oscillatory activity nor the specific pattern from [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]. The representational structure of DORA (a (de)compositional role-filler binding predicate logic) is what makes the oscillations take the form that the data from [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] have. In terms of the specifics of the observed 1-2-4 Hz pattern, both [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] and our simulations are highly shaped by the word presentation rate of 250 ms/4 Hz. But without time-based binding, there is no mechanism to produce oscillations in a network, even in a NLP parser or other system with representational hierarchy.</p>
<p>In sum, we remain relatively agnostic about the specific details of the required representational hierarchy because we do not yet know how to link the predicate calculus representations we use to natural language mental and cortical representations. What we are not agnostic about is the need for asynchronous time-based binding in order to produce oscillations in a neural network, as well as the need for representational hierarchy to produce the particular pattern of oscillations observed here and in [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>].</p>
<p>Furthermore, another difference between DORA, RNNs, and NLP parsers is that DORA is a model of how information is represented and computed in the human mind. In other words, DORA makes a specific theoretical claim about how the human mind represents (some kinds of) information—that is, it does so in a <italic>relational</italic>, structured, hierarchical manner, which is realised via time-based binding. RNNs and NLP parsers do not make such theoretical claims about the architectures and mechanisms of mental and cortical computation.</p>
<p>In DORA, structured representations form during learning and become activated during later processing. Our simulations and theoretical claims concern the way in which information is activated during processing. However, we note that it is the same computational mechanism that can generate representations during learning, which also performs processing: the use of time to carry information about the relations between inputs, implemented as systematic temporal asynchrony of unit firing across layers of the network, which we have called "time-based binding." Through temporal asynchrony of firing, the model can use separable populations of units to maintain activation of different levels of representation as they occur in time. This ability to maintain hierarchy is the computational feature that turns out to be crucial for representing and processing the kinds of relations that are necessary to represent human language and is likely to be necessary for generating hierarchical levels of representation in any cortical network. We note here that our argument that temporal asynchrony is the mechanism that gives rise to hierarchical representation finds traction also in the conceptual terminology of neural oscillations, namely that temporal asynchrony corresponds to neural desynchronisation. Importantly, synchrony and asynchrony of unit firing in time are not orthogonal mechanisms; in fact, they are the same function or variable with different input values (e.g., sin(x) and sin(2x)) that can carry different information (see [<xref ref-type="bibr" rid="pbio.2000663.ref038">38</xref>]). Our mechanistic claim is that it is <italic>asynchrony</italic> that allows the system to bind information for processing while maintaining (de)compositionality and generating hierarchical representations. Binding or forming representations through synchrony alone would fail at the superposition problem, effectively superimposing a variable and its value onto a single, undecomposable representation.</p>
<p>The generation of structured representations in DORA is a form of <italic>predication</italic> [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]. DORA’s representations are symbolic predicates that code for the invariant relations between features and objects (e.g., between the features “fun” and the object “games,” the feature “adjective” and the object “fun,” or the feature “noun” and the object “games”), or between objects and each other (e.g., between the object “fun games” and the object “waste” [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref039">39</xref>]). The resulting computational architecture thus explicitly represents invariant relations between input representations (or variables) and output representations (or values), as a function. This architecture is in contrast with RNNs and current deep-learning algorithms, which associate input and output via statistical association and, therefore, do not (currently) preserve compositionality or represent relational structures [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref025">25</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref026">26</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref030">30</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref040">40</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref041">41</xref>]. Because DORA explicitly codes invariance as a function, it can combine novel inputs with existing predicate structures, leading to productive, combinatorial generativity of representation that is neither maligned by violations of statistical regularity, and not reliant, in principle, although not demonstrated here, on hard coding of representations (cf. [<xref ref-type="bibr" rid="pbio.2000663.ref042">42</xref>]). Though not directly relevant to the claims we make here, DORA is able to learn hierarchical structure from unstructured input—a feature that is very important for any developmentally plausible model of cognition and to models that seek to explain cortical and biological system organisation and plasticity. The ability to learn structured representations contrasts with current Bayesian models, which assume structured representations a priori or fit them from a specified space of possible representations predefined by the modeller (cf. [<xref ref-type="bibr" rid="pbio.2000663.ref042">42</xref>]). Although Bayesian models have a powerful descriptive application, they do not currently offer a mechanistic explanation for how a biological system came to be the way it is.</p>
<p>What are the computational origins of predication, and where does it fall in the taxonomy of cortical computations? What might those origins tell us about why a model of analogy happens to be able to process sentences? Perhaps communicating information across time and space or needing to code for a relationship between representations that goes beyond, or even violates, statistical regularity (such as encountering novel objects and interacting with them, or interacting with old objects in new ways) were challenges that were sufficient to recruit a latent computational mechanism from existing neurocomputational subroutines, in response to the common computational requirement that these problems entailed. One hypothesis is that the underlying computation behind predication is a domain-general abstraction mechanism, such that language processing and analogical reasoning are instances of processing that call upon the same abstraction subroutine, one that might also underlie other “binding”-like phenomena in cognition and perception [<xref ref-type="bibr" rid="pbio.2000663.ref013">13</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref027">27</xref>], in any neural system that requires relationality or representational hierarchy. An abstraction sub-routine might be a cognitive or computational mechanistic “kind” that is at work in much of human cognition [<xref ref-type="bibr" rid="pbio.2000663.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref022">22</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref023">23</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref032">32</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref043">43</xref>].</p>
<p>Our results suggest a formal and mechanistic synergy between how representational structures are computed, and how energy is expended in both cortical and machine oscillators. The pattern of oscillatory activation observed in the layers of DORA arises directly from the online processing of hierarchical sentence representations—by inductive inference, the cortical signal reflects generation too, rather than “mere” tracking, of hierarchical linguistic representation. Time-based binding via asynchrony, the computational mechanism in our model, links the generation of hierarchical structure to the observable “read-out” in the machine and cortical signal, with broad implications as a computational first principle of cognition in the human brain. Minimally, it explains how a computation gives rise to hierarchical representation <italic>and</italic> why cortical signals stemming from said computation appear the way they do. As such, our results can make the broad prediction that there ought to be temporally dissociable populations oscillating asynchronously; that is, desynchronisation between neural assemblies should occur as a function of the level of linguistic analysis that is being represented in a cortical network at a given time step. In other words, DORA's representational architecture predicts desynchronisation between assemblies oscillating at different frequencies in order to represent, for example, the speech envelope, acoustic phonetic features, syllables/phonemes, morphemes, words, phrases, and sentences. Whether the cross-frequency desynchronisation signal is more strongly observable at stimulus onset and offset or whether representational (de)synchronisation signals should be thought of as the dynamics of phase-to-power cross-frequency coupling over time, as well as a myriad of other important complications, are difficult to predict concretely at this stage. But, it is likely that functional characterisation of the dynamic recruitment or entrainment of cell assemblies during information processing could reveal powerful mechanistic first principles of cortical representation and computation [<xref ref-type="bibr" rid="pbio.2000663.ref032">32</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref038">38</xref>].</p>
<p>Our results support a view where the organisational principles of cognitive, cortical, and biological systems arise from the nature of the mechanisms that carry out the computations that the system must perform. Our results provide a mechanistic explanation for (1) how our brains form discrete hierarchical representations from holistic unstructured input, as in language comprehension and other domains of cognition, (2) how patterns in cortical oscillations relate to representational structure building, and (3) how the system exploits the fact that time carries information to achieve a representational system that is generative and (de)compositional. The mechanism that gives rise to this state of affairs is time-based binding—the asynchrony of unit firing across layers of the network that allows the model to represent information independently at multiple timescales. The class of possible processing mechanisms that accounts for the output of the cortical computation signal must correspond in some fundamental way to the mechanism through which DORA forms representations. This computational mechanism gives rise to a generative representational hierarchy, as observed in machine and cortical oscillations, serving as an “abstraction engine” for representation in the human brain. Our results suggest that the identification of biological mechanisms, circuits, and subroutines that can perform computations beyond the domain in which they arose, or from which they were derived—a form of computational "recycling"—offers an approach to understanding biological systems, that, through modelling, can derive mechanistic explanations for why systems are the way they are.</p>
</sec>
<sec id="sec007" sec-type="materials|methods">
<title>Methods</title>
<p>DORA is a model of how structured relational representations can be learned and represented in a connectionist network. Starting with unstructured representations of objects (i.e., flat feature vectors), DORA learns structured (i.e., predicated) representations of object properties and relations. Importantly, these representations allow DORA to solve a number of problems in analogical and inductive reasoning [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>].</p>
<p>For the purposes of the simulations described in this paper, DORA’s learning is not fundamental and so we avoid discussing it further for the purposes of clarity of exposition. It should be noted, however, that all of the propositional structures we describe and use in the simulations can be learned by DORA from scratch. Full details of how these propositions might be learned are given in Doumas et al. [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]. Prior to the simulations, we hardcoded DORA with the Grammatical stimuli from Ding et al., from which it learned propositional sentence structures. For the simulations, we had DORA process stimuli in the different conditions after it had learned and stored sentence structures in its memory.</p>
<p>There are two aspects of DORA’s operation that are fundamental to the current simulations. The first is the manner in which DORA represents propositional structures (sentences) and role-filler bindings (phrases). The second is the manner in which activation spreads from propositions currently in DORA’s focus of attention to propositions in long-term memory (LTM). We describe both of these operations in the sections that follow.</p>
<sec id="sec008">
<title>Representations in DORA</title>
<p>In DORA, before learning (although again, not demonstrated here), objects are represented as flat feature vectors (see <xref ref-type="fig" rid="pbio.2000663.g007">Fig 7</xref>). After learning, relational structures are represented by a hierarchy of distributed and localist codes (see Figs <xref ref-type="fig" rid="pbio.2000663.g001">1</xref>, <xref ref-type="fig" rid="pbio.2000663.g002">2</xref> and <xref ref-type="fig" rid="pbio.2000663.g008">8</xref>). At the bottom, “semantic” units represent the features of objects and roles in a distributed fashion. At the next level, these distributed representations are connected to localist units (called POs in DORA) representing individual predicates (or roles) and objects; in these simulations, these units represent words. Localist RBs link object and predicate units into role-filler binding pairs for processing; these units represent phrases in these simulations. At the top of the hierarchy, a localist P-unit that represents the sentence links RBs (phrases) into a whole relational proposition or sentence (see Figs <xref ref-type="fig" rid="pbio.2000663.g001">1</xref> and <xref ref-type="fig" rid="pbio.2000663.g002">2</xref>).</p>
<fig id="pbio.2000663.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Initial state of the network before learning.</title>
<p>The model assumes the existence of objects and features and initially must learn the relationships between features sets and objects. After learning, the model's internal representations are in a predicate calculus (see Figs <xref ref-type="fig" rid="pbio.2000663.g001">1</xref> and <xref ref-type="fig" rid="pbio.2000663.g002">2</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g007" xlink:type="simple"/>
</fig>
<fig id="pbio.2000663.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Graphical depiction of banks of units in DORA containing represented propositional structures.</title>
<p>The comparison process that is crucial for learning occurs across the driver and recipient (see [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] for details).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g008" xlink:type="simple"/>
</fig>
<p>The token units at the various layers of DORA represent information at a progressively more conjunctive level. While independence of roles and filler is maintained in the semantic and PO units, RB units code conjunctively for specific phrases or role-filler bindings, and P-units code for conjunctions of role-filler sets into full relational propositions, or sentences in this case. Conjunctive binding is sufficient for long-term storage but violates role-filler independence and so fails fundamentally for any tasks that require independent representation of roles and fillers [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref044">44</xref>], as processing hierarchical and symbolic structure requires that representational elements in a system can be composed into structures in a manner that does not violate the independence of those elements (see [<xref ref-type="bibr" rid="pbio.2000663.ref026">26</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref040">40</xref>]). Consequently, during structured processing, DORA must maintain binding information in those units (semantics and POs) that maintain role-filler independence.</p>
<p>DORA is composed of a number of sets or banks of units (see <xref ref-type="fig" rid="pbio.2000663.g008">Fig 8</xref>). The driver is the current focus of attention, or what DORA is thinking about, at any given moment. The recipient is a set of units representing propositions that are available for comparison with the propositions in the driver. Hummel and Holyoak [<xref ref-type="bibr" rid="pbio.2000663.ref045">45</xref>] have described the recipient in terms of Cowan’s [<xref ref-type="bibr" rid="pbio.2000663.ref046">46</xref>] active memory. Finally, LTM is the set of propositions that DORA has encountered in the past that are not currently active. All banks of units are connected via a common pool of semantic units. During processing, activation flows from units in the driver, to the semantic units, and then to units in the recipient and LTM. The flow of activation from driver to recipient is fundamental for a number of DORA’s operations, including analogical mapping, inference, and predicate learning and refinement. The flow of activation from driver to LTM is important for retrieval. None of the current simulations rely on operations involving flow of activation between driver and recipient, so we do not discuss the recipient set any further.</p>
<p>The driver, as the focus of DORA’s attention, is the starting point for all of DORA’s processing. When DORA performs any structured processing, role-filler bindings must be maintained in the units that maintain role-filler independence (see above). This dynamic binding (bindings that do not violate role-filler independence and can be created and destroyed on the fly [<xref ref-type="bibr" rid="pbio.2000663.ref020">20</xref>]) information is carried in DORA using time. Specifically, DORA uses systematic asynchrony of firing to maintain role-filler bindings.</p>
<p>In DORA, binding information can be carried either by synchrony (as in the symbolic-connectionist model Learning and Inference with Schemas and Analogies [LISA] [<xref ref-type="bibr" rid="pbio.2000663.ref021">21</xref>]) or by systematic asynchrony of firing, with bound role-filler pairs firing in direct sequence. Asynchrony-based binding, or what we call "time-based binding," allows roles and fillers to be coded by the same pool of semantic units, which allows DORA to learn representations of relations from representations of objects (Doumas et al. [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]). During asynchronous binding, in which a proposition like <italic>waste</italic> (games, time) becomes active in the driver (see <xref ref-type="fig" rid="pbio.2000663.g008">Fig 8</xref>), the units representing <italic>waster</italic> fire (along with units conjunctively coding for <italic>waster</italic>+games and for the <italic>waste</italic> [games, time] proposition; see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1A</xref>), followed directly by the units representing games (along with units conjunctively coding for <italic>waster</italic>+games and for the <italic>waste</italic> [games, time] proposition; see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1B</xref>), representing the binding of <italic>waster</italic> to games.</p>
<p>Then, the units representing <italic>wasted</italic> fire (along with units conjunctively coding for <italic>wasted</italic>+time and for the <italic>waste</italic> (game, time) proposition; see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1C</xref>), followed directly by the units representing time (along with units conjunctively coding for <italic>wasted</italic>+time and for the <italic>waste</italic> [time, games] proposition; see <xref ref-type="fig" rid="pbio.2000663.g001">Fig 1D</xref>), representing the binding of <italic>wasted</italic> to time. In short, bound role-filler pairs fire in direct sequence and out of synchrony with other bound role-filler pairs. These patterns of sequential oscillation dynamically code role-filler bindings in DORA and underlie DORA’s capacity to use the representations that it learns to support relational reasoning (e.g., analogical mapping, schema induction, and relational induction; see [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]) and to learn structured relational representations from unstructured object representations.</p>
<p>While establishing time-sharing patterns of firing in a connectionist model might seem complicated, as demonstrated by Hummel and Holyoak [<xref ref-type="bibr" rid="pbio.2000663.ref021">21</xref>,<xref ref-type="bibr" rid="pbio.2000663.ref045">45</xref>] and Doumas et al. [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>], it is actually rather simple. In DORA, each token unit is actually a coupling of two units, an exciter and a yoked inhibitor. The exciter unit behaves like a conventional node in a neural network, taking and passing input to units at higher and lower levels and laterally inhibiting and being inhibited by units in the same layer. Each exciter is also yoked to an inhibitor unit that integrates input over time and, when a threshold is reached, forces the yoked exciter unit to inactivity, allowing other units to become active.</p>
<p>Continuing the above example, when the phrase/RB unit coding for <italic>waster</italic>+games fires, the two word/PO units connected to that phrase/RB—<italic>waster</italic> and games—compete to become active. Due to noise in the system, one of these tokens will become slightly more active and inhibit the other to inactivity. For example, <italic>waster</italic> might become slightly more active and inhibit games to inactivity. After some time firing, <italic>waster</italic>’s inhibitor unit will fire, forcing it to inactivity and allowing the word/PO representing games to fire. Similarly, after some time firing, the inhibitor yoked to the phrase/RB unit coding <italic>waster</italic>+games will fire, forcing that unit to inactivity and allowing another phrase/RB (e.g., <italic>wasted</italic>+time) to fire. Establishing the pattern of firing described above requires units in different layers firing at different timescales. This pattern can be achieved in any number of ways, the simplest being setting the threshold of the inhibitor units appropriately (e.g., word/PO inhibitors have half the firing threshold of phrase/RB inhibitors). In DORA, inhibitor units of words/POs integrate input both from their yoked exciter and from the exciters of all units in the above layers (e.g., phrases/RBs). Consequently, words/POs naturally oscillate at twice the frequency of phrases/RBs.</p>
<p>Crucially, sequential firing of related constituent elements is a necessary property of binding via synchrony and systematic asynchrony. When DORA performs any structured processing, a pattern will invariably emerge wherein bound elements within a larger compositional proposition will fire in direct sequence and at a different timescale than units coding for conjunctions of independently bound elements and full propositional compounds. In the following section, we show that the pattern of activation produced by DORA as it processes compositional structures very closely matches the temporal pattern of spike activity observed in Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] when people process sentences.</p>
</sec>
<sec id="sec009">
<title>Simulation of Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>]</title>
<p>We simulated the Ding et al. [<xref ref-type="bibr" rid="pbio.2000663.ref006">6</xref>] studies using the same English sentences used in their Experiments 5 and 6 (with native English speakers). All of these sentences took the form modifier-noun-verb-noun, forming sentences like “new plans give hope,” “fun games waste time,” and “dry fur rubs skin.” DORA can represent hierarchical propositions by representing propositional structures as arguments of other propositional structures. For example, to represent “dry fur rubs skin,” the modified noun phrase “dry fur” can be represented explicitly by the propositional structure <italic>dry</italic> (fur), which can then serve as the argument of the agent role of the <italic>rubs</italic> relation (see <xref ref-type="fig" rid="pbio.2000663.g002">Fig 2</xref>; details of higher-order structure representation in LISA, from which DORA is descended, and from DORA can be found in Hummel &amp; Holyoak [<xref ref-type="bibr" rid="pbio.2000663.ref021">21</xref>] and in Doumas et al. [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>] respectively).</p>
<p>To simulate Ding et al.’s main experimental procedure, we allowed DORA to process Ding et al.’s English sentences one at a time, using the representations of those sentences. Representations of the sentence structures (e.g., Figs <xref ref-type="fig" rid="pbio.2000663.g001">1</xref> and <xref ref-type="fig" rid="pbio.2000663.g002">2</xref>) entered the driver (i.e., were attended to). We then activated these sentence structures one word at a time. That is, we activated the semantic units encoding the first word for 110 iterations, then the second word for 110 iterations, and so forth. As semantic units became active, they passed activation to token units in the driver. The units in the driver responded to the pattern of firing in the semantic units (i.e., the units fired to represent and encode binding information; see <xref ref-type="fig" rid="pbio.2000663.g008">Fig 8</xref>).</p>
<p>To simulate Ding et al.’s control condition we allowed DORA to process Ding et al.’s random word sequences one at a time. In this <italic>Word List</italic> condition, there were no syntactic relationships between words. Representations of the sentence word sequence entered the driver (i.e., were attended to). DORA processed the word as it normally would (i.e., the units coding each term fired, but because the sequence of words included no propositional structure, only word/PO units fired during processing). We tracked firing rate of all the nodes in the driver as DORA processed the sentences. The results of the simulation and the comparison to the patterns observed by Ding et al. are presented in <xref ref-type="fig" rid="pbio.2000663.g003">Fig 3</xref>. Interestingly, the pattern of firing of the nodes in the various layers of DORA very closely mirror the patterns observed by Ding et al. Specifically, just like the human participants, DORA showed an activation burst only at the rate of word representation, or four times the rate of the whole sentence burst (i.e., the word/PO units firing in the 4 Hz range).</p>
</sec>
<sec id="sec010">
<title>Retrieval and activation of LTM</title>
<p>DORA, like LISA, performs memory retrieval by firing propositions in the driver and allowing activation to flow to LTM via the shared semantic units. Units in LTM respond to the pattern of activation in the semantic units imposed by the units in the driver and are retrieved into active memory (the recipient) via a Luce [<xref ref-type="bibr" rid="pbio.2000663.ref047">47</xref>] choice function (see Doumas et al. [<xref ref-type="bibr" rid="pbio.2000663.ref019">19</xref>]). For example, when DORA is "thinking about" how games waste time, and the proposition <italic>wastes</italic> (games, time) becomes active in the driver, activation will flow through the semantic units to units in LTM that share semantic overlap with the word/PO units becoming active in the driver (i.e., <italic>wasters</italic>, <italic>wasted-things</italic>, games, time, and things like them). We used this property of the model to further test whether the model is representing syntactic structure. We had DORA to process versions of Ding et al.’s word sequences in the Word List condition (please see <xref ref-type="supplementary-material" rid="pbio.2000663.s001">S1 Text</xref>), and we created a <italic>Jabberwocky</italic> condition where there were only syntactic relationships between words but no typical compositional semantic relationships. Representations of the sentence word sequence entered the driver (i.e., were attended to). DORA processed the sentences one word at a time (i.e., the units fired to represent and encode binding information, as above). We tracked firing rate of all the nodes in the driver as DORA processed the sentences. The results of the simulation and the comparison to the patterns observed in the experimental conditions are in Figs <xref ref-type="fig" rid="pbio.2000663.g003">3</xref>, <xref ref-type="fig" rid="pbio.2000663.g004">4</xref>, <xref ref-type="fig" rid="pbio.2000663.g005">5</xref> and <xref ref-type="fig" rid="pbio.2000663.g006">6</xref>. Ding et al. also manipulated a form of constituency—the linguistic relationship between discrete units and larger units, in this case, between syllables and words—to determine if there was evidence for cortical tracking of these various units. They found that words with multiple syllables elicited oscillations that tracked with the duration of the phrase boundary, not just syllables, which were fixed at 250 ms or 4 Hz. Ding et al. found 2 Hz and 4 Hz activity for two disyllabic words that together formed a phrase (a stimulus stream of (xx)(xx), but no 1 Hz activity. Since DORA does not have the perceptual apparatus to process auditory signals, we created a text analogue of the phrase condition (Phrases condition).</p>
</sec>
<sec id="sec011">
<title>Simulations in a RNN</title>
<p>To test whether the oscillatory pattern observed by Ding et al. and in our DORA simulations can be observed in a system without time-based binding and without representational hierarchy, we repeated all four simulations in a RNN implemented in Theano [<xref ref-type="bibr" rid="pbio.2000663.ref048">48</xref>]. The network had one hidden layer (see <xref ref-type="fig" rid="pbio.2000663.g009">Fig 9</xref>).</p>
<fig id="pbio.2000663.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2000663.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Architecture of the three-layer RNN.</title>
<p><italic>x</italic>, <italic>s</italic>, and <italic>o</italic> represent vectors of activation of units in the input, recurrent, and output layers, respectively. <italic>U</italic>, <italic>V</italic>, and <italic>W</italic> represent input, output, and recurrent weight matrices, respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.g009" xlink:type="simple"/>
</fig>
<sec id="sec012">
<title>Sentence and word coding</title>
<p>In order to train the RNNs in the current simulations, two encoding systems were used. In the input encoding system, each word was assigned a randomly generated vector of zeros and ones of size ten. One additional 1-valued vector of size ten was used to represent the beginning of any sentence.</p>
<p>In the output encoding system, a localist representation was used instead. For this, each word was assigned a randomly chosen one-hot vector of size <italic>n</italic> = 333, where 333 is the number of different words across datasets plus one word for the end of the sentence marker. In this system, then, unit <italic>i</italic> = 1 represents word <italic>i</italic>, and the rest of the units are zeros.</p>
</sec>
<sec id="sec013">
<title>Network architecture</title>
<p>The RNN used for the simulations described here consisted of three layers of units: input, recurrent, and output (see <xref ref-type="fig" rid="pbio.2000663.g009">Fig 9</xref>). Input layers have ten units. Output layers have 333 units, one unit per word in the dataset plus one extra unit for the “end of the sentence” word. Recurrent layers have (number of words per phrase * ten) units. The network processed the input sequence over <italic>t</italic> (number of words per phrase) time steps, receiving one word per time step. The input layer activation at time step <italic>t</italic> corresponds to the random generated vector of zeros and ones of size ten that was associated with the word in the input encoding scheme. The activation in the recurrent and output layers of the RNN is calculated according to:
<disp-formula id="pbio.2000663.e002">
<alternatives>
<graphic id="pbio.2000663.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2000663.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>U</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pbio.2000663.e003">
<alternatives>
<graphic id="pbio.2000663.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2000663.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>s</italic><sub><italic>t</italic></sub> and <italic>o</italic><sub><italic>t</italic></sub> are the activation vectors at time step t of the recurrent and output layers; <italic>U</italic>, <italic>V</italic>, and <italic>W</italic> are the input, recurrent, and output weight matrices. The softmax function <inline-formula id="pbio.2000663.e004"><alternatives><graphic id="pbio.2000663.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2000663.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> results in a vector of activations that sum to one and can be interpreted as probabilities. Since a localist representation was used in the output layer, each element <italic>i</italic> of <italic>o</italic><sub><italic>t</italic></sub> is the network’s estimated probability that word <italic>i</italic> will be the input at time step <italic>t</italic> + 1.</p>
</sec>
<sec id="sec014">
<title>Network training</title>
<p>For each condition (Grammatical, Jabberwocky, Word List, Phrases), the RNN was presented with the sentences, one word by time step, and was trained to predict the next word in the current sentence. The prediction of the network was taken to be the index of the output unit with maximum activation.</p>
<p>For all conditions, the input sentences always had the form (“beginning” “word_1” “word_2”…“word_n”), while the criterion sentences always had the form (“word_1” “word_2”…”word_n” “end”). Since the first word of the sentence could not be predicted from the beginning vector, the recurrent state activations <italic>s</italic> resulting from feeding the beginning vector to the RNNs were discarded and are not discussed further.</p>
<p>The RNNs were trained with gradient descent and adaptive learning rate. The learning rate decreased to half its magnitude every time the cost increased (the initial learning rate was set to 0.005 for all conditions except for the Only NPs condition in which it was set to 0.01). All RNNs were trained for 150 epochs, at which point the network achieved perfect classification. The loss function used for training was cross-entropy.</p>
</sec>
</sec>
</sec>
<sec id="sec015">
<title>Supporting information</title>
<supplementary-material id="pbio.2000663.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pbio.2000663.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Stimuli.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We are grateful to Guillermo Puebla Ramirez for research assistance.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>adj</term>
<def><p>adjective</p></def>
</def-item>
<def-item><term>AdjP</term>
<def><p>adjective phrase</p></def>
</def-item>
<def-item><term>DORA</term>
<def><p>Discovery of Relations by Analogy</p></def>
</def-item>
<def-item><term>IP</term>
<def><p>inflectional phrase or sentence</p></def>
</def-item>
<def-item><term>LISA</term>
<def><p>Learning and Inference with Schemas and Analogies</p></def>
</def-item>
<def-item><term>NLP</term>
<def><p>natural language processing</p></def>
</def-item>
<def-item><term>n</term>
<def><p>noun</p></def>
</def-item>
<def-item><term>NP</term>
<def><p>noun phrase</p></def>
</def-item>
<def-item><term>PO</term>
<def><p>propositional object</p></def>
</def-item>
<def-item><term>P-unit</term>
<def><p>proposition node</p></def>
</def-item>
<def-item><term>RB unit</term>
<def><p>role-filler binding node</p></def>
</def-item>
<def-item><term>RNN</term>
<def><p>recurrent neural network</p></def>
</def-item>
<def-item><term>v</term>
<def><p>verb</p></def>
</def-item>
<def-item><term>VP</term>
<def><p>verb phrase</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2000663.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zatorre</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Lafaille</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ahad</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pike</surname> <given-names>B</given-names></name>. <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source>. <year>2000</year> <month>Jan</month> <day>20</day>;<volume>403</volume>(<issue>6767</issue>):<fpage>309</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35002078" xlink:type="simple">10.1038/35002078</ext-link></comment> <object-id pub-id-type="pmid">10659849</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title>. <source>Neuron</source>. <year>2007</year> <month>Jun</month> <day>21</day>;<volume>54</volume>(<issue>6</issue>):<fpage>1001</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.06.004" xlink:type="simple">10.1016/j.neuron.2007.06.004</ext-link></comment> <object-id pub-id-type="pmid">17582338</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nat Neurosci</source>. <year>2012</year> <month>Apr</month> <day>1</day>;<volume>15</volume>(<issue>4</issue>):<fpage>511</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3063" xlink:type="simple">10.1038/nn.3063</ext-link></comment> <object-id pub-id-type="pmid">22426255</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghitza</surname> <given-names>O.</given-names></name> <article-title>Linking speech perception and neurophysiology: speech decoding guided by cascaded oscillators locked to the input rhythm</article-title>. <source>Front Psychol</source>. <volume>2</volume> (<issue>2011</issue>): <fpage>130</fpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelle</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>MH</given-names></name>. <article-title>Neural oscillations carry speech rhythm through to comprehension</article-title>. <source>Front Psychol</source>. <year>2012</year> <month>Sep</month> <day>6</day>;<volume>3</volume>:<fpage>320</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fpsyg.2012.00320" xlink:type="simple">10.3389/fpsyg.2012.00320</ext-link></comment> <object-id pub-id-type="pmid">22973251</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Melloni</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nat Neurosci</source>. <year>2016</year> <month>Jan</month> <day>1</day>;<volume>19</volume>(<issue>1</issue>):<fpage>158</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4186" xlink:type="simple">10.1038/nn.4186</ext-link></comment> <object-id pub-id-type="pmid">26642090</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Giraud</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lund</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Frackowiak</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Laufs</surname> <given-names>H</given-names></name>. <article-title>Endogenous cortical rhythms determine cerebral specialization for speech perception and production</article-title>. <source>Neuron</source>. <year>2007</year> <month>Dec</month> <day>20</day>;<volume>56</volume>(<issue>6</issue>):<fpage>1127</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2007.09.038" xlink:type="simple">10.1016/j.neuron.2007.09.038</ext-link></comment> <object-id pub-id-type="pmid">18093532</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>A hierarchy of time-scales and the brain</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year> <month>Nov</month> <day>14</day>;<volume>4</volume>(<issue>11</issue>):<fpage>e1000209</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000209" xlink:type="simple">10.1371/journal.pcbi.1000209</ext-link></comment> <object-id pub-id-type="pmid">19008936</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>The neuroanatomic and neurophysiological infrastructure for speech and language</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year> <month>Oct</month> <day>31</day>;<volume>28</volume>:<fpage>142</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2014.07.005" xlink:type="simple">10.1016/j.conb.2014.07.005</ext-link></comment> <object-id pub-id-type="pmid">25064048</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barsalou</surname> <given-names>LW</given-names></name>. <article-title>Perceptions of perceptual symbols</article-title>. <source>Behav Brain Sci</source>. <year>1999</year> <month>Aug</month> <day>1</day>;<volume>22</volume>(<issue>04</issue>):<fpage>637</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname> <given-names>CR</given-names></name>. <source>The organization of learning</source>. <publisher-name>The MIT Press</publisher-name>; <year>1990</year>.</mixed-citation></ref>
<ref id="pbio.2000663.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pinker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jackendoff</surname> <given-names>R</given-names></name>. <article-title>The faculty of language: what's special about it?</article-title> <source>Cognition</source>. <year>2005</year> <month>Mar</month> <day>31</day>;<volume>95</volume>(<issue>2</issue>):<fpage>201</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2004.08.004" xlink:type="simple">10.1016/j.cognition.2004.08.004</ext-link></comment> <object-id pub-id-type="pmid">15694646</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Von der Malsburg</surname> <given-names>C</given-names></name>. <article-title>The what and why of binding: the modeler’s perspective</article-title>. <source>Neuron</source>. <year>1999</year> <month>Sep</month> <day>30</day>;<volume>24</volume>(<issue>1</issue>):<fpage>95</fpage>–<lpage>104</lpage>. <object-id pub-id-type="pmid">10677030</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berwick</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Friederici</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Chomsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bolhuis</surname> <given-names>JJ</given-names></name>. <article-title>Evolution, brain, and the nature of language</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year> <month>Feb</month> <day>28</day>;<volume>17</volume>(<issue>2</issue>):<fpage>89</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2012.12.002" xlink:type="simple">10.1016/j.tics.2012.12.002</ext-link></comment> <object-id pub-id-type="pmid">23313359</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Chomsky</surname> <given-names>N</given-names></name>. <source>Syntactic structures</source>. <publisher-name>Walter de Gruyter</publisher-name>; <year>1957</year>.</mixed-citation></ref>
<ref id="pbio.2000663.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Halle</surname> <given-names>M</given-names></name>. <article-title>Phonology in generative grammar</article-title>. <source>Word</source>. <year>1962</year> <month>Jan</month> <day>1</day>;<volume>18</volume>(<issue>1–3</issue>):<fpage>54</fpage>–<lpage>72</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Idsardi</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Van Wassenhove</surname> <given-names>V</given-names></name>. <article-title>Speech perception at the interface of neurobiology and linguistics</article-title>. <source>Phil Trans R Soc B-Biol Sci</source>. <year>2008</year> <month>Mar</month> <day>12</day>;<volume>363</volume>(<issue>1493</issue>):<fpage>1071</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martin</surname> <given-names>AE</given-names></name>. <article-title>Language processing as cue integration: Grounding the psychology of language in perception and neurophysiology</article-title>. <source>Front Psychol</source>. <year>2016</year>;<volume>7</volume>.</mixed-citation></ref>
<ref id="pbio.2000663.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Sandhofer</surname> <given-names>CM</given-names></name>. <article-title>A theory of the discovery and predication of relational concepts</article-title>. <source>Psychol Rev</source>. <year>2008</year> <month>Jan</month>;<volume>115</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.115.1.1" xlink:type="simple">10.1037/0033-295X.115.1.1</ext-link></comment> <object-id pub-id-type="pmid">18211183</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref020"><label>20</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>. <chapter-title>Approaches to modeling human mental representations: What works, what doesn’t and why</chapter-title>. <source>The Cambridge handbook of thinking and reasoning</source>, ed. <name name-style="western"><surname>Holyoak</surname> <given-names>KJ</given-names></name> &amp; <name name-style="western"><surname>Morrison</surname> <given-names>RG</given-names></name>. <year>2005</year> <month>Apr</month> <volume>18</volume>:<fpage>73</fpage>–<lpage>94</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Holyoak</surname> <given-names>KJ</given-names></name>. <article-title>Distributed representations of structure: A theory of analogical access and mapping</article-title>. <source>Psychol Rev</source>. <year>1997</year> <month>Jul</month>;<volume>104</volume>(<issue>3</issue>):<fpage>427</fpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K.</given-names></name> <article-title>Structural and functional brain networks: from connections to cognition</article-title>. <source><italic>Science</italic></source>. <year>2013</year>, <volume>342</volume>. Jg., Nr. <issue>6158</issue>, S. 1238411.</mixed-citation></ref>
<ref id="pbio.2000663.ref023"><label>23</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AP</given-names></name>. <source>Memory and the computational brain: Why cognitive science will transform neuroscience</source>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2011</year>.</mixed-citation></ref>
<ref id="pbio.2000663.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pylkkänen</surname> <given-names>L.</given-names></name>, <name name-style="western"><surname>Bemis</surname> <given-names>D. K.</given-names></name>, &amp; <name name-style="western"><surname>Elorrieta</surname> <given-names>E. B.</given-names></name> (<year>2014</year>). <article-title>Building phrases in language production: An MEG study of simple composition</article-title>. <source>Cognition</source>, <volume>133</volume>(<issue>2</issue>), <fpage>371</fpage>–<lpage>384</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2014.07.001" xlink:type="simple">10.1016/j.cognition.2014.07.001</ext-link></comment> <object-id pub-id-type="pmid">25128795</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pinker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Prince</surname> <given-names>A</given-names></name>. <article-title>On language and connectionism: Analysis of a parallel distributed processing model of language acquisition</article-title>. <source>Cognition</source>. <year>1988</year> <month>Mar</month> <day>1</day>;<volume>28</volume>(<issue>1–2</issue>):<fpage>73</fpage>–<lpage>193</lpage>. <object-id pub-id-type="pmid">2450717</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref026"><label>26</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Norvig</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Canny</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Edwards</surname> <given-names>DD</given-names></name>. <source>Artificial intelligence: a modern approach</source>. <publisher-loc>Upper Saddle River</publisher-loc>: <publisher-name>Prentice hall</publisher-name>; <year>2003</year> <month>Jan</month>.</mixed-citation></ref>
<ref id="pbio.2000663.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treisman</surname> <given-names>A</given-names></name>. <article-title>The binding problem</article-title>. <source>Curr Opin Neurobiol</source>. <year>1996</year> <month>Apr</month> <day>30</day>;<volume>6</volume>(<issue>2</issue>):<fpage>171</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">8725958</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref028"><label>28</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hebb</surname> <given-names>DO</given-names></name>. <source>The organization of behavior: A neuropsychological approach</source>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>1949</year>.</mixed-citation></ref>
<ref id="pbio.2000663.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shastri</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ajjanagadde</surname> <given-names>V</given-names></name>. <article-title>From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings using temporal synchrony</article-title>. <source>Behav Brain Sci</source>. <year>1993</year> <month>Sep</month> <day>1</day>;<volume>16</volume>(<issue>03</issue>):<fpage>417</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marcus</surname> <given-names>GF</given-names></name>. <article-title>Rethinking eliminative connectionism</article-title>. <source>Cogn Psychol</source>. <year>1998</year> <month>Dec</month> <day>31</day>;<volume>37</volume>(<issue>3</issue>):<fpage>243</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/cogp.1998.0694" xlink:type="simple">10.1006/cogp.1998.0694</ext-link></comment> <object-id pub-id-type="pmid">9892549</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penn</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Holyoak</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Povinelli</surname> <given-names>DJ</given-names></name>. <article-title>Darwin's mistake: Explaining the discontinuity between human and nonhuman minds</article-title>. <source>Behav Brain Sci</source>. <year>2008</year> <month>Apr</month> <day>1</day>;<volume>31</volume>(<issue>02</issue>):<fpage>109</fpage>–<lpage>30</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title>. <source>Neuron</source>. <year>2010</year> <month>Nov</month> <day>4</day>;<volume>68</volume>(<issue>3</issue>):<fpage>362</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.09.023" xlink:type="simple">10.1016/j.neuron.2010.09.023</ext-link></comment> <object-id pub-id-type="pmid">21040841</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>. <chapter-title>Computational models of higher cognition</chapter-title>. <source>The Oxford Handbook of Thinking and Reasoning</source>. <year>2012</year> <month>Apr</month> <volume>19</volume>:<fpage>52</fpage>–<lpage>66</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>. <article-title>A computational account of the development of the generalization of shape information</article-title>. <source>Cogn Sci</source>. <year>2010</year> <month>May</month> <day>1</day>;<volume>34</volume>(<issue>4</issue>):<fpage>698</fpage>–<lpage>712</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1551-6709.2010.01103.x" xlink:type="simple">10.1111/j.1551-6709.2010.01103.x</ext-link></comment> <object-id pub-id-type="pmid">21564231</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref035"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Lim A, Doumas LA, Sinnett S. Supramodal representations in melodic perception. In 36th Annual Conference of the Cognitive Science Society, Quebec, Canada 2014.</mixed-citation></ref>
<ref id="pbio.2000663.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morrison</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Richland</surname> <given-names>LE</given-names></name>. <article-title>A computational account of children’s analogical reasoning: balancing inhibitory control in working memory and relational representation</article-title>. <source>Dev Sci</source>. <year>2011</year> <month>May</month> <day>1</day>;<volume>14</volume>(<issue>3</issue>):<fpage>516</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-7687.2010.00999.x" xlink:type="simple">10.1111/j.1467-7687.2010.00999.x</ext-link></comment> <object-id pub-id-type="pmid">21477191</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sandhofer</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>. <article-title>Order of presentation effects in learning color categories</article-title>. <source>J Cogn Dev</source>. <year>2008</year> <month>Apr</month> <day>30</day>;<volume>9</volume>(<issue>2</issue>):<fpage>194</fpage>–<lpage>221</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hanslmayr</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Staresina</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Bowman</surname> <given-names>H</given-names></name>. <article-title>Oscillations and Episodic Memory: Addressing the Synchronization/Desynchronization Conundrum</article-title>. <source>Trends Neurosci</source>. <year>2016</year> <month>Jan</month> <day>31</day>;<volume>39</volume>(<issue>1</issue>):<fpage>16</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2015.11.004" xlink:type="simple">10.1016/j.tins.2015.11.004</ext-link></comment> <object-id pub-id-type="pmid">26763659</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref039"><label>39</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Heim</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kratzer</surname> <given-names>A</given-names></name>. <source>Semantics in generative grammar</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Blackwell</publisher-name>; <year>1998</year> <month>Jan</month> <day>2</day>.</mixed-citation></ref>
<ref id="pbio.2000663.ref040"><label>40</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Markman</surname> <given-names>AB</given-names></name>. <source>Knowledge representation</source>. <publisher-name>Psychology Press</publisher-name>; <year>1999</year>.</mixed-citation></ref>
<ref id="pbio.2000663.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Osindero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>. <article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural Comput</source>. <year>2006</year> <month>Jul</month>;<volume>18</volume>(<issue>7</issue>):<fpage>1527</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2006.18.7.1527" xlink:type="simple">10.1162/neco.2006.18.7.1527</ext-link></comment> <object-id pub-id-type="pmid">16764513</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Kemp</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>ND</given-names></name>. <article-title>How to grow a mind: Statistics, structure, and abstraction</article-title>. <source>Science</source>. <year>2011</year> <month>Mar</month> <day>11</day>;<volume>331</volume>(<issue>6022</issue>):<fpage>1279</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1192788" xlink:type="simple">10.1126/science.1192788</ext-link></comment> <object-id pub-id-type="pmid">21393536</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname> <given-names>D</given-names></name>. <article-title>The maps problem and the mapping problem: two challenges for a cognitive neuroscience of speech and language</article-title>. <source>Cogn Neuropsychol</source>. <year>2012</year> <month>Mar</month> <day>1</day>;<volume>29</volume>(<issue>1–2</issue>):<fpage>34</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643294.2012.710600" xlink:type="simple">10.1080/02643294.2012.710600</ext-link></comment> <object-id pub-id-type="pmid">23017085</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doumas</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>. <article-title>Comparison and mapping facilitate relation discovery and predication</article-title>. <source>PLoS ONE</source>. <year>2013</year> <month>Jun</month> <day>25</day>;<volume>8</volume>(<issue>6</issue>):<fpage>e63889</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0063889" xlink:type="simple">10.1371/journal.pone.0063889</ext-link></comment> <object-id pub-id-type="pmid">23825521</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hummel</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Holyoak</surname> <given-names>KJ</given-names></name>. <article-title>A symbolic-connectionist theory of relational inference and generalization</article-title>. <source>Psychol Rev</source>. <year>2003</year> <month>Apr</month>;<volume>110</volume>(<issue>2</issue>):<fpage>220</fpage>. <object-id pub-id-type="pmid">12747523</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowan</surname> <given-names>N</given-names></name>. <article-title>Metatheory of storage capacity limits</article-title>. <source>Behav Brain Sci</source>. <year>2001</year> <month>Feb</month> <day>1</day>;<volume>24</volume>(<issue>01</issue>):<fpage>154</fpage>–<lpage>76</lpage>.</mixed-citation></ref>
<ref id="pbio.2000663.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luce</surname> <given-names>RD</given-names></name>. <article-title>On the possible psychophysical laws</article-title>. <source>Psychol Rev</source>. <year>1959</year> <month>Mar</month>;<volume>66</volume>(<issue>2</issue>):<fpage>81</fpage>. <object-id pub-id-type="pmid">13645853</object-id></mixed-citation></ref>
<ref id="pbio.2000663.ref048"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Bergstra J, Breuleux O, Bastien F, Lamblin P, Pascanu R, Desjardins G, Turian J, Warde-Farley D, Bengio Y. Theano: A CPU and GPU math compiler in Python. InProc. 9th Python in Science Conf 2010 Jun (pp. 1–7).</mixed-citation></ref>
</ref-list>
</back>
</article>