<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-02095</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005667</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject><subj-group><subject>Old World monkeys</subject><subj-group><subject>Macaque</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A mixture of sparse coding models explaining properties of face neurons related to holistic and parts-based processing</article-title>
<alt-title alt-title-type="running-head">Mixture of sparse coding models and face neurons</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5660-0801</contrib-id>
<name name-style="western">
<surname>Hosoya</surname> <given-names>Haruo</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5806-4432</contrib-id>
<name name-style="western">
<surname>Hyvärinen</surname> <given-names>Aapo</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Cognitive Mechanisms Laboratories, ATR International, Kyoto, Japan</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Computer Science and HIIT, University of Helsinki, Helsinki, Finland</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Gatsby Computational Neuroscience Unit, University College London, London, UK</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Einhäuser</surname> <given-names>Wolfgang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Technische Universitat Chemnitz, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> HH AH.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> HH AH.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> HH AH.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> HH AH.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> HH.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> HH AH.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> HH AH.</p>
</list-item>
</list>
</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: 2–2–2 Hikaridai, Keihanna Science City, Kyoto, Japan</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">hosoya@atr.jp</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>7</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>25</day>
<month>7</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>7</issue>
<elocation-id>e1005667</elocation-id>
<history>
<date date-type="received">
<day>3</day>
<month>1</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>7</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Hosoya, Hyvärinen</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005667"/>
<abstract>
<p>Experimental studies have revealed evidence of both parts-based and holistic representations of objects and faces in the primate visual system. However, it is still a mystery how such seemingly contradictory types of processing can coexist within a single system. Here, we propose a novel theory called mixture of sparse coding models, inspired by the formation of category-specific subregions in the inferotemporal (IT) cortex. We developed a hierarchical network that constructed a mixture of two sparse coding submodels on top of a simple Gabor analysis. The submodels were each trained with face or non-face object images, which resulted in separate representations of facial parts and object parts. Importantly, evoked neural activities were modeled by Bayesian inference, which had a top-down explaining-away effect that enabled recognition of an individual part to depend strongly on the category of the whole input. We show that this explaining-away effect was indeed crucial for the units in the face submodel to exhibit significant selectivity to face images over object images in a similar way to actual face-selective neurons in the macaque IT cortex. Furthermore, the model explained, qualitatively and quantitatively, several tuning properties to facial features found in the middle patch of face processing in IT as documented by Freiwald, Tsao, and Livingstone (2009). These included, in particular, tuning to only a small number of facial features that were often related to geometrically large parts like face outline and hair, preference and anti-preference of extreme facial features (e.g., very large/small inter-eye distance), and reduction of the gain of feature tuning for partial face stimuli compared to whole face stimuli. Thus, we hypothesize that the coding principle of facial features in the middle patch of face processing in the macaque IT cortex may be closely related to mixture of sparse coding models.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Does the brain represent an object as a combination of parts or as a whole? Past experiments have found both types of representation; but how can such opposing notions coexist in a single visual system? Here, we introduce a novel theory called mixture of sparse coding models for investigating the possible computational principles underlying the primate visual object processing. We constructed a hierarchical network combining two sparse coding modules that each represented one feature set, of either facial parts or non-facial object parts. Competitive computation between the modules, formalized as Bayesian inference, enabled parts to be recognized with a strong top-down influence from the category of the whole input. We show that the latter computation is crucial to explain in detail neural selectivity and tuning properties that were experimentally reported for a particular face processing region called the middle patch. Thus, we offer the first theoretical account of neural face processing in relation to parts-based and holistic representations.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Gatsby Charitable Foundation (GB)</institution>
</funding-source>
<award-id>GAT3528</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5806-4432</contrib-id>
<name name-style="western">
<surname>Hyvärinen</surname> <given-names>Aapo</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>New Energy and Industrial Technology Development Organization (JP)</institution>
</funding-source>
<award-id>P15009</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5660-0801</contrib-id>
<name name-style="western">
<surname>Hosoya</surname> <given-names>Haruo</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Academy of Finland</institution>
</funding-source>
<award-id>291538</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5806-4432</contrib-id>
<name name-style="western">
<surname>Hyvärinen</surname> <given-names>Aapo</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>Academy of Finland</institution>
</funding-source>
<award-id>250215</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5806-4432</contrib-id>
<name name-style="western">
<surname>Hyvärinen</surname> <given-names>Aapo</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>HH was supported by the New Energy and Industrial Technology Development Organization (P15009; <ext-link ext-link-type="uri" xlink:href="http://www.nedo.go.jp" xlink:type="simple">www.nedo.go.jp</ext-link>). AH was supported by Academy of Finland (291538 and 250215; <ext-link ext-link-type="uri" xlink:href="http://www.aka.fi" xlink:type="simple">www.aka.fi</ext-link>) and Gatsby Charitable Foundation (GAT3528; <ext-link ext-link-type="uri" xlink:href="http://www.gatsby.org.uk" xlink:type="simple">www.gatsby.org.uk</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="13"/>
<table-count count="0"/>
<page-count count="27"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2017-08-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The presented results are all reproducible from the information contained within the paper using two publicly available image databases: Caltech101 (<ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101" xlink:type="simple">www.vision.caltech.edu/Image_Datasets/Caltech101</ext-link>) and Labeled Faces In Wild (<ext-link ext-link-type="uri" xlink:href="http://vis-www.cs.umass.edu/lfw" xlink:type="simple">vis-www.cs.umass.edu/lfw</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The variety of objects that we see everyday is overwhelming and how our visual system deals with such complexity is a long-standing problem. Classical psychology has often debated on whether an object is represented as a combination of individual parts (parts-based processing) or as a whole (holistic processing) [<xref ref-type="bibr" rid="pcbi.1005667.ref001">1</xref>]. Experimental studies have revealed evidence of both types of processing in behaviors [<xref ref-type="bibr" rid="pcbi.1005667.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref002">2</xref>] and in neural activities in higher visual areas [<xref ref-type="bibr" rid="pcbi.1005667.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref005">5</xref>], somewhat favoring holistic representation for faces and parts-based representation for non-face objects [<xref ref-type="bibr" rid="pcbi.1005667.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref005">5</xref>]. However, a theoretical question is: how could a single system reconcile such two seemingly contradictory types of processing? Although a number of studies on computational vision models showed remarkable performance in visual recognition [<xref ref-type="bibr" rid="pcbi.1005667.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref010">10</xref>], success in modeling higher visual areas [<xref ref-type="bibr" rid="pcbi.1005667.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref012">12</xref>], or account for behavioral experiments on holistic face processing [<xref ref-type="bibr" rid="pcbi.1005667.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref013">13</xref>], none of these studies offered insight into the tension between parts-based and holistic processing in a comparative manner with neurophyisology.</p>
<p>In this study, we address this question in a novel theoretical framework, called mixture of sparse coding models. We assume two separate sparse coding models, one dedicated to encode face images and the other to encode non-face object images, that perform competitive interaction. Sparse coding is well known for its close relationship with representations in early visual areas [<xref ref-type="bibr" rid="pcbi.1005667.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref022">22</xref>]; we transfer this technique to the study of higher visual representations. That is, exploiting the fact that sparse coding to image data of a specific category can yield parts-based feature representations (cf. [<xref ref-type="bibr" rid="pcbi.1005667.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref024">24</xref>]), we constructed two separate category-specific representations for faces and objects analogously to the formation of specialized subregions for faces and objects in the inferotemporal (IT) cortex [<xref ref-type="bibr" rid="pcbi.1005667.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref026">26</xref>]. Furthermore, we combined the two sparse coding models into a mixture model and modeled neural activities in terms of Bayesian inference. Then, we found that this framework gave rise to a form of holistic computation: not only recognition of the whole object depends on the individual parts, but also recognition of a part depends on the whole. This is in fact a Bayesian explaining-away effect: an input image is first independently interpreted by each sparse coding submodel, but then the one offering the better interpretation is adopted and the other is dismissed. For example, even if a part of an input image is a potential facial feature (e.g., a half-moon-like shape <inline-formula id="pcbi.1005667.e001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e001" xlink:type="simple"/></inline-formula>), that feature would not be recognized as an actual facial feature (e.g., a mouth) if the whole image is a non-face object (<xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1B</xref>).</p>
<fig id="pcbi.1005667.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g001</object-id>
<label>Fig 1</label>
<caption>
<title/>
<p>(A) The architecture of our hierarchical model. It starts with an energy detector bank and proceeds to two sparse coding submodels for faces and objects, which are then combined into a mixture model. Inset: an energy detector model. (B) Cartoon face and boat. Note that the mouth of the face and the base of the boat are the same shapes. (C) Learning scheme. We assume explicit class information, either “face” or “object,” of input images to be given during training, which allows us to use a standard sparse coding learning for each submodel with the corresponding dataset. (D) Inference scheme. For testing response properties, the network first interprets the input separately by the sparse code of each submodel (step 1), then compares the goodnesses of the obtained interpretations as posterior probabilities (step 2), and finally modules multiplicatively the responses in each submodel with the corresponding posterior probability (step 3). Note that the normalization of the probablities in step 2 leads to competition between the submodels in step 3.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g001" xlink:type="simple"/>
</fig>
<p>We discovered that our model had a close relationship with computation known for a region of the macaque IT cortex called the face-selective middle patch, as documented by Freiwald et al. [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]. First, our model cells in the face submodel exhibited prominent selectivity to face images over non-face object images in a similar way to actual face-selective neurons, and this selectivity was crucially dependent on the above-mentioned explaining-away effect. Second, these model cells reproduced a number of tuning properties of face neurons in the middle patch. In particular, our model face cells tended to (a) be tuned to only a small number of facial features, often related to geometrically large parts such as face outline and hair, (b) prefer one extreme for a particular facial feature while anti-prefering the other extreme, and (c) reduce the gain of tuning when a partial face was presented compared to a whole face. We quantified these properties and compared these with the experimental data at the population level [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]; the result showed a good match. Thus, we propose the hypothesis that regions of the IT cortex representing objects or faces may employ a computational principle similar to mixture of sparse coding models.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Model</title>
<p>To investigate the computational principles underlying face and object processing in the IT cortex, we designed a multi-layer network model illustrated in <xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1A</xref>. The network had the architecture that received an image of 64 × 64 pixels, processed it with a fixed bank of standard energy detector models, and fed the results to two sparse coding models, called face submodel and object submodel (each with 400 model neurons), which were then combined into a mixture model to perform competitive interaction as explained later.</p>
<p>Each energy detector computed the squared norm of the outputs from two Gabor filters for the input image (<xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1A</xref>, inset). The two filters had the same center position, orientation, and spatial frequency, but had phases different by 90°. The entire bank of energy detectors had all combinations of 10 × 10 center positions (in a grid layout), 8 orientations, and 3 frequencies; thus, the output of this stage had a total of 2400 dimensions (see the section on Model details in <xref ref-type="sec" rid="sec009">Methods</xref>). In the actual visual cortex, inputs to IT areas are presumably computed between V1 and V4 and this computation must be much more complex than the energy detector bank in our model. However, some important aspects should still be reflected by this simple operation since a large number of V4 neurons are known to be orientation-selective [<xref ref-type="bibr" rid="pcbi.1005667.ref027">27</xref>]; moreover, this simple assumption was sufficient to reproduce certain response properties of face neurons as shown in what follows.</p>
<p>In training the mixture model, we assumed, for simplicity, that the class label of each input image, either “face” or “object,” was given (<xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1C</xref>). This allowed us to use a naive learning procedure that separately trained each face or object submodel with an existing sparse coding method. Specifically, we used publicly available face and object image datasets in which the faces or objects were properly aligned within each image frame [<xref ref-type="bibr" rid="pcbi.1005667.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref029">29</xref>] (see the section on Data preprocessing in <xref ref-type="sec" rid="sec009">Methods</xref>). Then, for each image class <italic>k</italic>, which was either 1 (face) or 2 (object), we learned the basis matrix <bold>A</bold><sup><italic>k</italic></sup> and the mean vector <bold>b</bold><sup><italic>k</italic></sup> by sparse coding of the corresponding set of images that were processed by the energy model. (The basis matrix and mean vector were used for determining the responses of the model neurons to an input as explained below.) Classical mixture models are usually trained with an unsupervised learning method without class labels [<xref ref-type="bibr" rid="pcbi.1005667.ref030">30</xref>]. However, such learning is generally not easy and not our main interest here since we focus on inference, i.e., on computation of evoked responses, not on learning or plasticity. (We come back to this point in the <xref ref-type="sec" rid="sec008">Discussion</xref> section.)</p>
<p>To perform sparse coding learning, we adopted our previously developed approach based on independent component analysis (ICA) [<xref ref-type="bibr" rid="pcbi.1005667.ref022">22</xref>], which is known to be a good approximation of sparse coding [<xref ref-type="bibr" rid="pcbi.1005667.ref031">31</xref>] and for which efficient algorithms exist. In this approach, an important step was to drastically reduce the input dimensions, from 2400 to 100 dimensions here, by principal component analysis (PCA) before performing ICA. This is, in fact, a simple modification of a standard preprocessing used in any classical sparse coding or ICA methods. However, we have previously discovered that such strong dimension reduction has an effect of spatial pooling [<xref ref-type="bibr" rid="pcbi.1005667.ref032">32</xref>] and thereby produces much larger basis patterns than without it [<xref ref-type="bibr" rid="pcbi.1005667.ref022">22</xref>]. In the present case, we later show that weaker dimension reduction resulted in representations of overly small features, which led to a loss of discriminative power. After this step, to regain enough components from the reduced dimensions, we used overcomplete ICA [<xref ref-type="bibr" rid="pcbi.1005667.ref033">33</xref>], estimating 400 components from 100 dimensions. (See the section on Learning details in <xref ref-type="sec" rid="sec009">Methods</xref>).</p>
<p>Once the network was trained, the response properties of the model neurons were tested using various input images. In this phase, we never explicitly gave class information on each input image, but rather let the network estimate it by Bayesian inference, which worked in the following three steps (<xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1D</xref>).</p>
<list list-type="order">
<list-item>
<p>Given an input <bold>x</bold> (processed by the energy detectors), interpret it separately by each submodel <italic>k</italic>. Formally, infer the responses <inline-formula id="pcbi.1005667.e002"><alternatives><graphic id="pcbi.1005667.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup></mml:math></alternatives></inline-formula> in each submodel <italic>k</italic> that maximize the sparse coding objective <italic>L</italic><sub><italic>k</italic></sub>:
<disp-formula id="pcbi.1005667.e003"><alternatives><graphic id="pcbi.1005667.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>←</mml:mo> <mml:munder><mml:mo form="prefix">argmax</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup></mml:munder> <mml:msub><mml:mi>L</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where
<disp-formula id="pcbi.1005667.e004"><alternatives><graphic id="pcbi.1005667.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="bold">A</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mo>λ</mml:mo></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:mrow><mml:mo>|</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
using pre-fixed constants <italic>σ</italic> and λ. Recall that <bold>A</bold><sup><italic>k</italic></sup> and <bold>b</bold><sup><italic>k</italic></sup> are the basis matrix and the mean vector for submodel <italic>k</italic> that are obtained in the learning phase as described above.</p>
</list-item>
<list-item>
<p>Compare the goodnesses of the two interpretations in the form of posterior probabilities. Formally, for each <italic>k</italic>:
<disp-formula id="pcbi.1005667.e005"><alternatives><graphic id="pcbi.1005667.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>←</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>h</mml:mi></mml:msub> <mml:msub><mml:mi>π</mml:mi> <mml:mi>h</mml:mi></mml:msub> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>L</mml:mi> <mml:mi>h</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>h</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
using a pre-fixed constant <italic>π</italic><sub><italic>k</italic></sub> for prior probability. For simplicity, we assume <italic>π</italic><sub><italic>k</italic></sub> = 1/2.</p>
</list-item>
<list-item>
<p>Modulate multiplicatively the responses in each submodel by the corresponding posterior probability computed above. That is, for each <italic>k</italic>:
<disp-formula id="pcbi.1005667.e006"><alternatives><graphic id="pcbi.1005667.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>←</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
</list-item>
</list>
<p>Step 1 is similar to inference in the classical sparse coding [<xref ref-type="bibr" rid="pcbi.1005667.ref031">31</xref>], where the responses in each submodel are estimated so as to minimize the reconstruction error and maximize the sparsity at the same time. One difference is, however, that the sparsity constraint here is on the difference from the mean vector <bold>b</bold><sup><italic>k</italic></sup>. We assume here a non-zero mean since the mean of face images is not zero and such stimulus usually elicits non-zero responses of actual face neurons, while the classical sparse coding assumes a zero mean since the mean of natural image patches is a blank, gray image, and such stimulus evokes no response of V1 neurons. The last two steps in our inference are a major departure from the classical sparse coding, where step 2 computes the posterior probability indicating how well each submodel interprets the input and step 3 multiplies the responses in each submodel by the corresponding posterior probability. Note that, because of the normalization of the probabilities in step 2, the multiplication in step 3 results in a competitive behavior of the two submodels. Thus, even if the input contains a feature that can potentially activate some units in a submodel, such units may eventually be deactivated when the whole input was not interpreted well by this submodel compared to the other submodels (Bayesian explaining-away effect).</p>
<p>Finally, to compare with neural responses later, we passed the response value of each unit (after step 3) to the smooth half-wave rectifying function <italic>h</italic>(<italic>a</italic>) = log(1 + exp(<italic>a</italic>)), which always produces non-negative values.</p>
<p>Although we presented above the mixture model and its inference computation in an informal and procedural way, these can be formalized rigorously within a probabilistic generative model. Generally, the motivation for such formalization is to regard visual recognition as a process of inferring hidden causes in the external world that generate a natural image. Our model can be seen as one such approach: all the computations described above can be derived from Bayesian inference of posterior probabilities in a statistical framework of mixture of sparse coding models. The details can be found in the section on Theory of mixture of sparse coding models in <xref ref-type="sec" rid="sec009">Methods</xref>.</p>
</sec>
<sec id="sec004">
<title>Basis representations</title>
<p>We proceed to show the representation in our model obtained by the learning procedure described so far. The basis matrix <bold>A</bold><sup><italic>k</italic></sup> of each submodel defines its internal representation and each column vector of the matrix (basis vector) exposes the specific feature represented by each unit. <xref ref-type="fig" rid="pcbi.1005667.g002">Fig 2</xref> shows the basis vectors of three example units in the face submodel. Each unit is visualized as a set of ellipses corresponding to the energy detectors, where their underlying Gabor filters have the indicated center positions (in the visual field coordinates), orientations, and spatial frequencies (inversely proportional to the size of the ellipse). The color of the ellipse indicates the weight value normalized by the maximal weight value. For readability, we show only the ellipses corresponding to the maximal positive (excitatory) weight and the minimal negative (inhibitory) weight at each location. Although this visualization approach may seem a bit too radical, it did not lose much information: we confirmed by visual inspection that the local weight patterns for most units had only one positive peak and one negative peak at each position and frequency and the patterns of orientation integration did not have notable changes across frequencies. In <xref ref-type="fig" rid="pcbi.1005667.g002">Fig 2</xref>, we can see that unit #1 represented a face outline either on the left (excitatory) or on the right (inhibitory); unit #2 represented mainly eyes (excitatory); unit #3 mainly represented a mouth (excitatory) and nose (weakly inhibitory). <xref ref-type="fig" rid="pcbi.1005667.g003">Fig 3</xref> shows the basis vectors of 32 randomly selected units from (A) the face submodel and (B) the object submodel. The representations in these two submodels were qualitatively different: face units represented local facial features (i.e., facial parts like outline, eye, nose, and mouth) and object units represented local object features.</p>
<fig id="pcbi.1005667.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The basis representations of three sample model face units.</title>
<p>Each panel depicts the weighting pattern (basis vector) from a face unit to energy detectors by a set of ellipses, where each ellipse corresponds to the energy detector at the indicated x-y position, orientation, and frequency (inverse of the ellipse size); see the top right legend. The color shows the normalized weight value (color bar). Only the maximum positive and the minimum negative weights are shown at each position for readability.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005667.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The basis representations of (A) 32 example model face units and (B) 32 example model object units.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Selectivity to faces</title>
<p>Next, we show a series of comparisons between the response properties of our model and the experiments conducted by Freiwald et al. [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>] on the region in monkey IT cortex called the face middle patch.</p>
<p>As mentioned above, due to the Bayesian explaining-away effect in the mixture model, model face units exhibited selectivity to face images and object units to object images. We measured the responses of our model units to natural face and object images that were separate from the training images (without explicitly giving class labels). The left panel of <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4A</xref> shows the responses (<inline-formula id="pcbi.1005667.e007"><alternatives><graphic id="pcbi.1005667.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup></mml:math></alternatives></inline-formula> in step 3 of Bayesian inference) of the face units (top) and object units (bottom) to face images, where the images were sorted by the response magnitudes, separately for each unit. The right panel similarly shows the responses of the same units to object images. We can see that the face units were prominently responsive to many face images while indifferent to non-face object images; the object units had the opposite property. To quantify such face selectivity, we calculated the face-selectivity index for each unit, which was defined as the ratio between the difference and the sum of the mean response to faces and the mean response to objects (where the baseline, i.e., the response to a blank image, was subtracted from each response value). <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4D</xref> (blue) shows the distribution of face-selectivity indices for the face units. The result indicates almost no unit with index between −1/3 and 1/3, which is consistent with the experimental data [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 1b].</p>
<fig id="pcbi.1005667.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>(A) The responses of model face units (1–400) and model object units (401–800) to face images (left) and object images (right). The images are sorted by response magnitudes (color bar) for each unit. (B) The responses in the case of removing mixture computation. (C) The distribution of face posterior probabilities for face image inputs and for object image inputs. (D) The distribution of face-selectivity indices for the face units in the case of the mixture model (blue) or the case of the sparse coding model (yellow). The broken lines indicate the values −1/3 and 1/3. (E) The distribution of the number of face images in the top 10 (face or object) images that elicited the largest responses of each face unit.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g004" xlink:type="simple"/>
</fig>
<p>Such vivid selectivities disappeared when the mixture computation was removed. <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4B</xref> shows the analogous responses of the face and object units immediately after performing sparse coding (<inline-formula id="pcbi.1005667.e008"><alternatives><graphic id="pcbi.1005667.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup></mml:math></alternatives></inline-formula> in step 1); the face units became almost equally responsive to object images to face images. Indeed, <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4D</xref> (yellow) shows that the face-selectivity indices of those units became substantially lower by the removal of mixture, with a majority falling between −1/3 and 1/3.</p>
<p>To gain more insight into the underlying computations, see the distributions of face posterior probabilities (<italic>r</italic><sub>1</sub> in step 2) for face and object images in <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4C</xref>: faces and objects were clearly discriminated. In fact, those posterior probabilities modulated the response of each unit representing a part (step 3), which resulted in prominent face selectivity. (Note that the discrimination capability did not automatically arise from step 3 since it actually depended on proper training of both submodels; see the section on “Control simulations.”) Further, <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4E</xref> shows that the images that elicited the largest responses of the face units were mostly faces in the mixture model (blue), whereas it was not the case in the model without mixture (yellow). Thus, even though the face units by themselves could detect accidental features similar to facial parts, the mixture computation ensured that they responded only when the whole input was a face image. In other words, face selectivity can be interpreted as a form of holistic processing in our mixture model.</p>
</sec>
<sec id="sec006">
<title>Tuning to facial features</title>
<p>We next turn our attention to tuning properties to facial features. The experiment by Freiwald et al. [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>] used cartoon face stimuli for which facial features were controlled by 19 feature parameters, each ranging from −5 to +5. The authors recorded responses of a neuron in the face middle patch while presenting a number of cartoon face stimuli whose feature parameters were randomly varied. Then, for each feature parameter, they estimated a tuning curve by taking the average of the responses to the stimuli that had a particular value while varying other parameters (“full variation”). We simulated the same experiment and analysis on our model (see the section on Simulation details in <xref ref-type="sec" rid="sec009">Methods</xref>; see also <xref ref-type="supplementary-material" rid="pcbi.1005667.s003">S3 Fig</xref> for examples of cartoon face images.).</p>
<p>To illustrate tuning to facial features in our model, <xref ref-type="fig" rid="pcbi.1005667.g005">Fig 5</xref> shows the tuning curves of the face units in <xref ref-type="fig" rid="pcbi.1005667.g002">Fig 2</xref> to all 19 feature parameters. Each unit was significantly tuned to one to nine feature parameters (where significance was defined in terms of surrogate data; see <xref ref-type="sec" rid="sec009">Methods</xref>). Some tunings clearly reflected the corresponding parts in the basis representations. Unit#1 was tuned only to the face direction, preferring the left as opposed to the right. Unit#2 mainly showed tuning to eye-related features, in particular, preferring narrower inter-eye distances and larger irises. Unit#3 mainly showed tuning to mouth- and nose-related features, in particular, preferring smily mouths and longer noses.</p>
<fig id="pcbi.1005667.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The tuning curves (red) of the model face units shown in <xref ref-type="fig" rid="pcbi.1005667.g002">Fig 2</xref> to 19 feature parameters of cartoon faces.</title>
<p>The mean (blue) as well as the maximum and minimum (green) of the tuning curves estimated from surrogate data are also shown (see the section on Simulation details in <xref ref-type="sec" rid="sec009">Methods</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g005" xlink:type="simple"/>
</fig>
<p>Even in the whole population, most units were significantly tuned to only a small number of features similarly to the experiment [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]. <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6A</xref> shows the distribution of the numbers of tuned features per unit, which were on average 3.6 and substantially smaller than 19, the total number of features. The face neurons in the monkey face middle patch were also tuned to only a small number of features, i.e., 2.6 on average [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 3c] (replotted in red boxes in <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6A</xref>). <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6B</xref> shows the distribution of the numbers of significantly tuned units per feature. The distribution strongly emphasizes geometrically large parts, i.e., face aspect ratio, face direction, feature assembly height, and inter-eye distance. The shape of the distribution has a good match with the experimental result [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 3d] (replotted in <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6B</xref>), though iris size seems much more represented in the monkey case.</p>
<fig id="pcbi.1005667.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g006</object-id>
<label>Fig 6</label>
<caption>
<title/>
<p>(A) The distribution of the numbers of significantly tuned features per unit, overlaid with a replot of [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 3c]. (B) The distribution of the numbers of significantly tuned units for each feature parameter, overlaid with a replot (red boxes) of [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 3d].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g006" xlink:type="simple"/>
</fig>
<p>A prominent property of the experimentally obtained tuning curves was preference or anti-preference of extreme facial features [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]; our model reproduced this property as well. For example, <xref ref-type="fig" rid="pcbi.1005667.g005">Fig 5</xref> shows that many tuning curves were maximum or minimum at one of the extreme values (−5 or +5). For the entire population, <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7A</xref> shows all significant tuning curves of all face units, sorted by the peak feature values. To quantify this, <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7B</xref> shows the distributions of peak and trough feature values; the extremity preference index (the ratio of the average number of peaks in the extreme values to the number of peaks in the non-extreme values) was 9.1 and the extremity anti-preference index (analogously defined for troughs) was 12.0. These indicate that the tendency of preference or anti-preference of extreme features generally held for the population. This result is in good agreement with the monkey experiment [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>], which also reported distributions of peak and trough values that were biased to the extreme values [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 4a] (the extremity preference indices were 7.0, 5.5, and 7.1, and the extremity anti-preference indices were 12.6, 13.7, and 12.1 for three monkeys; the average distribution is replotted in <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7B</xref>).</p>
<fig id="pcbi.1005667.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g007</object-id>
<label>Fig 7</label>
<caption>
<title/>
<p>(A) All significant tuning curves of all model face units sorted by the peak parameter value. Each tuning curve (row) here was mean-subtracted and divided by the maximum. (B) The distributions of peak parameter values (top) and of trough parameter values (bottom). The overlaid red boxes are replots of [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 4a] averaged over three monkeys. (C) The distribution of minimal values of the significant tuning curves peaked at +5 and the flipped tuning curves peaked at −5, overlaid with a averaged replot of [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 4d]. (D) The average of the tuning curves for each minimal value in (C) (with the same color).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g007" xlink:type="simple"/>
</fig>
<p>In addition, the experimental study even observed monotonic tuning curves [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>], which were also found in our model as in <xref ref-type="fig" rid="pcbi.1005667.g005">Fig 5</xref>. To quantify this for the population, <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7C</xref> shows the distribution of minimal values of the significant tuning curves preferring value +5 pooled together with the tuning curves preferring value −5 that have then been flipped; the distribution has a clear peak at value −5. Further, for each minimal value in <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7C</xref>, the average of the tuning curves (normalized by the maximum response) with that minimal value is given in <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7D</xref>; the averaged tuning curve for minimal value −5 has a monotonic shape. These indicate that tuning curves preferring one extreme value tended to anti-prefer the other extreme value and be monotonic. This result is consistent with the experimental data, which also showed a distribution of minimal values that was peaked at −5 [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 4d] (replotted in <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7C</xref>) and a monotonic averaged tuning curve corresponding to minimal value −5 [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 4d, inset]. We discuss later why the model face units acquired such extremity preferences.</p>
<p>We have explained above the face selectivity property as a form of holistic processing in the mixture model. On the other hand, the experimental study investigated holistic face processing in the IT cortex by using partial face stimuli and inverted face stimuli [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]. To gain insight into these experiments, we also conducted simulations of the same experiments in our model.</p>
<p>To simulate the experiment with partial faces [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>], we estimated two kinds of tuning curves in addition to the one used so far (“full variation”), namely, the responses to full cartoon faces where one feature was varied and the other were fixed to standard ones (“single variation”) and the responses to partial faces where only one feature was presented and varied (“partial face”). (See the section on Simulation details in <xref ref-type="sec" rid="sec009">Methods</xref>). <xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8</xref> compares tuning curves in (A) full variation vs. single variation, (B) full variation vs. partial face, and (C) single variation vs. partial face. Overall, the shapes of the tunings were similar for all three kinds (average correlation 0.94 to 0.95). However, the gain of each tuning function (the slope of the fitted linear function) tended to drop after the removal of most of facial features (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8C</xref>); the average gain ratio was 2.0, which was close to 2.2, the experimentally reported number [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 6c]. This effect was not only because typical face units represented a combination of two features or more, but also because partial faces looked less face-like than full faces: <xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8E</xref> shows lower face posterior probabilities for the partial face condition than the full variation condition. Indeed, such drop was weakened when the mixture computation was removed: the average gain ratio was 1.5 when the same comparison was made for the responses of model face units without the mixture computation, i.e., using only step 1 in Bayesian inference (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8D</xref>). In addition to these, note that the tunings curves in full variation were slightly reduced compared to those in single variation (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8A and 8B</xref>); a similar tendency can be observed in the experimental result [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Fig. 6c]. This reduction in the model was because the face images used in the single variation condition took standard feature values for most parameters and such face images looked more face-like than others (giving slightly larger face posterior probabilities than the full variation condition; <xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8E</xref>).</p>
<fig id="pcbi.1005667.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g008</object-id>
<label>Fig 8</label>
<caption>
<title/>
<p>(A) Full-variation versus single-variation tuning curves. (B) Full-variation versus partial face tuning curves. (C) Single-variation versus partial face tuning curves. (D) Single-variation versus partial face tuning curves in the case of removing mixture computation. (E) The distributions of face posterior probabilities for the full variation, the single variation, the partial face, and the inverted face conditions. (F) The distribution of the numbers of tuned units per feature for inverted faces (left) and the mean correlation coefficient between the tunings for upright faces and for inverted faces for each facial feature (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g008" xlink:type="simple"/>
</fig>
<p>To simulate the experiment with inverted faces [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>], we presented, to the model, the same set of full cartoon faces except for their vertical inversion and estimated tuning curves for each facial feature in the same way (full variation). As a result, we found that the number of units that were tuned to each facial feature was more or less similar to the original model (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8F</xref>, left). However, the tuning curves for assembly height tended to be inverted, whereas those for most other features did not (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8F</xref>, right; for eye eccentricity, only two units had significant tunings and they happened to have a highly negative correlation between the upright and inverted cases). These results were consistent with the experiment [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 7ad]. However, we also observed that the overall responses of the model face units to inverted faces were much lower compared to upright faces (a somewhat similar tendency can be discernible in the experimental report [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 7bc]). This was because the mixture model could not classify well the inverted faces as faces since the face submodel was trained only with upright face images; consequently, the face posterior probabilities were generally low for inverted faces (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8E</xref>, violet). Taken together, our result indicates that feature tuning for inverted faces could be explained by representation of individual parts of upright faces, although whole inverted faces may not be recognized as faces.</p>
<p>Interaction between feature parameters was limited, though present. For each pair of feature parameters, a 2D tuning was estimated by averaging the responses to a pair of parameter values while varying the remaining parameters. Then, the 2D tuning for a pair of parameters was compared to another 2D tuning predicted by the sum of two (full-variation) 1D tunings for the same parameters or by the product of these. The distributions of correlation coefficients are given in <xref ref-type="fig" rid="pcbi.1005667.g009">Fig 9</xref>; the averages were both 0.90, which was similar to the experimental result (averages 0.88 and 0.89) [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Figure 5b].</p>
<fig id="pcbi.1005667.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The distributions of correlation coefficients between 2D tuning functions and additive (blue) or multiplicative predictors (red).</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Control simulations</title>
<p>How much do our results depend on the exact form of model? To address this question, we modified the original model in various ways and conducted the same analysis (Figs <xref ref-type="fig" rid="pcbi.1005667.g010">10</xref>, <xref ref-type="fig" rid="pcbi.1005667.g011">11</xref> and <xref ref-type="fig" rid="pcbi.1005667.g012">12</xref>; <xref ref-type="supplementary-material" rid="pcbi.1005667.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1005667.s002">S2</xref> Figs).</p>
<fig id="pcbi.1005667.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g010</object-id>
<label>Fig 10</label>
<caption>
<title>The distributions of (A) the number of tuned features per unit (cf. <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6A</xref>), (B) the number of tuned units per feature (cf. <xref ref-type="fig" rid="pcbi.1005667.g006">Fig 6B</xref>), and (C) the peak (top) and the trough (bottom) feature values (cf. <xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7B</xref>), in different model variations.</title>
<p>The color of each curve indicates the model variation (see legend).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g010" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005667.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g011</object-id>
<label>Fig 11</label>
<caption>
<title>The distribution of face posterior probabilities for face images (solid curve) or for object images (broken curve) in different model variations (cf. <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4C</xref>).</title>
<p>The color of each curve indicates the model variation (see legend).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g011" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005667.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g012</object-id>
<label>Fig 12</label>
<caption>
<title>The basis representations of 32 example model units from (A) the face submodel and (B) the object submodel, in the network trained with 300 reduced dimensions.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g012" xlink:type="simple"/>
</fig>
<p>First, we already showed that, when we omitted the mixture computation and simply used a sparse coding model of face images, the model units were deprived of selectivities to faces vs. objects (<xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4</xref>). However, tuning properties to facial features did not change much. <xref ref-type="fig" rid="pcbi.1005667.g010">Fig 10</xref> shows that the distributions of the number of tuned features per unit, of the number of tuned units per feature, of the peak feature values, and of the trough feature values for the modified model (cyan curves) are all similar to the original model (blue curves). Therefore, while the selectivities were from the mixture model, the tuning properties were produced by the sparse coding.</p>
<p>Next, we varied the strength of dimension reduction of the outputs of the energy detector bank before performing sparse coding learning (the original model reduced the dimensionality from 2400 to 100). Three observations were made. First, consistently with our previous observation in our V2 model [<xref ref-type="bibr" rid="pcbi.1005667.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref032">32</xref>], overall feature sizes tended to decrease while the reduced dimensionality was increased. <xref ref-type="fig" rid="pcbi.1005667.g012">Fig 12</xref> shows example face and object units in the case of 300 reduced dimensions; compare these with <xref ref-type="fig" rid="pcbi.1005667.g003">Fig 3</xref>. (When we further increased the reduced dimensionality, we obtained quite a few units with globally shaped, somewhat noisy basis representations. These seemed to be a kind of “junk units” that are commonly produced when the amount of data is insufficient compared to the input dimensionality.) Second, as the reduced dimensionality increased, face posterior probabilities (as in <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4C</xref>) were substantially decreased for face images (<xref ref-type="fig" rid="pcbi.1005667.g011">Fig 11</xref>); the face images could barely be discriminated in the case of 300 reduced dimensions. Meanwhile, face posterior probabilities remained low for object images. This seemed to happen because the object submodel now learned to represent spatially very small and generic features so that it could give sufficiently good interpretations not only to object images but also to face images. This justified our model construction approach that performs strong dimension reduction before sparse coding learning. Third, <xref ref-type="fig" rid="pcbi.1005667.g010">Fig 10A and 10B</xref> shows that the number of tuned features per unit and the number of tuned units per feature decreased in the case of 300 reduced dimensions (red curve). This was due to the weakened selectivity rather than the size decrease of feature representations since the effect disappeared when the mixture computation was omitted (yellow curve).</p>
<p>As an additional control simulation, we varied the number of units (200 or 800) in each submodel of the mixture model while keeping the other conditions. In either case, we observed no discernible difference in the results from the original model (<xref ref-type="supplementary-material" rid="pcbi.1005667.s001">S1 Fig</xref>).</p>
<p>We also examined a single sparse coding model (with no mixture model) trained with face and non-face images all together. In this model, we found almost no unit having face selectivity that was as vivid as in the original model; even for the units that gave average responses larger to faces than non-faces (which were only less than 10% of the whole population), selectivity to face images was rather weak, with face-selective indices mostly less than 1/3 (<xref ref-type="supplementary-material" rid="pcbi.1005667.s002">S2A Fig</xref>). However, such weakly face-selective units showed tuning properties similar to the original model (<xref ref-type="supplementary-material" rid="pcbi.1005667.s002">S2B Fig</xref>). Taken together, the response properties of those units were comparable to the sparse coding model trained only with faces without mixture model (Figs <xref ref-type="fig" rid="pcbi.1005667.g004">4B</xref> and <xref ref-type="fig" rid="pcbi.1005667.g010">10</xref>, cyan curves).</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we proposed a novel framework called mixture of sparse coding models and used this to investigate the computational principles underlying face and object processing in the IT cortex. In this model, two sparse feature representations, each specialized to faces or non-face objects, were built on top of an energy detector bank and combined into a mixture model (<xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1</xref>). Evoked responses of units were modeled by a form of Bayesian inference, in which each sparse coding submodel attempts to interpret a given input by its code set, but the best interpretation explains away the input, dismissing the explanation offered by the other submodel. The model units in our face submodel not only exhibited significant selectivity to face images similarly to actual face neurons (<xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4</xref>), but also reproduced qualitatively and quantitatively tuning properties of face neurons to facial features (Figs <xref ref-type="fig" rid="pcbi.1005667.g005">5</xref> to <xref ref-type="fig" rid="pcbi.1005667.g009">9</xref>) as reported for the face middle patch, a particular subregion in the macaque IT cortex [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]. Thus, computation in this cortical region might be somehow related to mixture of sparse coding models.</p>
<p>While sparse coding produced parts-based representations in each submodel (Figs <xref ref-type="fig" rid="pcbi.1005667.g002">2</xref> and <xref ref-type="fig" rid="pcbi.1005667.g003">3</xref>), the mixture model produced an explaining-away effect that led to holistic processing (<xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4E</xref>). This combination was key to simultaneous explanation of two important neural properties: tuning to a small number of facial features and face selectivity. That is, although the former property could be explained by sparse coding alone (<xref ref-type="fig" rid="pcbi.1005667.g010">Fig 10</xref>), the latter could not (<xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4B</xref>) presumably since facial parts could accidentally be similar to object parts. However, when the sparse coding submodels for faces and objects were combined in the mixture model, the individual face units could be activated only if the whole input was interpreted as a face. In this sense, our theory interprets the face selectivity property as a signature of holistic processing. (It should be noted that the face selectivity may not be considered an “emergent” property of the model in the same sense as the tuning properties, since some kind of enhanced selectivity might well be expected by the introduction of a mixture model.) We also linked our model with more classical experiments on holistic processing by reproducing the tuning properties for partial or inverted faces (<xref ref-type="fig" rid="pcbi.1005667.g008">Fig 8</xref>). However, we could not prove the necessity of the mixture computation in these cases since the results without mixture were still consistent, albeit more weakly, with the experimental data.</p>
<p>Having explained known response properties, we can draw a few testable predictions of unknown properties from our theory. First, since face selectivity depends on the computational progress of stimulus interpretation as a face or as an object, we can predict delayed suppression in responses of face-selective neurons to non-face stimuli. Second, since face selectivity depends on the failure of stimulus interpretation as an object, we can predict loss of selectivity of face-selective neurons after deactivation of the object-selective region by muscimol injection or cooling.</p>
<p>Among the reported properties of face neurons in the monkey IT cortex, preferences to extreme features (in particular, monotonic tuning curves) were considered as a surprising property [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>] since they were rather different from more typical bell-like shapes such as orientation and frequency tunings. We showed that our model explained quite well such extremity preferences (<xref ref-type="fig" rid="pcbi.1005667.g007">Fig 7</xref>). It is intriguing why our model face units had such property. First, we would like to point out that the facial features discussed here are mostly related to positions of facial parts and such features can be relatively easily encoded by a linear function of an image. This is not the case, however, for orientations and frequencies since encoding these seem to require a much more complicated nonlinear function, perhaps naturally leading to units with bell-like tunings. Second, we could speculate that the extremity preferences may be really necessary due to the statistical structure of natural face images, irrelevant to any particular details of our model. Indeed, even when we perform a very basic statistical analysis of principal components of face images (so-called eigenfaces, e.g., [<xref ref-type="bibr" rid="pcbi.1005667.ref034">34</xref>]), they look like linear representations of certain facial features, maximal in one extreme and minimal in the other extreme. However, this seems to be a rather deep question and fully answering it is beyond the scope of this study.</p>
<p>The results shown here relied on all computational components in mixture of sparse coding models, including inference computation of each sparse coding submodel and suppressive operations using computed posterior probabilities. Since these computations seem to be difficult to implement only with simple feedforward processing in the biological visual system, a natural assumption would be some kind of recurrent computation possibly involving feedback processing. While quite a few biologically plausible implementations have been proposed for sparse coding inference, e.g., [<xref ref-type="bibr" rid="pcbi.1005667.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref035">35</xref>], we prefer here not to speculate how the mixture computation might be implemented, in particular, whether class information as in the top layer in our model might be represented explicitly in some cortical area or implicitly as some kind of mutual inhibition circuit between the face-selective and the object-selective regions in IT.</p>
<p>Related to the previous point, it would also be interesting whether or not similar results could be reproduced by a deep (feedforward) neural network model [<xref ref-type="bibr" rid="pcbi.1005667.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref012">12</xref>]. Note that, although face-selective units, tuning properties to head orientation, or behavioral properties on holistic face processing (such as the face inversion effect) have been discovered in some models [<xref ref-type="bibr" rid="pcbi.1005667.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref036">36</xref>], no tuning properties to facial features like here have been reported yet. We particularly wonder whether the face-selective units in such models represent facial parts, since such parts are sometimes impossible to recognize correctly without any surrounding context if the input image does not contain enough detail, e.g., <xref ref-type="fig" rid="pcbi.1005667.g001">Fig 1B</xref>. While it is mathematically true that such nonlinear context-dependent computation could also be arbitrarily well approximated by a feedforward model, whether this can be achieved by a network optimized for image classification needs to be investigated empirically. In any case, however, we think that top-down feedback processing as formulated in our model would be a simpler and biologically more natural way of performing such computation.</p>
<p>Since we trained each submodel of our mixture model separately by face or object images, our learning algorithm was supervised, implicitly using class labels (“face” or “object”). This choice was primarily for simplification in the sense of avoiding the generally complicated problem of unsupervised learning of a mixture model. We do not claim by any means that face and object representations in the IT cortex should be learned exactly in this way. Nonetheless, the existence of such teaching signals may not be a totally unreasonable assumption in the actual neural system. In particular, since faces can be detected by a rather simple operation [<xref ref-type="bibr" rid="pcbi.1005667.ref037">37</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref038">38</xref>], some kind of innate mechanism would easily be imaginable. This may also be related to the well-known fact that infant monkeys and humans can recognize faces immediately after eye opening [<xref ref-type="bibr" rid="pcbi.1005667.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref040">40</xref>].</p>
<p>Early work on sparse coding concentrated on explaining receptive field properties of V1 simple cells in terms of local statistics of natural images [<xref ref-type="bibr" rid="pcbi.1005667.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref015">15</xref>], following Barlow’s efficient coding hypothesis [<xref ref-type="bibr" rid="pcbi.1005667.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref042">42</xref>]. The theory was subsequently extended to explain other properties of V1 complex cells [<xref ref-type="bibr" rid="pcbi.1005667.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref019">19</xref>] and V2 cells [<xref ref-type="bibr" rid="pcbi.1005667.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1005667.ref022">22</xref>]. The present study continues this approach to investigate higher visual representations, though a novel finding here is that an additional mechanism, a mixture model, is necessary to explain the neural properties discussed here. On the other hand, in computer vision, sparse-coding-like models have also been used for feature representation learning. In particular, the classical study on ICA of face images [<xref ref-type="bibr" rid="pcbi.1005667.ref034">34</xref>] may be related to the construction of our face sparse coding submodel, although the previous study reported global facial features as the resulting basis set [<xref ref-type="bibr" rid="pcbi.1005667.ref034">34</xref>]. (Because of this, it was once argued that parts-based representations require the non-negativity constraint [<xref ref-type="bibr" rid="pcbi.1005667.ref023">23</xref>]. However, it seems that such completely global ICA features may have been due to some kind of overlearning and, indeed, local feature representations were obtained when we used enough data as in <xref ref-type="fig" rid="pcbi.1005667.g003">Fig 3</xref>; we also confirmed this in the case with raw images.) Another relevant formalism is mixture of ICA models [<xref ref-type="bibr" rid="pcbi.1005667.ref043">43</xref>]. Although the idea is somewhat similar to ours, their full rank assumption on the basis matrix and the lack of Gaussian noise (reconstruction error) terms make it inappropriate in our case because the strong dimension reduction was essential for ensuring the face selectivity (<xref ref-type="fig" rid="pcbi.1005667.g011">Fig 11</xref>).</p>
<p>Our model presented here is not meant to explain all the properties of face neurons. Indeed, the properties explained here are a part of known properties of face neurons in the middle patch, which is in turn a part of the face network in the monkey IT cortex [<xref ref-type="bibr" rid="pcbi.1005667.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1005667.ref045">45</xref>]. In the middle patch, face neurons are also tuned to contrast polarities between facial parts [<xref ref-type="bibr" rid="pcbi.1005667.ref046">46</xref>]. In more anterior patches, face neurons are tuned to viewing angles in a mirror-symmetric manner or invariant to viewing angles but selective to identities [<xref ref-type="bibr" rid="pcbi.1005667.ref047">47</xref>]. Further, all these neurons are invariant to shift and size transformation as usual for IT neurons [<xref ref-type="bibr" rid="pcbi.1005667.ref047">47</xref>]. Explaining any of these properties seems to require a substantial extension of our current model and is thus left for future research. Finally, since most detailed and reliable experimental data on the IT cortex concerns face processing, we hope that the principles, such as presented here, found in face processing could serve to elucidate principles of general visual object processing.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec010">
<title>Model details</title>
<p>Our hierarchical model began with a bank of Gabor filters. The filters had all combinations of 10 × 10 center locations (arranged in a square grid within 64 × 64 pixels), 8 orientations (at 22.5° interval), 3 frequencies (0.25, 0.17, and 0.13 cycles/pixels), and 2 phases (0° and 90°). The Euclidean norm of each Gabor filter with frequency <italic>f</italic> was set to <italic>f</italic> <sup>1.15</sup> (following 1/<italic>f</italic> spectrum of natural images) and the Gaussian width and length were both set to 0.4/<italic>f</italic>.</p>
</sec>
<sec id="sec011">
<title>Data preprocessing</title>
<p>As a face image dataset, we used a version of Labeled Faces in Wild (LFW) [<xref ref-type="bibr" rid="pcbi.1005667.ref028">28</xref>] where face alignment was already performed using an algorithm called “deep funneling” [<xref ref-type="bibr" rid="pcbi.1005667.ref029">29</xref>]. By this alignment, faces had a more or less similar position, size, and (upright) posture across images. The dataset consisted of about 13,000 images in total. Each image was converted to gray scale, cropped to the central square region containing only the facial parts and hairs, and resized to 64 × 64 pixels. Since many images still contained some background, they were further passed to a disk-like filter, which retained the image region within 30 pixels from the center and gradually faded the region away from this circular area. Finally, the pixel values were standardized to zero mean and unit variance per image.</p>
<p>As an object image dataset, we used Caltech101 [<xref ref-type="bibr" rid="pcbi.1005667.ref024">24</xref>]. We removed four image categories containing human and animal face images (Faces, Faces_easy, Cougar_face, and Dalmetian). The objects within the images were already aligned. The dataset consisted of about 8,000 images in total. Like face images, each image was converted to gray scale, cropped to square, resized to 64 × 64 pixels, passed to the above mentioned disk-like filter, and standardized per image.</p>
<p>For each class, we reserved 1,000 images for selectivity test and used the rest for model training.</p>
</sec>
<sec id="sec012">
<title>Learning details</title>
<p>To train the mixture model, we first processed the images with the energy detectors and then subtracted, from each data <bold>x</bold>, the dimension along the mean <inline-formula id="pcbi.1005667.e009"><alternatives><graphic id="pcbi.1005667.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> of all (face and object) data:
<disp-formula id="pcbi.1005667.e010"><alternatives><graphic id="pcbi.1005667.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>←</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo mathvariant="bold">⊺</mml:mo></mml:msup> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow> <mml:mrow><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
Although this operation was not quite essential, this had the effect of a linear form of contrast normalization suppressing a part of inputs with prominently strong signals; in fact, we observed that, without this operation, some elements of mean vectors <bold>b</bold><sup><italic>k</italic></sup> estimated as below became outrageously large.</p>
<p>Then, for each submodel for image class <italic>k</italic>, we learned the basis matrix <bold>A</bold><sup><italic>k</italic></sup> and the mean vector <bold>b</bold><sup><italic>k</italic></sup> in the following two steps:</p>
<list list-type="order">
<list-item>
<p>perform strong dimension reduction using PCA [<xref ref-type="bibr" rid="pcbi.1005667.ref032">32</xref>] from 2400 to 100 dimensions while whitening;</p>
</list-item>
<list-item>
<p>apply overcomplete ICA [<xref ref-type="bibr" rid="pcbi.1005667.ref033">33</xref>] to estimate 400 components from 100 dimensions.</p>
</list-item>
</list>
<p>For overcomplete ICA, we used the score matching method for computational efficiency [<xref ref-type="bibr" rid="pcbi.1005667.ref033">33</xref>]. Formally, let <bold>d</bold><sup><italic>k</italic></sup> be the vector of top 100 eigenvalues (from PCA) sorted in descending order, <bold>E</bold><sup><italic>k</italic></sup> be the matrix of the corresponding (row) eigenvectors, and <bold>R</bold><sup><italic>k</italic></sup> be the weight matrix estimated by the overcomplete ICA. Then, using the filter matrix defined as
<disp-formula id="pcbi.1005667.e011"><alternatives><graphic id="pcbi.1005667.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="bold">R</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mtext>diag</mml:mtext> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">d</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mi mathvariant="bold">E</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
the basis matrix can be calculated as <bold>A</bold><sup><italic>k</italic></sup> = (<bold>W</bold><sup><italic>k</italic></sup>)<sup>#</sup> (# is the pseudo inverse) and the mean vector as <inline-formula id="pcbi.1005667.e012"><alternatives><graphic id="pcbi.1005667.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (where <inline-formula id="pcbi.1005667.e013"><alternatives><graphic id="pcbi.1005667.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup></mml:math></alternatives></inline-formula> is the mean of all data of class <italic>k</italic>). Note that the signs of the filter vectors obtained from ICA are arbitrary; for the present purpose, we adjusted each sign so that all elements of <bold>b</bold><sup><italic>k</italic></sup> became non-negative.</p>
</sec>
<sec id="sec013">
<title>Theory of mixture of sparse coding models</title>
<p>A mixture of sparse coding models is similar to a classical mixture of Gaussians [<xref ref-type="bibr" rid="pcbi.1005667.ref030">30</xref>] in that it describes data coming from a fixed number of categories, but different in that each category is defined by a sparse coding model [<xref ref-type="bibr" rid="pcbi.1005667.ref014">14</xref>].</p>
<p>Formally, we assume an observed variable <inline-formula id="pcbi.1005667.e014"><alternatives><graphic id="pcbi.1005667.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, a (discrete) hidden variable <italic>k</italic>: {1, 2, …, <italic>K</italic>}, and <italic>K</italic> hidden variables <inline-formula id="pcbi.1005667.e015"><alternatives><graphic id="pcbi.1005667.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>h</mml:mi></mml:msup> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (<italic>h</italic> = 1, 2, …, <italic>K</italic>). Intuitively, <bold>x</bold> represents a (processed) input image, <italic>k</italic> represents the index of an image class (submodel), and <bold>y</bold><sup><italic>h</italic></sup> represents features (responses) for the class <italic>h</italic>.</p>
<p>We define the generative process of these variables as follows (see <xref ref-type="fig" rid="pcbi.1005667.g013">Fig 13</xref> for the graphical diagram). First, an image class <italic>k</italic> is drawn from a pre-fixed prior <italic>π</italic><sub><italic>h</italic></sub> : [0, 1] (where ∑<sub><italic>h</italic></sub> <italic>π</italic><sub><italic>h</italic></sub> = 1):
<disp-formula id="pcbi.1005667.e016"><alternatives><graphic id="pcbi.1005667.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
We call <italic>k</italic> here the generating class. Next, features <bold>y</bold><sup><italic>k</italic></sup> for the class <italic>k</italic> are drawn from the Laplace distribution with mean vector <inline-formula id="pcbi.1005667.e017"><alternatives><graphic id="pcbi.1005667.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and a pre-fixed standard deviation λ (common for all dimensions)
<disp-formula id="pcbi.1005667.e018"><alternatives><graphic id="pcbi.1005667.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>y</mml:mi> <mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>m</mml:mi> <mml:mi>k</mml:mi></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mo>λ</mml:mo></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
and an observed image <bold>x</bold> is generated from the features <bold>y</bold><sup><italic>k</italic></sup> by transforming it by the basis matrix <inline-formula id="pcbi.1005667.e019"><alternatives><graphic id="pcbi.1005667.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msup><mml:mi mathvariant="bold">A</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>:</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, with a Gaussian noise of a pre-fixed variance <italic>σ</italic><sup>2</sup> added:
<disp-formula id="pcbi.1005667.e020"><alternatives><graphic id="pcbi.1005667.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold">A</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
Here, <bold>A</bold><sup><italic>k</italic></sup> and <bold>b</bold><sup><italic>k</italic></sup> are model parameters estimated from data (see the section on Learning details above). Features <bold>y</bold><sup><italic>h</italic></sup> for each non-generating class <italic>h</italic> ≠ <italic>k</italic> are drawn from the zero-mean Laplacian
<disp-formula id="pcbi.1005667.e021"><alternatives><graphic id="pcbi.1005667.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>h</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>h</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
and never used for generating <bold>x</bold>. Altogether, the model distribution is rewritten as follows:
<disp-formula id="pcbi.1005667.e022"><alternatives><graphic id="pcbi.1005667.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold">A</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:msup><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>[</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>h</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:munder> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>h</mml:mi></mml:msup> <mml:mo>∣</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Since data are generated from the mixture of <italic>K</italic> distributions each of which is a combination of a Laplacian and a Gaussian similar to the classical sparse coding model [<xref ref-type="bibr" rid="pcbi.1005667.ref031">31</xref>], we call the above framework mixture of sparse coding models.</p>
<fig id="pcbi.1005667.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005667.g013</object-id>
<label>Fig 13</label>
<caption>
<title>The graphical diagram for a mixture of sparse coding models.</title>
<p>The variable <italic>k</italic> is first drawn from its prior, then each variable <bold>y</bold><sup><italic>h</italic></sup> is drawn from a Laplace distribution depending on whether <italic>h</italic> = <italic>k</italic> or not, and finally the variable <bold>x</bold> is generated from a Gaussian distribution depending on <bold>y</bold><sup><italic>k</italic></sup>. (Note that, until <italic>k</italic> is determined, <bold>x</bold> is dependent on <italic>k</italic> and all of <bold>y</bold><sup>1</sup>, <bold>y</bold><sup>2</sup>, …, <bold>y</bold><sup><italic>K</italic></sup>.)</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.g013" xlink:type="simple"/>
</fig>
<p>However, we depart from standard formulation of mixture models or sparse coding in two ways, motivated for modeling face neurons. First, since the feature variable <bold>y</bold><sup><italic>h</italic></sup> for the non-generating classes <italic>h</italic> ≠ <italic>k</italic> are unused for generating <bold>x</bold>, a standard formulation would simply drop the factor <xref ref-type="disp-formula" rid="pcbi.1005667.e021">Eq (10)</xref>, leaving <bold>y</bold><sup><italic>h</italic></sup> unconstrained. However, our goal here is to model the responses of all (face or object) neurons for all stimuli (faces or objects). In fact, actual face neurons are normally strongly activated by face stimuli, but are deactivated by non-face stimuli, which is why our model uses a zero mean for non-generating feature variables. Second, the classical sparse coding uses a zero-mean prior [<xref ref-type="bibr" rid="pcbi.1005667.ref031">31</xref>], which is suitable for natural image patch inputs since their mean is zero (blank image) and this evokes no response like V1 neurons. However, the mean of face images is not zero and such mean face image usually elicits non-zero responses of actual face neurons. Therefore our model uses a prior with potentially non-zero mean <bold>b</bold><sup><italic>k</italic></sup> on the feature variable <bold>y</bold><sup><italic>k</italic></sup> for the generating class.</p>
<p>Given an input <bold>x</bold>, how do we infer the hidden variables <bold>y</bold><sup><italic>h</italic></sup>? Since evoked response values of neurons that are experimentally reported are usually the firing rates averaged over trials, we model these quantities as posterior expectations of the hidden variables. Since exact computation of those values would be too slow, we use the following approximation (see the derivation in the section on Approximating posterior later).</p>
<list list-type="order">
<list-item>
<p>For each image class <italic>k</italic>, compute the MAP (maximum a posteriori) estimates of the feature variables <bold>y</bold><sup>1</sup>, <bold>y</bold><sup>2</sup>, …, <bold>y</bold><sup><italic>K</italic></sup>, conditioned on the class <italic>k</italic>:
<disp-formula id="pcbi.1005667.e023"><alternatives><graphic id="pcbi.1005667.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo form="prefix">argmax</mml:mo> <mml:mrow><mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:munder> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula></p>
</list-item>
<list-item>
<p>Compute the approximate posterior probability of each image class <italic>k</italic>:
<disp-formula id="pcbi.1005667.e024"><alternatives><graphic id="pcbi.1005667.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>h</mml:mi></mml:msub> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula></p>
</list-item>
<list-item>
<p>Compute the approximate posterior expectation of each feature variable <italic>k</italic>:
<disp-formula id="pcbi.1005667.e025"><alternatives><graphic id="pcbi.1005667.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>h</mml:mi></mml:munder> <mml:msub><mml:mi>r</mml:mi> <mml:mi>h</mml:mi></mml:msub> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
</list-item>
</list>
<p>Note that, in <xref ref-type="disp-formula" rid="pcbi.1005667.e023">eq (12)</xref>, the feature variables for non-selected classes are always exactly zero:
<disp-formula id="pcbi.1005667.e026"><alternatives><graphic id="pcbi.1005667.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>h</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="1.em"/><mml:mtext>for</mml:mtext> <mml:mspace width="1.em"/><mml:mi>h</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>k</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
Therefore, even though an alternative approach would be to model neural responses by the MAP estimates of feature variable for the best image class, this may be too radical since responses becoming absolutely zero are a little unnatural.</p>
<p>The Bayesian inference described in the section on <xref ref-type="sec" rid="sec003">Model</xref> can be derived from steps 1 to 3 above in a straightforward manner using the model definition <xref ref-type="disp-formula" rid="pcbi.1005667.e022">Eq (11)</xref> and the property <xref ref-type="disp-formula" rid="pcbi.1005667.e026">Eq (15)</xref>.</p>
</sec>
<sec id="sec014">
<title>Approximating posterior</title>
<p>Given an input <italic>x</italic>, we intend to compute the posterior expectations of each <bold>y</bold><sup><italic>h</italic></sup>:
<disp-formula id="pcbi.1005667.e027"><alternatives><graphic id="pcbi.1005667.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">E</mml:mi> <mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:mo>∫</mml:mo> <mml:mo>∫</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>∫</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mi>d</mml:mi> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>⋯</mml:mo> <mml:mi>d</mml:mi> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
Direct computation of this value is not easy. Note, however, that, from the definition of the model (<xref ref-type="disp-formula" rid="pcbi.1005667.e022">eq 11</xref>), the posterior distribution has a single strong peak for each class <italic>k</italic>, with variances more or less similar across all classes. Therefore we approximate the posterior probability by
<disp-formula id="pcbi.1005667.e028"><alternatives><graphic id="pcbi.1005667.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mo>∣</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>K</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <inline-formula id="pcbi.1005667.e029"><alternatives><graphic id="pcbi.1005667.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005667.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>h</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the MAP estimate of <bold>y</bold><sup><italic>h</italic></sup> when the selected image class is <italic>k</italic> (<xref ref-type="disp-formula" rid="pcbi.1005667.e023">eq 12</xref>) and <italic>r</italic><sub><italic>k</italic></sub> is the relative peak posterior probability for the class <italic>k</italic> (<xref ref-type="disp-formula" rid="pcbi.1005667.e024">eq 13</xref>). Here, <italic>δ</italic>(⋅) is the delta function that takes infinity for the specified input value and zero for other values. Substituting the approximation <xref ref-type="disp-formula" rid="pcbi.1005667.e028">Eq (17)</xref> into <xref ref-type="disp-formula" rid="pcbi.1005667.e027">eq (16)</xref> yields <xref ref-type="disp-formula" rid="pcbi.1005667.e025">eq (14)</xref>.</p>
</sec>
<sec id="sec015">
<title>Simulation details</title>
<p>Cartoon face images were created by using the method described by Freiwald et al. [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>]. Each face image was drawn as a linear combination of 7 facial parts (outline, hair, eye pair, iris pair, eyebrows, nose, and mouth). The facial parts were controlled by 19 feature parameters: (1) face aspect ratio (round to long), (2) face direction (left to right), (3) feature assembly height (up to down), (4) hair length (short to long), (5) hair thickness (thin to thick), (6) eyebrow slant (angry to worried), (7) eyebrow width (short to long), (8) eyebrow height (up to down), (9) inter-eye distance (narrow to wide), (10) eye eccentricity (long to round), (11) eye size (small to large), (12) iris size (small to large), (13) gaze direction (11 <italic>x</italic>-<italic>y</italic> positions), (14) nose base (narrow to wide), (15) nose altitude (short to long), (16) mouth-nose distance (short to long), (17) mouth size (narrow to wide), (18) mouth top (smily to frowny), and (19) mouth bottom (closed to open). Note that the first three parameters globally affected the actual geometry of all the facial parts, while the rest locally determined only the relevant facial part. See <xref ref-type="supplementary-material" rid="pcbi.1005667.s003">S3 Fig</xref> for example images.</p>
<p>Following the method in the same study [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>], we estimated three kinds of tuning curves: (1) full variation, (2) single variation, and (3) partial face. For full variation, a set of 5000 cartoon face images were generated while the 19 parameters were randomly varied. For each unit and each feature parameter, a tuning curve at each feature value was estimated as the average of the unit responses to the cartoon face images for which the feature parameter took that value. The tuning curve was then smoothed by a Gaussian kernel with unit variance. To determine the significance of each tuning curve, 5000 surrogate tuning curves were generated by destroying the correspondences between the stimuli and the responses. Then, a tuning curve was regarded significant if (1) its maximum was at least 25% greater than its minimum and (2) its heterogeneity exceeded 99.9% of those of the surrogates, where the heterogeneity of a tuning curve was defined as the negative entropy when the values in the curve were taken as relative probabilities.</p>
<p>For single variation, a tuning curve for a feature parameter at each value was estimated as the response to a cartoon face image for which the feature parameter took that value and the other were fixed to standard values. The standard parameter values were obtained by a manual adjustment with the stimuli used in the experiment [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>, Suppl. Fig. 1]. For partial face, cartoon face images with only one facial part (hair, outline, eyebrows, eyes, nose, mouth, or irises) were created. Each tuning curve for each feature parameter was obtained similarly to single variation, except that only the relevant facial part was present in the stimulus.</p>
</sec>
</sec>
<sec id="sec016">
<title>Supporting information</title>
<supplementary-material id="pcbi.1005667.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Control simulations varying the number of units.</title>
<p>A mixture model was constructed in the same way as the original one, except that each submodel here had 200 units (upper half) or 800 units (lower half). (A) The responses of model face units and object units to natural face images (left) or natural object images (right), together with the distribution of face-selective indices for the face units (bottom); compare these with <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4A and 4D</xref> (blue). (B) The distributions of the numbers of significantly tuned features (of cartoon faces) per unit (left), of numbers of significantly tuned units for each feature parameter (middle), of peak and trough parameter values (right); compare these with Figs <xref ref-type="fig" rid="pcbi.1005667.g006">6</xref> and <xref ref-type="fig" rid="pcbi.1005667.g007">7B</xref>. Overlaid red boxes are replots of corresponding experimental data [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005667.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Control simulation with a single sparse coding model.</title>
<p>A single sparse coding model with 800 units was constructed on top of the same energy model and trained with an ensemble of face and non-face images. In the resulting model, only 71 units gave larger average responses to face images than non-face images. The response properties of these units are shown. (A) The responses of face and object units to face images (left) or object images (right), with the distribution of face-selective indices for the face units (bottom). No prominent selectivity like in <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4A</xref> can be observed; the result is more similar to <xref ref-type="fig" rid="pcbi.1005667.g004">Fig 4B</xref>. (B) The distributions of the numbers of significantly tuned features per unit (left), of numbers of significantly tuned units for each cartoon face feature parameter (middle), of peak and trough parameter values (right); compare these with Figs <xref ref-type="fig" rid="pcbi.1005667.g006">6</xref> and <xref ref-type="fig" rid="pcbi.1005667.g007">7B</xref> as well as <xref ref-type="fig" rid="pcbi.1005667.g010">Fig 10</xref> (cyan curves). Overlaid red boxes are replots of corresponding experimental data [<xref ref-type="bibr" rid="pcbi.1005667.ref004">4</xref>].</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1005667.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005667.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Random examples of cartoon face images.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005667.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Farah</surname> <given-names>MJ</given-names></name>. <article-title>Parts and wholes in face recognition</article-title>. <source>The Quarterly journal of experimental psychology</source>. <year>1993</year>;<volume>46A</volume>(<issue>2</issue>):<fpage>225</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14640749308401045" xlink:type="simple">10.1080/14640749308401045</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McKone</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Duchaine</surname> <given-names>BC</given-names></name>. <article-title>Can generic expertise explain special processing for faces?</article-title> <source>Trends in cognitive sciences</source>. <year>2007</year> <month>Jan</month>;<volume>11</volume>(<issue>1</issue>):<fpage>8</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.11.002" xlink:type="simple">10.1016/j.tics.2006.11.002</ext-link></comment> <object-id pub-id-type="pmid">17129746</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tsunoda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Yamane</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nishizaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tanifuji</surname> <given-names>M</given-names></name>. <article-title>Complex objects are represented in macaque inferotemporal cortex by the combination of feature columns</article-title>. <source>Nature Neuroscience</source>. <year>2001</year> <month>Aug</month>;<volume>4</volume>(<issue>8</issue>):<fpage>832</fpage>–<lpage>838</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/90547" xlink:type="simple">10.1038/90547</ext-link></comment> <object-id pub-id-type="pmid">11477430</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>MS</given-names></name>. <article-title>A face feature space in the macaque temporal lobe</article-title>. <source>Nature Neuroscience</source>. <year>2009</year> <month>Aug</month>;<volume>12</volume>(<issue>9</issue>):<fpage>1187</fpage>–<lpage>1196</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2363" xlink:type="simple">10.1038/nn.2363</ext-link></comment> <object-id pub-id-type="pmid">19668199</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schiltz</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Rossion</surname> <given-names>B</given-names></name>. <article-title>Faces are represented holistically in the human occipito-temporal cortex</article-title>. <source>NeuroImage</source>. <year>2006</year> <month>Sep</month>;<volume>32</volume>(<issue>3</issue>):<fpage>1385</fpage>–<lpage>1394</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuroimage.2006.05.037" xlink:type="simple">10.1016/j.neuroimage.2006.05.037</ext-link></comment> <object-id pub-id-type="pmid">16870475</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fukushima</surname> <given-names>K</given-names></name>. <article-title>Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological cybernetics</source>. <year>1980</year>;<volume>36</volume>(<issue>4</issue>):<fpage>193</fpage>–<lpage>202</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00344251" xlink:type="simple">10.1007/BF00344251</ext-link></comment> <object-id pub-id-type="pmid">7370364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>. <year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>2324</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/5.726791" xlink:type="simple">10.1109/5.726791</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year> <month>Apr</month>;<volume>104</volume>(<issue>15</issue>):<fpage>6424</fpage>–<lpage>6429</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0700622104" xlink:type="simple">10.1073/pnas.0700622104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref009">
<label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. In: Advances in neural information processing systems; 2012. p. 1097–1105.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref010">
<label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Taigman Y, Yang M, Ranzato MA. Deepface: Closing the gap to human-level performance in face verification. In: The IEEE Conference on Computer Vision and Pattern Recognition; 2014. p. 1701–1708.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year> <month>Jun</month>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Farzmahdi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rajaei</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ghodrati</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ebrahimpour</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name>. <article-title>A specialized face-processing model inspired by the organization of monkey face patches explains several face-specific phenomena observed in humans</article-title>. <source>Scientific Reports</source>. <year>2016</year>;<volume>6</volume>:<fpage>25025</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep25025" xlink:type="simple">10.1038/srep25025</ext-link></comment> <object-id pub-id-type="pmid">27113635</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tan</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Neural Tuning Size in a Model of Primate Visual Processing Accounts for Three Key Markers of Holistic Face Processing</article-title>. <source>PloS one</source>. <year>2016</year> <month>Mar</month>;<volume>11</volume>(<issue>3</issue>):<fpage>e0150980</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0150980" xlink:type="simple">10.1371/journal.pone.0150980</ext-link></comment> <object-id pub-id-type="pmid">26985989</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>(<issue>6583</issue>):<fpage>607</fpage>–<lpage>609</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381607a0" xlink:type="simple">10.1038/381607a0</ext-link></comment> <object-id pub-id-type="pmid">8637596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Hateren</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>van der Schaaf</surname> <given-names>A</given-names></name>. <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source>. <year>1998</year> <month>Mar</month>;<volume>265</volume>(<issue>1394</issue>):<fpage>359</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1998.0303" xlink:type="simple">10.1098/rspb.1998.0303</ext-link></comment> <object-id pub-id-type="pmid">9523437</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rao</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source>. <year>1999</year> <month>Jan</month>;<volume>2</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/4580" xlink:type="simple">10.1038/4580</ext-link></comment> <object-id pub-id-type="pmid">10195184</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hoyer</surname> <given-names>P</given-names></name>. <article-title>Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</article-title>. <source>Neural Computation</source>. <year>2000</year>;<volume>12</volume>(<issue>7</issue>):<fpage>1705</fpage>–<lpage>1720</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300015312" xlink:type="simple">10.1162/089976600300015312</ext-link></comment> <object-id pub-id-type="pmid">10935923</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Karklin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>Emergence of complex cell properties by learning to generalize in natural scenes</article-title>. <source>Nature</source>. <year>2009</year> <month>Jan</month>;<volume>457</volume>(<issue>7225</issue>):<fpage>83</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07481" xlink:type="simple">10.1038/nature07481</ext-link></comment> <object-id pub-id-type="pmid">19020501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwartz</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nature Neuroscience</source>. <year>2001</year>;<volume>4</volume>(<issue>8</issue>):<fpage>819</fpage>–<lpage>825</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/90526" xlink:type="simple">10.1038/90526</ext-link></comment> <object-id pub-id-type="pmid">11477428</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>. <article-title>Multinomial Bayesian learning for modeling classical and nonclassical receptive field properties</article-title>. <source>Neural Computation</source>. <year>2012</year> <month>Aug</month>;<volume>24</volume>(<issue>8</issue>):<fpage>2119</fpage>–<lpage>2150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00310" xlink:type="simple">10.1162/NECO_a_00310</ext-link></comment> <object-id pub-id-type="pmid">22509962</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gutmann</surname> <given-names>MU</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>A three-layer model of natural image statistics</article-title>. <source>Journal of Physiology-Paris</source>. <year>2013</year>;<volume>107</volume>(<issue>5</issue>):<fpage>369</fpage>–<lpage>398</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jphysparis.2013.01.001" xlink:type="simple">10.1016/j.jphysparis.2013.01.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>A Hierarchical Statistical Model of Natural Images Explains Tuning Properties in V2</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year> <month>Jul</month>;<volume>35</volume>(<issue>29</issue>):<fpage>10412</fpage>–<lpage>10428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5152-14.2015" xlink:type="simple">10.1523/JNEUROSCI.5152-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26203137</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>. <year>1999</year> <month>Oct</month>;<volume>401</volume>(<issue>6755</issue>):<fpage>788</fpage>–<lpage>791</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/44565" xlink:type="simple">10.1038/44565</ext-link></comment> <object-id pub-id-type="pmid">10548103</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fergus</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2007</year> <month>Apr</month>;<volume>106</volume>(<issue>1</issue>):<fpage>59</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cviu.2005.09.012" xlink:type="simple">10.1016/j.cviu.2005.09.012</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tootell</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>MS</given-names></name>. <article-title>A cortical region consisting entirely of face-selective cells</article-title>. <source>Science</source>. <year>2006</year>;<volume>311</volume>(<issue>5761</issue>):<fpage>670</fpage>–<lpage>674</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1119983" xlink:type="simple">10.1126/science.1119983</ext-link></comment> <object-id pub-id-type="pmid">16456083</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name>. <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>The Journal of Neuroscience</source>. <year>1997</year> <month>Jun</month>;<volume>17</volume>(<issue>11</issue>):<fpage>4302</fpage>–<lpage>4311</lpage>. <object-id pub-id-type="pmid">9151747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Schein</surname> <given-names>SJ</given-names></name>. <article-title>Visual properties of neurons in area V4 of the macaque: sensitivity to stimulus form</article-title>. <source>Journal of Neurophysiology</source>. <year>1987</year>;<volume>57</volume>(<issue>3</issue>):<fpage>835</fpage>–<lpage>868</lpage>. <object-id pub-id-type="pmid">3559704</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref028">
<label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Ramesh</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Berg</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Learned-Miller</surname> <given-names>E</given-names></name>. <source>Labeled faces in the wild: A database for studying face recognition in unconstrained environments</source>. <publisher-name>University of Massachusetts</publisher-name>, <publisher-loc>Amherst</publisher-loc>; <year>2007</year>. <fpage>07</fpage>–<lpage>49</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Huang G, Mattar M, Lee H. Learning to align from scratch. Advances in neural information processing systems. 2012;p. 764–772.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern recognition and machine learning (information science and statistics)</source>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>DJ</given-names></name>. <article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1?</article-title> <source>Vision Research</source>. <year>1997</year> <month>Dec</month>;<volume>37</volume>(<issue>23</issue>):<fpage>3311</fpage>–<lpage>3325</lpage>. <object-id pub-id-type="pmid">9425546</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hosoya</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>Learning Visual Spatial Pooling by Strong PCA Dimension Reduction</article-title>. <source>Neural Computation</source>. <year>2016</year>;<volume>28</volume>:<fpage>1249</fpage>–<lpage>1263</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00843" xlink:type="simple">10.1162/NECO_a_00843</ext-link></comment> <object-id pub-id-type="pmid">27171856</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>Estimation of Non-Normalized Statistical Models by Score Matching</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2005</year> <month>Apr</month>;<volume>6</volume>:<fpage>695</fpage>–<lpage>709</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bartlett</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Movellan</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Face recognition by independent component analysis</article-title>. <source>IEEE Transactions on Neural Networks</source>. <year>2002</year> <month>Nov</month>;<volume>13</volume>(<issue>6</issue>):<fpage>1450</fpage>–<lpage>1464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNN.2002.804287" xlink:type="simple">10.1109/TNN.2002.804287</ext-link></comment> <object-id pub-id-type="pmid">18244540</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wiltschut</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hamker</surname> <given-names>FH</given-names></name>. <article-title>Efficient coding correlates with spatial frequency tuning in a model of V1 receptive field organization</article-title>. <source>Visual Neuroscience</source>. <year>2009</year>;<volume>26</volume>(<issue>01</issue>):<fpage>157</fpage>–<lpage>157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0952523808080966" xlink:type="simple">10.1017/S0952523808080966</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Yildirim I, Kulkarni TD, Freiwald WA. Efficient and robust analysis-by-synthesis in vision: A computational framework, behavioral tests, and modeling neuronal representations. Annual Conference of the Cognitive Science Society. 2015;.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sinha</surname> <given-names>P</given-names></name>. <article-title>Qualitative representations for recognition</article-title>. <source>Workshop on Biologically Motivated Computer Vision (Lecture Notes in Computer Science)</source>. <year>2002</year>;<volume>2525</volume>:<fpage>249</fpage>–<lpage>262</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Viola</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MJ</given-names></name>. <article-title>Robust real-time face detection</article-title>. <source>International Journal of Computer Vision</source>. <year>2004</year>;<volume>57</volume>(<issue>2</issue>):<fpage>137</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fb" xlink:type="simple">10.1023/B:VISI.0000013087.49260.fb</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sugita</surname> <given-names>Y</given-names></name>. <article-title>Face perception in monkeys reared with no exposure to faces</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2008</year> <month>Jan</month>;<volume>105</volume>(<issue>1</issue>):<fpage>394</fpage>–<lpage>398</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0706079105" xlink:type="simple">10.1073/pnas.0706079105</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morton</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MH</given-names></name>. <article-title>CONSPEC and CONLERN: a two-process theory of infant face recognition</article-title>. <source>Psychological review</source>. <year>1991</year>;<volume>98</volume>(<issue>2</issue>):<fpage>164</fpage>–<lpage>181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-295X.98.2.164" xlink:type="simple">10.1037/0033-295X.98.2.164</ext-link></comment> <object-id pub-id-type="pmid">2047512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref041">
<label>41</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. <chapter-title>Possible principles underlying the transformation of sensory messages</chapter-title>. <source>Sensory communication</source>. <year>1961</year>;p. <fpage>217</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barlow</surname> <given-names>HB</given-names></name>. <article-title>Single units and sensation: a neuron doctrine for perceptual psychology?</article-title> <source>Perception</source>. <year>1972</year>;<volume>1</volume>:<fpage>371</fpage>–<lpage>394</lpage>. <object-id pub-id-type="pmid">4377168</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>. <article-title>ICA mixture models for unsupervised classification of non-Gaussian classes and automatic context switching in blind signal separation</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2000</year>;<volume>22</volume>(<issue>10</issue>):1078–1–90.</mixed-citation>
</ref>
<ref id="pcbi.1005667.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moeller</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Patches with Links: A Unified System for Processing Faces in the Macaque Temporal Lobe</article-title>. <source>Science</source>. <year>2008</year> <month>Jun</month>;<volume>320</volume>(<issue>5881</issue>):<fpage>1355</fpage>–<lpage>1359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1157436" xlink:type="simple">10.1126/science.1157436</ext-link></comment> <object-id pub-id-type="pmid">18535247</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grimaldi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Saleem</surname> <given-names>KS</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>D</given-names></name>. <article-title>Anatomical Connections of the Functionally Defined “Face Patches” in the Macaque Monkey</article-title>. <source>Neuron</source>. <year>2016</year> <month>Jun</month>;<volume>90</volume>(<issue>6</issue>):<fpage>1325</fpage>–<lpage>1342</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2016.05.009" xlink:type="simple">10.1016/j.neuron.2016.05.009</ext-link></comment> <object-id pub-id-type="pmid">27263973</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ohayon</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>What Makes a Cell Face Selective? The Importance of Contrast</article-title>. <source>Neuron</source>. <year>2012</year> <month>May</month>;<volume>74</volume>(<issue>3</issue>):<fpage>567</fpage>–<lpage>581</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.03.024" xlink:type="simple">10.1016/j.neuron.2012.03.024</ext-link></comment> <object-id pub-id-type="pmid">22578507</object-id></mixed-citation>
</ref>
<ref id="pcbi.1005667.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Freiwald</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Tsao</surname> <given-names>DY</given-names></name>. <article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title>. <source>Science</source>. <year>2010</year> <month>Nov</month>;<volume>330</volume>(<issue>6005</issue>):<fpage>845</fpage>–<lpage>851</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1194908" xlink:type="simple">10.1126/science.1194908</ext-link></comment> <object-id pub-id-type="pmid">21051642</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>