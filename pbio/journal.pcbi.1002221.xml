<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00434</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002221</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Cognitive neuroscience</subject>
              <subj-group>
                <subject>Decision making</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Nonlinear dynamics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Cognitive psychology</subject>
              <subj-group>
                <subject>Learning</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>A Neurodynamic Account of Spontaneous Behaviour</article-title><alt-title alt-title-type="running-head">A Neurodynamic Account of Spontaneous Behaviour</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Namikawa</surname>
            <given-names>Jun</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Nishimoto</surname>
            <given-names>Ryunosuke</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Tani</surname>
            <given-names>Jun</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Brain Science Institute, RIKEN, Wako, Japan</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">tani@brain.riken.jp</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JN RN JT. Performed the experiments: RN. Analyzed the data: JN. Wrote the paper: JN JT. Designed the software used in analysis: JN.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>20</day>
        <month>10</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>10</issue><elocation-id>e1002221</elocation-id><history>
        <date date-type="received">
          <day>1</day>
          <month>4</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>8</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Namikawa et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The current article suggests that deterministic chaos self-organized in cortical dynamics could be responsible for the generation of spontaneous action sequences. Recently, various psychological observations have suggested that humans and primates can learn to extract statistical structures hidden in perceptual sequences experienced during active environmental interactions. Although it has been suggested that such statistical structures involve chunking or compositional primitives, their neuronal implementations in brains have not yet been clarified. Therefore, to reconstruct the phenomena, synthetic neuro-robotics experiments were conducted by using a neural network model, which is characterized by a generative model with intentional states and its multiple timescales dynamics. The experimental results showed that the robot successfully learned to imitate tutored behavioral sequence patterns by extracting the underlying transition probability among primitive actions. An analysis revealed that a set of primitive action patterns was embedded in the fast dynamics part, and the chaotic dynamics of spontaneously sequencing these action primitive patterns was structured in the slow dynamics part, provided that the timescale was adequately set for each part. It was also shown that self-organization of this type of functional hierarchy ensured robust action generation by the robot in its interactions with a noisy environment. This article discusses the correspondence of the synthetic experiments with the known hierarchy of the prefrontal cortex, the supplementary motor area, and the primary motor cortex for action generation. We speculate that deterministic dynamical structures organized in the prefrontal cortex could be essential because they can account for the generation of both intentional behaviors of fixed action sequences and spontaneous behaviors of pseudo-stochastic action sequences by the same mechanism.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Various psychological observations have suggested that the spontaneously generated behaviors of humans reflect statistical structures extracted via perceptual learning of everyday practices and experiences while interacting with the world. Although those studies have further suggested that such acquired statistical structures use chunking, which generates a variety of complex actions recognized in compositional manner, the underlying neural mechanism has not been clarified. The current neuro-robotics study presents a model prediction for the mechanism and an evaluation of the model through physically grounded experiments on action imitation learning. The model features learning of a mapping from intentional states to action sequences based on multiple timescales dynamics characteristics. The experimental results suggest that deterministic chaos self-organized in the slower timescale part of the network dynamics is responsible for generating spontaneous transitions among primitive actions by reflecting the extracted statistical structures. The robustness of action generation in a noisy physical environment is preserved. These results agree with other neuroscience evidence of the hierarchical organization in the cortex for voluntary actions. Finally, as presented in a discussion of the results, the deterministic cortical dynamics are presumed crucial in generating not only more intentional fixed action sequences but also less intentional spontaneously transitive action sequences.</p>
      </abstract><funding-group><funding-statement>The authors would like to thank Sony Corporation for providing the humanoid robot as a research platform. The present study was supported in part by a Grant-in-Aid for Scientific Research on Innovative Areas “The study on the neural dynamics for understanding communication in terms of complex hetero systems” from the Japanese Ministry of Education, Culture, Sports, Science and Technology. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Our everyday actions are full of spontaneity. For example, imagine that a man makes a cup of instant coffee every morning. After he pours hot water into his mug, which is already filled with a spoonful of coffee crystals, he may either add sugar first and next add milk, or add milk first and then add sugar. Or, sometimes he may even forget about adding sugar and notice it later when he first tastes the cup of coffee. Some parts of these action sequences are definite, but other parts are varied because we see spontaneity in the action generation. Similar actions can be seen in improvisations in playing jazz or in contemporary dance, where musical phrases or body movement patterns are inspired freely from one to another in an unpredicted manner. An essential question is from where the spontaneity for generating voluntary actions or images originates. The current article presents a model prediction for the underlying neural mechanism.</p>
      <p>We speculate that the necessary neural structures for generating such spontaneous actions are acquired as the results of learning from everyday experiences and practices while interacting with the world. Gibson and Pick <xref ref-type="bibr" rid="pcbi.1002221-Gibson1">[1]</xref> once wrote that infants are active learners who perceptually engage their environments and extract information from them. In their ecological approach, learning an action is not just about learning a motor command sequence. Rather, it involves learning the possible perceptual structures extracted during intentional interactions with the environment. Other researchers <xref ref-type="bibr" rid="pcbi.1002221-Ito1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1002221-Wolpert1">[4]</xref> have proposed that perceptual structures experienced during environmental interactions can be acquired by using forward models that are assumed to be located in the cerebellum. A forward model outputs the prediction of the next sensation by receiving the inputs of the current sensation and motor commands. Although this idea is generic and theoretical in the sense that a forward model can predict the sensory outcomes of arbitrary motor commands at every time step, it is impossible in practice due to the combinatorial explosion problem if the motor dimension becomes large. This problem is analogous to the frame problem <xref ref-type="bibr" rid="pcbi.1002221-McCarthy1">[5]</xref> that discusses that there are no rational means to stop inferences about the outcomes of infinite action possibilities.</p>
      <p>In humans, learning by environmental interaction does not proceed with all possible combinations in motor command sequences, but with more purposeful or intentional stances <xref ref-type="bibr" rid="pcbi.1002221-Gibson1">[1]</xref>. Therefore, it could be sufficient for humans to predict sensory outcomes according only to purposeful behavior trajectories. With regard to this theory, Tani <xref ref-type="bibr" rid="pcbi.1002221-Tani1">[6]</xref> proposed an alternative idea that an agent should learn mapping from a set of particular intentional states with the consequent sensory sequences that are expected to be experienced in corresponding purposeful environmental interactions. Ay et al. <xref ref-type="bibr" rid="pcbi.1002221-Ay1">[7]</xref> also proposed a prediction based model to control an autonomous robot, but the model lacked internal states. In the connectionist implementation by Tani and his colleagues <xref ref-type="bibr" rid="pcbi.1002221-Tani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Tani2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Nishimoto1">[9]</xref>, the proposed neuro-dynamic model is operated in three modes: generation, recognition and learning. In the generation mode, the corresponding visuo-proprioceptive sequence is predicted for a given intentional state. The scheme can generate mental simulation for future behaviors or motor imagery (in terms of visuo-proprioceptive sequences) <xref ref-type="bibr" rid="pcbi.1002221-Jeannerod1">[10]</xref>, and can also generate the corresponding physical movement by sending the next predicted proprioceptive state to the motor controller as the next target. In the recognition mode, a given visuo-proprioceptive sequence can be recognized by identifying the corresponding intentional state through an iterative search to minimize the prediction error. In the learning mode, the learning is formulated as a process to search for the optimal values for both a common synaptic weight matrix as well as a set of individual intentional states that can regenerate all visuo-proprioceptive sequences of an experience for training under minimum error criteria. This idea is formally related to “active inference” <xref ref-type="bibr" rid="pcbi.1002221-Friston1">[11]</xref>, which can be regarded as a form of predictive coding <xref ref-type="bibr" rid="pcbi.1002221-Rao1">[12]</xref>. Friston <xref ref-type="bibr" rid="pcbi.1002221-Friston2">[13]</xref> showed that the three aspects of our neurodynamic model (generation, recognition and learning) can be unified in terms of minimising prediction error.</p>
      <p>Moreover, it is presumed that the neural structures acquired through intentional interactions with the environment should support “compositionality” <xref ref-type="bibr" rid="pcbi.1002221-Evans1">[14]</xref> or chunk knowledge for generating and recognizing the variety of complex actions <xref ref-type="bibr" rid="pcbi.1002221-Arbib1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1002221-Tani3">[17]</xref>. Diverse intentional actions can be generated by combining a set of reusable behavior primitives or chunks adaptively by following the acquired rules. For example, an attempt at drinking a cup of water can be decomposed into multiple behavior primitives, such as reaching for a cup, grasping the cup and moving the cup toward one's mouth. Each behavior primitive can be re-used as a component for other intentional actions, e.g., reaching for a cup to take it away. Psychological observation on infant development as well as adult learning have suggested that chunking structures in perceptual streams can be extracted by statistical learning with sufficient amounts of passive perceptual experiences <xref ref-type="bibr" rid="pcbi.1002221-Saffran1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Kirkham1">[19]</xref> as well as for active behavioral interactions <xref ref-type="bibr" rid="pcbi.1002221-Baldwin1">[20]</xref>. Here, chunking structures are represented by repeatable sensory sequences within chunks and the probabilistic state transition among those chunks. From his observation of skill acquisitions for food processing by mountain gorillas, Byrne <xref ref-type="bibr" rid="pcbi.1002221-Byrne1">[21]</xref> proposed that actions can be acquired with statistical structures through imitation. It is said that juvenile gorillas take a few years to effectively imitate behaviors by observing the mothers' food processing behaviors, which are characterized by nondeterministic transition sequences of behavior primitives or chunks. After the skill acquisition by extracting the underlying statistical structures, the primitive sequences of the juveniles resemble those of their mothers. In another example, improvisers of jazz music make substantial efforts into developing vocabularies of musical patterns or phrases, which they then freely combine and vary in a manner that is sensitive to the on-going musical context <xref ref-type="bibr" rid="pcbi.1002221-Ashley1">[22]</xref>. In recent years, considerable evidence has been assembled in support of statistical learning for both musical pitch sequences <xref ref-type="bibr" rid="pcbi.1002221-Saffran2">[23]</xref> and rhythm <xref ref-type="bibr" rid="pcbi.1002221-Desain1">[24]</xref> by organizing chunking structures.</p>
      <p>In searching for the neuronal mechanisms for chunking, a monkey electrophysiological study <xref ref-type="bibr" rid="pcbi.1002221-Nakamura1">[25]</xref> showed that some neurons in the presupplementary motor area (preSMA) fire only at the beginning of each chunk in the regeneration of trained sequences. Also, a human behavior study <xref ref-type="bibr" rid="pcbi.1002221-Kennerley1">[26]</xref> showed that inactivation of the preSMA by transcranial magnetic stimulation (TMS) affects the performance of regenerating sequences only when the TMS is applied between the chunks after extensive learning of the sequences with chunking structures. These studies suggest that the preSMA might play a crucial role in segmenting sequences into chunks. Sakai et al. <xref ref-type="bibr" rid="pcbi.1002221-Sakai1">[27]</xref> proposed a hierarchical function in a cortical network, in which the prefrontal cortex and the preSMA were responsible for the cognitive control of segmenting sequences and selecting the next chunk, whereas more motor-related areas, including the premotor cortex and the primary motor cortex, are responsible for processing within each chunk. This proposal agrees with the results of a monkey electrophysiological recording <xref ref-type="bibr" rid="pcbi.1002221-Tanji1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Shima1">[29]</xref>, which showed that firing some cells in the preSMA and the SMA encode specific sequences of joystick movement patterns or specific transitions from one movement pattern to another.</p>
      <p>Model studies by the authors have shown that chunking by segmenting continuous sensory flow can be achieved by applying the criteria of prediction error minimization to connectionist models with local <xref ref-type="bibr" rid="pcbi.1002221-Tani3">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Namikawa1">[30]</xref> and distributed <xref ref-type="bibr" rid="pcbi.1002221-Tani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Yamashita1">[31]</xref> representation schemes. Those model studies have also shown that a hierarchy is indispensable in acquiring chunking structures. In the hierarchy, the lower level learns to acquire a set of behavior primitives and the higher level puts those primitives into sequences of intentional actions. The idea of intentional actions is analogous to that by Sakai et al. <xref ref-type="bibr" rid="pcbi.1002221-Sakai2">[32]</xref>.</p>
      <p>One essential question so far is how the next behavior primitives or actions can be decided by following the learned statistical expectation. For example, if we suppose that someone has learned that the next behavior primitive to use is either primitive-A or primitive-B, given an even chance from past experiences, it is plausible to consider that either the primitives can be decided by her/his conscious will or alternatively they can be determined automatically without consciousness. Philosophers have discussed for a long time whether humans have a “free will” to determine a next action arbitrarily, and, if the free will exists, how it is implemented in our minds <xref ref-type="bibr" rid="pcbi.1002221-Hume1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1002221-Dennett1">[35]</xref>. Interestingly, recent neuropsychological studies on free decision <xref ref-type="bibr" rid="pcbi.1002221-Libet1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Soon1">[37]</xref> have suggested that the conscious will of initiating or selecting actions arbitrarily follows certain neural activity that does not accompany awareness. Libet et al. <xref ref-type="bibr" rid="pcbi.1002221-Libet1">[36]</xref> showed that a conscious decision to press a button was preceded for a few hundred milliseconds by a negative brain potential, referred to as the readiness potential, which originates from the SMA. In a functional magnetic resonance imaging (fMRI) study, Soon et al. <xref ref-type="bibr" rid="pcbi.1002221-Soon1">[37]</xref> demonstrated that brain activity is initiated in the prefrontal cortex (PFC) and the parietal cortex up to seven seconds before a conscious decision. It was further found that the observed brain activity can predict the outcome of a motor decision, which the subject did not consciously make, such as pushing the left button by the left index finger or the right button by the right index finger. This result implies the possibility that even though someone can believe that he had consciously decided a particular action among multiple choices, such as primitive-A and primitive-B, the conscious decision was not the direct cause of the action selection, but was the preceded neural activity without awareness. It is also plausible to assume a purely mechanist model in which an itinerant trajectory of neural dynamics <xref ref-type="bibr" rid="pcbi.1002221-Tsuda1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1002221-Friston3">[41]</xref>, instead of a conscious will, determines the next behavior primitive to be performed.</p>
      <p>The current article introduces a neuro-robotics experiment that examines how statistical structures hidden in skilled behaviors can be extracted via imitation learning and how the behaviors can be regenerated by following the statistical structures. The experiment uses a dynamic neural network model based on two essential ideas from our previous proposals. First, in the dynamic neural network model, a mapping from the initial states of the internal neural dynamics to the expected visuo-proprioceptive trajectories is self-organized, and the initial states encode various intentional states for the resultant behavioral trajectories <xref ref-type="bibr" rid="pcbi.1002221-Nishimoto1">[9]</xref>. Second, the model network employs multiple timescale dynamics <xref ref-type="bibr" rid="pcbi.1002221-Yamashita1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Kiebel1">[42]</xref> that allow self-organization of the functional hierarchy, which is analogous to the known cortical hierarchical network consisting of the PFC, the preSMA, and the M1 <xref ref-type="bibr" rid="pcbi.1002221-Badre1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Mita1">[44]</xref> for generating goal-directed actions.</p>
      <p>It is demonstrated that itinerant behaviors with accompanying spontaneous transitions among behavior primitives can be generated by reflecting the observed statistical structure and by assuring the robustness of the behavior primitives against possible noise during physical interactions, when deterministic chaos is self-organized at the higher level of the functional hierarchy (the slow dynamics part of the network). Based on the results, the current article discusses the importance of deterministic neural dynamics in action generation because they account for both itinerant behaviors with accompanying spontaneous transitions of behavior primitives and intentional fixed behaviors (repeatedly executable) by the same dynamic mechanism. The article also discusses how the model prediction presented can be applied in future neurophysiological studies.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Model Overview</title>
        <p>The proposed hierarchical dynamic neural network can be regarded as a generative model of visuo-proprioceptive inputs <xref ref-type="bibr" rid="pcbi.1002221-Tani1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Friston1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Yamashita1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Namikawa2">[45]</xref>. (Precise mathematical descriptions are provided in the <xref ref-type="sec" rid="s4">Methods</xref> section.) The network was divided into three levels based on the value of the time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e001" xlink:type="simple"/></inline-formula>. Time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e002" xlink:type="simple"/></inline-formula> for each unit primarily determined the timescale of the activation dynamics of the unit (see <xref ref-type="fig" rid="pcbi-1002221-g001">Figure 1</xref>). The higher level consisted of slow neural units with a larger time constant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e003" xlink:type="simple"/></inline-formula>), the middle level with a moderate time constant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e004" xlink:type="simple"/></inline-formula>), and the lower level with a small time constant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e005" xlink:type="simple"/></inline-formula>). The lower level was assembled with a set of gated modular networks <xref ref-type="bibr" rid="pcbi.1002221-Jacobs1">[46]</xref> that interacted directly with the visuo-proprioceptive sequences. The higher level was mutually connected to the middle level but was not directly connected to the lower level. The middle level interacted with the lower level by sending the gate opening signals and receiving the sensory inputs.</p>
        <fig id="pcbi-1002221-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Hierarchical neural network consisting of three levels characterized by time constants of the neural activation dynamics.</title>
            <p>The higher-level, middle-level, and lower-level networks consist of neural units in which the activations are characterized by large (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e006" xlink:type="simple"/></inline-formula>), moderate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e007" xlink:type="simple"/></inline-formula>), and small (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e008" xlink:type="simple"/></inline-formula>) time constants, respectively. The visuo-proprioceptive input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e009" xlink:type="simple"/></inline-formula> for each time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e010" xlink:type="simple"/></inline-formula> reaches the middle- and lower-level networks, and the middle level relays the input to the higher level. The lower-level network contains a set of modular networks generating motor commands, which, in turn, are forwarded to the gate. The gate, which prevents undesired motor commands from being released, is controlled by the middle-level network. Since the supplementary motor area (SMA) has been reported to trigger the movement by suppressing the inhibitory signal exerted on the primary motor cortex <xref ref-type="bibr" rid="pcbi.1002221-Ball1">[61]</xref>, the middle-level and lower-level networks may correspond to the SMA and the primary motor cortex, respectively. The higher-level network may correspond to the prefrontal cortex (PFC), which projects to the SMA <xref ref-type="bibr" rid="pcbi.1002221-Selemon1">[62]</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g001" xlink:type="simple"/>
        </fig>
        <p>The parameters of this model were optimized to minimize sensory prediction errors (i.e., maximizing the probability of the predictions given the sensory data). In this sense, our model was concerned with, and only with, perception. Action per se, was a result of movements that conformed to the proprioceptive predictions of the joint angles. This means that perception and action were both trying to minimize prediction errors throughout the hierarchy, where movement minimized the prediction errors at the level of proprioceptive sensations. With this perspective, the high-level network provided predictions of the middle-level network that, in turn, provided predictions about which expert will be engaged for prediction at the lowest (sensory) level. This engagement depended upon the gating variables, which selected the most appropriate expert in the lowest level of the hierarchy.</p>
        <p>The network was trained to predict a set of given visuo-proprioceptive sequences by optimizing the following two types of parameters in order to minimize the prediction error: the synaptic weights and the initial state of the internal units in the whole network for each sequence <xref ref-type="bibr" rid="pcbi.1002221-Nishimoto1">[9]</xref>. This intuitively means that the learning involves determining the dynamic function of the network by changing the synaptic weights and also by inferring the intention or goal of each action sequence. The learning scheme was implemented by using the error back-propagation through time algorithm <xref ref-type="bibr" rid="pcbi.1002221-Rumelhart1">[47]</xref>. Although the biological plausibility of error back-propagation in neuronal circuits is a matter of debate, some supportive evidence <xref ref-type="bibr" rid="pcbi.1002221-Fitzsimonds1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Du1">[49]</xref> and related discussions <xref ref-type="bibr" rid="pcbi.1002221-Harris1">[50]</xref> exist. We speculate that the retrograde axonal signal <xref ref-type="bibr" rid="pcbi.1002221-Harris1">[50]</xref> conveying the error information might propagate from the sensory periphery area to the higher-order cortical area by passing through multiple stages of synapses and neurons for modulating the intentional states.</p>
        <p>After minimizing the error, each visuo-proprioceptive sequence of the training can be regenerated by setting the initial state of the internal units with the optimized value. Because the initial state specified the expected visuo-proprioceptive sequence, the initial states are considered to represent the intentions of generating specific actions. The forward dynamics of the trained network can generate motor imagery in terms of visuo-proprioceptive sequences by feeding back the sensory prediction computed at the previous time step into the current sensory inputs without the accompanying physical movements (closed-loop operation). However, the physical movements can be generated by sending the next time step prediction of the proprioceptive states (joint angles) to the motor controller as the target (open-loop operation).</p>
        <p>As one aspect of our work, we examine how the dynamic function of each level can be differentiated depending on the timescale differences. To study the timescale characteristics in more detail, we investigated cases applying various values of the time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e011" xlink:type="simple"/></inline-formula> set in the higher-level network.</p>
      </sec>
      <sec id="s2b">
        <title>Design of Robot Experiments</title>
        <p>Experiments on imitation learning of actions were conducted on a small humanoid robot platform (Sony Corporation), and the movie of a demonstration is available (<ext-link ext-link-type="uri" xlink:href="http://www.bdc.brain.riken.jp/tani/mov/PLoS11.html" xlink:type="simple">http://www.bdc.brain.riken.jp/tani/mov/PLoS11.html</ext-link>). The robot experiments on the aforementioned dynamic neural network model involved imitative training of the sequences of primitive actions and autonomous generation of those imitated behaviors. The target primitive action sequences to be imitated were designed with a statistical structure and with transitioning of the primitive actions, and the sequences were directly tutored to the robot, i.e., a human assistant directly guided the movements of both hands of the robot by grasping them. In the beginning of each training sequence, the assistant guided both hands of the robot, which was positioned in front of a workbench (see <xref ref-type="fig" rid="pcbi-1002221-g002">Figure 2</xref>). A cubic object was placed on the workbench at one of three positions (center, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e012" xlink:type="simple"/></inline-formula> cm left of center, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e013" xlink:type="simple"/></inline-formula> cm right of center), and the assistance repeated primitive actions of grasping the object, moving it to one of the three positions and releasing it by guiding the hands of the robot while deciding the next object position randomly with equal probability (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e014" xlink:type="simple"/></inline-formula>). Although the hands of the robot were guided by the assistant, the visuo-proprioceptive sequences were recorded for later training. The neural network was trained in an off-line manner, since all training sequences gathered at each teaching session were used for the subsequent consolidation learning. Thus, the neural network learned to predict visuo-proprioceptive sequences on the basis of the experiences obtained during the imitative training session. Note that no explicit cues for segmenting the sequences into primitive actions were prepared. The related chunking structures were acquired via iterative experiences of the continuous visuo-proprioceptive sequences.</p>
        <fig id="pcbi-1002221-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Schematic diagram of the teaching sequence generation process.</title>
            <p>As shown on the left, the assistant selected one action randomly while guiding the hands of the robot to move the cubic object. For each action, the assistant started from a posture in which the hands of the robot were open and then the assistant grasped the object by using the hands of the robot. Whenever the object was moved to another position, the assistant released the object and returned the robot hands to the starting posture for a brief period of time. The graph on the right depicts the trajectory of the center of the object captured by the vision sensor.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g002" xlink:type="simple"/>
        </fig>
        <p>After the imitative training, each training sequence was regenerated by setting the initial state with the optimized value. For the subsequent analysis, primitive action sequences were generated and recorded for far longer than the training sequences. For the detailed analysis on the possible dynamic structures acquired in the network model, the itinerant trajectories by motor imagery for longer steps were also generated.</p>
      </sec>
      <sec id="s2c">
        <title>Training and Action Generation</title>
        <p>The network was trained by a learning scheme in which the higher-level time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e015" xlink:type="simple"/></inline-formula> was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e016" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e017" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e018" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002221-Namikawa2">[45]</xref> (see the <xref ref-type="sec" rid="s4">Methods</xref> section for details). For each condition of the higher-level time constant, training trials were conducted for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e019" xlink:type="simple"/></inline-formula> sample networks with different initial parameters and training data. Descriptions of the learning parameters are provided in the <xref ref-type="sec" rid="s4">Methods</xref> section. The trained networks were tested for pseudo-generation by motor imagery, and it was shown that the networks regenerated primitive action sequences pseudo-stochastically in their deterministic itinerant trajectories. Here, pseudo-stochasticity denotes stochasticity observed through the discretization of deterministic continuous value sensory sequences into symbolically labeled primitive action sequences. The action generation test by motor imagery, also described in the <xref ref-type="sec" rid="s4">Methods</xref> section, revealed that the trained neural networks were able to create novel sequential combinations of the primitive actions that were not contained in all teaching sequences. This implies that primitive actions can be generated pseudo-stochastically, as taught in all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e020" xlink:type="simple"/></inline-formula> conditions.</p>
        <p>We also tested cases of training primitive action sequences having different probabilities of selected primitive actions in a specific object position, as detailed in <xref ref-type="supplementary-material" rid="pcbi.1002221.s001">Text S1</xref>. It was observed that these probabilities were reconstructed in the generation of the primitive action sequences by motor imagery. The results suggest that the network is capable of extracting underlying statistical structures in the imitated primitive action sequences.</p>
        <p>Next, we tested the generation of actual actions of interacting with the physical environment by the robot. First, the behavior of the robot was generated from each acquired initial state with the higher-level time constant, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e021" xlink:type="simple"/></inline-formula> set at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e022" xlink:type="simple"/></inline-formula>. Although it was observed that the trained primitive action sequences can be regenerated exactly during the initial period (7.3 primitive action transitions on average), the sequences gradually became different from the trained ones. It was considered that this result was due to the initial sensitivity characteristics organized in the trained network.</p>
        <p>Then, we tested actual action generations for cases of different values of time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e023" xlink:type="simple"/></inline-formula>. Although no particular difference was found between the cases with a different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e024" xlink:type="simple"/></inline-formula> in the action generation test by the motor imagery mode, the stability in actual action generation in the physical environment was different for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e025" xlink:type="simple"/></inline-formula>. The success rate for moving the object without dropping it was evaluated for each trained network with a different time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e026" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1002221-g003">Figure 3</xref> shows the frequencies of the networks classified according to the success rate, where populations of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e027" xlink:type="simple"/></inline-formula> individual networks were trained for each time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e028" xlink:type="simple"/></inline-formula>. In all cases we found a network with a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e029" xlink:type="simple"/></inline-formula> success rate, but the average success rate was different for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e030" xlink:type="simple"/></inline-formula>. The success rate for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e031" xlink:type="simple"/></inline-formula> was higher than the success rates for the other values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e032" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002221-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Success rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e033" xlink:type="simple"/></inline-formula> of a robot controlled by a neural network to move an object.</title>
            <p>For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e034" xlink:type="simple"/></inline-formula> indicates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e035" xlink:type="simple"/></inline-formula> success of moving the object without dropping it. The graph depicts the frequencies of networks occurring in certain ranges of the success rate. We recorded the behaviors of the robot for up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e036" xlink:type="simple"/></inline-formula> trials of moving the object for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e037" xlink:type="simple"/></inline-formula> sample networks. The average success rates for the higher-level time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e038" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e039" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e040" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e041" xlink:type="simple"/></inline-formula> were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e042" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e043" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e044" xlink:type="simple"/></inline-formula>, respectively.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g003" xlink:type="simple"/>
        </fig>
        <p>This result indicates that motor patterns can be generated stably in the physical environment when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e045" xlink:type="simple"/></inline-formula> is set larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e046" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2d">
        <title>Self-Organized Functional Hierarchy</title>
        <p>As mentioned in the previous section, the stability in actual action generation is dependent on the timescale characteristics. This fact implies that the developed dynamic structure is different for each timescale condition. In the following we discuss the characteristics of the self-organized functional hierarchy in terms of the timescale differences. As an example, <xref ref-type="fig" rid="pcbi-1002221-g004">Figure 4</xref> illustrates the sensory motor sequences and neural states. It can be seen that an action primitive of moving the object to the left, to the right, or to the center consisted of a few different gate openings, which generated sequential switching of the stored reusable motor patterns, such as reaching for the object, picking up the object, and moving back its hand position to the starting posture. It is considered that the middle- and higher-level network dynamics encoded the combinations of these reusable motor patterns into primitive actions and further into their stochastically switching sequences.</p>
        <fig id="pcbi-1002221-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Time series generated by a trained network (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e047" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e048" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e049" xlink:type="simple"/></inline-formula>).</title>
            <p>In the vision case, the two lines correspond to the relative position of the object (red: horizontal, green: vertical). Each label over the vision sensor values denotes the object position (L: left, C: center, and R: right). In proprioception, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e050" xlink:type="simple"/></inline-formula> out of a total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e051" xlink:type="simple"/></inline-formula> dimensions were plotted (red: left arm pronation, green: left shoulder flexion). The lower three figures show the time series described by grayscale plots, where the vertical axis represents the indices of the neural units. A long sideways rectangle thus indicates the activity of a single unit over many time steps.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g004" xlink:type="simple"/>
        </fig>
        <p>The auto-correlation for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e052" xlink:type="simple"/></inline-formula> is shown in <xref ref-type="fig" rid="pcbi-1002221-g005">Figure 5</xref> to clarify the timescale characteristics. The auto-correlation measures the correlation between values at different points in time and is sometimes used to find repeating patterns. A high auto-correlation at time difference <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e053" xlink:type="simple"/></inline-formula> means that similar values appear repeatedly with a period of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e054" xlink:type="simple"/></inline-formula>. We found a periodic pattern of auto-correlation common to both the visuo-proprioception and the middle-level network units in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e055" xlink:type="simple"/></inline-formula> cases. This periodicity is considered to occur because the periodicities of all primitive actions were approximately the same. Conversely, such a periodic pattern was not found in the higher-level network if the higher-level network had a relatively large time constant. The characteristics of auto-correlation seem to correspond to the functionality obtained by each subnetwork.</p>
        <fig id="pcbi-1002221-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Auto-correlation for each neural unit, shown by grayscale plots.</title>
            <p>The vertical axes denote the indices of the neural units, and the horizontal axes denote the length of time.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g005" xlink:type="simple"/>
        </fig>
        <p>To examine the functionality developed at each subnetwork, we investigated the effect of artificial lesions in the higher-level network. To model the artificial lesions, we removed all the neurons in the higher-level network. <xref ref-type="fig" rid="pcbi-1002221-g006">Figure 6</xref> is a comparison of the trajectory of the object position captured by the vision sensor in the motor imagery mode for the normal case and that with the artificial lesions. In this figure, we used networks having a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e056" xlink:type="simple"/></inline-formula> success rate in actual action generation. If the trajectory generated by the network traced the test data (see <xref ref-type="fig" rid="pcbi-1002221-g002">Figure 2</xref>), the network moved the object by correctly using the hands. When the time constant of the higher level was set as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e057" xlink:type="simple"/></inline-formula>, the network was able to generate each single primitive action correctly, even if the higher level was removed. However, in this case, the capability for combining diverse primitive actions significantly deteriorated. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e058" xlink:type="simple"/></inline-formula> set at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e059" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e060" xlink:type="simple"/></inline-formula>, the removal of the higher level significantly affected the generation of each primitive action. This implies that the functions for generating each primitive action and for generating stochastic combinations of these actions were self-organized and became segregated between the higher and the middle/low levels if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e061" xlink:type="simple"/></inline-formula> was set significantly larger than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e063" xlink:type="simple"/></inline-formula>. Otherwise, both functions were self-organized but not segregated. That is, the functions were distributed throughout the entire network. In this situation, a lesion in the higher-level network could affect the lower sensory-motor control level. The abovementioned functional segregation between levels could contribute to the stability in the action performance evaluated in <xref ref-type="fig" rid="pcbi-1002221-g003">Figure 3</xref>.</p>
        <fig id="pcbi-1002221-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Trajectory of the object position captured by the vision sensor in the motor imagery mode.</title>
            <p>Left: normal case, Right: removal of the higher-level network case (lesion case).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g006" xlink:type="simple"/>
        </fig>
        <p>In addition, the maximum Lyapunov exponents for the subnetworks were computed while varying <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e064" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e065" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e066" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e067" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e069" xlink:type="simple"/></inline-formula> were fixed at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e070" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e071" xlink:type="simple"/></inline-formula>, respectively. The computation was repeated for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e072" xlink:type="simple"/></inline-formula> neural networks trained using different initial synaptic weights but under the same learning condition. If the Lyapunov exponent is found to be positive for specific subnetworks or for the entire network, then the intrinsic dynamics of the subnetworks or network are identified as chaotic. In chaotic dynamics, almost any minute change in an internal neural state brings about a drastic change in subsequent network outputs because of the dependence on the initial conditions. Therefore, it can be inferred that subnetworks having positive maximum Lyapunov exponents act to combine action primitives with pseudo-stochasticity. The results are summarized in <xref ref-type="table" rid="pcbi-1002221-t001">Table 1</xref>. The maximum Lyapunov exponent of the entire network was positive for all values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e073" xlink:type="simple"/></inline-formula>. This was expected because the network was able to generate pseudo-stochastic primitive action sequences regardless of the value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e074" xlink:type="simple"/></inline-formula>. However, the values of the maximum Lyapunov exponent were negative in the higher level and the middle level, when the higher-level network had a small time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e075" xlink:type="simple"/></inline-formula>. These results indicate that the function to generate chaos was globally distributed over the entire network. In contrast, when the higher-level network was set with a large time constant, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e076" xlink:type="simple"/></inline-formula>, the maximum Lyapunov exponent of the higher-level network became positive in most cases (94 out of 100 network learning cases), whereas that of the middle-level network became negative. The abovementioned results demonstrate that if the time constant of the higher level is sufficiently larger than the time constants of the other regions, then chaotic dynamics are formed primarily in the higher-level network, separate from the other regions. These results agree with the results in the case having lesions. Furthermore, the abovementioned analysis on the artificial lesion cases indicates that the segregation of chaos from the middle and lower levels provides more stable motor generation. In summary, if we regard our neurodynamic model as a generative model of visuo-proprioceptive sequences, the anatomical segregation between sequential dynamics and motor primitives (e.g., within premotor and motor cortex, respectively) emerges only when we accommodate the separation of temporal scales implicit in the hierarchical composition of those sequences.</p>
        <table-wrap id="pcbi-1002221-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.t001</object-id><label>Table 1</label><caption>
            <title>Maximum Lyapunov exponents of various regions (mean of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e077" xlink:type="simple"/></inline-formula> sample networks).</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002221-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">Time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e078" xlink:type="simple"/></inline-formula> of the higher level</td>
                <td align="left" colspan="1" rowspan="1">Entire network</td>
                <td align="left" colspan="1" rowspan="1">Middle-level network</td>
                <td align="left" colspan="1" rowspan="1">Higher-level network</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e079" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e080" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e081" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e082" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e083" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e084" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e085" xlink:type="simple"/></inline-formula>)</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e086" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e087" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e088" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e089" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e090" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e091" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e092" xlink:type="simple"/></inline-formula>)</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e093" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e094" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e095" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e096" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e097" xlink:type="simple"/></inline-formula>)</td>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e098" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e099" xlink:type="simple"/></inline-formula>)</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>The percentages in parentheses represent the probability that the maximum Lyapunov exponent of each network was positive in multiple training trials.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>Finally, we examined how the generation of chaos that allows pseudo-stochastic transition among action primitives depends on the characteristics of the training sets. For this purpose, the networks with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e100" xlink:type="simple"/></inline-formula> set at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e101" xlink:type="simple"/></inline-formula> were trained by changing the length (i.e., number of transitions of primitive actions) of the training sequences, as detailed in <xref ref-type="supplementary-material" rid="pcbi.1002221.s001">Text S1</xref>. It was observed that the possibility of generating chaos is reduced as the length of the training sequences is reduced. If no chaos was generated, it was observed that neural activity in the higher-level network often converged to a fixed point some steps after the tutored sequences were regenerated. This result implies that the mechanism for spontaneous transition by chaos can be acquired only through training of long sequences that contain statistically enough probabilistic transitions for generalization.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <sec id="s3a">
        <title>Spontaneous Action Generation by Chaos</title>
        <p>The current experimental results revealed that the chaos self-organized in the higher-level network with slow-timescale dynamics facilitates spontaneous transitions among primitive actions by following statistical structures extracted from the set of visuo-proprioceptive sequences imitated, whereas primitive actions were generated in the faster-timescale networks in the lower level. The finding was repeatable in the robotic experiments, provided that the timescale differences were set adequately among the different levels of subnetworks. The results appear to be consistent with human fMRI recordings, which indicate that free-decision-related activity without consciousness is slowly built up in the PFC seconds before the conscious decision <xref ref-type="bibr" rid="pcbi.1002221-Soon1">[37]</xref>. This buildup of activity in the PFC could initiate a sharp response in the SMA just a few hundred milliseconds before the decision <xref ref-type="bibr" rid="pcbi.1002221-Libet1">[36]</xref>. Activation in the SMA leads to immediate motor activity <xref ref-type="bibr" rid="pcbi.1002221-Tanji1">[28]</xref>, and buildup of action-related cell activity in the PFC in the monkey brain takes a few seconds, whereas that in the primary motor area takes only a fraction of a second <xref ref-type="bibr" rid="pcbi.1002221-Hoshi1">[51]</xref>. These observations support recent arguments concerning the possible hierarchy along the rostro-caudal axis of the frontal lobe. Badre and D'Esposito <xref ref-type="bibr" rid="pcbi.1002221-Badre1">[43]</xref> proposed that levels of abstraction might decrease gradually from the prefrontal cortex (PFC) through the premotor cortex (PMC) to the primary motor cortex (M1) along the rostro-caudal axis in the frontal cortex in both the monkey brain and the human brain <xref ref-type="bibr" rid="pcbi.1002221-Mita1">[44]</xref>. Here, the rostral part is considered to be more integral in processing information than the caudal part in terms of its slower timescale dynamics. By considering the possible roles of the M1, such as encoding the posture or direction of limbs <xref ref-type="bibr" rid="pcbi.1002221-Georgopoulos1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Graziano1">[53]</xref>, it is speculated that this hierarchy in the frontal cortex contributes to the predictive coding of proprioceptive sequences through the M1 in one direction, and to that of visual sequences in the other direction via the possible connection between the inferior parietal cortex and the SMA, known as the parieto-frontal circuits <xref ref-type="bibr" rid="pcbi.1002221-Wise1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Rizzolatti1">[55]</xref>.</p>
        <p>Additionally, our experiments showed that the behavior generation of the robot in the real environment becomes substantially unstable when the timescale of the higher-level network is set smaller, that is, when the timescale is similar to the values in the middle-level network. Our analysis in such cases revealed that the two functions of generating primitive motor patterns and sequencing them cannot be segregated in the whole network if the chaos dynamics tend to be distributed. Gros <xref ref-type="bibr" rid="pcbi.1002221-Gros1">[56]</xref>, who referred to higher-level as the reservoir and to the middle/low levels as attractor networks, also discussed the generation and stabilization of transient state dynamics, in terms of attractor ruins. The discussion supports one's opinion that the slower timescale part exhibits robustness of influence of external stimuli. Therefore, it is concluded that the hierarchical timescale differences, which are assumed to be in the human frontal cortex and to be responsible for the generation of voluntary actions, are essential for achieving the two functions of freely combining actions in a compositional manner and generating them stably in a physical environment.</p>
      </sec>
      <sec id="s3b">
        <title>Deterministic versus Stochastic Process</title>
        <p>The uniqueness in the presented model is that deterministic chaos is self-organized in the process of imitating stochastic sequences, provided that sufficient training sequences are utilized to support the generalization in learning (it was observed that the reduction of length in the training sequences can hinder the self-organization of chaos). Therefore, it might be asked why deterministic dynamical system models are considered more crucial than stochastic process models such as the Markov Chain <xref ref-type="bibr" rid="pcbi.1002221-Markov1">[57]</xref> or Langevin equation <xref ref-type="bibr" rid="pcbi.1002221-Langevin1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002221-Grabert1">[59]</xref>. A fundamental reason for focussing on deterministic (as opposed to stochastic) dynamics is that they allow for mean field approximations to neuronal dynamics and motor kinetics. This is important because although individual neuronal dynamics may be stochastic, Fokker Planck formulations and related mean field treatments render the dynamics deterministic again; and these deterministic treatments predominate in the theoretical and modelling literature. Furthermore, we speculate that deterministic neuronal dynamic systems are indispensable for generating both spontaneous behaviors and intentional behaviors under the same dynamic mechanism, especially by applying the initial sensitivity characteristics. When the system is initiated from unspecified initial states of the internal units, the resultant itinerant behavioral trajectories exhibit spontaneous transitions of primitive actions by reflecting the statistical structures extracted through the generalization in learning. However, it is also true that a particular sequence of shorter length can be regenerated by resetting with the corresponding initial state values by the deterministic nature of the model. Our robotics experiments showed that the robot can regenerate trained sequences up to several transitions of primitive actions under a noisy physical environment. In addition, our preliminary experiments suggested that longer primitive action sequences can be stably regenerated if those intentional sequences are trained more frequently than other non-intentional ones. This implies that frequently activated fixed sequences can be remembered by cash memories of their initial states. If the Markov chain model is employed for reconstructing the same feature, the model must handle the dualistic representation, namely the probabilistic state transition graph for generating itinerant behaviors and the deterministic linear sequence for generating each intentional behavior.</p>
        <p>Empirical support for the idea of encoding action sequences by initial states can been found. Tanji and Shima <xref ref-type="bibr" rid="pcbi.1002221-Tanji1">[28]</xref> found that some cells in the SMA and the preSMA fire during the preparatory period immediately before generating specific primitive action sequences in the electrophysiological recording of monkeys. The results can be interpreted such that those cell firings during the preparatory period may represent the initial states that determine which primitive action sequences to be generated subsequently. In the future, if this study can be extended to simultaneous observations of the populations of animal cells during spontaneous action sequence generation by employing the recent developments of multiple-electrode recording techniques, our model prediction can be further evaluated.</p>
      </sec>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Model Implementation</title>
        <p>The evolution of a continuous-time-rate coding model is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e102" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e103" xlink:type="simple"/></inline-formula> is a time constant, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e104" xlink:type="simple"/></inline-formula> is a weight matrix, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e105" xlink:type="simple"/></inline-formula> is a bias vector, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e106" xlink:type="simple"/></inline-formula> is the activation function of a unit (typically the sigmoid function or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e107" xlink:type="simple"/></inline-formula>). When this differential equation is put into the form of an approximate difference equation with step size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e108" xlink:type="simple"/></inline-formula>, we obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e109" xlink:type="simple"/><label>(2)</label></disp-formula>In the present paper, we assume <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e110" xlink:type="simple"/></inline-formula> without the loss of generality.</p>
        <p>As the hierarchical neural network for controlling a humanoid robot, we used a mixture of the recurrent neural network (RNN) experts model <xref ref-type="bibr" rid="pcbi.1002221-Namikawa2">[45]</xref>, in which the gating network is a multiple-timescale RNN <xref ref-type="bibr" rid="pcbi.1002221-Yamashita1">[31]</xref>. The mixture of RNN experts consists of expert networks together with the gating network. All experts receive the same input and have the same number of output units. The gating network receives the previous gate opening values and the input, and then controls the gate opening. The role of each expert is to compute a specific input-output function, and the role of the gating network is to decide which single expert is the winner on each occasion. In the present paper, the set of expert RNNs is referred to as the lower-level network. The middle- and higher-level networks differentiated by time constants are contained in the gating network.</p>
        <p>The dynamic states of the lowest-level neural networks (the mixture of experts) at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e111" xlink:type="simple"/></inline-formula> are updated according to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e112" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e113" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e114" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e115" xlink:type="simple"/></inline-formula> is an input vector representing the current visuo-proprioception, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e116" xlink:type="simple"/></inline-formula> is an output vector representing the predicted visuo-proprioception. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e117" xlink:type="simple"/></inline-formula> denotes a component-wise application of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e118" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e119" xlink:type="simple"/></inline-formula> is the number of experts, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e120" xlink:type="simple"/></inline-formula> is the feedback time delay of the controlled robot (in the experiment <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e121" xlink:type="simple"/></inline-formula>). In the present paper, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e122" xlink:type="simple"/></inline-formula> denotes the concatenation of vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e123" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e124" xlink:type="simple"/></inline-formula>. For each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e125" xlink:type="simple"/></inline-formula>, the terms <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e126" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e127" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e128" xlink:type="simple"/></inline-formula> denote the gate opening value, the internal neural state, and the output state of the expert network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e129" xlink:type="simple"/></inline-formula>, respectively. The gate opening vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e130" xlink:type="simple"/></inline-formula> represents the winner-take-all competition among experts to determine the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e131" xlink:type="simple"/></inline-formula>. The gate opening vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e132" xlink:type="simple"/></inline-formula> is the output of the gating network, defined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e133" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e134" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e135" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e136" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e137" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e138" xlink:type="simple"/></inline-formula> denote the internal neural states of the middle-level and higher-level networks, respectively. To satisfy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e139" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e140" xlink:type="simple"/></inline-formula>, Equation (9) is given by the soft-max function. Using the sigmoid function denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e141" xlink:type="simple"/></inline-formula>, Equation (9) can be expressed as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e142" xlink:type="simple"/><label>(10)</label></disp-formula>This equation indicates that the output of the gating network is given by the sigmoid function with global suppression. Note that the visuo-proprioceptive input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e143" xlink:type="simple"/></inline-formula> enters the state Equations (3) and (6). During imagery, this is replaced by its prediction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e144" xlink:type="simple"/></inline-formula> so that these equations become (in discrete form) nonlinear autoregression equations that embody the learned dynamics. During inference and action, optimising the parameters of these equations can be thought of as making them into generalised Bayesian filters such that the predictions become maximum a posterior predictions under the (formal) priors on the dynamics specified by the form of these equations.</p>
      </sec>
      <sec id="s4b">
        <title>Learning Method</title>
        <p>The procedure for training the hierarchical neural network model is organized into the following two phases:</p>
        <list list-type="order">
          <list-item>
            <p>We first train the experts (lower level) by optimizing their parameters. Crucially, in this phase, the gating variables are treated as unknown parameters and are optimized according to the teaching sensory data.</p>
          </list-item>
          <list-item>
            <p>We then train the gating networks (middle and higher levels) by optimizing their parameters to predict the gating variables optimized in Phase (1).</p>
          </list-item>
        </list>
        <p>The training procedure progresses to phase (2) after the convergence of phase (1). For each phase, the learning involves choosing the best parameter based on the maximum a posteriori estimation. A learning algorithm with a likelihood function and a prior distribution was proposed in <xref ref-type="bibr" rid="pcbi.1002221-Namikawa2">[45]</xref>. In the following, we describe a learning algorithm corresponding to the above description of the hierarchical neural network model.</p>
        <sec id="s4b1">
          <title>Probability distribution</title>
          <p>To define the learning method, we assign a probability distribution to the hierarchical neural network. Here, the adjustable parameters of an expert network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e145" xlink:type="simple"/></inline-formula> and a gating network are denoted as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e146" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e147" xlink:type="simple"/></inline-formula>, respectively. The estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e148" xlink:type="simple"/></inline-formula> of the gate opening vector is given in terms of the variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e149" xlink:type="simple"/></inline-formula>, as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e150" xlink:type="simple"/><label>(11)</label></disp-formula></p>
          <p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e151" xlink:type="simple"/></inline-formula> be an input sequence of visuo-proprioception, and let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e152" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e153" xlink:type="simple"/></inline-formula> be parameters, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e154" xlink:type="simple"/></inline-formula> is a set of parameters given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e155" xlink:type="simple"/></inline-formula>. Given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e156" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e157" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e158" xlink:type="simple"/></inline-formula>, the probability density function (p.d.f.) for the output <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e159" xlink:type="simple"/></inline-formula> is defined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e160" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e161" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e162" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e163" xlink:type="simple"/></inline-formula> is the output dimension, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e164" xlink:type="simple"/></inline-formula> is the output from expert <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e165" xlink:type="simple"/></inline-formula>, as computed by Equations (3), (4), and (5) with the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e166" xlink:type="simple"/></inline-formula>. Thus, the output of the model is governed by a mixture of normal distributions. These equations result from the assumption that the observable sequence data is embedded in additive Gaussian noise. Minimizing the mean square error is equivalent to maximizing the likelihood determined by a normal distribution for learning in a single neural network. Therefore, Equation (13) is a natural extension of neural network learning.</p>
          <p>Given a parameter set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e167" xlink:type="simple"/></inline-formula> and an input sequence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e168" xlink:type="simple"/></inline-formula>, the probability of an output sequence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e169" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e170" xlink:type="simple"/><label>(14)</label></disp-formula>The function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e171" xlink:type="simple"/></inline-formula> of data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e172" xlink:type="simple"/></inline-formula> parameterized by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e173" xlink:type="simple"/></inline-formula> is defined by the product of the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e174" xlink:type="simple"/></inline-formula> and a prior distribution, denoted as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e175" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e176" xlink:type="simple"/></inline-formula> is the p.d.f. of a prior distribution given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e177" xlink:type="simple"/><label>(16)</label></disp-formula>This equation indicates that the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e178" xlink:type="simple"/></inline-formula> is governed by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e179" xlink:type="simple"/></inline-formula>-dimensional Brownian motion. The prior distribution has the effect of suppressing changes of the gate opening values. We can now optimize the unknown parameters in Equation (15) with respect to function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e180" xlink:type="simple"/></inline-formula>. These parameters include the variables underlying the optimum gating, the connection strengths of the lower level, and the parameters controlling the variance of sensory noise. We next consider Phase 2, in which the parameters of the gating network are optimized.</p>
          <p>Assume that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e181" xlink:type="simple"/></inline-formula> is given. We define the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e182" xlink:type="simple"/></inline-formula> by the multinomial distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e183" xlink:type="simple"/><label>(17)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e184" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e185" xlink:type="simple"/></inline-formula> is the normalization constant, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e186" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e187" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e188" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e189" xlink:type="simple"/></inline-formula> in Equation (18) is calculated by the teacher forcing technique <xref ref-type="bibr" rid="pcbi.1002221-Williams1">[60]</xref>, i.e., using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e190" xlink:type="simple"/></inline-formula> instead of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e191" xlink:type="simple"/></inline-formula> in Equation (6). In addition, because of the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e192" xlink:type="simple"/></inline-formula> on the right-hand side of Equation (6), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e193" xlink:type="simple"/></inline-formula> cannot be computed if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e194" xlink:type="simple"/></inline-formula>. Then, we assume that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e195" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e196" xlink:type="simple"/></inline-formula>. If we consider <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e197" xlink:type="simple"/></inline-formula> to be the probability of choosing an expert <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e198" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e199" xlink:type="simple"/></inline-formula>, maximizing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e200" xlink:type="simple"/></inline-formula> is equivalent to minimizing the Kullback-Leibler divergence<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e201" xlink:type="simple"/><label>(19)</label></disp-formula></p>
        </sec>
        <sec id="s4b2">
          <title>Maximum a posteriori estimation</title>
          <p>The learning method chooses the best parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e202" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e203" xlink:type="simple"/></inline-formula> by maximizing (or integrating over) the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e204" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e205" xlink:type="simple"/></inline-formula>. More precisely, we use the gradient descent method with a momentum term as the training procedure. The model parameters at learning step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e206" xlink:type="simple"/></inline-formula> are updated according to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e207" xlink:type="simple"/><label>(20)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e208" xlink:type="simple"/><label>(21)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e209" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e210" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e211" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e212" xlink:type="simple"/></inline-formula> is the learning rate, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e213" xlink:type="simple"/></inline-formula> is the momentum term parameter.</p>
          <p>Although we have explained only the case in which the training data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e214" xlink:type="simple"/></inline-formula> is a single sequence, the proposed method can easily be extended to the learning of several sequences by using the sum of the gradients for each sequence. When several sequences are used as training data, initial states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e215" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e216" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e217" xlink:type="simple"/></inline-formula> and an estimate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e218" xlink:type="simple"/></inline-formula> of the gate opening vector must be provided for each sequence.</p>
        </sec>
        <sec id="s4b3">
          <title>Acceleration of gating network learning</title>
          <p>In principle, the present training procedure is defined by Equations (21) and (20). However, the learning for a gating network sometimes becomes unstable, and so the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e219" xlink:type="simple"/></inline-formula> often decreases when updating the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e220" xlink:type="simple"/></inline-formula> with certain learning rates <xref ref-type="bibr" rid="pcbi.1002221-Namikawa2">[45]</xref>. Of course, if we use sufficiently small learning rates, the likelihood does not decrease for any learning step. However, considerable computational time is required. Hence, to practically accelerate gating network learning, we update the learning rate adaptively by the following algorithm.</p>
          <p>1. For each learning step, updated parameters are computed by Equations (21) and (20) by applying the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e221" xlink:type="simple"/></inline-formula>, and the rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e222" xlink:type="simple"/></inline-formula> defined by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e223" xlink:type="simple"/><label>(22)</label></disp-formula>is also computed, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e224" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e225" xlink:type="simple"/></inline-formula> are sequences of the gate opening values corresponding to the current parameters and the updated parameters, respectively.</p>
          <p>2. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e226" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e227" xlink:type="simple"/></inline-formula> is replaced with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e228" xlink:type="simple"/></inline-formula>, and return to (1). Otherwise, go to (3).</p>
          <p>3. If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e229" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e230" xlink:type="simple"/></inline-formula> is replaced with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e231" xlink:type="simple"/></inline-formula>. Go to the next learning step.</p>
          <p>In the present study, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e232" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e233" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e234" xlink:type="simple"/></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="s4c">
        <title>Parameter Settings for Training</title>
        <p>In the training processes, we used <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e235" xlink:type="simple"/></inline-formula> training sequences, including <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e236" xlink:type="simple"/></inline-formula> action primitives of moving an object to the left, to the center, and to the right. The time constants for the lower-level and middle-level networks were set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e237" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e238" xlink:type="simple"/></inline-formula>, respectively. The time constant for the higher level was chosen to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e239" xlink:type="simple"/></inline-formula>. The number of experts was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e240" xlink:type="simple"/></inline-formula>, and the number of internal units for each expert was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e241" xlink:type="simple"/></inline-formula>. There were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e242" xlink:type="simple"/></inline-formula> internal units for the middle-level network and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e243" xlink:type="simple"/></inline-formula> for the higher-level network, i.e., the total number of internal units in the gating network was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e244" xlink:type="simple"/></inline-formula>. Each element of the matrices and biases of either an expert or a gating network was initialized by random selection from the uniform distribution on the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e245" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e246" xlink:type="simple"/></inline-formula> is the number of internal units. The initial states for the experts and the gating network were also initialized randomly from the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e247" xlink:type="simple"/></inline-formula>. The parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e248" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e249" xlink:type="simple"/></inline-formula> were initialized such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e250" xlink:type="simple"/></inline-formula> for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e251" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e252" xlink:type="simple"/></inline-formula>, respectively. Since the maximum value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e253" xlink:type="simple"/></inline-formula> depends on the total length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e254" xlink:type="simple"/></inline-formula> of the training sequences and the dimension <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e255" xlink:type="simple"/></inline-formula> of the output units, the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e256" xlink:type="simple"/></inline-formula> was scaled by a parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e257" xlink:type="simple"/></inline-formula> that satisfies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e258" xlink:type="simple"/></inline-formula>. The parameter settings were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e259" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e260" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e261" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e262" xlink:type="simple"/></inline-formula>.</p>
        <p>For each training trial, we conducted learning for the experts up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e263" xlink:type="simple"/></inline-formula> steps and learning for the gating network up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e264" xlink:type="simple"/></inline-formula> steps. We performed training of the experts for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e265" xlink:type="simple"/></inline-formula> samples having different initial parameters and training data. In addition, for each set of trained experts, we performed training of the gating network while varying the higher-level time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e266" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e267" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e268" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e269" xlink:type="simple"/></inline-formula>. As a result, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e270" xlink:type="simple"/></inline-formula> samples of a mixture of RNN experts were provided for each condition of the higher-level time constant.</p>
      </sec>
      <sec id="s4d">
        <title>Robotic Platform</title>
        <p>The behaviors of the robot were described by a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e271" xlink:type="simple"/></inline-formula>-dimensional time series, which consists of proprioception (an eight-dimensional vector representing the angles of the arm joints) and vision sense (a two-dimensional vector representing the object position). On the basis of the visuo-proprioception, the neural network generated predictions of the proprioception and vision sense for the next time step. This prediction of the proprioception was sent to the robot in the form of target joint angles, which acted as motor commands for the robot to generate movements and interact with the physical environment. This process, in which values for the motor torque were computed from the desired state, was considered at a computational level to correspond to the inverse model. This inverse computation process was preprogrammed in the robot control system. Changes in the environment, including changes in the object position and changes in the actual positions of the joints, were sent back to the system as sensory feedback.</p>
      </sec>
      <sec id="s4e">
        <title>Action Generation Test</title>
        <p>We demonstrate how a trained network learns to generate combinations of primitive actions. Let us consider a sequence of symbols labeled according to the object position, e.g., “CLRLRCL<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e272" xlink:type="simple"/></inline-formula>”, where C, L, and R are the center, left, and right positions, respectively. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e273" xlink:type="simple"/></inline-formula> be a block if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e274" xlink:type="simple"/></inline-formula> is a finite sequence of symbols. An <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e275" xlink:type="simple"/></inline-formula>-block is simply a block of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e276" xlink:type="simple"/></inline-formula>. To evaluate the performance in creating novel sequential combinations, we counted the number of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e277" xlink:type="simple"/></inline-formula>-blocks that appeared in the visuo-proprioceptive time series generated by the network. We computed the ratio of the number of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e278" xlink:type="simple"/></inline-formula>-blocks generated by the network to the total number of possible <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e279" xlink:type="simple"/></inline-formula>-blocks (note that the total number of possible <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e280" xlink:type="simple"/></inline-formula>-blocks is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e281" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1002221-g007">Figure 7</xref> shows the ratios averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e282" xlink:type="simple"/></inline-formula> sample networks with higher-level time constants <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e283" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e284" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e285" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e286" xlink:type="simple"/></inline-formula> in the motor imagery mode. Note that the teaching sequences do not include all acceptable combinations, because the teaching data are finite.</p>
        <fig id="pcbi-1002221-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002221.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Performance for creating novel sequential combinations of primitive actions.</title>
            <p>This graph shows the ratio of the number of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e287" xlink:type="simple"/></inline-formula>-blocks generated by the network to the total number of possible <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e288" xlink:type="simple"/></inline-formula>-blocks. The total number of acceptable combinations (of length <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e289" xlink:type="simple"/></inline-formula> of the primitive action series) was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e290" xlink:type="simple"/></inline-formula>. To evaluate the ratio, the data for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e291" xlink:type="simple"/></inline-formula> sample networks were averaged for each condition of the higher-level time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e292" xlink:type="simple"/></inline-formula> in a numerical simulation.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.g007" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4f">
        <title>Maximum Lyapunov Exponent</title>
        <p>The maximum Lyapunov exponent of a dynamic system is a quantity that characterizes the rate of exponential divergence from the perturbed initial conditions. Consider two points, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e293" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e294" xlink:type="simple"/></inline-formula>, in a state space, each of which generates an orbit in the space by the dynamic system. The maximum Lyapunov exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e295" xlink:type="simple"/></inline-formula> can be defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e296" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e297" xlink:type="simple"/></inline-formula> is the initial separation vector of two trajectories, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e298" xlink:type="simple"/></inline-formula> is the separation vector at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e299" xlink:type="simple"/></inline-formula>.</p>
        <p>To evaluate the maximum Lyapunov exponent for each neural network, we computed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e300" xlink:type="simple"/></inline-formula> sample sequences of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002221.e301" xlink:type="simple"/></inline-formula> time steps with a random initial state and an initial separation vector by a numerical simulation. In the numerical simulation, a network received predictions of the visuo-proprioception generated by the network itself as input for the next step. When the maximum Lyapunov exponent of a middle- or higher-level network was measured, we computed the dynamics of the entire network, but evaluated a separation vector containing only the component of a subnetwork as a middle- or higher-level component. This method measures the contribution of the subnetwork to the initial sensitivity of the dynamics. Note that if the subnetwork has a positive Lyapunov exponent, as measured in the abovementioned manner, then the entire network also has a positive Lyapunov exponent.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002221.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002221.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p><bold>Supporting online material for “A Neurodynamic Account of Spontaneous Behaviour.”</bold> This file describes two types of additional experiment, namely “Reconstruction of probability” and “Dependency on training data length”. These experimental results clarify the possible relation between chaos generated and the condition of the statistical learning employed in the model network.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002221-Gibson1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gibson</surname><given-names>E</given-names></name><name name-style="western"><surname>Pick</surname><given-names>A</given-names></name></person-group>             <year>2000</year>             <article-title>An ecological approach to perceptual learning and development.</article-title>             <comment>Oxford University Press, USA</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Ito1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name></person-group>             <year>1970</year>             <article-title>Neurophysiological aspects of the cerebellar motor control system.</article-title>             <source>Int J Neurol</source>             <volume>7</volume>             <fpage>162</fpage>             <lpage>176</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Uno1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Uno</surname><given-names>Y</given-names></name><name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name><name name-style="western"><surname>Suzuki</surname><given-names>R</given-names></name></person-group>             <year>1989</year>             <article-title>Formation and control of optimal trajectory in human multi-joint arm movement.</article-title>             <source>Biol Cybern</source>             <volume>61</volume>             <fpage>89</fpage>             <lpage>101</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Wolpert1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wolpert</surname><given-names>D</given-names></name><name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name></person-group>             <year>1995</year>             <article-title>An internal model for sensorimotor integration.</article-title>             <source>Science</source>             <volume>269</volume>             <fpage>1880</fpage>             <lpage>1882</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-McCarthy1">
        <label>5</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McCarthy</surname><given-names>J</given-names></name></person-group>             <year>1963</year>             <article-title>Situations, actions, and causal laws.</article-title>             <comment>Technical Report AIM-2, Artificial Intelligence Project, Stanford University</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tani1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2003</year>             <article-title>Learning to generate articulated behavior through the bottom-up and the top-down interaction processes.</article-title>             <source>Neural Netw</source>             <volume>16</volume>             <fpage>11</fpage>             <lpage>23</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Ay1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name><name name-style="western"><surname>Bertschinger</surname><given-names>N</given-names></name><name name-style="western"><surname>Der</surname><given-names>R</given-names></name><name name-style="western"><surname>Güttler</surname><given-names>F</given-names></name><name name-style="western"><surname>Olbrich</surname><given-names>E</given-names></name></person-group>             <year>2008</year>             <article-title>Predictive information and explorative behavior of autonomous robots.</article-title>             <source>Eur Phys J B</source>             <volume>63</volume>             <fpage>329</fpage>             <lpage>339</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tani2">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name><name name-style="western"><surname>Sugita</surname><given-names>Y</given-names></name></person-group>             <year>2004</year>             <article-title>Self-organization of distributedly represented multiple behavior schemata in a mirror system: reviews of robot experiments using rnnpb.</article-title>             <source>Neural Netw</source>             <volume>17</volume>             <fpage>1273</fpage>             <lpage>1289</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Nishimoto1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nishimoto</surname><given-names>R</given-names></name><name name-style="western"><surname>Namikawa</surname><given-names>J</given-names></name><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>Learning multiple goal-directed actions through self-organization of a dynamic neural network model: A humanoid robot experiment.</article-title>             <source>Adapt Behav</source>             <volume>16</volume>             <fpage>166</fpage>             <lpage>181</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Jeannerod1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jeannerod</surname><given-names>M</given-names></name></person-group>             <year>1994</year>             <article-title>The representing brain: Neural correlates of motor intention and imagery.</article-title>             <source>Behav Brain Sci</source>             <volume>17</volume>             <fpage>187</fpage>             <lpage>202</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Friston1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name><name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name><name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name></person-group>             <year>2011</year>             <article-title>Action understanding and active inference.</article-title>             <source>Biol Cybern</source>             <fpage>1</fpage>             <lpage>24</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Rao1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>R</given-names></name><name name-style="western"><surname>Ballard</surname><given-names>D</given-names></name></person-group>             <year>1999</year>             <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>79</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Friston2">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>             <year>2010</year>             <article-title>The free-energy principle: a unified brain theory?</article-title>             <source>Nat Rev Neurosci</source>             <volume>11</volume>             <fpage>127</fpage>             <lpage>138</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Evans1">
        <label>14</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Evans</surname><given-names>G</given-names></name></person-group>             <year>1981</year>             <article-title>Semantic theory and tacit knowledge.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Holdzman</surname><given-names>S</given-names></name><name name-style="western"><surname>Leich</surname><given-names>C</given-names></name></person-group>             <source>Wittgenstein: To follow a rule</source>             <volume>118</volume>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Routledge and Kegan Paul</publisher-name>             <fpage>118</fpage>             <lpage>137</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Arbib1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arbib</surname><given-names>M</given-names></name></person-group>             <year>1981</year>             <article-title>Perceptual structures and distributed motor control.</article-title>             <source>Handbook of Phys: The Nerv Syst II Motor Control</source>             <fpage>1448</fpage>             <lpage>1480</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Sternad1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sternad</surname><given-names>D</given-names></name><name name-style="western"><surname>Schaal</surname><given-names>S</given-names></name></person-group>             <year>1999</year>             <article-title>Segmentation of endpoint trajectories does not imply segmented control.</article-title>             <source>Exp Brain Res</source>             <volume>124</volume>             <fpage>118</fpage>             <lpage>136</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tani3">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name><name name-style="western"><surname>Nolfi</surname><given-names>S</given-names></name></person-group>             <year>1999</year>             <article-title>Learning to perceive the world as articulated: an approach for hierarchical learning in sensory-motor systems.</article-title>             <source>Neural Netw</source>             <volume>12</volume>             <fpage>1131</fpage>             <lpage>1141</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Saffran1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Saffran</surname><given-names>J</given-names></name><name name-style="western"><surname>Aslin</surname><given-names>R</given-names></name><name name-style="western"><surname>Newport</surname><given-names>E</given-names></name></person-group>             <year>1996</year>             <article-title>Statistical learning by 8-month-old infants.</article-title>             <source>Science</source>             <volume>274</volume>             <fpage>1926</fpage>             <lpage>1928</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Kirkham1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kirkham</surname><given-names>N</given-names></name><name name-style="western"><surname>Slemmer</surname><given-names>J</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>S</given-names></name></person-group>             <year>2002</year>             <article-title>Visual statistical learning in infancy: Evidence for a domain general learning mechanism.</article-title>             <source>Cognition</source>             <volume>83</volume>             <fpage>B35</fpage>             <lpage>B42</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Baldwin1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baldwin</surname><given-names>D</given-names></name><name name-style="western"><surname>Andersson</surname><given-names>A</given-names></name><name name-style="western"><surname>Saffran</surname><given-names>J</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>M</given-names></name></person-group>             <year>2008</year>             <article-title>Segmenting dynamic human action via statistical structure.</article-title>             <source>Cognition</source>             <volume>106</volume>             <fpage>1382</fpage>             <lpage>1407</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Byrne1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Byrne</surname><given-names>R</given-names></name></person-group>             <year>2003</year>             <article-title>Imitation as behaviour parsing.</article-title>             <source>Philos T Roy Soc B</source>             <volume>358</volume>             <fpage>529</fpage>             <lpage>536</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Ashley1">
        <label>22</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ashley</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Musical improvisation.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Hallam</surname><given-names>S</given-names></name><name name-style="western"><surname>Cross</surname><given-names>I</given-names></name><name name-style="western"><surname>Thaut</surname><given-names>M</given-names></name></person-group>             <source>Oxford Handbook of Music Psychology</source>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>             <fpage>413</fpage>             <lpage>420</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Saffran2">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Saffran</surname><given-names>J</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>E</given-names></name><name name-style="western"><surname>Aslin</surname><given-names>R</given-names></name><name name-style="western"><surname>Newport</surname><given-names>E</given-names></name></person-group>             <year>1999</year>             <article-title>Statistical learning of tone sequences by human infants and adults.</article-title>             <source>Cognition</source>             <volume>70</volume>             <fpage>27</fpage>             <lpage>52</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Desain1">
        <label>24</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Desain</surname><given-names>P</given-names></name><name name-style="western"><surname>Honing</surname><given-names>H</given-names></name><name name-style="western"><surname>Sadakata</surname><given-names>M</given-names></name></person-group>             <year>2003</year>             <article-title>Predicting rhythm perception from rhythm production and score counts: The bayesian approach.</article-title>             <comment>In: Proceedings of the Society for Music Perception and Cognition 2003 Conference; 18 June 2003; Las Vegas, Nevada. SMPC 2003</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Nakamura1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nakamura</surname><given-names>K</given-names></name><name name-style="western"><surname>Sakai</surname><given-names>K</given-names></name><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name></person-group>             <year>1998</year>             <article-title>Neuronal activity in medial frontal cortex during learning of sequential procedures.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>2671</fpage>             <lpage>2687</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Kennerley1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kennerley</surname><given-names>S</given-names></name><name name-style="western"><surname>Sakai</surname><given-names>K</given-names></name><name name-style="western"><surname>Rushworth</surname><given-names>M</given-names></name></person-group>             <year>2004</year>             <article-title>Organization of action sequences and the role of the pre-sma.</article-title>             <source>J Neurophysiol</source>             <volume>91</volume>             <fpage>978</fpage>             <lpage>993</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Sakai1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sakai</surname><given-names>K</given-names></name><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name><name name-style="western"><surname>Miyauchi</surname><given-names>S</given-names></name><name name-style="western"><surname>Takino</surname><given-names>R</given-names></name><name name-style="western"><surname>Tamada</surname><given-names>T</given-names></name><etal/></person-group>             <year>1999</year>             <article-title>Neural representation of a rhythm depends on its interval ratio.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>10074</fpage>             <lpage>10081</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tanji1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name></person-group>             <year>1994</year>             <article-title>Role for supplementary motor area cells in planning several movements ahead.</article-title>             <source>Nature</source>             <volume>371</volume>             <fpage>413</fpage>             <lpage>416</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Shima1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name><name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name></person-group>             <year>2000</year>             <article-title>Neuronal activity in the supplementary and presupplementary motor areas for temporal organization of multiple movements.</article-title>             <source>J Neurophysiol</source>             <volume>84</volume>             <fpage>2148</fpage>             <lpage>2160</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Namikawa1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Namikawa</surname><given-names>J</given-names></name><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>A model for learning to segment temporal sequences, utilizing a mixture of RNN experts together with adaptive variance.</article-title>             <source>Neural Netw</source>             <volume>21</volume>             <fpage>1466</fpage>             <lpage>1475</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Yamashita1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yamashita</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000220</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Sakai2">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sakai</surname><given-names>K</given-names></name><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name><name name-style="western"><surname>Nakamura</surname><given-names>K</given-names></name></person-group>             <year>2004</year>             <article-title>Emergence of rhythm during motor learning.</article-title>             <source>Trends Cogn Sci</source>             <volume>8</volume>             <fpage>547</fpage>             <lpage>553</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Hume1">
        <label>33</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hume</surname><given-names>D</given-names></name></person-group>             <year>1748/1977</year>             <source>An enquiry concerning human understanding</source>             <publisher-loc>Indianapolis</publisher-loc>             <publisher-name>Hackett Publishing</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-James1">
        <label>34</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>James</surname><given-names>W</given-names></name></person-group>             <year>1956</year>             <source>The Dilemma of Determinism, Reprinted in The Will to Believe</source>             <publisher-loc>Mineola</publisher-loc>             <publisher-name>Dover Publications</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Dennett1">
        <label>35</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dennett</surname><given-names>D</given-names></name></person-group>             <year>1984</year>             <source>Elbow room: The varieties of free will worth wanting</source>             <publisher-name>The MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Libet1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Libet</surname><given-names>B</given-names></name></person-group>             <year>1985</year>             <article-title>Unconscious cerebral initiative and the role of conscious will in voluntary action.</article-title>             <source>Behav Brain Sci</source>             <volume>8</volume>             <fpage>529</fpage>             <lpage>539</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Soon1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Soon</surname><given-names>C</given-names></name><name name-style="western"><surname>Brass</surname><given-names>M</given-names></name><name name-style="western"><surname>Heinze</surname><given-names>H</given-names></name><name name-style="western"><surname>Haynes</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>Unconscious determinants of free decisions in the human brain.</article-title>             <source>Nat Neurosci</source>             <volume>11</volume>             <fpage>543</fpage>             <lpage>545</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tsuda1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name></person-group>             <year>1991</year>             <article-title>Chaotic itinerancy as a dynamical basis of hermeneutics in brain and mind.</article-title>             <source>World Futures</source>             <volume>32</volume>             <fpage>167</fpage>             <lpage>184</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Skarda1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Skarda</surname><given-names>C</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>W</given-names></name></person-group>             <year>1987</year>             <article-title>How brains make chaos in order to make sense of the world.</article-title>             <source>Behav Brain Sci</source>             <volume>10</volume>             <fpage>161</fpage>             <lpage>173</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Tani4">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>1998</year>             <article-title>An interpretation of the ‘self’ from the dynamical systems perspective: a constructivist approach.</article-title>             <source>J Conscious Stud</source>             <volume>5</volume>             <fpage>516</fpage>             <lpage>542</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Friston3">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name></person-group>             <year>2009</year>             <article-title>Attractors in song.</article-title>             <source>New Math Nat Comput</source>             <volume>5</volume>             <fpage>83</fpage>             <lpage>114</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Kiebel1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>             <year>2008</year>             <article-title>A hierarchy of time-scales and the brain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000209</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Badre1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Badre</surname><given-names>D</given-names></name><name name-style="western"><surname>D'Esposito</surname><given-names>M</given-names></name></person-group>             <year>2009</year>             <article-title>Is the rostro-caudal axis of the frontal lobe hierarchical?</article-title>             <source>Nat Rev Neurosci</source>             <volume>10</volume>             <fpage>659</fpage>             <lpage>669</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Mita1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mita</surname><given-names>A</given-names></name><name name-style="western"><surname>Mushiake</surname><given-names>H</given-names></name><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name><name name-style="western"><surname>Matsuzaka</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Interval time coding by neurons in the presupplementary and supplementary motor areas.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>502</fpage>             <lpage>507</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Namikawa2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Namikawa</surname><given-names>J</given-names></name><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2010</year>             <article-title>Learning to imitate stochastic time series in a compositional way by chaos.</article-title>             <source>Neural Netw</source>             <volume>23</volume>             <fpage>625</fpage>             <lpage>638</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Jacobs1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jacobs</surname><given-names>R</given-names></name><name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name><name name-style="western"><surname>Nowlan</surname><given-names>S</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name></person-group>             <year>1991</year>             <article-title>Adaptive mixtures of local experts.</article-title>             <source>Neural Comput</source>             <volume>3</volume>             <fpage>79</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Rumelhart1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rumelhart</surname><given-names>D</given-names></name><name name-style="western"><surname>Hintont</surname><given-names>G</given-names></name><name name-style="western"><surname>Williams</surname><given-names>R</given-names></name></person-group>             <year>1986</year>             <article-title>Learning representations by back-propagating errors.</article-title>             <source>Nature</source>             <volume>323</volume>             <fpage>533</fpage>             <lpage>536</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Fitzsimonds1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fitzsimonds</surname><given-names>R</given-names></name><name name-style="western"><surname>Song</surname><given-names>H</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M</given-names></name></person-group>             <year>1997</year>             <article-title>Propagation of activity-dependent synaptic depression in simple neural networks.</article-title>             <source>Nature</source>             <volume>388</volume>             <fpage>439</fpage>             <lpage>448</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Du1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>J</given-names></name><name name-style="western"><surname>Poo</surname><given-names>M</given-names></name></person-group>             <year>2004</year>             <article-title>Rapid bdnf-induced retrograde synaptic modification in a developing retino- tectal system.</article-title>             <source>Nature</source>             <volume>429</volume>             <fpage>878</fpage>             <lpage>883</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Harris1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Harris</surname><given-names>K</given-names></name></person-group>             <year>2008</year>             <article-title>Stability of the fittest: organizing learning through retroaxonal signals.</article-title>             <source>Trends Neurosci</source>             <volume>31</volume>             <fpage>130</fpage>             <lpage>136</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Hoshi1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hoshi</surname><given-names>E</given-names></name><name name-style="western"><surname>Shima</surname><given-names>K</given-names></name><name name-style="western"><surname>Tanji</surname><given-names>J</given-names></name></person-group>             <year>2000</year>             <article-title>Neuronal activity in the primate prefrontal cortex in the process of motor selection based on two behavioral rules.</article-title>             <source>J Neurophysiol</source>             <volume>83</volume>             <fpage>2355</fpage>             <lpage>2373</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Georgopoulos1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Georgopoulos</surname><given-names>A</given-names></name><name name-style="western"><surname>Kalaska</surname><given-names>J</given-names></name><name name-style="western"><surname>Caminiti</surname><given-names>R</given-names></name><name name-style="western"><surname>Massey</surname><given-names>J</given-names></name></person-group>             <year>1982</year>             <article-title>On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex.</article-title>             <source>J Neurosci</source>             <volume>2</volume>             <fpage>1527</fpage>             <lpage>1537</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Graziano1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Graziano</surname><given-names>M</given-names></name><name name-style="western"><surname>Taylor</surname><given-names>C</given-names></name><name name-style="western"><surname>Moore</surname><given-names>T</given-names></name></person-group>             <year>2002</year>             <article-title>Complex movements evoked by microstimulation of pre- central cortex.</article-title>             <source>Neuron</source>             <volume>34</volume>             <fpage>841</fpage>             <lpage>851</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Wise1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wise</surname><given-names>S</given-names></name><name name-style="western"><surname>Boussaoud</surname><given-names>D</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>P</given-names></name><name name-style="western"><surname>Caminiti</surname><given-names>R</given-names></name></person-group>             <year>1997</year>             <article-title>Premotor and parietal cortex: Corticocortical connectivity and combinatorial computations 1.</article-title>             <source>Annu Rev Neurosci</source>             <volume>20</volume>             <fpage>25</fpage>             <lpage>42</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Rizzolatti1">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rizzolatti</surname><given-names>G</given-names></name><name name-style="western"><surname>Luppino</surname><given-names>G</given-names></name><name name-style="western"><surname>Matelli</surname><given-names>M</given-names></name></person-group>             <year>1998</year>             <article-title>The organization of the cortical motor system: new concepts.</article-title>             <source>Electroen Clin Neuro</source>             <volume>106</volume>             <fpage>283</fpage>             <lpage>296</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Gros1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gros</surname><given-names>C</given-names></name></person-group>             <year>2009</year>             <article-title>Cognitive computation with autonomously active neural networks: an emerging field.</article-title>             <source>Cogn Comput</source>             <volume>1</volume>             <fpage>77</fpage>             <lpage>90</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Markov1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Markov</surname><given-names>A</given-names></name></person-group>             <year>1971</year>             <article-title>Extension of the limit theorems of probability theory to a sum of variables connected in a chain.</article-title>             <source>Dynam Probabilist Syst</source>             <volume>1</volume>             <fpage>552</fpage>             <lpage>577</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Langevin1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Langevin</surname><given-names>P</given-names></name></person-group>             <year>1908</year>             <article-title>On the theory of brownian motion.</article-title>             <source>CR Acad Sci (Paris)</source>             <volume>146</volume>             <fpage>530</fpage>             <lpage>533</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Grabert1">
        <label>59</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Grabert</surname><given-names>H</given-names></name></person-group>             <year>1982</year>             <source>Projection operator techniques in nonequilibrium statistical mechanics</source>             <publisher-name>Springer-Verlag</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Williams1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>R</given-names></name><name name-style="western"><surname>Zipser</surname><given-names>D</given-names></name></person-group>             <year>1989</year>             <article-title>A learning algorithm for continually running fully recurrent neural networks.</article-title>             <source>Neural Comput</source>             <volume>1</volume>             <fpage>270</fpage>             <lpage>280</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Ball1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ball</surname><given-names>T</given-names></name><name name-style="western"><surname>Schreiber</surname><given-names>A</given-names></name><name name-style="western"><surname>Feige</surname><given-names>B</given-names></name><name name-style="western"><surname>Wagner</surname><given-names>M</given-names></name><name name-style="western"><surname>Lücking</surname><given-names>C</given-names></name><etal/></person-group>             <year>1999</year>             <article-title>The role of higher-order motor areas in voluntary movement as revealed by high-resolution EEG and fMRI.</article-title>             <source>Neuroimage</source>             <volume>10</volume>             <fpage>682</fpage>             <lpage>694</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002221-Selemon1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Selemon</surname><given-names>L</given-names></name><name name-style="western"><surname>Goldman-Rakic</surname><given-names>P</given-names></name></person-group>             <year>1988</year>             <article-title>Common cortical and subcortical targets of the dorsolateral prefrontal and posterior parietal cortices in the rhesus monkey: evidence for a distributed neural network subserving spatially guided behavior.</article-title>             <source>J Neurosci</source>             <volume>8</volume>             <fpage>4049</fpage>             <lpage>4068</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>