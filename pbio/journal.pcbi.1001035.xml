<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2386R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001035</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Incremental Mutual Information: A New Method for Characterizing the Strength and Dynamics of Connections in Neuronal Circuits</article-title><alt-title alt-title-type="running-head">Incremental Mutual Information</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Singh</surname><given-names>Abhinav</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lesica</surname><given-names>Nicholas A.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1">          <addr-line>Ear Institute, University College London, London, United Kingdom</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Latham</surname><given-names>Peter E.</given-names></name>
<role>Editor</role>
</contrib>
</contrib-group><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">n.lesica@ucl.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: AS NAL. Performed the experiments: AS NAL. Analyzed the data: AS NAL. Wrote the paper: NAL.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>12</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>9</day><month>12</month><year>2010</year></pub-date><volume>6</volume><issue>12</issue><elocation-id>e1001035</elocation-id><history>
<date date-type="received"><day>17</day><month>6</month><year>2010</year></date>
<date date-type="accepted"><day>12</day><month>11</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Singh, Lesica</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Understanding the computations performed by neuronal circuits requires characterizing the strength and dynamics of the connections between individual neurons. This characterization is typically achieved by measuring the correlation in the activity of two neurons. We have developed a new measure for studying connectivity in neuronal circuits based on information theory, the incremental mutual information (IMI). By conditioning out the temporal dependencies in the responses of individual neurons before measuring the dependency between them, IMI improves on standard correlation-based measures in several important ways: 1) it has the potential to disambiguate statistical dependencies that reflect the connection between neurons from those caused by other sources (e.g. shared inputs or intrinsic cellular or network mechanisms) provided that the dependencies have appropriate timescales, 2) for the study of early sensory systems, it does not require responses to repeated trials of identical stimulation, and 3) it does not assume that the connection between neurons is linear. We describe the theory and implementation of IMI in detail and demonstrate its utility on experimental recordings from the primate visual system.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The root of our brain's computational power lies in its trillions of connections. With our increasing ability to study these connections experimentally comes the need for analytical tools that can be used to develop meaningful quantitative characterizations. In this manuscript, we present a new such tool, incremental mutual information (IMI), that enables the characterization of the strength and dynamics of the connection between a pair of neurons based on the statistical dependencies in their spiking activity. IMI is an important step forward from existing approaches, as it has the potential to disambiguate dependencies due to the connection between two neurons from those due to other sources, such as shared external inputs, provided that the dependencies have appropriate timescales. We demonstrate the utility of IMI through the analysis of simulated neuronal activity as well as activity recorded in the primate visual system.</p>
</abstract><funding-group><funding-statement>This work was supported by the Wellcome Trust and the German Research Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>To understand the function of neuronal circuits and systems, it is essential to characterize the connections between individual neurons. The major connections between and within many brain areas have been mapped through anatomical studies, but these maps specify only the existence of connections, not their strength or dynamics (temporal properties). Measuring the strength and dynamics of the connection between two neurons requires physiological experiments in which the activity of both neurons is measured. The most direct of these experiments involves intracellular recordings, which allow the connection between the two neurons to be directly investigated. However, intracellular recordings are difficult to perform <italic>in vivo</italic> and impossible to obtain from more than a few cells at a time. Instead, most physiological studies of connectivity rely on extracellular recordings from multi-electrode arrays (or, more recently, imaging of calcium activity). In these experiments, it is not usually possible to explicitly verify anatomical connectivity, nor to directly characterize the connections. Instead, the strength and dynamics of ‘functional’ connectivity must be inferred through statistical methods.</p>
<p>The traditional method for characterizing the strength and dynamics of the connection between two neurons is the cross correlation function (<italic>C<sub>XY</sub></italic>), which measures the linear correlation between two signals over a range of specified delays <xref ref-type="bibr" rid="pcbi.1001035-Perkel1">[1]</xref>. While <italic>C<sub>XY</sub></italic> and its variants have been used successfully in a number of studies (see, for example, Usrey and Reid <xref ref-type="bibr" rid="pcbi.1001035-Usrey1">[2]</xref> for a review of many such studies in the visual system), it has limitations that must be considered when studying the connection between neurons <xref ref-type="bibr" rid="pcbi.1001035-Aertsen1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1001035-Melssen1">[5]</xref>. The limitations of <italic>C<sub>XY</sub></italic> arise from the fact that it is a measure of the total (linear) dependency between two signals and, thus, implicitly assumes that all dependencies between them are due to their connection. In the case of neurons, there are in fact many potential sources of dependency – shared external stimuli, intrinsic cellular and network properties, etc. – and <italic>C<sub>XY</sub></italic> cannot disambiguate these dependencies from those due to the actual connection. Several modified versions of <italic>C<sub>XY</sub></italic> have been proposed to address these drawbacks. For example, if neuronal activity in response to repeated trials of the same external stimulus is available for analysis, as is often the case in early sensory systems, the ‘shift-predictor’ can be used to remove some of the correlations due to the stimulus <xref ref-type="bibr" rid="pcbi.1001035-Perkel1">[1]</xref>. Further modifications to <italic>C<sub>XY</sub></italic> have also been proposed to remove the correlations due to stimulus-driven covariations in activity <xref ref-type="bibr" rid="pcbi.1001035-Aertsen2">[6]</xref> and background activity <xref ref-type="bibr" rid="pcbi.1001035-Brody2">[7]</xref>. While these modified approaches have certainly improved upon the standard <italic>C<sub>XY</sub></italic>, the confound of dependencies due to the connection and those arising from other sources remains a general problem.</p>
<p>In addition to correlation-based methods, there are several other approaches to characterizing the dependency between two signals that can be used to study the connection between two neurons. These methods can be generally divided into two classes: model-based and model-free. The most common model-based approach to characterizing dependency is Granger causality (GC) <xref ref-type="bibr" rid="pcbi.1001035-Granger1">[8]</xref>. With GC, one signal is predicted in two different ways: 1) using an autoregressive model based on its own past and 2) using a multivariate autoregressive model based on its own past and the past of the second signal. The strength of the dependency is given by the difference in the predictive power of the two models and the dynamics of the dependency are reflected in the regression parameters that correspond to the influence of the second signal. The power of model-based approaches such as GC is dependent on the validity of the underlying model; if the dependency between the two signals is approximately linear, then the characterization provided by GC will be accurate, but in situations where the properties of the dependency are complex or unknown, as is often the case with neurons, a model-free approach may be more appropriate. The most common model-free approach to characterizing dependency is transfer entropy (TE), the information-theoretic analog of GC <xref ref-type="bibr" rid="pcbi.1001035-Schreiber1">[9]</xref>. TE measures the reduction in the entropy of one signal that is achieved by conditioning on its own past and the past of the second signal relative to the reduction in entropy achieved by conditioning on its own past alone. TE is a powerful tool for measuring the overall strength of a dependency, but is not suitable for characterizing its dynamics.</p>
<p>In this paper, we detail a new model-free approach for characterizing both the strength and dynamics of a dependency by ‘conditioning out’ the temporal correlations in both signals before assessing the strength of the dependency at different delays. This approach can overcome some of the confounds that are common in studies of neuronal connectivity <xref ref-type="bibr" rid="pcbi.1001035-Wang1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1001035-Wang2">[12]</xref>, as it has the potential to disambiguate statistical dependencies that reflect the connection between neurons from those caused by other sources (e.g. shared inputs or intrinsic cellular or network mechanisms) provided that the dependencies have appropriate timescales. In the following sections, we outline the theory behind our measure, which we call incremental mutual information, illustrate its usage on simulated neuronal activity and experimental recordings from the primate visual system, and consider its relationship to other common measures of dependence.</p>
<p>Matlab code for measuring incremental mutual information is available for download at <ext-link ext-link-type="uri" xlink:href="http://www.ucl.ac.uk/ear/research/lesicalab" xlink:type="simple">http://www.ucl.ac.uk/ear/research/lesicalab</ext-link></p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2a">
<title>Correlation</title>
<p>In order to characterize the strength and dynamics of the connection between two signals, it is necessary to quantify how much one signal at one point in time influences the other signal at nearby points in time. Most measures of dependence between two signals <italic>X</italic> and <italic>Y</italic> seek to quantify the difference between the joint distribution <italic>p</italic>(<italic>X,Y</italic>) and the product of the marginal distributions <italic>p</italic>(<italic>X</italic>) <italic>p</italic>(<italic>Y</italic>). For example, the cross correlation function measures the difference between the mean of the joint distribution and the product of the means of the marginal distributions (the covariance), normalized by the product of the standard deviations for a given delay <italic>δ</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <italic>C<sub>XY</sub></italic>[<italic>δ</italic>] is the correlation coefficient between <italic>X</italic>[n] and <italic>Y</italic>[n], which are assumed to be discretized signals, at integer delay <italic>δ</italic>.</p>
</sec><sec id="s2b">
<title>Partial correlation</title>
<p>As described in the <xref ref-type="sec" rid="s1">Introduction</xref>, <italic>C<sub>XY</sub></italic> has limitations that are important to consider when studying neuronal connectivity. Most importantly, <italic>C<sub>XY</sub></italic>, as with all dependency measures that operate only on the joint distribution <italic>p</italic>(<italic>X,Y</italic>) and the marginal distributions <italic>p</italic>(<italic>X</italic>) and <italic>p</italic>(<italic>Y</italic>), cannot differentiate between the dynamics of the connection between the neurons and the temporal correlations in their activity that are due to other sources. It is possible to overcome this limitation by conditioning out the temporal correlations in each signal before measuring the dependency between them, i.e. rather than operate on <italic>p</italic>(<italic>X,Y</italic>), <italic>p</italic>(<italic>X</italic>), and <italic>p</italic>(<italic>Y</italic>), operate on <italic>p</italic>(<italic>X,Y</italic>|<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e002" xlink:type="simple"/></inline-formula>), <italic>p</italic>(<italic>X</italic>|<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e003" xlink:type="simple"/></inline-formula>), and <italic>p</italic>(<italic>Y</italic>|<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e004" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e005" xlink:type="simple"/></inline-formula> is a vector containing the past and future of <italic>X</italic>[n] and <italic>Y</italic>[n] relative to the delay of interest<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e006" xlink:type="simple"/><label>(2)</label></disp-formula>as shown in the schematic diagram in <xref ref-type="fig" rid="pcbi-1001035-g001">figure 1</xref>.</p>
<fig id="pcbi-1001035-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001035.g001</object-id><label>Figure 1</label><caption>
<title>The quantities involved in computing incremental mutual information.</title>
<p>The incremental mutual information (IMI) between two signals <italic>X</italic> and <italic>Y</italic> is computed by first computing the entropy of <italic>X</italic>[<italic>n</italic>] after conditioning on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e007" xlink:type="simple"/></inline-formula>, a vector comprised of the past and future of both signals relative to a delay <italic>δ</italic>. This entropy is then compared the entropy of <italic>X</italic>[<italic>n</italic>] after further conditioning on <italic>Y</italic>[<italic>n−δ</italic>]. The reduction in entropy due to this further conditioning is the incremental mutual information.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.g001" xlink:type="simple"/></fig>
<p>The analog of <italic>C<sub>XY</sub></italic> for conditional distributions is the partial cross correlation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e008" xlink:type="simple"/><label>(3)</label></disp-formula>While <italic>C<sub>XY|Z</sub></italic> overcomes the major limitation of <italic>C<sub>XY</sub></italic>, it is still a linear measure and may not accurately characterize nonlinear dependencies.</p>
</sec><sec id="s2c">
<title>Incremental mutual information</title>
<p>The idea of partial correlation can be generalized for the study of any dependency by formulating the information-theoretic analog of <italic>C<sub>XY|Z</sub></italic> as a partial mutual information <xref ref-type="bibr" rid="pcbi.1001035-Frenzel1">[13]</xref>: First, the entropy of <italic>X</italic> is measured after conditioning on its own past and future, as well as the past and future activity of <italic>Y</italic> relative to the delay of interest. Then, the strength of the influence of <italic>Y</italic> on <italic>X</italic> at the delay of interest can be measured as the additional reduction in entropy that occurs after observing <italic>Y</italic> at that delay:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e009" xlink:type="simple"/><label>(4)</label></disp-formula>Because this quantity, which we call the incremental mutual information (IMI), reduces the uncertainty of <italic>X</italic> as much as possible before measuring the influence of <italic>Y</italic> at each delay, it has the potential to provide an accurate description of both the strength and dynamics of their dependency. In this form, Δ<italic>I<sub>XY</sub></italic> is similar to a partial covariance in that its value is dependent on the properties of the individual signals (e.g. the total entropy of <italic>X</italic>, the strength of the temporal correlations in <italic>X</italic>, etc.). In some cases, it may be preferable to use a normalized measure that is more similar to a partial correlation coefficient, i.e. a measure that expresses the incremental mutual information as a fraction of its maximum possible value:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e010" xlink:type="simple"/><label>(5)</label></disp-formula>To determine whether IMI is appropriate for use in any particular context, it is important to consider the relative timescales of the dependency between the signals and the other dependencies to be conditioned out. At any particular delay, the effects of dependencies with durations that are long relative to the time bins used for discretization will be predictable from the past and future values of the signals, so their contribution to the IMI will be small, i.e. dependencies with a slow timescale will make a relatively large contribution to initial reduction in the entropy of <italic>X</italic> based on past and future values of <italic>X</italic> and <italic>Y</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e011" xlink:type="simple"/></inline-formula>, but not to the additional reduction in the entropy of <italic>X</italic> based on the present value of <italic>Y</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e012" xlink:type="simple"/></inline-formula>. Conversely, the effects of dependencies that have a duration that is similar to the time bins used for discretization will not be predictable from the past and future values of the signals, so their contribution to the IMI will be large, i.e. dependencies with a fast timescale will make a small contribution to the initial reduction in entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e013" xlink:type="simple"/></inline-formula>, but will make a large contribution to the additional reduction in entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e014" xlink:type="simple"/></inline-formula>. Thus, IMI will be most useful when the duration of the dependency between the signals is similar to the size of the time bins used for discretization and the durations of the other dependencies to be conditioned out are longer. Fortunately, this is often the case for neurons in sensory systems, as will be illustrated in the examples in the <xref ref-type="sec" rid="s3">Results</xref>.</p>
</sec><sec id="s2d">
<title>Implementation</title>
<p>As with any measure based on entropies, the calculation of IMI requires careful consideration. Because IMI is a model-free approach, the number of samples required to produce a result of a given precision are likely to significantly exceed those of model-based approaches. The bias and variability of the entropy estimates that underlie the computation of IMI can vary substantially depending on the size of the data sample, the number of possible values that a signal can take on, and the signal dimensionality. Fortunately, neuronal activity typically has only a few possible values (e.g. the number of spikes in each time bin). However, the terms <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e015" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e016" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e017" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e018" xlink:type="simple"/></inline-formula> representing the past and future of the signals are vectors. In practice, these vectors must be limited to some finite length, which we term <italic>ω</italic>, and this length will determine their dimensionality:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e019" xlink:type="simple"/><label>(6)</label></disp-formula>Thus, the calculation of IMI requires a tradeoff: increasing the value of <italic>ω</italic> allows the entropy of the first signal to be reduced as much as possible before measuring the influence of the second signal, but also increases the chances that the entropy estimates may be biased or highly variable. There are a number of bias correction techniques available that may be useful in mitigating problems related to sample size <xref ref-type="bibr" rid="pcbi.1001035-Panzeri1">[14]</xref>. For the examples below, we corrected the entropy estimates using ‘quadratic extrapolation’ bias correction via the information toolbox software available at <ext-link ext-link-type="uri" xlink:href="http://www.ibtb.org" xlink:type="simple">http://www.ibtb.org</ext-link> <xref ref-type="bibr" rid="pcbi.1001035-Magri1">[15]</xref>. Also, for all of the examples below, time is discretized into sufficiently small bins such that each bin contains no more than one spike, limiting the possible values of <italic>X</italic> and <italic>Y</italic> to 0 and 1.</p>
</sec><sec id="s2e">
<title>Statistical inference</title>
<p>Because the bias and variability of entropy estimates are dependent on sample size, it is critical to establish the validity and precision of any calculation of IMI using statistical methods. In the experimental examples presented below, we use two different bootstrap procedures with random sampling to establish 95% confidence intervals and to determine whether the observed values are significantly different from zero. To establish 95% confidence intervals, we calculated IMI from 100 random samples of the same size drawn with replacement from the original sample. To preserve the temporal dependencies in the data, sampling was performed after the vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e020" xlink:type="simple"/></inline-formula> were formed and the three vectors were sampled together. Confidence intervals were defined as the mean ± 2 standard deviations of the values calculated from the random samples. To establish the significance of the observed values, the same procedure was followed, but <italic>Y</italic> was sampled separately from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e021" xlink:type="simple"/></inline-formula>. This sampling preserved the dependencies between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e022" xlink:type="simple"/></inline-formula>, but removed the dependencies between <italic>X</italic> and <italic>Y</italic> (and, thus, in theory, removed any IMI between them). The observed values were considered significantly different from zero if they were greater than 2 standard deviations above the mean of the values calculated from the random samples.</p>
</sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Simulated example 1: Differentiating input correlations and connection dynamics</title>
<p>IMI is designed to give accurate measures of the strength and dynamics of the connections between neurons even in cases when the correlation may not, i.e. when the activities of individual neurons contain temporal correlations unrelated to the connection between them. In these cases, the cross correlation function can be ambiguous – its shape can reflect either the true dynamics of the connection, temporal correlations in the activities of the individual neurons, or a combination of both. A simple example of this ambiguity is illustrated in <xref ref-type="fig" rid="pcbi-1001035-g002">figure 2a</xref>.</p>
<fig id="pcbi-1001035-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001035.g002</object-id><label>Figure 2</label><caption>
<title>Incremental mutual information disambiguates temporal correlations and connection dynamics.</title>
<p>a) A schematic diagram showing two neurons <italic>X</italic> and <italic>Y</italic>. The two neurons are driven by independent uncorrelated noise sources and <italic>Y</italic> drives <italic>X</italic> through a strong dynamic connection. The cross correlation function <italic>C<sub>XΥ</sub></italic> and normalized IMI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e023" xlink:type="simple"/></inline-formula> computed from the simulated activity of the two neurons at a range of delays are shown. b) A second pair of neurons <italic>X</italic> and <italic>Y</italic>. The two neurons are driven by independent noise sources. The source driving <italic>Y</italic> has temporal correlations while the source driving <italic>X</italic> is uncorrelated. <italic>Y</italic> drives <italic>X</italic> through a strong static connection with a delay of 4 samples. The cross correlation function <italic>C<sub>XΥ</sub></italic>, the normalized IMI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e024" xlink:type="simple"/></inline-formula>, and the normalized IMI with only past activity conditioned out <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e025" xlink:type="simple"/></inline-formula> computed from the simulated activity of the two neurons at a range of delays are shown. IMI was computed with <italic>ω</italic> = 2 for 2<sup>20</sup> samples.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.g002" xlink:type="simple"/></fig>
<p>We first simulated a pair of neurons <italic>X</italic> and <italic>Y</italic> with independent, uncorrelated inputs and a dynamic connection, i.e. a spike from neuron <italic>Y</italic> caused a prolonged increase in the spiking probability of neuron <italic>X</italic>. We simulated the activity of neuron <italic>Y</italic> as a dichotomized Gaussian noise and the activity of neuron <italic>X</italic> as the dichotomized sum of a Gaussian noise and the filtered activity of <italic>Y</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e026" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e027" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e028" xlink:type="simple"/></inline-formula> are uncorrelated, <italic>ε</italic> = 0.5 is a scaling factor determining the overall strength of the connection, <italic>θ</italic> = 1 is the spiking threshold, and the input from <italic>Y</italic> to <italic>X</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e029" xlink:type="simple"/></inline-formula>, is the convolution of the activity of <italic>Y</italic> with a Gaussian filter <italic>g</italic>[<italic>n</italic>] with a peak delay of 4 samples and a half width of 3 samples (note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e030" xlink:type="simple"/></inline-formula>, the filtered version of <italic>Y</italic>, is unobserved). From the simulated activity of this pair of neurons (with a sample size of 2<sup>20</sup>), we estimated the cross correlation function <italic>C<sub>XY</sub></italic> and normalized incremental mutual information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e031" xlink:type="simple"/></inline-formula> (with <italic>ω</italic> = 2) at delays ranging from <italic>δ</italic> = −10 to 10 samples. Both <italic>C<sub>XY</sub></italic> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e032" xlink:type="simple"/></inline-formula> for this pair were broad, reflecting the dynamics of the connection.</p>
<p>We next simulated another pair of neurons that was similar to the first one, except that <italic>Υ</italic> received input with temporal correlations and the connection between <italic>Υ</italic> and <italic>X</italic> was static with a delay of 4 samples:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e033" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e034" xlink:type="simple"/></inline-formula> is the convolution of <italic>s<sup>y</sup></italic> with a Gaussian filter <italic>g</italic>[<italic>n</italic>] with a peak at zero delay and a half width of 3 samples. While <italic>C<sub>XY</sub></italic> for this pair was also broad because of the temporal correlations in the activity of <italic>Y</italic>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e035" xlink:type="simple"/></inline-formula> was sharp, reflecting the static connection. Thus, while IMI captures the differences in the connections between these two pairs of neurons, correlation conflates connection dynamics with temporal correlations in individual activities and yields ambiguous results.</p>
<p>This example can also be used to illustrate the necessity of conditioning out the both past and future activities of the neurons. A modified version of IMI can be formulated in which only the past activities of the two neurons are conditioned out:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e036" xlink:type="simple"/><label>(9)</label></disp-formula>In this formulation, the IMI is related to transfer entropy (see <xref ref-type="sec" rid="s4">Discussion</xref>). As shown in <xref ref-type="fig" rid="pcbi-1001035-g002">figure 2b</xref>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e037" xlink:type="simple"/></inline-formula> correctly conditions out the effects of the temporal correlations in the activity of <italic>Y</italic> for delays that are smaller than that of the actual connection, but not for delays that are larger than that of the actual connection. This reason for this asymmetry is as follows: Because of the temporal correlations in the activity of <italic>Y</italic>, its value will be similar for neighboring samples. When the delay of interest <italic>δ</italic> is smaller than the delay corresponding to the actual connection <italic>δ<sup>*</sup></italic>, <italic>Y</italic>[<italic>n</italic>−<italic>δ<sup>*</sup></italic>] is included in the vector of past activity and, since <italic>Y</italic>[<italic>n</italic>−<italic>δ</italic>] carries no information about <italic>X</italic> beyond that which is carried by <italic>Y</italic>[<italic>n</italic>−<italic>δ<sup>*</sup></italic>], <italic>Y</italic>[<italic>n</italic>−<italic>δ</italic>] makes no contribution to the IMI. However, when <italic>Y</italic>[<italic>n</italic>−<italic>δ<sup>*</sup></italic>] is not included in the vector of past activities, <italic>Y</italic>[<italic>n</italic>−<italic>δ</italic>], which is similar to <italic>Y</italic>[<italic>n</italic>−<italic>δ<sup>*</sup></italic>] because of the temporal correlations in <italic>Y</italic>, will carry additional information about the activity of <italic>X</italic> and, thus, will contribute to the IMI.</p>
</sec><sec id="s3b">
<title>Simulated example 2: Unmasking a weak connection</title>
<p>As a further consequence of the ambiguity in the cross correlation function illustrated in the example above, temporal correlations in individual activities may mask weak connections between neurons entirely. A simple example of this problem is shown in <xref ref-type="fig" rid="pcbi-1001035-g003">figure 3a</xref>. We simulated a pair of neurons that received a shared input with temporal correlations and had a weak static connection between them with a delay of 3 samples:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e038" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e039" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e040" xlink:type="simple"/></inline-formula> are the convolution of Gaussian noise with a Gaussian filter as described above with a correlation coefficient of 0.5 between them, and <italic>ε</italic> = 0.25 (other parameter values are as described above). <italic>C<sub>XY</sub></italic> for this pair of neurons was broad, with no discernable increase at the delay corresponding to the connection (black arrow), while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e041" xlink:type="simple"/></inline-formula> exhibits a sharp peak at the appropriate delay. Thus, by conditioning out dependencies due to shared input, IMI is able to reveal connections that may not be evident in the cross correlation function.</p>
<fig id="pcbi-1001035-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001035.g003</object-id><label>Figure 3</label><caption>
<title>Incremental mutual information unmasks weak connections.</title>
<p>a) A schematic diagram showing two neurons <italic>X</italic> and <italic>Y</italic>. The two neurons are driven by a shared correlated noise source and <italic>Y</italic> drives <italic>X</italic> through a weak static connection. The cross correlation function <italic>C<sub>XΥ</sub></italic> and normalized IMI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e042" xlink:type="simple"/></inline-formula> computed from the simulated activity of the two neurons at a range of delays are shown. b) Results for the same simulated neurons driven by a shared uncorrelated source, presented as in panel a. IMI was computed with <italic>ω</italic> = 2 for 2<sup>20</sup> samples.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.g003" xlink:type="simple"/></fig></sec><sec id="s3c">
<title>Simulated example 3: Shared inputs that cannot be conditioned out</title>
<p>A slight modification of the previous example can be used to illustrate a situation where shared inputs cannot be conditioned out and contaminate the IMI. As described above, IMI will be most useful when the duration of the dependency between the signals is similar to the size of the time bins used for discretization and the durations of the other dependencies to be conditioned out are longer, as is the case in example 2. If the simulation in example 2 is modified so that the shared input is uncorrelated over time, the dependency resulting from the shared input can no longer be conditioned out, as the past and future activities of the neurons can no longer be used to infer the effects of the input at the delay of interest. As a result, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e043" xlink:type="simple"/></inline-formula> has two peaks, one with no delay reflecting the shared input, and another with a delay reflecting the actual connection, as shown in <xref ref-type="fig" rid="pcbi-1001035-g003">figure 3b</xref>. It should be noted that this type of contamination can potentially arise both from shared external sources such as sensory stimuli as well as from other unobserved neurons.</p>
</sec><sec id="s3d">
<title>Experimental example 1: Thalamic relay neurons and their retinal inputs</title>
<p>To test the utility of IMI on experimental data, we analyzed the activity of two pairs of thalamic relay neurons and their retinal ganglion cell (RGC) inputs recorded in the lateral geniculate nucleus (LGN) of an anesthetized monkey as shown in <xref ref-type="fig" rid="pcbi-1001035-g004">figure 4a</xref>. The details of the experimental procedures can be found in Carandini et al. <xref ref-type="bibr" rid="pcbi.1001035-Carandini1">[16]</xref>. During the recordings, visual stimulation was presented via an LED that illuminated the receptive field center with an intensity that varied naturally (i.e. with temporal correlations typical of the natural environment). In this example, the stimulus was approximately 12 min in duration and did not repeat.</p>
<fig id="pcbi-1001035-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001035.g004</object-id><label>Figure 4</label><caption>
<title>Incremental mutual information analysis of retinogeniculate pairs.</title>
<p>a) A schematic diagram showing two neurons <italic>X</italic> and <italic>Y</italic>. <italic>Y</italic> is a retinal ganglion cell driven by a stimulus with temporal correlations that are typical of the natural environment. <italic>X</italic> is an LGN relay cell driven by <italic>Y</italic> and an unobserved noise source. b) Histograms showing the distribution of time delays between each retinal PSP and the next LGN spike and the number of additional retinal PSPs that preceded the next LGN spike, as well as the cross correlation function <italic>C<sub>XΥ</sub></italic> and normalized IMI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e044" xlink:type="simple"/></inline-formula> computed from the responses of two retinogeniculate pairs to a non-repeating stimulus at a range of delays. For the IMI, the black line indicates the actual estimate, the yellow band indicates 95% confidence intervals, and the red dashed line indicates the significance level. Confidence intervals and significance levels were generated via bootstrap procedures with random sampling as described in the <xref ref-type="sec" rid="s2">Methods</xref>. Spike times were binned with a resolution of 2 ms and IMI was computed with <italic>ω</italic> = 4 for approximately 2<sup>18</sup> samples.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.g004" xlink:type="simple"/></fig>
<p>The histograms in <xref ref-type="fig" rid="pcbi-1001035-g004">figure 4b</xref> show the basic relationship between the activity of the retinal and thalamic neurons in each pair. For the first pair, less than half of the RGC postsynaptic potentials (PSPs) evoked immediate LGN spikes, while the connection between the second pair was stronger, with nearly 75% of PSPs evoking immediate spikes. We calculated the cross correlation function and incremental mutual information for these pairs after binarizing the spike trains in 2 ms time bins. <italic>C<sub>XY</sub></italic> for these pairs has a complex shape with 3 components: a broad positive peak with a half width of approximately 20 ms reflecting the temporal correlations in the visual stimulus, two sharp negative peaks reflecting refractory effects, and a sharp positive peak reflecting the actual connection between the cells. In contrast, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e045" xlink:type="simple"/></inline-formula> for these pairs had one main peak reflecting the connection between the neurons - the effects of statistical dependencies arising from the stimulus correlations have been completely removed and the refractory effects have been largely conditioned out. For the first pair, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e046" xlink:type="simple"/></inline-formula> had a relatively long tail, reflecting temporal summation of RGC PSPs that failed to evoke an immediate LGN spike. For the second pair, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e047" xlink:type="simple"/></inline-formula> was sharper, reflecting the stronger connection between the cells.</p>
</sec><sec id="s3e">
<title>Relation between incremental mutual information and signal and noise correlations</title>
<p>In early sensory systems, experiments are often designed such that the activity in response to repeated trials of an identical stimulus are observed so that the correlation between neurons can be separated into two distinct parts known as <italic>signal correlation</italic> and <italic>noise correlation</italic>. The signal correlation, which reflects both correlation in the stimulus itself and similarities in neurons' preferred stimulus features, will capture the correlation in the fraction of the response that is repeatable from trial to trial, i.e. the correlation that remains after the trial order has been randomized:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e048" xlink:type="simple"/><label>(11)</label></disp-formula>where <italic>X<sup>i</sup></italic>[<italic>n</italic>] is the response of neuron X on trial <italic>i</italic> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e049" xlink:type="simple"/></inline-formula> indicates the expectation over all possible combinations of trials <italic>i</italic> and <italic>j</italic> in which their values are not equal. In studies of neuronal connectivity, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e050" xlink:type="simple"/></inline-formula> is often referred to as the ‘shift-predictor’.</p>
<p>The noise correlation, which results from network and intrinsic cellular mechanisms, will capture the remaining correlation in the fraction of the response that is variable from trial to trial<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e051" xlink:type="simple"/><label>(12)</label></disp-formula>and, thus, captures the dependencies between the neurons that are not locked to the external stimulus. However, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e052" xlink:type="simple"/></inline-formula> may provide a better measure of the strength and dynamics of the connection between two neurons than <italic>C<sub>XY</sub></italic>, it still confounds connection dynamics and temporal correlations that are independent of the stimulus, e.g. refractory effects or coupled oscillations.</p>
<p>For comparison with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e054" xlink:type="simple"/></inline-formula>, the signal and noise IMI between <italic>X</italic> and <italic>Y</italic> can be formulated in an analogous fashion. The signal IMI is the reduction in the entropy of the response of <italic>X</italic> on trial <italic>i</italic> that results from observing the response of <italic>Y</italic> on trial <italic>j</italic> at the delay of interest, beyond that which results from observing the past and future responses of both neurons on trial <italic>i</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e055" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e056" xlink:type="simple"/></inline-formula>. The noise IMI is the difference between the total IMI and the signal IMI, i.e. the reduction in the entropy of the response of <italic>X</italic> on trial <italic>i</italic> that results from observing the response of <italic>Y</italic> on trial <italic>i</italic> at the delay of interest and the past and future responses of both neurons on trial <italic>i</italic>, beyond that which results from observing the response of <italic>Y</italic> at the delay of interest on trial <italic>j</italic> and the past and future responses of both neurons on trial <italic>i</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e057" xlink:type="simple"/><label>(14)</label></disp-formula></p>
</sec><sec id="s3f">
<title>Experimental example 2: Thalamic relay neurons and their retinal inputs revisited</title>
<p>We estimated the signal and noise correlations and signal and noise IMI for the same two retinogeniculate pairs that were analyzed in experimental example 1 using a different set of responses to 140 repeated trials of identical stimulation in which each trial was 5 seconds in duration. As shown in <xref ref-type="fig" rid="pcbi-1001035-g005">figure 5</xref>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e058" xlink:type="simple"/></inline-formula> for both pairs was broad, reflecting the temporal correlations in the visual stimulus. In contrast, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e059" xlink:type="simple"/></inline-formula> was nearly zero at all delays – because the temporal correlations in the visual stimulus were slow relative to the bin size used for discretization, there was little information about stimulus-induced dependencies to be gained by observing the RGC activity at any particular delay on a different trial when RGC and LGN activity at surrounding delays on the current trial were already known. While the effects of the stimulus correlations were removed from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e060" xlink:type="simple"/></inline-formula> for both pairs, these functions still had a complex shape, with two negative peaks reflecting refractory effects, and one positive peak reflecting the actual connection between the neurons. Thus, while shuffling removed some of the confounding correlations in <italic>C<sub>XY</sub></italic>, others still remained, while in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e061" xlink:type="simple"/></inline-formula>, which has one main peak reflecting the connection between the neurons, most of the confounding dependencies have been conditioned out.</p>
<fig id="pcbi-1001035-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001035.g005</object-id><label>Figure 5</label><caption>
<title>Signal and noise incremental mutual information.</title>
<p>The signal and noise cross correlation functions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e063" xlink:type="simple"/></inline-formula> and the normalized signal and noise IMI <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e064" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e065" xlink:type="simple"/></inline-formula> computed from the responses of the same two retinogeniculate pairs as in <xref ref-type="fig" rid="pcbi-1001035-g004">figure 4b</xref> to repeated trials of an identical stimulus, presented as in <xref ref-type="fig" rid="pcbi-1001035-g004">figure 4b</xref>. Spike times were binned with a resolution of 2 ms and IMI was computed with <italic>ω</italic> = 4 for approximately 2<sup>18</sup> samples.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001035.g005" xlink:type="simple"/></fig>
<p>This example illustrates an important property of IMI. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e066" xlink:type="simple"/></inline-formula> as shown in <xref ref-type="fig" rid="pcbi-1001035-g005">figure 5</xref> is nearly identical to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e067" xlink:type="simple"/></inline-formula> for the same two pairs shown in <xref ref-type="fig" rid="pcbi-1001035-g004">figure 4</xref>. Thus, unlike the cross correlation function, IMI does not require multiple trials in order to differentiate the temporal correlations in the responses of individual neurons from the dynamics of the connection between them.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>We have presented IMI as a new approach to characterizing the strength and dynamics of the connection between neurons. By conditioning out the temporal dependencies in the responses of individual neurons before assessing the connection between them, IMI improves on correlation-based measures in several important ways: 1) IMI has the potential to disambiguate connection dynamics from other temporal dependencies due to shared inputs or intrinsic cellular or network mechanisms provided that the dependencies have appropriate timescales, 2) for the study of sensory systems, IMI does not require responses to repeated trials of identical stimuli, and 3) IMI does not assume that the connection between neurons is linear. Through example applications of IMI to simulated and experimentally recorded neuronal activity, we have demonstrated that IMI has the potential to be both a powerful and practical tool for analyzing the functional connectivity in neuronal circuits.</p>
<sec id="s4a">
<title>Limitations</title>
<p>The major determinant of the ability of IMI to differentiate connection dynamics from other dependencies is the relative timescale of the other dependencies. If the other dependencies have a long duration relative to the time bins used for discretization, then their effects can be conditioned out through observation of past and future neuronal activity, as demonstrated in the experimental examples presented above. If the other dependencies have a duration that is similar to the bin size, then their effects cannot be conditioned out without explicit observation of their source.</p>
<p>As formulated here, IMI is designed to analyze the connection between a pair of neurons. However, in many brain areas, each neuron receives input from a large population, and correlations between these other inputs and the input under study could contaminate the IMI. If the other inputs are unobserved, it will be difficult to account for their effects with a model-free approach, though recent work with model-based approaches has demonstrated some success <xref ref-type="bibr" rid="pcbi.1001035-Kulkarni1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1001035-Pillow1">[20]</xref>. If the other inputs are observed (which is becoming increasingly common with recent advances in recording and imaging technology that allow for simultaneous recording of the activity complete or nearly complete local populations of neurons), there is no reason that, in principle, IMI cannot be extended to condition out dependencies due to the activity of the other neurons. However, adding the activity of additional neurons to the conditioning vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e068" xlink:type="simple"/></inline-formula> will increase its dimensionality, and, thus, the bias and variability of the entropy estimates that underlie the computation of IMI. While this may not be a problem for a small number of neurons, it is certain to be a problem for large populations. Thus, for large populations, it may be more appropriate to use a model-based approach such as Granger causality within a generalized linear model framework <xref ref-type="bibr" rid="pcbi.1001035-Kim1">[21]</xref>.</p>
</sec><sec id="s4b">
<title>Relation between incremental mutual information and transfer entropy</title>
<p>Of the existing approaches to characterizing dependencies between signals, IMI is most similar to transfer entropy <xref ref-type="bibr" rid="pcbi.1001035-Schreiber1">[9]</xref>. TE measures the dependency between two signals as the difference in the entropy of one signal after conditioning on its own past and conditioning on its own past and the past of the other signal, or, in the terminology used to define IMI, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001035.e069" xlink:type="simple"/></inline-formula>. From this definition, it is clear that TE and IMI are designed for different purposes: TE measures the overall causal strength of the dependency between two signals by first conditioning out the past of one signal and then measuring how much can be learned about the present value of that signal based on the past of the second signal, while IMI measures the strength and dynamics of the dependency between two signals by first conditioning out past and future of both signals and then measuring how much can be learned about the present value of one signal from the present value of the other relative to some delay. The key difference between TE and IMI, as illustrated in the simulated example presented above, is that, even if computed at a range of delays, TE is not suitable to assess the dynamics of a dependency because it considers only past activity and, as a result, conditions out temporal correlations appropriately for delays that are shorter than that of the actual dependency, but not for delays that are longer than that of the actual dependency.</p>
</sec><sec id="s4c">
<title>Relation between IMI and generalized linear models</title>
<p>The most effective model-based approach for studying the functional connectivity in a neuronal circuit is the generalized linear model (GLM) <xref ref-type="bibr" rid="pcbi.1001035-Pillow2">[22]</xref>–<xref ref-type="bibr" rid="pcbi.1001035-Truccolo2">[24]</xref>. The GLM attempts to predict a neuron's activity based not only on its own activity and the activity of other neurons, but also on external inputs. Because all of the filters in the model are fit simultaneously, the influence of the external inputs on the activity of each neuron, as well as those of its own past activity, are separated from the influence of other neurons. The power of the GLM lies in the fact that once the filters have been estimated, the model can be used to predict the activity of the entire group of neurons to any external input, but this power comes at the expense of assuming a particular parametric structure. Relative to IMI, which makes no assumptions about the connections between neurons, the drawback of the GLM is that the interactions between neurons are assumed to be of a particular nature (usually additive). However, this assumption also allows the GLM to be readily applied to large populations.</p>
</sec></sec></body>
<back>
<ack>
<p>We thank Jose-Manuel Alonso, Alex Dimitrov, Maneesh Sahani, Edward Milli, and Peter Latham for their comments on the manuscript and Lawrence Sincich for providing the experimental data and helpful suggestions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1001035-Perkel1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Perkel</surname><given-names>DH</given-names></name>
<name name-style="western"><surname>Gerstein</surname><given-names>GL</given-names></name>
<name name-style="western"><surname>Moore</surname><given-names>GP</given-names></name>
</person-group>             <year>1967</year>             <article-title>Neuronal spike trains and stochastic point processes. II. Simultaneous spike trains.</article-title>             <source>Biophys J</source>             <volume>7</volume>             <fpage>419</fpage>             <lpage>40</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Usrey1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Usrey</surname><given-names>WM</given-names></name>
<name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name>
</person-group>             <year>1999</year>             <article-title>Synchronous activity in the visual system.</article-title>             <source>Annu Rev Physiol</source>             <volume>61</volume>             <fpage>435</fpage>             <lpage>456</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Aertsen1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Aertsen</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Gerstein</surname><given-names>GL</given-names></name>
</person-group>             <year>1985</year>             <article-title>Evaluation of neuronal connectivity: sensitivity of cross-correlation.</article-title>             <source>Brain Res</source>             <volume>340</volume>             <fpage>341</fpage>             <lpage>54</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Brody1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name>
</person-group>             <year>1999</year>             <article-title>Correlations without synchrony.</article-title>             <source>Neural Comput</source>             <volume>11</volume>             <fpage>1537</fpage>             <lpage>51</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Melssen1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Melssen</surname><given-names>WJ</given-names></name>
<name name-style="western"><surname>Epping</surname><given-names>WJ</given-names></name>
</person-group>             <year>1987</year>             <article-title>Detection and estimation of neural connectivity based on crosscorrelation analysis.</article-title>             <source>Biol Cybern</source>             <volume>57</volume>             <fpage>403</fpage>             <lpage>14</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Aertsen2"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Aertsen</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Gerstein</surname><given-names>GL</given-names></name>
<name name-style="western"><surname>Habib</surname><given-names>MK</given-names></name>
<name name-style="western"><surname>Palm</surname><given-names>G</given-names></name>
</person-group>             <year>1989</year>             <article-title>Dynamics of neuronal firing correlation: modulation of “effective connectivity”.</article-title>             <source>J Neurophysiol</source>             <volume>61</volume>             <fpage>900</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Brody2"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name>
</person-group>             <year>1999</year>             <article-title>Disambiguating different covariation types.</article-title>             <source>Neural Comput</source>             <volume>11</volume>             <fpage>1527</fpage>             <lpage>35</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Granger1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Granger</surname><given-names>CWJ</given-names></name>
</person-group>             <year>1969</year>             <article-title>Investigating Causal Relations by Econometric Models and Cross-Spectral Methods.</article-title>             <source>Econometrica</source>             <volume>37</volume>             <fpage>424</fpage>             <lpage>38</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Schreiber1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schreiber</surname><given-names>T</given-names></name>
</person-group>             <year>2000</year>             <article-title>Measuring Information Transfer.</article-title>             <source>Phys Rev Lett</source>             <volume>85</volume>             <fpage>461</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Wang1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Bressler</surname><given-names>SL</given-names></name>
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
</person-group>             <year>2007</year>             <article-title>Granger causality between multiple interdependent neurobiological time series: blockwise versus pairwise methods.</article-title>             <source>Int J Neural Syst</source>             <volume>17</volume>             <fpage>71</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Rajagovindan1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rajagovindan</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Decomposing neural synchrony: toward an explanation for near-zero phase-lag in cortical oscillatory networks.</article-title>             <source>PLoS One</source>             <volume>3</volume>             <fpage>3649</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Wang2"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Estimating Granger causality after stimulus onset: A cautionary note.</article-title>             <source>NeuroImage</source>             <volume>41</volume>             <fpage>767</fpage>             <lpage>776</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Frenzel1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frenzel</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Pompe</surname><given-names>B</given-names></name>
</person-group>             <year>2007</year>             <article-title>Partial Mutual Information for Coupling Analysis of Multivariate Time Series.</article-title>             <source>Phys Rev Lett</source>             <volume>99</volume>             <fpage>204101</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Panzeri1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Senatore</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Montemurro</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Petersen</surname><given-names>RS</given-names></name>
</person-group>             <year>2007</year>             <article-title>Correcting for the sampling bias problem in spike train information measures.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>1064</fpage>             <lpage>1072</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Magri1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Magri</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Whittingstall</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Singh</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>
<name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>
</person-group>             <year>2009</year>             <article-title>A toolbox for the fast information analysis of multiple-site LFP, EEG and spike train recordings.</article-title>             <source>BMC Neurosci</source>             <volume>10</volume>             <fpage>81</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Carandini1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>
</person-group>             <year>2007</year>             <article-title>Thalamic filtering of retinal spike trains by postsynaptic summation.</article-title>             <source>J Vis</source>             <volume>7</volume>             <fpage>20.1</fpage>             <lpage>11</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Kulkarni1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kulkarni</surname><given-names>JE</given-names></name>
<name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>
</person-group>             <year>2007</year>             <article-title>Common-input models for multiple neural spike-train data.</article-title>             <source>Network</source>             <volume>18</volume>             <fpage>375</fpage>             <lpage>407</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Nykamp1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nykamp</surname><given-names>DQ</given-names></name>
</person-group>             <year>2007</year>             <article-title>A mathematical framework for inferring connectivity in probabilistic neuronal networks.</article-title>             <source>Math Biosci</source>             <volume>205</volume>             <fpage>204</fpage>             <lpage>251</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Nykamp2"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nykamp</surname><given-names>DQ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Pinpointing connectivity despite hidden nodes within stimulus-driven networks.</article-title>             <source>Phys Rev E Stat Nonlin Soft Matter Phys</source>             <volume>78</volume>             <fpage>021902</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Pillow1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>Latham</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>Neural characterization in partially observed populations of spiking neurons.</article-title>             <source>Adv Neural Inf Process Syst</source>             <volume>20</volume>             <fpage>1161</fpage>             <lpage>1168</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Kim1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kim</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Brown</surname><given-names>EN</given-names></name>
</person-group>             <year>2010</year>             <article-title>A general statistical framework for Granger causality.</article-title>             <fpage>2222</fpage>             <lpage>2225</lpage>             <comment>In: Proc IEEE Int Conf Acoust Speech Signal Process; 14–19 March 2010; Dallas, Texas, United States</comment>          </element-citation></ref>
<ref id="pcbi.1001035-Pillow2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Litke</surname><given-names>A</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population.</article-title>             <source>Nature</source>             <volume>454</volume>             <fpage>995</fpage>          </element-citation></ref>
<ref id="pcbi.1001035-Truccolo1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Eden</surname><given-names>UT</given-names></name>
<name name-style="western"><surname>Fellows</surname><given-names>MR</given-names></name>
<name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>Brown</surname><given-names>EN</given-names></name>
</person-group>             <year>2005</year>             <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects.</article-title>             <source>J Neurophysiol</source>             <volume>93</volume>             <fpage>1074</fpage>             <lpage>1089</lpage>          </element-citation></ref>
<ref id="pcbi.1001035-Truccolo2"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Hochberg</surname><given-names>LR</given-names></name>
<name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name>
</person-group>             <year>2010</year>             <article-title>Collective dynamics in human and monkey sensorimotor cortex: predicting single neuron spikes.</article-title>             <source>Nat Neurosci</source>             <volume>13</volume>             <fpage>105</fpage>             <lpage>111</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>