<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pbio</journal-id><journal-id journal-id-type="allenpress-id">plbi</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="doi">10.1371/journal.pbio.0060126</article-id><article-id pub-id-type="publisher-id">07-PLBI-RA-2244R4</article-id><article-id pub-id-type="sici">plbi-06-05-17</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Low-Level Information and High-Level Perception: The Case of Speech in Noise </article-title><alt-title alt-title-type="running-head">Speech Perception and Low-Level Information</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Nahum</surname>
            <given-names>Mor</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Nelken</surname>
            <given-names>Israel</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Ahissar</surname>
            <given-names>Merav</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
				<label>1</label><addr-line> Interdisciplinary Center for Neural Computation (ICNC), Hebrew University, Jerusalem, Israel
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line> Department of Neurobiology, Hebrew University, Jerusalem, Israel
			</addr-line></aff><aff id="aff3">
				<label>3</label><addr-line> Department of Psychology, Hebrew University, Jerusalem, Israel
			</addr-line></aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Poeppel</surname>
            <given-names>David</given-names>
          </name>
          <role>Academic Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Maryland College Park, United States of America</aff><author-notes>
        <corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">msmerava@mscc.huji.ac.il</email></corresp>
        <fn fn-type="con" id="ack1">
          <p> MN, IN, and MA conceived and designed the experiments. MN performed the experiments. MN analyzed the data. IN contributed reagents/materials/analysis tools. MN, IN, and MA wrote the paper.</p>
        </fn>
      <fn fn-type="conflict" id="ack3">
        <p> The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="ppub">
        <month>5</month>
        <year>2008</year>
      </pub-date><pub-date pub-type="epub">
        <day>20</day>
        <month>5</month>
        <year>2008</year>
      </pub-date><volume>6</volume><issue>5</issue><elocation-id>e126</elocation-id><history>
        <date date-type="received">
          <day>18</day>
          <month>7</month>
          <year>2007</year>
        </date>
        <date date-type="accepted">
          <day>11</day>
          <month>4</month>
          <year>2008</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder> Nahum et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article related-article-type="companion" xlink:href="info:doi/10.1371/journal.pbio.0060155" xlink:type="simple">
        <article-title>On the Emergence and Awareness of Auditory Objects</article-title>
      </related-article><abstract>
        <p>Auditory information is processed in a fine-to-crude hierarchical scheme, from low-level acoustic information to high-level abstract representations, such as phonological labels. We now ask whether fine acoustic information, which is not retained at high levels, can still be used to extract speech from noise. Previous theories suggested either full availability of low-level information or availability that is limited by task difficulty. We propose a third alternative, based on the Reverse Hierarchy Theory (RHT), originally derived to describe the relations between the processing hierarchy and visual perception. RHT asserts that only the higher levels of the hierarchy are immediately available for perception. Direct access to low-level information requires specific conditions, and can be achieved only at the cost of concurrent comprehension. We tested the predictions of these three views in a series of experiments in which we measured the benefits from utilizing low-level binaural information for speech perception, and compared it to that predicted from a model of the early auditory system. Only auditory RHT could account for the full pattern of the results, suggesting that similar defaults and tradeoffs underlie the relations between hierarchical processing and perception in the visual and auditory modalities.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <sec id="st1">
          <title/>
          <p>One of the central questions in sensory neuroscience is the determination of the maximal amount of task-relevant information that is encoded in our brain. It is often assumed that all of this information is available for making perceptual decisions. We now show that this assumption does not hold generally. We find that when discriminating or understanding speech masked by noise, only the information that is represented at higher cortical areas is generally accessible for perception. Thus, when we need to decide whether the speaker said “day” or “night,” we are likely to succeed in this discrimination. However, when fine discriminations are required (e.g., “day” vs. “bay”), the information regarding the fine spectral and temporal details, which are necessary to discriminate these two words, can be fully utilized only under special conditions. These conditions include, for example, systematic repetitions of the stimuli, as often done in psychoacoustic experiments, or when one eliminates the need for comprehension and focuses on mere identification. These conditions are nonecological, and are not afforded in most daily situations.</p>
        </sec>
      </abstract><abstract abstract-type="toc">
        <p>The ability to both perceive and understand speech accurately depends on the loss of information along the acoustic-to-phonological hierarchy of auditory processing. A general theory of hierarchical processing in the brain predicts the conditions under which we can or cannot do both.</p>
      </abstract><funding-group><funding-statement> We thank the Israeli Science Foundation “Center of Excellence,” the Israeli Institute for Psychobiology, and the Volkswagen Foundation for supporting this study.</funding-statement></funding-group><counts>
        <page-count count="14"/>
      </counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group>
        <custom-meta>
          <meta-name>citation</meta-name>
          <meta-value>Nahum M, Nelken I, Ahissar M (2008) Low-level information and high-level perception: The case of speech in noise. PLoS Biol 6(5): e126. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0060126" xlink:type="simple">10.1371/journal.pbio.0060126</ext-link></meta-value>
        </custom-meta>
      </custom-meta-group></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>It is commonly accepted that auditory information is processed along the auditory pathways in a hierarchical manner [<xref ref-type="bibr" rid="pbio-0060126-b001">1</xref>–<xref ref-type="bibr" rid="pbio-0060126-b008">8</xref>], as in other sensory systems [<xref ref-type="bibr" rid="pbio-0060126-b009">9</xref>,<xref ref-type="bibr" rid="pbio-0060126-b010">10</xref>]. Although the functions of the various stages of this hierarchy, particularly at its cortical levels, are not well understood, the auditory hierarchy can be crudely divided into lower and higher representation levels [<xref ref-type="bibr" rid="pbio-0060126-b001">1</xref>,<xref ref-type="bibr" rid="pbio-0060126-b004">4</xref>,<xref ref-type="bibr" rid="pbio-0060126-b008">8</xref>]. Lower level representations reliably and selectively encode fine spectrotemporal acoustic features. Thus, at the brainstem level of the superior olivary complex (SOC), inputs from the two ears are compared within narrow frequency bands and with microsecond resolution [<xref ref-type="bibr" rid="pbio-0060126-b011">11</xref>–<xref ref-type="bibr" rid="pbio-0060126-b016">16</xref>]. In contrast, cortical levels integrate across time and frequency, and form more abstract, spectrotemporally broader, categories [<xref ref-type="bibr" rid="pbio-0060126-b005">5</xref>–<xref ref-type="bibr" rid="pbio-0060126-b008">8</xref>,<xref ref-type="bibr" rid="pbio-0060126-b017">17</xref>–<xref ref-type="bibr" rid="pbio-0060126-b022">22</xref>]. One of these higher representation levels is believed to be the phonological representation that underlies human speech perception [<xref ref-type="bibr" rid="pbio-0060126-b023">23</xref>–<xref ref-type="bibr" rid="pbio-0060126-b031">31</xref>]. A crucial property of these higher levels is the fact that acoustically different stimuli may belong to the same category (e.g., different instances of /ba/), whereas acoustically more similar stimuli may belong to different categories (e.g., similar instances of /ba/ and /da/) [<xref ref-type="bibr" rid="pbio-0060126-b020">20</xref>].</p>
      <p>The fact that fine acoustic differences may be encoded at low levels of the auditory hierarchy, but not at its high levels, raises the question of whether such differences can be utilized for perceptual discriminations even when they are lost at the high representation levels. Although this question addresses the basic relations between the information available to the auditory system and our ability to use it for conscious perception, it is still unresolved. At least two different answers have been previously suggested.</p>
      <p>A vast body of psychoacoustic studies proposes that all the information represented in the low levels of the auditory system is available for perception (termed here the <italic>unlimited</italic> view). Thus, perception fully utilizes low-level information, which is limited only by the variability of the neuronal responses at lower representation levels [<xref ref-type="bibr" rid="pbio-0060126-b032">32</xref>–<xref ref-type="bibr" rid="pbio-0060126-b037">37</xref>]. This claim is based on the ability of “ideal listener” models to account for human performance in a broad range of psychoacoustical tasks (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b016">16</xref>,<xref ref-type="bibr" rid="pbio-0060126-b038">38</xref>–<xref ref-type="bibr" rid="pbio-0060126-b051">51</xref>]). These models usually assume two basic processing stages: a low-level neuronal representation of the input, which encapsulates all the information believed to be available at the level of the brainstem (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b052">52</xref>]), and a subsequent decision-making stage [<xref ref-type="bibr" rid="pbio-0060126-b037">37</xref>], which performs statistically optimal decisions based on the full array of low-level activity [<xref ref-type="bibr" rid="pbio-0060126-b037">37</xref>].</p>
      <p>On the other hand, a separate body of literature proposes that the answer depends on the behavioral context. Attention studies demonstrate that under demanding behavioral conditions, performance is lower than expected based on the information available at the low representation levels (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b053">53</xref>–<xref ref-type="bibr" rid="pbio-0060126-b057">57</xref>]), i.e., poorer than the ideal listener prediction. Though most of these studies were conducted in the visual modality, there are also some compelling examples in the auditory domain. Particularly strong illustrations are provided by masking studies in which the stimuli are designed so that the low-level representations of the target and the masker do not overlap. Still, performance of many listeners is substantially degraded by the masking stimulus, indicating poor use of low-level information (“informational masking” [<xref ref-type="bibr" rid="pbio-0060126-b058">58</xref>–<xref ref-type="bibr" rid="pbio-0060126-b063">63</xref>]). Recent conceptualization of attention studies (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b057">57</xref>,<xref ref-type="bibr" rid="pbio-0060126-b064">64</xref>–<xref ref-type="bibr" rid="pbio-0060126-b068">68</xref>]) defines demanding behavioral conditions in terms of the “load” they pose on the limited attentional and perceptual resources. These “limited-capacity” models, therefore, predict that under low attentional load, low-level information can be fully utilized, whereas under high load, the perceptual system can only process a portion of the relevant low-level information. The term <italic>load</italic> is not accurately defined, but is intuitively associated with task difficulty. Thus, as long as task difficulty remains the same, the ability to utilize low-level information should not change.</p>
      <p>We now propose that the ability to use all the available low-level information depends on the stimulation protocol, rather than on the behavioral difficulty per se. This proposal is derived from the Reverse Hierarchy Theory (RHT), originally developed to address the relations between hierarchical processing and perception in the visual modality [<xref ref-type="bibr" rid="pbio-0060126-b069">69</xref>,<xref ref-type="bibr" rid="pbio-0060126-b070">70</xref>]. RHT had been successful in accounting for the discrepancy between the accurate spatial information available at lower levels of the visual hierarchy and its limited use in fast perceptual discriminations. We now apply its concepts to the auditory domain. According to RHT, high-level representations (such as phonological representations in the auditory domain) are immediately accessible to perception and therefore underlie our initial perceptual experience. Low-level representations (such as high-resolution interaural time differences) are accessible only under specific, privileged conditions. Hence, in general, low-level information would be available for perceptual discriminations only when high-level representations are essentially equivalent to the low-level representations. When this equivalence fails, perceptual discriminations can fully benefit from low-level resolution only under special behavioral conditions, which allow a search backwards along the “reverse hierarchy” for tracking the most informative low-level population.</p>
      <p>In order to critically test the predictions of these three views, we measured the utilization of low-level information when extracting speech from noise, in a variety of behavioral conditions, which were administered in two studies. We calculated the expected ideal listener performance in each of these conditions. According to the “unlimited” view, performance should match ideal listener thresholds in all conditions. According to the “limited capacity” view, utilization of low-level information should depend on task difficulty (Study 2; <xref ref-type="table" rid="pbio-0060126-t002">Table 2</xref>) and would not change when task difficulty remains the same (Study 1; <xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref>).</p>
      <table-wrap content-type="2col" id="pbio-0060126-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.0060126.t001</object-id><label>Table 1</label><caption>
          <p>The Success of the Predictions of the Three Models (Unlimited, Limited Capacity, and Reverse Hierarchy Theory) for Experiments I–IV of Study 1</p>
        </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.t001" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb1col1" align="left" charoff="0" char=""/><col id="tb1col2" align="left" charoff="0" char=""/><col id="tb1col3" align="left" charoff="0" char=""/><col id="tb1col4" align="left" charoff="0" char=""/><col id="tb1col5" align="left" charoff="0" char=""/><col id="tb1col6" align="left" charoff="0" char=""/><col id="tb1col7" align="left" charoff="0" char=""/><col id="tb1col8" align="left" charoff="0" char=""/><col id="tb1col9" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" rowspan="3"><hr/>Experiment</td><td rowspan="3"><hr/>Task</td><td rowspan="3"><hr/>Binaural Protocol</td><td colspan="6" valign="middle"><hr/>Predictions of the Three Models</td></tr><tr><td colspan="2" valign="middle"><hr/>Unlimited</td><td colspan="2" valign="middle"><hr/>Limited Capacity</td><td colspan="2" valign="middle"><hr/>RHT</td></tr><tr><td valign="middle"><hr/>Phonologically Similar</td><td valign="middle"><hr/>Phonologically Different</td><td valign="middle"><hr/>Phonologically Similar</td><td valign="middle"><hr/>Phonologically Different</td><td valign="middle"><hr/>Phonologically Similar</td><td valign="middle"><hr/>Phonologically Different</td></tr></thead><tbody><tr><td valign="middle">I</td><td valign="middle">Identification</td><td valign="middle">Consistent</td><td valign="middle">&check;</td><td valign="middle">&check;</td><td valign="middle">&check;<sup>a</sup></td><td valign="middle">&check;<sup>a</sup></td><td valign="middle">&check;</td><td valign="middle">&check;</td></tr><tr><td valign="middle">II</td><td valign="middle">Semantic</td><td valign="middle">Consistent</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&check; &dArr;</td><td valign="middle">&check;</td></tr><tr><td valign="middle">III</td><td valign="middle">Identification</td><td valign="middle">Mixed</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&check; &dArr;</td><td valign="middle">&check;</td></tr><tr><td valign="middle">IV</td><td valign="middle">Semantic</td><td valign="middle">Mixed</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&times;</td><td valign="middle">&check;</td><td valign="middle">&check; &dArr;</td><td valign="middle">&check;</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt101"><p>Experimental results included measures of binaural benefits, or difference in sensitivity to noise in conditions when discrimination between either phonologically similar or phonologically different words, which were presented to both ears, was required. A &check; sign indicates that the experimental result fitted the prediction of the model. A &times; sign indicates that the experimental results did not fit the prediction of the model. A downward arrow (&dArr;) sign indicates that the model predicts less than ideal listener level of performance. In all other cases, the model predicts an ideal listener level of performance.</p></fn><fn id="nt102"><p><sup>a</sup>For the limited capacity view, the prediction is that as long as the difficulty is not changed, binaural benefits should remain the same. Since ideal listener levels were obtained for the simplest condition (Experiment I), performance in all other experiments is expected to be similar.</p></fn></table-wrap-foot> --></table-wrap>
      <table-wrap content-type="2col" id="pbio-0060126-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.0060126.t002</object-id><label>Table 2</label><caption>
          <p>The Success of the Predictions of the Three Models (Unlimited, Limited Capacity, and Reverse Hierarchy Theory) for Experiments I and II of Study 2</p>
        </caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.t002" xlink:type="simple"/><!-- <table frame="hsides" rules="none"><colgroup><col id="tb2col1" align="left" charoff="0" char=""/><col id="tb2col2" align="left" charoff="0" char=""/><col id="tb2col3" align="left" charoff="0" char=""/><col id="tb2col4" align="left" charoff="0" char=""/><col id="tb2col5" align="left" charoff="0" char=""/><col id="tb2col6" align="left" charoff="0" char=""/><col id="tb2col7" align="left" charoff="0" char=""/><col id="tb2col8" align="left" charoff="0" char=""/><col id="tb2col9" align="left" charoff="0" char=""/><col id="tb2col10" align="left" charoff="0" char=""/></colgroup><thead><tr><td align="left" rowspan="3"><hr/>Experiment</td><td rowspan="3"><hr/>Experimental Manipulation</td><td rowspan="3"><hr/>Condition</td><td rowspan="3"><hr/>Binaural Protocol</td><td colspan="6"><hr/>Predictions of the Three Models</td></tr><tr><td colspan="2"><hr/>Unlimited</td><td colspan="2"><hr/>Limited Capacity</td><td colspan="2"><hr/>RHT</td></tr><tr><td><hr/>Phonologically Similar</td><td><hr/>Phonologically Different</td><td><hr/>Phonologically Similar</td><td><hr/>Phonologically Different</td><td><hr/>Phonologically Similar</td><td><hr/>Phonologically Different</td></tr></thead><tbody><tr><td>I</td><td>Set size (&ldquo;cognitive load&rdquo;)</td><td>Set size 2</td><td>Mixed</td><td>&times;</td><td>&check;</td><td>&times;</td><td>&check;</td><td>&check; &dArr;</td><td>&check;</td></tr><tr><td>Set size 10</td><td>Mixed</td><td>&times;</td><td>&check;</td><td>&check; &dArr;</td><td>&times; &dArr;</td><td>&check; &dArr;</td><td>&check;</td></tr><tr><td>II</td><td>Success level (&ldquo;perceptual load&rdquo;)</td><td>60&percnt;</td><td>Consistent</td><td>&check;</td><td>&check;</td><td>&times; &dArr;</td><td>&times; &dArr;</td><td>&check;</td><td>&check;</td></tr><tr><td>80&percnt;</td><td>Consistent</td><td>&check;</td><td>&check;</td><td>&check;</td><td>&check;</td><td>&check;</td><td>&check;</td></tr><tr><td>60&percnt;</td><td>Mixed</td><td>&times;</td><td>&check;</td><td>&check; &dArr;</td><td>&times; &dArr;</td><td>&check; &dArr;</td><td>&check;</td></tr><tr><td>80&percnt;</td><td>Mixed</td><td>&times;</td><td>&check;</td><td>&times;</td><td>&check;</td><td>&check; &dArr;</td><td>&check;</td></tr></tbody></table> --><!-- <table-wrap-foot><fn id="nt201"><p>Experimental results included measures of binaural benefits, or difference in sensitivity to noise in conditions when discrimination between either phonologically similar or phonologically different words, which were presented to both ears, was required. Notations as in <xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref>. Note that the unlimited view fails in its prediction only for the phonologically similar words, as in Study 1; the limited capacity view predicts subideal performance in cases of increased difficulty, whereas the experimental results are different. RHT predicts ideal listener levels for phonologically different words under all conditions, but only under the consistent protocol for the phonologically similar words, regardless of task difficulty.</p></fn></table-wrap-foot> --></table-wrap>
      <p>In order to assess RHT predictions, we used two types of word sets, composed of phonologically different and phonologically similar words, respectively. This distinction is irrelevant for the ability of listeners to use low-level cues according to either the unlimited or the limited capacity views. However, RHT makes specific predictions for these two cases. Phonologically different words have distinctive low-level representations (since they are acoustically different) and distinctive high-level representations (since they are phonologically different). Thus, phonologically different words have the property that low-level and high-level representations are equivalent, and therefore, RHT predicts full use of low-level information, regardless of task difficulty (right-most column in <xref ref-type="table" rid="pbio-0060126-t001">Tables 1</xref> and <xref ref-type="table" rid="pbio-0060126-t002">2</xref>). In contrast, phonologically similar words have distinctive low-level representations (as will be demonstrated below for the word pairs used in this study), but at the phonological level, their representations will have a high degree of overlap (since they are phonologically similar). In this case, extracting the more abstract phonological categories causes partial loss of low-level information at the higher representation levels. Therefore, RHT predicts that the benefit from low-level information should match the performance predicted by ideal listener models only in specific protocols that allow backward search to find the informative low-level populations.</p>
      <p>In the two studies we conducted, our measure for utilization of low-level information was the ability to use fine temporal cues between the inputs reaching the two ears in order to extract speech from noise (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b013">13</xref>,<xref ref-type="bibr" rid="pbio-0060126-b044">44</xref>]). In ecological conditions, such time differences may arise when the source of the noise has a different azimuth than the source of the speech. Such time differences, which in humans are less than 1 ms, are usually expressed as phase differences since they are calculated within narrow frequency bands at the SOC [<xref ref-type="bibr" rid="pbio-0060126-b013">13</xref>,<xref ref-type="bibr" rid="pbio-0060126-b016">16</xref>,<xref ref-type="bibr" rid="pbio-0060126-b044">44</xref>]. We thus measured performance under two configurations of interaural phase differences, diotic and dichotic. The diotic configuration contains no phase information, since identical input (signal + noise) is presented to the two ears. The dichotic configuration maximizes phase information for separation between signal and noise [<xref ref-type="bibr" rid="pbio-0060126-b045">45</xref>,<xref ref-type="bibr" rid="pbio-0060126-b071">71</xref>]: the noise is identical in the two ears, while the signal is added with opposite phase to the two ears. The ability of listeners to use the low-level phase information was measured by the difference between dichotic and diotic thresholds (termed <italic>binaural benefit</italic>, typically in the range of 3–7 dB, e.g., [<xref ref-type="bibr" rid="pbio-0060126-b038">38</xref>,<xref ref-type="bibr" rid="pbio-0060126-b045">45</xref>,<xref ref-type="bibr" rid="pbio-0060126-b051">51</xref>,<xref ref-type="bibr" rid="pbio-0060126-b071">71</xref>–<xref ref-type="bibr" rid="pbio-0060126-b074">74</xref>]). Task difficulty was measured by diotic thresholds.</p>
      <p>We found that human performance does not consistently match that of the ideal listener, in contrast to the unlimited view. However, task difficulty per se does not affect the ability to use low-level information, in contrast to the limited capacity view. Low-level information is always fully utilized when phonologically different words are used, but only under one specific protocol when phonologically similar words are used. This pattern matches the predictions of RHT, suggesting that its concept, of reversed relations between the hierarchy of processing and perceptual accessibility, is also applicable to the auditory modality.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>The auditory stimuli we used were disyllabic Hebrew words and non-words embedded in speech noise [<xref ref-type="bibr" rid="pbio-0060126-b075">75</xref>], presented under both diotic and dichotic configurations (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). All experiments were administered with both phonologically different word pairs (e.g., /tamid/ and /chalom/), and with phonologically similar word pairs, which differed in only one phoneme (e.g., /tamid/ and /amid/).</p>
      <sec id="s2a">
        <title>Study 1—Manipulating Task Requirements while Retaining Its Difficulty</title>
        <p>In this series of experiments, we asked whether binaural benefits can be modified without changing task difficulty, namely, without changing diotic thresholds. We used an ideal listener model (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> and <xref ref-type="supplementary-material" rid="pbio-0060126-se001">Text S1</xref>) to calculate the expected performance. The two free parameters of the model (noise levels in the energy and correlation channels, respectively) were calculated from performance with a single, different set of words (used in Experiment II of Study 2). Thus, in all the calculations for this study, the model had no free parameters.</p>
      </sec>
      <sec id="s2b">
        <title>Experiment I—Word Identification with No Binaural Uncertainty</title>
        <p>The first experiment was designed to replicate studies that found binaural benefits to match those calculated by ideal listener models. The behavioral task was to identify which of the two words comprising the stimulus set (either phonologically different or phonologically similar) was presented in a given trial. Diotic and dichotic configurations were administered in separate blocks, so that the same binaural configuration was repeated across trials for each threshold measurement (“consistent”), eliminating binaural uncertainty within a block.</p>
        <p><xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref> (A and B) shows the average changes of signal level during the adaptive tracks in the diotic (thick lines) and dichotic (thin lines) blocks, for both types of word sets, respectively. The plots denote the estimated signal to noise ratio (SNR—the difference between stimulus and noise levels) in decibels, as a function of trial number during the assessment. The initial SNR reflects an experimenter-selected level, but subsequent signal levels were set according to performance. Typically, a steady state level of performance is reached by the 40th trial, reflecting the SNR needed to attain 80% correct (which we use here as the discrimination threshold).</p>
        <fig id="pbio-0060126-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pbio.0060126.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Results of Study 1, Experiments I–IV</title>
            <p>Left: results using phonologically different word pairs (blue). Right: results using phonologically similar pairs (red). (A–H) The dynamics of the adaptive threshold assessment as a function of trial number (averaged across subjects ± SEM, <italic>n</italic> = 10 for each of the eight conditions). The level of the signal was modified in relation to subject's performance (following a three down–one up adaptive procedure). Illustrations of diotic (thick curves) and dichotic (thin curves) thresholds, which are calculated as the mean of last five reversals (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>), are marked by dashed lines in (A). Thresholds are denoted in decibel SNR. Binaural benefits (vertical arrows in all panels) are calculated as the difference between the diotic and dichotic thresholds.</p>
            <p>(A and B) Experiment I: the identification task with no binaural uncertainty (consistent binaural protocol).</p>
            <p>(C and D) Experiment II: the semantic task with no binaural uncertainty (consistent binaural protocol.</p>
            <p>(E and F) Experiment III: the identification task with binaural uncertainty (mixed binaural protocol).</p>
            <p>(G and H) Experiment IV: the semantic task with binaural uncertainty (mixed binaural protocol).</p>
            <p>(I and J) A summary of the average binaural benefits obtained in Experiments I–IV (filled shaded bars), and the benefits calculated by an ideal listener model (open bars; see <xref ref-type="supplementary-material" rid="pbio-0060126-sg002">Figure S2</xref> and <xref ref-type="supplementary-material" rid="pbio-0060126-se001">Text S1</xref>), for phonologically different (left, [I]) and phonologically similar (right, [J]) pairs.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.g001" xlink:type="simple"/>
        </fig>
        <p>As expected, the diotic threshold for discriminating between phonologically similar words (<xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>B, thick red curve) was higher than that for discriminating between phonologically different words (<xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>A, thick blue curve), since this discrimination is more difficult. However, binaural benefits for both types of word pairs were similar, and reached 9–10 dB (10.2 ± 0.9 dB and 9.1 ± 0.8 dB for phonologically different and phonologically similar sets, respectively; <italic>F</italic>(1,18) = 0.84, not significant [n.s.]). In both cases, the measured binaural benefits reached the benefits predicted by the ideal listener calculations (see <xref ref-type="supplementary-material" rid="pbio-0060126-sg002">Figure S2</xref> for full details). The large binaural benefit obtained with the phonologically similar words might seem surprising given their perceived similarity. However, as shown by the ideal listener calculations, low-level representations of the phonologically similar pair are distinct enough and contain informative binaural cues (<xref ref-type="supplementary-material" rid="pbio-0060126-sg001">Figure S1</xref>).</p>
        <p>These results show that, in line with previous reports, under these simple conditions, binaural benefits reach ideal listener levels both when discriminating between phonologically similar words and when discriminating between phonologically different word pairs. These results are therefore consistent with all three views (<xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref>, Experiment I).</p>
      </sec>
      <sec id="s2c">
        <title>Experiment II—Semantic Task with No Binaural Uncertainty</title>
        <p>In Experiment II, we manipulated the nature of the behavioral task without modifying its absolute level of difficulty. Semantic processing was not necessary in Experiment I, in which listeners were asked only to discriminate between the two words. Thus, in Experiment I, listeners could have used any low-level acoustic cue that differentiated between the two stimuli. We now wanted to ensure that listeners would process word meaning, as they typically do in more ecological conditions. In Experiment II, we therefore used a semantic-association task in which participants were asked to determine whether a visually presented word is semantically related to the auditory word, which was chosen from the same two-word set used in Experiment I. Visual presentation was brief, and subjects were instructed to respond immediately after stimulus presentation, imposing temporal constraints on the behavioral task (see below). The visually presented word in each trial was randomly selected from a large word set, inducing cross-trial variability in the association required and, hence, forcing semantic processing anew in every trial. Yet, low-level acoustic information was identical to that of Experiment I since the same two-word auditory sets were used, and the diotic and dichotic configurations were administered in separate blocks (consistent).</p>
        <p>Introducing the semantic requirement did not affect task difficulty, as measured by absolute diotic thresholds, either for the phonologically different (−15.3 ± 0.8 dB for the semantic-association task; −16.9 ± 0.8 dB for the identification task; <italic>F</italic>(1,36) = 0.46, n.s.; compare <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>C and <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>A) or for the phonologically similar pair (−8.9 ± 0.5 dB for the semantic-association task; −9 ± 0.4 dB for the identification task; <italic>F</italic>(1,35) = 3.9, n.s.; compare <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>D and <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>B). However, its impact on binaural benefits greatly differed between these conditions. When the semantic task was performed with the phonologically different pair, binaural benefits remained as large as those of an ideal listener as measured in Experiment I (10.9 ± 1 dB compared with 10.2 ± 0.9 dB for the identification task; no effect of task: <italic>F</italic>(1,36) = 1.5, n.s.; <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>C and <xref ref-type="fig" rid="pbio-0060126-g001">1</xref>I). However, when the task was performed with the phonologically similar pair, dichotic thresholds were elevated, i.e., binaural benefits decreased (4.1 ± 0.9 dB compared with 9.1 ± 0.8 dB for the identification task; effect of task: <italic>F</italic>(1,36) = 5.3; <italic>p</italic> &lt; 0.03; <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>D and <xref ref-type="fig" rid="pbio-0060126-g001">1</xref>J). The differences between performance with the phonologically similar and phonologically different sets cannot be attributed to differences in response times (RTs), as those were the same for the two word pairs used (672 ± 66 ms and 670 ± 112 ms for the phonologically similar and phonologically different pairs; <italic>t</italic>-test: <italic>t</italic> = −0.13, <italic>df</italic> = 17, n.s.).</p>
        <p>The finding that binaural benefits remained equivalent to those of an ideal listener when the semantic task involved phonologically different words is in line with the unlimited view, which predicts full use of low-level information. However, this account cannot explain the failure of an ideal listener model to account for binaural benefits in the case of phonologically similar words. Since absolute diotic thresholds were not increased, there is no basis on which to assume an increase in perceptual or cognitive load. Moreover, had an increase in attentional load occurred with no impact on absolute thresholds, it should have reduced the ability to use binaural cues for both pair types. Thus, the ideal listener levels of binaural benefits for phonologically different words, but poorer benefits for phonologically similar words, are inconsistent with both the unlimited view and with the limited capacity view, but are in line with RHT predictions (<xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref>, Experiment II).</p>
      </sec>
      <sec id="s2d">
        <title>Experiment III—Word Identification with Binaural Uncertainty</title>
        <p>In Experiment III, we asked whether introducing uncertainty in the low-level binaural configuration affects the use of binaural cues. We used the same word sets and the same identification task as in Experiment I. However, the diotic and dichotic configurations were randomly interleaved across trials (“mixed”). This manipulation therefore caused the low-level binaural cues required for correct performance to vary from trial to trial. Yet, the higher-level phonological and semantic representations as well as the definition of task demands were identical to those of Experiment I.</p>
        <p>As expected, absolute diotic thresholds were not affected by this binaural variability, either for the phonologically similar pair (−9.8 ± 0.5 dB and −9 ± 0.4 dB in Experiments III and I, plotted in <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>F and <xref ref-type="fig" rid="pbio-0060126-g001">1</xref>B, respectively; effect of protocol: <italic>F</italic>(1,35) = 0.6, n.s.) or for the phonologically different pair (−16 ± 0.8 dB compared with −16.9 ± 0.8 dB, plotted in <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>E and <xref ref-type="fig" rid="pbio-0060126-g001">1</xref>A, respectively; effect of protocol: <italic>F</italic>(1,36) = 0.19, n.s.). However, the use of binaural cues for discriminating between phonologically similar words was substantially smaller than that predicted by the ideal listener model (3.6 ± 0.8 dB compared with 9.1 ± 0.8 dB for the consistent protocol; <italic>F</italic>(1,36) = 7.96, <italic>p</italic> &lt; 0.01), whereas for the phonologically different words, binaural benefits remained equivalent to those of the ideal listener (9 ± 1 dB and 10.2 ± 0.9 dB in the mixed and consistent protocols, respectively; no effect of protocol: <italic>F</italic>(1,36) = 0.5, n.s. see <xref ref-type="supplementary-material" rid="pbio-0060126-sg002">Figure S2</xref>, right).</p>
        <p>Thus, introducing variability of the informative low-level information across trials disabled listeners from reaching ideal listener levels of binaural benefits when discriminating between phonologically similar words, but not when discriminating between phonologically different words. The results of this experiment pose an even greater challenge to the limited capacity view, since not only measurable (diotic) thresholds remained the same, but also introspective task demands were exactly as in Experiment I (<xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref>, Experiment III). In post-test questionnaires, listeners failed to report any information regarding the binaural configuration, indicating that they were not aware of this low-level variability.</p>
      </sec>
      <sec id="s2e">
        <title>Experiment IV—Semantic Task with Binaural Uncertainty</title>
        <p>In Experiment IV, we combined the two types of manipulations. Subjects were asked to perform a semantic-association task (similar to the one in Experiment II) while diotic and dichotic configurations were mixed (i.e., randomly interleaved) within the block (as in Experiment III).</p>
        <p>The results of this experiment (<xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>G and <xref ref-type="fig" rid="pbio-0060126-g001">1</xref>H) were similar to those of Experiments II and III. Thus, having the two constraints together yielded the same results that each of them produced separately. Absolute diotic thresholds were similar to those of Experiment I for both phonologically different pairs (−15.2 ± 1.2 dB and −16.9 ± 0.8 dB in Experiments IV and I, respectively; interaction of task × protocol: <italic>F</italic>(1,36) = 1.4, n.s.) and for phonologically similar pairs (−7.6 ± 0.7 dB and −9 ± 0.4 dB for Experiments IV and I, respectively; no significant interaction of task × protocol: <italic>F</italic>(1,35) = 2.9, n.s.). However, binaural benefits were similar to those of Experiments II and III. They matched those of the ideal listener (as measured for Experiment III) for the phonologically different word set (10.7 ± 1.1 dB; no significant interaction of task × protocol: <italic>F</italic>(1,36) = 0.21, n.s.), and were significantly poorer than the ideal listener prediction for the phonologically similar words (4.7 ± 0.9; significant interaction of task × protocol: <italic>F</italic>(1,36) = 12.7, <italic>p</italic> &lt; 0.005; see Table I, Experiment IV). As in Experiment II, RTs were kept below 1 s, and did not differ significantly between phonologically different (722 ± 70 ms) and phonologically similar (723 ± 95 ms) pairs (<italic>t</italic>-test: <italic>t</italic> = −0.14, <italic>df</italic> = 17, n.s.).</p>
      </sec>
      <sec id="s2f">
        <title>Summary and Discussion of Study 1</title>
        <p>In Study 1, we found that, in line with the unlimited view, full use of binaural information can be obtained with both phonologically similar and phonologically different word sets. However, the unlimited view fails to predict binaural benefits for phonologically similar words when low-level cross-trial uncertainty is introduced. A similar drop in utilization of low-level information is found when semantic processing is required. These failures cannot be explained by limited capacity models either (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b045">45</xref>,<xref ref-type="bibr" rid="pbio-0060126-b046">46</xref>,<xref ref-type="bibr" rid="pbio-0060126-b071">71</xref>]), since these manipulations did not increase task difficulty, as reflected by the unchanged diotic thresholds (Experiments II–IV), and were in some cases transparent to participants (Experiment III). <xref ref-type="table" rid="pbio-0060126-t001">Table 1</xref> summarizes the predictions and results of the three views for Experiments I–IV.</p>
        <p>In order to verify that this set of results systematically characterizes the manipulations we introduced and is not specific to the two word pairs that we used in Study 1, we fully replicated Study 1 with two other word pairs, and obtained similar results (detailed in <xref ref-type="supplementary-material" rid="pbio-0060126-sg005">Figure S5</xref>).</p>
      </sec>
      <sec id="s2g">
        <title>Study 2—Manipulating Task Difficulty</title>
        <p>In Study 1, we manipulated explicit (Experiment II) and implicit (Experiment III) task requirements without modifying task difficulty, and assessed the impact of these manipulations on binaural benefits. In Study 2, we designed manipulations that were aimed at modifying task difficulty (diotic thresholds) in order to assess whether this type of change affects the use of binaural cues, as predicted by the limited capacity view.</p>
      </sec>
      <sec id="s2h">
        <title>Experiment I—Manipulating Set Size (“Cognitive Load”)</title>
        <p>In this experiment, we increased the cognitive load of the task by increasing stimulus set size. This manipulation (increasing “memory set size”) has been shown to increase the cognitive load both in the visual (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b076">76</xref>]) and in the auditory (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b077">77</xref>]) domains. We expected that diotic thresholds would increase and tested the resulting effects on binaural benefits. In the new condition with high cognitive load, the presented word on a given trial was selected from a set of ten words rather than two words. Sets were composed of either phonologically different (ten <italic>different</italic> words) or phonologically similar (five pairs of <italic>similar</italic> words) words. We used the mixed binaural protocol with randomly interleaved diotic and dichotic trials (as was used in Experiments III and IV of Study 1).</p>
        <p>As expected, increasing the set size from two to ten significantly increased diotic thresholds for both the phonologically different set (from −21.8 ± 0.6 dB to −16.7 ± 0.2 dB SNR; <xref ref-type="fig" rid="pbio-0060126-g002">Figure 2</xref>A vs. <xref ref-type="fig" rid="pbio-0060126-g002">2</xref>C) and the phonologically similar set (from −18 ± 1.1 dB to −8 ± 0.2 dB SNR; <xref ref-type="fig" rid="pbio-0060126-g002">Figure 2</xref>B vs. <xref ref-type="fig" rid="pbio-0060126-g002">2</xref>D; <italic>F</italic>(1,18) = 391, <italic>p</italic> &lt; 0.00001). A larger increase in thresholds was found for the phonologically similar set (a significant interaction of set size × similarity: <italic>F</italic>(1,18) = 71.8, <italic>p</italic> &lt; 0.00001). However, binaural benefits did not significantly change (no effect of set size <italic>F</italic>(1,18) = 0.09, n.s.). They were quite large for the phonologically different words (5.8 ± 0.3 dB and 6.6 ± 0.7 dB for set sizes of ten and two, respectively, <xref ref-type="fig" rid="pbio-0060126-g002">Figure 2</xref>E), matching those of the ideal listener (<xref ref-type="fig" rid="pbio-0060126-g002">Figure 2</xref>E, see details in <xref ref-type="supplementary-material" rid="pbio-0060126-sg003">Figure S3</xref>). They were smaller for the phonologically similar words (3 ± 0.5 dB and 2.3 ± 0.3 dB for set sizes of ten and two, respectively), and did not reach the values predicted by the ideal listener model (<xref ref-type="fig" rid="pbio-0060126-g002">Figures 2</xref>F and S3). Thus, binaural benefits reached ideal listener levels for phonologically different words, but failed to reach these levels for phonologically similar words, regardless of set size (a significant effect of phonological similarity, <italic>F</italic>(1,18) = 51, <italic>p</italic> &lt; 0.0001; no significant interaction of set size × similarity, <italic>F</italic>(1,18) = 3.7, n.s.).</p>
        <fig id="pbio-0060126-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pbio.0060126.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Results of Study 2, Experiments I</title>
            <p>Left: results using phonologically different words (blue). Right: results using phonologically similar words (red).</p>
            <p>(A–D) The dynamics of the adaptive threshold assessment as a function of trial number (averaged across subjects ± SEM, <italic>n</italic> = 25). Notations as in <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>. Vertical arrows denote binaural benefits. All measurements were done using the mixed binaural protocol. (A and B) An identification task using a set size of two words.</p>
            <p>(C and D) An identification task using a set size of ten words.</p>
            <p>(E and F) A summary of the average binaural benefits obtained in the experiment (filled shaded bars) and the benefits calculated by the ideal listener model (open bars; see <xref ref-type="supplementary-material" rid="pbio-0060126-sg003">Figure S3</xref>), for the set size 2 and set size 10 conditions.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.g002" xlink:type="simple"/>
        </fig>
        <p>The results of this experiment show that although increasing the cognitive load (by increasing the set size from two to ten) yields the expected increase in diotic identification thresholds, it does not change binaural benefits. This experiment thus clearly dissociates between task difficulty and the ability to use low-level information, and its results are therefore inconsistent with limited capacity models, but are in line with RHT predictions (<xref ref-type="table" rid="pbio-0060126-t002">Table 2</xref>, Experiment I).</p>
      </sec>
      <sec id="s2i">
        <title>Experiment II—Manipulating Success Level (“Perceptual Load”)</title>
        <p>In this experiment, we increased the perceptual load by modifying the adaptive procedure to a procedure that converges at approximately 60% rather than 80% correct [<xref ref-type="bibr" rid="pbio-0060126-b078">78</xref>]. Subjects reported that this protocol “felt more difficult,” presumably due to the lower SNRs at which most stimulus presentations occurred. We asked whether this change in difficulty affects binaural benefits. We calculated ideal listener performance for both levels of difficulty and compared them to the measured binaural benefits.</p>
        <p>First, we replicated Experiments I and III of Study 1, using the original adaptive procedure converging at 80% correct, using other word pairs (/barul/ vs. /parul/ and /dilen/ vs. /talug/, respectively). Indeed, when the task required identification and was administered with the consistent binaural protocol (with separate measurements of the diotic and dichotic thresholds, as in Study 1, Experiment I), binaural benefits reached the ideal listener levels, of 9–10 dB, for both word sets (10.5 ± 0.7 dB and 9.2 ± 0.8 dB for the phonologically different and phonologically similar pairs, respectively; <xref ref-type="fig" rid="pbio-0060126-g003">Figure 3</xref>A and <xref ref-type="fig" rid="pbio-0060126-g003">3</xref>B). However, only the phonologically different set yielded similar benefits under the mixed binaural protocol, when diotic and dichotic trials were randomly interleaved (9.2 ± 0.7 dB compared with 4.6 ± 0.6 dB obtained with the phonologically similar pair; <xref ref-type="fig" rid="pbio-0060126-g003">Figure 3</xref>C and <xref ref-type="fig" rid="pbio-0060126-g003">3</xref>D), fully replicating Experiments I and III of Study 1 (see <xref ref-type="table" rid="pbio-0060126-t002">Table 2</xref>, Experiment II).</p>
        <fig id="pbio-0060126-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pbio.0060126.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Results of Study 2, Experiments II</title>
            <p>Left: results using phonologically different word pairs (blue). Right: results using phonologically similar pairs (red).</p>
            <p>(A–H) The dynamics of the adaptive threshold assessment for identification of word pairs as a function of trial number (averaged across subjects ± SEM, <italic>n</italic> = 15). Notations as in <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>. Vertical arrows denote binaural benefits.</p>
            <p>(A and B) The adaptive protocol converging to 80% correct identification with no uncertainty (i.e., using the consistent binaural protocol).</p>
            <p>(C and D) The adaptive protocol converging to 80% correct identification with uncertainty (mixed binaural protocol).</p>
            <p>(E and F) The adaptive protocol converging to 60% correct identification with no uncertainty (consistent binaural protocol).</p>
            <p>(G and H) The adaptive protocol converging to 60% correct identification with uncertainty (mixed binaural protocol).</p>
            <p>(I and J) A summary of the average binaural benefits obtained in the experiment (filled shaded bars), and the benefits calculated by an ideal listener model (open bars; see <xref ref-type="supplementary-material" rid="pbio-0060126-sg004">Figure S4</xref>).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.g003" xlink:type="simple"/>
        </fig>
        <p>We then asked whether a similar pattern of binaural benefits would be found with the adaptive protocol converging to approximately 60% success in the task, rather than to 80% success. As expected, diotic thresholds for both phonologically similar and phonologically different pairs were lower for the 60% correct condition (<xref ref-type="fig" rid="pbio-0060126-g003">Figure 3</xref>E–<xref ref-type="fig" rid="pbio-0060126-g003">3</xref>H) compared with the 80% correct condition (analysis of variance [ANOVA]: percent correct: <italic>F</italic>(1,23) = 52.6, <italic>p</italic> &lt; 0.00001; similarity: <italic>F</italic>(1,23) = 18.2; <italic>p</italic> &lt; 0.0005; and between-subjects factor of binaural protocol: <italic>F</italic>(1,23) = 1.9, n.s.). Moreover, binaural benefits obtained with 60% correct had the same pattern, and did not significantly differ from those obtained with 80% correct (<italic>F</italic>(1,23) = 0.43, n.s.). They were large for both sets under the consistent binaural protocol (7.8 ± 0.8 dB and 9.7 ± 1 dB for phonologically different and phonologically similar pairs, respectively). Yet, only the phonologically different set yielded similar binaural benefits with the mixed binaural protocol (10.5 ± 1.5 compared with 3.2 ± 1.2 obtained with the phonologically similar pair). Thus, there was a significant effect of protocol (<italic>F</italic>(1,23) = 8.9, <italic>p</italic> &lt; 0.008) and a significant interaction between similarity and protocol (<italic>F</italic>(1,23) = 14.9, <italic>p</italic> &lt; 0.001).</p>
        <p>We calculated the ideal listener performance for these conditions as well. Discrimination between phonologically similar words under the consistent binaural protocol at 80% correct was used for calculating the variances in neural activity (these were the values used at all other ideal listener calculations in this paper). We then calculated ideal listener performance for all other conditions. The ideal listener model accounted for performance in all conditions when discriminating between phonologically different words pairs, but only for the performance in the consistent binaural protocol when discriminating between phonologically similar words (see <xref ref-type="supplementary-material" rid="pbio-0060126-sg004">Figure S4</xref>).</p>
      </sec>
      <sec id="s2j">
        <title>Summary of Study 2</title>
        <p>The two experiments of Study 2 show different manipulations that affect task difficulty and yet do not affect the use of binaural cues. The finding that increased difficulty does not decrease the use of low-level information indicates that, in contrast to the limited capacity view, attentional load is not the bottleneck for our ability to use low-level information. <xref ref-type="table" rid="pbio-0060126-t002">Table 2</xref> summarizes the predictions and results of the three views for Experiments I and II.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We tested the use of low-level information for the extraction of speech from noise, and contrasted the predictions of three theoretical frames, as listed in <xref ref-type="table" rid="pbio-0060126-t001">Tables 1</xref> and <xref ref-type="table" rid="pbio-0060126-t002">2</xref>. We found that when the set of stimuli was composed of phonologically different words, binaural benefits matched those predicted by the ideal listener model under different types of task requirements (Study 1, Experiments I and II), different levels of task difficulty (Study 2), and different binaural protocols (Study 1, Experiments III and IV). Thus, they were fully accounted for by the unlimited view, which predicts ideal listener levels of utilization under all conditions. However, when exactly the same conditions were administered with phonologically similar pairs, binaural benefits were substantially lower than those predicted by the ideal listener model under most conditions. This difference cannot be explained in terms of differences in available low-level binaural information, since the ideal listener model explicitly accounts for these differences (see <xref ref-type="supplementary-material" rid="pbio-0060126-se001">Text S1</xref>). Moreover, the binaural benefits predicted by the ideal listener model (and hence by the unlimited view) were achieved in Experiment I of Study 1 and in Experiment II of Study 2. Thus, contrary to the unlimited view (which was supported by, e.g., [<xref ref-type="bibr" rid="pbio-0060126-b032">32</xref>,<xref ref-type="bibr" rid="pbio-0060126-b033">33</xref>,<xref ref-type="bibr" rid="pbio-0060126-b038">38</xref>,<xref ref-type="bibr" rid="pbio-0060126-b039">39</xref>,<xref ref-type="bibr" rid="pbio-0060126-b046">46</xref>]), low-level information is not always fully used. The results of Study 2 further dissociate between task difficulty and the ability to use binaural cues, thus ruling out limited-capacity models of attention by which performance is expected to be limited by task difficulty per se (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b065">65</xref>,<xref ref-type="bibr" rid="pbio-0060126-b066">66</xref>,<xref ref-type="bibr" rid="pbio-0060126-b079">79</xref>,<xref ref-type="bibr" rid="pbio-0060126-b080">80</xref>]).</p>
      <p>We conclude that there are indeed constraints on the use of low-level information, but these constraints have to be formulated in terms of the properties of the stimulus set rather than in terms of behavioral difficulty, or general cognitive or attentional demands. The main difference between the two types of stimulus sets that we used is the phonological contrast between the words composing them. The acoustic contrast was large for both types of sets, and hence, at low representation levels both word sets presumably had distinct, nonoverlapping representations (see <xref ref-type="supplementary-material" rid="pbio-0060126-sg001">Figure S1</xref> and <xref ref-type="supplementary-material" rid="pbio-0060126-se001">Text S1</xref>). However, the phonological contrast was small for one type and large for the other. Hence, for the similar sets, high-level phonological representations of the words were close and largely overlapping, whereas for the different sets they were distant. We therefore conclude that the main factor determining whether the use of low-level information would reach ideal listener levels is the nature of high-level representations of the stimuli composing the stimulus set. Among the relevant theoretical accounts, only the RHT [<xref ref-type="bibr" rid="pbio-0060126-b069">69</xref>,<xref ref-type="bibr" rid="pbio-0060126-b081">81</xref>,<xref ref-type="bibr" rid="pbio-0060126-b082">82</xref>] concretely addresses the relations between the use of low-level information for perception and the underlying hierarchy of representations. Though RHT was originally derived to explain visual perception, we argue here that it also applies to the auditory system.</p>
      <sec id="s3a">
        <title>Binaural Benefits and the Reverse Hierarchy Theory</title>
        <p>The basic tenets of RHT are the presence of a local-to-global hierarchy of stimulus representations, and the presence of massive feedback connections throughout this hierarchy. Feedback connections are well established throughout the brain [<xref ref-type="bibr" rid="pbio-0060126-b083">83</xref>–<xref ref-type="bibr" rid="pbio-0060126-b085">85</xref>]. There is also an increasing amount of evidence for an auditory processing hierarchy in which lower stations represent acoustic features of sounds, whereas higher stations represent sounds more abstractly [<xref ref-type="bibr" rid="pbio-0060126-b001">1</xref>–<xref ref-type="bibr" rid="pbio-0060126-b008">8</xref>]. Along this hierarchy, acoustic fidelity is presumably gradually replaced by ecologically relevant representations [<xref ref-type="bibr" rid="pbio-0060126-b017">17</xref>–<xref ref-type="bibr" rid="pbio-0060126-b021">21</xref>]. In analogy to the visual system, low-level representations are determined by the physical (acoustic or visual) nature of the stimulus, and high-level representations converge across different low-level representations that denote the same objects or events.</p>
        <p>Anatomically, the lower, acoustic levels may roughly correspond to the stages up to, and including, the inferior colliculus (IC; e.g., [<xref ref-type="bibr" rid="pbio-0060126-b019">19</xref>]), whereas the more abstract levels, though less well understood, may correspond to cortical areas. For example, according to some recent imaging data, cortical areas ventral (“belt”) and posterior (“parabelt”) to A1, and portions of the superior temporal sulcus (STS), process temporal and spectral feature combinations that may be related to phoneme discrimination [<xref ref-type="bibr" rid="pbio-0060126-b023">23</xref>–<xref ref-type="bibr" rid="pbio-0060126-b028">28</xref>]. Cortical areas in posterior middle temporal regions [<xref ref-type="bibr" rid="pbio-0060126-b023">23</xref>,<xref ref-type="bibr" rid="pbio-0060126-b024">24</xref>,<xref ref-type="bibr" rid="pbio-0060126-b029">29</xref>–<xref ref-type="bibr" rid="pbio-0060126-b031">31</xref>,<xref ref-type="bibr" rid="pbio-0060126-b086">86</xref>] may process semantic information.</p>
        <p>RHT asserts that perception is based, by default, on stimulus representations at higher levels of the processing hierarchy, which are immediately accessible to perception. This functional structure allows rapid, and yet crude, evaluation of meaningful objects and events. The finding that binaural benefits utilize all low-level information in the case of phonologically different words is therefore consistent with RHT assertions. This is because the phonological representations of phonologically distant stimuli are as informative as the low-level representations, and therefore, can be used to achieve the performance level suggested by ideal observer models.</p>
        <p>However, in the case of phonologically similar words, the phonological representations of the two words are close and largely overlapping, resulting in information loss about the acoustic differences between them, since much of the acoustic difference between the two words is irrelevant at the phonological level and is therefore not explicitly represented (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b087">87</xref>]; see <xref ref-type="supplementary-material" rid="pbio-0060126-sg001">Figure S1</xref>). To discriminate between the two words, it is necessary to access the discriminative features that are represented at lower, acoustic representation levels. These features depend on the binaural configuration [<xref ref-type="bibr" rid="pbio-0060126-b088">88</xref>,<xref ref-type="bibr" rid="pbio-0060126-b089">89</xref>]; they are energy cues in diotic trials and correlation cues in dichotic trials, which are presumably coded in different low-level representations [<xref ref-type="bibr" rid="pbio-0060126-b088">88</xref>,<xref ref-type="bibr" rid="pbio-0060126-b090">90</xref>]. According to RHT, access to the appropriate lower level representations requires a backward search down the auditory hierarchy, since there are a number of possibly informative low-level representations. For example, there are monaural pathways through the ventral and dorsal cochlear nuclei, binaural pathways through the medial and lateral superior olivary nuclei (SOC), and pathways through the nuclei of the lateral lemniscus, all of which reach the inferior colliculus and remain partially segregated there (see [<xref ref-type="bibr" rid="pbio-0060126-b090">90</xref>]). RHT postulates that the backward search for the specific low-level neural population that best represents the discriminative acoustic features is difficult. In particular, it is gradual, and <italic>cannot</italic> be conducted on a trial-by-trial basis; RHT suggests that this search is aimed at allocating a population that is consistently informative across several trials [<xref ref-type="bibr" rid="pbio-0060126-b070">70</xref>].</p>
        <p>This logic therefore accounts for the substantially reduced binaural benefits in the case of phonologically similar pairs when binaural conditions vary in an uncertain (mixed) manner across trials: this presumably requires access to different low-level populations that vary from trial to trial. Given that identification of the most discriminative population requires several stimulus repetitions, a successful backward search can be achieved only in the consistent protocol.</p>
        <p>Listeners' limited ability to use binaural information in the semantic-association task, even when the binaural configuration is consistent across trials, can also be accounted for by this logic. Comprehending the visually presented word, which immediately follows the auditory presentation and changes on a trial-by-trial basis, requires access to higher, semantic representation levels on every trial and interferes with the backward search for informative low-level representations. Therefore, the requirement for semantic processing prevents access to low-level representations, and thus limits the use of binaural information in the case of phonologically similar words.</p>
        <p>The same RHT-based interpretation can also explain many examples from previous studies showing a tradeoff between understanding speech, i.e., processing its semantic content (based on high-level representations), and perceiving its fine details, when the latter requires direct access to appropriate low-level populations [<xref ref-type="bibr" rid="pbio-0060126-b091">91</xref>–<xref ref-type="bibr" rid="pbio-0060126-b096">96</xref>]. Similarly, auditory attention (particularly “informational masking”) studies report impaired use of low-level information when high-level confusion between the target and the masker is introduced (e.g., [<xref ref-type="bibr" rid="pbio-0060126-b058">58</xref>,<xref ref-type="bibr" rid="pbio-0060126-b059">59</xref>,<xref ref-type="bibr" rid="pbio-0060126-b062">62</xref>]). According to RHT, the low-level degree of segregation between the target and the masker could have been obtained had a gradual backward search for the informative low-level representation been applied successfully. However, in these studies, target selection is based on high-level representations (e.g., target is defined by its semantic content [<xref ref-type="bibr" rid="pbio-0060126-b059">59</xref>,<xref ref-type="bibr" rid="pbio-0060126-b060">60</xref>,<xref ref-type="bibr" rid="pbio-0060126-b062">62</xref>]). Accessing these high-level representations disables a concurrent backward search. Thus, according to RHT, similar constraints underlie the limited use of low-level information in these studies and in our semantic-association task using phonologically similar word pairs.</p>
      </sec>
      <sec id="s3b">
        <title>Reverse Hierarchy Theory, Speech Perception, and Ecology</title>
        <p>We propose that the immediate access to higher levels of the processing hierarchy allows fast word identification in an overall slow system [<xref ref-type="bibr" rid="pbio-0060126-b020">20</xref>]. Specifically, in general conversational situations, the context usually provides prior information that limits the expected word set to words that are semantically related, but are typically phonologically dissimilar. We now find that in these situations, the auditory system discriminates as well as an ideal listener regardless of the attentional load imposed by the conversation. Thus, in the majority of the daily discriminations we need, the system fully utilizes all relevant information. However, in those cases that require finer phonological discriminations, and ideal listener levels cannot be provided by the broad, abstract high-level representations, a different process occurs. Thus, for example, when the speaker might say either /day/ or /bay/ outside of context, we are likely to ask “what?”, “forcing” the speaker to repeat, perhaps at a higher signal level, which improves SNR. In parallel, an implicit attempt to apply a backward search to find more discriminative low-level representations is made. A successful backward search requires a relatively specific expectation (/day/ vs. /bay/), another repetition of the same condition, and disables concurrent semantic processing. Yet, it can provide a better discrimination, even under the less common conditions that require such access.</p>
        <p>Taken together, the auditory system seems to favor ecologically more likely conditions and yet retains flexibility for the less likely ones. Discriminations that are prevalent in natural situations are fast and still use all low-level information, whereas discriminations that are less likely to occur are either fast or use all low-level information. The results presented here, however, show that the auditory system cannot achieve both. These results are in line with our earlier results in the visual system [<xref ref-type="bibr" rid="pbio-0060126-b069">69</xref>,<xref ref-type="bibr" rid="pbio-0060126-b070">70</xref>], which showed that it too can attain low-level accuracy only under similarly limited conditions and at the cost of concurrently broad object perception. This resemblance suggests that similar defaults and tradeoffs characterize the relations between processing hierarchies and perception at the various sensory modalities.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Materials and Methods</title>
      <sec id="s4a">
        <title>Behavioral experiments—Participants.</title>
        <p>In Study 1, we tested a total of 80 subjects, whose mean age was 24 ± 3 y. In each of the four experiments (I–IV), we tested 20 subjects, ten in each type of phonological similarity (phonologically different and phonologically similar). Thus, different subjects were tested in the different experiments and different conditions, to avoid effects of task and protocol learning. In Study 2, we tested a total of 40 subjects (mean age: 24 ± 3 y): 25 subjects in Experiment I and 15 subjects in Experiment II. In this study, each subject performed all conditions in each experiment. All subjects were undergraduate students at the Hebrew University of Jerusalem. All were native Hebrew speakers, had normal hearing, and gave their informed consent for participation.</p>
      </sec>
      <sec id="s4b">
        <title>Behavioral experiments—Stimuli<italic>.</italic></title>
        <p>Stimuli were either disyllabic pseudowords (Experiment II of Study 2) or familiar Hebrew words, all recorded by the same female speaker. Each word had two different instances. Overall root mean square (RMS) and duration were equated for all words. In Study 1, the same word pairs were used in all four experiments: a phonologically similar pair, within which the difference was in a single phoneme (/tamid/ vs. /amid/), and a phonologically different pair, in which words differed in most phonemes (/tamid/ vs. /chalom/). In Experiment I of Study 2, we used the following ten-word sets: a set of ten Hebrew digits for the phonologically different condition (/efes/, /ahat/, /shtaim/, /shalosh/, /arba/, /hamesh/, /shesh/, /sheva/, /shmone/, and /tesha/), and a set of ten familiar words, composed of five phonologically similar pairs, for the phonologically similar condition (/shalom/ vs. /chalom/, /tamid/ vs. /amid/, /banuy/ vs. /panuy/, /tmuna/ vs. /tluna/, and /shanim/ vs. /panim/). For the “set size 2” condition, we used one set of digits (4 /arba/ and 9 /tesha/) for the phonologically different condition, and a pair of similar words (/shalom/ vs. /chalom/) out of the list of ten words for the phonologically similar condition. In Experiment II of Study 2, we used pairs of phonologically similar (/barul/ vs. /parul/) and phonologically different (/dilen/ vs. /talug/) pseudowords.</p>
        <p>The masking noise in both studies was speech noise [<xref ref-type="bibr" rid="pbio-0060126-b075">75</xref>], played at a constant level of 66 dB SPL (sound pressure level) to both ears. The noise was always identical in both ears. Words were played in two different configurations: diotic (<italic>N<sub>0</sub>S<sub>0</sub></italic>), in which the word was added to the noise in-phase at both ears, and dichotic (<italic>N<sub>0</sub>S<sub>π</sub></italic>), in which the word was phase-inverted in one of the ears before it was added to the noise. The duration of the noise was 1.4 s, whereas the duration of the word was 0.8 s. Thus, the noise began 0.3 s before and ended 0.3 s after the word. All stimuli were digitally played by a TDT system III signal generator (Tucker Davis Technologies), and presented to listeners through HD-256 Sennheiser headphones.</p>
      </sec>
      <sec id="s4c">
        <title>Behavioral experiments—Procedure.</title>
        <p>All experiments were conducted in a sound-attenuated room.</p>
        <p><italic>Study 1—Experiments I and III</italic> (<italic>identification</italic>). In each trial, one of two possible words was presented, masked by noise, and the listener had to press the left/right button on the computer screen whose label matched the played word. Feedback was given after every button press: a positive feedback for correct responses (happy face) and a negative feedback for incorrect responses (sad face).</p>
        <p><italic>Study 1—Experiments II and IV</italic> (<italic>semantic-association</italic>)<italic>.</italic> In each trial, one of the two words was presented in noise. Immediately following the auditory presentation, a word was visually presented on the screen for 500 ms. Listeners had to decide whether the acoustically presented word was semantically related to the visually presented word. In each trial, the visually presented word was selected from a set of 20 different words, ten of which were semantically associated to one auditory word and ten to the other word. Subjects had to press the right button (green: “match”) if it matched the auditory word and the left button (red: “no match”) if it did not. Feedback protocol was the same as for the identification experiments. Subjects performing these experiments were given a short, 20-trial training session prior to the experiment. Subjects were instructed to respond accurately and quickly. We verified that they did so by measuring their RTs (from the end of the visual presentation until button press). Average RTs were calculated for the 75 trials comprising each assessment, and were further averaged across the diotic and dichotic binaural configurations. Comparison of RTs between the relevant word pairs was performed using nonpaired two-tailed Student <italic>t</italic>-tests.</p>
        <p><italic>Study 2—Experiment I</italic> (<italic>cognitive load</italic>)<italic>.</italic> For the “set size 2” condition, identification thresholds were measured similarly to those measured in Experiments I and III of Study 1. For the “set size 10” condition, subjects heard on each trial one of the ten words, masked in noise, and were requested to report the word to the experimenter. The experimenter pressed a green or red button following a correct or incorrect response, respectively. For this condition, subjects were first given a short practice of 20 trials in which they had to correctly identify the words presented without any masking noise.</p>
        <p><italic>Study 2—Experiment II</italic> (<italic>perceptual load</italic>). Identification thresholds were measured similarly to those measured in Experiments I and III of Study 1 (see above).</p>
        <p><italic>Protocol for measuring thresholds.</italic> Thresholds for correct identification were measured in both studies using an adaptive staircase procedure [<xref ref-type="bibr" rid="pbio-0060126-b078">78</xref>]. In most experiments (excluding part of Experiment II of Study 2), thresholds were measured using a three down–one up adaptive staircase procedure, converging at 79.4% correct. In Experiment II of Study 2, the “60% correct” condition was measured using another up–down procedure, converging to 61.8% correct. In this method, signal level was decreased after at least two consecutive successes out of every three trials. Signal level was increased after any of the other five combinations of successes and errors out of every three trials.</p>
        <p>The level of the masking noise was kept constant while the presentation level of the word was adaptively varied (see left panel of <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>A). In all experiments, we used five different step sizes, beginning at 2 dB and switching to smaller steps after every four reversals (1, 0.5, 0.2, and 0.1 dB). Each experiment was composed of 75 trials for each binaural configuration. Thresholds were calculated as the arithmetic mean of signal amplitude in the last five reversals. The binaural benefit was calculated as the difference (in decibels) between the measured diotic and dichotic thresholds (illustrated in <xref ref-type="fig" rid="pbio-0060126-g001">Figure 1</xref>A). In Study 1, each subject was administered one assessment per word pair with each binaural configuration (i.e., 150 trials with each word pair). Each subject performed the same experiment twice, with two different word pairs. Both were either phonologically similar or phonologically different. In Study 2, each subject performed all conditions of each experiment (different subjects for Experiments I and II). Thus, in Experiment I of Study 2, each subject performed both set size 2 and 10 conditions, with both phonologically different and phonologically similar pairs. In Experiment II of Study 2, each subject performed both the 60% and 80% correct conditions, with both types of pseudoword pairs.</p>
        <p><italic>Binaural protocol.</italic> In Study 1, two groups of subjects (Experiments I and II) performed the experiments with a consistent binaural protocol. In these groups, diotic and dichotic configurations were measured in different experimental blocks of 75 trials each, administered in immediate succession. The order of the sessions was counterbalanced between subjects. The other two groups (Experiments III and IV) performed the task with a mixed binaural protocol. In this protocol, diotic and dichotic configurations were randomly interleaved across the block: on each trial, either a diotic or a dichotic configuration was chosen uniformly at random. The interleaved blocks consisted of 150 trials, 75 per each binaural configuration. Although the configurations were administered in an interleaved manner, the adaptive thresholds were tracked separately throughout the assessment. In Study 2, Experiment I was administered using only the mixed binaural protocol, whereas Experiment II was administered using both protocols.</p>
      </sec>
      <sec id="s4d">
        <title>Data analysis</title>
        <p><italic>Study 1.</italic> We used univariate analysis with between-subject factors of task (two levels: identification and semantic-association) and protocol (two levels: consistent and mixed), thus comparing results of Experiments I–IV. Binaural benefits and diotic thresholds were separately used as the dependent variables. Data analysis was performed separately for each word set (phonologically similar and phonologically different sets). Comparison of RTs between the relevant word pairs was performed using nonpaired two-tailed Student <italic>t</italic>-tests.</p>
        <p><italic>Study 2.</italic> In Experiment 1, we used ANOVA with within-subject factors of set size (two levels: 2 and 10) and similarity (two levels: phonologically similar and phonologically different). In Experiment 2, we used ANOVA with within-subject factors of percent correct (two levels: 60% and 80% correct) and similarity (two levels: similar and different), and a between-subject factor of protocol (two levels: consistent and mixed). Results were corrected using the Greenhouse-Geisser correction.</p>
      </sec>
      <sec id="s4e">
        <title>“Ideal listener” simulation.</title>
        <p>We used an ideal listener model to calculate performance given access to all low-level information. The model consisted of a peripheral stage ending with a binaural cross-correlator (roughly simulating the auditory system up to the level of the SOC), followed by an ideal listener under the assumption of additive Gaussian noise. The stimuli used in the behavioral experiments were filtered into narrow frequency bands, half-wave rectified, compressed, and low-pass filtered at 1,200 Hz, generating a simulated activity pattern of auditory nerve fibers (using the AIM software package [<xref ref-type="bibr" rid="pbio-0060126-b097">97</xref>]; 32 bands equally spaced along the basilar membrane between 100 and 4,000 Hz). The signals in each of these bands were used to calculate energy and binaural correlation signals, sampled every 10 ms. Close to threshold, the binaural correlation is dominated by the in-phase noise, and is maximal at an interaural delay of zero. Therefore, only this delay was used. The energy and correlation signals (as a function of frequency and time) were fed to an optimal decision maker, which compared them to stored templates of each of the possible words. Under the assumption of Gaussian noise, optimal decision consisted of selecting the template that was closer (in the least squared difference sense) to the incoming signal. Dynamic time warping (DTW) was used for computing the distance between the input signals and the stored templates, simulating templates with various temporal relations between their subparts. Consequently, the optimal decision maker had full access to the low-level pattern of activation on the one hand, and to temporally flexible representations of the stimulus set on the other hand.</p>
        <p>The Gaussian noise was assumed to be identically distributed and independent in each frequency and time bin, and its variance was determined by fitting the diotic and dichotic thresholds of a single word pair (/barul/ and /parul/) in a single condition: consistent binaural protocol with an adaptive three down–one up procedure converging to 80% correct identification. These words were used as the phonologically similar pair in Study 2, Experiment II. The estimated thresholds for Study 1 (all experiments), Study 2 Experiment I, and Study 2 Experiment II (phonologically different word pair and all other conditions for the phonologically similar pair) and the replication reported in <xref ref-type="supplementary-material" rid="pbio-0060126-sg005">Figure S5</xref> were all computed with these estimates for the variances. Thus, in all other cases (except for /barul/ and /parul/ under the consistent binaural protocol), the simulation had no free parameters. Detailed description of the simulation can be found in <xref ref-type="supplementary-material" rid="pbio-0060126-se001">Text S1</xref> online.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pbio-0060126-sg001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sg001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <title>Methods</title>
          <p>(A and B) The auditory nerve activity patterns for the left and right ears for the phonologically similar pseudowords /barul/ (A) and /parul/ (B) at a SNR of +10 dB. Patterns are calculated at 32 frequency channels between 100–4,000 Hz, at 80 time bins of 10 ms each. Note the difference in patterns, despite the similarity of the words.</p>
          <p>(C and D) The energy (left of each panel) and binaural correlation (right of each panel) templates for the same pseudowords, calculated from the auditory nerve pattern in panels (A) and (B).</p>
          <p>(E) Euclidean (left) and DTW (right) distances calculated for a pair of phonologically different (blue bars) and a pair of phonologically similar (red bars) words. Distances are normalized by standard deviations. Euclidean distances are essentially equal for both pairs, whereas the DTW distance is much smaller for the phonologically similar pair (see text). The use of the DTW distance was needed in order to account for the higher discrimination thresholds of phonologically similar word pairs.</p>
          <p>(768 KB TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0060126-sg002" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sg002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <title>Comparing Results of Ideal Listener Model to Experimental Results of Study 1</title>
          <p>Graphs compare simulated (empty bars) and experimental (filled bars) thresholds (A–D) and binaural benefits (E and F) for both the phonologically different (/tamid/ vs. /chalom/; blue bars) and phonologically similar (/tamid/ vs. /amid/; red bars) word pairs, under both consistent (left; Experiment I) and mixed (right; Experiment III) binaural protocols.</p>
          <p>(A and B) Diotic thresholds; (C and D) dichotic thresholds; (E and F) binaural benefits. Note the difference between simulated and experimental binaural benefits for the phonologically similar words, measured under the mixed binaural protocol (red bars of [F]).</p>
          <p>(2.11 MB EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0060126-sg003" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sg003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <title>Comparing Results of Ideal Listener Model to Experimental Results of Experiment I of Study 2</title>
          <p>Results are compared for set sizes of two (left) and ten (right) of phonologically different and phonologically similar word pairs. Notations as in <xref ref-type="supplementary-material" rid="pbio-0060126-sg002">Figure S2</xref>. (A and B) Diotic thresholds; (C and D) dichotic thresholds; (E and F) binaural benefits. Note the difference between simulated and measured binaural benefits for the phonologically similar pair (red bars).</p>
          <p>(2.15 MB EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0060126-sg004" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sg004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <title>Comparing Results of Ideal Listener Model to Experimental Results of Experiment II of Study 2</title>
          <p>Results are compared for performance levels of 60% and 80% correct for both phonologically different (blue bars) and phonologically similar (red bars) pseudoword pairs, measured under consistent (left) and mixed (right) binaural protocols. Notations as in <xref ref-type="supplementary-material" rid="pbio-0060126-sg002">Figure S2</xref>. (A and B) Diotic thresholds; (C and D) dichotic thresholds; (E and F) binaural benefits.</p>
          <p>(3.11 MB EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0060126-sg005" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sg005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <title>Replication of the Results of Study 1, Experiments I–IV, with Two Other Pairs of Words (/Sikum/ versus /Amid/ and /Shalom/ versus /Chalom/)</title>
          <p>Left: the phonologically different word pair (blue). Right: the phonologically similar pair (red). (A–D) Diotic and dichotic thresholds measured for Experiments I–IV of Study 1 (averaged across subjects ± standard error of the mean [SEM], <italic>n</italic> = 10 for each of the eight conditions). Binaural benefits are the differences between the two thresholds.</p>
          <p>(A and B) Identification task with no binaural uncertainty (consistent protocol; left, Experiment I) and with binaural uncertainty (mixed protocol; right, Experiment III). Diotic thresholds were similar under both protocols (phonologically different: −25 ± 1.1 dB vs. −25 ± 0.7 dB under consistent and mixed binaural protocols, respectively; effect of protocol: <italic>F</italic>(1,35) = 0.07, n.s.; phonologically similar: −17.7 ± 0.5 dB vs. −18.6 ± 1.2 dB under consistent and mixed binaural protocols, respectively; <italic>F</italic>(1,35) = 0.16, n.s.). Binaural benefits were relatively large for both pairs under the consistent protocol (phonologically different: 7.2 ± 1.5 dB; phonologically similar: 5.2 ± 0.5 dB). Note that although the binaural benefit was a bit smaller for the phonologically similar pair, it still matched that simulated by the ideal listener model (see below and [E and F]). However, under the mixed binaural protocol, binaural benefit was reduced for the phonologically similar pair (1.9 ± 0.3 dB; <italic>F</italic>(1,35) = 16.8; <italic>p</italic> &lt; 0.0005), but remained at the ideal listener level for the phonologically different pair (7.7 ± 0.5 dB, <italic>F</italic>(1,35) = 0.17, n.s.).</p>
          <p>(C and D) Semantic-association task with no uncertainty (consistent protocol; left, Experiment II) and with uncertainty (mixed protocol; right, Experiment IV). Diotic thresholds were similar to those measured in the identification task (compare to [A and B]) for both phonologically different (−23 ± 1 dB and −23 ± 0.8 dB in the consistent and mixed binaural protocols; [C]) and phonologically similar (−17.9 ± 0.8 dB and −16.3 ± 0.6 dB; no effect of task: <italic>F</italic>(1,35) = 1.5, n.s.) pairs. Statistically, there was no significant interaction of task × protocol for both the phonologically similar (<italic>F</italic>(1,35) = 2.1, n.s.) and phonologically different (<italic>F</italic>(1,35) = 0.99, n.s.) pairs. Binaural benefits remained ideal-like only for the phonologically different pair (9.6 ± 0.7 dB and 8.2 ± 0.9 dB for the consistent and mixed binaural protocols; no effect of task: <italic>F</italic>(1,35) = 2, n.s.; no significant interaction of task × protocol: <italic>F</italic>(1,35) = 0.9, n.s.). However, for the phonologically similar pair, binaural benefits decreased (2.8 ± 0.4 dB and 2.6 ± 0.4 dB for consistent and mixed protocols, respectively; effect of task: <italic>F</italic>(1,35) = 4.4; <italic>p</italic> &lt; 0.05; interaction: <italic>F</italic>(1,35) = 14.4, <italic>p</italic> &lt; 0.001).</p>
          <p>(E and F) A summary of the average binaural benefits (the difference between diotic and dichotic bars in each panel) obtained in Experiments I–IV (filled shaded bars), and the benefits calculated by an ideal listener model (open bars).</p>
          <p>(2.13 MB EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pbio-0060126-se001" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pbio.0060126.sd001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <title>Supplemental Methods and Results</title>
          <p>Description of the “ideal listener” simulation and its results on the various word stimuli used in Study 1 and 2.</p>
          <p>(64 KB DOC)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Ehud Ahissar, Shaul Hochstein, Sygal Amitay, Karen Banai, and Alit Stark for their insightful comments on the manuscript.</p>
    </ack>
    
    <glossary>
      <title>Abbreviations</title>
      <def-list>
        <def-item>
          <term>DTW</term>
          <def>
            <p>dynamic time warping</p>
          </def>
        </def-item>
        <def-item>
          <term>n.s.</term>
          <def>
            <p>not significant</p>
          </def>
        </def-item>
        <def-item>
          <term>RHT</term>
          <def>
            <p>Reverse Hierarchy Theory</p>
          </def>
        </def-item>
        <def-item>
          <term>RT</term>
          <def>
            <p>response time</p>
          </def>
        </def-item>
        <def-item>
          <term>SNR</term>
          <def>
            <p>signal to noise ratio</p>
          </def>
        </def-item>
        <def-item>
          <term>SOC</term>
          <def>
            <p>superior olivary complex</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    <ref-list>
      <title>References</title>
      <ref id="pbio-0060126-b001">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kaas</surname><given-names>JH</given-names></name><name name-style="western"><surname>Hackett</surname><given-names>TA</given-names></name></person-group>
					<year>1998</year>
					<article-title>Subdivisions of auditory cortex and levels of processing in primates.</article-title>
					<source>Audiol Neurootol</source>
					<volume>3</volume>
					<fpage>73</fpage>
					<lpage>85</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b002">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group>
					<year>1998</year>
					<article-title>Cortical processing of complex sounds.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>8</volume>
					<fpage>516</fpage>
					<lpage>521</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b003">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Romanski</surname><given-names>LM</given-names></name><name name-style="western"><surname>Bates</surname><given-names>JF</given-names></name><name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group>
					<year>1999</year>
					<article-title>Auditory belt and parabelt projections to the prefrontal cortex in the rhesus monkey.</article-title>
					<source>J Comp Neurol</source>
					<volume>403</volume>
					<fpage>141</fpage>
					<lpage>157</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b004">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wessinger</surname><given-names>CM</given-names></name><name name-style="western"><surname>VanMeter</surname><given-names>J</given-names></name><name name-style="western"><surname>Tian</surname><given-names>B</given-names></name><name name-style="western"><surname>Van Lare</surname><given-names>J</given-names></name><name name-style="western"><surname>Pekar</surname><given-names>J</given-names></name><etal/></person-group>
					<year>2001</year>
					<article-title>Hierarchical organization of the human auditory cortex revealed by functional magnetic resonance imaging.</article-title>
					<source>J Cogn Neurosci</source>
					<volume>13</volume>
					<fpage>1</fpage>
					<lpage>7</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b005">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>JD</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TD</given-names></name></person-group>
					<year>2003</year>
					<article-title>Distinct mechanisms for processing spatial sequences and pitch sequences in the human auditory brain.</article-title>
					<source>J Neurosci</source>
					<volume>23</volume>
					<fpage>5799</fpage>
					<lpage>5804</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b006">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Belin</surname><given-names>P</given-names></name></person-group>
					<year>2001</year>
					<article-title>Spectral and temporal processing in human auditory cortex.</article-title>
					<source>Cereb Cortex</source>
					<volume>11</volume>
					<fpage>946</fpage>
					<lpage>953</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b007">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zatorre</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Bouffard</surname><given-names>M</given-names></name><name name-style="western"><surname>Belin</surname><given-names>P</given-names></name></person-group>
					<year>2004</year>
					<article-title>Sensitivity to auditory object features in human temporal neocortex.</article-title>
					<source>J Neurosci</source>
					<volume>24</volume>
					<fpage>3637</fpage>
					<lpage>3642</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b008">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Griffiths</surname><given-names>TD</given-names></name><name name-style="western"><surname>Penhune</surname><given-names>V</given-names></name><name name-style="western"><surname>Peretz</surname><given-names>I</given-names></name><name name-style="western"><surname>Dean</surname><given-names>JL</given-names></name><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name><etal/></person-group>
					<year>2000</year>
					<article-title>Frontal processing and auditory perception.</article-title>
					<source>Neuroreport</source>
					<volume>11</volume>
					<fpage>919</fpage>
					<lpage>922</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b009">
        <label>9</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name><name name-style="western"><surname>Mishkin</surname><given-names>M</given-names></name></person-group>
					<year>1982</year>
					<article-title>Two cortical visual systems.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>lngle</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name><name name-style="western"><surname>MansfIeld</surname><given-names>RJW</given-names></name></person-group>
					<source>Analysis of visual behaviour</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>549</fpage>
					<lpage>586</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b010">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>1991</year>
					<article-title>Distributed hierarchical processing in the primate cerebral cortex.</article-title>
					<source>Cereb Cortex</source>
					<volume>1</volume>
					<fpage>1</fpage>
					<lpage>47</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b011">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>D</given-names></name><name name-style="western"><surname>McAlpine</surname><given-names>D</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>AR</given-names></name></person-group>
					<year>1997</year>
					<article-title>Responses of neurons in the inferior colliculus to binaural masking level difference stimuli measured by rate-versus-level functions.</article-title>
					<source>J Neurophysiol</source>
					<volume>77</volume>
					<fpage>3085</fpage>
					<lpage>3106</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b012">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Palmer</surname><given-names>AR</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>D</given-names></name><name name-style="western"><surname>McAlpine</surname><given-names>D</given-names></name></person-group>
					<year>2000</year>
					<article-title>Neural responses in the inferior colliculus to binaural masking level differences created by inverting the noise in one ear.</article-title>
					<source>J Neurophysiol</source>
					<volume>84</volume>
					<fpage>844</fpage>
					<lpage>852</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b013">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>TC</given-names></name><name name-style="western"><surname>Chan</surname><given-names>JC</given-names></name></person-group>
					<year>1990</year>
					<article-title>Interaural time sensitivity in medial superior olive of cat.</article-title>
					<source>J Neurophysiol</source>
					<volume>64</volume>
					<fpage>465</fpage>
					<lpage>488</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b014">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Batra</surname><given-names>R</given-names></name><name name-style="western"><surname>Kuwada</surname><given-names>S</given-names></name><name name-style="western"><surname>Fitzpatrick</surname><given-names>DC</given-names></name></person-group>
					<year>1997</year>
					<article-title>Sensitivity to interaural temporal disparities of low- and high-frequency neurons in the superior olivary complex. I. Heterogeneity of responses.</article-title>
					<source>J Neurophysiol</source>
					<volume>78</volume>
					<fpage>1222</fpage>
					<lpage>1236</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b015">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Batra</surname><given-names>R</given-names></name><name name-style="western"><surname>Kuwada</surname><given-names>S</given-names></name><name name-style="western"><surname>Fitzpatrick</surname><given-names>DC</given-names></name></person-group>
					<year>1997</year>
					<article-title>Sensitivity to interaural temporal disparities of low- and high-frequency neurons in the superior olivary complex. II. Coincidence detection.</article-title>
					<source>J Neurophysiol</source>
					<volume>78</volume>
					<fpage>1237</fpage>
					<lpage>1247</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b016">
        <label>16</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Blauert</surname><given-names>J</given-names></name></person-group>
					<year>1997</year>
					<source>Spatial hearing: the psychophysics of human sound localization</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">494</size>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b017">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Chechik</surname><given-names>G</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Bar-Yosef</surname><given-names>O</given-names></name><name name-style="western"><surname>Young</surname><given-names>ED</given-names></name><name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name><etal/></person-group>
					<year>2006</year>
					<article-title>Reduction of information redundancy in the ascending auditory pathway.</article-title>
					<source>Neuron</source>
					<volume>51</volume>
					<fpage>359</fpage>
					<lpage>368</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b018">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Las</surname><given-names>L</given-names></name><name name-style="western"><surname>Stern</surname><given-names>EA</given-names></name><name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name></person-group>
					<year>2005</year>
					<article-title>Representation of tone in fluctuating maskers in the ascending auditory system.</article-title>
					<source>J Neurosci</source>
					<volume>25</volume>
					<fpage>1503</fpage>
					<lpage>1513</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b019">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name></person-group>
					<year>2004</year>
					<article-title>Processing of complex stimuli and natural scenes in the auditory cortex.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>14</volume>
					<fpage>474</fpage>
					<lpage>480</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b020">
        <label>20</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name></person-group>
					<year>2006</year>
					<article-title>High-level and low-level processing in the auditory system: the role of primary auditory cortex.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Divenyi</surname><given-names>P</given-names></name><name name-style="western"><surname>Greenberg</surname><given-names>S</given-names></name><name name-style="western"><surname>Meyer</surname><given-names>G</given-names></name></person-group>
					<source>Dynamics of speech production and perception</source>
					<publisher-loc>Amsterdam</publisher-loc>
					<publisher-name>IOS Press</publisher-name>
					<fpage>343</fpage>
					<lpage>354</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b021">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name><name name-style="western"><surname>Lu</surname><given-names>T</given-names></name><name name-style="western"><surname>Snider</surname><given-names>RK</given-names></name><name name-style="western"><surname>Liang</surname><given-names>L</given-names></name></person-group>
					<year>2005</year>
					<article-title>Sustained firing in auditory cortex evoked by preferred stimuli.</article-title>
					<source>Nature</source>
					<volume>435</volume>
					<fpage>341</fpage>
					<lpage>346</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b022">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Winer</surname><given-names>JA</given-names></name><name name-style="western"><surname>Miller</surname><given-names>LM</given-names></name><name name-style="western"><surname>Lee</surname><given-names>CC</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name></person-group>
					<year>2005</year>
					<article-title>Auditory thalamocortical transformation: structure and function.</article-title>
					<source>Trends Neurosci</source>
					<volume>28</volume>
					<fpage>255</fpage>
					<lpage>263</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b023">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Binder</surname><given-names>J</given-names></name></person-group>
					<year>2000</year>
					<article-title>The new neuroanatomy of speech perception.</article-title>
					<source>Brain</source>
					<volume>123</volume>
					<issue>Pt 12</issue>
					<fpage>2371</fpage>
					<lpage>2372</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b024">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Binder</surname><given-names>JR</given-names></name><name name-style="western"><surname>Frost</surname><given-names>JA</given-names></name><name name-style="western"><surname>Hammeke</surname><given-names>TA</given-names></name><name name-style="western"><surname>Bellgowan</surname><given-names>PS</given-names></name><name name-style="western"><surname>Springer</surname><given-names>JA</given-names></name><etal/></person-group>
					<year>2000</year>
					<article-title>Human temporal lobe activation by speech and nonspeech sounds.</article-title>
					<source>Cereb Cortex</source>
					<volume>10</volume>
					<fpage>512</fpage>
					<lpage>528</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b025">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Demonet</surname><given-names>JF</given-names></name><name name-style="western"><surname>Chollet</surname><given-names>F</given-names></name><name name-style="western"><surname>Ramsay</surname><given-names>S</given-names></name><name name-style="western"><surname>Cardebat</surname><given-names>D</given-names></name><name name-style="western"><surname>Nespoulous</surname><given-names>JL</given-names></name><etal/></person-group>
					<year>1992</year>
					<article-title>The anatomy of phonological and semantic processing in normal subjects.</article-title>
					<source>Brain</source>
					<volume>115</volume>
					<issue>Pt 6</issue>
					<fpage>1753</fpage>
					<lpage>1768</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b026">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name></person-group>
					<year>2007</year>
					<article-title>The cortical organization of speech processing.</article-title>
					<source>Nat Rev Neurosci</source>
					<volume>8</volume>
					<fpage>393</fpage>
					<lpage>402</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b027">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Price</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Wise</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Warburton</surname><given-names>EA</given-names></name><name name-style="western"><surname>Moore</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Howard</surname><given-names>D</given-names></name><etal/></person-group>
					<year>1996</year>
					<article-title>Hearing and saying. The functional neuro-anatomy of auditory word processing.</article-title>
					<source>Brain</source>
					<volume>119</volume>
					<issue>Pt 3</issue>
					<fpage>919</fpage>
					<lpage>931</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b028">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Scott</surname><given-names>SK</given-names></name><name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group>
					<year>2003</year>
					<article-title>The neuroanatomical and functional organization of speech perception.</article-title>
					<source>Trends Neurosci</source>
					<volume>26</volume>
					<fpage>100</fpage>
					<lpage>107</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b029">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Binder</surname><given-names>JR</given-names></name><name name-style="western"><surname>Frost</surname><given-names>JA</given-names></name><name name-style="western"><surname>Hammeke</surname><given-names>TA</given-names></name><name name-style="western"><surname>Cox</surname><given-names>RW</given-names></name><name name-style="western"><surname>Rao</surname><given-names>SM</given-names></name><etal/></person-group>
					<year>1997</year>
					<article-title>Human brain language areas identified by functional magnetic resonance imaging.</article-title>
					<source>J Neurosci</source>
					<volume>17</volume>
					<fpage>353</fpage>
					<lpage>362</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b030">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name><name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group>
					<year>2003</year>
					<article-title>Hierarchical processing in spoken language comprehension.</article-title>
					<source>J Neurosci</source>
					<volume>23</volume>
					<fpage>3423</fpage>
					<lpage>3431</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b031">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jancke</surname><given-names>L</given-names></name><name name-style="western"><surname>Wustenberg</surname><given-names>T</given-names></name><name name-style="western"><surname>Scheich</surname><given-names>H</given-names></name><name name-style="western"><surname>Heinze</surname><given-names>HJ</given-names></name></person-group>
					<year>2002</year>
					<article-title>Phonetic perception and the temporal cortex.</article-title>
					<source>Neuroimage</source>
					<volume>15</volume>
					<fpage>733</fpage>
					<lpage>746</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b032">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dau</surname><given-names>T</given-names></name><name name-style="western"><surname>Puschel</surname><given-names>D</given-names></name><name name-style="western"><surname>Kohlrausch</surname><given-names>A</given-names></name></person-group>
					<year>1996</year>
					<article-title>A quantitative model of the “effective” signal processing in the auditory system. I. Model structure.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>99</volume>
					<fpage>3615</fpage>
					<lpage>3622</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b033">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Durlach</surname><given-names>NI</given-names></name><name name-style="western"><surname>Braida</surname><given-names>LD</given-names></name><name name-style="western"><surname>Ito</surname><given-names>Y</given-names></name></person-group>
					<year>1986</year>
					<article-title>Towards a model for discrimination of broadband signals.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>80</volume>
					<fpage>63</fpage>
					<lpage>72</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b034">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gresham</surname><given-names>LC</given-names></name><name name-style="western"><surname>Collins</surname><given-names>LM</given-names></name></person-group>
					<year>1998</year>
					<article-title>Analysis of the performance of a model-based optimal auditory signal processor.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>103</volume>
					<fpage>2520</fpage>
					<lpage>2529</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b035">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Pfafflin</surname><given-names>SM</given-names></name><name name-style="western"><surname>Mathews</surname><given-names>MV</given-names></name></person-group>
					<year>1962</year>
					<article-title>Energy-detection model for monaural auditory detection.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>34</volume>
					<fpage>1842</fpage>
					<lpage>1853</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b036">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sherwin</surname><given-names>CW</given-names></name><name name-style="western"><surname>Kodman</surname><given-names>F</given-names><suffix>Jr</suffix></name><name name-style="western"><surname>Kovaly</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Prothe</surname><given-names>WC</given-names></name><name name-style="western"><surname>Melrose</surname><given-names>J</given-names></name></person-group>
					<year>1956</year>
					<article-title>Detection of signals in noise: a comparison between the human detector and electronic detector.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>28</volume>
					<fpage>617</fpage>
					<lpage>622</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b037">
        <label>37</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Green</surname><given-names>DM</given-names></name><name name-style="western"><surname>Swets</surname><given-names>JA</given-names></name></person-group>
					<year>1966</year>
					<source>Signal detection theory and psychophysics</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Wiley</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">455</size>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b038">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bernstein</surname><given-names>LR</given-names></name><name name-style="western"><surname>Trahiotis</surname><given-names>C</given-names></name><name name-style="western"><surname>Freyman</surname><given-names>RL</given-names></name></person-group>
					<year>2006</year>
					<article-title>Binaural detection of 500-Hz tones in broadband and in narrowband masking noise: effects of signal/masker duration and forward masking fringes.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>119</volume>
					<fpage>2981</fpage>
					<lpage>2993</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b039">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bernstein</surname><given-names>LR</given-names></name><name name-style="western"><surname>van de Par</surname><given-names>S</given-names></name><name name-style="western"><surname>Trahiotis</surname><given-names>C</given-names></name></person-group>
					<year>1999</year>
					<article-title>The normalized interaural correlation: accounting for NoS pi thresholds obtained with Gaussian and “low-noise” masking noise.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>106</volume>
					<fpage>870</fpage>
					<lpage>876</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b040">
        <label>40</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Zurek</surname><given-names>PM</given-names></name></person-group>
					<year>1993</year>
					<article-title>Binaural advantages and directional effects in speech intelligibility.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Studebaker</surname><given-names>G</given-names></name><name name-style="western"><surname>Hochberg</surname><given-names>I</given-names></name></person-group>
					<source>Acoustical factors affecting hearing aid performance</source>
					<publisher-loc>Boston</publisher-loc>
					<publisher-name>Allyn &amp; Bacon</publisher-name>
					<fpage>255</fpage>
					<lpage>276</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b041">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Colburn</surname><given-names>HS</given-names></name></person-group>
					<year>1977</year>
					<article-title>Theory of binaural interaction based on auditory-nerve data. II. Detection of tones in noise.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>61</volume>
					<fpage>525</fpage>
					<lpage>533</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b042">
        <label>42</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Colburn</surname><given-names>HS</given-names></name></person-group>
					<year>1995</year>
					<article-title>Computational model of binaural processing.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Hawkins</surname><given-names>HL</given-names></name><name name-style="western"><surname>McMullen</surname><given-names>TA</given-names></name><name name-style="western"><surname>Popper</surname><given-names>AN</given-names></name><name name-style="western"><surname>Fay</surname><given-names>RR</given-names></name></person-group>
					<source>Auditory computation</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Springer-Verlag</publisher-name>
					<fpage>332</fpage>
					<lpage>400</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b043">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Durlach</surname><given-names>NI</given-names></name></person-group>
					<year>1963</year>
					<article-title>Equalization and cancellation theory of binaural masking-level differences.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>35</volume>
					<fpage>1206</fpage>
					<lpage>1218</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b044">
        <label>44</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Durlach</surname><given-names>NI</given-names></name><name name-style="western"><surname>Colburn</surname><given-names>HS</given-names></name></person-group>
					<year>1978</year>
					<article-title>Binaural phenomena.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Carterette</surname><given-names>EC</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>M</given-names></name></person-group>
					<source>Handbook of perception</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Academic Press</publisher-name>
					<fpage>405</fpage>
					<lpage>466</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b045">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Licklider</surname><given-names>J</given-names></name></person-group>
					<year>1948</year>
					<article-title>The influence of interaural phase relation upon the masking of speech by white noise.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>20</volume>
					<fpage>150</fpage>
					<lpage>159</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b046">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Levitt</surname><given-names>H</given-names></name><name name-style="western"><surname>Rabiner</surname><given-names>LR</given-names></name></person-group>
					<year>1967</year>
					<article-title>Predicting binaural gain in intelligibility and release from masking for speech.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>42</volume>
					<fpage>820</fpage>
					<lpage>829</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b047">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Verhey</surname><given-names>JL</given-names></name><name name-style="western"><surname>Dau</surname><given-names>T</given-names></name><name name-style="western"><surname>Kollmeier</surname><given-names>B</given-names></name></person-group>
					<year>1999</year>
					<article-title>Within-channel cues in comodulation masking release (CMR): experiments and model predictions using a modulation-filterbank model.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>106</volume>
					<fpage>2733</fpage>
					<lpage>2745</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b048">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>H</given-names></name><name name-style="western"><surname>Green</surname><given-names>DM</given-names></name></person-group>
					<year>1993</year>
					<article-title>Discrimination of spectral shape as a function of stimulus duration.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>93</volume>
					<fpage>957</fpage>
					<lpage>965</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b049">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>H</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>Q</given-names></name><name name-style="western"><surname>Green</surname><given-names>DM</given-names></name></person-group>
					<year>1996</year>
					<article-title>Decision rules of listeners in spectral-shape discrimination with or without signal-frequency uncertainty.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>99</volume>
					<fpage>2298</fpage>
					<lpage>2306</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b050">
        <label>50</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Delgutte</surname><given-names>B</given-names></name></person-group>
					<year>1995</year>
					<article-title>Physiological models for basic auditory percepts.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Hawkins</surname><given-names>HL</given-names></name><name name-style="western"><surname>McMullen</surname><given-names>TA</given-names></name><name name-style="western"><surname>Popper</surname><given-names>AN</given-names></name><name name-style="western"><surname>Fay</surname><given-names>RR</given-names></name></person-group>
					<source>Auditory computation</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Springer-Verlag</publisher-name>
					<fpage>157</fpage>
					<lpage>220</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b051">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Carhart</surname><given-names>R</given-names></name><name name-style="western"><surname>Tillman</surname><given-names>TW</given-names></name><name name-style="western"><surname>Dallos</surname><given-names>PJ</given-names></name></person-group>
					<year>1968</year>
					<article-title>Unmasking for pure tones and spondees: interaural phase and time disparities.</article-title>
					<source>J Speech Hear Res</source>
					<volume>11</volume>
					<fpage>722</fpage>
					<lpage>734</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b052">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name><name name-style="western"><surname>Allerhand</surname><given-names>MH</given-names></name><name name-style="western"><surname>Giguere</surname><given-names>C</given-names></name></person-group>
					<year>1995</year>
					<article-title>Time-domain modelling of peripheral auditory processing: A modular architecture and a software platform.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>98</volume>
					<fpage>1890</fpage>
					<lpage>1894</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b053">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>DK</given-names></name><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Braun</surname><given-names>J</given-names></name></person-group>
					<year>1999</year>
					<article-title>Attention activates winner-take-all competition among visual filters.</article-title>
					<source>Nat Neurosci</source>
					<volume>2</volume>
					<fpage>375</fpage>
					<lpage>381</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b054">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hafter</surname><given-names>ER</given-names></name><name name-style="western"><surname>Saberi</surname><given-names>K</given-names></name></person-group>
					<year>2001</year>
					<article-title>A level of stimulus representation model for auditory detection and attention.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>110</volume>
					<fpage>1489</fpage>
					<lpage>1497</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b055">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Muller-Gass</surname><given-names>A</given-names></name><name name-style="western"><surname>Schroger</surname><given-names>E</given-names></name></person-group>
					<year>2007</year>
					<article-title>Perceptual and cognitive task difficulty has differential effects on auditory distraction.</article-title>
					<source>Brain Res</source>
					<volume>1136</volume>
					<fpage>169</fpage>
					<lpage>177</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b056">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Treue</surname><given-names>S</given-names></name><name name-style="western"><surname>Maunsell</surname><given-names>JH</given-names></name></person-group>
					<year>1999</year>
					<article-title>Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas.</article-title>
					<source>J Neurosci</source>
					<volume>19</volume>
					<fpage>7591</fpage>
					<lpage>7602</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b057">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Yi</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Woodman</surname><given-names>GF</given-names></name><name name-style="western"><surname>Widders</surname><given-names>D</given-names></name><name name-style="western"><surname>Marois</surname><given-names>R</given-names></name><name name-style="western"><surname>Chun</surname><given-names>MM</given-names></name></person-group>
					<year>2004</year>
					<article-title>Neural fate of ignored stimuli: dissociable effects of perceptual and working memory load.</article-title>
					<source>Nat Neurosci</source>
					<volume>7</volume>
					<fpage>992</fpage>
					<lpage>996</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b058">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Durlach</surname><given-names>NI</given-names></name><name name-style="western"><surname>Mason</surname><given-names>CR</given-names></name><name name-style="western"><surname>Kidd</surname><given-names>G</given-names><suffix>Jr</suffix></name><name name-style="western"><surname>Arbogast</surname><given-names>TL</given-names></name><name name-style="western"><surname>Colburn</surname><given-names>HS</given-names></name><etal/></person-group>
					<year>2003</year>
					<article-title>Note on informational masking.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>113</volume>
					<fpage>2984</fpage>
					<lpage>2987</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b059">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Brungart</surname><given-names>DS</given-names></name></person-group>
					<year>2001</year>
					<article-title>Informational and energetic masking effects in the perception of two simultaneous talkers.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>109</volume>
					<fpage>1101</fpage>
					<lpage>1109</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b060">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Durlach</surname><given-names>NI</given-names></name><name name-style="western"><surname>Mason</surname><given-names>CR</given-names></name><name name-style="western"><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name name-style="western"><surname>Arbogast</surname><given-names>TL</given-names></name><name name-style="western"><surname>Colburn</surname><given-names>HS</given-names></name><etal/></person-group>
					<year>2003</year>
					<article-title>Informational masking: counteracting the effects of stimulus uncertainty by decreasing target-masker similarity.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>114</volume>
					<fpage>368</fpage>
					<lpage>379</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b061">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Freyman</surname><given-names>RL</given-names></name><name name-style="western"><surname>Balakrishnan</surname><given-names>U</given-names></name><name name-style="western"><surname>Helfer</surname><given-names>KS</given-names></name></person-group>
					<year>2001</year>
					<article-title>Spatial release from informational masking in speech recognition.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>109</volume>
					<fpage>2112</fpage>
					<lpage>2122</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b062">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Kidd</surname><given-names>G</given-names><suffix>Jr</suffix></name><name name-style="western"><surname>Mason</surname><given-names>CR</given-names></name><name name-style="western"><surname>Arbogast</surname><given-names>TL</given-names></name></person-group>
					<year>2002</year>
					<article-title>Similarity, uncertainty, and masking in the identification of nonspeech auditory patterns.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>111</volume>
					<fpage>1367</fpage>
					<lpage>1376</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b063">
        <label>63</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name name-style="western"><surname>Ihlefeld</surname><given-names>A</given-names></name></person-group>
					<article-title>Selective and divided attention: extracting information from simultaneous sources.</article-title>
					<comment>In:</comment>
					<source>Proceedings of ICAD 04: Tenth Meeting of the International Conference on Auditory Display</source>
					<conf-loc>Sydney, Australia,</conf-loc>
					<conf-date>6–9 July 2004;</conf-date>
					<publisher-loc>Sydney, Australia</publisher-loc>
					<comment>Available: <ext-link ext-link-type="uri" xlink:href="http://cns-web.bu.edu/~shinn/pages/pdf/ICAD_04.pdf" xlink:type="simple">cns-web.bu.edu/∼shinn/pages/pdf/ICAD_04.pdf</ext-link>. Accessed 18 April 2008.</comment>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b064">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Huang-Pollock</surname><given-names>CL</given-names></name><name name-style="western"><surname>Nigg</surname><given-names>JT</given-names></name><name name-style="western"><surname>Carr</surname><given-names>TH</given-names></name></person-group>
					<year>2005</year>
					<article-title>Deficient attention is hard to find: applying the perceptual load model of selective attention to attention deficit hyperactivity disorder subtypes.</article-title>
					<source>J Child Psychol Psychiatry</source>
					<volume>46</volume>
					<fpage>1211</fpage>
					<lpage>1218</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b065">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lavie</surname><given-names>N</given-names></name></person-group>
					<year>2005</year>
					<article-title>Distracted and confused?: selective attention under load.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>9</volume>
					<fpage>75</fpage>
					<lpage>82</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b066">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Lavie</surname><given-names>N</given-names></name><name name-style="western"><surname>Hirst</surname><given-names>A</given-names></name><name name-style="western"><surname>de Fockert</surname><given-names>JW</given-names></name><name name-style="western"><surname>Viding</surname><given-names>E</given-names></name></person-group>
					<year>2004</year>
					<article-title>Load theory of selective attention and cognitive control.</article-title>
					<source>J Exp Psychol Gen</source>
					<volume>133</volume>
					<fpage>339</fpage>
					<lpage>354</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b067">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>S</given-names></name><name name-style="western"><surname>Vuilleumier</surname><given-names>P</given-names></name><name name-style="western"><surname>Hutton</surname><given-names>C</given-names></name><name name-style="western"><surname>Maravita</surname><given-names>A</given-names></name><name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name><etal/></person-group>
					<year>2005</year>
					<article-title>Attentional load and sensory competition in human vision: modulation of fMRI responses by load at fixation during task-irrelevant stimulation in the peripheral visual field.</article-title>
					<source>Cereb Cortex</source>
					<volume>15</volume>
					<fpage>770</fpage>
					<lpage>786</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b068">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>P</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X</given-names></name></person-group>
					<year>2006</year>
					<article-title>Processing multidimensional objects under different perceptual loads: the priority of bottom-up perceptual saliency.</article-title>
					<source>Brain Res</source>
					<volume>1114</volume>
					<fpage>113</fpage>
					<lpage>124</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b069">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name></person-group>
					<year>2002</year>
					<article-title>View from the top: hierarchies and reverse hierarchies in the visual system.</article-title>
					<source>Neuron</source>
					<volume>36</volume>
					<fpage>791</fpage>
					<lpage>804</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b070">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name><name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name></person-group>
					<year>2004</year>
					<article-title>The reverse hierarchy theory of visual perceptual learning.</article-title>
					<source>Trends Cogn Sci</source>
					<volume>8</volume>
					<fpage>457</fpage>
					<lpage>464</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b071">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hirsch</surname><given-names>I</given-names></name></person-group>
					<year>1948</year>
					<article-title>The influence of interaural phase on interaural summation and inhibition.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>20</volume>
					<fpage>536</fpage>
					<lpage>544</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b072">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Levitt</surname><given-names>H</given-names></name><name name-style="western"><surname>Rabiner</surname><given-names>LR</given-names></name></person-group>
					<year>1967</year>
					<article-title>Binaural release from masking for speech and gain in intelligibility.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>42</volume>
					<fpage>601</fpage>
					<lpage>608</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b073">
        <label>73</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Johansson</surname><given-names>MS</given-names></name><name name-style="western"><surname>Arlinger</surname><given-names>SD</given-names></name></person-group>
					<year>2002</year>
					<article-title>Binaural masking level difference for speech signals in noise.</article-title>
					<source>Int J Audiol</source>
					<volume>41</volume>
					<fpage>279</fpage>
					<lpage>284</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b074">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wilson</surname><given-names>RH</given-names></name><name name-style="western"><surname>Hopkins</surname><given-names>JL</given-names></name><name name-style="western"><surname>Mance</surname><given-names>CM</given-names></name><name name-style="western"><surname>Novak</surname><given-names>RE</given-names></name></person-group>
					<year>1982</year>
					<article-title>Detection and recognition masking-level differences for the individual CID W-1 spondaic words.</article-title>
					<source>J Speech Hear Res</source>
					<volume>25</volume>
					<fpage>235</fpage>
					<lpage>242</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b075">
        <label>75</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dreschler</surname><given-names>WA</given-names></name><name name-style="western"><surname>Verschuure</surname><given-names>H</given-names></name><name name-style="western"><surname>Ludvigsen</surname><given-names>C</given-names></name><name name-style="western"><surname>Westermann</surname><given-names>S</given-names></name></person-group>
					<year>2001</year>
					<article-title>ICRA noises: artificial noise signals with speech-like spectral and temporal properties for hearing instrument assessment.</article-title>
					<source>Audiology</source>
					<volume>40</volume>
					<fpage>148</fpage>
					<lpage>157</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b076">
        <label>76</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shiffrin</surname><given-names>RM</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>W</given-names></name></person-group>
					<year>1977</year>
					<article-title>Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory.</article-title>
					<source>Psychol Rev</source>
					<volume>84</volume>
					<fpage>127</fpage>
					<lpage>190</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b077">
        <label>77</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Poltrock</surname><given-names>SE</given-names></name><name name-style="western"><surname>Lansman</surname><given-names>M</given-names></name><name name-style="western"><surname>Hunt</surname><given-names>E</given-names></name></person-group>
					<year>1982</year>
					<article-title>Automatic and controlled attention processes in auditory target detection.</article-title>
					<source>J Exp Psychol Hum Percept Perform</source>
					<volume>8</volume>
					<fpage>37</fpage>
					<lpage>45</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b078">
        <label>78</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Levitt</surname><given-names>H</given-names></name></person-group>
					<year>1971</year>
					<article-title>Transformed up-down methods in psychoacoustics.</article-title>
					<source>J Acoust Soc Am</source>
					<volume>49</volume>
					<fpage>467</fpage>
					<lpage>477</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b079">
        <label>79</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Allport</surname><given-names>DA</given-names></name></person-group>
					<year>1980</year>
					<article-title>Attention and performance.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Claxton</surname><given-names>G</given-names></name></person-group>
					<source>Cognitive psychology: new directions</source>
					<publisher-loc>London</publisher-loc>
					<publisher-name>Routledge &amp; Kegan Paul</publisher-name>
					<fpage>112</fpage>
					<lpage>153</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b080">
        <label>80</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bundesen</surname><given-names>C</given-names></name></person-group>
					<year>1990</year>
					<article-title>A theory of visual attention.</article-title>
					<source>Psychol Rev</source>
					<volume>97</volume>
					<fpage>523</fpage>
					<lpage>547</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b081">
        <label>81</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name><name name-style="western"><surname>Laiwand</surname><given-names>R</given-names></name><name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name></person-group>
					<year>2001</year>
					<article-title>Attentional demands following perceptual skill training.</article-title>
					<source>Psychol Sci</source>
					<volume>12</volume>
					<fpage>56</fpage>
					<lpage>62</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b082">
        <label>82</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name><name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name></person-group>
					<year>1997</year>
					<article-title>Task difficulty and the specificity of perceptual learning.</article-title>
					<source>Nature</source>
					<volume>387</volume>
					<fpage>401</fpage>
					<lpage>406</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b083">
        <label>83</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bajo</surname><given-names>VM</given-names></name><name name-style="western"><surname>Moore</surname><given-names>DR</given-names></name></person-group>
					<year>2005</year>
					<article-title>Descending projections from the auditory cortex to the inferior colliculus in the gerbil, Meriones unguiculatus.</article-title>
					<source>J Comp Neurol</source>
					<volume>486</volume>
					<fpage>101</fpage>
					<lpage>116</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b084">
        <label>84</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bajo</surname><given-names>VM</given-names></name><name name-style="western"><surname>Nodal</surname><given-names>FR</given-names></name><name name-style="western"><surname>Bizley</surname><given-names>JK</given-names></name><name name-style="western"><surname>Moore</surname><given-names>DR</given-names></name><name name-style="western"><surname>King</surname><given-names>AJ</given-names></name></person-group>
					<year>2006</year>
					<article-title>The ferret auditory cortex: descending projections to the inferior colliculus.</article-title>
					<source>Cereb Cortex</source>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b085">
        <label>85</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maunsell</surname><given-names>JH</given-names></name><name name-style="western"><surname>van Essen</surname><given-names>DC</given-names></name></person-group>
					<year>1983</year>
					<article-title>The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey.</article-title>
					<source>J Neurosci</source>
					<volume>3</volume>
					<fpage>2563</fpage>
					<lpage>2586</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b086">
        <label>86</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rodd</surname><given-names>JM</given-names></name><name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name><name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group>
					<year>2005</year>
					<article-title>The neural mechanisms of speech comprehension: fMRI studies of semantic ambiguity.</article-title>
					<source>Cereb Cortex</source>
					<volume>15</volume>
					<fpage>1261</fpage>
					<lpage>1269</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b087">
        <label>87</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Darwin</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Hukin</surname><given-names>RW</given-names></name></person-group>
					<year>1999</year>
					<article-title>Auditory objects of attention: the role of interaural time differences.</article-title>
					<source>J Exp Psychol Hum Percept Perform</source>
					<volume>25</volume>
					<fpage>617</fpage>
					<lpage>629</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b088">
        <label>88</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name><name name-style="western"><surname>Ahissar</surname><given-names>E</given-names></name><name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name><name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name></person-group>
					<year>1992</year>
					<article-title>Encoding of sound-source location and movement: activity of single neurons and interactions between adjacent neurons in the monkey auditory cortex.</article-title>
					<source>J Neurophysiol</source>
					<volume>67</volume>
					<fpage>203</fpage>
					<lpage>215</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b089">
        <label>89</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Stecker</surname><given-names>GC</given-names></name><name name-style="western"><surname>Harrington</surname><given-names>IA</given-names></name><name name-style="western"><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group>
					<year>2005</year>
					<article-title>Location coding by opponent neural populations in the auditory cortex.</article-title>
					<source>PLoS Biol</source>
					<volume>3</volume>
					<elocation-id>e78.</elocation-id>
					<comment>doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030078" xlink:type="simple">10.1371/journal.pbio.0030078</ext-link></comment>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b090">
        <label>90</label>
        <element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Casseday</surname><given-names>J</given-names></name><name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name><name name-style="western"><surname>Covey</surname><given-names>E</given-names></name></person-group>
					<year>2002</year>
					<article-title>The inferior colliculus: a hub for the central auditory system.</article-title>
					<comment>In:</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Oertel</surname><given-names>D</given-names></name><name name-style="western"><surname>Fay</surname><given-names>R</given-names></name><name name-style="western"><surname>Popper</surname><given-names>R</given-names></name></person-group>
					<source>Integratice functions in the mammalian auditory pathway</source>
					<publisher-loc>New York</publisher-loc>
					<publisher-name>Springer</publisher-name>
					<fpage>238</fpage>
					<lpage>318</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b091">
        <label>91</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>RM</given-names></name></person-group>
					<year>1970</year>
					<article-title>Perceptual restoration of missing speech sounds.</article-title>
					<source>Science</source>
					<volume>167</volume>
					<fpage>392</fpage>
					<lpage>393</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b092">
        <label>92</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>RM</given-names></name><name name-style="western"><surname>Bashford</surname><given-names>JA</given-names><suffix>Jr</suffix></name><name name-style="western"><surname>Healy</surname><given-names>EW</given-names></name><name name-style="western"><surname>Brubaker</surname><given-names>BS</given-names></name></person-group>
					<year>1994</year>
					<article-title>Auditory induction: reciprocal changes in alternating sounds.</article-title>
					<source>Percept Psychophys</source>
					<volume>55</volume>
					<fpage>313</fpage>
					<lpage>322</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b093">
        <label>93</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Saberi</surname><given-names>K</given-names></name><name name-style="western"><surname>Perrott</surname><given-names>DR</given-names></name></person-group>
					<year>1999</year>
					<article-title>Cognitive restoration of reversed speech.</article-title>
					<source>Nature</source>
					<volume>398</volume>
					<fpage>760</fpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b094">
        <label>94</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Salasoo</surname><given-names>A</given-names></name><name name-style="western"><surname>Pisoni</surname><given-names>AG</given-names></name></person-group>
					<year>1985</year>
					<article-title>Interaction of knowledge sources in spoken word identification.</article-title>
					<source>J Mem Lang</source>
					<volume>24</volume>
					<fpage>210</fpage>
					<lpage>231</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b095">
        <label>95</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Liberman</surname><given-names>AM</given-names></name><name name-style="western"><surname>Mattingly</surname><given-names>IG</given-names></name></person-group>
					<year>1989</year>
					<article-title>A specialization for speech perception.</article-title>
					<source>Science</source>
					<volume>243</volume>
					<fpage>489</fpage>
					<lpage>494</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b096">
        <label>96</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>GA</given-names></name><name name-style="western"><surname>Isard</surname><given-names>S</given-names></name></person-group>
					<year>1963</year>
					<article-title>Some perceptual consequences of linguistic rules.</article-title>
					<source>J Verbal Learn Verbal Behav</source>
					<volume>2</volume>
					<fpage>212</fpage>
					<lpage>228</lpage>
				</element-citation>
      </ref>
      <ref id="pbio-0060126-b097">
        <label>97</label>
        <element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Bleeck</surname><given-names>S</given-names></name><name name-style="western"><surname>Ives</surname><given-names>T</given-names></name><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name></person-group>
					<year>2004</year>
					<article-title>Aim-mat: the auditory image model in MATLAB.</article-title>
					<source>Acta Acustica</source>
					<volume>90</volume>
					<fpage>781</fpage>
					<lpage>788</lpage>
				</element-citation>
      </ref>
    </ref-list>
  </back>
</article>