<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00863</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006693</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject><subj-group><subject>Allergic diseases</subject><subj-group><subject>Food allergies</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject><subj-group><subject>Allergic diseases</subject><subj-group><subject>Food allergies</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject><subj-group><subject>Allergic diseases</subject><subj-group><subject>Food allergies</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Microbiology</subject><subj-group><subject>Medical microbiology</subject><subj-group><subject>Microbiome</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Microbial genomics</subject><subj-group><subject>Microbiome</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Microbiology</subject><subj-group><subject>Microbial genomics</subject><subj-group><subject>Microbiome</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject><subj-group><subject>Hidden Markov models</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Immunology</subject><subj-group><subject>Clinical immunology</subject><subj-group><subject>Allergies</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Deep learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Short term memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Short term memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Utilizing longitudinal microbiome taxonomic profiles to predict food allergy via Long Short-Term Memory networks</article-title>
<alt-title alt-title-type="running-head">Deep learning method to predict food allergy from longitudinal gut microbiome profiles</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0155-7412</contrib-id>
<name name-style="western">
<surname>Metwally</surname> <given-names>Ahmed A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3491-5968</contrib-id>
<name name-style="western">
<surname>Yu</surname> <given-names>Philip S.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7955-3980</contrib-id>
<name name-style="western">
<surname>Reiman</surname> <given-names>Derek</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Dai</surname> <given-names>Yang</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Finn</surname> <given-names>Patricia W.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Perkins</surname> <given-names>David L.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Bioengineering, University of Illinois at Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Computer Science, University of Illinois at Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Medicine, University of Illinois at Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Microbiology/Immunology, University of Illinois at Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Surgery, University of Illinois at Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bromberg</surname> <given-names>Yana</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Rutgers University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ametwa2@uic.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>2</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>4</day>
<month>2</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>2</issue>
<elocation-id>e1006693</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>12</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Metwally et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006693"/>
<abstract>
<p>Food allergy is usually difficult to diagnose in early life, and the inability to diagnose patients with atopic diseases at an early age may lead to severe complications. Numerous studies have suggested an association between the infant gut microbiome and development of allergy. In this work, we investigated the capacity of Long Short-Term Memory (LSTM) networks to predict food allergies in early life (0-3 years) from subjects’ longitudinal gut microbiome profiles. Using the DIABIMMUNE dataset, we show an increase in predictive power using our model compared to Hidden Markov Model, Multi-Layer Perceptron Neural Network, Support Vector Machine, Random Forest, and LASSO regression. We further evaluated whether the training of LSTM networks benefits from reduced representations of microbial features. We considered sparse autoencoder for extraction of potential latent representations in addition to standard feature selection procedures based on Minimum Redundancy Maximum Relevance (mRMR) and variance prior to the training of LSTM networks. The comprehensive evaluation reveals that LSTM networks with the mRMR selected features achieve significantly better performance compared to the other tested machine learning models.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Deep learning has become a powerful methodology to derive knowledge from the vast amount of data available from biomedical studies. However, successful applications of deep learning models to clinical studies remain challenging due to the complexity of biological data generated from the new experimental platforms as well as the inconsistency in clinical sample collection in a longitudinal study. The present work introduces our results using Long Short-Term Memory (LSTM) networks for the prediction of infant food allergy based on gut microbiome data collected from infants and young children. Our framework is empowered by the LSTM’s capacity for learning the dependency on gut microbiome longitudinal observations. The LSTM performance can be further improved by incorporating procedures of feature selection such as Minimum Redundancy Maximum Relevance. Promising performance of our new approach is demonstrated through the comparison with several traditional machine learning models. Our study suggests that LSTM network learning may be useful to other longitudinal microbiome data for prediction of a subject’s clinical outcome.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100011118</institution-id>
<institution>Office of the Vice Chancellor for Research, University of Illinois at Chicago</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0155-7412</contrib-id>
<name name-style="western">
<surname>Metwally</surname> <given-names>Ahmed A</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000050</institution-id>
<institution>National Heart, Lung, and Blood Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01HL138628</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Finn</surname> <given-names>Patricia W</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>National Institutes of Health (US)</institution>
</funding-source>
<award-id>U01AI132898</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Perkins</surname> <given-names>David</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution>UIC CCTS</institution>
</funding-source>
<award-id>UL1TR002003</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0155-7412</contrib-id>
<name name-style="western">
<surname>Metwally</surname> <given-names>Ahmed A</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work is supported by a University of Illinois at Chicago (UIC) Chancellor’s Graduate Research Fellowship awarded to AAM, UIC Center for Clinical and Translational Science (CCTS) Pre-doctoral Education for Clinical and Translational Scientists fellowship (UL1TR002003) awarded to AAM, and US National Institutes of Health under grants R01HL138628 and U01AI132898 awarded to PWF and DLP. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="2"/>
<page-count count="16"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>We used the longitudinal gut microbiome profiles from the DIABIMMUNE project (<ext-link ext-link-type="uri" xlink:href="https://pubs.broadinstitute.org/diabimmune" xlink:type="simple">https://pubs.broadinstitute.org/diabimmune</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote>
<p>This is a <italic>PLoS Computational Biology</italic> Methods paper.</p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Food sensitization and allergy are characterized by an immunologic reaction caused by exposure to antigenic products derived from food, such as Ara h1 (peanuts) or tropomyosin (shellfish). The estimated prevalence of food sensitization and allergy in the US is 8% [<xref ref-type="bibr" rid="pcbi.1006693.ref001">1</xref>], with peak prevalence between the ages of one and two years old. Food sensitization is often associated with a positive reaction to skin prick testing or by increased levels of serum specific Immunoglobulin E (IgE) to specific food antigens. Food allergy can be diagnosed by the clinical history of symptoms after food ingestion or by direct food challenge and monitoring of symptoms. Notably, not all individuals who are sensitized develop allergy, but the prevalence of food allergy is substantially higher for individuals with food sensitization. In turn, not all individuals with food allergy are sensitized to food allergens and thus serologic or skin testing alone is not sufficient for diagnosis of the food allergy. There is a need for more objective measures that have predictive value in diagnosing food allergy.</p>
<p>Food allergies are categorized into three groups: IgE-mediated, non-IgE-mediated, and mixed reactions. IgE-mediated food reactions are caused by the cross-linking of IgE on the surface of mast cells or basophils by food proteins. This leads to rapid degranulation of these cells and release of histamine which is the primary mediator of IgE symptoms including urticaria, angioedema, and anaphylaxis, which can be life-threatening. These symptoms present acutely within minutes after the ingestion of food allergen. In contrast, non-IgE-mediated and mixed reactions present in a subacute to chronic time-frame and their mechanisms are less defined. Subacute symptoms associated with non-IgE food allergy are localized to the gastrointestinal tract, such as blood/mucus filled stools or vomiting, which can lead to chronic symptoms such as weight loss, dehydration, lethargy, and failure to thrive. Mixed reactions are characterized by food allergens exacerbating IgE-mediated diseases, such as atopic dermatitis. Thus, food allergy represents a spectrum of diseases that are currently diagnosed by subjective measurements during early life.</p>
<p>The increasing incidence of food allergy and other allergic diseases has been attributed to “westernized” lifestyles, as prevalence of these diseases is substantially higher in the developed world. One over-arching theme as to why the incidence of allergy is increasing is the loss or disturbance in communities of micro-organisms that live on and in us (i.e., the microbiome). Importantly, differences in composition of the gut microbiome have been associated with food sensitization and/or IgE and non-IgE-mediated reactions [<xref ref-type="bibr" rid="pcbi.1006693.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006693.ref007">7</xref>], symptom resolution [<xref ref-type="bibr" rid="pcbi.1006693.ref008">8</xref>], and prevention and treatment [<xref ref-type="bibr" rid="pcbi.1006693.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006693.ref010">10</xref>]. This opens the door to develop more rigorous food allergy prediction models that are based on gut microbiome profiles of newborns, which could be used to predict food allergy and inform early intervention with novel therapies.</p>
<p>Longitudinal microbiome studies have been widely utilized to study disease prognosis and microbial dynamics within an ecosystem such as the gut, lung, or kidney [<xref ref-type="bibr" rid="pcbi.1006693.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1006693.ref016">16</xref>]. The exponential reduction in sequencing cost has resulted in the increase in popularity of longitudinal microbiome studies. Usually, a microbiome study is performed by sequencing the extracted DNA from a biological sample using either metagenomic shotgun (MGS) or 16S rRNA gene sequencing [<xref ref-type="bibr" rid="pcbi.1006693.ref017">17</xref>]. Metagenomic reads are processed for each sample independently to construct the taxonomic and/or functional profiles [<xref ref-type="bibr" rid="pcbi.1006693.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1006693.ref020">20</xref>]. Developing methods that predict the host phenotype from longitudinal microbiome samples comes with some challenges, e.g., variable sample collection times and an uneven number of timepoints along the subjects’ longitudinal time-line, especially when samples are collected from human subjects [<xref ref-type="bibr" rid="pcbi.1006693.ref021">21</xref>]. Hence, using standard prediction methods such as Hidden Markov Models (HMMs) [<xref ref-type="bibr" rid="pcbi.1006693.ref022">22</xref>] and Auto Regressive (AR) models [<xref ref-type="bibr" rid="pcbi.1006693.ref023">23</xref>] may not be suitable in these cases.</p>
<p>Deep learning has revolutionized various fields by offering robust strategies to extract abstract nonlinear features that are refractory to traditional methods [<xref ref-type="bibr" rid="pcbi.1006693.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1006693.ref025">25</xref>]. Multiple deep learning frameworks have been developed to predict phenotype from snapshot microbiome profiles [<xref ref-type="bibr" rid="pcbi.1006693.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006693.ref029">29</xref>]. On the other hand, a powerful approach to analyze temporal data is the Recurrent Neural Network (RNN). RNNs have shown success in different fields such as natural language processing [<xref ref-type="bibr" rid="pcbi.1006693.ref030">30</xref>] and speech recognition [<xref ref-type="bibr" rid="pcbi.1006693.ref031">31</xref>]. Although in theory, the RNN can learn dependent representation from distant events, it fails in practice due to problems with vanishing gradients [<xref ref-type="bibr" rid="pcbi.1006693.ref025">25</xref>]. This problem occurs because the error loss is back-propagated through the deep network by multiplying the derivative of the utilized activation function, which is usually the sigmoid or hyperbolic tangent. The derivative of these activation functions is usually less than one. Hence, multiplying the error loss by many of these less than one numbers causes the vanishing gradient problem. Fortunately, Long Short-Term Memory (LSTM) networks, a modified variant of the RNN, have the ability to learn temporal behavior for a time series event and to overcome the vanishing gradient problem in standard RNNs [<xref ref-type="bibr" rid="pcbi.1006693.ref032">32</xref>].</p>
<p>In this work, we present a deep learning framework to predict food allergy in infants based on longitudinal gut microbiome profiles. In our model, we use all historical samples up to timepoint <italic>t</italic> (features at timepoint <italic>t</italic> included) from each subject to predict the phenotype (food allergy vs. non-food allergy) at timepoint <italic>t</italic>. We hypothesize that adding the information from past gut microbiome profiles increases the predictive power of food allergy versus training a model with each timepoint independently. The proposed framework is based on LSTM networks and is flexible such that it can analyze subjects with a different number of timepoints.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<p>
<xref ref-type="fig" rid="pcbi.1006693.g001">Fig 1</xref> illustrates an overview of our proposed framework to predict food allergy from longitudinal gut microbiome taxonomic profiles. It consists of two main modules; a feature selection/extraction module and an LSTM network. The input to the feature selection/extraction module is a vector representing a normalized taxonomic profile of a subject’s microbial sample. The selected or extracted features are then passed to the LSTM module to learn temporal dependency between sequence profiles. Subsequently, the output from the last cell of the LSTM model is then fed to a softmax output layer where the prediction can be determined (e.g., food allergy vs. non-food allergy). The methodology of learning temporal dependency and obtaining the features’ representation is explained in details in the following sections.</p>
<fig id="pcbi.1006693.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The proposed deep learning framework, where <italic>n</italic> denotes the number of timepoints from each subject, which is not the same for all subjects.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.g001" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Learning temporal context via Long Short-Term Memory (LSTM) networks</title>
<p>The LSTM network is a variant of the vanilla RNN that has the ability to learn long sequences [<xref ref-type="bibr" rid="pcbi.1006693.ref032">32</xref>]. This ability is due to the presence of a memory, usually referred to as Cell state (<italic>C</italic>), that stores long-term information so that errors will not be propagated through distant states. LSTM networks solve the two major problems of RNNs, the vanishing and exploding gradient descent problems. It accomplishes this by using 3 gates to control the cell state: forget, input, and output gates. The forget gate (<bold>f</bold><sub><italic>t</italic></sub>) controls the amount of information that should be forgotten from the previous cell state by analyzing the current input <bold>x</bold><sub><italic>t</italic></sub> and the previously hidden state <bold>h</bold><sub><italic>t</italic>−1</sub>. The input gate (<bold>i</bold><sub><italic>t</italic></sub>) controls how much of the current input <bold>x</bold><sub><italic>t</italic></sub> should be used in training. Then, a list of new candidates (<inline-formula id="pcbi.1006693.e001"><alternatives><graphic id="pcbi.1006693.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mover accent="true"><mml:mi mathvariant="bold">C</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>) for the cell state is calculated as in <xref ref-type="disp-formula" rid="pcbi.1006693.e002">Eq 1</xref>, where <bold>W</bold><sub><italic>c</italic></sub> and <bold>U</bold><sub><italic>c</italic></sub> are weight matrices, and <bold>b</bold><sub><italic>c</italic></sub> is a bias term. Updating the cell state is performed as formulated in <xref ref-type="disp-formula" rid="pcbi.1006693.e003">Eq 2</xref>.
<disp-formula id="pcbi.1006693.e002"><alternatives><graphic id="pcbi.1006693.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">C</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula> <disp-formula id="pcbi.1006693.e003"><alternatives><graphic id="pcbi.1006693.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">f</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">i</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi mathvariant="bold">C</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>To calculate the hidden state (<bold>h</bold><sub><italic>t</italic></sub>) of an LSTM unit that is passed to the next sample in a sequence, the output of the output gate (<bold>o</bold><sub><italic>t</italic></sub>, <xref ref-type="disp-formula" rid="pcbi.1006693.e005">Eq 3</xref>) is multiplied by the squashed cell state <bold>C</bold><sub><italic>t</italic></sub> via tanh function (<xref ref-type="disp-formula" rid="pcbi.1006693.e006">Eq 4</xref>), where <bold>W</bold><sub><italic>o</italic></sub> and <bold>U</bold><sub><italic>o</italic></sub> are weight matrices, <bold>b</bold><sub><italic>o</italic></sub> is a bias term, and <inline-formula id="pcbi.1006693.e004"><alternatives><graphic id="pcbi.1006693.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.
<disp-formula id="pcbi.1006693.e005"><alternatives><graphic id="pcbi.1006693.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula> <disp-formula id="pcbi.1006693.e006"><alternatives><graphic id="pcbi.1006693.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>*</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">C</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>Since the number of samples for each subject is not identical, we extract the LSTM output of the last sample of each subjects’ sequence. This output is then fed into a dense layer with a linear activation function and with the dimension of the number of hidden neurons by the number of classes (64x2 in our case). The output of the dense layer (<bold>z</bold><sub><italic>t</italic></sub>) is then fed to a softmax function in order to give an output probability for each class (<inline-formula id="pcbi.1006693.e007"><alternatives><graphic id="pcbi.1006693.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>softmax</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>). The class with higher probability is considered the predicted class. Because our main target in this study is to predict the phenotype (food allergic vs. non-food allergic), we used the cross-entropy (<inline-formula id="pcbi.1006693.e008"><alternatives><graphic id="pcbi.1006693.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">y</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:msub><mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) between target (<bold>y</bold><sub><italic>t</italic></sub>) and predicted output (<inline-formula id="pcbi.1006693.e009"><alternatives><graphic id="pcbi.1006693.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>) in the loss function (<xref ref-type="disp-formula" rid="pcbi.1006693.e010">Eq 5</xref>), where <italic>N</italic> denotes the number of data sequences (subjects in our case). To prevent overfitting, we used L2 regularization in the loss function (<xref ref-type="disp-formula" rid="pcbi.1006693.e010">Eq 5</xref>), where <italic>J</italic> = {<italic>f</italic>, <italic>i</italic>, <italic>c</italic>, <italic>o</italic>, <italic>z</italic>}. We used the back-propagation algorithm to minimize the loss function (<italic>Loss</italic><sup>(1)</sup>).
<disp-formula id="pcbi.1006693.e010"><alternatives><graphic id="pcbi.1006693.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>Loss</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow/></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo> <mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula></p>
</sec>
<sec id="sec004">
<title>Feature selection or extraction procedures</title>
<p>Since microbial profiles usually consist of hundreds or thousands of features, it is of importance to select or extract the most meaningful features to increase the model predictive power. In this study, we investigated the incorporation of sparse autoencoder to learn a compressed latent representation of a sample’s microbial features. Besides sparse autoencoders, we tried various traditional feature selection methods, such as Minimum Redundancy Maximum Relevance (mRMR) [<xref ref-type="bibr" rid="pcbi.1006693.ref033">33</xref>], and selecting the top variable features. A major advantage of mRMR and the selection of top variable features over autoencoders is the interpretability of the selected features. On the other hand, autoencoders have the advantage of extracting compressed latent representation of high dimensional datasets.</p>
<p>Autoencoders are neural network architectures that use unsupervised learning to extract compressed latent representations from unlabeled data. <xref ref-type="fig" rid="pcbi.1006693.g002">Fig 2</xref> shows a schematic diagram of the autoencoder architecture that we used in our framework. It consists of one input layer, 3 hidden layers, and one output layer. The number of neurons in the input layer equals the number of raw features (215 in our case). The three hidden layers consist of 60, 25, and 60 neurons in that order. The number of neurons for the output layer equals the number of raw features (215 in our case). The output of layer <italic>l</italic> follows (<xref ref-type="disp-formula" rid="pcbi.1006693.e011">Eq 6</xref>), where <bold>x</bold><sub><italic>l</italic></sub> is the input feature vector, <bold>W</bold><sub><italic>l</italic></sub> is edge weight matrix, and <bold>b</bold><sub><italic>l</italic></sub> is a bias term. We used the Rectified Linear Unit (ReLU(<italic>x</italic>) = max(0, <italic>x</italic>)) as the activation function since it has been shown to help the objective function converge faster [<xref ref-type="bibr" rid="pcbi.1006693.ref034">34</xref>]. The output of the autoencoder <bold>x</bold>′ is calculated as in <xref ref-type="disp-formula" rid="pcbi.1006693.e012">Eq 7</xref>, where <italic>m</italic> denotes the number of layers of the autoencoder.
<disp-formula id="pcbi.1006693.e011"><alternatives><graphic id="pcbi.1006693.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>ReLU</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">b</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula> <disp-formula id="pcbi.1006693.e012"><alternatives><graphic id="pcbi.1006693.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>→</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow/><mml:mo>∘</mml:mo> <mml:mo>…</mml:mo> <mml:mo>.</mml:mo> <mml:mrow/><mml:mo>∘</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<fig id="pcbi.1006693.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Architecture of the autoencoder used in the proposed model.</title>
<p>It has three hidden layers with 60, 25, and 60 neurons in that order. The number of neurons for the output layer equals the number of raw features (215 in our case).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.g002" xlink:type="simple"/>
</fig>
<p>The weights and biases of the autoencoder are learned by minimizing the error between the input <bold>x</bold> and the reconstructed input <bold>x</bold>′ (<xref ref-type="disp-formula" rid="pcbi.1006693.e014">Eq 8</xref>), where <italic>n</italic> is the number of datapoints. In order to prevent over-fitting, an L2 regularization on the weights is added to the loss function with regularization parameter λ, where <italic>m</italic> denotes the number of layers (<xref ref-type="disp-formula" rid="pcbi.1006693.e014">Eq 8</xref>). Additionally, in order to enforce the sparsity on the hidden layer neurons, we added Kullback-Leibler (KL) divergence [<xref ref-type="bibr" rid="pcbi.1006693.ref035">35</xref>] to the loss function (<xref ref-type="disp-formula" rid="pcbi.1006693.e014">Eq 8</xref>), where <italic>ρ</italic> denotes the sparsity parameter, <inline-formula id="pcbi.1006693.e013"><alternatives><graphic id="pcbi.1006693.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msubsup><mml:mi>ρ</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is the average activation, meaning output value, of neuron <italic>j</italic> in the latent representation layer of the autoencoder over all datapoints, <italic>β</italic> is a parameter that controls the weight of the sparsity penalty term, and <italic>k</italic> denotes the number of neurons on the latent representation layer. KL-divergence is a standard function to measure the difference between two distributions and by putting KL-divergence into the loss function, latent representation neurons are forced to activate a small fraction of their neurons [<xref ref-type="bibr" rid="pcbi.1006693.ref036">36</xref>]. This is useful to force the neurons to learn certain patterns of data which in turn increase their specificity in performance contrasted to the more general training. The sparse autoencoder is trained via the backpropagation algorithm [<xref ref-type="bibr" rid="pcbi.1006693.ref036">36</xref>] to minimize the loss function (<italic>Loss</italic><sup>(2)</sup>). After the training is completed, the latent features are extracted and passed to the LSTM to train the model for phenotype prediction (food allergy vs. non-food allergy).
<disp-formula id="pcbi.1006693.e014"><alternatives><graphic id="pcbi.1006693.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mtext>Loss</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>‖</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>‖</mml:mo></mml:mrow></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:mrow><mml:mtext>KL</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>ρ</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
</sec>
</sec>
<sec id="sec005">
<title>Experiments</title>
<sec id="sec006">
<title>DIABIMMUNE dataset</title>
<p>In order to evaluate our proposed model, we used the longitudinal gut microbiome profiles from the DIABIMMUNE project (<ext-link ext-link-type="uri" xlink:href="https://pubs.broadinstitute.org/diabimmune" xlink:type="simple">https://pubs.broadinstitute.org/diabimmune</ext-link>), a study that aimed to characterize host-microbe immune interactions contributing to autoimmunity and allergy. These diseases were evaluated in relationship to the hygiene hypothesis, which postulates that subjects with high bacterial exposure tend to have a more powerful immune system and fewer allergic diseases [<xref ref-type="bibr" rid="pcbi.1006693.ref012">12</xref>]. To test this hypothesis, stool samples were collected from 222 infants, equally distributed among three countries (74 from Russia, 74 from Finland, and 74 from Estonia) from birth to 3 years of age. At the time of stool sample collection, various food allergen-specific IgE levels were measured for each subject, and based on a predefined threshold, infants were annotated as allergic or non-allergic to the corresponding food allergen. <xref ref-type="fig" rid="pcbi.1006693.g003">Fig 3A</xref> shows the breakdown of the number of subjects with milk, egg, or peanut allergic responses. It is clear that the prevalence of the allergies is highest in Finland and lowest in Russia with Estonia intermediate, which is aligned with the hygiene hypothesis. For the purpose of evaluating our framework, we labeled subjects as food allergy positive if they are allergic to milk, eggs, or peanuts (<xref ref-type="fig" rid="pcbi.1006693.g003">Fig 3A</xref>).</p>
<fig id="pcbi.1006693.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Summary of the DIABIMMUNE dataset.</title>
<p>(A) Number of subjects allergic to milk, egg, and peanut within the DIABIMMUNE cohort after filtering out missing data. The food-allergy group is the summation of milk, egg, and peanut allergy. (B) Time distribution of 731 samples from 195 subjects (281 samples from 71 Finnish, 197 samples from 70 Estonian, and 253 samples from 54 Russian) stool samples sequenced using MGS from the DIABIMMUNE project. The collected samples have various forms of inconsistencies, such as different numbers of samples per subject.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.g003" xlink:type="simple"/>
</fig>
<p>As a preprocessing step, we removed all samples without a food allergy class label, i.e., missing data, resulting in 731 samples from 195 subjects (281 from 71 Finnish, 197 from 70 Estonian, and 253 from 54 Russian). The 195 subjects are categorized as 68 food allergic and 127 non-food allergic. <xref ref-type="fig" rid="pcbi.1006693.g003">Fig 3B</xref> shows the distribution of timepoints of samples collected from each class from each country. As shown, these samples suffer from all forms of variability, such as a different number of subjects per phenotypic group (food allergy vs non-food allergy), a different number of samples per subject, and samples not collected at consistent timepoints. Since our main objective in this study is to explore the capacity of LSTM to learn temporal representation of the microbial features, we removed samples from subjects with less than 3 timepoints. This results in keeping 658 samples from 148 subjects (256 from 56 Finnish, 238 from 43 Russian, and 164 from 49 Estonian), 52 subjects out of the 148 are food-allergic. These samples have been sequenced using MGS sequencing. As previously described in [<xref ref-type="bibr" rid="pcbi.1006693.ref012">12</xref>], reads were quality-controlled by filtering out low-quality reads, short reads (&lt; 60 bp), and human reads. Taxonomic profiles were constructed using <italic>MetaPhlAn2</italic> [<xref ref-type="bibr" rid="pcbi.1006693.ref037">37</xref>]. The number of reads mapped to each taxonomic feature was then normalized to reads per kilobase per million (RPKM) of sample reads to correct for bias due to differences in genome size and sequencing depth. The aggregated taxonomic profiles of all 658 samples revealed 215 genera.</p>
</sec>
<sec id="sec007">
<title>Benchmarking procedure</title>
<p>We benchmarked the proposed LSTM model against other predictive models, such as Hidden Markov Model (HMM) [<xref ref-type="bibr" rid="pcbi.1006693.ref022">22</xref>], Multi-Layer Perceptron Neural Network (MLPNN) [<xref ref-type="bibr" rid="pcbi.1006693.ref038">38</xref>], Support Vector Machine (SVM) [<xref ref-type="bibr" rid="pcbi.1006693.ref039">39</xref>], Random Forests (RF) [<xref ref-type="bibr" rid="pcbi.1006693.ref040">40</xref>], and Least Absolute Shrinkage and Selection Operator (LASSO) [<xref ref-type="bibr" rid="pcbi.1006693.ref041">41</xref>]. In our evaluation, we benchmarked two aspects: (1) the effect on the prediction of using the latent representations versus features selected using traditional feature selection methods such as mRMR [<xref ref-type="bibr" rid="pcbi.1006693.ref033">33</xref>] or ranking based on variance, or using raw features, (2) the effect on the prediction of learning temporal dependency between the sequence of samples, as in LSTM or HMM, versus learning from each sample independently using methods such as MLPNN, SVM, RF, or LASSO. We used the mRMR method in the <italic>praznik</italic> R-package (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/praznik/index.html" xlink:type="simple">https://cran.r-project.org/web/packages/praznik/index.html</ext-link>) to select top 25 microbial features that distinguish food-allergic samples from non-food-allergic samples. A major advantage of the mRMR method is the interpretability of the selected features. We also selected the top 25 most variable features.</p>
<p>We trained the previously described autoencoder and LSTM models separately using <italic>Tensorflow</italic> (v1.6.0) [<xref ref-type="bibr" rid="pcbi.1006693.ref042">42</xref>]. The autoencoder consists of 215, 60, 25, 60, and 215 neurons for the input layer, first hidden layer, second hidden layer (latent representation), third hidden layer, and output layer, respectively. We trained the model with the back-propagation algorithm using <italic>Adam</italic> optimizer [<xref ref-type="bibr" rid="pcbi.1006693.ref043">43</xref>] with a learning rate of 0.001 and a batch size of 5. The model was trained for 300 epochs, and the best model was saved based on the loss value on the test set. For L2 regularization we used λ = 0.05. For the sparsity constraint, we used <italic>ρ</italic> = 0.01 and <italic>β</italic> = 3. For the LSTM module, we used 64 neurons for the LSTM hidden neurons. Similar to the autoencoder, the LSTM model was trained with <italic>Adam</italic> Optimizer with a learning rate of 0.001 and batch size = 5. Similar to LSTM, MLPNN was implemented using <italic>Tensorflow</italic> (v1.6.0). Two hidden layers were used, the first has 128 neurons and the second has 256 neurons. Output layer has two neurons (number of classes). The network was trained with a learning rate of 0.001 for 50 epochs, and the best model was retrieved based on the validation set loss to be used on the test set. RELU was used as the activation function, and the network was regularized using dropout [<xref ref-type="bibr" rid="pcbi.1006693.ref044">44</xref>] with probability of dropping neurons (<italic>p</italic> = 0.5).</p>
<p>To benchmark our LSTM model against another model which can incorporate sequential data, we trained a hidden Markov model (HMM) using a Gaussian Mixture Model (GMM). We trained the HMM using the <italic>hmmlearn</italic> python library and trained in an unsupervised manner using two states and four mixtures over 100 iterations. The state labels were determined based on which labeling yielded the highest accuracy. Although GMM’s can be trained in a supervised fashion, they are usually used for clustering and are much more flexible than more classical clustering methods (e.g. k-means). The HMM models were first order and contained two states. The two methods are merged by forcing the emission probabilities of the HMM to follow a GMM. The RF, SVM, and LASSO models were all trained using Python’s <italic>scikit-learn</italic> package (<ext-link ext-link-type="uri" xlink:href="http://scikit-learn.org" xlink:type="simple">http://scikit-learn.org</ext-link>). The RF models were trained by setting a maximum of 500 trees. All other parameters were left as the default values. The SVM models were trained using an exhaustive grid search with 5-fold cross-validation over the linear and Gaussian kernels, using the parameters 1, 10, 100, 1000 for error terms and the parameters 0.001, 0.0001 for <italic>γ</italic> values in Gaussian kernels. The LASSO models were trained using iterative fitting with 5-fold cross-validation for the error term <italic>α</italic> over a set of 50 numbers, evenly log-spaced between 4-10 and 0.5-10.</p>
<p>For the HMM model, the final state of the sequence was used for evaluation. In the case of RF, SVM, or LASSO, we trained the classifiers on the samples of the last timepoint of each subject because these methods do not have the capacity to learn temporal representation. This strategy ensures a fair comparison with LSTM, which uses all of a subject’s timepoints to predict the phenotype of the last timepoint.</p>
</sec>
<sec id="sec008">
<title>Evaluation metrics</title>
<p>To ensure robustness of the performance evaluation, we divided the data into test set (20%) and performed 10-fold cross-validation on the remaining 80% of the dataset. We repeated this process 10 times and each time we shuffled the dataset before dividing the data. Various performance metrics were calculated such as <inline-formula id="pcbi.1006693.e015"><alternatives><graphic id="pcbi.1006693.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mi>S</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>v</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>y</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>P</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006693.e016"><alternatives><graphic id="pcbi.1006693.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>S</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>y</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mi>N</mml:mi></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:mi>N</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. These metrics have been used to obtain the area under the Receiver Operating Characteristic (auROC) curve. We also measured Matthew Correlation Coefficient (<inline-formula id="pcbi.1006693.e017"><alternatives><graphic id="pcbi.1006693.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006693.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mi>M</mml:mi> <mml:mi>C</mml:mi> <mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mi>P</mml:mi> <mml:mo>*</mml:mo> <mml:mi>T</mml:mi> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mi>F</mml:mi> <mml:mi>P</mml:mi> <mml:mo>*</mml:mo> <mml:mi>F</mml:mi> <mml:mi>N</mml:mi></mml:mrow> <mml:msqrt><mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mi>P</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>P</mml:mi> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mi>P</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mi>N</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>P</mml:mi> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mi>N</mml:mi> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>) on the test set after each 10-fold. The MCC criterion is a way of evaluating classifiers considering all true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN). A score of 1 indicates a perfect classification and a score of -1 indicates a completely incorrect classification.</p>
<p>Given the fact that our dataset is imbalanced (52 allergic and 96 non-allergic), we up-sampled samples from allergic infants and down-sampled samples from non-allergic infants. However, the change of the predictive power of the used classifiers was not significant (p-value&lt;0.5, Mann-Whitney test), so we report here the results without up/down sampling procedures.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Results and discussion</title>
<p>Firstly, we measured the loss function for several autoencoder architectures in order to select the most suitable architecture for our data (<xref ref-type="table" rid="pcbi.1006693.t001">Table 1</xref>). For each architecture, the model with the lowest loss on the validation set was saved and then tested on the test set. Many of the tested architectures gave a similar loss on the validation set, however loss on test set varies. It tends to be small either when the autoencoder has many parameters (more neurons) or when the ratio between the number of neurons in the first layer to the latent layer is not high. Otherwise, loss on test set is high because either the latent layer does not have the capacity to learn a compressed representation (as in 50x25x50 or 50x15x50), or the first hidden layer does not learn enough compressed features to be passed to the latent layer (as in 100x25x100). Our criterion for architecture selection was that the autoencoder should yield small loss on test set while using as few parameters as possible. Giving this criterion, the autoencoder with 60x25x60 was our choice. <xref ref-type="fig" rid="pcbi.1006693.g004">Fig 4A</xref> shows the trajectory of the loss on the training set and validation set. The loss progressively decreases by more epochs until it stabilizes after 270 epochs. The loss never reaches zero due to the regularization we put in the autoencoder loss function to prevent over-fitting.</p>
<table-wrap id="pcbi.1006693.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.t001</object-id>
<label>Table 1</label>
<caption>
<title>Losses (training, validation, and testing) of the best trained model for several autoencoder architectures.</title>
<p>The 60x25x60 architecture is the chosen model because it achieves small loss while using the least number of parameters.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006693.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Architecture</th>
<th align="center">Training</th>
<th align="center">Validation</th>
<th align="center">Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">150x50x150</td>
<td align="char" char=".">16.95</td>
<td align="char" char=".">17.26</td>
<td align="char" char=".">16.90</td>
</tr>
<tr>
<td align="center">150x25x150</td>
<td align="char" char=".">16.90</td>
<td align="char" char=".">18.13</td>
<td align="char" char=".">16.59</td>
</tr>
<tr>
<td align="center">150x15150</td>
<td align="char" char=".">16.87</td>
<td align="char" char=".">17.31</td>
<td align="char" char=".">17.74</td>
</tr>
<tr>
<td align="center">150x10x150</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">125x50x125</td>
<td align="char" char=".">17.01</td>
<td align="char" char=".">16.74</td>
<td align="char" char=".">17.35</td>
</tr>
<tr>
<td align="center">125x25x125</td>
<td align="char" char=".">16.98</td>
<td align="char" char=".">17.52</td>
<td align="char" char=".">17.31</td>
</tr>
<tr>
<td align="center">125x15x125</td>
<td align="char" char=".">16.91</td>
<td align="char" char=".">16.54</td>
<td align="char" char=".">17.21</td>
</tr>
<tr>
<td align="center">125x10x125</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">100x50x100</td>
<td align="char" char=".">16.83</td>
<td align="char" char=".">19.00</td>
<td align="char" char=".">16.09</td>
</tr>
<tr>
<td align="center">100x25x100</td>
<td align="char" char=".">16.95</td>
<td align="char" char=".">17.58</td>
<td align="center">160.9</td>
</tr>
<tr>
<td align="center">100x15x100</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">100x10x100</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">75x50x75</td>
<td align="char" char=".">16.07</td>
<td align="char" char=".">17.96</td>
<td align="char" char=".">16.30</td>
</tr>
<tr>
<td align="center">75x25x75</td>
<td align="char" char=".">17.27</td>
<td align="char" char=".">14.56</td>
<td align="center">112.0</td>
</tr>
<tr>
<td align="center">75x15x75</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">75x10x75</td>
<td align="center" colspan="3">Model does not converge</td>
</tr>
<tr>
<td align="center">60x50x60</td>
<td align="char" char=".">16.80</td>
<td align="char" char=".">17.55</td>
<td align="char" char=".">17.06</td>
</tr>
<tr>
<td align="center">60x25x60</td>
<td align="char" char=".">16.18</td>
<td align="char" char=".">16.79</td>
<td align="char" char=".">17.04</td>
</tr>
<tr>
<td align="center">60x15x60</td>
<td align="char" char=".">16.75</td>
<td align="char" char=".">16.83</td>
<td align="char" char=".">50.90</td>
</tr>
<tr>
<td align="center">60x10x60</td>
<td align="char" char=".">16.60</td>
<td align="char" char=".">35.70</td>
<td align="char" char=".">51.99</td>
</tr>
<tr>
<td align="center">50x50x50</td>
<td align="char" char=".">15.87</td>
<td align="char" char=".">17.34</td>
<td align="char" char=".">35.05</td>
</tr>
<tr>
<td align="center">50x25x50</td>
<td align="char" char=".">17.09</td>
<td align="char" char=".">16.02</td>
<td align="char" char=".">34.00</td>
</tr>
<tr>
<td align="center">50x15x50</td>
<td align="char" char=".">17.09</td>
<td align="char" char=".">16.08</td>
<td align="center">207.0</td>
</tr>
<tr>
<td align="center">50x10x50</td>
<td align="char" char=".">17.19</td>
<td align="char" char=".">16.55</td>
<td align="center">263.1</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1006693.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>(A) Trajectory of training and validation loss function (<italic>Loss</italic><sup>(2)</sup>) of the autoencoder with 60x25x60 architecture. The lowest loss on validation set was 16.79 and when the test set applied to this best model, the loss was 17.04. (B) Trajectory of training and validation loss function (<italic>Loss</italic><sup>(1)</sup>) of the LSTM. The lowest loss on validation set was 0.91 and when the test set applied to this best model, the loss was 1.02</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.g004" xlink:type="simple"/>
</fig>
<p>As highlighted previously, features selected using mRMR or variance-based methods have interpretability advantages over features extracted by the autoencoder. For example, the top selected genus by mRMR is <italic>Faecalibacterium</italic>, which is more abundant in the non-food allergy group, which is consistent with studies that associate <italic>Faecalibacterium</italic> with allergies, and specifically food-allergy [<xref ref-type="bibr" rid="pcbi.1006693.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006693.ref046">46</xref>]. Other top selected features includes <italic>Burkholderiales</italic>, <italic>Neisseria</italic>, and <italic>Enterococcus</italic>. On the other hand, features with highest variability include: <italic>Bifidobacterium</italic>, <italic>Bacteroides</italic>, <italic>Prevotella</italic>, <italic>Escherichia</italic>, <italic>Klebsiella</italic>, and <italic>Faecalibacterium</italic>.</p>
<p>Subsequently, we evaluated how our proposed LSTM model compares with the commonly used classification methods. <xref ref-type="table" rid="pcbi.1006693.t002">Table 2</xref> summarizes the evaluation of auROC and MCC on the test set using 10 times 10-fold cross-validation experiments. In this table, we evaluated six classifiers; LSTM, HMM, MLPNN, RF, SVM, and LASSO. For each classifier, we evaluated four types of input features; latent features which extracted from the trained autoencoder, 25 features selected by mRMR method, 25 most variable features, and 215 raw taxonomic profile features. P-values are calculated using Mann-Whitney U test between LSTM-mRMR-25 versus each corresponding model since LSTM-mRMR-25 achieved the highest auROC and MCC among all benchmarked models.</p>
<table-wrap id="pcbi.1006693.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006693.t002</object-id>
<label>Table 2</label>
<caption>
<title>Evaluation of auROC and MCC for the proposed LSTM model versus baseline models.</title>
<p>In this table, we evaluated six classifiers; LSTM, HMM, MLPNN, RF, SVM, and LASSO. For each classifier, we evaluated four types of input features; latent features which extracted from the trained autoencoder, 25 features selected by mRMR method, 25 most variable features, and 215 raw taxonomic profile features. The auROC and MCC results shown below are the average of auROC and MCC measured on the test set. The experiments were repeated 10 times and samples were shuffled after each 10-fold cross-validation to test the robustness of each classifier. P-values were calculated using Mann-Whitney U test between LSTM-mRMR-25 versus each corresponding method.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006693.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006693.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Features</th>
<th align="center">auROC [mean (sd)]</th>
<th align="center">auROC [p-value]</th>
<th align="center">MCC [mean (sd)]</th>
<th align="center">MCC [p-value]</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="4"><bold>LSTM</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.67 (0.07)</td>
<td align="center">4.20E-01</td>
<td align="center">0.29 (0.22)</td>
<td align="center">3.47E-03</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.69 (0.09)</td>
<td align="center">1</td>
<td align="center">0.40 (0.19)</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.61 (0.12)</td>
<td align="center">7.00E-02</td>
<td align="center">0.23 (0.20)</td>
<td align="center">9.51E-04</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.65 (0.10)</td>
<td align="center">1.70E-01</td>
<td align="center">0.37 (0.18)</td>
<td align="center">5.10E-02</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>HMM</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.52 (0.06)</td>
<td align="center">5.99E-22</td>
<td align="center">0.14 (0.20)</td>
<td align="center">2.25E-12</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.55 (0.08)</td>
<td align="center">8.92E-16</td>
<td align="center">0.21 (0.19)</td>
<td align="center">3.67E-07</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.52 (0.07)</td>
<td align="center">4.68E-22</td>
<td align="center">0.16 (0.15)</td>
<td align="center">1.18E-10</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.50 (0.07)</td>
<td align="center">5.63E-25</td>
<td align="center">0.06 (0.21)</td>
<td align="center">7.66E-17</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>MLPNN</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.54 (0.08)</td>
<td align="center">5.50E-04</td>
<td align="center">0.11 (0.13)</td>
<td align="center">5.93E-06</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.57 (0.06)</td>
<td align="center">1.95E-03</td>
<td align="center">0.18 (0.11)</td>
<td align="center">5.95E-03</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.54 (0.03)</td>
<td align="center">6.66E-05</td>
<td align="center">0.05 (0.07)</td>
<td align="center">1.13E-05</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.63 (0.05)</td>
<td align="center">4.93E-02</td>
<td align="center">0.12 (0.12)</td>
<td align="center">1.13E-03</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>RF</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.44 (0.14)</td>
<td align="center">3.00E-22</td>
<td align="center">0.05 (0.25)</td>
<td align="center">1.99E-15</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.63 (0.13)</td>
<td align="center">6.73E-02</td>
<td align="center">0.23 (0.23)</td>
<td align="center">6.00E-03</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.57 (0.12)</td>
<td align="center">1.08E-07</td>
<td align="center">0.08 (0.23)</td>
<td align="center">2.31E-15</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.59 (0.13)</td>
<td align="center">2.00E-03</td>
<td align="center">0.14 (0.23)</td>
<td align="center">2.71E-11</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>SVM</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.52 (0.15)</td>
<td align="center">6.49E-12</td>
<td align="center">0.06 (0.21)</td>
<td align="center">9.20E-31</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.57 (0.13)</td>
<td align="center">1.46E-07</td>
<td align="center">-0.04 (0.23)</td>
<td align="center">1.64E-28</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.46 (0.15)</td>
<td align="center">1.45E-20</td>
<td align="center">0.03 (0.22)</td>
<td align="center">9.20E-31</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.52 (0.14)</td>
<td align="center">7.21E-13</td>
<td align="center">-0.04 (0.23)</td>
<td align="center">1.23E-27</td>
</tr>
<tr>
<td align="center" rowspan="4"><bold>LASSO</bold></td>
<td align="center"><bold>latent-25</bold></td>
<td align="center">0.51 (0.14)</td>
<td align="center">1.51E-14</td>
<td align="center">-0.04 (0.21)</td>
<td align="center">5.90E-27</td>
</tr>
<tr>
<td align="center"><bold>mRMR-25</bold></td>
<td align="center">0.59 (0.12)</td>
<td align="center">2.00E-04</td>
<td align="center">0.01 (0.23)</td>
<td align="center">1.45E-26</td>
</tr>
<tr>
<td align="center"><bold>var-25</bold></td>
<td align="center">0.59 (0.13)</td>
<td align="center">3.20E-05</td>
<td align="center">-0.17 (0.01)</td>
<td align="center">2.36E-30</td>
</tr>
<tr>
<td align="center"><bold>raw</bold></td>
<td align="center">0.57 (0.14)</td>
<td align="center">1.00E-03</td>
<td align="center">-0.17 (0.00)</td>
<td align="center">1.66E-30</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In general, LSTM outperformed the other classifiers, supporting the concept that learning a sequence of events increases the prediction power. While HMM has the capability to learn time sequences, it fails to work on the current dataset mainly because the DIABIMMUNE, similar to all human longitudinal studies, suffers from sampling inconsistency, while sampling consistency is required by the HMM to learn enough information. In general, the variability in auROC of LSTM and HMM is smaller compared to methods that do not learn time dependency. LSTM trained with the top 25 features selected by mRMR gave the best performance measured by auROC and MCC. Generally, latent features learned by autoencoder are not associated with high performance in all tested models. One reason for the low performance of the latent representation is that the way they have learned to extract the compressed representation from microbial profiles was without considering the phenotype in the loss function. All other classifiers that do not consider time sequence data perform poorly compared to LSTM, supporting the concept that learning a sequence of events increases the prediction power. Additionally, we have investigated the usage of 50 latent features, 50 features selected by mRMR, and top 50 features ranked by variance, but they all show a decrease in prdictive performance compared to the 25 features selected by each of the aforementioned methods.</p>
<sec id="sec010">
<title>Execution time</title>
<p>The execution time of training LSTM on different types of features is comparable and depends on the number of epochs and batch size. 10-times 10-fold cross validation took, on average, 92 minutes on the DIABIMMUNE dataset given all the parameters stated above. On the other hand, on average, HMM took 9 minutes, MLPNN took 33 minutes, SVM took 12.5 minutes, RF took 7 minutes, and LASSO took 1.5 minutes. The prediction time is linear with respect to the size of the test set. The evaluation was conducted on a MAC machine with 2.5 GHz Intel Core i7 processor and 16 GB 1600 MHz RAM. No GPU was used for training of the LSTM.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Conclusion</title>
<p>Food allergy is usually difficult to diagnose at young ages, and the inability to diagnose patients with this atopic disease at an earlier age may lead to severe complications due to the lack of treatment. In this work, we have developed a deep learning framework that has the capacity to predict food allergy from longitudinal gut microbiome profiles. The framework is based on Long Short-Term Memory (LSTM) networks with a feature selection procedure. To effectively choose features prior to LSTM training, we have evaluated four procedures, including sparse autoencoder for extracting potential latent representation in microbiome, Minimum Redundancy Maximum Relevance, ranking based on variance, and raw taxonomic profile features. We tested the framework on the DIABIMMUNE dataset, a study that aimed to characterize host-microbe interactions contributing to autoimmunity and allergy. Our results demonstrate the increase in predictive power of the LSTM model compared to Hidden Markov Model, Multi-Layer Perceptron Neural Network, Random Forest, SVM, and LASSO regression. The LSTM models coupled with the mRMR selected features achieved the best performance in our evaluation.</p>
<p>Although our deep learning framework shows the potential to predict allergic phenotype from a sequence of gut microbiome profiles and outperforms other classical methods, it does not reach a prediction level for optimal clinical utilization. This is mainly due to the nature of the training dataset that we used to train our model. The DIABIMMUNE dataset is small and each subject has a few timepoints (6 on average). With the further reduction in sequencing costs, we anticipate that data from multiple large-scale longitudinal microbiome projects will be available which in turn could be used to train models like ours for better prediction power. As more data becomes available, there is also the potential to explore transfer learning, where the information found in models trained on one task is used to improve the prediction of models trained on another task. This has the potential of allowing longitudinal microbiome profiling that can be used for predicting diseases before they are clinically apparent, especially for autoimmune diseases that are linked to host microbiome interactions, such as asthma and diabetes mellitus. The project source code is publicly available on (<ext-link ext-link-type="uri" xlink:href="https://github.com/aametwally/FoodAllergyPredictor" xlink:type="simple">https://github.com/aametwally/FoodAllergyPredictor</ext-link>).</p>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Ozge Aktas and Benjamin Turturice from University of Illinois at Chicago for several fruitful discussions about food allergy.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006693.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gupta</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Springston</surname> <given-names>EE</given-names></name>, <name name-style="western"><surname>Warrier</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kumar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pongracic</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The prevalence, severity, and distribution of childhood food allergy in the United States</article-title>. <source>Pediatrics</source>. <year>2011</year>;<volume>128</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1542/peds.2011-0204" xlink:type="simple">10.1542/peds.2011-0204</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stefka</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Feehley</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tripathi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Qiu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McCoy</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mazmanian</surname> <given-names>SK</given-names></name>, <etal>et al</etal>. <article-title>Commensal bacteria protect against food allergen sensitization</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>36</issue>):<fpage>13145</fpage>–<lpage>13150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1412008111" xlink:type="simple">10.1073/pnas.1412008111</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Noval Rivas</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Burton</surname> <given-names>OT</given-names></name>, <name name-style="western"><surname>Wise</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Yq</given-names></name>, <name name-style="western"><surname>Hobson</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Garcia Lloret</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A microbiota signature associated with experimental food allergy promotes allergic sensitization and anaphylaxis</article-title>. <source>Journal of Allergy and Clinical Immunology</source>. <year>2013</year>;<volume>131</volume>(<issue>1</issue>):<fpage>201</fpage>–<lpage>212</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jaci.2012.10.026" xlink:type="simple">10.1016/j.jaci.2012.10.026</ext-link></comment> <object-id pub-id-type="pmid">23201093</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Savage</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Lee-Sarwar</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Sordillo</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bunyavanich</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>O’Connor</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>A prospective microbiome-wide association study of food sensitization and food allergy in early childhood</article-title>. <source>Allergy</source>. <year>2018</year>;<volume>73</volume>(<issue>1</issue>):<fpage>145</fpage>–<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/all.13232" xlink:type="simple">10.1111/all.13232</ext-link></comment> <object-id pub-id-type="pmid">28632934</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hua</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Goedert</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Pu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Shi</surname> <given-names>J</given-names></name>. <article-title>Allergy associations with the adult fecal microbiota: Analysis of the American Gut Project</article-title>. <source>EBioMedicine</source>. <year>2016</year>;<volume>3</volume>:<fpage>172</fpage>–<lpage>179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ebiom.2015.11.038" xlink:type="simple">10.1016/j.ebiom.2015.11.038</ext-link></comment> <object-id pub-id-type="pmid">26870828</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ling</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>X</given-names></name>, <etal>et al</etal>. <article-title>Altered fecal microbiota composition associated with food allergy in infants</article-title>. <source>Applied and environmental microbiology</source>. <year>2014</year>;<volume>80</volume>(<issue>8</issue>):<fpage>2546</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1128/AEM.00003-14" xlink:type="simple">10.1128/AEM.00003-14</ext-link></comment> <object-id pub-id-type="pmid">24532064</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref007">
<label>7</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Aktas</surname> <given-names>ON</given-names></name>, <name name-style="western"><surname>Turturice</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Perkins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Finn</surname> <given-names>PW</given-names></name>. <chapter-title>Microbiome: Allergic Diseases of Childhood</chapter-title>. In: <source>Mechanisms Underlying Host-Microbiome Interactions in Pathophysiology of Human Diseases</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>; <year>2018</year>. p. <fpage>35</fpage>–<lpage>53</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/978-1-4939-7534-1_2" xlink:type="simple">http://link.springer.com/10.1007/978-1-4939-7534-1_2</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bunyavanich</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Grishin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Burks</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dawson</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Early-life gut microbiome composition and milk allergy resolution</article-title>. <source>Journal of Allergy and Clinical Immunology</source>. <year>2016</year>;<volume>138</volume>(<issue>4</issue>):<fpage>1122</fpage>–<lpage>1130</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jaci.2016.03.041" xlink:type="simple">10.1016/j.jaci.2016.03.041</ext-link></comment> <object-id pub-id-type="pmid">27292825</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McKenzie</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vuillermin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Goverse</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vinuesa</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mebius</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Dietary Fiber and Bacterial SCFA Enhance Oral Tolerance and Protect against Food Allergy through Diverse Cellular Pathways</article-title>. <source>Cell Reports</source>. <year>2016</year>;<volume>15</volume>(<issue>12</issue>):<fpage>2809</fpage>–<lpage>2824</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.celrep.2016.05.047" xlink:type="simple">10.1016/j.celrep.2016.05.047</ext-link></comment> <object-id pub-id-type="pmid">27332875</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Berni Canani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sangwan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Stefka</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Nocerino</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Paparo</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Aitoro</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Lactobacillus rhamnosus GG-supplemented formula expands butyrate-producing bacterial strains in food allergic infants</article-title>. <source>The ISME Journal</source>. <year>2016</year>;<volume>10</volume>(<issue>3</issue>):<fpage>742</fpage>–<lpage>750</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ismej.2015.151" xlink:type="simple">10.1038/ismej.2015.151</ext-link></comment> <object-id pub-id-type="pmid">26394008</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kostic</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Gevers</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Siljander</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Vatanen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hyötyläinen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hämäläinen</surname> <given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>The dynamics of the human infant gut microbiome in development and in progression toward type 1 diabetes</article-title>. <source>Cell host &amp; microbe</source>. <year>2015</year>;<volume>17</volume>(<issue>2</issue>):<fpage>260</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.chom.2015.01.001" xlink:type="simple">10.1016/j.chom.2015.01.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vatanen</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kostic</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>d’Hennezel</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Siljander</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Franzosa</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Yassour</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Variation in Microbiome LPS Immunogenicity Contributes to Autoimmunity in Humans</article-title>. <source>Cell</source>. <year>2016</year>;<volume>165</volume>(<issue>4</issue>):<fpage>842</fpage>–<lpage>853</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cell.2016.04.007" xlink:type="simple">10.1016/j.cell.2016.04.007</ext-link></comment> <object-id pub-id-type="pmid">27133167</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Caporaso</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Lauber</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Costello</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Berg-Lyons</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gonzalez</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Stombaugh</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Moving pictures of the human microbiome</article-title>. <source>Genome biology</source>. <year>2011</year>;<volume>12</volume>(<issue>5</issue>):<fpage>R50</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/gb-2011-12-5-r50" xlink:type="simple">10.1186/gb-2011-12-5-r50</ext-link></comment> <object-id pub-id-type="pmid">21624126</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Turnbaugh</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Hamady</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yatsunenko</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Cantarel</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Duncan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ley</surname> <given-names>RE</given-names></name>, <etal>et al</etal>. <article-title>A core gut microbiome in obese and lean twins</article-title>. <source>Nature</source>. <year>2009</year>;<volume>457</volume>(<issue>7228</issue>):<fpage>480</fpage>–<lpage>484</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07540" xlink:type="simple">10.1038/nature07540</ext-link></comment> <object-id pub-id-type="pmid">19043404</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koenig</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Spor</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Scalfone</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Fricker</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Stombaugh</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Succession of microbial consortia in the developing infant gut microbiome</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2011</year>;<volume>108</volume> <issue>Suppl 1</issue>(Supplement 1):<fpage>4578</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1000081107" xlink:type="simple">10.1073/pnas.1000081107</ext-link></comment> <object-id pub-id-type="pmid">20668239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morris</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Paulson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Talukder</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tipton</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kling</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cui</surname> <given-names>L</given-names></name>, <etal>et al</etal>. <article-title>Longitudinal analysis of the lung microbiota of cynomolgous macaques during long-term SHIV infection</article-title>. <source>Microbiome</source>. <year>2016</year>;<volume>4</volume>(<issue>1</issue>):<fpage>38</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s40168-016-0183-0" xlink:type="simple">10.1186/s40168-016-0183-0</ext-link></comment> <object-id pub-id-type="pmid">27391224</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ranjan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Metwally</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>McGee</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Perkins</surname> <given-names>DL</given-names></name>. <article-title>Analysis of the microbiome: Advantages of whole genome shotgun versus 16S amplicon sequencing</article-title>. <source>Biochemical and Biophysical Research Communications</source>. <year>2015</year>;<volume>469</volume>(<issue>4</issue>):<fpage>967</fpage>–<lpage>977</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bbrc.2015.12.083" xlink:type="simple">10.1016/j.bbrc.2015.12.083</ext-link></comment> <object-id pub-id-type="pmid">26718401</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Metwally</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Dai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Finn</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Perkins</surname> <given-names>DL</given-names></name>. <article-title>WEVOTE: Weighted Voting Taxonomic Identification Method of Microbial Sequences</article-title>. <source>PLOS ONE</source>. <year>2016</year>;<volume>11</volume>(<issue>9</issue>):<fpage>e0163527</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0163527" xlink:type="simple">10.1371/journal.pone.0163527</ext-link></comment> <object-id pub-id-type="pmid">27683082</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bolyen</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rideout</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Dillon</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Bokulich</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Abnet</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Al-Ghalith</surname> <given-names>GA</given-names></name>, <etal>et al</etal>. <article-title>QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science</article-title>. <source>PeerJ</source>. <year>2018</year>;</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franzosa</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>McIver</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Rahnavard</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Schirmer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weingart</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Species-level functional profiling of metagenomes and metatranscriptomes</article-title>. <source>Nature Methods</source>. <year>2018</year>;<volume>15</volume>(<issue>11</issue>):<fpage>962</fpage>–<lpage>968</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41592-018-0176-y" xlink:type="simple">10.1038/s41592-018-0176-y</ext-link></comment> <object-id pub-id-type="pmid">30377376</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Metwally</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ascoli</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Finn</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Perkins</surname> <given-names>DL</given-names></name>. <article-title>MetaLonDA: a flexible R package for identifying time intervals of differentially abundant features in metagenomic longitudinal studies</article-title>. <source>Microbiome</source>. <year>2018</year>;<volume>6</volume>(<issue>1</issue>):<fpage>32</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s40168-018-0402-y" xlink:type="simple">10.1186/s40168-018-0402-y</ext-link></comment> <object-id pub-id-type="pmid">29439731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rabiner</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Juang</surname> <given-names>B</given-names></name>. <article-title>An introduction to hidden Markov models</article-title>. <source>IEEE ASSP Magazine</source>. <year>1986</year>;<volume>3</volume>(<issue>1</issue>):<fpage>4</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MASSP.1986.1165342" xlink:type="simple">10.1109/MASSP.1986.1165342</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>Fitting autoregressive models for prediction</article-title>. <source>Annals of the institute of Statistical Mathematics</source>. <year>1969</year>;<volume>21</volume>(<issue>1</issue>):<fpage>243</fpage>–<lpage>247</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02532251" xlink:type="simple">10.1007/BF02532251</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref024">
<label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>. <source>Deep Learning</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2016</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/citation.cfm?id=3086952" xlink:type="simple">https://dl.acm.org/citation.cfm?id=3086952</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Pascanu R, Mikolov T, Bengio Y. On the difficulty of training recurrent neural networks; 2013. Available from: <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/citation.cfm?id=3043083" xlink:type="simple">https://dl.acm.org/citation.cfm?id=3043083</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ditzler</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Polikar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rosen</surname> <given-names>G</given-names></name>. <article-title>Multi-Layer and Recursive Neural Networks for Metagenomic Classification</article-title>. <source>IEEE transactions on nanobioscience</source>. <year>2015</year>;<volume>14</volume>(<issue>6</issue>):<fpage>608</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNB.2015.2461219" xlink:type="simple">10.1109/TNB.2015.2461219</ext-link></comment> <object-id pub-id-type="pmid">26316190</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Reiman D, Metwally A, Dai Y. Using Convolutional Neural Networks to Explore the Microbiome. In: 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). Jeju Island, South Korea: IEEE; 2017. p. 4269–4272.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reiman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Metwally</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Dai</surname> <given-names>Y</given-names></name>. <article-title>PopPhy-CNN: A Phylogenetic Tree Embedded Architecture for Convolution Neural Networks for Metagenomic Data</article-title>. <source>bioRxiv</source>. <year>2018</year>; p. <fpage>257931</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fioravanti</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Giarratano</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Maggio</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Agostinelli</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chierici</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jurman</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Phylogenetic convolutional neural networks in metagenomics</article-title>. <source>BMC Bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>S2</issue>):<fpage>49</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s12859-018-2033-5" xlink:type="simple">10.1186/s12859-018-2033-5</ext-link></comment> <object-id pub-id-type="pmid">29536822</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wen</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Gasic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mrksic</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Vandyke</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>S</given-names></name>. <article-title>Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</article-title>. <source>arXiv</source>. <year>2015</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Graves</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mohamed</surname> <given-names>Ar</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Speech Recognition with Deep Recurrent Neural Networks</article-title>. <source>arXiv</source>. <year>2013</year>;.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Long Short-Term Memory</article-title>. <source>Neural Computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>1780</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1997.9.8.1735" xlink:type="simple">10.1162/neco.1997.9.8.1735</ext-link></comment> <object-id pub-id-type="pmid">9377276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peng</surname> <given-names>Hanchuan</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>Fuhui</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>C</given-names></name>. <article-title>Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2005</year>;<volume>27</volume>(<issue>8</issue>):<fpage>1226</fpage>–<lpage>1238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2005.159" xlink:type="simple">10.1109/TPAMI.2005.159</ext-link></comment> <object-id pub-id-type="pmid">16119262</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hahnloser</surname> <given-names>RHR</given-names></name>, <name name-style="western"><surname>Sarpeshkar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mahowald</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Erratum: Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</article-title>. <source>Nature</source>. <year>2000</year>;<volume>405</volume>(<issue>6789</issue>):<fpage>947</fpage>–<lpage>951</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35016072" xlink:type="simple">10.1038/35016072</ext-link></comment> <object-id pub-id-type="pmid">10879535</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kullback</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Leibler</surname> <given-names>RA</given-names></name>. <article-title>On Information and Sufficiency</article-title>. <source>The Annals of Mathematical Statistics</source>. <year>1951</year>;<volume>22</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aoms/1177729694" xlink:type="simple">10.1214/aoms/1177729694</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Ng A. Sparse autoencoder; 2011. Available from: <ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" xlink:type="simple">https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Truong</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Franzosa</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Tickle</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Scholz</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weingart</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pasolli</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>MetaPhlAn2 for enhanced metagenomic taxonomic profiling</article-title>. <source>Nature Methods</source>. <year>2015</year>;<volume>12</volume>(<issue>10</issue>):<fpage>902</fpage>–<lpage>903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nmeth.3589" xlink:type="simple">10.1038/nmeth.3589</ext-link></comment> <object-id pub-id-type="pmid">26418763</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref038">
<label>38</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haykin</surname> <given-names>SS Simon</given-names></name>. <source>Neural networks: a comprehensive foundation</source>. <publisher-name>Macmillan</publisher-name>; <year>1994</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://dl.acm.org/citation.cfm?id=541500" xlink:type="simple">https://dl.acm.org/citation.cfm?id=541500</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cortes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vapnik</surname> <given-names>V</given-names></name>. <article-title>Support-vector networks</article-title>. <source>Machine Learning</source>. <year>1995</year>;<volume>20</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00994018" xlink:type="simple">10.1007/BF00994018</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Tin Kam Ho. Random decision forests. In: Proceedings of 3rd International Conference on Document Analysis and Recognition. vol. 1. IEEE Comput. Soc. Press;. p. 278–282. Available from: <ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/598994/" xlink:type="simple">http://ieeexplore.ieee.org/document/598994/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regression Shrinkage and Selection via the Lasso</article-title>. <source>Journal of the Royal Statistical Society</source>. <year>1996</year>;<volume>58</volume>(<issue>1</issue>):<fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. In: OSDI’16 Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation; 2016. p. 265–283. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.04467" xlink:type="simple">http://arxiv.org/abs/1603.04467</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma DP, Ba J. Adam: A Method for Stochastic Optimization. In: International Conference on Learning Representations (ICLR); 2015.Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980" xlink:type="simple">http://arxiv.org/abs/1412.6980</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Srivastava</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>. <article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>;<volume>15</volume>:<fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006693.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fieten</surname> <given-names>KB</given-names></name>, <name name-style="western"><surname>Totté</surname> <given-names>JEE</given-names></name>, <name name-style="western"><surname>Levin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Reyman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Meijer</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Knulst</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Fecal Microbiome and Food Allergy in Pediatric Atopic Dermatitis: A Cross-Sectional Pilot Study</article-title>. <source>International archives of allergy and immunology</source>. <year>2018</year>;<volume>175</volume>(<issue>1-2</issue>):<fpage>77</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000484897" xlink:type="simple">10.1159/000484897</ext-link></comment> <object-id pub-id-type="pmid">29393195</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006693.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yoo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Na</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>HS</given-names></name>. <article-title>Faecalibacterium prausnitzii subspecies–level dysbiosis in the human gut microbiome underlying atopic dermatitis</article-title>. <source>Journal of Allergy and Clinical Immunology</source>. <year>2016</year>;<volume>137</volume>(<issue>3</issue>):<fpage>852</fpage>–<lpage>860</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jaci.2015.08.021" xlink:type="simple">10.1016/j.jaci.2015.08.021</ext-link></comment> <object-id pub-id-type="pmid">26431583</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>