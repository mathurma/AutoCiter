<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00745</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003272</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>The Convallis Rule for Unsupervised Learning in Cortical Networks</article-title>
<alt-title alt-title-type="running-head">Unsupervised Learning in Cortical Networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Yger</surname><given-names>Pierre</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>Kenneth D.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>UCL Institute of Neurology and UCL Department of Neuroscience, Physiology, and Pharmacology, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Oxford, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">p.yger@ucl.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: PY KDH. Performed the experiments: PY. Analyzed the data: PY. Contributed reagents/materials/analysis tools: PY. Wrote the paper: PY KDH.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>10</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>24</day><month>10</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>10</issue>
<elocation-id>e1003272</elocation-id>
<history>
<date date-type="received"><day>1</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>28</day><month>8</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Yger, Harris</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>The phenomenology and cellular mechanisms of cortical synaptic plasticity are becoming known in increasing detail, but the computational principles by which cortical plasticity enables the development of sensory representations are unclear. Here we describe a framework for cortical synaptic plasticity termed the “Convallis rule”, mathematically derived from a principle of unsupervised learning via constrained optimization. Implementation of the rule caused a recurrent cortex-like network of simulated spiking neurons to develop rate representations of real-world speech stimuli, enabling classification by a downstream linear decoder. Applied to spike patterns used in <italic>in vitro</italic> plasticity experiments, the rule reproduced multiple results including and beyond STDP. However STDP alone produced poorer learning performance. The mathematical form of the rule is consistent with a dual coincidence detector mechanism that has been suggested by experiments in several synaptic classes of juvenile neocortex. Based on this confluence of normative, phenomenological, and mechanistic evidence, we suggest that the rule may approximate a fundamental computational principle of the neocortex.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The circuits of the sensory cortex are able to extract useful information from sensory inputs because of their exquisitely organized synaptic connections. These connections are wired largely through experience-dependent synaptic plasticity. Although many details of both the phenomena and cellular mechanisms of cortical synaptic plasticity are now known, an understanding of the computational principles by which synaptic plasticity wires cortical networks lags far behind this experimental data. In this study, we provide a theoretical framework for cortical plasticity termed the “Convallis rule”. The computational power of this rule is demonstrated by its ability to cause simulated cortical networks to learn representations of real-world speech data. Application of the rule to paradigms used to probe synaptic plasticity in vitro reproduced a large number of experimental findings, and the mathematical form of the rule is consistent with a dual coincidence detector mechanism that has been suggested experimentally in juvenile neocortex. Based on this confluence of normative, phenomenological, and mechanistic evidence, we suggest that the rule may approximate a fundamental computational principle of the neocortex.</p>
</abstract>
<funding-group><funding-statement>This work was supported by NIH (DC009947), NSF (SBE-0542013 to the Temporal Dynamics of Learning Center, and NSF Science of Learning Center), EPSRC (EP/I005102), and a Wellcome Trust Investigator Award (KDH). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Animal learning is believed to occur primarily through changes in synaptic strengths. Experimental work has revealed an increasingly detailed picture of synaptic plasticity <xref ref-type="bibr" rid="pcbi.1003272-Feldman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Malenka1">[2]</xref>, at the level of both phenomenology and cellular mechanisms. However an understanding of synaptic plasticity's computational role in cortical circuits lags far behind this experimental knowledge. While spike timing dependent plasticity (STDP) has gained much attention, the STDP rule is simply a description of how synapses respond to one particular paradigm of temporally offset spike pairings, and is neither a complete description of synaptic behaviour, nor a computational principle that explains how learning could occur in cortex <xref ref-type="bibr" rid="pcbi.1003272-Shouval1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Feldman2">[6]</xref>. It therefore seems likely that STDP is just an approximation to a more fundamental computational principle that explains the form and function of cortical synaptic plasticity. Such a principle would not only have to be consistent with experimental results on the phenomena and mechanisms of synaptic plasticity, but also explain why it provides a computational benefit. A strong test of the latter is whether simulated cortex-like circuits employing the same principle can learn to perform real-world information processing tasks.</p>
<p>The nature and mechanisms of synaptic plasticity differ between brain regions, developmental stages, and cell types, likely indicating different computational roles of synaptic plasticity in different contexts. In the sensory cortex, synaptic plasticity is strongest at early ages <xref ref-type="bibr" rid="pcbi.1003272-Larsen1">[7]</xref>, and is believed to play an important role in the development of sensory representations. The juvenile cortex learns to form representations of sensory stimuli even in the absence of any required behavior or reward: the acquisition of native language sounds, for example, begins through passive exposure to speech before infants can themselves speak <xref ref-type="bibr" rid="pcbi.1003272-Kuhl1">[8]</xref>. The outcome of such learning is not simply a more faithful representation of the learned stimuli — which are already faithfully represented by sensory receptors themselves — but a transformation of this representation into a form where relevant information can be more easily read out by downstream structures <xref ref-type="bibr" rid="pcbi.1003272-DiCarlo1">[9]</xref>. This problem of forming easily-decoded representations of a data set, without reward or training signals, is called “unsupervised learning” <xref ref-type="bibr" rid="pcbi.1003272-Barlow1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Hastie1">[11]</xref>.</p>
<p>Unsupervised learning has long been proposed as a primary function of the sensory cortex <xref ref-type="bibr" rid="pcbi.1003272-Marr1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Konorski1">[13]</xref>. An intriguing connection between cortical plasticity and artificial algorithms for unsupervised learning arises from work of Bienenstock, Cooper, and Munro (BCM) <xref ref-type="bibr" rid="pcbi.1003272-Bienenstock1">[14]</xref>. A key feature of the BCM rule is that inputs occurring when the postsynaptic firing rate is below a “plasticity threshold” will be weakened, whereas inputs firing when postsynaptic firing rate exceeds the plasticity threshold will be strengthened; the rule is made stable by allowing the plasticity threshold to “slide” as a function of mean postsynaptic activity. The BCM rule operates at the level of firing rate neurons, and at this level has been successful in modelling a number of experimental results such as the development of visual receptive fields <xref ref-type="bibr" rid="pcbi.1003272-Cooper1">[15]</xref>. Theoretical analysis <xref ref-type="bibr" rid="pcbi.1003272-Intrator1">[16]</xref> has shown that this scheme allows simplified neuron models to implement an unsupervised learning algorithm similar to projection pursuit <xref ref-type="bibr" rid="pcbi.1003272-Friedman1">[17]</xref> or independent component analysis (ICA) <xref ref-type="bibr" rid="pcbi.1003272-Bell1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Hyvrinen1">[19]</xref>, extracting non-Gaussian features of their inputs which are <italic>a priori</italic> more likely than Gaussian features to correspond to signals of interest.</p>
<p>Although the BCM theory was originally defined at the level of firing rates, more recent modeling work <xref ref-type="bibr" rid="pcbi.1003272-Izhikevich1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Senn1">[24]</xref> has reproduced a dependence of the direction of synaptic plasticity on postsynaptic firing rate in spike-based neurons. In cortical neurons synaptic plasticity depends not only of postsynaptic firing rates, but also shows a similar dependence on subthreshold depolarization, with presynaptic spikes during strong postsynaptic depolarizations leading to potentiation, and during weak postsynaptic depolarization leading to depression <xref ref-type="bibr" rid="pcbi.1003272-Artola1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>. Computational models incorporating such behavior have successfully matched several experimental findings of <italic>in vitro</italic> plasticity <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>.</p>
<p>In the present work, we present a framework for unsupervised learning in cortical networks. The rule is derived as an optimization of the skewness of a cell's postsynaptic membrane potential distribution under a constraint of constant firing rate, and leads to a voltage-dependence similar to that observed experimentally <xref ref-type="bibr" rid="pcbi.1003272-Artola1">[25]</xref>. We term the resulting framework the Convallis rule after the Latin word for “valley”, in reference to the shape of the voltage objective function. We show that the Convallis rule causes simulated recurrent spiking networks to perform unsupervised learning of speech sounds, forming representations that enable a downstream linear classifier to accurately identify spoken words from the spike counts of the simulated neurons. When presented with paired pre- and postsynaptic spikes or other paradigms used <italic>in vitro</italic>, predictions of the Convallis rule more accurately match experimental results than the predictions of STDP alone. Furthermore, simulation of STDP alone (or of previously published plasticity rules <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>) produced poorer performance on speech learning than the full Convallis rule, indicating that STDP may be just one signature of a cortical plasticity principle similar to Convallis. The mathematical form of the Convallis rule suggests implementation by a dual coincidence detector mechanism, consistent with experimental data from juvenile sensory cortex <xref ref-type="bibr" rid="pcbi.1003272-Feldman2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Nevian1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Min1">[33]</xref>.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We derived the Convallis rule from two principles, analogous to those underlying artificial unsupervised learning algorithms such as ICA. The first principle is that synaptic changes should tend to increase the skewness of a neuron's subthreshold membrane potential distribution. Because the physical processes that produce structure in real-world data sets often show substantial higher-order moments, whereas random and uninformative combinations follow a Gaussian distribution, projections with non-Gaussian distribution are <italic>a priori</italic> more likely to extract useful information from many real-world data sets <xref ref-type="bibr" rid="pcbi.1003272-Hyvrinen1">[19]</xref>. The second principle is that despite synaptic plasticity, neurons should maintain a constant average firing rate. This principle is required for stable operation of the rule, and is again analogous to a step of the ICA algorithm (see below).</p>
<p>To derive the rule, we first defined an objective function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e001" xlink:type="simple"/></inline-formula> that measures the non-Gaussianity of the subthreshold distribution. The function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e002" xlink:type="simple"/></inline-formula> has the valley-shaped form shown in <xref ref-type="fig" rid="pcbi-1003272-g001">Figure 1B</xref>. Optimization of this objective function ensures that the postsynaptic neuron spends as much time as possible close to either resting potential or spiking threshold, but as little time as possible in a zone of intermediate membrane potential, i.e. exhibiting a skewed, non-Gaussian subthreshold distribution. The form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e003" xlink:type="simple"/></inline-formula> used in simulations is described in the Materials &amp; Methods, although our results did not depend critically on this precise formula (data not shown).</p>
<fig id="pcbi-1003272-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g001</object-id><label>Figure 1</label><caption>
<title>Illustration of the Convallis rule.</title>
<p>(<bold>A</bold>) Schematic of a particular plastic synapse (blue) onto a post-synaptic neuron with membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e004" xlink:type="simple"/></inline-formula>. (<bold>B</bold>) The objective function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e005" xlink:type="simple"/></inline-formula> optimized by the neuron: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e006" xlink:type="simple"/></inline-formula> values in between resting state or threshold are penalized, while values close to rest or spike threshold are rewarded. (<bold>C</bold>) Illustration of the learning rule. Presynaptic spike times (top, gray lines), are filtered by the EPSP shape <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e007" xlink:type="simple"/></inline-formula> (top black trace). This activity is multiplied by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e008" xlink:type="simple"/></inline-formula> (shown to the right of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e009" xlink:type="simple"/></inline-formula>), to yield a function that is positive when the presynaptic cell fires shortly before <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e010" xlink:type="simple"/></inline-formula> is close to threshold, and negative for presynaptic spikes at intermediate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e011" xlink:type="simple"/></inline-formula> (blue trace). This function is then accumulated through a slowly decaying exponential (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e012" xlink:type="simple"/></inline-formula>, green, bottom), and passed through a shrinkage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e013" xlink:type="simple"/></inline-formula> (right) to yield the weight changes. The horizontal orange lines indicate the thresholds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e014" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e015" xlink:type="simple"/></inline-formula> that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e016" xlink:type="simple"/></inline-formula> must cross to yield potentiation and depression. (<bold>D</bold>) Illustration of the rate constraint mechanism. Deviations of the long-run average firing rate from a target value lead to multiplicative scaling of excitatory synaptic inputs and suppression of activity-dependent plasticity until the rate target is restored.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g001" position="float" xlink:type="simple"/></fig>
<p>To implement the first principle of skewness optimization, we first compute the derivative of this objective function with respect to the neuron's input weights. Making certain assumptions (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for a full derivation) we obtain:<disp-formula id="pcbi.1003272.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e017" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e018" xlink:type="simple"/></inline-formula> is the reversal potential of synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e019" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e020" xlink:type="simple"/></inline-formula> is the rest voltage of the neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e021" xlink:type="simple"/></inline-formula> are the times of action potentials incoming onto synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e022" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e023" xlink:type="simple"/></inline-formula> is the shape of a postsynaptic potential elicited by synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e024" xlink:type="simple"/></inline-formula>. When a presynaptic input fires shortly before the neuron is close to spiking threshold, the integrand is positive leading to an increase in synaptic weight, but when a presynaptic neuron fires shortly prior to a potential only just above rest the integrand is negative leading to a decrease in synaptic weight. This voltage dependence is similar to that observed experimentally in cortical neurons <xref ref-type="bibr" rid="pcbi.1003272-Artola1">[25]</xref> and also employed in previous phenomenological models <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>. We note that a direct computation of this integral would be computationally prohibitive, as it would require numerical solution of a differential equation for every synapse and at every time step of the simulation. Tractable simulation of this rule was however made possible by a trick that enabled solution of only a single differential equation per neuron (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). In our simulations, voltage was reset to a level of −55 mV after action potential firing, followed by an afterdepolarization simulating the effects of active dendritic conductances <xref ref-type="bibr" rid="pcbi.1003272-Larkum1">[34]</xref> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). This reset mechanism, rather than the reset to rest commonly employed in integrate-and-fire simulations, was necessary in order to produce voltage traces similar to those seen in experimental recordings of cortical pyramidal cells (see <xref ref-type="supplementary-material" rid="pcbi.1003272.s001">Figure S1</xref>), and also played an important role in matching <italic>in vitro</italic> plasticity results (see below).</p>
<p>While <xref ref-type="disp-formula" rid="pcbi.1003272.e017">equation 1</xref> is sufficient to implement our first principle of skewness optimization, we found that better learning performance, as well as a closer match to physiological data, could be obtained with an additional feature modeled after the statistical technique of shrinkage <xref ref-type="bibr" rid="pcbi.1003272-Hastie1">[11]</xref>. Specifically, the integrand of <xref ref-type="disp-formula" rid="pcbi.1003272.e017">equation 1</xref> was not used directly to modify weights, but first convolved with a decaying exponential to yield a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e025" xlink:type="simple"/></inline-formula>, and then passed through a nonlinear shrinkage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e026" xlink:type="simple"/></inline-formula> to ensure plasticity only occurs in response to multiple coincidences: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e027" xlink:type="simple"/></inline-formula> (<xref ref-type="bibr" rid="pcbi.1003272-Boustani1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Senn1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Brader1">[35]</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for more details). This ensures that weight changes occur only due to reliable and repeated relationships between presynaptic activity and postsynaptic membrane potentials, rather than random occurrence of single spikes. An illustration of how pre- and post-synaptic activity lead to weight changes under this rule is shown in <xref ref-type="fig" rid="pcbi-1003272-g001">Figure 1C</xref>. Physiologically, such an integration mechanism could be instantiated via self-exciting kinases as suggested previously <xref ref-type="bibr" rid="pcbi.1003272-Boustani1">[22]</xref>.</p>
<p>The second principle underlying the Convallis rule is a constraint on the mean firing rate of each neuron to a target value. Analogous principles are also often found in machine learning algorithms: in ICA, for example, the root-mean-square activity of each unit is fixed at a constant value by a constraint on the weight vector norm together with sphering of inputs <xref ref-type="bibr" rid="pcbi.1003272-Hyvrinen1">[19]</xref>. Such constraints are typically implemented in one of two ways: by including a penalty term in the objective function, whose gradient is then added to the learning rule resulting in “weight decay”; or by repeated projection of the system parameters onto a subspace satisfying the constraint <xref ref-type="bibr" rid="pcbi.1003272-Hyvrinen1">[19]</xref>. In our simulations, we found that simple gradient ascent was not effective at enforcing stability, and therefore used a projection method. This was implemented by a mechanism which responded to deviations from the target firing rate by linearly scaling all excitatory synaptic weights up or down <xref ref-type="bibr" rid="pcbi.1003272-Turrigiano1">[36]</xref>, and suppressing activity-dependent plasticity until the rate constraint was restored (<xref ref-type="fig" rid="pcbi-1003272-g001">Figure 1D</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). Physiologically, the “metaplasticity” <xref ref-type="bibr" rid="pcbi.1003272-Abraham1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Hulme1">[38]</xref> required for suppression of synaptic changes until rate homeostasis is restored, could be instantiated via one of the many molecular pathways gating induction and expression of synaptic plasticity.</p>
<p>To study the rule's effects, we first considered the behaviour of an individual neuron implementing the rule on a simple artificial data set. The parameters used in the learning rule were fixed in this and all subsequent simulations (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for more details). For this first artificial task, inputs consisted of a population of 1000 excitatory sources (see <xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2A</xref>). The simulated postsynaptic neuron received plastic excitatory synapses from these sources, as well as constant inhibitory background with input at 10 Hz through 250 synapses which were not subject to plasticity. We first considered a simple case where inputs fired as Poisson spike trains with rates determined as spatial Gaussian profiles whose centre changed location every 100 ms (<xref ref-type="fig" rid="pcbi-1003272-g001">Figure 1A</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Boustani1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Gjorgjieva1">[39]</xref>. When weights evolved according to the rate constraint only, no structure was seen in the weight patterns. With the Convallis rule, postsynaptic neurons developed strong weights from groups of closely-spaced and thus correlated inputs, but zero weights from neurons uncorrelated with this primary group. When weights instead evolved by classical all-to-all STDP augmented by the rate constraint (called rcSTDP, see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details), the firing rate was kept at the desired value of 10 Hz, and weights became more selective, but in a manner less closely related to the input statistics. Examination of post-synaptic voltage traces showed that after learning with the Convallis rule, but not after rate constraint alone, the membrane potential spent considerably longer close to resting potential (<xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2C</xref>), corresponding to an increased skewness of the membrane potential histogram, (<xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2D</xref>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e028" xlink:type="simple"/></inline-formula>, t-test). This in turn reflected the development of selectivity of the neurons to particular stimuli (<xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2E</xref>) (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e029" xlink:type="simple"/></inline-formula>, t-test). Application of rcSTDP caused an increase in skewness tuning intermediate between rate constraint alone and the Convallis rule, even after optimizing by parameter search (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e030" xlink:type="simple"/></inline-formula>, t-test; see <xref ref-type="supplementary-material" rid="pcbi.1003272.s002">Figure S2</xref>). This confirms that the Convallis rule is able to perform unsupervised learning in a simple artificial task, causing neurons to select inputs from groups of coactive neurons; STDP produces a poorer approximation to the same behavior.</p>
<fig id="pcbi-1003272-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g002</object-id><label>Figure 2</label><caption>
<title>Operation of the Convallis rule in a simple feed-forward situation.</title>
<p>(<bold>A</bold>) The activity of a population of input neurons was simulated by ascribing each input a location on a virtual circle. Every 100 ms, the firing rate of the inputs was updated as a circular Gaussian distribution with random center, and spike trains were simulated as Poisson processes with this rate. These inputs were fed to a single output cell that employed rate constraint alone, the Convallis rule, or STDP together with rate constraint (rcSTDP). (<bold>B</bold>) Evolution of the input weights and mean firing rate of an example neuron during learning. Note the development of spatially selective inputs for the Convallis rule, but not rate constraint, and the development of approximate selectivity by rcSTDP. (<bold>C</bold>) Illustrative membrane potential trace after learning in the three different conditions. (<bold>D</bold>) Probability distribution of the membrane potential for the neurons shown in <bold>B</bold>, and skewness values averaged over a population of 1000 neurons. (<bold>E</bold>) Tuning for the neurons shown in <bold>B</bold>, and tuning index values averaged over a population of 1000 neurons. Black bars and traces represent rate constraint rule only; red represents Convallis rule; and blue represents rcSTDP. Error bars show standard error of the mean. * represents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e031" xlink:type="simple"/></inline-formula>, t-test, and ** represents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e032" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g002" position="float" xlink:type="simple"/></fig>
<p>We next asked whether the Convallis rule would enable individual simulated neurons to perform unsupervised learning in a real-world problem. Because we are interested in the development of cortical representations of sensory stimuli, we asked whether the Convallis rule could promote unsupervised formation of representations of speech sounds. Spike train inputs were generated from the TIDIGITS database of spoken digits <xref ref-type="bibr" rid="pcbi.1003272-Leonard1">[40]</xref>, by pre-processing with a cochlear model filter bank <xref ref-type="bibr" rid="pcbi.1003272-Lyon1">[41]</xref>, followed by transformation into inhomogeneous Poisson spike trains that contacted the simulated neuron with a range of synaptic delays (<xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3A</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3B</xref> (top row) shows a representation of the output of the cochleogram for utterances of the digits “four”, and “five”. To the right is a pseudocolor representation of the excitatory weights developed by neurons initialized to random weights and trained on 326 utterances of all digits by the rate constraint mechanism alone, by the Convallis rule, or by rcSTDP. Each digit was repeated ten times. <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3B</xref> (lower three rows) shows the response of these three neurons to a test set consisting of previously unheard utterances of the same digits by different speakers. The neuron trained by Convallis responds selectively to “four” while the response to “five” is largely eliminated, whereas the neuron trained by rate constraint alone responds equally to both. Thus, the Convallis rule has enabled the neuron to develop a differential response to the presented digits, which has generalized to utterances of the same digits spoken by new speakers.</p>
<fig id="pcbi-1003272-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g003</object-id><label>Figure 3</label><caption>
<title>Illustration of Convallis rule as applied to speech data.</title>
<p>(<bold>A</bold>) Preprocessing pipeline. Waveforms corresponding to utterances of eleven spoken digits (zero to nine plus “oh”) by multiple speakers were processed by a cochleogram model <xref ref-type="bibr" rid="pcbi.1003272-Lyon1">[41]</xref>, which was used to produce inhomogeneous Poisson spike trains of 100 input cells. (<bold>B</bold>) Illustration of spiking and voltage responses after learning for two particular digits. Top row: examples of the input population spike patterns corresponding to a single presentation of the digits “four” and “five”. Top row right, pseudocolor representation of the simulated neuron's input weights after learning, for rate constraint, the Convallis rule, and rcSTDP. Bottom three rows show a raster representation of the trained neuron's responses to a test set consisting of 300 utterances of these digits by previously unheard speakers, together with a membrane potential trace from a single test-set utterance. Right column shows mean firing rate vs. time averaged over the whole test set, illustrating the development of selective responses by the Convallis rule.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g003" position="float" xlink:type="simple"/></fig>
<p>To verify that this behaviour holds in general, we performed five thousand independent simulations of the Convallis rule in single neurons, with excitatory and inhibitory inputs drawn from the simulated cochlear cells, each trained by 10 presentations of the TIDIGITS training set, which we found sufficient to ensure convergence of all learning rules (<xref ref-type="supplementary-material" rid="pcbi.1003272.s003">Figure S3</xref>). Each simulation began from a different random weight configuration. The mean firing rate constraint was fixed to 1.5 Hz for all cells. As previously seen with artificial inputs, the membrane distribution produced in response to this real-world input was more skewed after training with the Convallis rule (<xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4A</xref> for the example cell shown in <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3</xref>, <xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4B</xref> for population summary). On average, over 1000 independent runs, there was a significant difference in skewness between Convallis and rate constraint alone, with rcSTDP producing an intermediate increase in skewness (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e033" xlink:type="simple"/></inline-formula>). We measured the selectivity of the simulated neurons using an F-statistic that measured differences in spike count between different digits (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The Convallis rule caused neurons to become more selective (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e034" xlink:type="simple"/></inline-formula>, t-test), whereas application of rate constraint alone or rcSTDP led to output neurons that were actually less selective than the raw cochleogram input (<xref ref-type="fig" rid="pcbi-1003272-g004">Figures 4C</xref> for the same example cell shown in <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3</xref>, <xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4D</xref> for population average). Similar results were found when comparing Convallis to multiple implementations of the STDP rule as well as for other plasticity rules described in the modelling literature <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref> (see <xref ref-type="supplementary-material" rid="pcbi.1003272.s004">Figure S4</xref>).</p>
<fig id="pcbi-1003272-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g004</object-id><label>Figure 4</label><caption>
<title>Feed-forward processing of speech data.</title>
<p>(<bold>A</bold>) Histogram of subthreshold potentials for the cell illustrated in <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3</xref>, accumulated over all test-set data after learning with three different plasticity rules. (<bold>B</bold>) Distribution of skewness for 4500 neurons trained similarly from random initial weights. Skewness after Convallis training is significantly higher than after rate constraint or rcSTDP, but rcSTDP and rate constraint do not differ. (<bold>C</bold>) Mean rate response of the example neuron to all digits. Errors bar show s.e.m. (<bold>D</bold>) The strength of tuning for each neuron was summarized by an F-statistic that measured the selectivity of its spike counts for particular digits (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The main graph shows an histogram of tuning strength across the simulated population for the 3 learning rules and the raw cochleogram input, while the inset shows mean and standard error. Note that while the Convallis rule produces sharper tuning than the cochleogram inputs, rate constraint alone and rcSTDP produce weaker tuning. (<bold>E</bold>) To evaluate the ability of these rules to perform unsupervised learning, the spike count responses of up to 4500 cells were used as input to a linear classifier trained to distinguish digits. (<bold>F</bold>) Mean classification performance as a function of the number of unsupervised neurons. (Errors bars show s.e.m over 10 independent runs of the analysis). The left axis marks the number of neurons in the cochleogram representation, and the horizontal dashed line indicates classification from the raw cochleogram. Note that Convallis outperforms the raw cochleogram, populations trained by rate constraint, and populations trained by rcSTDP for all numbers of neurons.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g004" position="float" xlink:type="simple"/></fig>
<p>The aim of unsupervised learning is to generate representations of input data that enable downstream neurons to easily form associations with them. Although complete information about the stimulus is of course present in the raw input, a downstream cell may not be able to extract this information unless it is represented in a suitable form. We next asked whether the representation generated by the Convallis rule allowed improved classification by a linear downstream readout in which spike timing information was discarded; this choice was motivated by results indicating that information in higher sensory cortices can be progressively more easily read out in such a format <xref ref-type="bibr" rid="pcbi.1003272-DiCarlo1">[9]</xref>. Specifically, we used a linear support vector machine to predict which digit was uttered, from the spike counts of a population of simulated cells arranged in a feedforward configuration (<xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4E</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>; note that while the SVM was trained with a biologically unrealistic quadratic programming algorithm, the same solution would be found by a large-margin perceptron <xref ref-type="bibr" rid="pcbi.1003272-Freund1">[42]</xref>). <xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4F</xref> shows the generalization performance of the classifier (measured on the TIDIGITS test set) as a function of population size. Performing the classification from a layer of neurons that used rate constraint alone produced an improvement over prediction directly from the cochleogram. The size of this improvement increased with the number of cells used, consistent with reports that large numbers of random projections can provide useful data representations <xref ref-type="bibr" rid="pcbi.1003272-Huang1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Luo1">[44]</xref>. Applying the Convallis rule produced a substantially improved representation over the rate constraint alone (18% vs 29.9% errors; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e035" xlink:type="simple"/></inline-formula>, t-test), whereas rcSTDP produced an intermediate improvement (25.9% error; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e036" xlink:type="simple"/></inline-formula>, t-test). Evaluation of performance with time-reversed digit stimuli indicated that the neurons had learned specific temporal features of the input rather than simply frequency content (<xref ref-type="supplementary-material" rid="pcbi.1003272.s003">Figure S3</xref>). Evaluation of several other proposed learning rules for spiking neurons taken from the literature, such as rcNN-STDP (STDP with interactions only between neighbouring pairs of spikes, and the rate constraint), triplet STDP <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref> with rate constraint, or phenomenological rules also based on post-synaptic voltages <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details) also confirmed that their performance did not match those of the Convallis rule (25.0%, 27% and 25.9% vs 18.0% errors; see <xref ref-type="supplementary-material" rid="pcbi.1003272.s004">Figure S4</xref>).</p>
<p>The above analysis showed that the Convallis rule caused individual neurons to develop selective representations of the digit stimuli, which when arranged together in a feedforward configuration formed a population code that enabled the spoken digit to be decoded with 82% accuracy. The cortex, however, is a recurrent rather than a feedforward network, and we next asked whether a recurrent architecture would lead to further improved classification performance (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5A</xref>). Recurrent spiking network models can exhibit multiple global patterns of population activity, of which the asynchronous irregular state provides the closest match to <italic>in vivo</italic> cortical activity in alert animals <xref ref-type="bibr" rid="pcbi.1003272-Brunel1">[45]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Harris1">[47]</xref>. We set the initial conductances (prior to training) to obtain asynchronous irregular activity at a mean spontaneous activity at 1.5 Hz, and with the coefficient of variation of inter-spike intervals (CV ISI) equal to 1.1 (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5C</xref>; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). When a sound input was presented to the network, mean firing rates increased from 1.5 Hz to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e037" xlink:type="simple"/></inline-formula> 15 Hz (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5B</xref>), while remaining in the asynchronous irregular regime.</p>
<fig id="pcbi-1003272-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g005</object-id><label>Figure 5</label><caption>
<title>Learning and classification in a balanced recurrent network.</title>
<p>(<bold>A</bold>) Network illustration. A set of 3600 excitatory and 900 inhibitory recurrently connected neurons are driven by an external excitatory input drawn from a cochleogram simulated as before. Excitatory synapses within the network are plastic while inhibitory synapses and external inputs are kept fixed. A population of linear readout neurons use the spike counts of the recurrent excitatory neurons to classify the spoken digits. (<bold>B</bold>) Illustration of population activity in the network, before and after learning, in response to a particular digit. The rasters show activity of the entire population of excitatory and inhibitory neurons (red and blue) to a single digit presentation; the lower curves show population-averaged firing rate throughout this trial. Note that training produces no visible change in global network dynamics, which maintains an asynchronous regular state. (<bold>C–E</bold>) Distributions of ISI CVs, firing rates, and pairwise correlation coefficients (averaged over 2000 randomly chosen pairs of cells) in the network before and after learning with rate constraint only, rcSTDP, or with the Convallis learning rule. Note that none of the learning rules produce a change in any of these measures of network dynamics. Error bars show the standard deviation. (<bold>F</bold>) Distribution of membrane potential skewness for 200 randomly chosen cells in the network before or after learning. Note that skewness is highest with the Convallis rule. (<bold>G</bold>) Distribution of the tuning sharpness (as measured by F-statistic) for all neurons before and after learning. Inset displays the mean of the distributions. Error bars show standard deviation. (<bold>H</bold>) Classification performance as a function of the number of neurons considered by the external classifier. Errors bars show s.e.m over 10 different simulations, run independently from different random seeds. While Convallis learning produces improved performance, rate constraint did not, and rcSTDP produced a smaller but still significant improvement (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e038" xlink:type="simple"/></inline-formula>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g005" position="float" xlink:type="simple"/></fig>
<p>To measure the ability of the Convallis rule to produce unsupervised learning in recurrent spiking networks, we trained the network with 10 iterations of the TIDIGITS training set, which were again sufficient for convergence (see <xref ref-type="supplementary-material" rid="pcbi.1003272.s005">Figure S5</xref>). All recurrent excitatory connections in the network were plastic, while inhibitory and input connections were fixed. Running the learning rule did not disrupt the asynchronous irregular dynamics of the network, as indicated by the ISI CV, mean firing rate distribution, and mean spontaneous correlation values (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5B</xref> and <xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5C, D, E</xref>). As in the feed-forward case, the network's constituent neurons showed increased tuning and membrane potential skewness after training (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5F, G</xref>).</p>
<p>The ability to perform unsupervised learning in a recurrent network was again measured by ability to identify the spoken digits using a linear classifier trained on the spike counts of the network's excitatory neurons (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5H</xref>). We note that even prior to training, as in the feed-forward case, the representation generated by the recurrent network allowed higher classification performance than the raw cochleogram input (5.8% error), consistent with previous reports that randomly connected “liquid-state” networks can compute useful representations of spatiotemporal input patterns <xref ref-type="bibr" rid="pcbi.1003272-Maass1">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Buonomano2">[50]</xref>. Training with the Convallis rule significantly boosted performance to reach 3.3% error (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5H</xref>). As in the feedforward case, application of rcSTDP produced error rates more than 50% higher than those of the full Convallis rule (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5H</xref>) (5.1% error; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e039" xlink:type="simple"/></inline-formula>). Thus, the Convallis rule enables spiking neurons to perform unsupervised learning on real-world problems, arranged either in a feedforward or in a recurrent configuration. As in the feed-forward scenario, performance with time-reversed digit stimuli indicated that the neurons had learned specific temporal features of the input rather than simply frequency content (<xref ref-type="supplementary-material" rid="pcbi.1003272.s005">Figure S5</xref>). Once again, we were unable to produce comparable results with rules previously published in the literature, which resulted in error rates more than 50% higher than those produced by Convallis (5.2% and 5.3% errors for rcNN-STDP and rcTriplet, respectively; see <xref ref-type="supplementary-material" rid="pcbi.1003272.s006">Figure S6</xref>).</p>
<p>The Convallis rule was derived mathematically from an optimization principle, rather than by fitting to experimentally measured parameters. Before suggesting that an analogous process might occur in the cortex, it is thus important to check how a neuron employing this rule would behave in paradigms that have been used to experimentally probe cortical synaptic plasticity. Although we found simulation of rcSTDP alone produced poorer learning than Convallis, STDP is a robustly observed experimental result that the Convallis rule must reproduce if a similar rule does occur in cortical neurons. To test this, we applied a spike-pairing paradigm to two simulated cells, using the same parameters as in the previous speech-classification simulations. <xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6A</xref> shows a close-up view of the Convallis rule in operation for three spike pairings. The green trace shows a pre-post interval of 10 ms. Here, the period immediately after the presynaptic spike (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e040" xlink:type="simple"/></inline-formula> is positive) contains an action potential, leading to a high value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e041" xlink:type="simple"/></inline-formula>, and synaptic potentiation. The black trace shows a post-pre pairing of −10 ms. In this case, the period immediately following the presynaptic spike occurs during the postsynaptic afterdepolarization, a moderately depolarized voltage range for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e042" xlink:type="simple"/></inline-formula> is negative. The gray trace shows a pre-post interval of 30 ms, longer than the duration of the kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e043" xlink:type="simple"/></inline-formula>. Now, the postsynaptic potential during the entire period while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e044" xlink:type="simple"/></inline-formula> is very close to rest, leading to a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e045" xlink:type="simple"/></inline-formula> close to zero, and neither potentiation nor depression. <xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6B</xref> shows the results of similar simulations for a range of pre-post intervals, applying 60 spike pairings performed at 1 Hz. The Convallis rule reproduces a STDP curve similar to bi-exponential form found in many computational models <xref ref-type="bibr" rid="pcbi.1003272-Morrison1">[51]</xref>.</p>
<fig id="pcbi-1003272-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g006</object-id><label>Figure 6</label><caption>
<title>Reproduction of experimental findings.</title>
<p>(<bold>A</bold>) Schematic illustrating the Convallis learning rule in case of a 10 ms post-pre (black), a 10 ms pre-post pairing (green), or a 30 ms pre-post pairing (brown). (<bold>B</bold>) Synaptic modifications arising after 60 spike pairings repeated at 1 Hz, as a function of time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e046" xlink:type="simple"/></inline-formula> between pre- and post-synaptic spikes. The red curve indicates the results of the Convallis rule, the blue curve indicates the traditional bi-exponential STDP curve for comparison purposes. (<bold>C</bold>) Effect of tetanic stimuli at various frequencies. Red curve indicates Convallis rule results, errorbars are data reproduced from <xref ref-type="bibr" rid="pcbi.1003272-Kirkwood1">[55]</xref>. For the Convallis rule, as for the original data, high frequency stimulation yields potentiation, intermediate frequency stimulation yields depression, whereas the lowest frequencies yield no effect. (<bold>D</bold>) Effect of post-pre-post (top) and pre-post-pre (bottom) spike triplets at various intervals. White bars represent data from <xref ref-type="bibr" rid="pcbi.1003272-Wang1">[52]</xref>, red represents Convallis simulation, blue represents STDP. (<bold>E</bold>) Effect of repeating post-pre (top) and pre-post (bottom) pairings at frequencies between 0.1 and 50 Hz. Errorbars indicate data from <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>, red indicates Convallis simulation and blue indicates STDP. (<bold>F</bold>) After training synapses are stronger between neurons representing similar features, as found experimentally in mouse cortex <xref ref-type="bibr" rid="pcbi.1003272-Ko1">[57]</xref>. Histograms show mean synaptic weights after training the recurrent network on speech sounds, (<xref ref-type="fig" rid="pcbi-1003272-g005">Figure 5</xref>) for neuronal pairs maximally responsive to the same (top schematic, example of two neurons both tuned to digit 5), or different digits (bottom schematic, two neurons tuned to different digits). * represents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e047" xlink:type="simple"/></inline-formula>, t-test, and ** represents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e048" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g006" position="float" xlink:type="simple"/></fig>
<p>STDP does not fully summarize the nature of cortical synaptic plasticity, which cannot be explained by linear superposition of effects caused by individual spike pairs. Various <italic>in vitro</italic> pairing protocols, in hippocampus <xref ref-type="bibr" rid="pcbi.1003272-Wang1">[52]</xref> or in cortex <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Froemke1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Froemke2">[54]</xref> showed that LTP and LTD pathways can not be reduced to additive interactions of nearby spikes. Therefore, we next asked whether the Convallis rule would also be able to predict additional experimental results beyond STDP. As one of the pieces of evidence in favor of the original BCM theory is the dependence of the sign of plasticity on the rate of tetanic stimulation, we asked if the Convallis rule could produce a similar result. To simulate extracellular stimulation <italic>in vitro</italic>, we synchronously simulated multiple excitatory and inhibitory presynaptic synapses at a range of frequencies ranging from 0.1 Hz to 100 Hz, and investigated the amount of plasticity produced in a downstream neuron. Consistent with experimental data in cortical <xref ref-type="bibr" rid="pcbi.1003272-Kirkwood1">[55]</xref> as well as hippocampal <xref ref-type="bibr" rid="pcbi.1003272-OConnor1">[56]</xref> slices <italic>in vitro</italic>, low frequencies resulted in depression while higher frequencies resulted in potentiation (<xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6C</xref>). As a second example, we considered spike triplets in paired recordings (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Linear superposition of STDP would predict that presentation of post-pre-post spike triplets should cause no synaptic change; experimentally however, this causes robust potentiation (although pre-post-pre triplets do not) <xref ref-type="bibr" rid="pcbi.1003272-Wang1">[52]</xref>. The Convallis rule is able to reproduce this finding (<xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6D</xref>). A third example of nonlinear plasticity effects concerns the spike pairing repetition frequency. In cortical slices, post-pre pairings at low repetition rates cause synaptic depression, but this converts to potentiation for fast enough repetition rates, a non-linear effect that likely reflects subthreshold phenomena <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>. The Convallis rule produces a similar effect (<xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6E</xref>, top). For pre-post pairings, potentiation is not seen experimentally at low (0.1 Hz) repetition rates in L5 of juvenile cortex <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>. The Convallis rule also replicated this finding (<xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6E</xref>, bottom); for this, the shrinkage mechanism was critical (data not shown). Finally, we asked whether network-level plasticity using the Convallis rule left traces similar to those seen experimentally <italic>in vivo</italic>. Specifically, we assessed whether simulated neurons with similar receptive fields would exhibit higher connection probabilities, as has been reported in mouse visual cortex <xref ref-type="bibr" rid="pcbi.1003272-Ko1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Ko2">[58]</xref>. This was indeed the case (<xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6F</xref>), strongly for Convallis (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e049" xlink:type="simple"/></inline-formula>, t-test), weakly for rcSTDP (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e050" xlink:type="simple"/></inline-formula>, t-test), but not for rate constraint alone. We therefore conclude that the Convallis rule is consistent with a wide range of plasticity phenomena described <italic>in vitro</italic> and <italic>in vivo</italic>, supporting the possibility that a similar process occurs in cortex.</p>
<p>If cortical neurons do indeed implement a rule similar to Convallis, what cellular mechanisms might underlie it? Plasticity in the developing neocortex appears to involve different cellular mechanisms to those of the well-studied hippocampal Schaffer collateral synapse. One of the leading mechanistic models of hippocampal synaptic plasticity is the calcium concentration hypothesis <xref ref-type="bibr" rid="pcbi.1003272-Lisman3">[59]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Graupner1">[61]</xref>. In this model, both LTP and LTD are triggered by calcium influx through NMDA receptors, with LTP triggered by high Ca<sup>2+</sup> concentrations, and LTD triggered by low concentrations (see <xref ref-type="fig" rid="pcbi-1003272-g007">Figure 7A</xref>). This model has a similarity with Convallis in that weak activation causes LTD and strong activation LTP. Nevertheless, the functional form of the Convallis rule (<xref ref-type="disp-formula" rid="pcbi.1003272.e017">Eqn. 1</xref>) has a critical difference to the calcium hypothesis. In the Convallis rule, the nonlinear function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e051" xlink:type="simple"/></inline-formula> that determines the sign of synaptic plasticity operates directly on the membrane potential prior to coincidence detection with presynaptic input, whereas in the calcium rule this nonlinearity happens after coincidence detection. This leads to a diverging experimental predictions, with the calcium model predicting a triphasic STDP curve <xref ref-type="bibr" rid="pcbi.1003272-Shouval2">[60]</xref> (but see also <xref ref-type="bibr" rid="pcbi.1003272-Graupner1">[61]</xref>). This has been reported in some hippocampal experiments <xref ref-type="bibr" rid="pcbi.1003272-Nishiyama1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Wittenberg1">[63]</xref>, but not in the neocortex (<xref ref-type="fig" rid="pcbi-1003272-g007">Figure 7B</xref>).</p>
<fig id="pcbi-1003272-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003272.g007</object-id><label>Figure 7</label><caption>
<title>The Convallis rule is inconsistent with the Calcium Hypothesis but consistent with a dual-sensor model.</title>
<p>(<bold>A</bold>) Illustration of the calcium hypothesis. In this scheme, the direction of synaptic plasticity depends on calcium concentration, with high concentrations leading to LTP and lower concentrations leading to LTD. The calcium hypothesis predicts that short pre-post pairings produce LTP (green), short post-pre pairings predict LTD (black), but unlike the Convallis also predicts that long pre-post pairings should produce LTD (gray). (<bold>B</bold>) Triphasic STDP curve predicted by the Calcium hypothesis, set against prediction of the Convallis rule. (<bold>C</bold>) Hypothesized cellular mechanism for Convallis rule. LTP is induced by coincidence detection via an NMDA receptor, requiring glutamate and strongly depolarized membrane potential. LTD is induced by a separate coincidence detector with a lower voltage threshold, in which activation of phospholipase C<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e052" xlink:type="simple"/></inline-formula> requires coincident activity of group I metabotropic glutamate receptors and T-type (low threshold) calcium channels. (<bold>D</bold>) Summation of the voltage-dependence curves for high-threshold potentiation and low-threshold depression gives the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e053" xlink:type="simple"/></inline-formula> function of the Convallis rule.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003272.g007" position="float" xlink:type="simple"/></fig>
<p>A substantial body of experimental evidence suggests that in juvenile neocortical neurons, the potentiation and depression components of STDP are produced by different cellular mechanisms <xref ref-type="bibr" rid="pcbi.1003272-Nevian1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Min1">[33]</xref>. While these data are obtained from different sensory cortices (visual, somatosensory), and for different cortical synapse types (typically L4→L2/3 or L5→L5), they suggest a hypothesis for a common mechanism underlying STDP in at least some neocortical synapses <xref ref-type="bibr" rid="pcbi.1003272-Feldman2">[6]</xref>. In these systems, LTP appears of the conventional type, dependent on postsynaptic NMDA activation caused by coincident glutamate release and release of magnesium block by postsynaptic depolarization. For LTD however, induction is independent of postsynaptic NMDA receptors, and instead appears to be induced by a separate mechanism in which postsynaptic phospholipase C<italic>β</italic> acts as a coincidence detector for the activation of group I metabotropic glutamate receptors, and postsynaptic depolarization detected by voltage-sensitive calcium channels (VSCCs), leading to presynaptic expression of LTD via retrograde endocannabinoid signaling. Importantly, the VSCCs implicated are of the low-threshold T-type <xref ref-type="bibr" rid="pcbi.1003272-Nevian1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Bender1">[30]</xref>. Together, these results suggest a hypothesis that in the developing sensory cortex, there exist two separate molecular coincidence detectors for LTP and LTD, and that the coincidence detector for LTD has a lower voltage threshold (<xref ref-type="fig" rid="pcbi-1003272-g007">Figure 7C</xref>; <xref ref-type="bibr" rid="pcbi.1003272-Feldman2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Karmarkar1">[32]</xref>.</p>
<p>The mathematical form of the Convallis rule is consistent with just such a mechanism. The function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e054" xlink:type="simple"/></inline-formula> can be expressed as a difference of two non-negative functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e055" xlink:type="simple"/></inline-formula>, both sigmoidal in shape, but with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e056" xlink:type="simple"/></inline-formula> having a lower threshold. The rule can then be expressed as a sum of two terms<disp-formula id="pcbi.1003272.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e057" xlink:type="simple"/></disp-formula>This equation has a natural mechanistic interpretation, as the result of two coincidence detectors. The first, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e058" xlink:type="simple"/></inline-formula>, is activated when the membrane is strongly depolarized after a presynaptic spike fires, and leads to synaptic potentiation. The second, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e059" xlink:type="simple"/></inline-formula>, is activated when the membrane is moderately depolarized after presynaptic firing, and leads to synaptic depression. Linear addition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e060" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e061" xlink:type="simple"/></inline-formula> would be expected due to their implementation by separate coincidence detectors, triggered by spatially separated calcium sources <xref ref-type="bibr" rid="pcbi.1003272-Rizzuto1">[64]</xref>. The mathematical form of the Convallis rule therefore bears a striking resemblance to a leading hypothesis for the mechanisms synaptic plasticity in the juvenile sensory cortex.</p>
</sec><sec id="s3">
<title>Discussion</title>
<p>We derived a synaptic plasticity rule for unsupervised learning in spiking neurons, based on an optimization principle that increases the skewness of subthreshold membrane potential distributions, under the constraint of a fixed mean firing rate. Applying this rule to a speech recognition task caused individual neurons to develop skewed membrane potential distributions and selective receptive fields both in a feedforward configuration and within a recurrent network. The spike count outputs of the recurrent network were sufficient to allow good readout by a linear classifier, suggesting that this unsupervised rule had enabled the network to form an easily-decoded representation of the key spatiotemporal features of the input that distinguished the spoken digits. Simulation of paradigms used to study synaptic plasticity <italic>in vitro</italic> produced similar behaviour to that found experimentally. Furthermore the form of the rule is consistent with a dual-sensor mechanism that has been suggested experimentally for cortical neurons.</p>
<p>The phenomenon of spike-timing dependent plasticity has been robustly observed in a large number of neuronal systems (see for example <xref ref-type="bibr" rid="pcbi.1003272-Dan1">[65]</xref> for review). It is important to remember however that STDP is not a fundamental description of synaptic plasticity, but simply an experimental observation that describes how synapses respond to one particular stimulus of temporally offset spike pairings <xref ref-type="bibr" rid="pcbi.1003272-Shouval1">[3]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Feldman2">[6]</xref>. We found that the Convallis rule, when presented with paired spikes, reproduced a biphasic STDP curve. However, implementation of all-to-all STDP alone produced both a worse fit to experimental plasticity paradigms, and poorer unsupervised learning of speech sounds than the full Convallis rule. Implementation of other learning rules described in the literature which match more experimental observations than STDP alone <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref> also produced poorer results.The higher performance of Convallis compared to rules based on spike timing alone may reflect the fact that the subthreshold potential conveys additional information that is useful to guide synaptic plasticity. We note however that better unsupervised learning was also obtained compared to a previous phenomenological rule <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref> that exhibited a similar voltage dependence, but was derived primarily to match experimental observations, rather than derived from an optimality principle. Other than the similar voltage dependence, this rule was different in many details to Convallis, for example with regard to the precise temporal relationship of presynaptic activity and postsynaptic voltage required for potentiation or depression. The derivation of these relationships from an optimality principle might underlie Convallis' better performance. Additionally or alternatively, the difference might reflect a difference in the stabilizing mechanism between the two rules. For Convallis, we found that a penalty-based weight decay term could not provide optimal stability, and much better performance was obtained with a hard constraint on firing rate with plasticity inhibited until the constraint was satisfied. In our simulations of the framework of <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>, we were similarly unable to obtain robust stabilization of firing rates, which may have contributed to poorer learning performance.</p>
<p>Although unsupervised learning has long been proposed as a primary function of the sensory cortex <xref ref-type="bibr" rid="pcbi.1003272-Marr1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Konorski1">[13]</xref>, the circuit mechanisms underlying it are still unknown. One influential class of models holds that unsupervised learning occurs through the coordinated plasticity of top-down and bottom-up projections, leading to the development of “generative models” by which the brain learns to form compressed representations of sensory stimuli <xref ref-type="bibr" rid="pcbi.1003272-Hinton1">[66]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Hinton2">[68]</xref>. Although these models have produced good performance in real-world tasks such as optical character recognition, the mapping between these abstract models and concrete experimental results on cortical circuitry and plasticity is as yet unclear, and their implementation in spiking neuron models has yet to be demonstrated. Here we describe an alternative scheme for unsupervised learning in cortex, in which every neuron acts essentially independently, using a plasticity rule to form an unsupervised representation of its own synaptic inputs. Despite the simplicity of this approach, it could be applied in recurrent spiking networks to produce good unsupervised learning. We hypothesize that incorporating other mechanisms to coordinate plasticity at the network level <xref ref-type="bibr" rid="pcbi.1003272-Harris2">[69]</xref> may further improve network performance.</p>
<p>In psychophysical experiments, perceptual learning is typically studied by repeated practice at sensory discrimination tasks. In such cases, learning might be boosted by attention directed to the stimuli to be learned, or rewards delivered after a correct response. Nevertheless, purely unsupervised perceptual learning can also occur in humans, both in development <xref ref-type="bibr" rid="pcbi.1003272-Kuhl1">[8]</xref> and adulthood <xref ref-type="bibr" rid="pcbi.1003272-Watanabe1">[70]</xref>. The Convallis rule as simulated here is a purely unsupervised rule that operates continuously. The effects of attention, reward and task-relevance could be captured in the same framework by a modulation of learning rates by neuromodulatory tone <xref ref-type="bibr" rid="pcbi.1003272-Bear1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Takata1">[72]</xref>. This would allow cortical networks to devote their limited resources to representing those stimulus features most likely to require behavioural associations.</p>
<p>Models of synaptic plasticity typically fall into three classes: phenomenological models, which aim to quantitatively summarize the ever-growing body of experimental data <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>; mechanistic models, which aim to explain how these phenomena are produced by underlying biophysical processes <xref ref-type="bibr" rid="pcbi.1003272-Shouval2">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Zou1">[73]</xref>; and normative models, which aim to explain the information-processing benefit that synaptic plasticity achieves within the brain <xref ref-type="bibr" rid="pcbi.1003272-Gerstner1">[74]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Rao1">[79]</xref>. The Convallis rule bridges all three levels of analysis. Being mathematically derived from an optimization principle, it belongs in the normative class, and the fact that it can organize recurrent spiking networks to perform unsupervised learning in a real-world task supports the idea that a similar principle could enhance cortical information processing. The rule is consistent with a number of experimental findings on cortical plasticity, including but not limited to STDP, suggesting that a similar principle may indeed operate in cortical cells. Finally, the functional form of the Convallis rule has a direct mechanistic interpretation in terms of a dual coincidence-detector model, for which substantial evidence exists in neocortical synapses <xref ref-type="bibr" rid="pcbi.1003272-Nevian1">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003272-Karmarkar1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Min1">[32,33]</xref>. Based on this confluence of normative, phenomenological, and mechanistic evidence, we suggest that the Convallis rule may approximate a fundamental computational principle of the neocortex.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Neuron model</title>
<p>Simulations of the spiking neurons were performed using a custom version of the <bold>NEST</bold> simulator <xref ref-type="bibr" rid="pcbi.1003272-Diesmann1">[80]</xref> and the <bold>PyNN</bold> interface <xref ref-type="bibr" rid="pcbi.1003272-Davison1">[81]</xref>, with a fixed time step of 0.1 ms. In all simulations, we used an integrate-and-fire neuron model with a membrane time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e062" xlink:type="simple"/></inline-formula>, a leak conductance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e063" xlink:type="simple"/></inline-formula>, and a resting membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e064" xlink:type="simple"/></inline-formula>. Spikes were generated when the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e065" xlink:type="simple"/></inline-formula> reaches the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e066" xlink:type="simple"/></inline-formula>. To model the shape of the action potential, the voltage was set to 20 mV after threshold crossing, and then decayed linearly during a refractory period of time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e067" xlink:type="simple"/></inline-formula> to a reset value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e068" xlink:type="simple"/></inline-formula>, following which an exponentially decaying after-depolarizing current <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e069" xlink:type="simple"/></inline-formula> of initial magnitude 50 pA and time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e070" xlink:type="simple"/></inline-formula> was applied. We used this scheme with a high reset voltage and ADP, rather than the more common low reset value, as it provided a better match to intracellular recordings <italic>in vitro</italic> and <italic>in vivo</italic> (see supplementary <xref ref-type="supplementary-material" rid="pcbi.1003272.s001">Figure S1</xref>). Synaptic connections were modelled as transient conductance changes with instantaneous rise followed by exponential decay. Synaptic time constants were chosen to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e072" xlink:type="simple"/></inline-formula> for excitation and inhibition respectively, and reversal potentials were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e073" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e074" xlink:type="simple"/></inline-formula>.</p>
<p>The complete set of equations describing the dynamics of a neuron is thus given by<disp-formula id="pcbi.1003272.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e075" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e076" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e077" xlink:type="simple"/></inline-formula> are the incoming synaptic spike trains represented as sums of delta functions.</p>
</sec><sec id="s4b">
<title>Learning rule</title>
<p>In the Convallis rule, a neuron adapts its synapses in order to optimize an objective function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e078" xlink:type="simple"/></inline-formula> depending on its membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e079" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003272.e080"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e080" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>To enforce skewness of the distribution of postsynaptic potentials, we chose an objective function that penalized intermediate membrane potential values, but rewarded membrane potentials close to either resting potential or spike threshold. Because the neuron spent considerably less time depolarized than hyperpolarized, the objective function was chosen to reward potentials close to spike threshold more strongly than potentials close to rest. For all simulations in the present paper, we used a sum of a logistic function and of its integral. More precisely:<disp-formula id="pcbi.1003272.e081"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e081" xlink:type="simple"/><label>(4)</label></disp-formula>Parameters values were taken as <italic>V</italic><sub>0</sub> = −55 mV, <italic>V</italic><sub>1</sub> = −52 mV, <italic>σ</italic><sub>0</sub> = 4 mV, <italic>σ</italic><sub>1</sub> = 2 mv and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e082" xlink:type="simple"/></inline-formula>, and the same parameters were used for both the speech processing application and simulation of <italic>in vitro</italic> experiments. The shape of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e083" xlink:type="simple"/></inline-formula> was therefore constant in all the simulations of the paper, and its exact form did not appear to be crucial (as long as a clear valley-shaped function was used), since similar results were achieved with a variety of functions (not shown).</p>
<p>To derive the Convallis rule, we used a gradient ascent method. Differentiating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e084" xlink:type="simple"/></inline-formula> with respect to incoming synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e085" xlink:type="simple"/></inline-formula> gives<disp-formula id="pcbi.1003272.e086"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e086" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>To compute <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e087" xlink:type="simple"/></inline-formula>, we considered the variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e088" xlink:type="simple"/></inline-formula>. <xref ref-type="disp-formula" rid="pcbi.1003272.e075">Equation 2</xref> can be rewritten as<disp-formula id="pcbi.1003272.e089"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e089" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e090" xlink:type="simple"/></inline-formula> is the total synaptic conductance and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e091" xlink:type="simple"/></inline-formula> the synaptic current. Specifically, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e092" xlink:type="simple"/></inline-formula> are the times at which a particular synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e093" xlink:type="simple"/></inline-formula> of weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e094" xlink:type="simple"/></inline-formula> is active, and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e095" xlink:type="simple"/></inline-formula> (if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e096" xlink:type="simple"/></inline-formula>) is the kernel function representing the conductance time course,<disp-formula id="pcbi.1003272.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e097" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e098" xlink:type="simple"/></inline-formula> is the reversal potential of synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e099" xlink:type="simple"/></inline-formula>. Inspecting <xref ref-type="disp-formula" rid="pcbi.1003272.e089">equation 6</xref>, we see that for a conductance-based neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e100" xlink:type="simple"/></inline-formula> integrates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e101" xlink:type="simple"/></inline-formula> with an effective time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e102" xlink:type="simple"/></inline-formula>. Approximating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e103" xlink:type="simple"/></inline-formula> by a constant equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e104" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e105" xlink:type="simple"/></inline-formula> denotes a running average of the synaptic conductance <xref ref-type="bibr" rid="pcbi.1003272-Gtig1">[82]</xref>, we can approximate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e106" xlink:type="simple"/></inline-formula> by the following equation:<disp-formula id="pcbi.1003272.e107"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e107" xlink:type="simple"/><label>(8)</label></disp-formula>where<disp-formula id="pcbi.1003272.e108"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e108" xlink:type="simple"/><label>(9)</label></disp-formula>Note that this approximation holds as long as we ignore the reset mechanism and non-linearity due to the spike, an approximation that will be more accurate when using a “soft” reset mechanism as described here. Substituting in <xref ref-type="disp-formula" rid="pcbi.1003272.e086">equation 5</xref>, we obtain the following equation for the gradient:<disp-formula id="pcbi.1003272.e109"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e109" xlink:type="simple"/><label>(10)</label></disp-formula></p>
<p>This generic form is similar to previous supervised learning rules that were also based onto the post-synaptic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e110" xlink:type="simple"/></inline-formula>, such as the Tempotron <xref ref-type="bibr" rid="pcbi.1003272-Gtig1">[82]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Gtig2">[83]</xref> or Chronotron <xref ref-type="bibr" rid="pcbi.1003272-Florian1">[84]</xref>. As noted by <xref ref-type="bibr" rid="pcbi.1003272-Urbanczik1">[85]</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e111" xlink:type="simple"/></inline-formula> is used here as a proxy for the input current flowing into the cells, which is the only relevant quantity at the cell level to measure the correlation between incoming pre and post-synaptic activity.</p>
<p>To prevent plastic changes for spurious single pairings, plasticity changes are accumulated through the convolution of a slowly decaying exponential, and then expressed at the synapse level only if the accumulated value crosses thresholds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e112" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e113" xlink:type="simple"/></inline-formula> for respectively potentiation and depression. Specifically, we define<disp-formula id="pcbi.1003272.e114"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e114" xlink:type="simple"/><label>(11)</label></disp-formula></p>
<p>The time constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e115" xlink:type="simple"/></inline-formula> of the slowly decaying exponential is taken to be 1 second throughout the paper. The final weight changes are then given by<disp-formula id="pcbi.1003272.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e116" xlink:type="simple"/><label>(12)</label></disp-formula>where the shrinkage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e117" xlink:type="simple"/></inline-formula> is defined as<disp-formula id="pcbi.1003272.e118"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e118" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>Throughout the paper, we fixed the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e119" xlink:type="simple"/></inline-formula> to −10 and 50 respectively. A graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e120" xlink:type="simple"/></inline-formula> can be seen in <xref ref-type="fig" rid="pcbi-1003272-g001">Figure 1C</xref>. Note that the weights are clipped to hard bounds values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e121" xlink:type="simple"/></inline-formula>nS and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e122" xlink:type="simple"/></inline-formula>nS. The Convallis rule has therefore have 3 parameters in addition of the shape of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e123" xlink:type="simple"/></inline-formula>: the time at which the changes are accumulated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e124" xlink:type="simple"/></inline-formula>, and those two thresholds for the shrinkage function.</p>
</sec><sec id="s4c">
<title>Implementation</title>
<p>Direct calculation of the above integrals would be prohibitive in large-scale simulations, as it would require computing the products <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e125" xlink:type="simple"/></inline-formula>, for all synapses and for each time step, resulting in a complexity scaling in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e126" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e127" xlink:type="simple"/></inline-formula> is the number of synapses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e128" xlink:type="simple"/></inline-formula> the time step, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e129" xlink:type="simple"/></inline-formula> the simulation length. To speed up implementation of the algorithm, we write:<disp-formula id="pcbi.1003272.e130"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e130" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e131" xlink:type="simple"/></inline-formula>. We can implement the rule much faster by first computing and storing the history <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e132" xlink:type="simple"/></inline-formula> for neuron, and computing weight changes as a sum over all input spikes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e133" xlink:type="simple"/></inline-formula> for all synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e134" xlink:type="simple"/></inline-formula>, which is of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e135" xlink:type="simple"/></inline-formula>. To compute <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e136" xlink:type="simple"/></inline-formula>, we note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e137" xlink:type="simple"/></inline-formula> is the convolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e138" xlink:type="simple"/></inline-formula> and a filter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e139" xlink:type="simple"/></inline-formula> which is a difference of decaying exponentials (see <xref ref-type="disp-formula" rid="pcbi.1003272.e108">Equation 9</xref>). By defining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e140" xlink:type="simple"/></inline-formula>, we can write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e141" xlink:type="simple"/></inline-formula>. Integrating by parts, we obtain<disp-formula id="pcbi.1003272.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e142" xlink:type="simple"/></disp-formula>Therefore, we have a differential equation that can be used to compute look-up tables of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e143" xlink:type="simple"/></inline-formula> for all neurons during this period, by running backwards in time from starting values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e144" xlink:type="simple"/></inline-formula>. Weight changes are then calculated by summing over spikes. We note that this method of running backward in time is simply a trick to speed up execution time, and is equivalent to the original deterministic algorithm. In practice, we perform this by stopping the simulation after the presentation of each input pattern (T = 1 s). This implementation does not impact the results when the frequency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e145" xlink:type="simple"/></inline-formula> of the updates is changed (data not shown), as long as the assumption <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e146" xlink:type="simple"/></inline-formula> is valid, which will hold provided the support of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e147" xlink:type="simple"/></inline-formula> filter is shorter than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e148" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4d">
<title>Firing rate constraint</title>
<p>Run in isolation, the above rule is unstable, as the response of the neuron tends to accumulate either above or below the plasticity threshold, leading to either explosive increases in synaptic weights or convergence of all weights to zero. In the BCM theory, this problem was solved by a sliding plasticity threshold, computed as a long-running average of the firing history of the post-synaptic neuron. For the Convallis rule we found that a sliding threshold was not necessary, provided a mechanism was in place to constrain the neurons firing rate to a fixed value. We implemented this via “synaptic scaling” <xref ref-type="bibr" rid="pcbi.1003272-Turrigiano2">[86]</xref>, using an approach analogous to the projected subgradient method for constrained optimization. In the projected subgradient method, gradient-following steps are allowed to temporarily break the constraint, but are followed by a projection onto the constraint subspace. Because direct projection onto the subspace of synaptic weights corresponding to the targeted mean firing rate would not be computationally tractable or biologically realistic, we instead used a Proportional-Integral (PI) controller <xref ref-type="bibr" rid="pcbi.1003272-vanRossum1">[87]</xref> to enforce the constraint, and suppress gradient learning until the constraint was re-established. Specifically, we define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e149" xlink:type="simple"/></inline-formula> to be the deviation from target mean firing rate, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e150" xlink:type="simple"/></inline-formula> is a cell's firing rate computed as a running average over its past-history with a time constant T (10 s in our simulations) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e151" xlink:type="simple"/></inline-formula> is the targeted mean rate. The output of the PI controller is<disp-formula id="pcbi.1003272.e152"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e152" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e153" xlink:type="simple"/></inline-formula> is a coefficient regulating the contribution of the integral term. The value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e154" xlink:type="simple"/></inline-formula> balances speed of convergence against the possibility of oscillation; in all simulations, we fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e155" xlink:type="simple"/></inline-formula>. To suppress gradient descent until the constraint was satisfied, we scaled the synaptic plasticity rule by a term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e156" xlink:type="simple"/></inline-formula> that was small if either <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e157" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e158" xlink:type="simple"/></inline-formula> was not close to zero, leading to a final form of<disp-formula id="pcbi.1003272.e159"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e159" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>The parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e160" xlink:type="simple"/></inline-formula> were set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e161" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e162" xlink:type="simple"/></inline-formula>, respectively. We found this latter feature was essential for stable operation of the Convallis rule.</p>
</sec><sec id="s4e">
<title>Circular Gaussian simulations</title>
<p>In simulations of artificial data (<xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2</xref>), 1000 excitatory and 250 inhibitory inputs were connected to a single post-synaptic neuron. Only excitatory connections were plastic. Initial values of the weights were drawn from Gaussian distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e163" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e164" xlink:type="simple"/></inline-formula>. The values were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e165" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e166" xlink:type="simple"/></inline-formula>, and the target output rate was fixed to 10 Hz. Pre-synaptic neurons were stimulated with wrapped Gaussian profiles of rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e167" xlink:type="simple"/></inline-formula> spikes/sec, the centre <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e168" xlink:type="simple"/></inline-formula> being shifted randomly every 100 ms over all possible positions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e169" xlink:type="simple"/></inline-formula> and with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e170" xlink:type="simple"/></inline-formula>. The tuning index used in <xref ref-type="fig" rid="pcbi-1003272-g002">Figure 2</xref> was computed as a directional statistic: for each cell, the distance between neuron 0 and 1000 was mapped into an angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e171" xlink:type="simple"/></inline-formula>, and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e172" xlink:type="simple"/></inline-formula> is the average firing rate for this particular angle, the tuning was defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e173" xlink:type="simple"/></inline-formula>. The closer the tuning is to 1, the more the neuron is responding only to one particular angle.</p>
</sec><sec id="s4f">
<title>TIDIGITS database</title>
<p>To test the ability of the rule to perform unsupervised learning in a real-world context, we applied it to a problem of speech recognition, using the TIDIGITS database <xref ref-type="bibr" rid="pcbi.1003272-Leonard1">[40]</xref>. This data consists of recordings of eleven English digits (“zero” to “nine” plus “oh”), spoken twice each by 326 speakers of various ages and genders (man, woman, boy, girl), at a sampling rate of 20 KHz. The TIDIGITS database was separated into its standard training and test sets of 167 speakers each. The raw recorded waveforms were pre-processed into spike trains using the Lyon model <xref ref-type="bibr" rid="pcbi.1003272-Lyon1">[41]</xref>, to produce a simulated cochleogram of 93 frequency channels. The cochleogram output for each digit was centered in a one second epoch, sampled at 500 Hz, and normalized to equalize the summed activity of all frequencies for all digit utterances. Input spike trains were generated as inhomogeneous Poisson spike trains with intensity function given by the cochleogram output, at an average frequency of 5 Hz.</p>
<p>For feedforward simulations (<xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3</xref>), each target neuron received plastic excitatory projections from 50% of randomly chosen cochleogram cells with initial conductances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e174" xlink:type="simple"/></inline-formula> uniformly drawn in [0, 10 nS] and synaptic delays uniformly drawn from [0.1 ms,5 ms], while also receiving static inhibitory projections from all cells in the cochleogram with conductances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e175" xlink:type="simple"/></inline-formula> uniformly drawn in [0, 40 nS].</p>
<p>For recurrent network simulations, 4500 neurons were simulated with an excitatory/inhibitory neuron ratio of 4∶1 on a square sheet with periodic boundary conditions. Every neuron was sparsely connected with the rest of the network with a connection probability of 5%. Synaptic delays were drawn randomly from a uniform distribution between 0.1 and 5 ms. Initial synaptic conductances were taken randomly from Gaussian distributions with means <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e176" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e177" xlink:type="simple"/></inline-formula>, and standard deviations equal to a third of their means. To sustain spontaneous activity, each neuron also received an independent Poisson spike train at a frequency of 300 Hz, through an excitatory synapse of weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e178" xlink:type="simple"/></inline-formula>. Although recurrent connections were uniform, input connections were arranged in a tonotopic manner, with each cochleogram cell projecting with excitatory synapses to a fraction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e179" xlink:type="simple"/></inline-formula> of neurons in the network, with a probability following a Gaussian profile <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e180" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e181" xlink:type="simple"/></inline-formula> being the distance between the source and a target neuron within the network, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e182" xlink:type="simple"/></inline-formula> being equal to 0.2 unit). The mean conductances of the external connections were equal to the recurrent ones, i.e <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e183" xlink:type="simple"/></inline-formula>, and all external inputs were fixed rather than plastic.</p>
<p>To measure the selectivity of a neuron to the digit stimuli, we used the F-statistic, commonly used in one-way analysis of variance (ANOVA). Specifically, to measure the difference between mean spike counts of each digit, relative to within-digit variance, we computed<disp-formula id="pcbi.1003272.e184"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003272.e184" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e185" xlink:type="simple"/></inline-formula> is the spike count the neuron produces on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e186" xlink:type="simple"/></inline-formula> presentation of digit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e187" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e188" xlink:type="simple"/></inline-formula> is the mean response to digit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e189" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e190" xlink:type="simple"/></inline-formula> the overall mean response, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e191" xlink:type="simple"/></inline-formula> the number of digits, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e192" xlink:type="simple"/></inline-formula> the total number of stimulus presentations.</p>
<p>To quantify the efficacy of unsupervised learning, we evaluated the ability of a downstream linear classifier to identify the digit spoken from the spike counts of each simulated neuron. This approach therefore evaluates the network's ability to form a linearly separable representation of the digit inputs that can be read out without requiring temporal analysis. Specifically, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e193" xlink:type="simple"/></inline-formula> is a matrix of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e194" xlink:type="simple"/></inline-formula> containing the mean firing rate of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e195" xlink:type="simple"/></inline-formula> cells to each of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e196" xlink:type="simple"/></inline-formula> digit utterances in the training set, and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e197" xlink:type="simple"/></inline-formula> is an “answer” matrix of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e198" xlink:type="simple"/></inline-formula> with each row consisting of all zeros except a single 1 indicating the presented digit during this trial, we used multi-class linear support vector machine <xref ref-type="bibr" rid="pcbi.1003272-Pedregosa1">[88]</xref> to find a matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e199" xlink:type="simple"/></inline-formula> of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e200" xlink:type="simple"/></inline-formula> to predict B from A. Performance was evaluated by computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e201" xlink:type="simple"/></inline-formula> on the test set, and classifying each utterance according to the highest value. The cost parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e202" xlink:type="simple"/></inline-formula> used for the support vector machine was set to 0.01. We note that while the SVM was for efficiency trained with a (biologically unrealistic) quadratic programming algorithm, the same solution would be found by the perceptron rule <xref ref-type="bibr" rid="pcbi.1003272-Freund1">[42]</xref>. Ridge regression learning was also tried (data not shown), leading to qualitatively similar results.</p>
</sec><sec id="s4g">
<title>Comparison with other learning rules</title>
<p>Throughout the paper, the rcSTDP rule is implemented as a normal additive STDP rule combined with the PI mechanism described for the Convallis rule (<xref ref-type="disp-formula" rid="pcbi.1003272.e159">Equation 15</xref>), in order to ensure that the same output firing rate is achieved. Optimization of this rule's parameters is described in <xref ref-type="supplementary-material" rid="pcbi.1003272.s002">Figure S2</xref>. To compare the Convallis rule with NN-STDP (STDP with interactions only between neighbouring pairs of spikes <xref ref-type="bibr" rid="pcbi.1003272-Izhikevich1">[20]</xref>) or triplet STDP <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, we again combined these rules with a PI mechanism to make sure that they were stable and had the same rate constraint. For the rule of <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>, we did not add the firing rate constraint, as it already contains a homeostatic mechanism. In all cases, we used the parameter values in the originally published manuscripts; in the case of the triplet rule, we used the data obtained from the fit to visual cortex data.</p>
</sec><sec id="s4h">
<title>Simulations of <italic>in vitro</italic> experiments</title>
<p>For all <italic>in vitro</italic> simulations (except <xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6C</xref>), we considered only two neurons with a single connection between them. The parameters used for the learning rules were the same as in the learning applications. The initial synaptic strength of the connection, if not specified elsewhere or varied, was taken to be 2 nS. All parameters had the same values as in the network simulations, but since it is assumed that these <italic>in vitro</italic> protocols are taking place over a short time scale, the rate constraint mechanism of the model was turned off. For <xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6C</xref>, we considered a group of 20 excitatory and 5 inhibitory synapses, connected onto a single post-synaptic neuron. For each stimulation of the simulated afferent fibers, every synapse had 50% chance of being active. The fibers were stimulated with 100 presynaptic pulses at varying frequencies, as in <italic>in vitro</italic> experiments <xref ref-type="bibr" rid="pcbi.1003272-Kirkwood1">[55]</xref>. To reproduce the triplet experiment <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003272-Wang1">[52]</xref>, we use a stimulation protocol of 60 triplet of spikes repeated at 1 Hz. Each triplet consists of two pre and one post synaptic spikes or two post and one pre-synaptic spikes, as can be seen in the inset of <xref ref-type="fig" rid="pcbi-1003272-g006">Figure 6D</xref> (see references for more details). To reproduce the dependance on frequency <xref ref-type="bibr" rid="pcbi.1003272-Sjstrm1">[26]</xref>, we used a protocol as in the original paper: interdigitated burst of 5 spikes paired with a given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e203" xlink:type="simple"/></inline-formula> and frequency repeated 15 times at a 0.1 Hz frequency, thus leading to 75 spikes in total.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003272.s001" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Reset mechanism.</bold> In integrate-and-fire neuronal simulations, the membrane potential is often reset to its resting value after each spike. Although this might be an appropriate model of certain neuronal classes, cortical pyramidal cells do not show this behavior. Instead, pyramidal cells return to a voltage only just below spike threshold after an action potential is fired, and frequently exhibit an after-depolarization caused by activation of dendritic voltage-gated conductances, which is believed to underlie burst firing <xref ref-type="bibr" rid="pcbi.1003272-Larkum1">[34]</xref>. (<bold>A</bold>) Intracellular recording trace of a L5 pyramidal cell in mouse visual cortex (courtesy of M. Okun). Note the lack of reset to resting potential after spike firing. (<bold>B</bold>) Illustration of membrane potential trace generated in response to white noise injection by a neuron with hard reset to resting potential after spike firing. Note the clear difference in reset behavior to the data in (A), and the lack of burst firing. (<bold>C</bold>) Illustration of membrane potential trace generated in response to the same input, by a neuron with soft reset to −55 mV after spike firing and ADP (the model used in all simulations). Note the more realistic spike reset and presence of burst firing.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003272.s002" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Calibration of the rcSTDP rule.</bold> (<bold>A</bold>) To obtain optimal performance with the rcSTDP rule, we performed a parameter search varying the the rate constraint parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e204" xlink:type="simple"/></inline-formula> and the STDP learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e205" xlink:type="simple"/></inline-formula>, initially on a linear scale, for the wrapped Gaussian stimulus ensemble. Performance was assessed as the skewness of the final <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e206" xlink:type="simple"/></inline-formula> distributions, shown in the pseudocolor matrix presented. Note the peak for values around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e207" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e208" xlink:type="simple"/></inline-formula>. (<bold>B</bold>) To gain further accuracy we performed an additional parameter search fixing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e209" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e210" xlink:type="simple"/></inline-formula> now on a log scale. A peak was seen at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003272.e211" xlink:type="simple"/></inline-formula>.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003272.s003" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Additional details of Convallis performance, feedforward case.</bold> (<bold>A</bold>) Weight distribution after learning the speech data. Note that Convallis leads to a highly skewed distribution with a large mode at 0 and a secondary peak at larger values, corresponding to a sparse weight matrix consisting of mainly silent synapses. STDP by contrast leads to a single-peaked distribution. (<bold>B</bold>) Convergence analysis. To show that all rules had converged we plotted the mean-square weight change in weight between consecutive training iterations. For all rules, the mean change tended to zero, indicating that weights had converged. (<bold>C</bold>) To evaluate whether the Convallis rule had detected true temporal features, rather than simply power in different frequencies, we evaluated performance on time-reversed digit stimuli. The unsupervised representation was trained using forward presentations only, and spike counts were measured in response to time-reversed digits. These spike counts were then fed into the SVM classifier to predict the presented digit. Classification performance was poorer, even when the SVM was retrained on the spike counts generated in response to time-reversed digits. This indicates that the Convallis rule has produced an unsupervised representation of temporal features in the input stimulus, rather than just frequency selectivity.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003272.s004" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Comparison to alternative learning rules, feedforward case.</bold> In addition to rcSTDP, whose performance is shown in the main text, we also compared the Convallis rule to various other learning rules described in the literature, specifically nearest-neighbor STDP (NN-STDP) <xref ref-type="bibr" rid="pcbi.1003272-Izhikevich1">[20]</xref>, triplet STDP <xref ref-type="bibr" rid="pcbi.1003272-Pfister1">[21]</xref>, and a rule based on post-synaptic voltage <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref>. This figure shows the same analyses as <xref ref-type="fig" rid="pcbi-1003272-g004">Figure 4</xref> for these rules. (<bold>A</bold>) Histogram of subthreshold potentials for the cell illustrated in <xref ref-type="fig" rid="pcbi-1003272-g003">Figure 3</xref>, accumulated over all test-set data after learning with the three alternative plasticity rules. (<bold>B</bold>) Distribution of skewness for 4500 neurons trained similarly from random initial weights. Note that skewness after Convallis training is markedly higher than after the rcTriplet, NN-rcSTDP, or Clopath rules. (<bold>C</bold>) Mean rate response of the same example neuron to all digits. Errors bar show s.e.m. (<bold>D</bold>) The strength of tuning for each neuron was summarized by an F-statistic that measured the selectivity of its spike counts for particular digits (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The main graph shows a histogram of tuning strength across the simulated population for the 3 learning rules and the raw cochleogram input, while the inset shows mean and standard error. Again, Convallis shows greater selectivity. (<bold>E</bold>) To evaluate the ability of these rules to perform unsupervised learning, the spike count responses of up to 4500 cells were used as input to a linear classifier trained to distinguish digits. (<bold>F</bold>) Mean classification performance as a function of the number of unsupervised neurons. (Errors bars show s.e.m over 10 independent runs of the analysis). Note that while the alternative rules exhibit better performance than rate constraint alone, they do not match the Convallis performance.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003272.s005" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>additional details of Convallis performance, recurrent case.</bold> (<bold>A</bold>) Weight distribution after learning the speech data. As in the feedforward case, the Convallis rule exhibits a sparse weight distribution while STDP produces a single-peaked distribution. (<bold>B</bold>) Convergence analysis showing the mean-square weight change in weight between consecutive training iterations. For all rules, the mean change tended to zero, indicating that weights had converged. (<bold>C</bold>) To evaluate whether the Convallis rule had detected true temporal features, rather than simply power in different frequencies, we evaluated performance on time-reversed digit stimuli. The unsupervised representation was trained using forward presentations only, and spike counts were measured in response to time-reversed digits. These spike counts were then fed into the SVM classifier to predict the presented digit. Classification performance was poorer, even when the SVM was retrained on the spike counts generated in response to time-reversed digits. This indicates that the Convallis rule has produced an unsupervised representation of temporal features in the input stimulus.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003272.s006" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003272.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p><bold>Comparison to alternative learning rules, recurrent case.</bold> To evaluate how other plasticity rules from the literature operated in a recurrent framework, we attempted to simulate the same plasticity rules as in <xref ref-type="supplementary-material" rid="pcbi.1003272.s004">Figure S4</xref>. Although the triplet and NN-STDP rules (supplemented with the firing rate constraint) were stable and fast enough to simulate in our recurrent network of 4500 cells, we were not able to simulate the rule of <xref ref-type="bibr" rid="pcbi.1003272-Clopath1">[23]</xref> as we found large networks implementing this rule were unstable. (<bold>A–C</bold>) Distributions of ISI CVs, firing rates, and pairwise correlation coefficients (averaged over 2000 randomly chosen pairs of cells) in the network before and after learning with rcTriplet and NN-rcSTDP rules. Note that none of the learning rules produce a change in any of these measures of network dynamics. Error bars show the standard deviation. (<bold>D</bold>) Distribution of membrane potential skewness for 200 randomly chosen cells in the network before or after learning. Note that skewness is highest with the Convallis rule. (<bold>E</bold>) Distribution of the tuning sharpness (as measured by F-statistic) for all neurons before and after learning. Inset displays the mean of the distributions. Error bars show standard deviation. (<bold>F</bold>) Classification performance as a function of the number of neurons considered by the external classifier, for various learning rules. Errors bars show s.e.m over 10 different simulations, run independently from different random seeds. Neither rule gave significantly improved performance over rate constraint alone (p&gt;0.05).</p>
<p>(EPS)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank M. Okun for providing intracellular recording data (<xref ref-type="supplementary-material" rid="pcbi.1003272.s001">Figure S1</xref>), and S. Lewis for many helpful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003272-Feldman1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name> (<year>2009</year>) <article-title>Synaptic mechanisms for plasticity in neocortex</article-title>. <source>Annu Rev Neurosci</source> <volume>32</volume>: <fpage>33</fpage>–<lpage>55</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Malenka1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malenka</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name> (<year>2004</year>) <article-title>LTP and LTD: an embarrassment of riches</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>5</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Shouval1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shouval</surname><given-names>HZ</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>SSH</given-names></name>, <name name-style="western"><surname>Wittenberg</surname><given-names>GM</given-names></name> (<year>2010</year>) <article-title>Spike timing dependent plasticity: a consequence of more fundamental learning rules</article-title>. <source>Front Comput Neurosci</source> <volume>4</volume>: <fpage>19</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Lisman1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Spruston</surname><given-names>N</given-names></name> (<year>2005</year>) <article-title>Postsynaptic depolarization requirements for LTP and LTD: a critique of spike timing-dependent plasticity</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>839</fpage>–<lpage>841</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Lisman2"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Spruston</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Questions about STDP as a general model of synaptic plasticity</article-title>. <source>Front Synaptic Neurosci</source> <volume>2</volume>: <fpage>140</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Feldman2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name> (<year>2012</year>) <article-title>The spike-timing dependence of plasticity</article-title>. <source>Neuron</source> <volume>75</volume>: <fpage>556</fpage>–<lpage>571</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Larsen1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larsen</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Rao</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Manis</surname><given-names>PB</given-names></name>, <name name-style="western"><surname>Philpot</surname><given-names>BD</given-names></name> (<year>2010</year>) <article-title>STDP in the Developing Sensory Neocortex</article-title>. <source>Frontiers in synaptic neuroscience</source> <volume>2</volume>: <fpage>9</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Kuhl1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name> (<year>2004</year>) <article-title>Early language acquisition: cracking the speech code</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>: <fpage>831</fpage>–<lpage>843</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-DiCarlo1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Zoccolan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name> (<year>2012</year>) <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source> <volume>73</volume>: <fpage>415</fpage>–<lpage>434</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Barlow1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name> (<year>1989</year>) <article-title>Unsupervised learning</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>295</fpage>–<lpage>311</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Hastie1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Hastie T, Tibshirani R, Friedman JH (2003) The Elements of Statistical Learning. Springer, corrected edition.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Marr1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name> (<year>1970</year>) <article-title>A theory for cerebral neocortex</article-title>. <source>Proc R Soc Lond B Biol Sci</source> <volume>176</volume>: <fpage>161</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Konorski1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Konorski</surname><given-names>J</given-names></name> (<year>1967</year>) <article-title>Some new ideas concerning the physiological mechanisms of perception</article-title>. <source>Acta Biol Exp (Warsz)</source> <volume>27</volume>: <fpage>147</fpage>–<lpage>161</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Bienenstock1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bienenstock</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Munro</surname><given-names>PW</given-names></name> (<year>1982</year>) <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>J Neurosci</source> <volume>2</volume>: <fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Cooper1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name> (<year>2012</year>) <article-title>The BCM theory of synapse modification at 30: interaction of theory with experiment</article-title>. <source>Nat Rev Neurosci</source> <volume>13</volume>: <fpage>798</fpage>–<lpage>810</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Intrator1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name> (<year>1992</year>) <article-title>Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions</article-title>. <source>Neural Networks</source> <volume>5</volume>: <fpage>3</fpage>–<lpage>17</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Friedman1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Stuetzle</surname><given-names>W</given-names></name> (<year>1981</year>) <article-title>Projection pursuit regression</article-title>. <source>Journal of the American Statistical Association</source> <volume>76</volume>: <fpage>817</fpage>–<lpage>823</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Bell1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>1995</year>) <article-title>An information-maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput</source> <volume>7</volume>: <fpage>1129</fpage>–<lpage>1159</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Hyvrinen1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Hyvärinen A, Karhunen J, Oja E (2001) Independent Component Analysis. Wiley-Interscience, 1 edition, citeulike:105835 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Izhikevich1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Desai</surname><given-names>NS</given-names></name> (<year>2003</year>) <article-title>Relating STDP to BCM</article-title>. <source>Neural Comput</source> <volume>15</volume>: <fpage>1511</fpage>–<lpage>1523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Pfister1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>9673</fpage>–<lpage>9682</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Boustani1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boustani</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Yger</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Frégnac</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Stable learning in stochastic network states</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>194</fpage>–<lpage>214</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Clopath1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Senn1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>An algorithm for modifying neurotransmitter release probability based on pre- and postsynaptic spike timing</article-title>. <source>Neural Comput</source> <volume>13</volume>: <fpage>35</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Artola1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Artola</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bröcher</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name> (<year>1990</year>) <article-title>Different voltage-dependent thresholds for inducing long-term depression and long-term potentiation in slices of rat visual cortex</article-title>. <source>Nature</source> <volume>347</volume>: <fpage>69</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Sjstrm1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2001</year>) <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source> <volume>32</volume>: <fpage>1149</fpage>–<lpage>1164</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Nevian1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nevian</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name> (<year>2006</year>) <article-title>Spine Ca<sup>2+</sup> signaling in spike-timing-dependent plasticity</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>11001</fpage>–<lpage>11013</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Sjstrm2"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2003</year>) <article-title>Neocortical LTD via coincident activation of presynaptic NMDA and cannabinoid receptors</article-title>. <source>Neuron</source> <volume>39</volume>: <fpage>641</fpage>–<lpage>654</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Sjstrm3"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2004</year>) <article-title>Endocannabinoid-dependent neocortical layer-5 LTD in the absence of postsynaptic spiking</article-title>. <source>J Neurophysiol</source> <volume>92</volume>: <fpage>3338</fpage>–<lpage>3343</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Bender1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bender</surname><given-names>VA</given-names></name>, <name name-style="western"><surname>Bender</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Brasier</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name> (<year>2006</year>) <article-title>Two coincidence detectors for spike timingdependent plasticity in somatosensory cortex</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>4166</fpage>–<lpage>4177</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-RodriguezMoreno1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodriguez-Moreno</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Paulsen</surname><given-names>O</given-names></name> (<year>2008</year>) <article-title>Spike timing-dependent long-term depression requires presynaptic NMDA receptors</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>744</fpage>–<lpage>745</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Karmarkar1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karmarkar</surname><given-names>UR</given-names></name>, <name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name> (<year>2002</year>) <article-title>A model of spike-timing dependent plasticity: one or two coincidence detectors?</article-title> <source>J Neurophysiol</source> <volume>88</volume>: <fpage>507</fpage>–<lpage>513</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Min1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Min</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Nevian</surname><given-names>T</given-names></name> (<year>2012</year>) <article-title>Astrocyte signaling controls spike timing-dependent depression at neocortical synapses</article-title>. <source>Nature neuroscience</source> <volume>15</volume>: <fpage>746</fpage>–<lpage>53</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Larkum1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name> (<year>1999</year>) <article-title>A new cellular mechanism for coupling inputs arriving at different cortical layers</article-title>. <source>Nature</source> <volume>398</volume>: <fpage>338</fpage>–<lpage>341</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Brader1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brader</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>2007</year>) <article-title>Learning real-world stimuli in a neural network with spike-driven synaptic dynamics</article-title>. <source>Neural computation</source> <volume>19</volume>: <fpage>2881</fpage>–<lpage>912</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Turrigiano1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Leslie</surname><given-names>KR</given-names></name>, <name name-style="western"><surname>Desai</surname><given-names>NS</given-names></name>, <name name-style="western"><surname>Rutherford</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>1998</year>) <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source> <volume>391</volume>: <fpage>892</fpage>–<lpage>896</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Abraham1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraham</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>M</given-names></name> (<year>1996</year>) <article-title>Metaplasticity: the plasticity of synaptic plasticity</article-title>. <source>Trends in neurosciences</source> <volume>19</volume>: <fpage>126</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Hulme1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hulme</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>OD</given-names></name>, <name name-style="western"><surname>Abraham</surname><given-names>WC</given-names></name> (<year>2013</year>) <article-title>Emerging roles of metaplasticity in behaviour and disease</article-title>. <source>Trends in neurosciences</source> <volume>36</volume>: <fpage>353</fpage>–<lpage>62</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Gjorgjieva1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gjorgjieva</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Audet</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name> (<year>2011</year>) <article-title>A triplet spike-timing-dependent plasticity model generalizes the bienenstock-cooper-munro rule to higher-order spatiotemporal correlations</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>108</volume>: <fpage>19383</fpage>–<lpage>19388</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Leonard1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leonard</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Doddington</surname><given-names>G</given-names></name> (<year>1993</year>) <article-title>Linguistic Data Consortium</article-title>. <source>Philadelphia</source></mixed-citation>
</ref>
<ref id="pcbi.1003272-Lyon1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Lyon R (1982) A computational model of filtering, detection, and compression in the cochlea. In: Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP '82. volume 7, pp. 1282–1285.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Freund1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freund</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Schapire</surname><given-names>RE</given-names></name> (<year>1999</year>) <article-title>Large margin classification using the perceptron algorithm</article-title>. <source>Machine Learning</source> <volume>37</volume>: <fpage>277</fpage>–<lpage>296</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Huang1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Siew</surname><given-names>C</given-names></name> (<year>2006</year>) <article-title>Extreme learning machine: Theory and applications</article-title>. <source>Neurocomputing</source> <volume>70</volume>: <fpage>489</fpage>–<lpage>501</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Luo1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname><given-names>SX</given-names></name>, <name name-style="western"><surname>Axel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2010</year>) <article-title>Generating sparse and selective third-order responses in the olfactory system of the fly</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>107</volume>: <fpage>10713</fpage>–<lpage>10718</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Brunel1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2000</year>) <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>J Comput Neurosci</source> <volume>8</volume>: <fpage>183</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Renart1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Renart</surname><given-names>A</given-names></name>, <name name-style="western"><surname>de la Rocha</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bartho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Hollender</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Parga</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>The asynchronous state in cortical circuits</article-title>. <source>Science</source> <volume>327</volume>: <fpage>587</fpage>–<lpage>590</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Harris1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Thiele</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Cortical state and attention</article-title>. <source>Nat Rev Neurosci</source> <volume>12</volume>: <fpage>509</fpage>–<lpage>523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Maass1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschlger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Buonomano1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>113</fpage>–<lpage>125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Buonomano2"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buonomano</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Merzenich</surname><given-names>M</given-names></name> (<year>1995</year>) <article-title>Temporal information transformed into a spatial code by a neural network with realistic properties</article-title>. <source>Science</source> <volume>267</volume>: <fpage>1028</fpage>–<lpage>1030</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Morrison1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>Phenomenological models of synaptic plasticity based on spike timing</article-title>. <source>Biological cybernetics</source> <volume>98</volume>: <fpage>459</fpage>–<lpage>78</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Wang1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>HX</given-names></name>, <name name-style="western"><surname>Gerkin</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Nauen</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name> (<year>2005</year>) <article-title>Coactivation and timing-dependent integration of synaptic potentiation and depression</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>187</fpage>–<lpage>193</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Froemke1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2002</year>) <article-title>Spike-timing-dependent synaptic modification induced by natural spike trains</article-title>. <source>Nature</source> <volume>416</volume>: <fpage>433</fpage>–<lpage>438</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Froemke2"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name> (<year>2010</year>) <article-title>Temporal modulation of spike-timing-dependent plasticity</article-title>. <source>Frontiers in synaptic neuroscience</source> <volume>2</volume>: <fpage>19</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Kirkwood1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirkwood</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rioult</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name> (<year>1996</year>) <article-title>Experience-dependent modification of synaptic plasticity in visual cortex</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>526</fpage>–<lpage>528</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-OConnor1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Connor</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wittenberg</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>SSH</given-names></name> (<year>2005</year>) <article-title>Graded bidirectional synaptic plasticity is composed of switch-like unitary events</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>102</volume>: <fpage>9679</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Ko1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ko</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hofer</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Pichler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Buchanan</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Sjöxström</surname><given-names>PJ</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Functional specificity of local synaptic connections in neocortical networks</article-title>. <source>Nature</source> <volume>473</volume>: <fpage>87</fpage>–<lpage>91</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Ko2"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ko</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cossell</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Baragli</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Antolik</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>The emergence of functional microcircuits in visual cortex</article-title>. <source>Nature</source> <volume>496</volume>: <fpage>96</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Lisman3"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname><given-names>J</given-names></name> (<year>1989</year>) <article-title>A mechanism for the Hebb and the anti-Hebb processes underlying learning and memory</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>86</volume>: <fpage>9574</fpage>–<lpage>9578</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Shouval2"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shouval</surname><given-names>HZ</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name> (<year>2002</year>) <article-title>A unified model of NMDA receptor-dependent bidirectional synaptic plasticity</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>99</volume>: <fpage>10831</fpage>–<lpage>10836</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Graupner1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graupner</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume>: <fpage>21551</fpage>–<lpage>21551</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Nishiyama1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nishiyama</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hong</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Mikoshiba</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Kato</surname><given-names>K</given-names></name> (<year>2000</year>) <article-title>Calcium stores regulate the polarity and input specificity of synaptic modification</article-title>. <source>Nature</source> <volume>408</volume>: <fpage>584</fpage>–<lpage>588</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Wittenberg1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wittenberg</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>SSH</given-names></name> (<year>2006</year>) <article-title>Malleability of spike-timing-dependent plasticity at the CA3-CA1 synapse</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>6610</fpage>–<lpage>6617</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Rizzuto1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rizzuto</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pozzan</surname><given-names>T</given-names></name> (<year>2006</year>) <article-title>Microdomains of intracellular Ca<sup>2+</sup>: molecular determinants and functional consequences</article-title>. <source>Physiological reviews</source> <volume>86</volume>: <fpage>369</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Dan1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name> (<year>2004</year>) <article-title>Spike timing-dependent plasticity of neural circuits</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>23</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Hinton1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname><given-names>RR</given-names></name> (<year>2006</year>) <article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source> <volume>313</volume>: <fpage>504</fpage>–<lpage>507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Lee1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Hierarchical bayesian inference in the visual cortex</article-title>. <source>J Opt Soc Am A Opt Image Sci Vis</source> <volume>20</volume>: <fpage>1434</fpage>–<lpage>1448</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Hinton2"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Frey</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name> (<year>1995</year>) <article-title>The “wake-sleep” algorithm for unsupervised neural networks</article-title>. <source>Science</source> <volume>268</volume>: <fpage>1158</fpage>–<lpage>1161</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Harris2"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2008</year>) <article-title>Stability of the fittest: organizing learning through retroaxonal signals</article-title>. <source>Trends Neurosci</source> <volume>31</volume>: <fpage>130</fpage>–<lpage>136</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Watanabe1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watanabe</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Nez</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Sasaki</surname><given-names>Y</given-names></name> (<year>2001</year>) <article-title>Perceptual learning without perception</article-title>. <source>Nature</source> <volume>413</volume>: <fpage>844</fpage>–<lpage>848</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Bear1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name> (<year>1986</year>) <article-title>Modulation of visual cortical plasticity by acetylcholine and noradrenaline</article-title>. <source>Nature</source> <volume>320</volume>: <fpage>172</fpage>–<lpage>176</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Takata1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takata</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mishima</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hisatsune</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Nagai</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ebisui</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Astrocyte calcium signaling transforms cholinergic modulation to cortical plasticity in vivo</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>18155</fpage>–<lpage>18165</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Zou1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zou</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Kinetic models of spike-timing dependent plasticity and their functional consequences in detecting correlations</article-title>. <source>Biol Cybern</source> <volume>97</volume>: <fpage>81</fpage>–<lpage>97</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Gerstner1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name>, <name name-style="western"><surname>van Hemmen</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Wagner</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>A neuronal learning rule for submillisecond temporal coding</article-title>. <source>Nature</source> <volume>383</volume>: <fpage>76</fpage>–<lpage>81</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Toyoizumi1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2007</year>) <article-title>Optimality model of unsupervised spiketiming-dependent plasticity: synaptic memory and weight distribution</article-title>. <source>Neural Comput</source> <volume>19</volume>: <fpage>639</fpage>–<lpage>671</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Legenstein1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Naeger</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2005</year>) <article-title>What can a neuron learn with spike-timing-dependent plasticity?</article-title> <source>Neural Comput</source> <volume>17</volume>: <fpage>2337</fpage>–<lpage>2382</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Izhikevich2"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name> (<year>2007</year>) <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling</article-title>. <source>Cereb Cortex</source> <volume>17</volume>: <fpage>2443</fpage>–<lpage>2452</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Sprekeler1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Michaelis</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Wiskott</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Slowness: an objective for spike-timing-dependent plasticity?</article-title> <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>e112</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Rao1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2001</year>) <article-title>Spike-timing-dependent hebbian plasticity as temporal difference learning</article-title>. <source>Neural Comput</source> <volume>13</volume>: <fpage>2221</fpage>–<lpage>2237</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Diesmann1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gewaltig</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>NEST: An environment for neural systems simulations</article-title>. <source>Forschung und wisschenschaftliches Rechnen, Beitrage zum Heinz-Biling-Preis</source> <volume>58</volume>: <fpage>43</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Davison1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davison</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Brüderle</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Eppler</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kremkow</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Muller</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>PyNN: A common interface for neuronal network simulators</article-title>. <source>Front Neuroinformatics</source> <volume>2</volume>: <fpage>11</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Gtig1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gütig</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Time-warp-invariant neuronal processing</article-title>. <source>PLoS Biol</source> <volume>7</volume>: <fpage>e1000141</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Gtig2"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gütig</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>The tempotron: a neuron that learns spike timing-based decisions</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>420</fpage>–<lpage>428</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Florian1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Florian</surname><given-names>RV</given-names></name> (<year>2010</year>) <article-title>The chronotron: a neuron that learns to fire temporally-precise spike patterns</article-title>. <source>Nature Preceedings</source></mixed-citation>
</ref>
<ref id="pcbi.1003272-Urbanczik1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>A gradient learning rule for the tempotron</article-title>. <source>Neural Comput</source> <volume>21</volume>: <fpage>340</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Turrigiano2"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2004</year>) <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>: <fpage>97</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-vanRossum1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Rossum</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name> (<year>2000</year>) <article-title>Stable Hebbian learning from spike timingdependent plasticity</article-title>. <source>J Neurosci</source> <volume>20</volume>: <fpage>8812</fpage>–<lpage>8821</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003272-Pedregosa1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pedregosa</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Varoquaux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gramfort</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Michel</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Thirion</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Scikit-learn: Machine learning in python</article-title>. <source>Journal of Machine Learning Research</source> <volume>12</volume>: <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>