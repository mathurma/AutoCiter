<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004540</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00633</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Parallel Representation of Value-Based and Finite State-Based Strategies in the Ventral and Dorsal Striatum</article-title>
<alt-title alt-title-type="running-head">Value-Based and Finite State-Based Strategy Coding in the Striatum</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ito</surname>
<given-names>Makoto</given-names>
</name>
<xref rid="cor001" ref-type="corresp">*</xref>
<xref rid="aff001" ref-type="aff"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Doya</surname>
<given-names>Kenji</given-names>
</name>
<xref rid="aff001" ref-type="aff"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Okinawa Institute of Science and Technology Graduate University, Onna-son Okinawa, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Sporns</surname>
<given-names>Olaf</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MI KD. Performed the experiments: MI. Analyzed the data: MI. Contributed reagents/materials/analysis tools: MI. Wrote the paper: MI KD.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ito@oist.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>3</day>
<month>11</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>11</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>11</issue>
<elocation-id>e1004540</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>4</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>8</day>
<month>9</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Ito, Doya</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004540" xlink:type="simple"/>
<abstract>
<p>Previous theoretical studies of animal and human behavioral learning have focused on the dichotomy of the value-based strategy using action value functions to predict rewards and the model-based strategy using internal models to predict environmental states. However, animals and humans often take simple procedural behaviors, such as the “win-stay, lose-switch” strategy without explicit prediction of rewards or states. Here we consider another strategy, the finite state-based strategy, in which a subject selects an action depending on its discrete internal state and updates the state depending on the action chosen and the reward outcome. By analyzing choice behavior of rats in a free-choice task, we found that the finite state-based strategy fitted their behavioral choices more accurately than value-based and model-based strategies did. When fitted models were run autonomously with the same task, only the finite state-based strategy could reproduce the key feature of choice sequences. Analyses of neural activity recorded from the dorsolateral striatum (DLS), the dorsomedial striatum (DMS), and the ventral striatum (VS) identified significant fractions of neurons in all three subareas for which activities were correlated with individual states of the finite state-based strategy. The signal of internal states at the time of choice was found in DMS, and for clusters of states was found in VS. In addition, action values and state values of the value-based strategy were encoded in DMS and VS, respectively. These results suggest that both the value-based strategy and the finite state-based strategy are implemented in the striatum.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>The neural mechanism of decision-making, a cognitive process to select one action among multiple possibilities, is a fundamental issue in neuroscience. Previous studies have revealed the roles of the cerebral cortex and the basal ganglia in decision-making, by assuming that subjects take a value-based reinforcement learning strategy, in which the expected reward for each action candidate is updated. However, animals and humans often use simple procedural strategies, such as “win-stay, lose-switch.” In this study, we consider a finite state-based strategy, in which a subject acts depending on its discrete internal state and updates the state based on reward feedback. We found that the finite state-based strategy could reproduce the choice behavior of rats in a binary choice task with higher accuracy than the value-based strategy. Interestingly, neuronal activity in the striatum, a crucial brain region for reward-based learning, encoded information regarding both strategies. These results suggest that both the value-based strategy and the finite state-based strategy are implemented in the striatum.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by MEXT KAKENHI Grant Number 23120007(KD), MEXT KAKENHI Grant Number 26120729,(MI) and JSPS KAKENHI Grant Number 25430017(MI). KAKENHI: these grants cover a full range of creative and pioneering research from basic to applied fields across the humanities, social sciences and natural sciences (MEXT KAKENHI <ext-link ext-link-type="uri" xlink:href="http://www.mext.go.jp/english/" xlink:type="simple">http://www.mext.go.jp/english/</ext-link>; JSPS KAKENHI: <ext-link ext-link-type="uri" xlink:href="https://www.jsps.go.jp/english/index.html" xlink:type="simple">https://www.jsps.go.jp/english/index.html</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="2"/>
<page-count count="25"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Theoretical studies of decision-making have focused on the dichotomy of whether an environmental model is utilized, i.e. model-free or model-based strategies [<xref rid="pcbi.1004540.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1004540.ref002" ref-type="bibr">2</xref>]. In a typical model-free strategy, called a value-based strategy, the goodness of each action candidate is memorized and learned directly from experienced sequences of state, action, and reward in the form of an action value function [<xref rid="pcbi.1004540.ref002" ref-type="bibr">2</xref>–<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>]. The hypothesis that such value-based strategies are implemented in the cortico-basal ganglia circuit[<xref rid="pcbi.1004540.ref001" ref-type="bibr">1</xref>,<xref rid="pcbi.1004540.ref006" ref-type="bibr">6</xref>] is supported by a growing number of reports of action-value coding neuronal activities in the striatum, the input site of the basal ganglia, in rats [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref008" ref-type="bibr">8</xref>], monkeys [<xref rid="pcbi.1004540.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004540.ref009" ref-type="bibr">9</xref>–<xref rid="pcbi.1004540.ref011" ref-type="bibr">11</xref>], and humans [<xref rid="pcbi.1004540.ref012" ref-type="bibr">12</xref>]. By contrast, in a model-based strategy, the goodness of each action candidate is evaluated indirectly using an internal model of environmental state transitions. Recent fMRI studies found BOLD signals correlated with estimated states and state prediction errors in the prefrontal cortex [<xref rid="pcbi.1004540.ref013" ref-type="bibr">13</xref>–<xref rid="pcbi.1004540.ref015" ref-type="bibr">15</xref>].</p>
<p>While the value-based and model-based strategies have been helpful in dissecting the process of decision-making, the validity of such concepts and consequent predictions need to be assessed in light of actual animal and human behaviors. For example, animals often utilize a simple “win-stay, lose-switch” (WSLS) strategy, in which the same action is repeated if it is rewarded and switched if it is not rewarded [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref016" ref-type="bibr">16</xref>]. This strategy does not conform to either the value-based or the model-based strategy. Theoretical studies have shown that optimal behavior under uncertain state observation can be represented as a finite state machine in which an action is selected depending on the agent’s discrete internal state, and the state is updated based on sensory observation and reward feedback [<xref rid="pcbi.1004540.ref017" ref-type="bibr">17</xref>]. The WSLS strategy is simply realized as a finite state machine with two states.</p>
<p>Here we consider the validity of the finite state-based strategy as another class of model-free strategy along with the value-based strategy in modeling animal choice behaviors. We reanalyze a part of the data we published previously [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], and we show that the finite state strategy fits the choice behavior of rats in a free-choice task more accurately than the value-based strategy and the model-based strategy. We further reanalyze the firing of phasically active neurons (PANs; putative medial spiny neurons) recorded from the dorsolateral striatum (DLS), dorsomedial striatum (DMS), and the ventral striatum (VS) during the task. We show that the individual states of the finite state strategy are encoded in DMS at the time of choice and that clusters of states are encoded in VS. Furthermore, the action values used in the value-based strategy are also encoded in DMS. These results suggest that both the value-based strategy and the finite state strategy are implemented in the striatum.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>In our previous study [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], we gathered behavioral and neuronal data during free-choice and forced-choice tasks. In the present study, we reanalyzed the dataset from free-choice tasks (<xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1A and 1B</xref>), where rats were required to perform a nosepoke to either the left or right hole after cue-tone presentation. A food pellet was delivered probabilistically depending on the selected action. Reward probabilities were varied in a block-wise manner. The dataset contained behavioral and neuronal data from 34,459 trials (202 sessions) involving seven male Long-Evans rats (250–350 g body weight). Neuronal data comprised spike-timing of phasically activity neurons (PANs; putative medium spiny neurons): 204 PANs from the dorsolateral striatum (DLS), 112 PANs from the dorsomedial striatum (DMS), and 138 PANs from the ventral striatum (VS).</p>
<fig id="pcbi.1004540.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Design of the choice task.</title>
<p>(A) A schematic illustration of the experimental chamber. The chamber was equipped with three holes for nose poking (L, left hole; C, center hole; R, right hole) and a pellet dish (D).(B) The time sequence of the choice task. When a rat performed a nose poke in the center hole for 500–1,000 ms, a cue tone (white noise) was presented. The rat had to maintain the nose poke during the presentation of the cue tone, or the trial was terminated as an error trial after presentation of an error tone. After the cue tone, the rat was required to perform a nose-poke in either the left or right hole. Then either a reward tone or a no-reward tone was presented stochastically depending on the rat’s choice and the current left-right probability block. The reward tone was followed by delivery of a sucrose pellet to the pellet dish. Reward probabilities for left and right nose pokes were selected from four pairs [(left, right), (90%, 50%), (50%, 90%), (50%, 10%), and (10%, 50%)]. The probability pair was fixed during a block. Subsequently, the reward probability setting was changed when the choice frequency of the more advantageous side during the last 20 choice trials reached 80%. For this calculation, the same block was held until at least 20 choice trials were completed. A session consisted of four blocks, and the sequence of the reward probability pairs was given in a pseudorandom order, so all four pairs were used once per session. (C) Decision trees averaged by all rats. The left choice probability for all possible experiences in one and two previous trials in the higher reward probability blocks (left) and in the lower reward probability blocks (right). Four types of experiences in one trial [left or right times rewarded (1) or no reward (0)] are represented by different colors and line types. For instance, left probability after L1, <italic>P</italic>(L|L1), is indicated by the right edge of a blue solid line (upper black solid arrow in the left panel), and left probability after R1 L0 (R1 and then L0), <italic>P</italic>(L|R1 L0), is indicated by the right edge of a blue broken line connected to the red solid line (green arrow). Values of trials = 0 (x-axis) represent the left choice probability for all trials. Shaded bands indicate 95% confidence intervals. Significant differences in left choice probabilities for one previous trial between the higher and lower reward probability blocks are marked by brown circles in the right panel (thick circles for <italic>p</italic> &lt; 0.01, a thin circle for <italic>p</italic> &lt; 0.05; chi-squared tests).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g001" position="float" xlink:type="simple"/>
</fig>
<p>All rats successfully adapted to changing reward probabilities. The number of trials needed to reach the block change criterion was smaller in the higher reward probability blocks ((90%, 50%) and (50%, 90%); 33.9 trials on average with a standard deviation of 23.9 trials) than in the lower reward probability blocks ((50%, 10%) and (10%, 50%); 48.9 trials on average with a standard deviation of 29.4 trials, Mann-Whitney U test, <italic>p</italic> &lt; 0.0001). These numbers are significantly smaller than the number required for random choices to reach 80% optimal by chance (about 713 trials; estimated by Monte Carlo method).</p>
<p>We first analyzed how rat choices depended on past experience by calculating decision trees [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>] in the higher and lower reward probability blocks (<xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>). There are four possible types of experience in each trial: L1, L0, R1, and R0, where L or R denotes left or right choice, respectively, and 1 or 0 denotes rewarded or non-rewarded trials, respectively. Averaging all rats, left choice probability after L1, <italic>P</italic>(L|L1) was higher than 0.5 and its symmetric case, <italic>P</italic>(L|R1), was lower than 0.5 (namely, <italic>P</italic>(R|R1) = 1—<italic>P</italic>(L|R1) was higher than 0.5), indicating that a rewarded experience reinforced the tendency for the same choice in the next trial (black solid arrows, <xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>). On the other hand, left choice probability after L0, <italic>P</italic>(L|L0) was less than 0.5 and its symmetric case, <italic>P</italic>(L|R0), was larger than 0.5 (namely, <italic>P</italic>(R|R0) was less than 0.5), indicating that a non-rewarded experience increased the tendency to choose a different action in the next trial (broken arrows, <xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>). For all rats, staying tendency was stronger than switching tendency.</p>
<p>Not only the experience of the previous trial, but also the experiences before the previous trial affected choices. There are 4 x 4 = 16 possible experiences in two consecutive trials, and the left choice probability after each experience is plotted at trial 2 in <xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>. For instance, <italic>P</italic>(L|R1 L0), the left choice probability at trial <italic>t</italic> after R1 at trial <italic>t—</italic>2 then L0 at trial <italic>t—</italic>1 (green arrow, <xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>) is less than <italic>P</italic>(L|L0 L0), the left choice probability after double L0 experiences (orange arrow, <xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>), even though the experiences in the previous trial, L0, are the same.</p>
<p>The decision tree was affected by the reward probability setting (<xref rid="pcbi.1004540.g001" ref-type="fig">Fig 1C</xref>). The staying tendency was significantly stronger in the lower reward probability blocks than in the higher reward probability blocks (<italic>p</italic> &lt; 0.05 for L1, <italic>p</italic> &lt; 0.01 for R1, chi-squared test), and the switching tendency following unrewarded left choice (L0) was significantly stronger in the higher reward probability blocks than in the lower reward probability blocks (<italic>p</italic> &lt; 0.01 for L0, <italic>p</italic> = 0.62 for R0, chi-squared test).</p>
<sec id="sec003">
<title>Model-fitting to rat choice behavior</title>
<p>Next we explore more detailed descriptions of choice behavior using computational models that can predict rat choices based upon past experiences. Along with the Markov models and the value-based strategy tested in our previous study [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], we tested the model-based strategy and the finite state strategy.</p>
<sec id="sec004">
<title>Model-based strategy</title>
<p>For the model-based strategy, we introduced the environmental state estimate (ESE) model, which estimates the current reward setting from past experience using the knowledge that reward probabilities should be one of the following: (Left, Right) = (90%, 50%), (50%, 10%), (50%, 90%), and (10%, 50%). The estimated reward setting is used to calculate action values for left and right, which determine action probability. The performance of this model is characterized by two parameters; the block transition probability <italic>ε</italic>, and the magnitude of reward <italic>κ</italic>. The previous study showed that a binary version of the ESE model could explain human choice behavior better than reinforcement learning models [<xref rid="pcbi.1004540.ref013" ref-type="bibr">13</xref>]. As in the Q-learning with differential forgetting (DFQ-learning model) [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], we considered cases of fixed and time-varying parameters (for more detail, see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>).</p>
</sec>
<sec id="sec005">
<title>Finite state-based strategy</title>
<p>As a computational model for a finite state-based strategy, we introduced the finite state agent (FSA) model, which assumes that an animal has an internal state variable <italic>x</italic> that can assume <italic>N</italic> possible states from 1 to <italic>N</italic>. An action is selected stochastically according to the action probability associated with each state. After execution of an action and feedback of the reward outcome, the state changes according to state transition probabilities. Free parameters of the FSA model are the initial distribution of states, the action probability distribution at each state, and the state transition probability matrix for each action and reward outcome. The FSA model can be regarded as an extended version of the hidden Markov model (HMM). However, unlike the HMM, in the FSA model, the state transition probability depends on the selected action and the reward outcome. In the HMM, the Baum-Welch algorithm [<xref rid="pcbi.1004540.ref019" ref-type="bibr">19</xref>], a form of the expectation-maximization (EM) algorithm, is used to find parameters that maximize the likelihood of the given data. We reformulated the Baum-Welch algorithm for the FSA model (see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>) to fit its parameters to action and reward sequence data. Note that the FSA model is a descriptive model. It can mimic a choice behavior of an animal, but does not explain how the behavior is acquired by the animal.</p>
</sec>
<sec id="sec006">
<title>Evaluation of models</title>
<p>To evaluate how well the FSA and other models predict rat behaviors, we divided the behavioral data into training data (17603 trials, 101 sessions) and test data (16856 trials, 101 sessions). Free parameters of the models were determined to maximize the likelihood of the training data. We compared the performance of the models by the normalized likelihood of the test data, which shows the prediction accuracy of the choice data not used for parameter search (<xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2A</xref>; see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>).</p>
<fig id="pcbi.1004540.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Comparison of model fits.</title>
<p>(A) Normalized likelihoods for the Finite-state and Model-based strategies. For comparison, published data in Ito &amp; Doya 2015, the likelihoods of Markov models and Value-based strategy are also shown. The fitness of the models was measured by the normalized likelihood of the test data, which were obtained from the geometric average of prediction accuracy for unknown data. Numbers in parentheses on the upper x-axis correspond to arithmetic averages of prediction accuracy. Numbers followed by the name of the model indicate numbers of free parameters in each model. “const” or “variable” means that the parameters of each model were assumed to be constant or variable, respectively. Green and brown asterisks indicate a significant difference from the normalized likelihood of the FSA model with 8 states (green arrow) and the FQ-learning model with variable parameters (brown arrow), respectively. ** for <italic>p</italic> &lt; 0.01 and * for <italic>p</italic> &lt; 0.05 in a paired-sample Wilcoxon test (See <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). (B, C) Averaged likelihoods and standard errors (shaded bands) in last 20 trials in the higher (B) and the lower (C) reward probability blocks for the FQ-learning model with variable parameters (red), the FSA with 8 states (green), and in the ESE model with variable parameters (purple).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g002" position="float" xlink:type="simple"/>
</fig>
<p>The performance of the 3rd-order Markov models was the highest in the Markov models, and decreased in 4th- and 5th-order models due to over-fitting [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>]. Within the value-based strategy, the Q-learning with forgetting (FQ-learning) with time-varying parameters showed the highest performance, exceeding that of the best Markov model [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>]. The performance of the model-based strategy (ESE models) was less than that of the 2nd-order Markov model and significantly less than the best Q-learning model, suggesting that rats probably do not use the model-based strategy in this choice task (The parameters of the Q-learning models and the ESE models are reported in Tables <xref rid="pcbi.1004540.t001" ref-type="table">1</xref> and <xref rid="pcbi.1004540.t002" ref-type="table">2</xref>).</p>
<table-wrap id="pcbi.1004540.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.t001</object-id>
<label>Table 1</label> <caption><title>Summary of free parameters of Q-learning models.</title></caption>
<alternatives>
<graphic id="pcbi.1004540.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"/>
<th align="left" rowspan="1" colspan="1"># of parameters</th>
<th align="left" rowspan="1" colspan="1"><italic>α</italic><sub>1</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>α</italic><sub>2</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>κ</italic><sub>1</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>κ</italic><sub>2</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>σ</italic><sub><italic>α</italic></sub></th>
<th align="left" rowspan="1" colspan="1"><italic>σ</italic><sub><italic>κ</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>standard Q (const)</bold></td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">1.9</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>F-Q (const)</bold></td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1"><italic>α</italic><sub>1</sub></td>
<td align="left" rowspan="1" colspan="1">2.1</td>
<td align="left" rowspan="1" colspan="1">1.0</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DF-Q (const)</bold></td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.20</td>
<td align="left" rowspan="1" colspan="1">2.0</td>
<td align="left" rowspan="1" colspan="1">0.7</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Standard Q (variable)</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.10</td>
<td align="left" rowspan="1" colspan="1">0.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>F-Q (variable)</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.12</td>
<td align="left" rowspan="1" colspan="1">0.09</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DF-Q (variable)</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.11</td>
<td align="left" rowspan="1" colspan="1">0.06</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1004540.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.t002</object-id>
<label>Table 2</label> <caption><title>Summary of free parameters of ESE models.</title></caption>
<alternatives>
<graphic id="pcbi.1004540.t002g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.t002" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"/>
<th align="left" rowspan="1" colspan="1"># of parameters</th>
<th align="left" rowspan="1" colspan="1"><italic>ε</italic></th>
<th align="left" rowspan="1" colspan="1"><italic>κ</italic></th>
<th align="left" rowspan="1" colspan="1"><italic>σ</italic><sub><italic>ε</italic></sub></th>
<th align="left" rowspan="1" colspan="1"><italic>σ</italic><sub><italic>κ</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ESE (const)</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">0.19</td>
<td align="left" rowspan="1" colspan="1">6.9</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ESE (variable)</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">0.02</td>
<td align="left" rowspan="1" colspan="1">0.26</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>For the FSA models, interestingly, even with 4 internal states, the likelihood surpassed that of the best reinforcement learning model, the FQ-learning model with varying parameters. The likelihood of the FSA models increased as the number of states increased, with a peak at <italic>N</italic> = 8. The likelihood of the FSA model with 8 states was significantly higher than all other models (one-sided Mann-Whitney U test, <italic>p</italic> &lt; 0.05) except for the FSA model with 6 states. With <italic>N</italic> = 9 or more states, likelihood decreased due to over-fitting.</p>
<p>To clarify why the FSA model performed better than other models, we compared the average likelihood of the best models from the three strategies in the last 20 trials in higher reward probability blocks (<xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2B</xref>) and lower reward probability blocks (<xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2C</xref>). In the later part of higher reward probability blocks, the FSA model and the FQ model showed higher likelihoods than the ESE model. In the later part of lower reward probability blocks, the FSA model showed much higher likelihood than the other two models. The averaged likelihood had an increasing tendency throughout the trials in a block. Because each block ended when the choice probability of the more advantageous side reached 80%, the tendency of selecting the optimal side was stronger in the later part of the block. Therefore, it was easier for models to predict actions in the later. This is the reason for the increasing tendency of the averaged likelihood. As a result, differences between models were clearer in the later part of the block.</p>
<p>Higher likelihood is obtained by a correct prediction with higher confidence (Eq (<xref rid="pcbi.1004540.e004" ref-type="disp-formula">4</xref>) in Materials and Methods). For example, in the trials that the rat selected the left hole, the prediction <italic>P</italic><sub><italic>L</italic></sub> = 0.8 (<italic>P</italic><sub><italic>L</italic></sub>: the predictive probability that the rat would select left) results in higher likelihood than the prediction <italic>P</italic><sub><italic>L</italic></sub> = 0.7. In the trials that the rat selected right, the prediction <italic>P</italic><sub><italic>L</italic></sub> = 0.2 results in higher likelihood than the prediction <italic>P</italic><sub><italic>L</italic></sub> = 0.3. It is consistent with the predictive mode of the FSA model, showing more pronounced changes in action choice probability in lower reward blocks than the FQ model (<xref rid="pcbi.1004540.g003" ref-type="fig">Fig 3</xref>).</p>
<fig id="pcbi.1004540.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g003</object-id>
<label>Fig 3</label>
<caption>
<title>An example of behavioral performance and model fits.</title>
<p>(A) An example of behavioral performance and predictions made by the models. Vertical black lines indicate rat choice behavior. Left and right choices are represented by upper and lower bars, respectively. Rewarded and non-rewarded outcomes are represented by long and short bars, respectively. Model fits, representing the prediction probability that the rat selects left at trial <italic>t</italic>, were estimated using previous choices and the reward outcomes from trial 1 to <italic>t</italic>-1 based on the FQ-learning or the FSA model with 8 states. These are represented by red or green lines, respectively. (B) Estimated action values and varying parameters of the FQ-learning model and standard deviations of posterior probabilities. <italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>, action values for left and right; <italic>α</italic>, the learning rate for the selected action (= forgetting rate for the action not chosen); <italic>κ</italic><sub>1</sub>, the strength of reinforcement by reward; and <italic>κ</italic><sub>2</sub>, the strength of the aversion resulting from the no-reward outcome. (C) Posterior probabilities of internal states (upper panel) and clusters (lower panel) of the FSA model with 8 states shown by stacked graphs. The index of states and clusters corresponds to the index in <xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g003" position="float" xlink:type="simple"/>
</fig>
<p>With <italic>N</italic> = 4 (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4A</xref>), the four states formed two clusters (states 1 and 3, and states 2 and 4) corresponding to the sub-strategies or the belief that “left is better” (cluster left) and “right is better” (cluster right), respectively. In state 1, the model selects left with a high probability (88%) and stays there if it is rewarded, but moves to state 3 with a 48% probability if it is not rewarded. In state 3, the model can be interpreted as doubting the current belief that left is better; the model tries right (83%) and returns to state 1 if it is not rewarded (doubt is cleared), but transits to state 2 or 4 with a 49% probability if it is rewarded (doubt is confirmed). The transition probability for both beliefs is symmetric because we applied a symmetric constraint for the parameters (see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). We also tested the FSA models without a symmetric constraint, but performance was worse than with the constraint. When the number of states increased to <italic>N</italic> = 6, the model has an additional cluster composed of states 2 and 5 (cluster win-stay, lose-switch) (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4B</xref>). In this cluster, as long as the reward is obtained, the same action is selected. Otherwise, the state is changed with an 84% probability, and the model switches the action.</p>
<fig id="pcbi.1004540.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Estimated parameters of finite state agent (FSA) models.</title>
<p>Each state is represented by a blue or red circle, and numbers in circles represent indices of state and action probabilities (%) for left and right. States for which the probability of left (right) is larger than that of right (left) are shown in blue (red). Each arrow with a number indicates the transition probability (%) after left (blue) or right (red) is chosen and a reward is obtained (solid) or not obtained (dashed). For simplicity, only transition probabilities greater than 5% are shown. These parameters were estimated under symmetric constraints. States form clusters that represent different sub-strategies (cluster left, cluster right, and win-stay, lose-switch). See <xref rid="sec014" ref-type="sec">Materials and Methods</xref> for the mathematical definition of the clusters. (A) FSA model with 4 states, (B) 6 states, and (C) 8 states.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g004" position="float" xlink:type="simple"/>
</fig>
<p>The best FSA model with <italic>N</italic> = 8 states has additional states, 2 and 7, in left and right clusters, respectively (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). These additional states allow the model to represent the degree of beliefs more finely. The model believes “left is better” more strongly in state 1 than in state 2, which can directly transit to state 5, where the model doubts the current belief. In the example shown in <xref rid="pcbi.1004540.g003" ref-type="fig">Fig 3A and 3C</xref>, the estimated internal states were mostly in the win-stay, lose-switch cluster during higher reward probability blocks (50%–90% and 90%–50% for left-right) and in the left or right cluster during lower reward probability blocks (10%–50% and 50%–10% for left-right). It is consistent with the property of the win-stay, lose-switch strategy, which is effective only when the reward probability for the optimal action is high.</p>
</sec>
</sec>
<sec id="sec007">
<title>Comparison of simulated model behaviors with actual rat behaviors</title>
<p>While the likelihood of a model fitted to given choice sequences is a useful criterion for comparing models, it is also important to check how the model performs when it runs autonomously. One direct way to check this performance is to compare statistical features of the behavioral sequences produced by the model in a simulation with performance of rats in the actual task (see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). We simulated the Q, FQ, DFQ, and ESE models with constant parameters and the FSA models with 4, 6, and 8 states. We excluded the models with variable parameters because the random walk assumption was effective for fitting a model to a given choice sequence, but not for the generation of choice sequence in a free run.</p>
<p>We took the number of trials required to reach the block-change criterion (80% or more optimal choices in the last 20 trials) as a measure of the flexibility of adaptation (<xref rid="pcbi.1004540.g005" ref-type="fig">Fig 5A–5D</xref>) and the probability that the same action was selected after the rewarded or non-rewarded trial, <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = <italic>a</italic>(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 1) and <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = a(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 0), respectively, as a measures of the robustness of the action (<xref rid="pcbi.1004540.g005" ref-type="fig">Fig 5E and 5F</xref>). Statistics were calculated separately for blocks with higher reward probability settings [(90, 50%) and (50, 90%)] and lower reward probability settings [(50, 10%) and (10, 50%)].</p>
<fig id="pcbi.1004540.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparison of simulated model behaviors with actual rat behaviors.</title>
<p>(A, B) Distributions of trials needed to reach the 80% optimality criterion for rats (gray), FSA with 8 states (green), FQ with constant parameters (red), and ESE with constant parameters (purple) for blocks with higher reward probabilities (A) and for blocks with lower reward probabilities (B). (C, D) The mean number of trials in one block. Data from rats are indicated by blue vertical lines, and confidence intervals (100-5/6%; Bonferroni Method) of the hypothesis that the behavioral data were replicated by each model are represented by horizontal lines. The red color of confidence intervals means that behavioral data are within the confidence interval. (E, F) The mean probability that the same action is selected after rewarded (solid blue lines) and non-rewarded (dashed blue lines) trials, and corresponding confidence intervals of the models (horizontal lines).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g005" position="float" xlink:type="simple"/>
</fig>
<p>We tested the hypothesis that data from rats could be generated from each model using the mean of the six statistics (<xref rid="pcbi.1004540.g005" ref-type="fig">Fig 5C–5F</xref>). Only the FSA model with 8 states was not rejected by any statistical test (the level of the confidence interval for each statistic was set to (100–5/6)%, so that the chance of at least one false rejection is 5%; Bonferroni Method). This result shows that only the FSA model with 8 states sufficiently reproduces the behavior observed in the rats, although it does not exclude the possibility that there are other models better than the FSA model with 8 states.</p>
</sec>
<sec id="sec008">
<title>Neural coding</title>
<p>Previous studies have shown that striatal neurons code not only observable behavioral variables, such as action and reward [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004540.ref020" ref-type="bibr">20</xref>–<xref rid="pcbi.1004540.ref022" ref-type="bibr">22</xref>], but also hidden variables estimated from behavior using computational models, such as action values [<xref rid="pcbi.1004540.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>]. In our previous study [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], regression analysis revealed that action values, which were estimated from behavioral data based on the FQ-learning with variable parameters, were coded most strongly in DMS during action execution.</p>
<p>In this analysis, we re-analyzed the same neuronal data to examine whether a new class of hidden variables, namely, states and state clusters of the FSA with 8 states, were also coded. However, if we use a regression model that employs only states and clusters as regressors, it would lead to Type I errors (false positives). For instance, the estimate of state 1 is strongly correlated with the left action choice in the same trial, detecting action-coding neurons as state-coding neurons (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). To avoid this problem, we first considered a full model including all possible variables (30 variables) that might be coded by striatal neurons (Poisson regression model, see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). Then, we extracted only the important variables to explain the output using lasso regularization [<xref rid="pcbi.1004540.ref024" ref-type="bibr">24</xref>] (see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). The full model we used was:
<disp-formula id="pcbi.1004540.e001">
<alternatives>
<graphic id="pcbi.1004540.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">log</mml:mi><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mi>b</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.90em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>V</mml:mi></mml:msub><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>L</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>:</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.90em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.90em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mn>8</mml:mn><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.90em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>L</mml:mi><mml:mo>:</mml:mo><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>:</mml:mo><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <italic>μ</italic>(<italic>t</italic>) is the expected number of spikes at trial <italic>t</italic> in a certain time bin and <italic>β</italic><sub><italic>i</italic></sub> is the regression coefficient for each explanatory variable (regressor). <italic>b</italic>(<italic>t</italic>) is the monotonically increasing factor, namely, <italic>b</italic>(<italic>t</italic>) = <italic>t</italic>, which is inserted to capture the task event-independent monotonic increases or decreases in firing pattern. The remaining regressors are classified into three types:</p>
<list list-type="order">
<list-item><p>observable information: <italic>a</italic>(<italic>t</italic>) ∈ {1: Left, 2: Right}, the selected action; <italic>r</italic>(<italic>t</italic>) ∈ {1: Rewarded, 0: Non-rewarded}, reward availability; and <italic>a</italic>(<italic>t</italic>-1), and <italic>r</italic>(<italic>t</italic>-1), action and reward in the previous trial, respectively.</p></list-item>
<list-item><p>estimated information based on the FQ-learning: <italic>Q</italic><sub><italic>L</italic></sub>(<italic>t</italic>) and <italic>Q</italic><sub><italic>R</italic></sub>(<italic>t</italic>), action values estimated by the FQ-learning model with varying parameters [<xref rid="pcbi.1004540.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1004540.ref011" ref-type="bibr">11</xref>,<xref rid="pcbi.1004540.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>]; <italic>Q</italic><sub><italic>c</italic></sub>(<italic>t</italic>) ≡ <italic>Q</italic><sub><italic>a</italic>(<italic>t</italic>)</sub>(<italic>t</italic>), the action value for the selected action (chosen value) [<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1004540.ref025" ref-type="bibr">25</xref>]; <italic>V</italic>(<italic>t</italic>) ≡ <italic>P</italic><sub><italic>L</italic>:<italic>Q</italic></sub>(<italic>t</italic>)<italic>Q</italic><sub><italic>L</italic></sub>(<italic>t</italic>) + (1 – <italic>P</italic><sub><italic>L</italic>:<italic>Q</italic></sub>(<italic>t</italic>))<italic>Q</italic><sub><italic>R</italic></sub>(<italic>t</italic>), the state value as defined by the average of action values [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref025" ref-type="bibr">25</xref>]; and <italic>P</italic><sub><italic>L</italic>:<italic>Q</italic></sub>(<italic>t</italic>), the action probability estimated by the FQ-learning model</p></list-item>
<list-item><p>estimated information based on the FSA model: <italic>x</italic><sub>1</sub>(<italic>t</italic>),…, <italic>x</italic><sub>8</sub>(<italic>t</italic>) (same as <italic>γ</italic><sup><italic>n</italic></sup>(<italic>t</italic>) in the FSA model, see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>), the posterior probabilities of states that the FSA with 8 states may take at trial <italic>t</italic>; <italic>x</italic><sub>1</sub>(<italic>t</italic>+1),…, <italic>x</italic><sub>8</sub>(<italic>t</italic>+1), the posterior probabilities of transited states; <italic>C</italic><sub><italic>L</italic></sub>(<italic>t</italic>), <italic>C</italic><sub><italic>R</italic></sub>(<italic>t</italic>), and <italic>C</italic><sub><italic>WSLS</italic></sub>(<italic>t</italic>), the posterior probabilities of clusters, namely, the sum of the corresponding state probabilities; and <italic>P</italic><sub><italic>L</italic>:<italic>FSA</italic></sub>(<italic>t</italic>), the action probability estimated by the FSA model.</p></list-item>
</list>
<p>We applied lasso to this full model, which can identify minimally important regressors among many and redundant regressors (see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). When lasso identified certain regressors to explain the activity of a certain neuron, we interpreted this to mean that “the neuron coded the regressors.” A single striatum neuron tended to code multiple variables in different time bins as shown in <xref rid="pcbi.1004540.g006" ref-type="fig">Fig 6</xref>. Lasso detected significant populations of neurons that coded observable information (I) and estimated information based on the FQ-learning (II), similar to our previous analysis [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>]. In addition, this analysis detected neurons that coded states of the FSA model (III). <xref rid="pcbi.1004540.g006" ref-type="fig">Fig 6A–6C</xref> show an example of DMS neurons in which firing rate was significantly correlated with the posterior probability of states of the FSA model. During action selection, firing rate was best explained by the regression model including not only the action, but also <italic>x</italic><sub>5</sub>(<italic>t</italic>), in which the FSA model doubts the current belief that left hole is better and wants to choose the right hole (see <xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). <xref rid="pcbi.1004540.g006" ref-type="fig">Fig 6D–6F</xref> show an example of DMS neurons in which firing rate was significantly correlated with the posterior probability of a transited state of the FSA model. The firing rate during the rat’s entry to the left or right hole (note that the reward or non-reward tone was presented at the onset of the hole poke) was best explained by the regression model, including not only the action, reward, and <italic>x</italic><sub>7</sub>(<italic>t</italic>), but also <italic>x</italic><sub>7</sub>(<italic>t</italic>+1). Here the FSA model believes the right hole is better following an exploratory choice (see <xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). <xref rid="pcbi.1004540.g006" ref-type="fig">Fig 6G–6I</xref> show an example of VS neurons coding the rat’s sub-strategy (cluster). There was a significant, positive correlation between neuronal firing rate during action selection and the posterior probability of the win-stay, lose-switch cluster estimated by the FSA model with 8 states.</p>
<fig id="pcbi.1004540.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Examples of neuronal activities correlated with internal variables in the FSA model with 8 states.</title>
<p>(A) Firing activity of a DMS neuron that was correlated with the posterior probability of state 5 of the FSA model with 8 states. Firing rates in trials where the estimated posterior probability of state 5 was high and low were shown by green or gray event-aligned spike histograms (EASHs; see <xref rid="sec014" ref-type="sec">Materials and Methods</xref>). (B) Information coded in the neuron shown in (A). Blue and red time bins for each regressor indicate time bins where neuronal activity was positively and negatively correlated with the regressor, respectively. In regressors selected by lasso from the 30 regressors for each time bin, only regressors that were detected for more than two adjacent time bins are shown (two regressors, in this case). (C) The correlation between firing rate and posterior probability of state 5. The firing rate in yellow time bins shown in (A) and the posterior probability of state 5 for each trial are plotted with gray lines in the upper and lower panels, respectively. Black lines were smoothed with a Gaussian filter using the standard deviation of three trials. (D, E, F) Firing activity of a DMS neuron that was correlated with state 7 at the next trial estimated by the FSA model with 8 states. (G, H, I) Firing activity of a VS neuron that was correlated with the sub-strategy (win-stay, lose-switch; WSLS) estimated by the FSA model with 8 states.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g006" position="float" xlink:type="simple"/>
</fig>
<p>In our previous study, we detected action-value coding neurons and state-value coding neurons by linear regression analysis, in which action values estimated by the FQ-learning were used as regressors. In this study, we used an augmented regression model (Poisson regression model), including not only variables of the FQ-learning, but also variables of the FSA models. As a result, neurons coding variables of the FQ-learning were still detected (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7A–7C</xref>) as in our previous analysis [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], although the performance of the FQ-learning model was worse than that of the FSA model. Significant proportions of neurons in which the firing rates were correlated with action values (<italic>Q</italic><sub>L</sub> or/and <italic>Q</italic><sub>R</sub>) were found in all regions (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7B</xref>). Significant proportions of state value- (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7A</xref>) and chosen value-coding neurons (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7C</xref>) were found mainly in DMS and VS.</p>
<fig id="pcbi.1004540.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Proportions of neurons coding variables of the value-based and finite state-based strategies.</title>
<p>Proportions of neurons showing significant correlations (<italic>p</italic> &lt; 0.01, t test) with variables of the value-based strategy (FQ-learning) (A, B, C) and the finite state-based strategy (FSA model with 8 states) (D, E, F). These neurons were detected by lasso regularization of a Poisson regression model, which was conducted for 500 ms before and after the seven trial events (entry into the center hole, the tone onset, the tone offset, the exit from the center hole, the entry into the L/R hole, and the exit from the L/R hole) for DLS (blue), DMS (green), and VS (pink). Colored disks mean that the populations are significantly higher than by chance (<italic>p</italic> &lt; 0.05, binominal test). (A) Neurons coding state values, the average of action values. (B) Neurons coding action values, <italic>Q</italic><sub>L</sub> and/or <italic>Q</italic><sub>R</sub>. (C) Neurons coding chosen values, action values for the selected action. (D) Neurons coding at least one cluster (sub-strategy) of the FSA model with 8 states; cluster left, and/or cluster right, and/or win-stay, lose-switch. (E) Neurons coding at least one current state from <italic>x</italic><sub>1</sub>(<italic>t</italic>) to <italic>x</italic><sub>8</sub>(<italic>t</italic>) of the FSA model. (F) Neurons coding at least one next state from <italic>x</italic><sub>1</sub>(<italic>t</italic>+1) to <italic>x</italic><sub>8</sub>(<italic>t</italic>+1) of the FSA model.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g007" position="float" xlink:type="simple"/>
</fig>
<p>A substantial proportion of striatal neurons also coded internal states of the FSA model (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7D–7F</xref>). A significant proportion of cluster-coding (<italic>C</italic><sub>L</sub>, <italic>C</italic><sub>R</sub>, and/or <italic>C</italic><sub>WSLS</sub>) neurons were found in VS (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7D</xref>), which might be similar to the strategy-coding neurons reported in monkey striatum [<xref rid="pcbi.1004540.ref026" ref-type="bibr">26</xref>]. The proportion of neurons coding <italic>x</italic>(<italic>t</italic>) in DMS showed a peak during the action execution (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7E</xref>). After entry into the left or right hole (and the reward or no-reward tone was presented), populations of <italic>x</italic>(<italic>t</italic>+1) in all regions were increased (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7F</xref>), consistent with state transition dependence on reward feedback. Some neurons in DMS showed firing correlated with <italic>x</italic>(<italic>t</italic>+1) even before presentation of the reward or no-reward tone (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7F</xref>), which was possible because the reward was highly predictable (90% or 10%) in one of the actions in each block.</p>
<p>Were variables of the FQ-learning and the FSA models separately coded in different neurons? During action execution (500 ms before entry into the L/R hole), neurons coding only the variables of the FQ-learning model (state value, action value, chosen value) were 6.9% (14/204) in DLS, 8.9% (10/112) in DMS, and 2.9% (4/138) in VS. Neurons coding only FSA-related variables (sub strategy, <italic>x</italic>(<italic>t</italic>), <italic>x</italic>(<italic>t</italic>+1)) were 8.8% (18/204) in DLS, 22.3% (25/112) in DMS, and 14.5% (20/138) in VS. Neurons coding both variables were 2.0% (4/204) in DLS, 7.1% (8/112) in DMS, and 8.7% (12/138) in VS. While VS neurons significantly tended to code variables of both models, in DLS and DMS there were no significant tendencies (<italic>p</italic> = 0.10 for DLS, <italic>p</italic> = 0.13 for DMS, and <italic>p</italic> &lt; 0.0001 for VS, chi-squared tests).</p>
<p>Interestingly, not all states were equally coded in the striatum (<xref rid="pcbi.1004540.g008" ref-type="fig">Fig 8</xref>). During action execution (<xref rid="pcbi.1004540.g008" ref-type="fig">Fig 8A</xref>), only the proportion of state-4- and state-5-coding neurons in DMS and VS (also state 6 and 8 in DMS) were statistically significant, and both states preceded an exploratory action in the keep-left and keep-right clusters (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). After execution of an action and reward feedback (<xref rid="pcbi.1004540.g008" ref-type="fig">Fig 8B</xref>), representations of most subsequent states appeared in DLS and DMS, while representations of the same state <italic>x</italic><sub>5</sub>, persisted in VS. Interestingly, states 2 and 7 are major transition targets from states 4 and 5, and these signals, especially, the signal of state 7, were prominent in DLS.</p>
<fig id="pcbi.1004540.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004540.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Breakdowns of state-coding neurons shown in <xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7E and 7F</xref>.</title>
<p>(A, B), The proportion of neurons coding <italic>x</italic>(<italic>t</italic>) during the 500 ms before entry into the L/R hole, and <italic>x</italic>(<italic>t</italic>+1) during 500 ms after exit from L/R hole, respectively. The color for each state showing a significant proportion (<italic>p</italic> &lt; 0.05, binominal test) corresponds to the color in the simplified diagram of the state transition in the FSA model with 8 states shown in (C). Populations with less than chance probabilities are shown in gray.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.g008" position="float" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>To explore what types of decision-making algorithms are utilized and implemented in the basal ganglia, we evaluated three different strategies for reproducing choice behaviors of rats, and examined their neural correlates in the striatum. We found that the finite state strategy matched the choice behavior of rats most faithfully, both in the normalized likelihood with fitting to the choice sequences and in the statistical properties of the choice behavior in autonomous simulations. Neuronal activity analysis revealed that variables used in both the finite state and value-based strategies were encoded in the striatum. These findings suggest that both finite state and value-based strategies were processed in parallel in brain circuits that include the striatum, while actual choices of rats were predominantly determined by the finite state strategy in the present task.</p>
<sec id="sec010">
<title>Finite state-based strategy</title>
<p>The finite state-based strategy implemented with <italic>N</italic> = 8 states showed a significantly higher prediction accuracy (average likelihood) for rat choice behaviors than the best reinforcement learning model, the FQ-learning model [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>] [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>]. Furthermore, we compared statistical features of the time course of learning (the number of trials to reach 80% optimality) and the probabilities of repeating the same action after rewarded or non-rewarded outcomes of the rats and the algorithms when faced the same task (<xref rid="pcbi.1004540.g005" ref-type="fig">Fig 5</xref>). We found that only the FSA model with 8 states could reproduce those features similar to the rats. Therefore the FSA model is the best model to predict rat actions in individual trials and also to reproduce generic features of the time course of learning, although we cannot deny the possibility that there might be an even better model in both respects.</p>
<p>The FSA model is conceptually different from the other models. The Q-learning (FQ-learning) models and the ESE models are normative models that prescribe behaviors for maximization of rewards, whereas the FSA model is a descriptive model that seeks only to describe the behavior as it appears in the data [<xref rid="pcbi.1004540.ref027" ref-type="bibr">27</xref>]. The reformulated Baum-Welch algorithm was used not to find the parameters with which the models maximize the reward, but to find the parameters with which the models mimic the choice behavior of rats. The FSA models do not explain why and how the rats learned the procedure (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4C</xref>). If an FSA-like algorithm is implemented in the brain, how could the algorithm learn the appropriate choice and transition probabilities to efficiently obtain a reward? A possible scenario is that rats use the value-based strategy in the beginning of the training. Meanwhile, the finite state strategy monitored behavior to form a procedure that mimicked the value-based strategy without explicit value evaluation. After massive training, the procedure was formed, and the finite state strategy overrode action selection. We speculate that the finite state strategy could be regarded as generalized habit formation. Traditionally, habitual actions are considered automatic responses controlled by simple stimulus-response associations without any associative links to the outcome of those actions [<xref rid="pcbi.1004540.ref028" ref-type="bibr">28</xref>]. The finite state strategy could be considered as an extended habitual action that depends not only on stimuli, but also internal states. To test this idea, further behavioral experiment will be required.</p>
<p>Internal states of the FSA model were represented in the all three subregions of the striatum (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7E and 7F</xref>), while it has been reported that habitual actions involve DLS [<xref rid="pcbi.1004540.ref028" ref-type="bibr">28</xref>–<xref rid="pcbi.1004540.ref031" ref-type="bibr">31</xref>]. We speculate that retention of internal states required for the FSA model involves the working memory functions of the prefrontal cortex [<xref rid="pcbi.1004540.ref032" ref-type="bibr">32</xref>], which can explain the internal state representation in not only DLS, but also DMS and VS, where the prefrontal cortex projects [<xref rid="pcbi.1004540.ref033" ref-type="bibr">33</xref>].</p>
<p>Analysis of neuronal activities suggests that all striatal areas we recorded, namely, DLS, DMS, and VS, are involved in the finite state strategy. Interestingly, not all states were equally coded in the striatum (<xref rid="pcbi.1004540.g008" ref-type="fig">Fig 8A and 8B</xref>). While codings of <italic>x</italic><sub>4</sub>(<italic>t</italic>) and <italic>x</italic><sub>5</sub>(<italic>t</italic>) were found in DMS and VS, coding of <italic>x</italic><sub>1</sub>(<italic>t</italic>), <italic>x</italic><sub>2</sub>(<italic>t</italic>), <italic>x</italic><sub>3</sub>(<italic>t</italic>), and <italic>x</italic><sub>7</sub>(<italic>t</italic>) was not observed in any areas. Note that <italic>x</italic><sub>4</sub>(<italic>t</italic>) and <italic>x</italic><sub>5</sub>(<italic>t</italic>) are the states in which an action is likely to be switched after repeated unrewarded actions at <italic>x</italic><sub>1</sub>(<italic>t</italic>), <italic>x</italic><sub>2</sub>(<italic>t</italic>), <italic>x</italic><sub>8</sub>(<italic>t</italic>) or <italic>x</italic><sub>7</sub>(<italic>t</italic>). This uneven representation of states suggests that the finite state strategy is implemented in a larger brain circuit that includes the striatum. The requirement of working memory to store the current state suggests the involvement of other brain regions, such as the prefrontal cortex and the hippocampus. Then why are <italic>x</italic><sub>4</sub>(<italic>t</italic>) and <italic>x</italic><sub>5</sub>(<italic>t</italic>) are selectively coded in the striatum? It has been reported that the anterior cingulate cortex (ACC) plays an important role in switching behavior evoked by error feedback [<xref rid="pcbi.1004540.ref034" ref-type="bibr">34</xref>]. The connection from the ACC to the striatum for the execution of switching [<xref rid="pcbi.1004540.ref035" ref-type="bibr">35</xref>] may be the source of strong coding of <italic>x</italic><sub>4</sub>(<italic>t</italic>) and <italic>x</italic><sub>5</sub>(<italic>t</italic>) observed in DMS and VS.</p>
</sec>
<sec id="sec011">
<title>Value-based strategy</title>
<p>Previous studies have reported that action-value signals are represented in the striatum of rodents [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref008" ref-type="bibr">8</xref>], monkeys [<xref rid="pcbi.1004540.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004540.ref011" ref-type="bibr">11</xref>,<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1004540.ref036" ref-type="bibr">36</xref>] and humans [<xref rid="pcbi.1004540.ref012" ref-type="bibr">12</xref>], suggesting that the value-based strategy is implemented in the basal ganglia. Consistent with these reports, our previous study [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>] reported that state value signals were most strongly represented in VS, and that action value signals were most strongly represented in DMS during action execution.</p>
<p>In the present study, we reanalyzed the same dataset as the previous study, with a more complex regression model, including not only action values, but also state values, the chosen value, and variables of the FSA model that best explained animal behaviors. We applied lasso regularization to the augmented regression model, and similar results were reproduced; strong state-value coding in VS (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7A</xref>), and a peak of the proportion of action-value coding neurons in DMS during action execution (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7B</xref>). In addition, we found that the signal of the chosen value, previously reported in monkeys [<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1004540.ref026" ref-type="bibr">26</xref>] and rats [<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>], was represented in VS in our dataset (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7C</xref>).</p>
<p>It has been proposed that DMS is involved in goal-directed actions [<xref rid="pcbi.1004540.ref028" ref-type="bibr">28</xref>,<xref rid="pcbi.1004540.ref030" ref-type="bibr">30</xref>] based on lesion studies [<xref rid="pcbi.1004540.ref037" ref-type="bibr">37</xref>,<xref rid="pcbi.1004540.ref038" ref-type="bibr">38</xref>]. Formation of goal-directed action is thought to require an association between actions and outcomes, which is analogous to the action value in reinforcement learning. Accordingly, action-value coding in DMS matches the proposal of goal-directed action in DMS. The action value for the selected action, called the chosen value [<xref rid="pcbi.1004540.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004540.ref023" ref-type="bibr">23</xref>], which is necessary for updating action values, was observed in VS. Furthermore, consistent with previous reports in rodents [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>], state-value representation was observed in VS (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7E</xref>). These findings suggest that the value-based strategy is implemented in the striatum, although the final action choices are better characterized by the finite state-based strategy.</p>
</sec>
<sec id="sec012">
<title>Environmental model-based strategy</title>
<p>The likelihood of the ESE model for the model-based strategy was much lower than that of the FQ-learning model for the value-based strategy or that of the FSA model for the finite state strategy. Thus, rats may not have estimated the reward setting in our task. In this task, four pairs of reward probabilities were used, but in the previous report in human subjects [<xref rid="pcbi.1004540.ref013" ref-type="bibr">13</xref>], only two pairs were used. Therefore, it might be too difficult for rats to estimate one reward setting from four possible pairs.</p>
</sec>
<sec id="sec013">
<title>Hierarchical structure in the striatum</title>
<p>The present results support the notion of a hierarchical structure in the cortico-basal ganglia loops, but suggest specific roles for different loops in implementation of the value-based and finite state-based strategies. Representation of state values and sub-strategies (clusters) in VS (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7A and 7D</xref>) suggests a role for this region in higher-level decisions, namely, selection of sub-strategies depending on the frequency of reward [<xref rid="pcbi.1004540.ref039" ref-type="bibr">39</xref>,<xref rid="pcbi.1004540.ref040" ref-type="bibr">40</xref>]. Robust coding of action values and states responsible for action switching in DMS (<xref rid="pcbi.1004540.g007" ref-type="fig">Fig 7D and 7G</xref>) points to a role for this region in flexible action adaptation. Action coding in DLS was equal to or stronger than that in DMS before movement onset [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], suggesting a major role for this region in action preparation and initiation.</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec015">
<title>Ethics statement</title>
<p>All experimental procedures were performed in accordance with guidelines approved by the Okinawa Institute of Science and Technology Experimental Animal Committee.</p>
</sec>
<sec id="sec016">
<title>Dataset</title>
<p>A part of the dataset used in our previous study [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>] was reused in this study. Behavioral and neuronal data were gathered from seven Long-Evans rats. The number of sessions completed by each rat was from 24 to 33. The average (+ standard deviation) of the trials per session was 41.10 (+ 27.58) trials. Neurons stably recorded from at least two sessions were 260 in DLS, 178 in DMS, and 179 in VS (on average, recorded from 2.7 sessions). From this dataset, phasically active neurons (PANs; 204 from DLS, 112 from DMS, and 138 from VS) were extracted based on inter-spike interval statistics. The proportion of inter-spike intervals (ISIs) that was &gt; 1 s of total recoding time (Pr<italic>op</italic><sub><italic>ISIs</italic>&gt;1<italic>s</italic></sub>) was calculated for each neuron [<xref rid="pcbi.1004540.ref041" ref-type="bibr">41</xref>]. Then, neurons for which Pr<italic>op</italic><sub><italic>ISIs</italic>&gt;1<italic>s</italic></sub>&gt; 0.4 were regarded as PANs.</p>
</sec>
<sec id="sec017">
<title>Event-aligned spike histograms (EASHs)</title>
<p>Intervals of the six task events (entry into the center hole, onset of the cue tone, offset of the cue tone, exit from the center hole, entry into the left or right hole, and exit from the left or right hole) varied by trials. To align event timings for all trials, event-aligned spike histograms (EASHs) were proposed by Ito and Doya [<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>]. First, the average duration for each event interval was calculated. Then, spike timings in a certain event interval for each trial were linearly transformed into corresponding averaged event intervals. Finally, histograms of the number of spikes for each 100 ms time window were calculated (<xref rid="pcbi.1004540.g006" ref-type="fig">Fig 6A, 6D and 6G</xref>).</p>
</sec>
<sec id="sec018">
<title>Decision-making models</title>
<p>Any decision-making models for a single stimulus (state) and binary choice (action) can be defined by the conditional probability of a current action given past experiences:
<disp-formula id="pcbi.1004540.e002">
<alternatives>
<graphic id="pcbi.1004540.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <italic>e</italic>(1:<italic>t</italic>-1) is a simple description of <italic>e</italic>(1), <italic>e</italic>(2),…, <italic>e</italic>(<italic>t</italic>-1). <italic>e</italic>(<italic>t</italic>) is a set of an action and a reward <italic>e</italic>(<italic>t</italic>) = {<italic>a</italic>(<italic>t</italic>), <italic>r</italic>(<italic>t</italic>)}, and action <italic>a</italic>(<italic>t</italic>) and reward <italic>r</italic>(<italic>t</italic>) can be <italic>L</italic> or <italic>R</italic> and 1 or 0, respectively. Behavioral data are composed of a set of sequences (sessions) of actions and rewards. If necessary, we use the index <italic>l</italic> as the index of sessions, for example <italic>a</italic>{<italic>l</italic>}(<italic>t</italic>). The number of trials for session <italic>l</italic> is represented by <italic>T</italic><sub><italic>l</italic></sub>, and the number of sessions is <italic>L</italic>.</p>
<p>To fit parameters to choice data and to evaluate the models, we used the likelihood criterion, which is the probability that the observed data were produced by the model. The likelihood can be normalized, so that it equals 0.5 when predictions are made with chance-level accuracy (<italic>P</italic><sub><italic>L</italic></sub>(<italic>t</italic>) = 0.5 for all <italic>t</italic>). The normalized likelihood is defined by
<disp-formula id="pcbi.1004540.e003">
<alternatives>
<graphic id="pcbi.1004540.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi>z</mml:mi><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>z</italic>{<italic>l</italic>}(<italic>t</italic>) is the likelihood for a single trial:
<disp-formula id="pcbi.1004540.e004">
<alternatives>
<graphic id="pcbi.1004540.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e004" xlink:type="simple"/>
<mml:math display="block" id="M4" overflow="scroll">
<mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula></p>
<p>The (normalized) likelihood can be regarded as the prediction accuracy, namely, how accurately the model predicts actions using past experiences. Generally, models that have a larger number of free parameters can fit data more accurately and thus show a higher likelihood. However, these models may not be able to fit new data due to over-fitting. For fair comparison of models, choice data were divided into training data (101 sessions) and test data (101 sessions). Free parameters of a model were determined to maximize the likelihood of training data. Then, the model was evaluated by the likelihood or the normalized likelihood of the test data (holdout validation). Therefore, in this model fitting, each model was fitted to all training set trials from all seven rats with the same free parameters. <xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2A</xref> represents the normalized likelihood for the total of test 101 sessions. For statistical tests of the normalized likelihood between the models (<xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2A</xref>), we compared the normalized likelihood of each session for the same parameters between the models by a paired-sample Wilcoxon test.</p>
<p>From the above process, we obtained the likelihood of each trial (<xref rid="pcbi.1004540.e004" ref-type="disp-formula">4</xref>) in all sessions (both training and test data) for each model with the parameters estimated by training data. To compare fitting performance, we averaged the sequences of the likelihoods of the last 20 trials over all blocks with higher or lower reward probabilities (<xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2B and 2C</xref>). To test significant differences between the FSA model and the DFQ model, the Mann-Whitney U test was applied to the likelihoods for every trial.</p>
<p>Note that the normalized likelihood depends on the number of trials. If an animal’s choice probability does not change over trials, namely, <italic>P</italic>(<italic>a</italic>(<italic>t</italic>) <italic>=</italic> L) <italic>= P</italic>, and model prediction <italic>P</italic><sub><italic>L</italic></sub>(<italic>t</italic>) is also constant <italic>P</italic><sub><italic>L</italic></sub>, then the expected normalized likelihood for <italic>T</italic> trials is given by
<disp-formula id="pcbi.1004540.e005">
<alternatives>
<graphic id="pcbi.1004540.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e005" xlink:type="simple"/>
<mml:math display="block" id="M5" overflow="scroll">
<mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>T</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:msup><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
This expected normalized likelihood rapidly decreases when the number of trials increases, and when <italic>T</italic> goes to infinite, it converges to
<disp-formula id="pcbi.1004540.e006">
<alternatives>
<graphic id="pcbi.1004540.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e006" xlink:type="simple"/>
<mml:math display="block" id="M6" overflow="scroll">
<mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>P</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
For example, let’s assume that a rat’s choice probability is <italic>P</italic> = 0.8 and model A predicts it perfectly by <italic>P</italic><sub><italic>L</italic></sub> = 0.8, the (normalized) likelihood is <inline-formula id="pcbi.1004540.e007"><alternatives><graphic id="pcbi.1004540.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e007" xlink:type="simple"/>
<mml:math display="inline" id="M7" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.68</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, it’s less than <italic>P</italic><sub><italic>L</italic></sub>, and it decreases to <inline-formula id="pcbi.1004540.e008"><alternatives><graphic id="pcbi.1004540.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e008" xlink:type="simple"/>
<mml:math display="inline" id="M8" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.61</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula> when <italic>T</italic> increases. If model B predicts with <italic>P</italic><sub><italic>L</italic></sub> = 0.7, <inline-formula id="pcbi.1004540.e009"><alternatives><graphic id="pcbi.1004540.e009g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e009" xlink:type="simple"/>
<mml:math display="inline" id="M9" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.62</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula> and <inline-formula id="pcbi.1004540.e010"><alternatives><graphic id="pcbi.1004540.e010g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e010" xlink:type="simple"/>
<mml:math display="inline" id="M10" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.59</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, the difference in the normalized likelihood between model A and model B also decreases (0.07 → 0.02) when <italic>T</italic> changes from 1 to infinity. This is the reason why the normalized likelihoods of models shown in <xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2A</xref> (<italic>T</italic> = 16856 trials) are much less than the likelihoods shown in <xref rid="pcbi.1004540.g002" ref-type="fig">Fig 2B and 2C</xref> (<italic>T</italic> = 1 trial).</p>
</sec>
<sec id="sec019">
<title>Markov models</title>
<p><italic>d</italic>th-order Markov models are the simplest non-parametric models. They predict an action at trial <italic>t</italic>, <italic>a</italic>(<italic>t</italic>), from the past <italic>d</italic>-length sequence of experiences before <italic>t</italic>, <italic>e</italic>(<italic>t-d</italic>:<italic>t</italic>-1). The prediction of the <italic>d</italic>th-order Markov model was given by the following:
<disp-formula id="pcbi.1004540.e011">
<alternatives>
<graphic id="pcbi.1004540.e011g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e011" xlink:type="simple"/>
<mml:math display="block" id="M11" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
where <italic>N</italic><sub><italic>i</italic></sub>(<italic>e</italic>(<italic>t</italic> − <italic>d</italic>:<italic>t</italic> − 1)) is the number of <italic>i</italic> (L or R) chosen after every <italic>d</italic>-length sequence of the exact same sequence as <italic>e</italic>(<italic>t-d</italic>:<italic>t</italic>-1) in the whole training data [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>]. The <italic>d</italic>th-order Markov model has more than 4<sup><italic>d</italic></sup> free parameters because there are four types of possible experiences in a single trial (more precisely, the number of the parameters is 4<sup><italic>d</italic></sup>+4<sup>(<italic>d</italic>−1)</sup>+⋯+4. The <italic>d</italic>th-order Markov model uses the 1st-order Markov model for the prediction of the first trial in a session, and 2nd-order Markov model for the second trial). The Markov models are purely descriptive models, but they provide a useful measure to objectively evaluate other models.</p>
</sec>
<sec id="sec020">
<title>Q-learning models</title>
<p>The DFQ-learning model [<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004540.ref018" ref-type="bibr">18</xref>], which is an extension of the Q-learning model and which includes the original Q-learning model with certain parameters, is useful to test the Q-learning family. A key component of the DFQ-learning (and Q-learning) model is to use action values (<italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>) as predictions of the future cumulative reward that the agent would obtain after selecting left or right, respectively. The model selects an action that has a higher action value with a higher probability:
<disp-formula id="pcbi.1004540.e012">
<alternatives>
<graphic id="pcbi.1004540.e012g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e012" xlink:type="simple"/>
<mml:math display="block" id="M12" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
After determining the reward outcome, action values are updated by:
<disp-formula id="pcbi.1004540.e013">
<alternatives>
<graphic id="pcbi.1004540.e013g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e013" xlink:type="simple"/>
<mml:math display="block" id="M13" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where <italic>i</italic> ∈ {<italic>L</italic>,<italic>R</italic>}, <italic>α</italic><sub>1</sub> is the learning rate for the selected action, <italic>α</italic><sub>2</sub> is the forgetting rate for the action not chosen, <italic>κ</italic><sub>1</sub> represents the strength of reinforcement by reward, and <italic>κ</italic><sub>2</sub> represents the strength of the aversion resulting from the non-reward outcome. This set of equations can be reduced to the standard Q-learning by setting <italic>α</italic><sub>2</sub> = 0 (no forgetting for actions not chosen) and <italic>κ</italic><sub>2</sub> = 0 (no aversion from a lack of reward). The FQ-model is a version introducing the restriction <italic>α</italic><sub>1</sub> = <italic>α</italic><sub>2</sub>.</p>
<p>For the Q-learning models, we considered cases of fixed parameters and time-varying parameters. For fixed parameter models, <italic>α</italic><sub>1</sub>, <italic>α</italic><sub>2</sub>, <italic>κ</italic><sub>1</sub>, and <italic>κ</italic><sub>2</sub> are free parameters. For time-varying parameters, <italic>α</italic><sub>1</sub>, <italic>α</italic><sub>2</sub>, <italic>κ</italic><sub>1</sub>, and <italic>κ</italic><sub>2</sub> are not free parameters; they are assumed to vary according to the following:
<disp-formula id="pcbi.1004540.e014">
<alternatives>
<graphic id="pcbi.1004540.e014g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e014" xlink:type="simple"/>
<mml:math display="block" id="M14" overflow="scroll">
<mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ς</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>for</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>for</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <italic>ζ</italic><sub><italic>j</italic></sub> and <italic>ξ</italic><sub><italic>j</italic></sub> are noise terms drawn independently from the Gaussian distribution <italic>N</italic>(0,<italic>σ</italic><sub><italic>α</italic></sub><sup>2</sup>) and <italic>N</italic>(0,<italic>σ</italic><sub><italic>κ</italic></sub><sup>2</sup>), respectively. <italic>σ</italic><sub><italic>α</italic></sub> and <italic>σ</italic><sub><italic>κ</italic></sub> are free parameters that control the magnitude of the change. The predictive distribution <italic>P</italic>(<italic>h</italic>(<italic>t</italic>)| <italic>e</italic>(1:<italic>t</italic>-1)) of parameters <italic>h</italic> = [<italic>Q</italic><sub><italic>L</italic></sub>, <italic>Q</italic><sub><italic>R</italic></sub>, <italic>α</italic><sub>1</sub>, <italic>α</italic><sub>2</sub>, <italic>κ</italic><sub>1</sub>, <italic>κ</italic><sub>2</sub>] given past experiences <italic>e</italic>(1:<italic>t</italic>-1) was estimated using the particle filter [<xref rid="pcbi.1004540.ref004" ref-type="bibr">4</xref>,<xref rid="pcbi.1004540.ref005" ref-type="bibr">5</xref>]. The action probability <italic>P</italic><sub><italic>L</italic></sub>(<italic>t</italic>) was obtained from Eq (<xref rid="pcbi.1004540.e012" ref-type="disp-formula">8</xref>) with the mean of the predictive distribution of <italic>Q</italic><sub><italic>L</italic></sub> (<italic>t</italic>) and <italic>Q</italic><sub><italic>R</italic></sub> (<italic>t</italic>). In this study, 5,000 particles were used for the estimation.</p>
</sec>
<sec id="sec021">
<title>Environmental state estimation (ESE) models</title>
<p>The ESE model estimates a hidden environmental state, namely, the reward setting from past experience, using the knowledge that reward probabilities should be one of the following: (90, 50%), (50, 10%), (50, 90%) and (10, 50%) (five trials with zero reward probability inserted in the middle of each session were not considered.). The ESE model also assumes that the reward setting is changed with a small probability <italic>ε</italic> for each trial:
<disp-formula id="pcbi.1004540.e015">
<alternatives>
<graphic id="pcbi.1004540.e015g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e015" xlink:type="simple"/>
<mml:math display="block" id="M15" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>ε</mml:mi><mml:mo>/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
where <italic>s</italic>(<italic>t</italic>) ∈ {1,2,3,4} is the index of reward setting at trial <italic>t</italic> corresponding to (90, 50%), (50, 10%), (50, 90%) and (10, 50%), respectively. The prediction of the reward setting at trial <italic>t</italic> for all <italic>s</italic>(<italic>t</italic>) is obtained using
<disp-formula id="pcbi.1004540.e016">
<alternatives>
<graphic id="pcbi.1004540.e016g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e016" xlink:type="simple"/>
<mml:math display="block" id="M16" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
where <italic>P</italic>(<italic>s</italic>(<italic>t</italic>-1)| <italic>e</italic>(1:<italic>t</italic>-1)) is the prior probability of the reward setting. The prior probability for <italic>t</italic> = 1 was set to 1/4 for each <italic>s</italic>. Based on this prediction, action values are given by
<disp-formula id="pcbi.1004540.e017">
<alternatives>
<graphic id="pcbi.1004540.e017g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e017" xlink:type="simple"/>
<mml:math display="block" id="M17" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>κ</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
where <italic>P</italic>(<italic>r</italic>(<italic>t</italic>) = 1| <italic>s</italic>(<italic>t</italic>), <italic>a</italic>(<italic>t</italic>) = <italic>i</italic>) is the reward probability for the reward setting <italic>s</italic>(<italic>t</italic>) and action <italic>i</italic>. <italic>κ</italic> is the magnitude of the reward. An actual action, <italic>a</italic>(<italic>t</italic>), is selected according to the action probability, which is calculated from Eq (<xref rid="pcbi.1004540.e012" ref-type="disp-formula">8</xref>) with the action values. After knowing the reward outcome, <italic>r</italic>(<italic>t</italic>), the posterior probability of the reward setting for all <italic>s</italic>(<italic>t</italic>), was updated using Bayes’ theorem:
<disp-formula id="pcbi.1004540.e018">
<alternatives>
<graphic id="pcbi.1004540.e018g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e018" xlink:type="simple"/>
<mml:math display="block" id="M18" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
The first factor of the right side can be decomposed to
<disp-formula id="pcbi.1004540.e019">
<alternatives>
<graphic id="pcbi.1004540.e019g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e019" xlink:type="simple"/>
<mml:math display="block" id="M19" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
where the first factor on the right side of this equation can be simply written as <italic>P</italic>(<italic>r</italic>(<italic>t</italic>) | <italic>a</italic>(<italic>t</italic>),<italic>s</italic>(<italic>t</italic>)) because this factor comes from the reward probability setting of the task and is assumed to be independent of the past experience of rats, <italic>e</italic>(1:<italic>t</italic>-1). The second factor is the action probability of the agent. Although the agent estimates the current reward setting, <italic>s</italic>(<italic>t</italic>), from past experience, <italic>e</italic>(1:<italic>t</italic>-1), the agent cannot directly observe <italic>s</italic>(<italic>t</italic>). In other words, the action probability should be the same for the same past experience, <italic>e</italic>(1:<italic>t</italic>-1), without being affected by the true hidden state, <italic>s</italic>(<italic>t</italic>). Therefore, the second factor can be ignored because it takes the same values for all <italic>s</italic>(<italic>t</italic>). Then, Eq (<xref rid="pcbi.1004540.e018" ref-type="disp-formula">14</xref>) is simplified to
<disp-formula id="pcbi.1004540.e020">
<alternatives>
<graphic id="pcbi.1004540.e020g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e020" xlink:type="simple"/>
<mml:math display="block" id="M20" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula></p>
<p>Similar to the Q-learning models, we considered the cases of fixed and time-varying parameters. For fixed parameter models, <italic>ε</italic> and <italic>κ</italic> are free parameters. For time-varying parameters, <italic>ε</italic> and <italic>κ</italic> were assumed to vary by a random walk with the Gaussian distribution <italic>N</italic>(0, <italic>σ</italic><sub><italic>ε</italic></sub><sup>2</sup>) and <italic>N</italic>(0, <italic>σ</italic><sub><italic>κ</italic></sub><sup>2</sup>), respectively. <italic>σ</italic><sub><italic>ε</italic></sub> and <italic>σ</italic><sub><italic>κ</italic></sub> are the free parameters that control the magnitude of the change.</p>
</sec>
<sec id="sec022">
<title>Finite state agent (FSA) models</title>
<p>FSA models are non-parametric models that have internal variables <italic>x</italic> taking <italic>N</italic> possible states, <italic>x</italic> ∈ {1,2,⋯,<italic>N</italic>}. The initial distribution of the state is described by
<disp-formula id="pcbi.1004540.e021">
<alternatives>
<graphic id="pcbi.1004540.e021g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e021" xlink:type="simple"/>
<mml:math display="block" id="M21" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula>
The probability of an action selection depends on the state and is defined by
<disp-formula id="pcbi.1004540.e022">
<alternatives>
<graphic id="pcbi.1004540.e022g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e022" xlink:type="simple"/>
<mml:math display="block" id="M22" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
After execution of an action and the subsequent reward outcome, the state is probabilistically moved to another state according to the state transient function:
<disp-formula id="pcbi.1004540.e023">
<alternatives>
<graphic id="pcbi.1004540.e023g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e023" xlink:type="simple"/>
<mml:math display="block" id="M23" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula>
<italic>q</italic><sup><italic>n</italic></sup>, <italic>π</italic><sub><italic>n</italic></sub>(<italic>a</italic>), <inline-formula id="pcbi.1004540.e024"><alternatives><graphic id="pcbi.1004540.e024g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e024" xlink:type="simple"/>
<mml:math display="inline" id="M24" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, and <italic>N</italic> are the free parameters of the FSA models. Considering the probabilistic constraints, <inline-formula id="pcbi.1004540.e025"><alternatives><graphic id="pcbi.1004540.e025g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e025" xlink:type="simple"/>
<mml:math display="inline" id="M25" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e026"><alternatives><graphic id="pcbi.1004540.e026g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e026" xlink:type="simple"/>
<mml:math display="inline" id="M26" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e027"><alternatives><graphic id="pcbi.1004540.e027g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e027" xlink:type="simple"/>
<mml:math display="inline" id="M27" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, and symmetric constraints <italic>q</italic><sup><italic>l</italic></sup> = <italic>q</italic><sup><italic>N</italic>−<italic>l</italic>+1</sup>, <inline-formula id="pcbi.1004540.e028"><alternatives><graphic id="pcbi.1004540.e028g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e028" xlink:type="simple"/>
<mml:math display="inline" id="M28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e029"><alternatives><graphic id="pcbi.1004540.e029g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e029" xlink:type="simple"/>
<mml:math display="inline" id="M29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, for <italic>l</italic>, <italic>l’</italic> = 1, 2,…, <italic>N</italic>, where <inline-formula id="pcbi.1004540.e030"><alternatives><graphic id="pcbi.1004540.e030g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e030" xlink:type="simple"/>
<mml:math display="inline" id="M30" overflow="scroll"><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math>
</alternatives></inline-formula> is the other of actions, and the number of free parameters is (<italic>N</italic>/2-1) + <italic>N</italic>/2 + 2<italic>N</italic>(<italic>N</italic>-1) = 2<italic>N</italic><sup>2</sup>-<italic>N-</italic>1 when <italic>N</italic> is an even number and (<italic>N</italic>-1)/2 + (<italic>N</italic>-1)/2 + 2(<italic>N</italic>-1)<sup>2</sup> = 2<italic>N</italic><sup>2</sup> - 3<italic>N</italic> +1 when <italic>N</italic> is an odd number.</p>
<p>The FSA model can be regarded as an extended version of the hidden Markov model (HMM). However, unlike the HMM, in the FSA model, the state transition probability depends on the action and reward. In the HMM, the Baum-Welch algorithm [<xref rid="pcbi.1004540.ref019" ref-type="bibr">19</xref>], a form of the EM algorithm, is used to find the parameters that maximize the likelihood of the given data. We reformulated the Baum-Welch algorithm for the FSA model.</p>
</sec>
<sec id="sec023">
<title>EM algorithm for FSA model</title>
<p>1. Initialize parameters <italic>q</italic><sup><italic>n</italic></sup>, <italic>π</italic><sub><italic>n</italic></sub>(<italic>a</italic>), <inline-formula id="pcbi.1004540.e031"><alternatives><graphic id="pcbi.1004540.e031g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e031" xlink:type="simple"/>
<mml:math display="inline" id="M31" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, so the probabilistic constraints, <inline-formula id="pcbi.1004540.e032"><alternatives><graphic id="pcbi.1004540.e032g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e032" xlink:type="simple"/>
<mml:math display="inline" id="M32" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e033"><alternatives><graphic id="pcbi.1004540.e033g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e033" xlink:type="simple"/>
<mml:math display="inline" id="M33" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow/></mml:munderover><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e034"><alternatives><graphic id="pcbi.1004540.e034g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e034" xlink:type="simple"/>
<mml:math display="inline" id="M34" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math>
</alternatives></inline-formula>, and the symmetric constraints, <italic>q</italic><sup><italic>l</italic></sup> = <italic>q</italic><sup><italic>N</italic>−<italic>l</italic>+1</sup>, <inline-formula id="pcbi.1004540.e035"><alternatives><graphic id="pcbi.1004540.e035g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e035" xlink:type="simple"/>
<mml:math display="inline" id="M35" overflow="scroll"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, <inline-formula id="pcbi.1004540.e036"><alternatives><graphic id="pcbi.1004540.e036g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e036" xlink:type="simple"/>
<mml:math display="inline" id="M36" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</alternatives></inline-formula>, are satisfied (<italic>N</italic> is a fixed parameter). In this study, we set <italic>q</italic><sup><italic>n</italic></sup> = 1/<italic>N</italic> for all <italic>n</italic>, <italic>π</italic><sub><italic>n</italic></sub>(<italic>L</italic>) = 0.9−0.8(<italic>n</italic>−1)/(<italic>N</italic>−1) and <italic>π</italic><sub><italic>n</italic></sub>(<italic>R</italic>) = 1−<italic>π</italic><sub><italic>n</italic></sub>(<italic>L</italic>) for all <italic>n</italic>, and <inline-formula id="pcbi.1004540.e037"><alternatives><graphic id="pcbi.1004540.e037g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e037" xlink:type="simple"/>
<mml:math display="inline" id="M37" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math>
</alternatives></inline-formula> for all <italic>n</italic>, <italic>m</italic>, <italic>a</italic>, and <italic>r</italic>.</p>
<p>2. E-step</p>
<p>Estimate the posterior probability of the state for all <italic>t</italic> and <italic>l</italic>, <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) = <italic>P</italic>(<italic>x</italic>{<italic>l</italic>}(<italic>t</italic>) = <italic>n</italic>|<italic>a</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>),<italic>r</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>)), assuming that the data were produced with current parameters.</p>
<p>First, estimate <italic>α</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) = <italic>P</italic>(<italic>x</italic>{<italic>l</italic>}(<italic>t</italic>) = <italic>n</italic>|<italic>a</italic>{<italic>l</italic>}(1:<italic>t</italic>),<italic>r</italic>{<italic>l</italic>}(1:<italic>t</italic>)), the posterior probability of a state at trial <italic>t</italic> given the data from 1 to the current trial <italic>t</italic>. The probability can be obtained iteratively from <italic>t</italic> = 1 to <italic>t</italic> by:
<disp-formula id="pcbi.1004540.e038">
<alternatives>
<graphic id="pcbi.1004540.e038g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e038" xlink:type="simple"/>
<mml:math display="block" id="M38" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula>
where <italic>α</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic> = 1) = <italic>q</italic><sup><italic>n</italic></sup>.</p>
<p>Next, estimate <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) = <italic>P</italic>(<italic>x</italic>{<italic>l</italic>}(<italic>t</italic>) = <italic>n</italic>|<italic>a</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>),<italic>r</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>)), the posterior probability of a state, given all data using the already obtained <italic>α</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) and an additional variable, <italic>χ</italic><sup><italic>nm</italic></sup>{<italic>l</italic>}(<italic>t</italic>) = <italic>P</italic>(<italic>x</italic>{<italic>l</italic>}(<italic>t</italic>) = <italic>n</italic>, <italic>x</italic>{<italic>l</italic>}(<italic>t</italic> + 1) = <italic>m</italic> | <italic>a</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>),<italic>r</italic>{<italic>l</italic>}(1:<italic>T</italic><sub><italic>l</italic></sub>)). <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) and <italic>χ</italic><sup><italic>nm</italic></sup>{<italic>l</italic>}(<italic>t</italic>) are obtained in a backward manner from <italic>t = T</italic><sub><italic>l</italic></sub> (or <italic>T</italic><sub><italic>l</italic></sub><italic>-</italic>1 for <italic>χ</italic><sup><italic>nm</italic></sup>) to 1 in parallel. For <italic>t</italic> = <italic>T</italic><sub><italic>l</italic></sub>, <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>T</italic><sub><italic>l</italic></sub>) = <italic>α</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>T</italic><sub><italic>l</italic></sub>). Then, <italic>χ</italic><sup><italic>nm</italic></sup>{<italic>l</italic>}(<italic>T</italic><sub><italic>l</italic></sub>−1) is obtained using
<disp-formula id="pcbi.1004540.e039">
<alternatives>
<graphic id="pcbi.1004540.e039g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e039" xlink:type="simple"/>
<mml:math display="block" id="M39" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(21)</label>
</disp-formula>
and <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>T</italic><sub><italic>l</italic></sub>−1) is obtained using
<disp-formula id="pcbi.1004540.e040">
<alternatives>
<graphic id="pcbi.1004540.e040g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e040" xlink:type="simple"/>
<mml:math display="block" id="M40" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(22)</label>
</disp-formula>
By repeating (<xref rid="pcbi.1004540.e039" ref-type="disp-formula">21</xref>) and (<xref rid="pcbi.1004540.e040" ref-type="disp-formula">22</xref>), <italic>γ</italic><sup><italic>n</italic></sup> and <italic>χ</italic><sup><italic>nm</italic></sup> are obtained for all trials.</p>
<p>3. M-step</p>
<p>Update model parameters, assuming the state probabilities <italic>γ</italic><sup><italic>n</italic></sup> and <italic>χ</italic><sup><italic>nm</italic></sup> are true. Given the probabilistic and symmetric constraints, parameters are updated using
<disp-formula id="pcbi.1004540.e041">
<alternatives>
<graphic id="pcbi.1004540.e041g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e041" xlink:type="simple"/>
<mml:math display="block" id="M41" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>L</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(23)</label>
</disp-formula>
<disp-formula id="pcbi.1004540.e042">
<alternatives>
<graphic id="pcbi.1004540.e042g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e042" xlink:type="simple"/>
<mml:math display="block" id="M42" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(24)</label>
</disp-formula>
and
<disp-formula id="pcbi.1004540.e043">
<alternatives>
<graphic id="pcbi.1004540.e043g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e043" xlink:type="simple"/>
<mml:math display="block" id="M43" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(25)</label>
</disp-formula></p>
<p>4. Check for convergence of the parameters. If the convergence criterion is not satisfied, return to step 2. In this study, we stopped the iteration when the maximum change of the parameters was less than 0.00001.</p>
<sec id="sec024">
<title>Definition of clusters of states in FSA models</title>
<p>Let us consider a certain FSA model in which a state transition frequently occurs between state 1 and 2, but states 1 and 2 rarely transition to other states. In this case, the set of state 1 and 2 may be regarded as a state of a higher order (a cluster). This structure of states is helpful to understand the meanings of the procedures coded in the FSA models. To find the most plausible clustering structure, we calculated the state transition probability, <inline-formula id="pcbi.1004540.e044"><alternatives><graphic id="pcbi.1004540.e044g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e044" xlink:type="simple"/>
<mml:math display="inline" id="M44" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</alternatives></inline-formula>, by
<disp-formula id="pcbi.1004540.e045">
<alternatives>
<graphic id="pcbi.1004540.e045g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e045" xlink:type="simple"/>
<mml:math display="block" id="M45" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>{</mml:mo><mml:mi>l</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(26)</label>
</disp-formula>
where <italic>χ</italic><sup><italic>nm</italic></sup>{<italic>l</italic>}(<italic>t</italic>) and <italic>γ</italic><sup><italic>n</italic></sup>{<italic>l</italic>}(<italic>t</italic>) are estimated from the behavioral data. Then, we defined the cluster index by the average of the state transition probabilities in the clusters:
<disp-formula id="pcbi.1004540.e046">
<alternatives>
<graphic id="pcbi.1004540.e046g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e046" xlink:type="simple"/>
<mml:math display="block" id="M46" overflow="scroll">
<mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(27)</label>
</disp-formula>
where <italic>i</italic> is an index of the clusters, <italic>h</italic><sub><italic>i</italic></sub> is a set of states included in the cluster <italic>i</italic>, and <italic>N</italic><sub><italic>c</italic></sub> is the number of terms in the summations. For instance, if 6 states are clustered into [1 2] and [3 4 5 6] for the FSA model with 6 states, the cluster index is <inline-formula id="pcbi.1004540.e047"><alternatives><graphic id="pcbi.1004540.e047g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e047" xlink:type="simple"/>
<mml:math display="inline" id="M47" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>14</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>3</mml:mn><mml:mn>4</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>3</mml:mn><mml:mn>5</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>3</mml:mn><mml:mn>6</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>4</mml:mn><mml:mn>3</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>4</mml:mn><mml:mn>5</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>4</mml:mn><mml:mn>6</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>5</mml:mn><mml:mn>3</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>5</mml:mn><mml:mn>4</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>5</mml:mn><mml:mn>6</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>6</mml:mn><mml:mn>3</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>6</mml:mn><mml:mn>4</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mn>6</mml:mn><mml:mn>5</mml:mn></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>
</alternatives></inline-formula>. We calculated the cluster index for all possible ways of clustering for each FSA model with 4, 6, and 8 states, assuming that all clusters include more than one state and that all states belong to any cluster. Then, the clustering showing the highest cluster index was selected as the most plausible clustering structure (<xref rid="pcbi.1004540.g004" ref-type="fig">Fig 4</xref>).</p>
</sec>
</sec>
<sec id="sec025">
<title>Comparison of simulated behavior</title>
<p>To test whether the models can generate behavioral data that have the same statistics as behavioral data, we compared the simulated behavior of the models with the behavioral results. First, as a measure of adaptation speed to the change of reward probabilities, we calculated the mean number of trials in one block for the higher reward probability settings, (90, 50%) and (50, 90%), and for the lower reward probability settings, (50, 10%) and (10, 50%), from all 202 recorded sessions. Because each session consists of four blocks with different reward probability settings, there were 404 higher reward blocks and 404 lower reward blocks in the data. Second, as a measure of the strategy utilized by the rats, the probability that the same action was selected after a rewarded or non-rewarded trial, <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = <italic>a</italic>(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 1) and <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = a(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 0), for higher and lower reward probability settings, respectively, was calculated. These four action probabilities were calculated from the last 20 trials in four blocks for each session, and then the mean of these probabilities was calculated from all 202 sessions. Third, we conducted a model simulation for 202 sessions in which the same block sequences as those used in all 202 sessions were applied. Then, the six statistics noted above were calculated from the simulated data: [the number of trials in a block, <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = <italic>a</italic>(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 1), <italic>P</italic>(<italic>a</italic>(<italic>t</italic>+1) = a(<italic>t</italic>)| <italic>r</italic>(<italic>t</italic>) = 0)] x [higher, lower reward probability setting]. By repeating this simulation 10,000 times, the approximate distribution for each statistic was obtained.</p>
<p>Note that the statistics calculated from the rats are random variables. If the hypothesis that the choice behavior of rats was sampled from a certain model is true, then statistics obtained from behavioral measures should fall within the distribution of the statistics (inside the confidence interval with 1—<italic>ε</italic>) calculated by the model. Otherwise, the hypothesis is rejected. We considered six different tests for the same hypothesis, so the chance of at least one false rejection is much higher than <italic>ε</italic>. Therefore, the confidence interval for each statistic was set to 1—<italic>ε</italic>/6, so the chance of at least one false rejection is <italic>ε</italic> (Bonferroni Method). In this study, <italic>ε</italic> was set to 0.05. We tested the Q-, FQ-, and DFQ-learning models in addition to the ESE model in which the parameters were fixed. The FSA models with 4, 6, and 8 states were also tested. The free parameters that maximize the likelihood of the training data were used for the simulation.</p>
<p>The distributions of the statistics in one session (<xref rid="pcbi.1004540.g005" ref-type="fig">Fig 5A–5C</xref>) were calculated from 202 behavioral sessions and 10,000 x 202 sessions for the models. As a result, the shape of the distribution is smoother for the models than for behavioral data.</p>
</sec>
<sec id="sec026">
<title>Lasso regularization of Poisson regression</title>
<p>Linear regression is a popular method to find regressors that can explain the change in neuronal activity, where spikes are assumed to be sampled from a normal distribution. However, in the precise sense, this assumption is not correct because spikes take only non-negative integers. The lower the firing rate of the neuron is, the bigger the gap from the assumption is.</p>
<p>Therefore, in the present study, we used Poisson regression assuming that the spikes are sampled from a Poisson distribution (a distribution of non-negative integer variables). In Poisson regression, the expected number of spikes at trial <italic>t</italic>, <italic>μ</italic>(<italic>t</italic>), is predicted by the following exponential function,
<disp-formula id="pcbi.1004540.e048">
<alternatives>
<graphic id="pcbi.1004540.e048g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e048" xlink:type="simple"/>
<mml:math display="block" id="M48" overflow="scroll">
<mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(28)</label>
</disp-formula>
where <italic>x</italic><sub><italic>i</italic></sub> are regressors and <italic>β</italic><sub><italic>i</italic></sub> are regression coefficients. The prediction of the number of the spikes at trial <italic>t</italic> is represented by a Poisson distribution with the average <italic>μ</italic>(<italic>t</italic>),
<disp-formula id="pcbi.1004540.e049">
<alternatives>
<graphic id="pcbi.1004540.e049g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e049" xlink:type="simple"/>
<mml:math display="block" id="M49" overflow="scroll">
<mml:mrow><mml:mtext>Poi</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>y</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(29)</label>
</disp-formula>
Optimal regression coefficients are determined so that the objective function, namely, the log likelihood for all trials,
<disp-formula id="pcbi.1004540.e050">
<alternatives>
<graphic id="pcbi.1004540.e050g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e050" xlink:type="simple"/>
<mml:math display="block" id="M50" overflow="scroll">
<mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>T</mml:mtext></mml:munderover><mml:mrow><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Poi</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(30)</label>
</disp-formula>
is maximized. For this calculation, a function in MATLAB Statistics and Machine Learning Toolbox “glmfit(X, y, ‘poisson’)” is available. To select minimum regressors to explain the spikes among many and redundant regressors is, to add a penalty term for large <italic>β</italic> to the objective function,
<disp-formula id="pcbi.1004540.e051">
<alternatives>
<graphic id="pcbi.1004540.e051g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004540.e051" xlink:type="simple"/>
<mml:math display="block" id="M51" overflow="scroll">
<mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext>T</mml:mtext></mml:munderover><mml:mrow><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Poi</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow>
</mml:math>
</alternatives>
<label>(31)</label>
</disp-formula>
where <italic>λ</italic> is a free parameter called the regularization coefficient, and |<italic>β</italic><sub><italic>j</italic></sub>| is the absolute value of <italic>β</italic><sub><italic>j</italic></sub> (this method is called <italic>lasso</italic> [<xref rid="pcbi.1004540.ref024" ref-type="bibr">24</xref>]). It has the property that if <italic>λ</italic> is sufficiently large, some of the coefficients <italic>β</italic> are driven to zero [<xref rid="pcbi.1004540.ref042" ref-type="bibr">42</xref>]. In the present study, all regressors were normalized, so that the average was 0 and the variance was 1. Then, <italic>λ</italic> was optimized by 5-fold cross-validation for each time bin for each neuron, and regression coefficients were obtained. For these calculations, we used a function in the MATLAB Statistics and Machine Learning Toolbox,“lassoglm(X, y, ‘poission’, ‘cv’, 5)”. The regressors with non-zero coefficients were regarded as candidates for minimum regressors. Then, to calculate a <italic>p</italic>-value indicating the probability that each candidate could be incorrectly selected, we applied Poisson regression (MATLAB function, glmfit) to the regression model including only these candidates as regressors. We selected candidates for minimal regressors that had <italic>p</italic>-values &lt; 0.01.</p>
<p>Proportions of neurons coding variables shown in Figs <xref rid="pcbi.1004540.g007" ref-type="fig">7</xref> and <xref rid="pcbi.1004540.g008" ref-type="fig">8</xref> are the fraction of neurons for which corresponding variables were regarded as minimal regressors for each time bin. The significance of the proportion (<italic>p</italic> &lt; 0.05) was calculated with a binomial test, assuming that the probability that a regressor could be selected incorrectly was <italic>p</italic> = 0.01.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We are grateful to Steven D. Aird, technical editor of OIST Graduate University, for thorough editing and proofreading.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004540.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>1999</year>) <article-title>What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?</article-title> <source>Neural Netw</source> <volume>12</volume>: <fpage>961</fpage>–<lpage>974</lpage>. <object-id pub-id-type="pmid">12662639</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>. <object-id pub-id-type="pmid">16286932</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watkins</surname> <given-names>CJCH</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>1992</year>) <article-title>Q-learning</article-title>. <source>Machine Learning</source> <volume>8</volume>: <fpage>279</fpage>–<lpage>292</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004540.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Representation of action-specific reward values in the striatum</article-title>. <source>Science</source> <volume>310</volume>: <fpage>1337</fpage>–<lpage>1340</lpage>. <object-id pub-id-type="pmid">16311337</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2009</year>) <article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>9861</fpage>–<lpage>9874</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6157-08.2009" xlink:type="simple">10.1523/JNEUROSCI.6157-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19657038</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2000</year>) <article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control</article-title>. <source>Curr Opin Neurobiol</source> <volume>10</volume>: <fpage>732</fpage>–<lpage>739</lpage>. <object-id pub-id-type="pmid">11240282</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sul</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Huh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jung</surname> <given-names>MW</given-names></name> (<year>2009</year>) <article-title>Role of striatum in updating values of chosen actions</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>14701</fpage>–<lpage>14712</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2728-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2728-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19940165</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roesch</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Mullins</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname> <given-names>G</given-names></name> (<year>2009</year>) <article-title>Ventral striatal neurons encode the value of the chosen action in rats deciding between differently delayed or sized rewards</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>13365</fpage>–<lpage>13376</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2572-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2572-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19846724</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasquereau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Nadjar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Arkadir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bezard</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Goillandeau</surname> <given-names>M</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Shaping of motor responses by incentive values through the basal ganglia</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>1176</fpage>–<lpage>1183</lpage>. <object-id pub-id-type="pmid">17267573</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name> (<year>2007</year>) <article-title>Action and outcome encoding in the primate caudate nucleus</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>14502</fpage>–<lpage>14514</lpage>. <object-id pub-id-type="pmid">18160658</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hori</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Minamimoto</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name> (<year>2009</year>) <article-title>Neuronal encoding of reward value and direction of actions in the primate putamen</article-title>. <source>J Neurophysiol</source> <volume>102</volume>: <fpage>3530</fpage>–<lpage>3543</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00104.2009" xlink:type="simple">10.1152/jn.00104.2009</ext-link></comment> <object-id pub-id-type="pmid">19812294</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wunderlich</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name> (<year>2009</year>) <article-title>Neural computations underlying action-based decision making in the human brain</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>106</volume>: <fpage>17199</fpage>–<lpage>17204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0901077106" xlink:type="simple">10.1073/pnas.0901077106</ext-link></comment> <object-id pub-id-type="pmid">19805082</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hampton</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Bossaerts</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name> (<year>2006</year>) <article-title>The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>8360</fpage>–<lpage>8367</lpage>. <object-id pub-id-type="pmid">16899731</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Glascher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name> (<year>2010</year>) <article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>585</fpage>–<lpage>595</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2010.04.016" xlink:type="simple">10.1016/j.neuron.2010.04.016</ext-link></comment> <object-id pub-id-type="pmid">20510862</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name> (<year>2011</year>) <article-title>Model-based influences on humans' choices and striatal prediction errors</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barraclough</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Conroy</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name> (<year>2004</year>) <article-title>Prefrontal cortex and decision making in a mixed-strategy game</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>404</fpage>–<lpage>410</lpage>. <object-id pub-id-type="pmid">15004564</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaelbling</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Cassandra</surname> <given-names>AR</given-names></name> (<year>1998</year>) <article-title>Planning and acting in partially observable stochastic domains</article-title>. <source>Artificial Intelligence</source> <volume>101</volume>: <fpage>99</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004540.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2015</year>) <article-title>Distinct Neural Representation in the Dorsolateral, Dorsomedial, and Ventral Parts of the Striatum during Fixed- and Free-Choice Tasks</article-title>. <source>J Neurosci</source> <volume>35</volume>: <fpage>3499</fpage>–<lpage>3514</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1962-14.2015" xlink:type="simple">10.1523/JNEUROSCI.1962-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25716849</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baum</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Petrie</surname> <given-names>T</given-names></name> (<year>1966</year>) <article-title>Statistical inference for probabilistic functions of finite state Markov chains</article-title>. <source>Annals of Mathematical Statistics</source> <volume>37</volume>: <fpage>1554</fpage>–<lpage>1563</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004540.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hollerman</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Tremblay</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>1998</year>) <article-title>Influence of reward expectation on behavior-related neuronal activity in primate striatum</article-title>. <source>J Neurophysiol</source> <volume>80</volume>: <fpage>947</fpage>–<lpage>963</lpage>. <object-id pub-id-type="pmid">9705481</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>MX</given-names></name>, <name name-style="western"><surname>Axmacher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lenartz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Elger</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Sturm</surname> <given-names>V</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Neuroelectric signatures of reward learning and decision-making in the human nucleus accumbens</article-title>. <source>Neuropsychopharmacology</source> <volume>34</volume>: <fpage>1649</fpage>–<lpage>1658</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/npp.2008.222" xlink:type="simple">10.1038/npp.2008.222</ext-link></comment> <object-id pub-id-type="pmid">19092783</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kimchi</surname> <given-names>EY</given-names></name>, <name name-style="western"><surname>Laubach</surname> <given-names>M</given-names></name> (<year>2009</year>) <article-title>The dorsomedial striatum reflects response bias during learning</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>14891</fpage>–<lpage>14902</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4060-09.2009" xlink:type="simple">10.1523/JNEUROSCI.4060-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19940185</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name> (<year>2008</year>) <article-title>Value representations in the primate striatum during matching behavior</article-title>. <source>Neuron</source> <volume>58</volume>: <fpage>451</fpage>–<lpage>463</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.02.021" xlink:type="simple">10.1016/j.neuron.2008.02.021</ext-link></comment> <object-id pub-id-type="pmid">18466754</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name> (<year>1996</year>) <article-title>Regression shrinkage and selection via the Lasso</article-title>. <source>Journal of the Royal Statistical Society Series B-Methodological</source> <volume>58</volume>: <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004540.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Seo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name> (<year>2009</year>) <article-title>Valuation of uncertain and delayed rewards in primate prefrontal cortex</article-title>. <source>Neural Netw</source> <volume>22</volume>: <fpage>294</fpage>–<lpage>304</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2009.03.010" xlink:type="simple">10.1016/j.neunet.2009.03.010</ext-link></comment> <object-id pub-id-type="pmid">19375276</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamada</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Inokawa</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Matsumoto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Neuronal basis for evaluating selected action in the primate striatum</article-title>. <source>Eur J Neurosci</source> <volume>34</volume>: <fpage>489</fpage>–<lpage>506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-9568.2011.07771.x" xlink:type="simple">10.1111/j.1460-9568.2011.07771.x</ext-link></comment> <object-id pub-id-type="pmid">21781189</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corrado</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2007</year>) <article-title>Understanding neural coding through the model-based analysis of decision making</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>8178</fpage>–<lpage>8180</lpage>. <object-id pub-id-type="pmid">17670963</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>O'Doherty</surname> <given-names>JP</given-names></name> (<year>2010</year>) <article-title>Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action</article-title>. <source>Neuropsychopharmacology</source> <volume>35</volume>: <fpage>48</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/npp.2009.131" xlink:type="simple">10.1038/npp.2009.131</ext-link></comment> <object-id pub-id-type="pmid">19776734</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2004</year>) <article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title>. <source>Eur J Neurosci</source> <volume>19</volume>: <fpage>181</fpage>–<lpage>189</lpage>. <object-id pub-id-type="pmid">14750976</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2005</year>) <article-title>Neural bases of food-seeking: affect, arousal and reward in corticostriatolimbic circuits</article-title>. <source>Physiol Behav</source> <volume>86</volume>: <fpage>717</fpage>–<lpage>730</lpage>. <object-id pub-id-type="pmid">16257019</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2006</year>) <article-title>Inactivation of dorsolateral striatum enhances sensitivity to changes in the action-outcome contingency in instrumental conditioning</article-title>. <source>Behav Brain Res</source> <volume>166</volume>: <fpage>189</fpage>–<lpage>196</lpage>. <object-id pub-id-type="pmid">16153716</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kesner</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Churchwell</surname> <given-names>JC</given-names></name> (<year>2011</year>) <article-title>An analysis of rat prefrontal cortex in mediating executive function</article-title>. <source>Neurobiol Learn Mem</source> <volume>96</volume>: <fpage>417</fpage>–<lpage>431</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.nlm.2011.07.002" xlink:type="simple">10.1016/j.nlm.2011.07.002</ext-link></comment> <object-id pub-id-type="pmid">21855643</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Voorn</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Vanderschuren</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Groenewegen</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>CM</given-names></name> (<year>2004</year>) <article-title>Putting a spin on the dorsal-ventral divide of the striatum</article-title>. <source>Trends Neurosci</source> <volume>27</volume>: <fpage>468</fpage>–<lpage>474</lpage>. <object-id pub-id-type="pmid">15271494</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tanji</surname> <given-names>J</given-names></name> (<year>1998</year>) <article-title>Role for cingulate motor area cells in voluntary movement selection based on reward</article-title>. <source>Science</source> <volume>282</volume>: <fpage>1335</fpage>–<lpage>1338</lpage>. <object-id pub-id-type="pmid">9812901</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Isoda</surname> <given-names>M</given-names></name> (<year>2010</year>) <article-title>Switching from automatic to controlled behavior: cortico-basal ganglia mechanisms</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>154</fpage>–<lpage>161</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.01.006" xlink:type="simple">10.1016/j.tics.2010.01.006</ext-link></comment> <object-id pub-id-type="pmid">20181509</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name> (<year>2005</year>) <article-title>Different time courses of learning-related activity in the prefrontal cortex and striatum</article-title>. <source>Nature</source> <volume>433</volume>: <fpage>873</fpage>–<lpage>876</lpage>. <object-id pub-id-type="pmid">15729344</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2005</year>) <article-title>Blockade of NMDA receptors in the dorsomedial striatum prevents action-outcome learning in instrumental conditioning</article-title>. <source>Eur J Neurosci</source> <volume>22</volume>: <fpage>505</fpage>–<lpage>512</lpage>. <object-id pub-id-type="pmid">16045503</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Ostlund</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Knowlton</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2005</year>) <article-title>The role of the dorsomedial striatum in instrumental conditioning</article-title>. <source>Eur J Neurosci</source> <volume>22</volume>: <fpage>513</fpage>–<lpage>523</lpage>. <object-id pub-id-type="pmid">16045504</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2007</year>) <article-title>Multiple representations of belief states and action values in corticobasal ganglia loops</article-title>. <source>Ann N Y Acad Sci</source> <volume>1104</volume>: <fpage>213</fpage>–<lpage>228</lpage>. <object-id pub-id-type="pmid">17435124</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name> (<year>2011</year>) <article-title>Multiple representations and algorithms for reinforcement learning in the cortico-basal ganglia circuit</article-title>. <source>Curr Opin Neurobiol</source> <volume>21</volume>: <fpage>368</fpage>–<lpage>373</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2011.04.001" xlink:type="simple">10.1016/j.conb.2011.04.001</ext-link></comment> <object-id pub-id-type="pmid">21531544</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmitzer-Torbert</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name> (<year>2004</year>) <article-title>Neuronal activity in the rodent dorsal striatum in sequential navigation: separation of spatial and reward responses on the multiple T task</article-title>. <source>J Neurophysiol</source> <volume>91</volume>: <fpage>2259</fpage>–<lpage>2272</lpage>. <object-id pub-id-type="pmid">14736863</object-id></mixed-citation></ref>
<ref id="pcbi.1004540.ref042"><label>42</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name> (<year>2006</year>) <chapter-title>Pattern recognition and machine learning</chapter-title>; <name name-style="western"><surname>Jordan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kleinberg</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Scholkopf</surname> <given-names>B</given-names></name>, editors. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
</ref-list>
</back>
</article>