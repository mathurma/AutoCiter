<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005542</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-16-01791</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Working memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Approximation methods</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Algebra</subject><subj-group><subject>Linear algebra</subject><subj-group><subject>Eigenvalues</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A state space approach for piecewise-linear recurrent neural networks for identifying computational dynamics from neural measurements</article-title>
<alt-title alt-title-type="running-head">Nonlinear state space model for identifying computational dynamics</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Dept. of Theoretical Neuroscience, Bernstein Center for Computational Neuroscience Heidelberg-Mannheim, Central Institute of Mental Health, Medical Faculty Mannheim/ Heidelberg University, Mannheim, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The author has declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item>
<p><bold>Conceptualization:</bold> DD.</p></list-item> <list-item>
<p><bold>Formal analysis:</bold> DD.</p></list-item> <list-item>
<p><bold>Funding acquisition:</bold> DD.</p></list-item> <list-item>
<p><bold>Methodology:</bold> DD.</p></list-item> <list-item>
<p><bold>Software:</bold> DD.</p></list-item> <list-item>
<p><bold>Writing – original draft:</bold> DD.</p></list-item> <list-item>
<p><bold>Writing – review &amp; editing:</bold> DD.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">daniel.durstewitz@zi-mannheim.de</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>2</day>
<month>6</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2017</year>
</pub-date>
<volume>13</volume>
<issue>6</issue>
<elocation-id>e1005542</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>11</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Daniel Durstewitz</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005542"/>
<abstract>
<p>The computational and cognitive properties of neural systems are often thought to be implemented in terms of their (stochastic) network dynamics. Hence, recovering the system dynamics from experimentally observed neuronal time series, like multiple single-unit recordings or neuroimaging data, is an important step toward understanding its computations. Ideally, one would not only seek a (lower-dimensional) state space representation of the dynamics, but would wish to have access to its statistical properties and their generative equations for in-depth analysis. Recurrent neural networks (RNNs) are a computationally powerful and dynamically universal formal framework which has been extensively studied from both the computational and the dynamical systems perspective. Here we develop a semi-analytical maximum-likelihood estimation scheme for piecewise-linear RNNs (PLRNNs) within the statistical framework of state space models, which accounts for noise in both the underlying latent dynamics and the observation process. The Expectation-Maximization algorithm is used to infer the latent state distribution, through a global Laplace approximation, and the PLRNN parameters iteratively. After validating the procedure on toy examples, and using inference through particle filters for comparison, the approach is applied to multiple single-unit recordings from the rodent anterior cingulate cortex (ACC) obtained during performance of a classical working memory task, delayed alternation. Models estimated from kernel-smoothed spike time data were able to capture the essential computational dynamics underlying task performance, including stimulus-selective delay activity. The estimated models were rarely multi-stable, however, but rather were tuned to exhibit slow dynamics in the vicinity of a bifurcation point. In summary, the present work advances a semi-analytical (thus reasonably fast) maximum-likelihood estimation framework for PLRNNs that may enable to recover relevant aspects of the nonlinear dynamics underlying observed neuronal time series, and directly link these to computational properties.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Neuronal dynamics mediate between the physiological and anatomical properties of a neural system and the computations it performs, in fact may be seen as the ‘computational language’ of the brain. It is therefore of great interest to recover from experimentally recorded time series, like multiple single-unit or neuroimaging data, the underlying stochastic network dynamics and, ideally, even equations governing their statistical evolution. This is not at all a trivial enterprise, however, since neural systems are very high-dimensional, come with considerable levels of intrinsic (process) noise, are usually only partially observable, and these observations may be further corrupted by noise from measurement and preprocessing steps. The present article embeds piecewise-linear recurrent neural networks (PLRNNs) within a state space approach, a statistical estimation framework that deals with both process and observation noise. PLRNNs are computationally and dynamically powerful nonlinear systems. Their statistically principled estimation from multivariate neuronal time series thus may provide access to some essential features of the neuronal dynamics, like attractor states, generative equations, and their computational implications. The approach is exemplified on multiple single-unit recordings from the rat prefrontal cortex during working memory.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Deutsche Forschungsgemeinschaft (DE)</institution>
</funding-source>
<award-id>Du 354/8-1</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>SFB1134-D01</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002347</institution-id>
<institution>Bundesministerium für Bildung und Forschung</institution>
</institution-wrap>
</funding-source>
<award-id>01ZX1314G (SP10)</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9340-3786</contrib-id>
<name name-style="western">
<surname>Durstewitz</surname>
<given-names>Daniel</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was funded through two grants from the German Research Foundation (DFG, Du 354/8-1, and within the Collaborative Research Center 1134, D01) to the author, and by the German Ministry for Education and Research (BMBF, 01ZX1314G, SP10) within the e:Med program. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="0"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All Matlab code for PLRNN estimation, running major examples, and the parameter files and experimental data is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/durstdby/PLRNNstsp" xlink:type="simple">https://github.com/durstdby/PLRNNstsp</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Stochastic neural dynamics mediate between the underlying biophysical and physiological properties of a neural system and its computational and cognitive properties (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref004">4</xref>]). Hence, from a computational perspective, we are often interested in recovering the neural network dynamics of a given brain region or neural system from experimental measurements. Yet, experimentally, we commonly have access only to noisy recordings from a relatively small proportion of neurons (compared to the size of the brain area of interest), or to lumped surface signals like local field potentials or the EEG. Inferring from these the computationally relevant dynamics is therefore not trivial, especially since both the recorded signals (e.g., spike sorting errors; [<xref ref-type="bibr" rid="pcbi.1005542.ref005">5</xref>]) as well as the neural system dynamics itself (e.g., stochastic synaptic release; [<xref ref-type="bibr" rid="pcbi.1005542.ref006">6</xref>]) come with a good deal of noise. The stochastic nature of neural dynamics has, in fact, been deemed crucial for perceptual inference and decision making [<xref ref-type="bibr" rid="pcbi.1005542.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref009">9</xref>], and potentially helps to avoid local minima in task learning or problem solving [<xref ref-type="bibr" rid="pcbi.1005542.ref010">10</xref>].</p>
<p>Speaking in statistical terms, 'model-free' techniques which combine delay embedding methods with nonlinear basis expansions and kernel techniques have been one approach to the problem [<xref ref-type="bibr" rid="pcbi.1005542.ref011">11</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref012">12</xref>]. These techniques provide informative lower-dimensional visualizations of population trajectories and (local) approximations to the neural flow field, but they may highlight only certain, salient aspects of the dynamics (but see [<xref ref-type="bibr" rid="pcbi.1005542.ref013">13</xref>]) and, in any case, do not directly return distribution generating equations or underlying computations. Alternatively, state space models, a statistical framework particularly popular in engineering and ecology (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref014">14</xref>]), have been adapted to extract lower-dimensional, probabilistic neural trajectory flows from higher-dimensional recordings [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref025">25</xref>]. State space models link a process model of the unobserved (latent) underlying dynamics to the experimentally observed time series via observation equations, and differentiate between stochasticity in the process and observation noise (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>]). So far, with few exceptions (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref023">23</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>]), these models assumed linear latent dynamics, however. Although this may often be sufficient to yield lower-dimensional smoothed trajectories, it implies that the recovered dynamical model may be less apt for capturing highly nonlinear dynamical phenomena in the observations, and will by itself not be powerful enough to reproduce a range of important dynamical and computational processes in the nervous system, among them multi-stability which has been proposed to underlie neural activity during working memory [<xref ref-type="bibr" rid="pcbi.1005542.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref032">32</xref>], limit cycles (stable oscillations), or chaos (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref033">33</xref>]).</p>
<p>Here we derive a new state space algorithm based on piecewise-linear (PL) recurrent neural networks (RNN). It has been shown that RNNs with nonlinear activation functions can, in principle, approximate any dynamical system's trajectory or, in fact, dynamical system itself (given some general conditions; [<xref ref-type="bibr" rid="pcbi.1005542.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref036">36</xref>]). Thus, in theory, they are powerful enough to recover whatever dynamical system is underlying the experimentally observed time series. Piecewise linear activation functions, in particular, are by now the most popular choice in deep learning algorithms [<xref ref-type="bibr" rid="pcbi.1005542.ref037">37</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref039">39</xref>], and considerably simplify some of the derivations within the state space framework (as shown later). They may also be more apt for producing working memory-type activity with longer delays if for some units the transfer function happens to coincide with the bisectrix (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref040">40</xref>]), and ease the analysis of fixed points and stability. We then apply this newly derived algorithm to multiple single-unit recordings from the rat prefrontal cortex obtained during a classical delayed alternation working memory task [<xref ref-type="bibr" rid="pcbi.1005542.ref041">41</xref>].</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>State space model</title>
<p>This article considers simple discrete-time piecewise-linear (PL) recurrent neural networks (RNN) of the form
<disp-formula id="pcbi.1005542.e001">
<alternatives>
<graphic id="pcbi.1005542.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">max</mml:mi><mml:mo>{</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>}</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold">C</mml:mi><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">ε</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">ε</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <bold>z</bold><sub><italic>t</italic></sub> = (<italic>z</italic><sub>1<italic>t</italic></sub>…<italic>z</italic><sub><italic>Mt</italic></sub>)<sup><italic>T</italic></sup> is the (<italic>M</italic>×1)-dimensional (latent) neural state vector at time <italic>t</italic> = 1…<italic>T</italic>, <bold>A</bold> = <italic>diag</italic>([<italic>a</italic><sub>11</sub>…<italic>a</italic><sub><italic>MM</italic></sub>]) is an <italic>M</italic>×<italic>M</italic> diagonal matrix of auto-regression weights, <bold>W</bold> = (0 <italic>w</italic><sub>12</sub>…<italic>w</italic><sub>1<italic>M</italic></sub>, <italic>w</italic><sub>21</sub> 0 <italic>w</italic><sub>23</sub>…<italic>w</italic><sub>2<italic>M</italic></sub>, <italic>w</italic><sub>31</sub> <italic>w</italic><sub>32</sub> 0 <italic>w</italic><sub>34</sub>…<italic>w</italic><sub>3<italic>M</italic></sub>,…) is an <italic>M</italic>×<italic>M off</italic>-diagonal matrix of connection weights, <bold>θ</bold> = (<italic>θ</italic><sub>1</sub>…<italic>θ</italic><sub><italic>M</italic></sub>)<sup><italic>T</italic></sup> is a set of (constant) activation thresholds, <bold>s</bold><sub><italic>t</italic></sub> is a sequence of (known) external <italic>K</italic>-dimensional inputs, weighted by (<italic>M</italic>×<italic>K</italic>) matrix <bold>C</bold>, and <bold>ε</bold><sub><italic>t</italic></sub> denotes a Gaussian white noise process with diagonal covariance matrix <inline-formula id="pcbi.1005542.e002"><alternatives><graphic id="pcbi.1005542.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The max-operator is assumed to work element-wise.</p>
<p>In physiological terms, latent variables <italic>z</italic><sub><italic>mt</italic></sub> are often interpreted as a membrane potential (or current) which gives rise to spiking activity as soon as firing threshold <italic>θ</italic><sub><italic>m</italic></sub> is exceeded (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref042">42</xref>,<xref ref-type="bibr" rid="pcbi.1005542.ref043">43</xref>]). According to this interpretation, the diagonal elements in <bold>A</bold> may be seen as the neurons’ individual membrane time constants, while the off-diagonal elements in <bold>W</bold> represent the between-neuron synaptic connections which multiply with the presynaptic firing rates. In statistical terms, (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>) has the form of an auto-regressive model with a nonlinear basis expansion in variables <italic>z</italic><sub><italic>mt</italic></sub> (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref044">44</xref>;<xref ref-type="bibr" rid="pcbi.1005542.ref045">45</xref>]), which retains linearity in parameters <bold>W</bold> for ease of estimation. Restricting model parameters, e.g. <bold>Σ</bold>, to be of diagonal form, is common in such models to avoid over-specification and help identifiabiliy (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref046">46</xref>]; see also further below). For instance, including a diagonal in <bold>W</bold> would be partly redundant to parameters <bold>A</bold> (strictly so in a pure linear model). For similar reasons, and for ease of presentation, in the following we will focus on a model for which <italic>K</italic> = <italic>M</italic> and <bold>C</bold> = <bold>I</bold> (i.e., no separate scaling of the inputs), although the full model as stated above, <xref ref-type="disp-formula" rid="pcbi.1005542.e001">Eq 1</xref>, was implemented as well (and code for it is provided; of course, the case <italic>K</italic>&gt;<italic>M</italic> could always be accommodated by pre-multiplying <bold>s</bold><sub><italic>t</italic></sub> by some <italic>predefined</italic> matrix <bold>C</bold>, obtained e.g. by PCA on the input space). While different model formulations are around in the computational neuroscience and machine learning literature, often they may be related by a simple transformation of variables (see [<xref ref-type="bibr" rid="pcbi.1005542.ref047">47</xref>]) and, as long as the model is powerful enough to express the whole spectrum of basic dynamical phenomena, details of model specification may also not be overly crucial for the present purposes.</p>
<p>A particular advantage of the PLRNN model is that all its fixed points can be obtained easily analytically by solving (in the absence of external input) the 2<sup><italic>M</italic></sup> linear equations
<disp-formula id="pcbi.1005542.e003">
<alternatives>
<graphic id="pcbi.1005542.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mi mathvariant="bold">θ</mml:mi></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where Ω is to denote the set of indices of units for which we assume <italic>z</italic><sub><italic>m</italic></sub> ≤ <italic>θ</italic><sub><italic>m</italic></sub>, and <bold>W</bold><sub>Ω</sub> the respective connectivity matrix in which all columns from <bold>W</bold> corresponding to units in Ω are set to 0. Obviously, to make <bold>z</bold><sub>*</sub> a true fixed point of (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>), the solution to (<xref ref-type="disp-formula" rid="pcbi.1005542.e003">2</xref>) has to be consistent with the defined set Ω, that is <italic>z</italic><sub>*<italic>m</italic></sub> ≤ <italic>θ</italic><sub><italic>m</italic></sub> has to hold for all <italic>m</italic> ∈ Ω and <italic>z</italic><sub>*<italic>m</italic></sub> &gt; <italic>θ</italic><sub><italic>m</italic></sub> for all <italic>m</italic> ∉ Ω. For networks of moderate size (say <italic>M</italic>&lt;30) it is thus computationally feasible to explicitly check for all fixed points and their stability.</p>
<p>For estimation from experimental data, latent state model (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>) is then connected to some <italic>N</italic>-dimensional observed vector time series <bold>X</bold> = {<bold>x</bold><sub>t</sub>} via a simple linear-Gaussian model,
<disp-formula id="pcbi.1005542.e004">
<alternatives>
<graphic id="pcbi.1005542.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">η</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">η</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) ≔ max{<bold>0</bold>,<bold>z</bold><sub><italic>t</italic></sub>−<bold>θ</bold>}, {<bold>η</bold><sub><italic>t</italic></sub>} is the (white Gaussian) observation noise series with diagonal covariance matrix <inline-formula id="pcbi.1005542.e005"><alternatives><graphic id="pcbi.1005542.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and <bold>B</bold> an <italic>N</italic>×<italic>M</italic> matrix of regression weights. Thus, the idea is that only the PL-transformed activation <italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) reaches the ‘observation surface’ as, e.g., with spiking activity when the underlying membrane dynamics itself is not visible. We further assume for the initial state,
<disp-formula id="pcbi.1005542.e006">
<alternatives>
<graphic id="pcbi.1005542.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
which has, for simplicity, the same covariance matrix as the process noise in general (reducing the number of to be estimated parameters). In the case of multiple, temporally separated trials, we allow each one to have its own individual initial condition <bold>μ</bold><sub><italic>k</italic></sub>, <italic>k</italic> = 1…<italic>K</italic>.</p>
<p>The general goal here is to determine both the model’s unknown parameters <bold>Ξ</bold> = {<bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>Σ</bold>,<bold>B</bold>,<bold>Γ</bold>} (assuming fixed thresholds <bold>θ</bold> for now) as well as the unobserved, latent state path <bold>Z</bold> ≔ {<bold>z</bold><sub><italic>t</italic></sub>} (and its second-order moments) from the experimentally observed time series {<bold>x</bold><sub>t</sub>}. These could be, for instance, properly transformed multivariate spike time series or neuroimaging data. This is accomplished here by the Expectation-Maximization (EM) algorithm which iterates state (E) and parameter (M) estimation steps and is developed in detail for model (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005542.e004">3</xref>) in the Methods. In the following I will first discuss state and parameter estimation separately for the PLRNN, before describing results from the full EM algorithm in subsequent sections. This will be done along two toy problems, a higher-order nonlinear oscillation (stable limit cycle), and a simple 'working memory' paradigm in which one of two discrete stimuli had to be retained across a temporal interval. Finally, the application of the validated PLRNN EM algorithm will be demonstrated on multiple single-unit recordings obtained from rats on a standard working memory task (delayed alternation; data from [<xref ref-type="bibr" rid="pcbi.1005542.ref041">41</xref>], kindly provided by Dr. James Hyman, University of Nevada, Las Vegas).</p>
</sec>
<sec id="sec004">
<title>State estimation</title>
<p>The latent state distribution, as explained in Methods, is a high-dimensional (piecewise) Gaussian mixture with the number of components growing as 2<sup>T×M</sup> with sequence length T and number of latent states M. Here a semi-analytical, approximate approach was developed that treats state estimation as a combinatorial problem by first searching for the mode of the full distribution (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref048">48</xref>]; in contrast, e.g., to a recursive filtering-smoothing scheme that makes local (linear-Gaussian) approximations, e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>]). This approach amounts to solving a high (2<sup>M×T</sup>)-dimensional piecewise linear problem (due to the piecewise quadratic, in the states <bold>Z</bold>, log-likelihood Eqs <xref ref-type="disp-formula" rid="pcbi.1005542.e029">6</xref> and <xref ref-type="disp-formula" rid="pcbi.1005542.e031">7</xref>). Here this was accomplished by alternating between (1) solving the linear set of equations implied by a given set of linear constraints <bold>Ω</bold> ≔ {(<italic>m</italic>,<italic>t</italic>)|<italic>z</italic><sub><italic>mt</italic></sub> ≤ <italic>θ</italic><sub><italic>m</italic></sub>} (cf. <xref ref-type="disp-formula" rid="pcbi.1005542.e031">Eq 7</xref> in Methods) and (2) flipping the sign of the constraints violated by the current solution <bold>z</bold><sub>*</sub>(Ω) to the linear equations, thus following a path through the (M×T)-dimensional binary space of linear constraints using Newton-type iterations (similar as in [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>], see <xref ref-type="sec" rid="sec015">Methods</xref>; note that here the ‘constraints’ are not fixed as in quadratic programming problems). Given the mode and state covariance matrix (evaluated at the mode from the negative inverse Hessian), all other expectations needed for the EM algorithm were then derived analytically, with one exception that was approximated (see <xref ref-type="sec" rid="sec015">Methods</xref> for full details).</p>
<p>The toy problems introduced above were used to assess the quality of these approximations. For the first toy problem, an order-15 limit cycle was produced with a PLRNN consisting of three recurrently coupled units, inputs to units #1 and #2, and parameter settings as indicated in <xref ref-type="fig" rid="pcbi.1005542.g001">Fig 1</xref> and provided Matlab file ‘PLRNNoscParam’. The limit cycle was repeated for 50 full cycles (giving 750 data points) and corrupted by process noise (cf. <xref ref-type="fig" rid="pcbi.1005542.g001">Fig 1</xref>). These noisy states (arranged in a (3 x 750) matrix <bold>Z</bold>) were then transformed into a (3 x 750) output matrix <bold>X</bold>, to which observation noise was added, through a randomly drawn (3 x 3) regression weight matrix <bold>B</bold>. State estimation was started from a random initial condition. True (but noise-corrupted) and estimated states for this particular problem are illustrated in <xref ref-type="fig" rid="pcbi.1005542.g001">Fig 1A</xref>, indicating a tight fit (although some fraction of the linear constraints were still violated, ≈0.27% in the present example and &lt;2.3% in the working memory example below; see <xref ref-type="sec" rid="sec015">Methods</xref> on this issue).</p>
<fig id="pcbi.1005542.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g001</object-id>
<label>Fig 1</label>
<caption>
<title>State and parameter estimates for nonlinear cycle example.</title>
<p>(A) True (solid/ open-circle lines) and estimated (dashed-star lines) states over some periods of the simulated limit cycle generated by a 3-state PLRNN when true parameters were provided (for this example, <bold>θ</bold> ≈ (0.86,0.09,–0.85); all other parameters as in B, see also provided Matlab file ‘PLRNNoscParam.mat’). ‘True states’ refers to the actual states from which the observations <bold>X</bold> were generated. Inputs of <italic>s</italic><sub><italic>it</italic></sub> = 1 were provided to units <italic>i</italic> = 1 and <italic>i</italic> = 2 on time steps 1 and 10 of each cycle, respectively. Note that true and inferred states are tightly overlapping in this low-noise example (such that the ‘stars’ appear on top of the ‘open circles’). (B) True and estimated model parameters for (from top-left to bottom-right) <bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>Σ</bold>,<bold>B</bold>,<bold>Γ</bold>, when true states (but not their higher-order moments) were provided. Bisectrix lines (black) indicate identity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g001" xlink:type="simple"/>
</fig>
<p>To examine more systematically the quality of the approximate-analytical estimates of the first and second order moments of the joint distribution across states <italic>z</italic> and their piecewise linear transformations <italic>ϕ</italic>(<italic>z</italic>), samples from p(<bold>Z</bold>|<bold>X</bold>) were simulated using bootstrap particle filtering (see <xref ref-type="sec" rid="sec015">Methods</xref>). Although these simulated samples are based only on the filtering (not the smoothing) steps (and (re-)sampling schemes may have issues of their own; e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>], analytical and sampling estimates were in tight agreement, correlating almost to 1 for this example, as shown in <xref ref-type="fig" rid="pcbi.1005542.g002">Fig 2</xref>.</p>
<fig id="pcbi.1005542.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Agreement between simulated (<italic>x</italic>-axes) and semi-analytical (<italic>y</italic>-axes) solutions for state expectancies for the model from <xref ref-type="fig" rid="pcbi.1005542.g001">Fig 1</xref> across all three state variables and <italic>T</italic> = 750 time steps.</title>
<p>Here, <italic>ϕ</italic>(<italic>z</italic><sub><italic>i</italic></sub>) ≔ max{0,<italic>z</italic><sub><italic>i</italic></sub>−<italic>θ</italic><sub><italic>i</italic></sub>} is the PL activation function. Simulated state paths and their moments were generated using a bootstrap particle filter with 10<sup>4</sup> particles. Bisectrix lines in gray indicate identity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g002" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3A</xref> illustrates the setup of the ‘two-cue working memory task’, chosen for later comparability with the experimental setup. A 5-unit PLRNN was first trained by conventional gradient descent (‘real-time recurrent learning’ (RTRL), see [<xref ref-type="bibr" rid="pcbi.1005542.ref050">50</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref051">51</xref>]) to produce a series of six 1’s on unit #3 and six 0’s on unit #4 five time steps after an input (of 1) occurred on unit #1, and the reverse pattern (six 0’s on unit #3 and six 1’s on unit #4) five time steps after an input occurred on unit #2. A stable PLRNN with a reasonable solution to this problem was then chosen for further testing the present algorithm (cf. <xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3C</xref>). (While the RTRL approach was chosen to derive a working memory circuit with reasonably ‘realistic’ characteristics like a wider distribution of weights, it is noted that a multi-stable network is relatively straightforward to construct explicitly given the analytical accessibility of fixed points (see <xref ref-type="sec" rid="sec015">Methods</xref>); for instance, choosing <bold>θ</bold> = (0.5 0.5 0.5 0.5 2), <bold>A</bold> = (0.9 0.9 0.9 0.9 0.5), and <bold>W</bold> = (0 <italic>ω</italic> − <italic>ω</italic> − <italic>ω</italic> − <italic>ω</italic>, <italic>ω</italic> 0 − <italic>ω</italic> − <italic>ω</italic> – <italic>ω</italic>, − <italic>ω</italic> − <italic>ω</italic> 0 <italic>ω</italic> – <italic>ω</italic>, − <italic>ω</italic> − <italic>ω ω</italic> 0 − <italic>ω</italic>, 11110) with <italic>ω</italic> = 0.2, yields a tri-stable system.) Like for the limit cycle problem before, the number of observations was taken to be equal to the number of latent states, and process and observation noise were added (see <xref ref-type="fig" rid="pcbi.1005542.g004">Fig 4</xref> and Matlab file ‘PLRNNwmParam’ for specification of parameters). The system was simulated for 20 repetitions of each trial type (i.e., cue-1 or cue-2 presentations) with different noise realizations and each ‘trial’ started from its own initial condition <bold>μ</bold><sub><italic>k</italic></sub> (see <xref ref-type="sec" rid="sec015">Methods</xref>), resulting in a total series length of T = 20×2×20 = 800 (although, importantly, in this case the time series consisted of distinct, temporally segregated trials, instead of one continuous series, and was treated as such an ensemble of series by the algorithm). As before, state estimation started from random initial conditions and was provided with the correct parameters, as well as with the observation matrix <bold>X</bold>. While <xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3B</xref> illustrates the correlation between true (i.e., simulated) and estimated states across all trials and units, <xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3C</xref> shows true and estimated states for a representative cue-1 (left) and cue-2 (right) trial, respectively. Again, our procedure for obtaining (or approximating) the maximum a-posteriori (MAP) estimate of the state distribution appears to work quite well (in general, only locally optimal or approximate solutions may be achieved, however, and the algorithm may have to be repeated with different state initializations; see <xref ref-type="sec" rid="sec015">Methods</xref>).</p>
<fig id="pcbi.1005542.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g003</object-id>
<label>Fig 3</label>
<caption>
<title>State estimation for ‘working memory’ example when true parameters were provided.</title>
<p>(A) Setup of the simulated working memory task: Stimulus inputs (green bars, <italic>s</italic><sub><italic>it</italic></sub> = 1, and 0 otherwise) and requested outputs (black = 1, light-gray = 0, dark-grey = no output required) across the 20 time points of a working memory trial (with two different trial types) for the 5 PLRNN units. (B) Correlation between estimated and true states (i.e., those from which the observations <bold>X</bold> were generated) across all five state variables and <italic>T</italic> = 800 time steps. Bisectrix in black. (C) True (open-circle/ solid lines) and estimated (star-dashed lines) states for output units #3 (blue) and #4 (red) when <italic>s</italic><sub>15</sub> = 1 (left) or <italic>s</italic><sub>25</sub> = 1 (right) for single example trials. Note that true and inferred states are tightly overlapping in this low-noise example (such that the ‘stars’ often appear on top of the ‘open circles’). Although working memory PLRNNs may, in principle, be explicitly designed (see text), here a 5-state PLRNN was first trained by conventional gradient descent (real-time recurrent-learning [<xref ref-type="bibr" rid="pcbi.1005542.ref050">50</xref>]) to perform the task in A, to yield more ‘natural’ and less uniform ground truth states and parameters. Here, all <italic>θ</italic><sub><italic>i</italic></sub> = 0 (implying that there can only be one stable fixed point). See Matlab file ‘PLRNNwmParam.mat’ and <xref ref-type="fig" rid="pcbi.1005542.g004">Fig 4</xref> for details on parameters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g003" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g004</object-id>
<label>Fig 4</label>
<caption>
<title>True and estimated parameters for the working memory PLRNN (cf. <xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3</xref>) when true states were provided.</title>
<p>From top-left to bottom-right, estimates for: <bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>Σ</bold>,<bold>B</bold>,<bold>Γ</bold>. Note that most parameter estimates were highly accurate, although all state covariance matrices still had to be estimated as well (i.e., with the true states provided as initialization for the E-step). Bisectrix lines in black indicate identity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Parameter estimation</title>
<p>Given the true states, how well would the algorithm retrieve the parameters of the PLRNN? To assess this, the actual model states (which generated the observations <bold>X</bold>) from simulation runs of the oscillation and the working memory task described above were provided as initialization for the E-step. Based on these, the algorithm first estimated the state covariances for <italic>z</italic> and <italic>ϕ</italic>(<italic>z</italic>) (see above), and then the parameters in a second step (i.e., the M-step). Note that the parameters can all be computed analytically given the state distribution (see <xref ref-type="sec" rid="sec015">Methods</xref>), and, provided the state covariance matrices (summed across time) as required in Eq <xref ref-type="disp-formula" rid="pcbi.1005542.e055">17A</xref>, <xref ref-type="disp-formula" rid="pcbi.1005542.e058">17D</xref> and <xref ref-type="disp-formula" rid="pcbi.1005542.e060">17F</xref> are non-singular, have a unique solution. Hence, in this case, any misalignment with the true model parameters can only come from one of two sources: i) estimation was based on one finite-length noisy realization of the PLRNN process, ii) all <italic>second order moments</italic> of the state distribution were still <italic>estimated</italic> based on the true state vectors. However, as can be appreciated from <xref ref-type="fig" rid="pcbi.1005542.g001">Fig 1B</xref> (oscillation) and <xref ref-type="fig" rid="pcbi.1005542.g004">Fig 4</xref> (working memory), for the two (relatively low-noise) example scenarios studied here, all parameter estimates still agreed tightly with those describing the true underlying model.</p>
<p>In the more general case where <italic>both</italic> the states and the parameters are unknown and only the observations are given, note that the model as stated in Eqs <xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref> &amp; <xref ref-type="disp-formula" rid="pcbi.1005542.e004">3</xref> is over-specified as, for instance, at the level of the observations, additional variance placed into Σ may be compensated for by adjusting Γ accordingly, and by rescaling <bold>W</bold> and, within limits, <bold>A</bold> (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref052">52</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref053">53</xref>]). In the following we therefore always arbitrarily fixed <bold>Σ</bold> (to some scalar; see <xref ref-type="sec" rid="sec015">Methods</xref>), as common in many latent variable models (like factor analysis), including state space models (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref046">46</xref>]). It may be worth noting here that the relative size of <bold>Σ</bold> vs. <bold>Γ</bold> determines how much weight is put on temporal consistency among states (“<bold>Σ</bold>&lt;<bold>Γ</bold>”) vs. fitting of the observations (“<bold>Σ</bold>&gt;<bold>Γ</bold>”) within the likelihood, <xref ref-type="disp-formula" rid="pcbi.1005542.e028">Eq 5</xref>.</p>
</sec>
<sec id="sec006">
<title>Joint estimation of states and parameters by EM</title>
<p>The observations above confirm that our algorithm finds satisfactory approximations to the underlying state path and state covariances when started with the right parameters, and—vice versa—identifies the correct parameters when provided with the true states. Indeed, the M-step, since it is exact, can only increase the expected log-likelihood <xref ref-type="disp-formula" rid="pcbi.1005542.e028">Eq 5</xref> with the present state expectancies fixed. However, due to the system's piecewise-defined discrete nature, modifying the parameters may lead to a new set of constraint violations, that is may throw the system into a completely different linear subspace which may imply a decrease in the likelihood in the E-step. It is thus not guaranteed that a straightforward EM algorithm converges (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref054">54</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref055">55</xref>]), or that the likelihood would even monotonically increase with each EM iteration.</p>
<p>To examine this issue, full EM estimation of the WM model (as specified in <xref ref-type="fig" rid="pcbi.1005542.g004">Fig 4</xref>, using <italic>N</italic> = 20 outputs in this case) was run 240 times, starting from different random, uniformly distributed initializations for the parameters. <xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5B</xref> (Δ<italic>t</italic> = 0) gives, for the five highest likelihood solutions across all 240 runs (<xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5A</xref>), the mean squared error (MSE) <inline-formula id="pcbi.1005542.e007"><alternatives><graphic id="pcbi.1005542.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mi mathvariant="normal">avg</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> between actual neural observations <italic>x</italic><sub><italic>it</italic></sub> and model predictions <inline-formula id="pcbi.1005542.e008"><alternatives><graphic id="pcbi.1005542.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, which is close to 0 (and, correspondingly, correlations between predicted and actual observations were close to 1). With respect to the inferred states, note that estimated and true model states may not be in the same order, as any permutation of the latent state indices together with the respective columns of observation matrix <bold>B</bold> will be equally consistent with the data <bold>X</bold> (see also [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>]). For the WM model examined here, however, partial order information is implicitly provided to the EM algorithm through the definition of unit-specific inputs <italic>s</italic><sub><italic>it</italic></sub>. For the present example, true and estimated states for the highest likelihood solution were nicely linearly correlated for all 5 latent variables (<xref ref-type="fig" rid="pcbi.1005542.g006">Fig 6</xref>), but some of the regression slopes significantly differed from 1, indicating a degree of freedom in the scaling of the states. Note that if the system were strictly linear, the states would be identifiable only up to a linear transformation in general, since any multiplication of the latent states by some matrix <bold>V</bold> could essentially be reversed at the level of the outputs by back-multiplying <bold>B</bold> with <bold>V</bold><sup>-1</sup> (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>]). Likewise, in the present <italic>piecewise linear</italic> system, one may expect that there is a class of piecewise-linear transformations of the states which is still compatible with the observed outputs, and hence that the model is only identifiable up to this class of transformations (a general issue with state space models, of course, not particular to the present one; cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref053">53</xref>]). However, this might not be a too serious issue, if one is primarily interested in the latent dynamics (rather than in the exact parameters).</p>
<fig id="pcbi.1005542.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Performance of full EM algorithm on working memory model.</title>
<p>(A) Log-likelihood as a function of EM iteration for the highest-likelihood run out of all 240 initializations. As in this example, the log-likelihood, although generally increasing, was not always monotonic (note the little ripples; see <xref ref-type="sec" rid="sec010">discussion</xref> in Results). (B) Mean squared prediction error, <inline-formula id="pcbi.1005542.e009"><alternatives><graphic id="pcbi.1005542.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mi mathvariant="normal">avg</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, between true (<inline-formula id="pcbi.1005542.e010"><alternatives><graphic id="pcbi.1005542.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) and predicted (<inline-formula id="pcbi.1005542.e011"><alternatives><graphic id="pcbi.1005542.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) observations across all 20 output variables and the 5 highest-likelihood solutions, as a function of ahead-prediction time step Δ<italic>t</italic>, for the original PLRNN (blue curve) and for a linear dynamical system (LDS; red curve) estimated via EM from the same, PLRNN-generated data. Note that while the true and estimated observations agree almost perfectly for both the PLRNN and LDS if predicted directly from the inferred states (i.e., <inline-formula id="pcbi.1005542.e012"><alternatives><graphic id="pcbi.1005542.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>), prediction quality severely decays for the LDS while remaining high for the PLRNN if <inline-formula id="pcbi.1005542.e013"><alternatives><graphic id="pcbi.1005542.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>-predictions were made from states forecast Δ<italic>t</italic> time steps into the future (see text for further explanation; note that a slight decay in prediction quality across Δ<italic>t</italic> is inevitable because of the process noise). Error bars = SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g005" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g006</object-id>
<label>Fig 6</label>
<caption>
<title>State estimates for ML solution (cf. <xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5</xref>) from the full EM algorithm on the working memory model.</title>
<p>In this example, true and estimated states were nicely linearly related, although mostly with regression slopes deviating from 1 (see text for further <xref ref-type="sec" rid="sec010">discussion</xref>). State estimation in this case was performed by inverting only the single constraint corresponding to the largest deviation on each iteration (see <xref ref-type="sec" rid="sec015">Methods</xref>). Bisectrix lines in black indicate identity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g006" xlink:type="simple"/>
</fig>
<p><xref ref-type="fig" rid="pcbi.1005542.g007">Fig 7</xref> illustrates the distribution of initial and final parameter estimates around their true values across all 240 runs (before and after reordering the estimated latent states based on the rotation that would be required for achieving the optimal mapping onto the true states, as determined through Procrustes analysis). <xref ref-type="fig" rid="pcbi.1005542.g007">Fig 7</xref> reveals that a) the EM algorithm does clearly improve the estimates and b) these final estimates seemed to be relatively ‘unbiased’ (i.e., with deviations centered around 0).</p>
<fig id="pcbi.1005542.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Full EM algorithm on working memory model.</title>
<p>(A) Parameter estimates for ML solution from <xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5</xref>. True parameters (on <italic>x</italic>-axes or as blue bars, respectively), initial (gray circles or green bars) and final (black circles or yellow bars) parameter estimates for (from left to right) <bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>B</bold>,<bold>Γ</bold>. Bisectrix lines in blue. Correlations between true and final estimates are indicated on top (note from <xref ref-type="disp-formula" rid="pcbi.1005542.e057">Eq 17C</xref> that the estimates for <bold>μ</bold><sub>0</sub> are based on just one state, hence will naturally be less precise). (B) Distributions of initial (gray curves), final (black-solid curves), and final after reordering of states (black-dashed curves), deviations between estimated and true parameters across all 240 EM runs from different initial conditions. All final distributions were approximately centered around 0, indicating that final parameter estimates were relatively unbiased. Note that partial information about state assignments was implicitly provided to the network through the unit-specific inputs (and, more generally, may also come from the unit-specific thresholds <italic>θ</italic><sub><italic>i</italic></sub>, although these were all set to 0 for the present example), and hence state reordering only produced slight improvements in the parameter estimates.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Computational complexity of state inference and EM algorithm</title>
<p>How do the computational costs of the algorithm grow as the number of latent variables in the model is increased? As pointed out in Paninski et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>], exploiting the block-tridiagonal nature of the covariance matrices, the numerical operations within one iteration of the state inference algorithm (i.e., solving <inline-formula id="pcbi.1005542.e014"><alternatives><graphic id="pcbi.1005542.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <xref ref-type="disp-formula" rid="pcbi.1005542.e031">Eq 7</xref>) can be done in linear, <italic>O(M×T)</italic>, time, just like with the Kalman filter (due to the model’s Markov properties, full inversion of the Hessian is also not necessary to obtain the relevant moments of the posterior state distribution). This leaves open the question of how many more mode search iterations, i.e. linear equation solving (<xref ref-type="disp-formula" rid="pcbi.1005542.e031">Eq 7</xref>) and constraint-flipping (vector <bold>d</bold><sub>Ω</sub>) steps, are required as the number of latent variables (through either <italic>M</italic> or <italic>T</italic>) increases. The answer is provided in <xref ref-type="fig" rid="pcbi.1005542.g008">Fig 8A</xref> which is based on the experimental data set discussed below. Although a full computational complexity analysis is beyond the scope of this paper, at least for these example data (and similar to what has sometimes been reported for the somewhat related Simplex algorithm; [<xref ref-type="bibr" rid="pcbi.1005542.ref056">56</xref>]), the increase with <italic>M</italic> appears to be at most linear. Likewise, the <italic>total</italic> number of iterations within the full EM procedure, i.e. the number of mode-search steps summed across <italic>all</italic> EM iterations (thus reflecting the overall scaling of the full algorithm), was about linear (<xref ref-type="fig" rid="pcbi.1005542.g008">Fig 8B</xref>; in this case, single-constraint instead of complete flipping (see <xref ref-type="sec" rid="sec015">Methods</xref>) was used which, of course, increases the overall number of iterations but may perform more stably; note that in general the absolute number of iterations will also depend on detailed parameter settings of the algorithm, like the EM convergence criterion and error tolerance). Thus, overall, the present state inference algorithm seems to behave quite favorably, with an at most linear increase in the number of iterations required as the number of latent variables is ramped up.</p>
<fig id="pcbi.1005542.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Computational performance of state inference (E-step) and full EM algorithm as the number of latent states is increased.</title>
<p>(A) The number of full mode-search iterations, i.e. the number of constraint-sets <bold>Ω</bold> visited as defined through constraint vector <bold>d</bold> (cf. <xref ref-type="disp-formula" rid="pcbi.1005542.e031">Eq 7</xref>) within one E-step, increases (sub-)linearly with the number <italic>M</italic> of latent states included in the model. (B) Likewise, the <italic>total</italic> number of mode-search steps (evaluated with <italic>single-constraint</italic> flipping here) summed across all EM iterations increases about linearly with <italic>M</italic> (single-constraint flipping requires about 10-fold more iterations than full-constraint flipping, but was observed to perform more stably). Note that this measure combines the number of EM iterations with the number of mode-search steps during each EM pass, and in this sense reflects the scaling of the full EM procedure. Performance tests shown were run on the experimental data sets illustrated in Figs <xref ref-type="fig" rid="pcbi.1005542.g009">9</xref>–<xref ref-type="fig" rid="pcbi.1005542.g012">12</xref>. Means were obtained across 40 different initial conditions (with each, in turn, representing the mean from 3x14 = 42 runs in A, or 14 runs in B, separately for each of 14 trials). Error bars = SEM (across initial conditions).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g008" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Prediction of single unit responses.</title>
<p>(A) Top row: Example of an ACC unit (darker-gray curves) captured very well by the estimated PLRNN despite considerable trial to trial fluctuations (3 consecutive trials shown). Both model estimates from the directly inferred states (black curves) and from 1-step-ahead predictions of states <bold>z</bold><sub>t</sub> (dashed curves) are shown. Bottom row: Example of another ACC unit on the same three trials where only the average trend was captured by the PLRNN when firing rates were estimated from either the directly inferred or predicted states. Gray vertical bars in all panels indicate times of cue/ response. State estimation in this case was performed by inverting only the single constraint corresponding to the largest deviation on each iteration (see <xref ref-type="sec" rid="sec015">Methods</xref>). (B) Correlations among actual (<inline-formula id="pcbi.1005542.e015"><alternatives><graphic id="pcbi.1005542.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) and predicted (<inline-formula id="pcbi.1005542.e016"><alternatives><graphic id="pcbi.1005542.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) observations for all 19 neurons within this data set.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g009" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Log-likelihood of PLRNN and cross-validation performance of linear (LDS) and nonlinear (PLRNN) state space models on the ACC data.</title>
<p>(A) Examples of log-likelihood curves across EM iterations from the 5/36 highest-likelihood runs for a 5-state PLRNN estimated from 19 simultaneously recorded prefrontal neurons on a working memory task (cf. <xref ref-type="fig" rid="pcbi.1005542.g009">Fig 9</xref>). State estimation here was performed by inverting only the single constraint corresponding to the largest deviation on each iteration (see <xref ref-type="sec" rid="sec015">Methods</xref>). (B) Cross-validation error (CVE) for the PLRNN (red curve) and the LDS (blue curve) as a function of the number of latent states <italic>M</italic>. CVE was assessed on each of 14 left-out trials with model parameters estimated from the remaining 14–1 = 13 experimental trials. Shown are squared errors <inline-formula id="pcbi.1005542.e017"><alternatives><graphic id="pcbi.1005542.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> averaged across all units <italic>i</italic>, time points <italic>t</italic>, and 40 different initial conditions. (C) Same as A, but with outputs <inline-formula id="pcbi.1005542.e018"><alternatives><graphic id="pcbi.1005542.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> estimated from states predicted Δ<italic>t</italic> = 1 (solid curves) or Δ<italic>t</italic> = 3 (dashed curves) time steps ahead. Note that in this case the PLRNN consistently performs better than a LDS for all <italic>M</italic>, with the PLRNN-LDS difference growing as Δ<italic>t</italic> increases. Error bars represent SEMs across those of the 40 initial conditions for which stable models were obtained (same for the means).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g010" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Example (from one of the 5 highest likelihood solutions) for latent states of a PLRNN with <italic>M</italic> = 10 estimated from ACC multiple single-unit recordings during working memory (cf. Figs <xref ref-type="fig" rid="pcbi.1005542.g009">9</xref> and <xref ref-type="fig" rid="pcbi.1005542.g010">10</xref>).</title>
<p>Shown are trial averages for left-lever (blue) and right-lever (red) trials with SEM-bands computed across trials. Dashed vertical lines flank the 10 s period of the delay phase used for model estimation. Note that latent variables <italic>z</italic><sub>4</sub> and <italic>z</italic><sub>5</sub>, in particular, differentiate between left and right lever responses throughout most of the delay period.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g011" xlink:type="simple"/>
</fig>
<fig id="pcbi.1005542.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005542.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Initial (gray) and final (black) distributions of maximum (absolute) eigenvalues associated with all fixed points of 400 PLRNNs estimated from the experimental data (cf. Figs <xref ref-type="fig" rid="pcbi.1005542.g009">9</xref>–<xref ref-type="fig" rid="pcbi.1005542.g011">11</xref>) with different initializations of parameters, including the (fixed) threshold parameters <italic>θ</italic><sub><italic>i</italic></sub>.</title>
<p>Initial parameter configurations were deliberately chosen to yield a rather uniform distribution of absolute eigenvalues ≤ 3.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005542.g012" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Application to experimental recordings</title>
<p>I next was interested in what kind of structure the present PLRNN approach would retrieve from experimental multiple (<italic>N</italic> = 19) single-unit recordings obtained while rats were performing a simple and well-examined working memory task, namely spatial delayed alternation [<xref ref-type="bibr" rid="pcbi.1005542.ref041">41</xref>] (see <xref ref-type="sec" rid="sec015">Methods</xref>). (Note that in the present context this analysis is mainly meant as an exemplification of the current model approach, not as a detailed examination of the working memory issue itself.) The delay was always initiated by a nose poke of the animal into a port located on the side opposite from the response levers, and had a minimum length of 10 s. Spike trains were first transformed into kernel density estimates by convolution with a Gaussian kernel (see <xref ref-type="sec" rid="sec015">Methods</xref>), as done previously (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref012">12</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref057">57</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref058">58</xref>]), and binned with 500 ms resolution. This also renders the observed data more suitable to the Gaussian noise assumptions of the present observation model, <xref ref-type="disp-formula" rid="pcbi.1005542.e004">Eq 3</xref>. Models with different numbers of latent states were estimated, with <italic>M</italic> = 5 or <italic>M</italic> = 10 chosen for the examples below. Periods of cue presentation were indicated to the model by setting external inputs <italic>s</italic><sub><italic>it</italic></sub> = 1 to units <italic>i</italic> = 1 (left lever) or <italic>i</italic> = 2 (right lever) for three 500 ms time bins surrounding the event (and <italic>s</italic><sub><italic>it</italic></sub> = 0 otherwise), and the response period was indicated by setting <italic>s</italic><sub>3<italic>t</italic></sub> = 1 for 3 consecutive time bins irrespective of the correct response side (i.e., non-discriminatively). The EM algorithm was started from a range of different initializations of the parameters (including thresholds <bold>θ</bold>), and the 5 highest likelihood solutions were considered further for the examples below.</p>
<p><xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10A</xref> gives the log-likelihoods across EM iterations for these 5 highest-likelihood solutions (starting from 36 different initializations) for the <italic>M</italic> = 5 model. Interestingly, there were single neurons whose responses were predicted quite well by the estimated model despite large trial-to-trial fluctuations (<xref ref-type="fig" rid="pcbi.1005542.g009">Fig 9A</xref>, top row), while there were others with similar trial-to-trial fluctuations for which the model only captured the general trend (<xref ref-type="fig" rid="pcbi.1005542.g009">Fig 9A</xref>, bottom row; to put this into context, <xref ref-type="fig" rid="pcbi.1005542.g009">Fig 9B</xref> gives the full distribution of correlations between actual and predicted observations across all 19 neurons). This may potentially indicate that trial-to-trial fluctuations in single neurons could be for very different reasons: For instance, in those cases where strongly varying single unit responses are nevertheless tightly reproduced by the estimated model, a larger proportion of their trial-to-trial fluctuations may have been captured by the latent state dynamics, ultimately rooted in different (trial-unique) initializations of the states (recall that the states are not completely free to vary in accounting for the observations, but are constrained by the model’s temporal consistency requirements). In contrast, when only the average trend is captured, the neuron’s trial-to-trial fluctuations may be more likely to represent true intrinsic (or measurement) noise sources that the model’s deterministic part cannot account for. In practice, such conclusions would have to be examined more carefully to rule out that no other factors in the estimation procedure, like different local maxima, initializations, or over-fitting issues (see below), could account for these differences. Although this was not further investigated here, this observation nevertheless highlights the potential of (nonlinear) state space models to possibly provide new insights also into other long-standing issues in neurophysiology.</p>
<p>Cross-validation is an established means to address over-fitting [<xref ref-type="bibr" rid="pcbi.1005542.ref045">45</xref>], although due to the presence of both unknown parameters and unknown states, its application to state space models and its interpretation in this context may be a bit less straightforward. Here the cross-validation error was first assessed by leaving out each of the 14 experimental trials in turn, estimating model parameters <bold>Ξ</bold> from the remaining 13 trials, inferring states <bold>z</bold><sub><italic>t</italic></sub> given these parameters on the left-out trial, and computing the squared prediction errors <inline-formula id="pcbi.1005542.e019"><alternatives><graphic id="pcbi.1005542.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> between actual neural observations <italic>x</italic><sub><italic>it</italic></sub> and model predictions <inline-formula id="pcbi.1005542.e020"><alternatives><graphic id="pcbi.1005542.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> on the left-out trial. As shown in <xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10B</xref>, this measure steadily (albeit sub-linearly) decreases as the number <italic>M</italic> of latent states in the model is increased. At first sight, this seems to suggest that with <italic>M</italic> = 5 or even <italic>M</italic> = 10 the over-fitting regime is not yet reached. On the other hand, the latent states are, of course, not completely fixed by the transition equations, but have some freedom to vary as well (the true effective degrees of freedom for such systems are in fact very hard to determine, cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref059">59</xref>]). Hence, we also examined the Δ<italic>t</italic>-step-ahead prediction errors, that is, when the transition model were iterated Δ<italic>t</italic> steps forward in time, and <inline-formula id="pcbi.1005542.e021"><alternatives><graphic id="pcbi.1005542.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> estimated from the deterministically <italic>predicted</italic> states <inline-formula id="pcbi.1005542.e022"><alternatives><graphic id="pcbi.1005542.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">H</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (with H<sup>Δ<italic>t</italic></sup> the Δ<italic>t</italic>-times iterated map H(<bold>z</bold><sub><italic>t</italic></sub>) = <bold>Az</bold><sub><italic>t</italic></sub> + <bold>W</bold><italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) + <bold>Cs</bold><sub><italic>t</italic></sub>), not from the directly inferred states (that is, predictions were made on data points which were neither used to estimate parameters nor to infer the current state). These curves are shown for Δ<italic>t</italic> = 1 and Δ<italic>t</italic> = 3 in <xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10C</xref>, and confirm that <italic>M</italic> = 5 might be a reasonable choice at which over-fitting has not yet ensued. (Alternatively, the predictive log-likelihood, <inline-formula id="pcbi.1005542.e023"><alternatives><graphic id="pcbi.1005542.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>, may be used for model selection (i.e., choice of <italic>M</italic>), with <inline-formula id="pcbi.1005542.e024"><alternatives><graphic id="pcbi.1005542.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> either approximated through the E-step algorithm (with all <bold>X</bold>-dependent terms removed), or bootstrapped by generating <inline-formula id="pcbi.1005542.e025"><alternatives><graphic id="pcbi.1005542.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>-trajectories from the model with parameters <inline-formula id="pcbi.1005542.e026"><alternatives><graphic id="pcbi.1005542.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> (note that this is different from particle filtering since <inline-formula id="pcbi.1005542.e027"><alternatives><graphic id="pcbi.1005542.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> does not depend on test observations <bold>X</bold><sup><italic>test</italic></sup>). This is of course, however, computationally more costly to evaluate than the Δ<italic>t</italic>-step-ahead prediction error.)</p>
<p><xref ref-type="fig" rid="pcbi.1005542.g011">Fig 11</xref> shows trial-averaged latent states for both left- and right-lever trials, illustrated in this case for one of the five highest likelihood solutions (starting from 100 different initializations) for the <italic>M</italic> = 10 model. Recall that the first 3 PLRNN units received external inputs to indicate left cue (<italic>i</italic> = 1), right cue (<italic>i</italic> = 2), or response (<italic>i</italic> = 3) periods, and so, not too surprisingly, reflect these features in their activation. On the other hand, the cue response is not very prominent in unit <italic>i</italic> = 1, indicating that activity in the driven units is not completely dominated by the external regressors either, while unit <italic>i</italic> = 10 (not externally driven) shows a clear left-cue response. Most importantly, many of the remaining state variables clearly distinguish between the left and right lever options throughout the delay period of the task, in this sense carrying a memory of the cue (previous response) within the delay. Some of the activation profiles appear to systematically climb or decay across the delay period, as reported previously (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref001">1</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref060">60</xref>]), but are a bit harder to read (at least in the absence of more detailed behavioral information), such that one may want to stick with the simpler <italic>M</italic> = 5 model discussed above. Either way, for this particular data set, the extracted latent states appear to summarize quite well the most salient computational features of this simple working memory task.</p>
<p>Further insight about the dynamical mechanisms of working memory might be gained by examining the system’s fixed points and their eigenvalue spectrum. For this purpose, the EM algorithm was started from 400 different initial conditions (that is, initial parameter estimates and threshold settings <bold>θ</bold>) with maximum absolute eigenvalues (of the corresponding fixed points) drawn from a relatively uniform distribution within the interval [0 3]. Although the estimation process rarely returned truly multi-stable solutions (just 2.5% of all cases), one frequently discussed candidate mechanism for working memory (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref029">29</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref032">32</xref>]), there was a clear trend for the final maximum absolute eigenvalues to aggregate around 1 (<xref ref-type="fig" rid="pcbi.1005542.g012">Fig 12</xref>). For the discrete-time dynamical system (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>) this implies it is close to a bifurcation, with fixed points on the brink of becoming unstable, and will tend to produce (very) slow dynamics as the degree of convergence shrinks to zero along the maximum eigenvalue direction (strictly, a single eigenvalue near 1 does not yet guarantee a slow approach, but makes it very likely, especially in a (piecewise) linear system). Indeed, effectively slow dynamics is all that is needed to bridge the delays (see also [<xref ref-type="bibr" rid="pcbi.1005542.ref001">1</xref>]), while true multi-stability may perhaps even be the physiologically less likely scenario (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref061">61</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref062">62</xref>]). (Reducing the bin width from 500 ms to 100 ms appeared to produce solutions with eigenvalues even closer to 1 while retaining stimulus selectivity across the delay, but this observation was not followed up more systematically here).</p>
</sec>
<sec id="sec009">
<title>Comparison to linear dynamical systems</title>
<p>Linear dynamical systems (LDS) have frequently and successfully been used to infer smooth neural trajectories from spike train recordings [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref020">20</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>] or other measurement modalities [<xref ref-type="bibr" rid="pcbi.1005542.ref063">63</xref>]. However, as noted before, they cannot, on their own, as a matter of principle, produce a variety of dynamical phenomena essential for neural computation and observed experimentally, including multi-stability (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref029">29</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref002">2</xref>]), limit cycles (<italic>stable</italic> oscillations; e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref003">3</xref>]), chaos (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref033">33</xref>]), and many types of bifurcations and phase transitions. For instance, the question of whether working memory performance is better explained in terms of multi-stability or effectively slow dynamics (see above, <xref ref-type="fig" rid="pcbi.1005542.g012">Fig 12</xref>) is largely beyond the realm of an LDS, due to its inherent inability to express multi-stability in the first place. An LDS is therefore less suitable for retrieving system dynamics or computations in general.</p>
<p>Nevertheless, it may still be instructive to ask how much of the underlying dynamics could already be explained in linear terms. The most direct comparison of PLRNN to LDS performance is made by replacing the nonlinearity <italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) = max{<bold>0</bold>,<bold>z</bold><sub><italic>t</italic></sub> − <bold>θ</bold>} in <xref ref-type="disp-formula" rid="pcbi.1005542.e001">Eq 1</xref> simply by the linear function <italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) = <bold>z</bold><sub><italic>t</italic></sub>−<bold>θ</bold>, yielding an LDS with exactly the same parameters <bold>Ξ</bold> as the PLRNN which can be subjected to the very same estimation and inference procedures (only that state inference can now be done exactly in just one step). <xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10B</xref> reveals that a LDS fits the observed neural recordings about as well as the PLRNN for <italic>M</italic>≤5, and starts to excel PLRNN performance for <italic>M</italic>&gt;5. Since the major difference in this context is that the PLRNN places a tighter constraint on the temporal consistency of the states through the threshold-nonlinearity, it seems reasonable that this result is due to over-fitting, i.e. the LDS due to its smoothness allows for more freedom for the states to adjust to the actual observations (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref064">64</xref>]). It is important to bear in mind that consistency with the actual observations is just one objective of the maximum-likelihood formulation, <xref ref-type="disp-formula" rid="pcbi.1005542.e028">Eq 5</xref>; the other is consistency of states across time according to the model specification.</p>
<p>Either way, the PLRNN starts to significantly outperform the LDS in terms of the Δ<italic>t</italic>-step-ahead prediction errors (see above), with the gap in performance widening as Δ<italic>t</italic> increases (<xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10C</xref>). This strongly suggests that the PLRNN has internalized aspects of the system dynamics which the LDS fails to represent, i.e. supports the presence of nonlinear structure in the transition dynamics. Interestingly, looking back at <xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5B</xref>, it turns out that even for simulated data generated by a PLRNN (at least for this example), for Δ<italic>t</italic> = 0 an estimated LDS is about as good in reproducing the actual observations as an estimated PLRNN itself (with an MSE close to 0), that is, although, unlike the PLRNN it does not have the correct model structure. However, similar to what has been observed for the experimental data (<xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10B and 10C</xref>), this performance rapidly drops and falls far behind that of the PLRNN (which remains low) as 1 or more time steps into the future are to be predicted (note that for the simulated model, unlike the experimental example, the true number of states is known of course). This confirms that although the LDS may capture the actual observations quite well, it may not, unlike the PLRNN, be able to properly represent the underlying system within its internal dynamics.</p>
<p>As a note on the side, an LDS could be utilized to find proper, efficient initializations for the corresponding PLRNN, or to first improve initial estimates (although it remains to be examined whether this could potentially also bias the search space in an unfavorable way).</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<sec id="sec011">
<title>Reconstructing computational dynamics from neuronal recordings</title>
<p>In the present work, a semi-analytical, maximum-likelihood (ML) approach for estimating piecewise-linear recurrent neural networks (PLRNN) from brain recordings was developed. The idea is that such models would provide 1) a representation of neural trajectories and computationally relevant dynamical features underlying high-dimensional experimental time series in a much lower-dimensional latent variable space (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref020">20</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref025">25</xref>]) and 2) more direct access to the neural system’s statistical and computational properties. Specifically, once estimated to reproduce the data (in the ML sense), such models may, in principle, allow for more detailed analysis and in depth insight into the system’s probabilistic computational dynamics, e.g. through an analysis of fixed points and their linear stability (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref028">28</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref030">30</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref032">32</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref047">47</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref065">65</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref070">70</xref>]), properties which are not directly accessible from the experimental time series.</p>
<p>Model-free (non-parametric) techniques, usually based on Takens’ delay embedding theorem [<xref ref-type="bibr" rid="pcbi.1005542.ref071">71</xref>] and extensions thereof [<xref ref-type="bibr" rid="pcbi.1005542.ref072">72</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref073">73</xref>], have also frequently been applied to gain insight into neuronal dynamics and its essential features, like attracting states associated with different task phases from in-vivo multiple single-unit recordings [<xref ref-type="bibr" rid="pcbi.1005542.ref011">11</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref012">12</xref>] or unstable periodic orbits extracted from relatively low-noise slice recordings [<xref ref-type="bibr" rid="pcbi.1005542.ref074">74</xref>]. In neuroscience, however, one commonly deals with high-dimensional observations, as provided by current multiple single-unit or neuroimaging techniques (which still usually constitute just a minor subset of all the system’s dynamical variables). In addition, there is a large variety of both process and measurement noise sources. Measurement noise may come from direct physical sources like, for instance, instabilities and movement in the tissue surrounding the recording electrodes, noise properties of the recording devices themselves, the mere fact that only a fraction of all system variables is experimentally accessed (‘sampling noise’), or may result from preprocessing steps like spike sorting (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref075">75</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref076">76</xref>]). Process noise sources include thermal fluctuations and the probabilistic behavior of single ion channel gating [<xref ref-type="bibr" rid="pcbi.1005542.ref077">77</xref>], probabilistic synaptic release [<xref ref-type="bibr" rid="pcbi.1005542.ref006">6</xref>], fluctuations in neuromodulatory background and hormone levels, and a large variety of uncontrollable external noise sources via the sensory surfaces, including somatosensory and visceral feedback from within the body. In fact, the stochasticity of the neural dynamics itself has been deemed essential for a number of computational processes like those involved in decision making and inference [<xref ref-type="bibr" rid="pcbi.1005542.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref009">9</xref>]. This is therefore a quite different scenario from the comparatively low-dimensional and low-noise situations in, e.g., laser physics [<xref ref-type="bibr" rid="pcbi.1005542.ref078">78</xref>], and delay-embedding-based approaches to the reconstruction of neural dynamics may have to be augmented by machine learning techniques to retrieve at least some of its most salient features [<xref ref-type="bibr" rid="pcbi.1005542.ref011">11</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref012">12</xref>].</p>
<p>Of course, model-based approaches like the one developed here are also plagued by the high dimensionality and high noise levels inherent in neural data, but perhaps to a lesser extent than approaches like delay embeddings that aim to directly construct the state space from the observations (see also [<xref ref-type="bibr" rid="pcbi.1005542.ref079">79</xref>]). This is because models as pursued in the statistical state space framework explicitly incorporate process and measurement noise assumptions into the system’s description, performing smoothing in the latent space. Also, as long as the latent variable space itself is relatively small and related to the observations by simple linear equations, as here, the high dimensionality of the observations themselves does not constitute a too serious issue for estimation. More importantly, however, it is of clear advantage to have access to process equations generating state distributions consistent with the observations, as this allows for a more in depth analysis of the system’s stochastic dynamics and its relation to neural computation (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref002">2</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref028">28</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref030">30</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref047">47</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref068">68</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref070">70</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref033">33</xref>]). There have also been various attempts to account for the observed dynamics directly in terms of nonlinear time series models (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref078">78</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref080">80</xref>]), i.e. without reference to an underlying latent variable model, e.g. through differential equations expressed in terms of nonlinear basis expansions in the observations, estimated through strongly regularized (penalized) regression methods [<xref ref-type="bibr" rid="pcbi.1005542.ref011">11</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref013">13</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref080">80</xref>]. For neuroscientific data where usually only a small subset of all dimensions is observed, this implies that this approach has to be augmented by delay embedding techniques to replace the unobserved variables. This, in turn, may potentially lead to very high-dimensional systems (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref011">11</xref>,<xref ref-type="bibr" rid="pcbi.1005542.ref013">13</xref>]) that may necessitate further pre-processing steps to reduce the dimensionality again, in a way that preserves the dynamics. Also, there is no distinction between measurement and dynamical noise in these models, and, although functionally generic, the parameters of such models may be harder to interpret in a neuroscientific context. How these different assumptions and methodological steps affect the reconstruction of neural dynamics from high-dimensional, noisy neural time series, as compared to state space models, remains an open and interesting question at this point.</p>
</sec>
<sec id="sec012">
<title>Comparison to other neural state space models</title>
<p>State space models are a popular statistical tool in many fields of science (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref014">14</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref063">63</xref>]), although their applications in neuroscience are of more recent origin [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref018">18</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref019">19</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref024">24</xref>]. The Dynamic Causal Modeling (DCM) framework advanced in the human fMRI literature to infer the functional connectivity of brain networks and their dependence on task conditions [<xref ref-type="bibr" rid="pcbi.1005542.ref063">63</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref081">81</xref>] may be seen as a state space approach, although these models usually do not contain process noise (except for the more recently proposed ‘stochastic DCM’ [<xref ref-type="bibr" rid="pcbi.1005542.ref081">81</xref>]) and are commonly estimated through Bayesian inference, which imposes more constraints (via computational burden) on the complexity of the models that could potentially be dealt with in this framework. In neurophysiology, Smith &amp; Brown [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>] were among the first to suggest a state space model for multivariate spike count data by coupling a linear-Gaussian transition model with Poisson observations, with state estimation achieved by making locally Gaussian approximations to <xref ref-type="disp-formula" rid="pcbi.1005542.e062">Eq 18</xref>. Similar models have variously been used subsequently to infer local circuit coding properties [<xref ref-type="bibr" rid="pcbi.1005542.ref018">18</xref>] or, e.g., biophysical parameters of neurons or synaptic inputs from postsynaptic voltage recordings [<xref ref-type="bibr" rid="pcbi.1005542.ref082">82</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref017">17</xref>]. Yu et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref025">25</xref>] proposed Gaussian Process Factor Analysis (GPFA) for retrieving lower-dimensional, smooth latent neural trajectories from multiple spike train recordings. In GPFA, the correlation structure among the latent variables is specified (parameterized) explicitly rather than being given through a transition model. Buesing et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref020">20</xref>], finally, discuss regularized forms of neural state space models to enforce their stability.</p>
<p>By far most of the models discussed above are linear in their latent dynamics, however (although observations may be non-Gaussian). As demonstrated in the Results, linear state space models may potentially be similarly well fit for reproducing actual observations, at least for the particular model and experimental systems studied here. In fact, this is not at all guaranteed in general, if the underlying processes are highly nonlinear (unlike those in <xref ref-type="fig" rid="pcbi.1005542.g005">Fig 5</xref> where the nonlinearity was comparatively mild (not depending on multi-stability)). Thus, they may often be sufficient to obtain smoothed neural trajectories or lower-dimensional representations of the observed process [<xref ref-type="bibr" rid="pcbi.1005542.ref025">25</xref>], to uncover properties of the underlying connectivity [<xref ref-type="bibr" rid="pcbi.1005542.ref063">63</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref081">81</xref>], or to estimate synaptic/neuronal parameters [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref082">82</xref>]. However, as linear systems are strongly limited in the repertoire of dynamics and computations they can produce (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref065">65</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref083">83</xref>]), they cannot serve as a model for the underlying computational processes and dynamics in general, and do not allow for the type of analyses which led into <xref ref-type="fig" rid="pcbi.1005542.g012">Fig 12</xref>. A LDS can, on its own, express at most one <italic>isolated</italic> fixed point (or a neutrally un-/stable continuum), or (neutrally un-/stable) sinusoidal-like cycles, but cannot represent any of the more complex phenomena which characterize physiological activity and are a hallmark of most computation. On the other hand, a direct comparison of LDS vs. PLRNN predictive performance may be highly revealing in itself: While some cognitive processes (like decision making, sequence or syntax generation) would clearly be expected to be highly nonlinear in their underlying dynamics [<xref ref-type="bibr" rid="pcbi.1005542.ref004">4</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref084">84</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref085">85</xref>], others (early stimulus responses, or value updating, for instance) may follow more of a linear rule (e.g., if stimuli were projected into a high-dimensional space for linear separability; cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref086">86</xref>]). Directly contrasting LDS with PLRNN predictions on the same data set (as carried out in <xref ref-type="fig" rid="pcbi.1005542.g010">Fig 10</xref>), may uncover such important differences in computational mechanisms, and hence constitute an interesting analysis strategy in its own right.</p>
<p>There are a couple of other exceptions from the linear framework the current work builds on: Yu et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref023">23</xref>] suggested a RNN with sigmoid-type activation function (using the error function), coupled to Poisson spike count outputs, and used it to reconstruct the latent neural dynamics underlying motor preparation and planning in non-human primates. In their work, they combined the Gaussian approximation suggested by Smith &amp; Brown [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>] with the Extended Kalman Filter (EKF) for estimation within the EM framework. These various approximations in conjunction with the iterative EKF estimation scheme may be quite prone to numerical instabilities and accumulating errors, however (cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>]). Earlier work by Roweis &amp; Ghahramani [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>] used radial basis function (RBF) networks as a partly analytically tracktable approach. Nonlinear extensions to DCM, incorporating quadratic terms, have been proposed as well recently [<xref ref-type="bibr" rid="pcbi.1005542.ref087">87</xref>]. State and parameter estimation has also been attempted in (noisy) nonlinear biophysical models [<xref ref-type="bibr" rid="pcbi.1005542.ref088">88</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref089">89</xref>], but these approaches are usually computationally expensive, especially when based on numerical sampling [<xref ref-type="bibr" rid="pcbi.1005542.ref089">89</xref>], while at the same time pursuing objectives somewhat different from those targeted here (i.e., less focused on computational properties). A very recent article by Whiteway &amp; Butts [<xref ref-type="bibr" rid="pcbi.1005542.ref090">90</xref>] discusses an approach closely related to the present one in that it also assumed piecewise linear latent states (or, ‘rectified linear units (ReLU)’). Unlike here, however, the latent states were not connected through a dynamical systems model with separate process noise (but just constrained through a smoothness prior). Indeed, the objectives of this work were different, as Whiteway &amp; Butts [<xref ref-type="bibr" rid="pcbi.1005542.ref090">90</xref>] aimed more at capturing unobserved sources of input in accounting for observed neural activity (more in the spirit of factor analysis), rather than attempting to retrieve an underlying stochastic dynamics as in the present work. They found, however, that the inclusion of nonlinearities may help in accounting for observed data and improve interpretability of the latent factors.</p>
<p>In summary, nonlinear neural state space models remain a relatively under-researched topic in theoretical neuroscience. PLRNNs, as chosen here, have the advantage of being mathematically comparatively tracktable, which allowed for the present, reasonably fast, semi-analytical algorithm, yet they are computationally and dynamically still powerful [<xref ref-type="bibr" rid="pcbi.1005542.ref091">91</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref094">94</xref>].</p>
</sec>
<sec id="sec013">
<title>Alternative inference/training schemes, network architectures, and observation distributions</title>
<p>A number of other inference schemes have been suggested for state space models, comprising both analytical approximations [<xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>] and numerical (sampling) techniques (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>]). Among the former are the Extended Kalman filter (based on local Taylor series approximations), methods based on variational inference as reviewed in Macke et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>], or the (global) Laplace approximation advertized in Paniniski et al. ([<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>]; see also [<xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>]). Durbin &amp; Koopman [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>] review different variants of particle filter schemes for sequential numerical sampling. These may often be simpler to use, but are usually computationally much more costly than the semi-analytical methods. The Unscented Kalman Filter may be seen somewhere in between, using a few deliberately chosen sample (‘sigma’) points for a local parametric assessment [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>]. Here we chose a global approach rather than a recursive-sequential scheme, that is by solving the full <italic>M×T</italic> system of linear equations within each subspace defined by constraints Ω in one go. Apart from its generally nice computational properties as discussed in Paniniski et al. [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>], it seems particularly well-suited for the present piecewise-linear model Eqs (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1005542.e004">3</xref>), in dealing with the combinatorial explosion which builds up along the chain from <italic>t</italic> = 1…<italic>T</italic>. However, the mathematical properties of the present algorithm, among them issues of convergence/monotonicity, local maxima/ saddles, and uniqueness and existence of solutions, certainly require further illumination which may lead to algorithmic improvements. In particular, identifiability of dynamics, that is to what degree and under which conditions the true underlying dynamical system could be recovered by the PLRNN-EM approach, remains an open issue (one line of extension toward greater approximation power would be polynomial basis expansions, at the cost, however, of losing the straightforward interpretation in terms of ‘neural networks’).</p>
<p>Most commonly, different variants of gradient-based techniques are being used to train recurrent neural networks to fit observations [<xref ref-type="bibr" rid="pcbi.1005542.ref040">40</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref095">95</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref096">96</xref>]. For instance, recurrent network models have been trained to perform behavioral tasks [<xref ref-type="bibr" rid="pcbi.1005542.ref043">43</xref>] or reproduce behavioral data to infer the dynamical mechanisms potentially underlying working memory [<xref ref-type="bibr" rid="pcbi.1005542.ref097">97</xref>] or context-dependent decision making [<xref ref-type="bibr" rid="pcbi.1005542.ref068">68</xref>]. In these settings, however, the observations–that is behavioral data points or requested task outputs–are usually relatively sparse in time compared to the time scale of the underlying dynamics, unlike the neural time series settings studied here where the data can be as dense as the latent state vectors of the model. More importantly, in contrast to these previous gradient-based approaches, the present scheme embeds RNNs into a statistical framework that comes with explicit probability assumptions, thereby puts error bars on state and parameter estimates and returns the posterior probability distribution across latent states, which links in with the observations through a separate measurement function (enabling, for instance, dimensionality reduction), and allows for likelihood-based statistical inference and model comparison. Some preliminary analyses using stochastic Adagrad [<xref ref-type="bibr" rid="pcbi.1005542.ref098">98</xref>] for training PLRNNs on the time series from the working memory example (cf. <xref ref-type="fig" rid="pcbi.1005542.g003">Fig 3</xref>) seemed, on top, to indicate that the resulting parameter estimates may correlate less well (&lt;0.51 for <bold>A</bold> and <bold>W</bold>, after optimal reordering of states) with the true model parameters than those obtained with the present EM approach (&gt;0.78) for the lowest error/ highest likelihood solutions (this may potentially be improved through teacher forcing, which, however, is not applicable when the observed and latent space differ in dimensionality and are related through an, in general, not strictly invertible transform, as here).</p>
<p>Other observation models, like the Poisson model for spike counts [<xref ref-type="bibr" rid="pcbi.1005542.ref015">15</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>], are also relatively straightforward to accommodate within this framework (see [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>]). However, there are also other ways to deal with spike count observations, like simple Box-Cox-type transforms to make them more Gaussian, e.g. the sqrt-transform suggested for GPFA [<xref ref-type="bibr" rid="pcbi.1005542.ref025">25</xref>], or kernel-density smoothing (e.g. [<xref ref-type="bibr" rid="pcbi.1005542.ref058">58</xref>]) as used here. The latter has the additional advantage of reducing the impact of ‘binning noise’, due to the somewhat arbitrary mapping of real-valued spike times onto discrete (user-defined) time bins for the purpose of counting. In general, from a practical perspective, it may therefore still be an open question of whether the additional computational burden that comes with non-Gaussian observation models (e.g. the requirement of Newton-Raphson steps for each mode-search iteration) pays off in the end compared to these alternatives. In either case, for the time being, it seems useful to have a more general approach which can also deal with other measurement modalities, like neuroimaging data, which are not of a count-nature.</p>
<p>The present approach could also be extended by incorporating various additional structural features. For instance, a distinction between units with excitatory vs. inhibitory connections [<xref ref-type="bibr" rid="pcbi.1005542.ref043">43</xref>] could be accommodated quite easily within the present framework (requiring constrained optimization for weight parameters, however, e.g. through quadratic programming). Or special gated linear units which make LSTM networks so powerful [<xref ref-type="bibr" rid="pcbi.1005542.ref039">39</xref>,<xref ref-type="bibr" rid="pcbi.1005542.ref040">40</xref>] may potentially also yield improvements within the present EM/ state-space framework (although, in general, one may want to be cautious about the assumptions that additional structural elements like these may imply about the underlying neural system to be examined).</p>
</sec>
<sec id="sec014">
<title>Mechanisms of working memory</title>
<p>Although the primary focus of this work was to develop and evaluate a state space framework for PLRNNs, some discussion of the applicational example chosen here, working memory, is in order. Working memory is generally defined as the ability to actively hold an item in memory, in the absence of guiding external input, for short-term reference in subsequent choice situations [<xref ref-type="bibr" rid="pcbi.1005542.ref099">99</xref>]. Various neural mechanisms have been proposed to underlie this cognitive capacity, most prominently multi-stable neural networks which retain short-term memory items by switching into one of several stimulus-selective attractor states [<xref ref-type="bibr" rid="pcbi.1005542.ref028">28</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref029">29</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref032">32</xref>]. These attractors usually represent fixed points in the firing rates, with assemblies of recurrently coupled stimulus-selective cells exhibiting high rates while those cells not coding for the present stimulus in short-term memory remaining at a spontaneous low-rate base level. These models were inspired by the physiological observation of ‘delay-active’ cells [<xref ref-type="bibr" rid="pcbi.1005542.ref100">100</xref>–<xref ref-type="bibr" rid="pcbi.1005542.ref102">102</xref>], that is cells that switch into a high-rate state during the delay periods of working memory tasks, and back to a low-rate state after completion of a trial, similar to the ‘delay-active’ latent states observed in <xref ref-type="fig" rid="pcbi.1005542.g011">Fig 11</xref>. Nakahara &amp; Doya [<xref ref-type="bibr" rid="pcbi.1005542.ref103">103</xref>] were among the first to point out, however, that, for working memory, it may be completely sufficient (or even advantageous) to tune the system close to a bifurcation point where the dynamics becomes very slow (see also [<xref ref-type="bibr" rid="pcbi.1005542.ref001">1</xref>]), and true multi-stability may not be required. This is supported by the present observation that most of the estimated PLRNN models had fixed points with eigenvalues close to 1 but were not truly bi- or multi-stable (cf. <xref ref-type="fig" rid="pcbi.1005542.g012">Fig 12</xref>), yet this was sufficient to account for maintenance of stimulus-selectivity throughout the 10 s delay of the present task (cf. <xref ref-type="fig" rid="pcbi.1005542.g011">Fig 11</xref>) and for experimental observations (cf. <xref ref-type="fig" rid="pcbi.1005542.g009">Fig 9</xref>). Recently, a number of other mechanisms for supporting working memory, however, including sequential activation of cell populations [<xref ref-type="bibr" rid="pcbi.1005542.ref104">104</xref>] or synaptic mechanisms [<xref ref-type="bibr" rid="pcbi.1005542.ref105">105</xref>] have been discussed. Thus, the neural mechanisms of working memory remain an active research area to which statistical model estimation approaches as developed here may contribute, but too broad a topic in its own right to be covered in more depth by this mainly methodological work.</p>
</sec>
</sec>
<sec id="sec015">
<title>Models and methods</title>
<sec id="sec016">
<title>Expectation-maximization algorithm: State estimation</title>
<p>As with most previous work on estimation in (neural) state space models [<xref ref-type="bibr" rid="pcbi.1005542.ref020">20</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref022">22</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref023">23</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>], we use the Expectation-Maximization (EM) framework for obtaining estimates of both the model parameters and the underlying latent state path. Due to the piecewise-linear nature of model (<xref ref-type="disp-formula" rid="pcbi.1005542.e001">1</xref>), however, the conditional latent state path density p(<bold>Z</bold>|<bold>X</bold>) is a high-dimensional ‘mixture’ of partial Gaussians, with the number of integrations to perform to obtain moments of p(<bold>Z</bold>|<bold>X</bold>) scaling as 2<sup><italic>T</italic>×<italic>M</italic></sup>. Although analytically accessible, this will be computationally prohibitive for almost all cases of interest. Our approach therefore focuses on a computationally reasonably efficient way of searching for the mode (maximum a-posteriori, MAP estimate) of p(<bold>Z</bold>|<bold>X</bold>) which was found to be in good agreement with E(<bold>Z</bold>|<bold>X</bold>) in most cases. Covariances were then approximated locally around the MAP estimate.</p>
<p>More specifically, the EM algorithm maximizes the expected log-likelihood of the joint distribution p(<bold>X</bold>,<bold>Z</bold>) as a lower bound on log <italic>p</italic>(<bold>X</bold>|<bold>Ξ</bold>) [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>], where <bold>Ξ</bold> = {<bold>μ</bold><sub>0</sub>,<bold>A</bold>,<bold>W</bold>,<bold>Σ</bold>,<bold>B</bold>,<bold>Γ</bold>} denotes the set of to-be-optimized-for parameters (note that we dropped the thresholds <bold>θ</bold> from this for now):
<disp-formula id="pcbi.1005542.e028">
<alternatives>
<graphic id="pcbi.1005542.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e028" xlink:type="simple"/>
<mml:math display="block" id="M28">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≔</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">Ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3.25em"/><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4em"/><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4em"/><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Γ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">log</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>For state estimation (<italic>E</italic>-step), if <italic>ϕ</italic> were a linear function, obtaining <italic>E</italic>(<bold>Z</bold>|<bold>X</bold>,<bold>Ξ</bold>) would be equivalent to maximizing the argument of the expectancy in (<xref ref-type="disp-formula" rid="pcbi.1005542.e028">5</xref>) w.r.t. <bold>Z</bold>, i.e., E[<bold>Z</bold>|<bold>X</bold>,<bold>Ξ</bold>] ≡ arg max<sub>Z</sub>[log <italic>p</italic>(<bold>Z</bold>,<bold>X</bold>|<bold>Ξ</bold>)] (see [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>]; see also [<xref ref-type="bibr" rid="pcbi.1005542.ref106">106</xref>]). This is because for a Gaussian mean and mode coincide. In our case, p(<bold>X</bold>,<bold>Z</bold>) is piecewise Gaussian, and we still take the approach (suggested in [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>]) of maximizing log <italic>p</italic>(<bold>Z</bold>,<bold>X</bold>|<bold>Ξ</bold>) directly w.r.t. <bold>Z</bold> (essentially a Laplace approximation of <italic>p</italic>(<bold>X</bold>|<bold>Ξ</bold>) where we neglect the Hessian which is constant around the maximizer; cf. [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref048">48</xref>]).</p>
<p>Let Ω(<italic>t</italic>) ⊆ {1…<italic>M</italic>} be the set of all indices of the units for which we have <italic>z</italic><sub><italic>mt</italic></sub> ≤ <italic>θ</italic><sub><italic>m</italic></sub> at time <italic>t</italic>, and <bold>W</bold><sub>Ω(<italic>t</italic>)</sub> and <bold>B</bold><sub>Ω(<italic>t</italic>)</sub> the matrices <bold>W</bold> and <bold>B</bold>, respectively, with all columns with indices ∈ Ω(<italic>t</italic>) set to 0. The state estimation problem can then be formulated as
<disp-formula id="pcbi.1005542.e029">
<alternatives>
<graphic id="pcbi.1005542.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mspace width="3em"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≔</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4.25em"/><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="4.25em"/><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">B</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">B</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi mathvariant="bold">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Γ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">B</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">B</mml:mi><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi mathvariant="bold">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>w</mml:mi><mml:mo>.</mml:mo><mml:mi>r</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.25em"/><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>∀</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.5em"/><mml:mtext>AND</mml:mtext><mml:mspace width="0.5em"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mo>∀</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>∉</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>Let us concatenate all state variables into one long column vector, <bold>z</bold> = (<bold>z</bold><sub>1</sub>,…,<bold>z</bold><sub><italic>T</italic></sub>) = (<italic>z</italic><sub>11</sub>…<italic>z</italic><sub><italic>mt</italic></sub>…<italic>z</italic><sub><italic>MT</italic></sub>)<sup><italic>T</italic></sup>, and unwrap the sums across time into large, block-banded <italic>MT×MT</italic> matrices (see [<xref ref-type="bibr" rid="pcbi.1005542.ref016">16</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref083">83</xref>]) in which we combine all terms quadratic or linear in <bold>z</bold>, or <italic>ϕ</italic>(<bold>z</bold>), respectively. Further, define <bold>d</bold><sub>Ω</sub> as the binary (<italic>MT×</italic>1) indicator vector which has 1s everywhere except for the entries with indices ∈ <bold>Ω</bold> ⊆ {1…<italic>MT</italic>} which are set to 0, and let <bold>D</bold><sub>Ω</sub> ≔ <italic>diag</italic>(<bold>d</bold><sub>Ω</sub>) the <italic>MT×MT</italic> diagonal matrix formed from <bold>d</bold><sub>Ω</sub>. Let <bold>Θ</bold> ≔ (<bold>θ</bold>,<bold>θ</bold>,…,<bold>θ</bold>)<sub>(<italic>MT</italic>×1)</sub>, and <bold>Θ</bold><sup>+<italic>M</italic></sup> the same vector shifted downward by <italic>M</italic> positions, with the first <italic>M</italic> entries set to 0. One may then rewrite <inline-formula id="pcbi.1005542.e030"><alternatives><graphic id="pcbi.1005542.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in the form
<disp-formula id="pcbi.1005542.e031">
<alternatives>
<graphic id="pcbi.1005542.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">U</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mi mathvariant="bold">Θ</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mi mathvariant="bold">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>The <italic>MT×MT</italic> matrices <bold>U</bold><sub>{0…2}</sub> separate product terms that do not involve <italic>ϕ</italic>(<bold>z</bold>) (<bold>U</bold><sub>0</sub>), involve multiplication by <italic>ϕ</italic>(<bold>z</bold>) only from the left-hand or right-hand side (<bold>U</bold><sub>1</sub>), or from both sides (<bold>U</bold><sub>2</sub>). Likewise, for the terms linear in <bold>z</bold>, vector and matrix terms were separated that involved <italic>z</italic><sub><italic>mt</italic></sub> or <italic>θ</italic><sub><italic>m</italic></sub> conditional on <italic>z</italic><sub><italic>mt</italic></sub> &gt; <italic>θ</italic><sub><italic>m</italic></sub> (please see the provided MatLab code for the exact composition of these matrices). For now, the important point is that we have 2<sup><italic>M× T</italic></sup> different quadratic equations, depending on the bits on and off in the binary vector <bold>d</bold><sub>Ω</sub>. Consequently, to obtain the MAP estimator for <bold>z</bold>, in theory, one may consider all 2<sup><italic>M×T</italic></sup> different settings for <bold>d</bold><sub>Ω</sub>, for each solve the linear equations implied by <inline-formula id="pcbi.1005542.e032"><alternatives><graphic id="pcbi.1005542.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, and select among those for which the solution <bold>z</bold><sub>*</sub> is consistent with the considered set Ω (if one exists; see below) the one which produces the largest value <inline-formula id="pcbi.1005542.e033"><alternatives><graphic id="pcbi.1005542.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>In practice, this is generally not feasible. Various solution methods for piecewise linear equations have been suggested in the mathematical programming literature in the past [<xref ref-type="bibr" rid="pcbi.1005542.ref107">107</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref108">108</xref>]. For instance, some piecewise linear problems may be recast as a linear complementarity problem [<xref ref-type="bibr" rid="pcbi.1005542.ref109">109</xref>], but the pivoting methods often used to solve it work (numerically) well only for smaller scale settings [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>]. Here we therefore settled on a similar, simple Newton-type iteration scheme as proposed in [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>]. Specifically, if we denote by <bold>z</bold><sub>*</sub>(Ω) the solution to <xref ref-type="disp-formula" rid="pcbi.1005542.e031">Eq 7</xref> obtained with the set of constraints Ω active, the present scheme initializes with a random drawing of the {<italic>z</italic><sub><italic>mt</italic></sub>}, sets the components of <bold>d</bold><sub>Ω</sub> for which <italic>z</italic><sub><italic>mt</italic></sub> &gt; <italic>θ</italic><sub><italic>m</italic></sub> to 1 and all others to 0, and then keeps on alternating between (1) solving <inline-formula id="pcbi.1005542.e034"><alternatives><graphic id="pcbi.1005542.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for <bold>z</bold><sub>*</sub>(Ω) and (2) flipping the bits in <bold>d</bold><sub>Ω</sub> for which <inline-formula id="pcbi.1005542.e035"><alternatives><graphic id="pcbi.1005542.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>d</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>≠</mml:mo><mml:mi mathvariant="normal">sgn</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, that is, for which the components of the vector
<disp-formula id="pcbi.1005542.e036">
<alternatives>
<graphic id="pcbi.1005542.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e036" xlink:type="simple"/>
<mml:math display="block" id="M36">
<mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>≔</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∘</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
are positive, until the solution to <inline-formula id="pcbi.1005542.e037"><alternatives><graphic id="pcbi.1005542.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> is consistent with set Ω (i.e., <bold>c</bold> ≤ <bold>0</bold>).</p>
<p>For the problem as formulated in Brugnano &amp; Casulli [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>], these authors proved that such a solution always exists, and that the algorithm will always terminate after a finite (usually low) number of steps, given certain assumptions and provided the matrix that multiplies with the states <bold>z</bold> in <inline-formula id="pcbi.1005542.e038"><alternatives><graphic id="pcbi.1005542.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (i.e., the Hessian of <inline-formula id="pcbi.1005542.e039"><alternatives><graphic id="pcbi.1005542.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>), fulfills certain conditions (Stieltjes-type; see [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>] for details). This will usually <italic>not</italic> be the case for the present system; although the Hessian of <inline-formula id="pcbi.1005542.e040"><alternatives><graphic id="pcbi.1005542.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> will be symmetric and positive-definite (with proper parameter settings), its off-diagonal elements may be either larger or smaller than 0. Moreover, for the problem considered here, all elements of the Hessian in (<xref ref-type="disp-formula" rid="pcbi.1005542.e031">7</xref>) depend on Ω, while in [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>] this is only the case for the on-diagonal elements (i.e., in [<xref ref-type="bibr" rid="pcbi.1005542.ref049">49</xref>] <bold>D</bold><sub>Ω</sub> enters the Hessian only in additive, not multiplicative form as here). For these reasons, the Newton-type algorithm outlined above may not always converge to an exact solution (if one exists in this case) but may eventually cycle among non-solution configurations, or may not even always increase <italic>Q</italic>(<bold>Z</bold>) (i.e., <xref ref-type="disp-formula" rid="pcbi.1005542.e028">Eq 5</xref>!). To bypass this, the algorithm was always terminated if one of the following three conditions was met: (i) A solution to <inline-formula id="pcbi.1005542.e041"><alternatives><graphic id="pcbi.1005542.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> consistent with Ω was encountered; (ii) a previously probed set Ω was revisited; (iii) the constraint violation error defined by ‖<bold>c</bold><sub>+</sub>‖<sub>1</sub>, the <italic>l</italic><sub>1</sub> norm of the positive part of <bold>c</bold> defined in <xref ref-type="disp-formula" rid="pcbi.1005542.e036">Eq 8</xref>, went up beyond a pre-specified tolerance level (this is essentially a fast proxy for assessing the likelihood, intended to speed up iterations by using quantities already computed). With these modifications, we found that the algorithm would usually terminate after only a few iterations (&lt;10 for the examined toy examples) and yield approximate solutions with only a few constraints still violated (&lt;3% for the toy examples). As a caveat, unless condition (i) is met, this procedure implies that the returned solution may not even be locally optimal (in the strict mathematical sense–it would still be expected to live within an ‘elevated’ region of the optimization landscape defined by <italic>Q</italic>(<bold>Z</bold>)). On the other hand, since <italic>Q</italic>(<bold>Z</bold>) cannot keep on increasing along a closed cycle (it must ‘come back’), cycling implies there must be local maxima (or potentially saddles) located on the rims that separate different linear subspaces defined by <bold>d</bold><sub>Ω</sub>. Hence, for the elements <italic>k</italic> of <bold>z</bold> for which the constraints are still violated in the end, that is for which <italic>c</italic><sub><italic>k</italic></sub> &gt; 0 in <xref ref-type="disp-formula" rid="pcbi.1005542.e036">Eq 8</xref>, one may explicitly enforce the constraints by setting the violating states <bold>z</bold><sub>{<italic>k</italic>}</sub> = <bold>θ</bold><sub>{<italic>k</italic>}</sub>, then solve again for the remaining states <bold>z</bold><sub>{<italic>l</italic> ≠ <italic>k</italic>}</sub> (placing the solution on a ridge; or a quadratic program may be solved for the last step). Either way, it was found that even these approximate (and potentially not even locally optimal) solutions were generally (for the problems studied) in sufficiently good agreement with E(<bold>Z</bold>|<bold>X</bold>).</p>
<p>In the case of full EM iterations (with the parameters unknown as well), it appeared that flipping violated constraints in <bold>d</bold><sub>Ω</sub> one by one may often (for the scenarios studied here) improve overall performance, in the sense of yielding higher-likelihood solutions and less numerical problems (although it may leave more constraints violated in the end). Hence, this scheme was adopted here for the full EM, that is only the single bit <italic>k</italic><sup>*</sup> corresponding to the maximum element of vector <bold>c</bold> in <xref ref-type="disp-formula" rid="pcbi.1005542.e036">Eq 8</xref> was inverted on each iteration (the one with the largest wrong-side deviation from <bold>θ</bold>). In general, however, the resultant slow-down in the algorithm may not always be worth the performance gains; or a mixture of methods, with <inline-formula id="pcbi.1005542.e042"><alternatives><graphic id="pcbi.1005542.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>*</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>*</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mspace width="0.25em"/><mml:mtext>with</mml:mtext><mml:mspace width="0.25em"/><mml:msup><mml:mi>k</mml:mi><mml:mi>*</mml:mi></mml:msup><mml:mo>≔</mml:mo><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> early on, and <inline-formula id="pcbi.1005542.e043"><alternatives><graphic id="pcbi.1005542.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">d</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn mathvariant="bold">1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold">d</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>∀</mml:mo><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> during later iterations, may be considered.</p>
<p>Once a (local) maximum <bold>z</bold><sup>max</sup> (or approximation thereof) has been obtained, the covariances may be read off from the inverse negative Hessian at <bold>z</bold><sup>max</sup>, i.e. the elements of
<disp-formula id="pcbi.1005542.e044">
<alternatives>
<graphic id="pcbi.1005542.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e044" xlink:type="simple"/>
<mml:math display="block" id="M44">
<mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>≔</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">U</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">U</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi mathvariant="normal">Ω</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Note that this is a <italic>local</italic> estimate around the current maximizer <bold>z</bold><sup>max</sup> (i.e., oblivious to the discontinuities at the borders of the linear subspaces defined by <bold>d</bold><sub>Ω</sub>). We then use these covariance estimates to obtain (estimates of) E[<italic>ϕ</italic>(<bold>z</bold>)], E[<bold>z</bold><italic>ϕ</italic>(<bold>z</bold>)<sup><italic>T</italic></sup>], and E[<italic>ϕ</italic>(<bold>z</bold>)<italic>ϕ</italic>(<bold>z</bold>)<sup><italic>T</italic></sup>], as required for the maximization step. Denoting by <inline-formula id="pcbi.1005542.e045"><alternatives><graphic id="pcbi.1005542.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mi>λ</mml:mi><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> the complementary cumulative Gaussian, to ease subsequent derivations, let us introduce the following notation:
<disp-formula id="pcbi.1005542.e046">
<alternatives>
<graphic id="pcbi.1005542.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e046" xlink:type="simple"/>
<mml:math display="block" id="M46">
<mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≔</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≔</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≔</mml:mo><mml:mi mathvariant="normal">cov</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula></p>
<p>The elements of the expectancy vectors and matrices above are computed as
<disp-formula id="pcbi.1005542.e047">
<alternatives>
<graphic id="pcbi.1005542.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e047" xlink:type="simple"/>
<mml:math display="block" id="M47">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>The terms E[<italic>ϕ</italic>(<italic>z</italic><sub><italic>k</italic></sub>)<italic>ϕ</italic>(<italic>z</italic><sub><italic>l</italic></sub>)], for <italic>k</italic> ≠ <italic>l</italic>, are more tedious, and cannot be (to my knowledge and insight) computed exactly (analytically), so we develop them in a bit more detail here:
<disp-formula id="pcbi.1005542.e048">
<alternatives>
<graphic id="pcbi.1005542.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e048" xlink:type="simple"/>
<mml:math display="block" id="M48">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="6em"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="7em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula></p>
<p>The last term is just a (complementary) cumulative bivariate Gaussian evaluated with parameters specified through the approximate MAP solution (<bold>z</bold><sup>max</sup>, <bold>V</bold>) (and multiplied by the thresholds). The first term we may rewrite as follows:
<disp-formula id="pcbi.1005542.e049">
<alternatives>
<graphic id="pcbi.1005542.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e049" xlink:type="simple"/>
<mml:math display="block" id="M49">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="11.5em"/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>where</mml:mtext><mml:mspace width="2em"/><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="5em"/><mml:msub><mml:mi>λ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>≔</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="5em"/><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p>These are just standard results one can derive by the reverse chain rule for integration, with the <italic>λ</italic>’s the elements of the inverse bivariate (<italic>k</italic>,<italic>l</italic>)-covariance matrix. Note that if the variable <italic>z</italic><sub><italic>k</italic></sub> were removed from the first integrand in <xref ref-type="disp-formula" rid="pcbi.1005542.e049">Eq 13</xref>, i.e. as in the second term in <xref ref-type="disp-formula" rid="pcbi.1005542.e048">Eq 12</xref>, all terms in <xref ref-type="disp-formula" rid="pcbi.1005542.e049">Eq 13</xref> would just come down to uni- or bivariate Gaussians (times some factor) or a univariate Gaussian expectancy value, respectively. Noting this, one obtains for the second (and correspondingly for the third) term in <xref ref-type="disp-formula" rid="pcbi.1005542.e048">Eq 12</xref>:
<disp-formula id="pcbi.1005542.e050">
<alternatives>
<graphic id="pcbi.1005542.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e050" xlink:type="simple"/>
<mml:math display="block" id="M50">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="3em"/><mml:mtext>with</mml:mtext><mml:mspace width="0.25em"/><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula></p>
<p>The problematic bit is the product term <inline-formula id="pcbi.1005542.e051"><alternatives><graphic id="pcbi.1005542.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1005542.e049">Eq 13</xref>, which we resolve by making the approximation <inline-formula id="pcbi.1005542.e052"><alternatives><graphic id="pcbi.1005542.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. This way we have for the first term in <xref ref-type="disp-formula" rid="pcbi.1005542.e048">Eq 12</xref>:
<disp-formula id="pcbi.1005542.e053">
<alternatives>
<graphic id="pcbi.1005542.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e053" xlink:type="simple"/>
<mml:math display="block" id="M53">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="12em"/><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula></p>
<p>Putting (<xref ref-type="disp-formula" rid="pcbi.1005542.e049">13</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1005542.e053">15</xref>) together with the bivariate cumulative Gaussian yields an analytical approximation to <xref ref-type="disp-formula" rid="pcbi.1005542.e048">Eq 12</xref> that can be computed based on the quantities obtained from the approximate MAP estimate (<bold>z</bold><sup>max</sup>, <bold>V</bold>).</p>
</sec>
<sec id="sec017">
<title>Expectation-maximization algorithm: Parameter estimation</title>
<p>Once we have estimates for E[<bold>z</bold>], E[<bold>zz</bold><sup><italic>T</italic></sup>], E[<italic>ϕ</italic>(<bold>z</bold>)], E[<bold>z</bold><italic>ϕ</italic>(<bold>z</bold>)<sup><italic>T</italic></sup>], and E[<italic>ϕ</italic>(<bold>z</bold>)<italic>ϕ</italic>(<bold>z</bold>)<sup><italic>T</italic></sup>], the maximization step is standard and straightforward, so for convenience we just state the results here, using the notation
<disp-formula id="pcbi.1005542.e054">
<alternatives>
<graphic id="pcbi.1005542.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e054" xlink:type="simple"/>
<mml:math display="block" id="M54">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1.5em"/><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.5em"/><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1.5em"/><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>ϕ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>ϕ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mspace width="1em"/><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>≔</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula></p>
<p>With these expectancy sums defined, one has
<disp-formula id="pcbi.1005542.e055">
<alternatives>
<graphic id="pcbi.1005542.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e055" xlink:type="simple"/>
<mml:math display="block" id="M55">
<mml:mrow><mml:mi mathvariant="bold">B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow>
</mml:math>
</alternatives>
<label>(17A)</label>
</disp-formula>
<disp-formula id="pcbi.1005542.e056">
<alternatives>
<graphic id="pcbi.1005542.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e056" xlink:type="simple"/>
<mml:math display="block" id="M56">
<mml:mrow><mml:mi mathvariant="bold">Γ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="bold">B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(17B)</label>
</disp-formula>
<disp-formula id="pcbi.1005542.e057">
<alternatives>
<graphic id="pcbi.1005542.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e057" xlink:type="simple"/>
<mml:math display="block" id="M57">
<mml:mrow><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(17C)</label>
</disp-formula>
<disp-formula id="pcbi.1005542.e058">
<alternatives>
<graphic id="pcbi.1005542.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e058" xlink:type="simple"/>
<mml:math display="block" id="M58">
<mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(17D)</label>
</disp-formula>
<disp-formula id="pcbi.1005542.e059">
<alternatives>
<graphic id="pcbi.1005542.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e059" xlink:type="simple"/>
<mml:math display="block" id="M59">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="normal">var</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold">μ</mml:mi><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>5</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.5em"/><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>4</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mn>5</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="bold">A</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(17E)</label>
</disp-formula></p>
<p>Note that to avoid redundancy in the parameters, here we usually fixed <bold>Σ</bold> = <bold>I</bold> ⋅ 10<sup>−2</sup> (for the toy models) or <bold>Σ</bold> = <bold>I</bold> (for the experimental data).</p>
<p>For <bold>W</bold>, since we assumed this matrix to have an off-diagonal structure (i.e., with zeros on the diagonal), we solve for each row of <bold>W</bold> separately:
<disp-formula id="pcbi.1005542.e060">
<alternatives>
<graphic id="pcbi.1005542.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e060" xlink:type="simple"/>
<mml:math display="block" id="M60">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>≔</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>≔</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∀</mml:mo><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo><mml:mo>\</mml:mo><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo><mml:mo>\</mml:mo><mml:mtext>m</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>•</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>•</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo><mml:mo>\</mml:mo><mml:mtext>m</mml:mtext><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo><mml:mo>\</mml:mo><mml:mtext>m</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(17F)</label>
</disp-formula>
where the subscripts indicate the matrix elements to be pulled out, with the subscript dot denoting all elements of the corresponding column or row (e.g., ‘•m’ takes the <italic>m</italic><sup>th</sup> column of that matrix). Should matrices <bold>Γ</bold>, <bold>Σ</bold>, <bold>W</bold> of full form be desired, the derivations simplify a bit–in essence, the diagonal operator ‘∘<bold>I</bold>‘ in the equations above (except <xref ref-type="disp-formula" rid="pcbi.1005542.e058">Eq 17D</xref>) would have to be omitted, and <xref ref-type="disp-formula" rid="pcbi.1005542.e060">Eq 17F</xref> could be solved in full matrix form (instead of row-wise). An expression for input scaling matrix <bold>C</bold> (cf. <xref ref-type="disp-formula" rid="pcbi.1005542.e001">Eq 1</xref>) is given by <inline-formula id="pcbi.1005542.e061"><alternatives><graphic id="pcbi.1005542.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>4</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mn>3</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, but note that <bold>C</bold> would also show up in (<xref ref-type="disp-formula" rid="pcbi.1005542.e057">17C</xref>)–(<xref ref-type="disp-formula" rid="pcbi.1005542.e060">17F</xref>) (multiplying with <bold>s</bold><sub><italic>t</italic></sub> everywhere), as well as in the state inference equations; matrices <bold>A</bold>, <bold>W</bold>, and <bold>C</bold> would therefore need to be solved for simultaneously in this case (complicating the above expressions a bit; see provided MatLab code for full details).</p>
<p>Starting from a number of different random parameter initializations, the E- and M-steps are alternated until the log-likelihood ratio falls below a predefined tolerance level (while still increasing) or a preset maximum number of allowed iterations are exceeded. For reasons mentioned in the Results, sometimes it can actually happen that the log-likelihood ratio temporarily decreases, in which case the iterations are continued. If (<italic>N</italic>−<italic>M</italic>)<sup>2</sup> ≥ <italic>N</italic> + <italic>M</italic>, factor analysis may be used to derive initial estimates for the latent states and observation parameters in (<xref ref-type="disp-formula" rid="pcbi.1005542.e004">3</xref>) [<xref ref-type="bibr" rid="pcbi.1005542.ref027">27</xref>], although this was not attempted here. Another possibility is to improve initial estimates first through the much faster, corresponding LDS, before submitting them to full PLRNN estimation. For further implementational details see the MatLab code provided on GitHub (repository ‘PLRNNstsp’).</p>
</sec>
<sec id="sec018">
<title>Particle filter</title>
<p>To validate the approximations from our semi-analytical procedure developed above, a bootstrap particle filter as given in [<xref ref-type="bibr" rid="pcbi.1005542.ref026">26</xref>] was implemented. In bootstrap particle filtering, the state posterior distribution at time <italic>t</italic>,
<disp-formula id="pcbi.1005542.e062">
<alternatives>
<graphic id="pcbi.1005542.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e062" xlink:type="simple"/>
<mml:math display="block" id="M62">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="7em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:munder><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:msub><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
is numerically approximated through a set of ‘particles’ (samples) <inline-formula id="pcbi.1005542.e063"><alternatives><graphic id="pcbi.1005542.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, drawn from <italic>p</italic><sub>Ξ</sub>(<bold>z</bold><sub><italic>t</italic></sub> | <bold>x</bold><sub>1</sub>,…,<bold>x</bold><sub><italic>t</italic>−1</sub>), together with a set of normalized weights <inline-formula id="pcbi.1005542.e064"><alternatives><graphic id="pcbi.1005542.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1005542.e065"><alternatives><graphic id="pcbi.1005542.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>≔</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Based on this representation, moments of <italic>p</italic><sub>Ξ</sub>(<bold>z</bold><sub><italic>t</italic></sub> | <bold>x</bold><sub>1:<italic>t</italic></sub>) and <italic>p</italic><sub>Ξ</sub>(<italic>ϕ</italic>(<bold>z</bold><sub><italic>t</italic></sub>) | <bold>x</bold><sub>1:<italic>t</italic></sub>) can be easily obtained by evaluating <italic>ϕ</italic> (or any other function of <bold>z</bold>) on the set of samples <inline-formula id="pcbi.1005542.e066"><alternatives><graphic id="pcbi.1005542.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and summing the outcomes weighted with their respective normalized observation likelihoods <inline-formula id="pcbi.1005542.e067"><alternatives><graphic id="pcbi.1005542.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. A new set of samples <inline-formula id="pcbi.1005542.e068"><alternatives><graphic id="pcbi.1005542.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for <italic>t</italic>+1 is then generated by first drawing <italic>K</italic> times from <inline-formula id="pcbi.1005542.e069"><alternatives><graphic id="pcbi.1005542.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with replacement according to the weights <inline-formula id="pcbi.1005542.e070"><alternatives><graphic id="pcbi.1005542.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and then drawing <italic>K</italic> new samples according to the transition probabilities <inline-formula id="pcbi.1005542.e071"><alternatives><graphic id="pcbi.1005542.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1005542.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi mathvariant="bold">Ξ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi mathvariant="bold">z</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (thus approximating the integral in <xref ref-type="disp-formula" rid="pcbi.1005542.e062">Eq 18</xref>). Here we used <italic>K</italic> = 10<sup>4</sup> samples. Note that this numerical sampling scheme, like a Kalman filter, but unlike the procedure outlined above, only implements the filtering step (i.e., yields <italic>p</italic><sub>Ξ</sub>(<bold>z</bold><sub><italic>t</italic></sub> | <bold>x</bold><sub>1:<italic>t</italic></sub>), not <italic>p</italic><sub>Ξ</sub>(<bold>z</bold><sub><italic>t</italic></sub> | <bold>x</bold><sub>1:<italic>T</italic></sub>)). On the other hand, it gives (weakly) consistent (asymptotically unbiased; [<xref ref-type="bibr" rid="pcbi.1005542.ref110">110</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref111">111</xref>]) estimates of all expectancies across this distribution, that is, it does not rely on the type of approximations and locally optimal solutions of our semi-analytical approach that almost inevitably will come with some bias (since, among other factors, the local or approximate mode would usually deviate from the mean by some amount for the present model).</p>
</sec>
<sec id="sec019">
<title>Experimental data sets</title>
<p>Details of the experimental task and electrophysiological data sets used here could be found in [<xref ref-type="bibr" rid="pcbi.1005542.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1005542.ref112">112</xref>]. Briefly, rats had to alternate between left and right lever presses in a Skinner box to obtain a food reward dispensed on correct choices, with a ≥ 10 s delay enforced between consecutive lever presses. While the levers were located on one side of the Skinner box, animals had to perform a nosepoke on the opposite side of the box in between lever presses for initiating the delay period, to discourage them from developing an external coding strategy (e.g., through maintenance of body posture during the delay). While animals were performing the task, multiple single units were recorded with a set of 16 tetrodes implanted bilaterally into the anterior cingulate cortex (ACC, a subdivision of rat prefrontal cortex). For the present analyses, a data set from only one of the four rats recorded on this task was selected for the present exemplary purposes, namely the one where the clearest single unit traces of delay activity were observed in the first place. This data set consisted of 30 simultaneously recorded units, of which the 19 units with spiking rates &gt;1 Hz were retained, on 14 correct trials (only correct response trials were analyzed). The trials had variable length, but were all cut down to the same length of 14 s, including 2 s of pre-nosepoke, 5 s extending into the delay from the nosepoke, 5 s preceding the next lever press, and 2 s of post-response phase (note that this may imply temporal gaps in the middle of the delay on some trials, which were ignored here for convenience). All spike trains were convolved with Gaussian kernels (see, e.g., [<xref ref-type="bibr" rid="pcbi.1005542.ref012">12</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref057">57</xref>; <xref ref-type="bibr" rid="pcbi.1005542.ref112">112</xref>]), with the kernel standard deviation set individually for each unit to one half of its mean interspike-interval. Note that this also brings the observed series into tighter agreement with the Gaussian assumptions of the observation model, <xref ref-type="disp-formula" rid="pcbi.1005542.e004">Eq 3</xref>. Finally, the spike time series were binned into 500 ms bins (corresponding roughly to the inverse of the overall (across all 30 recorded cells) average neural firing rates of ≈2.2 Hz), which resulted in 14 trials of 28 time bins each submitted to the estimation process. As indicated in the section ‘<italic>State space model</italic>’, a trial-unique initial state mean <bold>μ</bold><sub><italic>k</italic></sub>, <italic>k</italic> = 1…14, was assumed for each of the 14 temporally segregated trials.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>I thank Dr. Georgia Koppe for her feedback on this manuscript, Dr. Hazem Toutounji for providing the Matlab routine for efficient inversion of block-tridiagonal matrices, and for discussion of related issues, and Drs. James Hyman and Jeremy Seamans for lending me their in-vivo electrophysiological recordings from rat ACC as an analysis testbed.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1005542.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>. <article-title>Self-organizing neural integrator predicts interval times through climbing activity</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>: <fpage>5342</fpage>–<lpage>5353</lpage>. <object-id pub-id-type="pmid">12832560</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>36</volume>: <fpage>955</fpage>–<lpage>968</lpage>. <object-id pub-id-type="pmid">12467598</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name>. <source>Dynamical Systems in Neuroscience</source>. <year>2007</year>; <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname> <given-names>VS</given-names></name>. <article-title>Transient cognitive dynamics, metastability, and decision making</article-title>. <source>PLoS Comput Biol</source>. <year>2008</year>;<volume>2</volume>: <fpage>e1000072</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Simoncelli</surname> <given-names>EP</given-names></name>. <article-title>A model-based spike sorting algorithm for removing correlation artifacts in multi-neuron recordings</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>: <fpage>e62123</fpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0062123" xlink:type="simple">10.1371/journal.pone.0062123</ext-link></comment> <object-id pub-id-type="pmid">23671583</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevens</surname> <given-names>CF</given-names></name>. <article-title>Neurotransmitter release at central synapses</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>40</volume>: <fpage>381</fpage>–<lpage>388</lpage>. <object-id pub-id-type="pmid">14556715</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>1, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nat Neurosci</source> <year>2013</year>,<fpage>1170</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3495" xlink:type="simple">10.1038/nn.3495</ext-link></comment> <object-id pub-id-type="pmid">23955561</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orbán</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <article-title>Representations of uncertainty in sensorimotor control</article-title>. <source>Curr Opin Neurobiol</source> <year>2011</year>, <fpage>629</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2011.05.026" xlink:type="simple">10.1016/j.conb.2011.05.026</ext-link></comment> <object-id pub-id-type="pmid">21689923</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname> <given-names>KP</given-names></name>, <name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <article-title>Bayesian integration in sensorimotor learning</article-title>. <source>Nature</source> <year>2004</year>, <volume>427</volume>: <fpage>244</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature02169" xlink:type="simple">10.1038/nature02169</ext-link></comment> <object-id pub-id-type="pmid">14724638</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <article-title>A few important points about dopamine’s role in neural network dynamics</article-title>. <source>Pharmacopsychiatry</source> <year>2006</year>, <volume>39</volume> <issue>Suppl 1</issue>: <fpage>S72</fpage>–<lpage>S75</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lapish</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>. <article-title>Attractor Dynamics of Cortical Populations During Memory-Guided Decision-Making</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>: <fpage>e1002057</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002057" xlink:type="simple">10.1371/journal.pcbi.1002057</ext-link></comment> <object-id pub-id-type="pmid">21625577</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lapish</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Phillips</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>. <article-title>Amphetamine Exerts Dose-Dependent Changes in Prefrontal Cortex Attractor Dynamics during Working Memory</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>: <fpage>10172</fpage>–<lpage>10187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2421-14.2015" xlink:type="simple">10.1523/JNEUROSCI.2421-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26180194</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunton</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Proctor</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Kutz</surname> <given-names>JN</given-names></name>. <article-title>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2016</year>;<volume>113</volume>: <fpage>3932</fpage>–<lpage>3937</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1517384113" xlink:type="simple">10.1073/pnas.1517384113</ext-link></comment> <object-id pub-id-type="pmid">27035946</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wood</surname> <given-names>SN</given-names></name>. <article-title>Statistical inference for noisy nonlinear ecological dynamic systems</article-title>. <source>Nature</source>. <year>2010</year>; <volume>466</volume>: <fpage>1102</fpage>–<lpage>1104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09319" xlink:type="simple">10.1038/nature09319</ext-link></comment> <object-id pub-id-type="pmid">20703226</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>EN</given-names></name>. <article-title>Estimating a state-space model from point process observations</article-title>. <source>Neural Comput</source>. <year>2003</year>;<volume>15</volume>: <fpage>965</fpage>–<lpage>991</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603765202622" xlink:type="simple">10.1162/089976603765202622</ext-link></comment> <object-id pub-id-type="pmid">12803953</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ferreira</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Koyama</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Rahnama</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Vidne</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A new look at state-space models for neural data</article-title>. <source>J Comput Neurosci</source>. <year>2010</year>;<volume>29</volume>: <fpage>107</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-009-0179-x" xlink:type="simple">10.1007/s10827-009-0179-x</ext-link></comment> <object-id pub-id-type="pmid">19649698</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Vidne</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>DePasquale</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ferreira</surname> <given-names>DG</given-names></name>. <article-title>Inferring synaptic inputs given a noisy voltage trace via sequential Monte Carlo methods</article-title>. <source>J Comput Neurosci</source>. <year>2012</year>;<volume>33</volume>: <fpage>1</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10827-011-0371-7" xlink:type="simple">10.1007/s10827-011-0371-7</ext-link></comment> <object-id pub-id-type="pmid">22089473</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source>. <year>2008</year>;<volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07140" xlink:type="simple">10.1038/nature07140</ext-link></comment> <object-id pub-id-type="pmid">18650810</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Ahmadian</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Model-based decoding, information estimation, and change-point detection techniques for multineuron spike trains</article-title>. <source>Neural Comput</source>. <year>2011</year>;<volume>23</volume>: <fpage>1</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00058" xlink:type="simple">10.1162/NECO_a_00058</ext-link></comment> <object-id pub-id-type="pmid">20964538</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>. <article-title>Learning stable, regularised latent models of neural population dynamics</article-title>. <source>Network</source>. <year>2012</year>;<volume>23</volume>: <fpage>24</fpage>–<lpage>47</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3109/0954898X.2012.677095" xlink:type="simple">10.3109/0954898X.2012.677095</ext-link></comment> <object-id pub-id-type="pmid">22663075</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Latimer</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Yates</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Meister</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Huk</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Pillow</surname> <given-names>JW</given-names></name>. <article-title>Single-trial spike trains in parietal cortex reveal discrete steps during decision-making</article-title>. <source>Science</source>. <year>2015</year>;<volume>349</volume>: <fpage>184</fpage>–<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aaa4056" xlink:type="simple">10.1126/science.aaa4056</ext-link></comment> <object-id pub-id-type="pmid">26160947</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref022"><label>22</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Buesing</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>. <chapter-title>Estimating State and Parameters in State Space Models of Spike Trains</chapter-title>. In: <name name-style="western"><surname>Chen</surname> <given-names>Z</given-names></name> editor. <source>Advanced State Space Methods for Neural and Clinical Data</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>University Press</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Afshar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>. <article-title>Extracting Dynamical Structure Embedded in Neural Activity</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2005</year>;<volume>18</volume>: <fpage>1545</fpage>–<lpage>1552</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Kemere</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Afshar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ryu SI Meng</surname> <given-names>TH</given-names></name>, <etal>et al</etal>. <article-title>Mixture of trajectory models for neural decoding of goal-directed movements</article-title>. <source>J Neurophysiol</source>. <year>2007</year>;<volume>5</volume>: <fpage>3763</fpage>–<lpage>3780</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Sahani</surname> <given-names>M</given-names></name>. <article-title>Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity</article-title>. <source>J Neurophysiol</source>. <year>2009</year>;<volume>102</volume>: <fpage>614</fpage>–<lpage>635</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.90941.2008" xlink:type="simple">10.1152/jn.90941.2008</ext-link></comment> <object-id pub-id-type="pmid">19357332</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref026"><label>26</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Durbin</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Koopman</surname> <given-names>SJ</given-names></name>. <source>Time Series Analysis by State Space Methods</source>. <publisher-name>Oxford Statistical Science</publisher-name>; <year>2012</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Roweis</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Ghahramani</surname> <given-names>Z</given-names></name>. <chapter-title>An EM algorithm for identification of nonlinear dynamical systems</chapter-title>. In: <name name-style="western"><surname>Haykin</surname> <given-names>S</given-names></name>, editor. <publisher-name>Kalman Filtering and Neural Networks</publisher-name>; <year>2001</year></mixed-citation></ref>
<ref id="pcbi.1005542.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cereb Cortex</source>. <year>1997</year>;<volume>7</volume>: <fpage>237</fpage>–<lpage>252</lpage>. <object-id pub-id-type="pmid">9143444</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Neurocomputational models of working memory</article-title>. <source>Nat Neurosci</source>. <year>2000</year>;<volume>3</volume>; <issue>Suppl</issue>: <fpage>1184</fpage>–<lpage>1191</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>. <article-title>Implications of synaptic biophysics for recurrent network dynamics and active memory</article-title>. <source>Neural Netw</source>. <year>2009</year>;<volume>22</volume>: <fpage>1189</fpage>–<lpage>1200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2009.07.016" xlink:type="simple">10.1016/j.neunet.2009.07.016</ext-link></comment> <object-id pub-id-type="pmid">19647396</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Effects of neuromodulation in a cortical network model of object working memory dominated by recurrent inhibition</article-title>. <source>J Comput Neurosci</source>. <year>2001</year>;<volume>11</volume>: <fpage>63</fpage>–<lpage>85</lpage>. <object-id pub-id-type="pmid">11524578</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Synaptic basis of cortical persistent activity: the importance of NMDA receptors to working memory</article-title>. <source>J Neurosci</source>. <year>1999</year>;<volume>19</volume>: <fpage>9587</fpage>–<lpage>9603</lpage>. <object-id pub-id-type="pmid">10531461</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gabriel</surname> <given-names>T</given-names></name>. <article-title>Dynamical basis of irregular spiking in NMDA-driven prefrontal cortex neurons</article-title>. <source>Cereb Cortex</source>. <year>2007</year>;<volume>17</volume>: <fpage>894</fpage>–<lpage>908</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhk044" xlink:type="simple">10.1093/cercor/bhk044</ext-link></comment> <object-id pub-id-type="pmid">16740581</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Funahashi</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Nakamura</surname> <given-names>Y</given-names></name>. <article-title>Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks</article-title>. <source>Neural Netw</source>. <year>1993</year>;<volume>6</volume>: <fpage>801</fpage>–<lpage>806</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nakano</surname> <given-names>R</given-names></name>. <article-title>Learning dynamical systems by recurrent neural networks from orbits</article-title>. <source>Neural Netw</source>. <year>1998</year>;<volume>11</volume>: <fpage>1589</fpage>–<lpage>1599</lpage>. <object-id pub-id-type="pmid">12662730</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chow</surname> <given-names>TWS</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>XD</given-names></name>. <article-title>Modeling of Continuous Time Dynamical Systems with Input by Recurrent Neural Networks</article-title>. <source>Trans Circuits Syst I Fundam Theory Theory Appl</source>. <year>2000</year>;<volume>47</volume>: <fpage>575</fpage>–<lpage>578</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>: <fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mnih</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kavukcuoglu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rusu</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Veness</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bellemare</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>: <fpage>529</fpage>–<lpage>533</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14236" xlink:type="simple">10.1038/nature14236</ext-link></comment> <object-id pub-id-type="pmid">25719670</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Deep learning in neural networks</article-title>. <source>Neural Networks</source> <year>2015</year>, <volume>61</volume>:<fpage>85</fpage>–<lpage>117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2014.09.003" xlink:type="simple">10.1016/j.neunet.2014.09.003</ext-link></comment> <object-id pub-id-type="pmid">25462637</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>. <year>1997</year>;<volume>9</volume>: <fpage>1735</fpage>–<lpage>1780</lpage>. <object-id pub-id-type="pmid">9377276</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyman</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Whitman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Emberly</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Woodward</surname> <given-names>TS</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>. <article-title>Action and outcome activity state patterns in the anterior cingulate cortex</article-title>. <source>Cereb Cortex</source>. <year>2013</year>;<volume>23</volume>: <fpage>1257</fpage>–<lpage>1268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhs104" xlink:type="simple">10.1093/cercor/bhs104</ext-link></comment> <object-id pub-id-type="pmid">22617853</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source>. <year>2009</year>; <volume>63</volume>: <fpage>544</fpage>–<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework</article-title>. <source>PLoS Comput Biol</source> <volume>12</volume>, <fpage>e1004792</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004792" xlink:type="simple">10.1371/journal.pcbi.1004792</ext-link></comment> <object-id pub-id-type="pmid">26928718</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Fan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yao</surname> <given-names>Q</given-names></name>. <source>Nonlinear Time Series: Nonparametric and Parametric Methods</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>. <year>2003</year></mixed-citation></ref>
<ref id="pcbi.1005542.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>. <chapter-title>The elements of statistical learning</chapter-title> (Vol. <volume>2</volume>, No. 1) <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc> <year>2009</year></mixed-citation></ref>
<ref id="pcbi.1005542.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Park M, Bohner G, Macke J. Unlocking neural population non-stationarity using a hierarchical dynamics model In: Advances in Neural Information Processing Systems 28, Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS 2015); 2016. pp.1-9.</mixed-citation></ref>
<ref id="pcbi.1005542.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beer</surname> <given-names>RD</given-names></name>. <article-title>Parameter Space Structure of Continuous-Time Recurrent Neural Networks</article-title>. <source>Neural Computation</source> <year>2006</year>;<volume>18</volume>: <fpage>3009</fpage>–<lpage>3051</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2006.18.12.3009" xlink:type="simple">10.1162/neco.2006.18.12.3009</ext-link></comment> <object-id pub-id-type="pmid">17052157</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koyama</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Pérez-Bolde</surname> <given-names>L.C.</given-names></name>, <name name-style="western"><surname>Shalizi</surname> <given-names>C.R.</given-names></name>, <name name-style="western"><surname>Kass</surname> <given-names>R.E.</given-names></name>: <article-title>Approximate Methods for State-Space Models</article-title>. <source>J. Am. Stat. Assoc.</source> <year>2010</year>; <volume>105</volume>: <fpage>170</fpage>–<lpage>180</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1198/jasa.2009.tm08326" xlink:type="simple">10.1198/jasa.2009.tm08326</ext-link></comment> <object-id pub-id-type="pmid">21753862</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brugnano</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Casulli</surname> <given-names>V</given-names></name>. <article-title>Iterative solution of piecewise linear systems</article-title>. <source>SIAM J Sci Comput</source>. <year>2008</year>;<volume>30</volume>: <fpage>463</fpage>–<lpage>472</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>. <article-title>A learning algorithm for continually running fully recurrent neural networks</article-title>. <source>Neural Computat</source>. <year>1990</year>;<volume>1</volume>: <fpage>256</fpage>–<lpage>263</lpage></mixed-citation></ref>
<ref id="pcbi.1005542.ref051"><label>51</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>RG</given-names></name>. <chapter-title>Introduction to the theory of neural computation</chapter-title>. <year>1991</year>; <publisher-name>Addison-Wesley Pub Co</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname> <given-names>A</given-names></name>. <article-title>A General Linear Non-Gaussian State-Space Model: Identifiability, Identification, and Applications</article-title>. <source>JMLR: Workshop and Conference Proceedings</source> <year>2011</year>;<volume>20</volume>: <fpage>113</fpage>–<lpage>128</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Auger-Méthé</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Field</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Albertsen</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Derocher</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Jonsen</surname> <given-names>ID</given-names></name>, <etal>et al</etal>. <article-title>State-space models’ dirty little secrets: even simple linear Gaussian models can have estimation problems</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>: <fpage>26677</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep26677" xlink:type="simple">10.1038/srep26677</ext-link></comment> <object-id pub-id-type="pmid">27220686</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>CFJ</given-names></name>. <article-title>On the Convergence Properties of the EM Algorithm</article-title>. <source>Ann Statist</source>. <year>1983</year>;<volume>11</volume>: <fpage>95</fpage>–<lpage>103</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boutayeb</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rafaralahy</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Darouach</surname> <given-names>M</given-names></name>. <article-title>Convergence analysis of the extended Kalman filter used as an observer for nonlinear deterministic discrete-time systems</article-title>. <source>IEEE Trans Autom Control</source>. <year>1997</year>;<volume>42</volume>: <fpage>581</fpage>–<lpage>586</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref056"><label>56</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Megiddo</surname></name>. <source>Advances in Economic Theory. Fifth World Congress</source>. Edited by <name name-style="western"><surname>Bewley</surname> <given-names>Truman F</given-names></name>. <publisher-name>Cambridge University Press</publisher-name> <year>1987</year></mixed-citation></ref>
<ref id="pcbi.1005542.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Vittoz</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Floresco</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name> (<year>2010</year>) <article-title>Abrupt transitions between prefrontal neural ensemble states accompany behavioral transitions during rule learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>438</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.03.029" xlink:type="simple">10.1016/j.neuron.2010.03.029</ext-link></comment> <object-id pub-id-type="pmid">20471356</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shimazaki</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shinomoto</surname> <given-names>S</given-names></name>. <article-title>Kernel Bandwidth Optimization in Spike Rate Estimation</article-title>. <source>J Comp Neurosci</source>. <year>2010</year>;<volume>29</volume>: <fpage>171</fpage>–<lpage>182</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Janson</surname></name> <etal>et al</etal>. <article-title>Effective Degrees of Freedom: A Flawed Metaphor. Lucas Janson, Will Fithian, Trevor Hastie</article-title>. <source>Biometrika</source>. <year>2015</year>, <volume>102</volume>: <fpage>479</fpage>–<lpage>485</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/asv019" xlink:type="simple">10.1093/biomet/asv019</ext-link></comment> <object-id pub-id-type="pmid">26977114</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyman</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Balaguer‐Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>. <article-title>Contextual encoding by ensembles of medial prefrontal cortex neurons</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>, <volume>109</volume>: <fpage>5086</fpage>–<lpage>5091</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1114415109" xlink:type="simple">10.1073/pnas.1114415109</ext-link></comment> <object-id pub-id-type="pmid">22421138</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Nirenberg</surname> <given-names>S</given-names></name>. <article-title>Computing and stability in cortical networks</article-title>. <source>Neural Comput</source>. <year>2004</year>;<volume>16</volume>: <fpage>1385</fpage>–<lpage>1412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976604323057434" xlink:type="simple">10.1162/089976604323057434</ext-link></comment> <object-id pub-id-type="pmid">15165395</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>. <article-title>Beyond bistability: biophysics and temporal dynamics of working memory</article-title>. <source>Neuroscience</source>. <year>2006</year>;<volume>139</volume>: <fpage>119</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroscience.2005.06.094" xlink:type="simple">10.1016/j.neuroscience.2005.06.094</ext-link></comment> <object-id pub-id-type="pmid">16326020</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>W</given-names></name>. <article-title>Dynamic causal modelling</article-title>. <source>Neuroimage</source>. <year>2003</year>;<volume>19</volume>: <fpage>1273</fpage>–<lpage>1302</lpage>. <object-id pub-id-type="pmid">12948688</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walter</surname> <given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Pronzato</surname> <given-names>L.</given-names></name> (<year>1996</year>). <article-title>On the identifiability and distinguishability of nonlinear parametric models</article-title>. <source>Mathematics and Computers in Simulation</source>, <volume>42</volume>(<issue>2–3</issue>), <fpage>125</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref065"><label>65</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Strogatz</surname> <given-names>SH</given-names></name>. <chapter-title>Nonlinear dynamics and chaos</chapter-title>. <publisher-name>Addison-Wesley Publ</publisher-name>; <year>1994</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kelc</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Güntürkün</surname> <given-names>O</given-names></name>. <article-title>A neurocomputational theory of the dopaminergic modulation of working memory functions</article-title>. <source>J Neurosci</source>. <year>1999</year>;<volume>19</volume>: <fpage>207</fpage>–<lpage>222</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons</article-title>. <source>J Comput Neurosci</source>. <year>2000</year>;<volume>8</volume>: <fpage>183</fpage>–<lpage>208</lpage>. <object-id pub-id-type="pmid">10809012</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>: <fpage>78</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12742" xlink:type="simple">10.1038/nature12742</ext-link></comment> <object-id pub-id-type="pmid">24201281</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hertäg</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Analytical approximations of the firing rate of an adaptive exponential integrate-and-fire neuron in the presence of synaptic noise</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>;<volume>8</volume>: <fpage>116</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00116" xlink:type="simple">10.3389/fncom.2014.00116</ext-link></comment> <object-id pub-id-type="pmid">25278872</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>. <article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title>. <source>Neural Comput</source>. <year>2013</year>;<volume>25</volume>: <fpage>626</fpage>–<lpage>649</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00409" xlink:type="simple">10.1162/NECO_a_00409</ext-link></comment> <object-id pub-id-type="pmid">23272922</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref071"><label>71</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Takens</surname> <given-names>F</given-names></name>. <source>Detecting strange attractors in turbulence. Lecture Notes in Mathematics</source> <volume>898</volume>. <publisher-name>Springer</publisher-name> <publisher-loc>Berlin</publisher-loc>;<year>1981</year>: <fpage>366</fpage>–<lpage>381</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauer</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Sauer</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Davies</surname> <given-names>DG</given-names></name>. <article-title>Embedology</article-title>. <source>J Stat Phys</source>. <year>1991</year>;<volume>65</volume>: <fpage>579</fpage>–<lpage>616</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauer</surname> <given-names>T</given-names></name>. <article-title>Reconstruction of dynamical systems from interspike intervals</article-title>. <source>Phys Rev Lett</source>. <year>1994</year>;<volume>72</volume>: <fpage>3811</fpage>–<lpage>3814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.72.3811" xlink:type="simple">10.1103/PhysRevLett.72.3811</ext-link></comment> <object-id pub-id-type="pmid">10056303</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>So</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Francis</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Netoff</surname> <given-names>TI</given-names></name>, <name name-style="western"><surname>Gluckman</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Schiff</surname> <given-names>SJ</given-names></name>. <article-title>Periodic Orbits: A New Language for Neuronal Dynamics</article-title>. <source>Biophys J</source>. <year>1998</year>;<volume>74</volume>: <fpage>2776</fpage>–<lpage>2785</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3495(98)77985-8" xlink:type="simple">10.1016/S0006-3495(98)77985-8</ext-link></comment> <object-id pub-id-type="pmid">9635732</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Anzai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>Y</given-names></name>. <article-title>A new approach to spike sorting for multi-neuronal activities recorded with a tetrode—how ICA can be practical</article-title>. <source>Neurosci Res</source>. <year>2003a</year>;<volume>46</volume>: <fpage>265</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Anzai</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>Y</given-names></name>. <article-title>Automatic sorting for multi-neuronal activity recorded with tetrodes in the presence of overlapping spikes</article-title>. <source>J Neurophysiol</source>. <year>2003b</year>;<volume>89</volume>: <fpage>2245</fpage>–<lpage>2258</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref077"><label>77</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hille</surname> <given-names>B</given-names></name>. <chapter-title>Ion channels of excitable membranes</chapter-title>. <edition>3rd ed.</edition> <publisher-name>Sinauer Assoc Inc</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref078"><label>78</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kantz</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>. <source>Nonlinear Time Series Analysis</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2004</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref079"><label>79</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kantz</surname> <given-names>H</given-names></name>. <chapter-title>Observing and predicting chaotic signals: Is 2% noise too much?</chapter-title> In: <name name-style="western"><surname>Kravtsov</surname> <given-names>YA</given-names></name>, <name name-style="western"><surname>Kadtke</surname> <given-names>JB</given-names></name>, editors. <source>Predictability of Complex Dynamical Systems</source>, <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>;<year>1996</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhao</surname> <given-names>Park</given-names></name> 2016. <article-title>Interpretable Nonlinear Dynamic Modeling of Neural Trajectories Yuan Zhao, Il Memming Park</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>29</volume>. NIPS <year>2016</year></mixed-citation></ref>
<ref id="pcbi.1005542.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Stochastic dynamic causal modelling of fMRI data: Should we care about neural noise?</article-title> <source>Neuroimage</source>. <year>2012</year>;<volume>2</volume>: <fpage>464</fpage>–<lpage>481</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huys</surname> <given-names>QJM</given-names></name>, <name name-style="western"><surname>Paninski</surname> <given-names>L</given-names></name>. <article-title>Smoothing of, and Parameter Estimation from, Noisy Biophysical Recordings</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>;<volume>5</volume>: <fpage>e1000379</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000379" xlink:type="simple">10.1371/journal.pcbi.1000379</ext-link></comment> <object-id pub-id-type="pmid">19424506</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref083"><label>83</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>. <chapter-title>Advanced Data Analysis in Neuroscience: Integrating statistical and computational models</chapter-title>. <publisher-loc>Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>; in press.</mixed-citation></ref>
<ref id="pcbi.1005542.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Neural dynamics and circuit mechanisms of decision-making</article-title>. <source>Curr Opin Neurobiol</source>. <year>2012</year>, <volume>22</volume>: <fpage>1039</fpage>–<lpage>1046</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.08.006" xlink:type="simple">10.1016/j.conb.2012.08.006</ext-link></comment> <object-id pub-id-type="pmid">23026743</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Insabato</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pannunzi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>. <article-title>Multiple Choice Neurodynamical Model of the Uncertain Option Task</article-title>. <source>PLoS Comput Biol</source> <year>2017</year>, <volume>13</volume>: <fpage>e1005250</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005250" xlink:type="simple">10.1371/journal.pcbi.1005250</ext-link></comment> <object-id pub-id-type="pmid">28076355</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source>. <year>2013</year>, <volume>497</volume>: <fpage>585</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Kasper</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Harrison</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>den Ouden</surname> <given-names>HE</given-names></name> <etal>et al</etal>. <article-title>Nonlinear dynamic causal models for fMRI</article-title>. <source>Neuroimage</source>. <year>2008</year>;<volume>42</volume>: <fpage>649</fpage>–<lpage>662</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2008.04.262" xlink:type="simple">10.1016/j.neuroimage.2008.04.262</ext-link></comment> <object-id pub-id-type="pmid">18565765</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toth</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Kostuk</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Meliza</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Margoliash</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abarbanel</surname> <given-names>HD</given-names></name>. <article-title>Dynamical estimation of neuron and network properties I: variational methods</article-title>. <source>Biol Cybern</source>. <year>2011</year>;<volume>105</volume>: <fpage>217</fpage>–<lpage>237</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-011-0459-1" xlink:type="simple">10.1007/s00422-011-0459-1</ext-link></comment> <object-id pub-id-type="pmid">21986979</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kostuk</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Toth</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Meliza</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Margoliash</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abarbanel</surname> <given-names>HD</given-names></name>. <article-title>Dynamical estimation of neuron and network properties II: path integral Monte Carlo methods</article-title>. <source>Biol Cybern</source>. <year>2012</year>;<volume>106</volume>: <fpage>155</fpage>–<lpage>167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-012-0487-5" xlink:type="simple">10.1007/s00422-012-0487-5</ext-link></comment> <object-id pub-id-type="pmid">22526358</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whiteway</surname> <given-names>M. R.</given-names></name>, <name name-style="western"><surname>Butts</surname> <given-names>D. A.</given-names></name> <article-title>Revealing unobserved factors underlying cortical activity with a rectified latent variable model applied to neural population recordings</article-title>. <source>J Neurophysiol</source> <year>2017</year>, <volume>117</volume>: <fpage>919</fpage>–<lpage>936</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00698.2016" xlink:type="simple">10.1152/jn.00698.2016</ext-link></comment> <object-id pub-id-type="pmid">27927786</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>TH</given-names></name>. <article-title>Multistability analysis for recurrent neural networks with unsaturating piecewise linear transfer functions</article-title>. <source>Neural Comput</source>. <year>2003</year>;<volume>15</volume>: <fpage>639</fpage>–<lpage>662</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603321192112" xlink:type="simple">10.1162/089976603321192112</ext-link></comment> <object-id pub-id-type="pmid">12620161</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tang</surname> <given-names>HJ</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>W</given-names></name>. <article-title>Analysis of cyclic dynamics for networks of linear threshold neurons</article-title>. <source>Neural Comput</source>. <year>2005</year>;<volume>17</volume>: <fpage>97</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0899766052530820" xlink:type="simple">10.1162/0899766052530820</ext-link></comment> <object-id pub-id-type="pmid">15563749</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>L</given-names></name>. <article-title>Representations of continuous attractors of recurrent neural networks</article-title>. <source>IEEE Trans Neural Netw</source>. <year>2009</year>;<volume>20</volume>: <fpage>368</fpage>–<lpage>372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNN.2008.2010771" xlink:type="simple">10.1109/TNN.2008.2010771</ext-link></comment> <object-id pub-id-type="pmid">19150791</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang L Yi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>J</given-names></name>. <article-title>Multiperiodicity and attractivity of delayed recurrent neural networks with unsaturating piecewise linear transfer functions</article-title>. <source>IEEE Trans Neural Netw</source>. <year>2008</year>;<volume>19</volume>: <fpage>158</fpage>–<lpage>167</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNN.2007.904015" xlink:type="simple">10.1109/TNN.2007.904015</ext-link></comment> <object-id pub-id-type="pmid">18269947</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref095"><label>95</label><mixed-citation publication-type="other" xlink:type="simple">Ruder S. An overview of gradient descent optimization algorithms. arXiv:1609.04747, 2016.</mixed-citation></ref>
<ref id="pcbi.1005542.ref096"><label>96</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Mandic</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Chambers</surname> <given-names>JA</given-names></name>. <source>Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability</source> <year>2001</year> <publisher-name>Wiley</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref097"><label>97</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kehoe</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Littlewort</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Fuster</surname> <given-names>J</given-names></name>. (1993) <article-title>A spiking network model of short-term active memory</article-title>. <source>J Neurosci</source>. <year>1993</year>;<volume>13</volume>: <fpage>3406</fpage>–<lpage>3420</lpage>. <object-id pub-id-type="pmid">8340815</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref098"><label>98</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duchi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hazan</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>Y</given-names></name>: <article-title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</article-title>. <source>Journal of Machine Learning Research</source> <year>2011</year>, <volume>12</volume>:<fpage>2121</fpage>–<lpage>2159</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref099"><label>99</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Fuster</surname> <given-names>JM</given-names></name>. <source>Prefrontal Cortex</source>. <edition>5th ed.</edition> <publisher-name>Academic Press</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fuster</surname> <given-names>JM</given-names></name>. <article-title>Unit activity in prefrontal cortex during delayed-response performance: neuronal correlates of transient memory</article-title>. <source>J Neurophysiol</source>. <year>1973</year>;<volume>36</volume>: <fpage>61</fpage>–<lpage>78</lpage>. <object-id pub-id-type="pmid">4196203</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Funahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex</article-title>. <source>J Neurophysiol</source>. <year>1989</year>;<volume>61</volume>: <fpage>331</fpage>–<lpage>349</lpage>. <object-id pub-id-type="pmid">2918358</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Erickson</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>. <article-title>Neural mechanisms of visual working memory in prefrontal cortex of the macaque</article-title>. <source>J Neurosci</source>. <year>1996</year>;<volume>16</volume>: <fpage>5154</fpage>–<lpage>5167</lpage>. <object-id pub-id-type="pmid">8756444</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakahara</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Near-saddle-node bifurcation behavior as dynamics in working memory for goal-directed behavior</article-title>. <source>Neural Comput</source>. <year>1998</year>;<volume>10</volume>: <fpage>113</fpage>–<lpage>132</lpage>. <object-id pub-id-type="pmid">9501506</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref104"><label>104</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baeg</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>YB</given-names></name>, <name name-style="western"><surname>Huh</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mook-Jung</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Jung</surname> <given-names>MW</given-names></name>. <article-title>Dynamics of population code for working memory in the prefrontal cortex</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>40</volume>: <fpage>177</fpage>–<lpage>188</lpage>. <object-id pub-id-type="pmid">14527442</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mongillo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Synaptic theory of working memory</article-title>. <source>Science</source>. <year>2008</year>;<volume>319</volume>: <fpage>1543</fpage>–<lpage>1546</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1150769" xlink:type="simple">10.1126/science.1150769</ext-link></comment> <object-id pub-id-type="pmid">18339943</object-id></mixed-citation></ref>
<ref id="pcbi.1005542.ref106"><label>106</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Fahrmeir</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Tutz</surname> <given-names>G</given-names></name>. <source>Multivariate Statistical Modelling Based on Generalized Linear Models</source>. <publisher-name>Springer</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref107"><label>107</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eaves</surname> <given-names>BC</given-names></name>. "<article-title>Solving Piecewise Linear Convex Equations</article-title>," <source>Mathematical Programming</source>, Study 1, <month>November</month>; <year>1974</year>. pp. <fpage>96</fpage>–<lpage>119</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref108"><label>108</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eaves</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Scarf</surname> <given-names>H</given-names></name>. <article-title>The solution of systems of piecewise linear equations</article-title>. <source>Math Oper Res</source>. <year>1976</year>;<volume>1</volume>: <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cottle</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Dantzig</surname> <given-names>GB</given-names></name>. <article-title>Complementary pivot theory of mathematical programming. ‎</article-title><source>Linear Algebra Appl</source>. <year>1968</year>;<volume>1</volume>: <fpage>103</fpage>–<lpage>125</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crisan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Doucet</surname> <given-names>A</given-names></name>. <article-title>A Survey of Convergence Results on Particle Filtering Methods for Practitioners</article-title>. <source>IEEE Trans Signal Process</source>. <year>2002</year>;<volume>50</volume>: <fpage>736</fpage>–<lpage>746</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005542.ref111"><label>111</label><mixed-citation publication-type="other" xlink:type="simple">Lee A, Whitley N. Variance estimation in the particle filter. arXiv:1509.00394v2</mixed-citation></ref>
<ref id="pcbi.1005542.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyman</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Balaguer-Ballester</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Durstewitz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname> <given-names>JK</given-names></name>. <article-title>Contextual encoding by ensembles of medial prefrontal cortex neurons</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2013</year>;<volume>109</volume>: <fpage>5086</fpage>–<lpage>5091</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>