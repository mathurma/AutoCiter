<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00148</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002372</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Efficient Sparse Coding in Early Sensory Processing: Lessons from Signal Recovery</article-title><alt-title alt-title-type="running-head">Efficient Sparse Coding in Sensory Processing</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Lörincz</surname>
            <given-names>András</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Palotai</surname>
            <given-names>Zsolt</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Szirtes</surname>
            <given-names>Gábor</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Software Technology and Methodology, Eötvös Loránd University, Budapest, Hungary</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Sparsense Inc., Boca Raton, Florida, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>ELTE-Soft Ltd, Budapest, Hungary</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Center for Integrative Neuroscience, University of Tuebingen, Tuebingen, Germany</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Graham</surname>
            <given-names>Lyle J.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Université Paris Descartes, Centre National de la Recherche Scientifique, France</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">andras.lorincz@elte.hu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: AL ZP. Performed the experiments: ZP. Analyzed the data: ZP GS. Wrote the paper: GS AL.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>3</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>1</day>
        <month>3</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>3</issue><elocation-id>e1002372</elocation-id><history>
        <date date-type="received">
          <day>3</day>
          <month>2</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>12</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Lörincz et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Sensory representations are not only sparse, but often overcomplete: coding units significantly outnumber the input units. For models of neural coding this overcompleteness poses a computational challenge for shaping the signal processing channels as well as for using the large and sparse representations in an efficient way. We argue that higher level overcompleteness becomes computationally tractable by imposing sparsity on synaptic activity and we also show that such <italic>structural</italic> sparsity can be facilitated by statistics based decomposition of the stimuli into typical and atypical parts prior to sparse coding. Typical parts represent large-scale correlations, thus they can be significantly compressed. Atypical parts, on the other hand, represent local features and are the subjects of actual sparse coding. When applied on natural images, our decomposition based sparse coding model can efficiently form overcomplete codes and both center-surround and oriented filters are obtained similar to those observed in the retina and the primary visual cortex, respectively. Therefore we hypothesize that the proposed computational architecture can be seen as a coherent functional model of the first stages of sensory coding in early vision.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Neural systems favor overcomplete sparse codes in which the number of potential output neurons may exceed the number of input neurons, but only a small subset of neurons become actually active. We argue that efficient use of such large dimensional overcomplete sparse codes requires structural sparsity by controlling the number of active synapses. Motivated by recent results in signal recovery, we introduce a particular signal decomposition as a pre-filtering stage prior to the actual sparse coding, which efficiently supports structural sparsity. In contrast to most models of sensory processing, we hypothesize that the observed transformations may actually realize parallel encoding of the stimuli into representations that describe typical and atypical parts. When trained on natural images, the resulting system can handle large, overcomplete representations and the learned transformations seem compatible with the various receptive fields characteristic to different stages of early vision. In particular, transformations realized by the prefiltering units can be approximated as ‘Difference-of-Gaussians’ filters, similar to the receptive fields of neurons in the retina and the LGN. In addition, sparse coding units have localized and oriented edge filters like the receptive fields of the simple cells in the primary visual cortex, V1.</p>
      </abstract><funding-group><funding-statement>The research reported in this paper is supported by the European Union and co-financed by the European Social Fund (grant agreement no. TAMOP 4.2.1/B-09/1/KMR-2010-0003). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="14"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>In the last decades a large body of research has been devoted to explain the nature of neural representations. Since experimental manipulation of the stimuli has the most direct impact on the sensory responses, most of our knowledge comes from studies about the early stages of sensory systems. Although we do not have a complete story yet, experimental and theoretical research did reveal important principles about the nature of neuronal representations together with specific constraints imposed by anatomy and physiology. Derived from the efficient coding theory <xref ref-type="bibr" rid="pcbi.1002372-Barlow1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Atick1">[2]</xref>, different popular models – emphasizing redundancy reduction (like <xref ref-type="bibr" rid="pcbi.1002372-Dong1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Bell1">[4]</xref>) or the sparsity constraint (Sparse Coding, SC, e.g. <xref ref-type="bibr" rid="pcbi.1002372-Olshausen1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Doi1">[6]</xref>) – can account for many, but not all relevant features of early sensory processing (e.g. <xref ref-type="bibr" rid="pcbi.1002372-Graham1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Ringach1">[8]</xref>). In this article we argue that a novel computational model of neural representation can be obtained by focusing on one of those relevant features: overcompleteness. For codes with this property the number of potential coding units is larger than that of the input units thus offering increased memory capacity and enhanced robustness against noise and structural perturbations. We will argue that the formation of large and sparse representations of high level of overcompleteness requires adaptive learning which can effectively control the number of active synapses. This structural sparsification has a significant impact on the overall metabolic cost of neural activity. We then present a new sparse coding scheme which is motivated by both theories mentioned above, but is built on a non-conventional signal model assuming an <italic>additive decomposition</italic> of stimuli into “typical” and “atypical” constituents. We also analyze the model's filtering properties when trained on natural images. The main contribution of our study is that principled pre-filtering based on this alternative signal model can indeed facilitate overcomplete SC by supporting structural sparsity. The pre-filtering process is motivated by recent results on efficient compression, completion and decomposition of high dimensional data; computational functions equally important for artificial and natural systems. Based on the finding that our model can simultaneously explain several features of early vision we then suggest a biological implementation of the two stage algorithm.</p>
      <p>The paper is organized as follows. In the <xref ref-type="sec" rid="s2">Results</xref> section first we review the computational problem of overcomplete sparse coding and argue about the importance to control synaptic activity. Then we introduce our two stage algorithm which can achieve structural sparsity thus supporting overcomplete sparse coding. In support of our model numerical experiments on natural images are also presented. In the <xref ref-type="sec" rid="s3">Discussion</xref> section we compare the computational properties and biological relevance of our model with alternative approaches. In the <xref ref-type="sec" rid="s4">Methods</xref> section the details of the numerical experiments are provided together with brief descriptions, pseudocodes and references to more elaborate presentations of the algorithmic building blocks.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>In this section we present the problem of (overcomplete) sparse coding (SC) with an emphasis on metabolic constraints (regarding spike activity) and briefly discuss some alternative algorithmic solutions. We then consider if further reduction in computational (metabolic) cost can be accomplished by targeting synaptic activity. Motivated by the insight that the presence of noise hinders the effective control of synaptic activity, we introduce a novel two stage sparse coding algorithm which facilitates structural sparsity (i.e. by keeping the number of active synapses low) and in turn supports the formation of overcomplete sparse codes. The model is then tested on natural images and the responses of the computing units are compared to neural responses in early vision.</p>
      <sec id="s2a">
        <title>Preliminaries</title>
        <p>Due to the high metabolic cost of spiking activity <xref ref-type="bibr" rid="pcbi.1002372-A1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1002372-Laughlin1">[11]</xref>, constraining average spiking rate (over time and population) seems to be a general principle in neural systems (but see <xref ref-type="bibr" rid="pcbi.1002372-Berkes1">[12]</xref>). Therefore we also consider sparsity central in our coding model. The objective of the sparse coding (SC) scheme is to find the sparsest representation of the data with low reconstruction error. It has been argued that this scheme offers a computationally and metabolically advantageous trade off between fully localized (like “grandmother”-cells) and distributed codes <xref ref-type="bibr" rid="pcbi.1002372-Fldik1">[13]</xref>. Sparse codes essentially try to approximate the underlying hidden structure (the generating sources) of the observed stimulus. The great advantage of SC over other coding schemes is that it directly controls energy consumption by setting the number of active coding units; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e001" xlink:type="simple"/></inline-formula> out of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e002" xlink:type="simple"/></inline-formula> coding units with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e003" xlink:type="simple"/></inline-formula> can be active at any given time. Another important property of neural codes is overcompleteness, when the number of coding units (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e004" xlink:type="simple"/></inline-formula>) is greater than the number of input units (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e005" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e006" xlink:type="simple"/></inline-formula>). For example, in area 17 of cat the ratio of the output fibers versus the input fibers from the LGN is estimated about 25∶1, while in macaque primary visual cortex, V1 the estimate is between 12∶1 and 160∶1 <xref ref-type="bibr" rid="pcbi.1002372-Doi2">[14]</xref> or even 500∶1 <xref ref-type="bibr" rid="pcbi.1002372-Essen1">[15]</xref>. In principle, overcompleteness provides more flexibility in finding even sparser representations. However, overcompleteness presents a non-trivial challenge for computational models on neural representations. In comparison with biological data, most computational models of SC can find the optimal solution if overcompleteness is 2 to 8-fold at most <xref ref-type="bibr" rid="pcbi.1002372-Lrincz1">[16]</xref>. Importantly, higher level of overcompleteness may increase the overall metabolic cost of neural coding for two reasons. First, non-optimal solutions require too many iterations thus generating excess spiking activity. Second, overcompleteness induces an asymmetry in the use of the encoder and decoder channels <italic>within</italic> one iteration: while the excitation process requires the use of all <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e007" xlink:type="simple"/></inline-formula> encoder channels, selected subsets of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e008" xlink:type="simple"/></inline-formula> active decoding units require only <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e009" xlink:type="simple"/></inline-formula> decoder channels. That is methods that avoid the heavy use of encoding are more favorable. The importance of controlling the number of active coding channels (that is the number of synapses which define the receptive field of a neuron) is highlighted by the fact that according to the estimates of <xref ref-type="bibr" rid="pcbi.1002372-Lennie1">[10]</xref>, more than 50% of the metabolic cost of a single spike can be attributed to the excitatory potentials at the postsynaptic sites (EPSPs). Our goal is thus to find an algorithmic model that can explain overcomplete sparse coding in the brain.</p>
        <p>Formally, SC can be stated as an alternating (two step) optimization problem:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e010" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e011" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e012" xlink:type="simple"/></inline-formula> signal, or input to be reconstructed, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e013" xlink:type="simple"/></inline-formula> is the number of training inputs, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e014" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e015" xlink:type="simple"/></inline-formula>) denotes the coefficient vector of the sparse decomposition also called (internal) representation and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e016" xlink:type="simple"/></inline-formula> is the basis, or <italic>dictionary</italic> of features. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e017" xlink:type="simple"/></inline-formula> denotes the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e018" xlink:type="simple"/></inline-formula>-norm, which is the number of nonzero components. The first term minimizes the reconstruction error, while the second one penalizes solutions with many non-zero components. Sparsity of representation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e019" xlink:type="simple"/></inline-formula> is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e020" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e021" xlink:type="simple"/></inline-formula> is the number of non-zero components. The resulting code is overcomplete, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e022" xlink:type="simple"/></inline-formula> and the difficulty of finding a sparse code with minimal reconstruction error depends on the level of overcompleteness (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e023" xlink:type="simple"/></inline-formula>) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e024" xlink:type="simple"/></inline-formula>. Parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e025" xlink:type="simple"/></inline-formula> controls the trade-off between the two terms. The reconstruction error or residual may be due to different noise sources that hide the structure of generating sources of the signal.</p>
        <p>At one step the basis set is adjusted (<italic>learning process</italic>) to minimize the reconstruction error while the activity of the coding units, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e026" xlink:type="simple"/></inline-formula> is kept fixed. The straightforward solution would be to let evolve <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e027" xlink:type="simple"/></inline-formula> by stochastic gradient on the cost function derived from the reconstruction error, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e028" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e029" xlink:type="simple"/></inline-formula> and ‘hat’ denotes the actual estimation. Because of the role of the reconstruction error, this rule is not directly local <xref ref-type="bibr" rid="pcbi.1002372-Widrow1">[17]</xref>, yet it can be translated <xref ref-type="bibr" rid="pcbi.1002372-Lrincz2">[18]</xref> into a <italic>set</italic> of Hebbian (local) interactions realized by particular network structures with feedback.</p>
        <p>During the selection of non-zero units (formation of the sparse code), features (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e030" xlink:type="simple"/></inline-formula>) are fixed. However, selection by exhaustive search is a combinatorially hard problem <xref ref-type="bibr" rid="pcbi.1002372-Natarajan1">[19]</xref>: the number of iterations becomes computationally prohibitive as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e031" xlink:type="simple"/></inline-formula> (the dimension of the internal representation) increases. For this reason several approximation method exist, but they either have slow convergence or provide non-optimal solutions. To overcome these limitations, we have chosen a heuristics that combines two approaches. The so called Subspace Pursuit (SP) method <xref ref-type="bibr" rid="pcbi.1002372-Tropp1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1002372-Dai1">[23]</xref> has been chosen because of its superior speed. It is a generalization of matching pursuit <xref ref-type="bibr" rid="pcbi.1002372-Mallat1">[24]</xref>, which finds local optima in a fast iterative fashion. Importantly, this method is able to discover the global optimum provided that certain conditions are met. Numerical experiments on natural visual stimuli indicate that methods, which assume these conditions, work surprisingly well <xref ref-type="bibr" rid="pcbi.1002372-Cands1">[25]</xref>, even though the conditions are unlikely to be met (but see <xref ref-type="bibr" rid="pcbi.1002372-Pati1">[26]</xref> on the inherent limitations of matching pursuit like methods). In contrast to SP, the other algorithmic component – the so called Cross Entropy method (CEM) <xref ref-type="bibr" rid="pcbi.1002372-Rubinstein1">[27]</xref> – is an optimization method designed to find the global optimum. Its main limitation is the slow convergence rate. The combination, termed Subspace Cross-Entropy (SCE) <xref ref-type="bibr" rid="pcbi.1002372-Lrincz1">[16]</xref> method inherits the best of both worlds: it is reasonably fast and still can yield the optimal solution even at a higher level of overcompleteness. Since we are interested in the formation of sparse codes at very high level of overcompleteness, we used SCE in our numerical experiments. The appendix contains the pseudocodes of SP, CEM and SCE for the sake of reproducibility. Detailed analysis of these methods can be found in <xref ref-type="bibr" rid="pcbi.1002372-Lrincz1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Lrincz3">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Szita1">[29]</xref>.</p>
      </sec>
      <sec id="s2b">
        <title>Improving Overcomplete Sparse Coding</title>
        <p>The learning process of Eq. (1) is prune to perturbations: excess activation caused by noise may induce changes in all features thus introducing global (long-range) and low spatial frequency correlations among the features. Such unwanted increase in the number of active synapses implies increased metabolic cost.</p>
        <p>Observation noise (e.g. induced by intrinsic neural activity) can significantly decrease the efficiency of OSC as it may easily generate access activation at the output (representation) level, which can only be mitigated by a number of further iterations in order to reduce the reconstruction error. In turn it is essential to counter this effect by actively controlling the number of non-zero components of the filters. This constraint is referred to as <italic>structural sparsity</italic> and implies that visual RFs with <italic>local</italic>, i.e., spatially restricted responses (like the high frequency, concentric RFs of the retinal ganglion cells, the relay neurons in the LGN, or the elongated oriented Gabor patch like RFs of the simple cells in V1) are metabolically more favorable over those that have large global structure with many synapses involved <xref ref-type="bibr" rid="pcbi.1002372-Vincent1">[30]</xref>. Approaches like weight thresholding or increasing overcompleteness (see <xref ref-type="sec" rid="s3">Discussion</xref>) fail to address this issue properly. Instead, we turn to an alternative approach by directly separating global (involving many synapses), i.e., <italic>low-frequency</italic> or long-range components of the stimuli <italic>before</italic> the actual sparse coding. Considering the famous <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e032" xlink:type="simple"/></inline-formula> frequency fall of the amplitude spectrum of natural images <xref ref-type="bibr" rid="pcbi.1002372-Simoncelli1">[31]</xref>, the low-frequency components carry most of the energy. Principal Component Analysis (PCA, <xref ref-type="bibr" rid="pcbi.1002372-Jolliffe1">[32]</xref>, often used decorrelation method), for example, represents the signal in a way that the first component would carry the largest amount of energy, while the last one would carry the least amount. In turn, by applying PCA and then projecting the data <italic>out of the subspace</italic> of the first principal components would yield a representation without the unwanted low-frequency content. Let us remark that this approach is in contrast to conventional thinking which would keep exactly those components with high energy and filter out the rest. While this idea is appealing, PCA based separation of the subspaces strongly depends on the signal statistics: components (“outliers”) with heavy tailed amplitude distribution (characteristic to natural stimuli) can easily break down PCA. In the next section we review a robust alternative to PCA, which can efficiently separate these outliers from the low frequency components. We then propose an overcomplete SC model in which SCE (or any other efficient SC solution) is complemented by this alternative prefiltering as it is expected to support structural sparsity in the subsequent SC stage.</p>
      </sec>
      <sec id="s2c">
        <title>Two-stage overcomplete SC with structural sparsity</title>
        <p>Our concept is based on recent findings of signal processing about recovering low-dimensional data from high dimensional observations <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref>. In signal processing, conventional analysis of large dimensional data, such as sensory observations, is often based on the assumption that data have low intrinsic dimensionality: they lie on a low-dimensional subspace. In <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e033" xlink:type="simple"/></inline-formula> norm (the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e034" xlink:type="simple"/></inline-formula>-norm of vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e035" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e036" xlink:type="simple"/></inline-formula> stands for transposition, is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e037" xlink:type="simple"/></inline-formula>), PCA provides rank-<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e038" xlink:type="simple"/></inline-formula> estimate of the data by solving the following problem:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e039" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e040" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e041" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e042" xlink:type="simple"/></inline-formula> is the matrix of observations (dimension of the observations: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e043" xlink:type="simple"/></inline-formula>, number of data points: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e044" xlink:type="simple"/></inline-formula>), rank of matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e045" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e046" xlink:type="simple"/></inline-formula> at most and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e047" xlink:type="simple"/></inline-formula> models a small noisy perturbation of each entry <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e048" xlink:type="simple"/></inline-formula>. If this perturbation is Gaussian noise, then PCA provides the statistically optimal estimate of the low-frequency, low dimensional subspace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e049" xlink:type="simple"/></inline-formula>. However, deviation from the Gaussian (e.g. gross perturbations or components with heavy tailed distribution) can easily yield incorrect estimates.</p>
        <p>Because of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e050" xlink:type="simple"/></inline-formula> frequency dependence natural stimuli often contain outliers and thus we need an alternative signal model. Let matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e051" xlink:type="simple"/></inline-formula> comprise the low frequency components (so it has low-rank as above), while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e052" xlink:type="simple"/></inline-formula> may have full rank, but it is a <italic>sparse</italic> matrix with arbitrarily large entries at random locations: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e053" xlink:type="simple"/></inline-formula>. The surprising result is that under certain conditions (on the rank of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e054" xlink:type="simple"/></inline-formula> and on the sparsity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e055" xlink:type="simple"/></inline-formula>) <italic>both</italic> matrices can be <italic>exactly</italic> recovered <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref>. Furthermore, it has been proved that efficient recovery is feasible by solving the following optimization problem (Robust Principal Component Analysis, RPCA):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e056" xlink:type="simple"/><label>(5)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e057" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e058" xlink:type="simple"/></inline-formula> denotes the <italic>sum</italic> of the singular values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e059" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e060" xlink:type="simple"/></inline-formula> denotes the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e061" xlink:type="simple"/></inline-formula> norm of matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e062" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e063" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e064" xlink:type="simple"/></inline-formula> is a trade-off parameter, which governs the dimension of matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e065" xlink:type="simple"/></inline-formula>. On the other hand, matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e066" xlink:type="simple"/></inline-formula> may assume maximal rank, independent of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e067" xlink:type="simple"/></inline-formula>.</p>
        <p>In addition to robustness against perturbation, the proposed decomposition allows an alternative interpretation of the signals. Instead of treating sparse components as corrupting noise to be filtered, we may consider these outliers as <italic>atypical signals</italic> that carry further information about higher order correlations (like configurational information) not revealed by the low-rank estimate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e068" xlink:type="simple"/></inline-formula>). Note that conventional methods (like ICA) would analyze the low rank part only.</p>
        <p>The suggested solution (the pseudocode is given in <xref ref-type="table" rid="pcbi-1002372-t001">Table 1</xref>) iteratively improves the estimation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e069" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e070" xlink:type="simple"/></inline-formula> and its computational complexity is only slightly larger than that of the traditional PCA <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref>. Another surprising result is that under the assumptions of the theorem, a whole range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e071" xlink:type="simple"/></inline-formula> values can return the correct solution, no matter what <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e073" xlink:type="simple"/></inline-formula> are. A simple reference value for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e074" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e075" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref> and so we will use a normalized parameter: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e076" xlink:type="simple"/></inline-formula>.</p>
        <table-wrap id="pcbi-1002372-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.t001</object-id><label>Table 1</label><caption>
            <title>RPCA pseudo-code.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002372-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">initialize:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e077" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e078" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e079" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e080" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e081" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e082" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e083" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e084" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e085" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e086" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e087" xlink:type="simple"/></inline-formula>.</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e088" xlink:type="simple"/></inline-formula> denotes a shrinkage operator, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e089" xlink:type="simple"/></inline-formula> acting on matrices componentwise. For matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e090" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e091" xlink:type="simple"/></inline-formula> denotes the singular value threshold operator: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e092" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e093" xlink:type="simple"/></inline-formula> is the singular value decomposition.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>Interestingly, as numerical experiments suggest <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref>, RPCA delivers meaningful signal decomposition even if conditions (about the sparseness of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e094" xlink:type="simple"/></inline-formula>) do not hold (like in the case of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e095" xlink:type="simple"/></inline-formula> spectra). In these cases, however, different RPCA decompositions can be obtained by setting different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e096" xlink:type="simple"/></inline-formula> values and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e097" xlink:type="simple"/></inline-formula> is not guaranteed to be sparse anymore. For this reason matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e098" xlink:type="simple"/></inline-formula> could be the subject of further sparsification. The corresponding sparse coding optimization (see Eq. (1)) in matrix form is given as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e099" xlink:type="simple"/><label>(7)</label></disp-formula>where the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e101" xlink:type="simple"/></inline-formula>, denotes the matrix of the outliers and the matrix of their sparse representations, respectively. The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e102" xlink:type="simple"/></inline-formula> norm based residual may denote full rank observation noise, which implies the following signal model: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e103" xlink:type="simple"/></inline-formula>. According to <xref ref-type="bibr" rid="pcbi.1002372-Zhou1">[34]</xref>, it is still possible to give stable estimates for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e105" xlink:type="simple"/></inline-formula>, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e106" xlink:type="simple"/></inline-formula> is bounded: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e107" xlink:type="simple"/></inline-formula>, for some <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e108" xlink:type="simple"/></inline-formula> value, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e109" xlink:type="simple"/></inline-formula> denotes the Froebenius norm. In the demonstrations we opted to use the simpler RPCA model (as in Eq. (6)) without explicit assumptions about the additive noise term.</p>
        <p>Let us note that even though the formalism used above is based on matrices, the RPCA procedure can be applied on a single input (thus it may be realized in a neurally plausible form) once an approximation of the low-rank part <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e110" xlink:type="simple"/></inline-formula> is available. Furthermore, – depending on the input statistics – <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e111" xlink:type="simple"/></inline-formula> can be approximated even from partial observation by ‘filling in’ missing information <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Cands3">[35]</xref>.</p>
      </sec>
      <sec id="s2d">
        <title>Computer experiments</title>
        <p>To test the impact of RPCA preprocessing on sparse coding, normalized natural image patches were first decomposed by RPCA at different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e112" xlink:type="simple"/></inline-formula> values, then the resulting full rank representations were further encoded by SCE (16-fold overcompleteness with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e113" xlink:type="simple"/></inline-formula> dimensional inputs and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e114" xlink:type="simple"/></inline-formula> dimensional representation; numerical details are in the <xref ref-type="sec" rid="s4">Methods</xref> Section). We have chosen this particular input set since there already exist a number of computer vision studies on their statistics and the corresponding neural representations under different optimality criteria <xref ref-type="bibr" rid="pcbi.1002372-Doi2">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Simoncelli1">[31]</xref>. The actual overcomplete sparse representations were formed by SCE and the corresponding SC filters were tuned online via stochastic gradient learning. While this level of overcompleteness is still below what has been estimated in the neural sensory systems <xref ref-type="bibr" rid="pcbi.1002372-Essen1">[15]</xref>, we believe it is a reasonable choice, as training time is still manageable, yet the results are convincing enough to support the central message of our proposal.</p>
        <p>A few basis features (for sparse coding, 10 out of 4096 columns of matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e115" xlink:type="simple"/></inline-formula>) are shown on <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1</xref>. For visualization purposes each basis vector is scaled into the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e116" xlink:type="simple"/></inline-formula> and displayed as a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e117" xlink:type="simple"/></inline-formula> image. Features in the first row of <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1A</xref> were obtained by conventional SC (applying SCE) without pre-filtering, which corresponds to the case of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e118" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002372-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Different basis types of RPCA preprocessing and Sparse Coding.</title>
            <p>Sample receptive fields are scaled into range [0,1]. (A) no RPCA, columns of dictionary <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e119" xlink:type="simple"/></inline-formula>. (B) receptive fields learned after PCA pre-filtering: features show wavy, global structure. (C) Features (‘global filters’) of the low dimensional signal for the case <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e120" xlink:type="simple"/></inline-formula> (dimension = 17). (D) reverse correlation of the full rank sparsified signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e121" xlink:type="simple"/></inline-formula> yields stereotypical DoG-like filters with symmetric 2D structure. The figure shows the profile of the central section as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e122" xlink:type="simple"/></inline-formula>. At higher values the negative basin around the peak gets deeper. (E) Randomly selected sparse coding filter sets (over-completeness is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e123" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e124" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e125" xlink:type="simple"/></inline-formula>) With increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e126" xlink:type="simple"/></inline-formula> the filters get smaller and more localized (i.e. <italic>cleaner</italic>). (F) For comparison, a set of sparse coding filters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e127" xlink:type="simple"/></inline-formula>) and the corresponding linear approximations (normalized reverse correlation, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e128" xlink:type="simple"/></inline-formula>) are shown at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e129" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g001" xlink:type="simple"/>
        </fig>
        <p>As we earlier argued, plain SC tends to learn large, global filters, thus preventing the reduction of synaptic cost. <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1B</xref> plots a few selected SC features when applied on the residuals of traditional PCA. Regarding locality we do not see much improvement: features are still global and manifest large, wavy structures. <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1E</xref> depicts example filters obtained by applying RPCA prior to SC. Different rows correspond to different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e130" xlink:type="simple"/></inline-formula> values. The main result of these studies is that the learned basis features get cleaner and more localized, that is, filters get <italic>structurally</italic> sparser as the <italic>single</italic> global parameter increases. On <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1F</xref> we re-plotted features for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e131" xlink:type="simple"/></inline-formula> together with the corresponding filters approximated by reverse correlation. Not only the estimation error is smaller compared to the error of the native SC method (<xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1A</xref>), but filters also show larger diversity in their shapes, similar to what has been found experimentally <xref ref-type="bibr" rid="pcbi.1002372-Ringach1">[8]</xref>. We also plotted the corresponding filters or RFs of the low-rank signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e132" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1C</xref> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e133" xlink:type="simple"/></inline-formula>, when the number of basis vectors was 17. <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1D</xref> shows the spatial-dependence of RFs of the sparsified signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e134" xlink:type="simple"/></inline-formula> after RPCA for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e135" xlink:type="simple"/></inline-formula>.</p>
        <p>A surprising result is that the shape of all the obtained RFs for sparsified matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e136" xlink:type="simple"/></inline-formula> can be described as ‘Difference of Gaussians’ which is the characteristic RF shape <xref ref-type="bibr" rid="pcbi.1002372-Rodieck1">[36]</xref> of the retinal ganglion cells and the neurons in LGN. The obtained concentric filters 1, are homogeneous and 2, uniformly tile the whole space. Due to their similarity, we show the cross-section of one unit only (<xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1D</xref>). Note that the peaky structure is due to the small image size (discretized DoGs have similar shape at this scale) and more typical DoG shapes could be obtained for larger image patches. We found that for higher <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e137" xlink:type="simple"/></inline-formula> values the negative basin around the peak gets deeper. This development may correspond to the experimentally found developmental changes of the LGN filter profiles in cat <xref ref-type="bibr" rid="pcbi.1002372-Cai1">[37]</xref>.</p>
        <p>Let us emphasize again that RPCA is not a projection: through an iterative process it extracts the large and sparse components and separates the low-rank part. Interestingly, for natural images, RPCA provides a basis visually almost indistinguishable from those of the PCA filters, but the corresponding representations are different. It implies that PCA may be a good first approximation or initialization for the RPCA iteration method (higher <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e138" xlink:type="simple"/></inline-formula> values allow more low-dimensional components).</p>
      </sec>
      <sec id="s2e">
        <title>Qualitative comparison between filters and RFs</title>
        <p>Traditionally, a simple cell RF in V1 is often characterized as a ‘Gabor-patch’ <xref ref-type="bibr" rid="pcbi.1002372-Jones1">[38]</xref>; Gaussian envelope around a cosine wave. To help compare the obtained filters with RFs of real neurons, we also approximated the filters as a Gabor-patch. As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e139" xlink:type="simple"/></inline-formula> increases the filters become more localized and cleaner, and the Gabor-patch like appearance gets more pronounced. On the other hand, at too large values the filters become small and stereotyped with diminishing harmonic content (see <xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1E</xref>).</p>
        <p>The distribution of the shape parameters of the Gabor-patch approximations (Eqs. (9)–(11)) is shown in <xref ref-type="fig" rid="pcbi-1002372-g002">Figure 2</xref> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e140" xlink:type="simple"/></inline-formula>. Filters localized at the edges of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e141" xlink:type="simple"/></inline-formula> visual space were discarded as their distortion prevents proper fitting. For small filters fitting is imprecise. Filters yielding Gaussian envelope with width less then 0.3 pixel were thus also discarded. It implies that the true number of learned filters at around point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e142" xlink:type="simple"/></inline-formula> is larger than what is shown in <xref ref-type="fig" rid="pcbi-1002372-g002">Figure 2</xref>. Visual inspection reveals that (i) filters become local and cleaner, (ii) the distribution deviates significantly from the bisection line, and (ii) a considerable portion of the filters is concentrated near the origin <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e143" xlink:type="simple"/></inline-formula>. For comparison, we also plotted the distribution of the fitted shape parameters of the experimentally measured RFs of simple cells reproduced from <xref ref-type="bibr" rid="pcbi.1002372-Ringach1">[8]</xref>. Considering that we had to drop a number of small filters, the match between numerical and experimental data seems quite good (see, e.g., <xref ref-type="bibr" rid="pcbi.1002372-Lcke1">[39]</xref> for comparison), indicating that the proposed model may have biological relevance. Let us note that the observed shape distribution may depend on the level of overcompleteness, but due to the relatively small input size we suspect that further increase in the number of coding units would not result in major changes.</p>
        <fig id="pcbi-1002372-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Distribution of the shape parameters for the model and for the experimental data.</title>
            <p>Receptive fields of simple cells in primary visual cortex, linearly approximated by spike triggered averaging. Data <xref ref-type="bibr" rid="pcbi.1002372-Ringach1">[8]</xref> are available at <ext-link ext-link-type="uri" xlink:href="http://web.mac.com/darioringach/lab/Data.html" xlink:type="simple">http://web.mac.com/darioringach/lab/Data.html</ext-link>. Our model filters show significant diversity in the fitted shapes similar to what has been found experimentally. While other models (e.g. <xref ref-type="bibr" rid="pcbi.1002372-Lcke1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref>) are also able to partially match the filters to the observed RFs, a significant difference is that our model uses highly overcomplete representations. For other differences, see the main text.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g002" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2f">
        <title>Numerical analysis of the prefiltering and sparse coding stages</title>
        <p>Since the assumed signal model is only an approximation for natural image patches, different trade-offs (defined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e144" xlink:type="simple"/></inline-formula> in Eq. (5)) between the contribution of the typical and atypical features to the reconstruction influence the emerging representations after RPCA prefiltering. <xref ref-type="fig" rid="pcbi-1002372-g003">Figure 3A</xref> depicts the influence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e145" xlink:type="simple"/></inline-formula> and thus the RPCA decomposition on the statistics of the SC filter shapes as measured by the histogram of the Gabor-patch fitting error. It shows how well the linear approximation of sparse coding filters can be described with a set of oriented Gabor patches often used to characterize experimentally measured receptive fields. If filters have ‘dilated’ global structure then the histogram of the fitting error is probably less peaked. And indeed this is the case: increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e146" xlink:type="simple"/></inline-formula> results in more homogeneous, smaller and point-like filters. Let us remark that discretization has a strong contribution to the observed fitting noise.</p>
        <fig id="pcbi-1002372-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>The impact of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e147" xlink:type="simple"/></inline-formula> on the signal decomposition and the overall quality of the sparse coding filters.</title>
            <p>(A) The empirical distribution of the Gabor patch fitting error as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e148" xlink:type="simple"/></inline-formula>. Larger spread signifies deviation from ideal Gabor patch, often used as model shape for experimentally recorded receptive fields. The shift of the mean toward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e149" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e150" xlink:type="simple"/></inline-formula> increases is a consequence of the decrease of the average filter size. For each mean value a sample filter is shown demonstrating this shrinkage effect. (B) The dimension and the relative weight of L (the low dimensional signal) in the reconstruction as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e151" xlink:type="simple"/></inline-formula>. Relevant range is where the dimensionality is low, yet L is able to capture most of the original signal. For image size 16×16 this range is about 0.3–0.8.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g003" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002372-g003">Figure 3B</xref> displays the dependence of the dimension of the low-rank component <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e152" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e153" xlink:type="simple"/></inline-formula> and the relative contribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e154" xlink:type="simple"/></inline-formula> to the reconstruction of the original observations. To calculate the intrinsic dimension of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e155" xlink:type="simple"/></inline-formula>, all singular values were zeroed out with amplitude less then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e156" xlink:type="simple"/></inline-formula> of the maximal amplitude. The important parameter range is where the intrinsic dimension is still low, yet <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e157" xlink:type="simple"/></inline-formula> role in the reconstruction is significant. Within that range, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e158" xlink:type="simple"/></inline-formula> provides the best fit to the experimental data. At higher <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e159" xlink:type="simple"/></inline-formula> values most of the filters loose their edge-like characteristics.</p>
        <p>We have also studied the algorithm's reconstruction ability. Due to the additive decomposition, reconstruction depends on both the “typical” part obtained by RPCA and the overcomplete sparse representation of the “atypical part”. As it is demonstrated on <xref ref-type="fig" rid="pcbi-1002372-g003">Figure 3</xref> the relative contribution of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e160" xlink:type="simple"/></inline-formula> as well as its dimension (number of coding neurons) depends on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e161" xlink:type="simple"/></inline-formula>. In turn, the fidelity of reconstruction is a function of both the number of units that encode typical features and the number of nonzero entries in the sparse code. <xref ref-type="fig" rid="pcbi-1002372-g004">Figure 4</xref> displays this dual dependence: reconstruction quality as a function of the total number of nonzero entries, which comprises the rank estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e162" xlink:type="simple"/></inline-formula> at the given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e163" xlink:type="simple"/></inline-formula> and the preserved number of nonzero entries in the overcomplete sparse representation (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e164" xlink:type="simple"/></inline-formula>). For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e165" xlink:type="simple"/></inline-formula> the chosen values were: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e166" xlink:type="simple"/></inline-formula> and for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e167" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e168" xlink:type="simple"/></inline-formula>. Reconstruction quality is measured by mean SNR: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e169" xlink:type="simple"/></inline-formula>. Interestingly, while SNR does not improve much when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e170" xlink:type="simple"/></inline-formula> has changed from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e171" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e172" xlink:type="simple"/></inline-formula>, the corresponding filters have significantly changed. Let us note that the overall low values of SNR are due to the fact that no high frequency components have been filtered out prior to decomposition (but see <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref>, where much higher SNR has been reported after filtering out those high frequency components).</p>
        <fig id="pcbi-1002372-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Reconstruction quality as a function of the number of nonzero coding units and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e173" xlink:type="simple"/></inline-formula>.</title>
            <p>Reconstruction quality is measured by mean SNR: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e174" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e175" xlink:type="simple"/></inline-formula> runs over the inputs. Since RPCA is an additive decomposition, the reconstruction error is given as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e176" xlink:type="simple"/></inline-formula>. The total number of nonzero entries is given as the sum of the rank estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e177" xlink:type="simple"/></inline-formula> and the preserved number of nonzero units (k) in the sparse overcomplete representation of the atypical part (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e178" xlink:type="simple"/></inline-formula>) of the RPCA output. Since sparseness level is automatically set by SCE, the following arbitrary values for k were chosen. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e179" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e180" xlink:type="simple"/></inline-formula> and for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e181" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e182" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g004" xlink:type="simple"/>
        </fig>
        <p>So far we have dealt with static images, but temporal sequences are more realistic: sensory systems are believed to adapt to the spatio-temporal structure of the stimuli. Since RPCA does not rely on prior knowledge about the spatial or temporal arrangement of the data, one expects to see similar decomposition results for data with temporal correlation. For the sake of illustration, temporal correlation was introduced by concatenating 16 image patches of size 8×8 extracted from image sequences on natural scenes. (This was the maximum size we could handle with overcompleteness ratio 16.) Sample filters of the obtained low-rank matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e183" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e184" xlink:type="simple"/></inline-formula> (the corresponding rank estimate is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e185" xlink:type="simple"/></inline-formula>) are shown on the left of <xref ref-type="fig" rid="pcbi-1002372-g005">Figure 5</xref>. Filters are ordered by their corresponding eigenvalues. Each filter is composed of 16 frames of size 8×8 pixels. Similar to the filters shown on (<xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1C</xref>), these filters can also be characterized by low spatial and temporal frequency.</p>
        <fig id="pcbi-1002372-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>RPCA on concatenated image sequences.</title>
            <p>Left: The first 10 spatio-temporal filters of the low rank signal,L (rank <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e186" xlink:type="simple"/></inline-formula>) are shown. Each filter is shown as a sequence of 16 frames of size 8×8 pixels. It can be seen that there are spatio-temporally separable as well as non-separable filters. All filters correspond to low frequency temporal or spatial changes Right: 10 selected spatio-temporal filters of the corresponding overcomplete sparse codes that display different spatio-temporal localization and dynamics. While many filters are similar to the presented ones, more training would be needed to achieve similar locality for the majority of filters at this input dimensionality (8×8×16) and level of overcompleteness (16×).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g005" xlink:type="simple"/>
        </fig>
        <p>The corresponding filters of the atypical parts (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e187" xlink:type="simple"/></inline-formula>, not shown) - as in the static case- are homogeneous, localized in space and time and uniformly tile the visual space. Furthermore, they show Mexican hat like characteristics in the temporal dimension. The regularity may be due to the particular concatenation method we chose.</p>
        <p>Sparse coding filters can also be derived from the overcomplete sparse representation of the image sequences after RPCA decomposition. As representations are temporally decorrelated, we obtained filters strongly localized in space and time which resemble to some extent to the receptive field dynamics of simple cells of V1 <xref ref-type="bibr" rid="pcbi.1002372-DeAngelis1">[41]</xref>. A sample set of the obtained sparse coding filters are shown on the right of <xref ref-type="fig" rid="pcbi-1002372-g005">Figure 5</xref>.</p>
        <p>It is expected to get better match with experimentally found filters if temporal correlations are introduced into the data model by convolution <xref ref-type="bibr" rid="pcbi.1002372-Szatmry1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Olshausen2">[43]</xref> as opposed to simple concatenation and if nonlinear response properties and nonlinear dynamic interactions are included to handle time warping, for example. These studies go beyond our present goals.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>While the resemblance to the biological system is appealing, the original motivation behind applying RPCA was to find means to facilitate the formation of overcomplete sparse representations, an important feature of neural processing that significantly boosts computational efficiency. As we previously argued, <italic>structural</italic> sparsity is needed to control the underlying metabolic cost of the formation of large, overcomplete sparse representations. In principle this control could be realized in different ways. The most straightforward solution would be weight thresholding by zeroing out all filter components (synaptic weights) below an arbitrary threshold value. However, this intuitive regularization may cause more problems than it solves. First, it introduces error for coordinates near zero, e.g. at zero crossing of the response function of a simple cell. In addition, it does not support adaptivity as it may eliminate gradual learning of less frequently represented features. At last it strongly depends on the arbitrary threshold parameter irrespective of the actual input.</p>
      <p>Another approach would be to further increase overcompleteness as it might implicitly reduce the number of required components (increased sparsity). However, this idea does not work <xref ref-type="bibr" rid="pcbi.1002372-Chennubhotla1">[44]</xref>: when tested on natural images, many filters still show global structure.</p>
      <p>We propose RPCA as a particular prefiltering stage prior to the actual sparse coding which indeed facilitates structural sparsity and preserves many useful properties of conventional PCA based decorrelation without its noise sensitivity. Our model may thus resolve the controversy between the hypothesis that PCA like decorrelation should precede subsequent transformations and the fact that the identified RFs cannot be generated by PCA.</p>
      <p>Although the proposed RPCA based sparse coding mechanism does not have a biologically feasible implementation yet, its functional relevance may be supported by the following arguments.</p>
      <p>The robustness of RPCA has been demonstrated <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref> by showing that RPCA yields meaningful representations for different data sets even if the composite signal model cannot be validated (e.g. separation of background (typical) and moving objects (atypical, outstanding features) or separation of face and shadows caused by anisotropic illumination). In particular, for natural stimuli with characteristic ‘scale-free’ statistics (cf. ‘1/frequency’ relation) the conditions of the RPCA theorem are definitely not met as the distinction between low-rank and sparse parts cannot be clearly defined. It may imply that a step-wise incremental separation would be better suited for the input statistics instead of the single layer iterative arrangement of RPCA.</p>
      <p>Another important finding is that the RPCA theorem of <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref> can be related to recent results on the problem of Exact Matrix Completion <xref ref-type="bibr" rid="pcbi.1002372-Cands3">[35]</xref>, which claims that <italic>typical regularities</italic> of a composite signal (represented by columns of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e188" xlink:type="simple"/></inline-formula>) can be completed even from a <italic>small</italic> set of randomly sampled (or partially observed) coordinates of the input. This “sampling advantage” would also improve energy efficiency.</p>
      <p>While our model is implicitly supported by the emerging filters, alternative models are also claimed to explain early vision by learning similar features. For this reason we briefly compare a few competing sparse coding models with our proposal.</p>
      <sec id="s3a">
        <title>Receptive field properties of sparse coding models</title>
        <p>The biological relevance of neural coding models is often judged by the similarity between their filtering properties and the receptive fields of the corresponding neurons. In the case of visual stimuli, one of the criticisms against theory driven (functional) models (e.g. Independent Component Analysis <xref ref-type="bibr" rid="pcbi.1002372-Bell1">[4]</xref> or Sparse Coding <xref ref-type="bibr" rid="pcbi.1002372-Olshausen1">[5]</xref>) is the lack of diversity in the filter shapes <xref ref-type="bibr" rid="pcbi.1002372-Ringach1">[8]</xref>. This failure might be due to the missing prefiltering stages as seen in the visual pathway. However, nave use of different, biologically motivated prefiltering methods does not seem to offer any improvement, either. For example, applying DoG as high-pass filtering is expected to enhance edge-like features thus yielding a shift of the Gabor-patch shape parameters toward higher values, but the structure of the shape distribution barely changes. Another example is the use of PCA to filter out global features before SC (or ICA), which yields wavy SC basis (<xref ref-type="fig" rid="pcbi-1002372-g001">Figure 1B</xref>). Furthermore, not all filters in V1 have elongated bar shape and most models fail to yield close to concentric shapes found experimentally (for a discussion, see e.g. <xref ref-type="bibr" rid="pcbi.1002372-Lcke1">[39]</xref>). As the filter shape distribution on <xref ref-type="fig" rid="pcbi-1002372-g002">Figure 2</xref> shows, when applied on natural images, RPCA preprocessing <italic>together with</italic> SC delivers the required diversity including the close to concentric shapes. It is worth noting there are other improved coding models (in particular, <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref> and <xref ref-type="bibr" rid="pcbi.1002372-Lcke1">[39]</xref>) that also claim similarities between the observed and predicted shape distributions of the fitted filters. Our model is similar in spirit to the functional model of <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref>, whereas the other approach <xref ref-type="bibr" rid="pcbi.1002372-Lcke1">[39]</xref> describes a self-organizing system governed by complex dynamics and feedforward inhibition. While the latter one is a promising approach, its dynamics is quite involved and its parameter sensitivity is not known. The other model of <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref> is also a sparse coding model and it uses greedy, iterative solutions as mentioned previously. It also uses prefiltering similar to that one used in <xref ref-type="bibr" rid="pcbi.1002372-Olshausen1">[5]</xref>. They claim the obtained similarity is due to the particular sparsity constraint. For the similar motivations let us remark some differences between the model of <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref> and the one proposed here. First, we believe their approach may not be suited to handle large overcompleteness for reasons discussed previously about greedy solutions. Second, the reported difference between the signal to noise ratio of their method and our model is likely due to two factors: we did not employ prefiltering and the overcompleteness in our case is larger. Less sparse codes can encode signals more faithfully then. A fair comparison would be to see the quality of the reconstruction of the high frequency components from sparse codes (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e189" xlink:type="simple"/></inline-formula>), but such comparison would depend on both sparsity and overcompleteness. In turn, an intriguing issue is the optimality of reconstruction quality with respect to the energy consumption. Interestingly, as <xref ref-type="fig" rid="pcbi-1002372-g006">Figure 6</xref> demonstrates, the linear approximation of the filtering properties of RPCA (seen as the amplitude spectrum of the “atypical” signal part of the RPCA output) looks quite similar to what an ideal whitening filter would yield. This similarity may have the following consequences. First, their result may be attributed both to the particular form of the filter and to the chosen form of sparse coding. Furthermore, it might be the case that such prefiltering behaves as a fast approximation to RPCA. Another difference to mention is that our two-stage model not only provides oriented band pass filters, but it also yields DoG-like filters at the RPCA pre-filtering stage thus providing a simultaneous explanation of two processing stages of early vision. Interestingly, as <xref ref-type="fig" rid="pcbi-1002372-g006">Figure 6</xref> demonstrates, linear approximation of the filtering properties of RPCA (seen as the amplitude spectrum of the “atypical” signal part of the RPCA output) looks quite similar to what an ideal whitening filter would yield. This similarity may have the following consequences. First, results of <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref> may be attributed both to the particular form of the filter <italic>and</italic> to the chosen form of sparse coding. Furthermore, it might be the case that such prefiltering behaves as a fast approximation to RPCA.</p>
        <fig id="pcbi-1002372-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>A comparison of the amplitude spectra of the “atypical” output part of RPCA, the whitened input and the whitened ideal input.</title>
            <p>This plot demonstrates that the particular whitening filter as used in <xref ref-type="bibr" rid="pcbi.1002372-Olshausen1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Rehn1">[40]</xref> can be seen as a linear approximation of the filtering properties of RPCA when only the atypical output is considered. The thick (red) line is the amplitude spectrum of the RPCA output. The dashed (blue) line with square markers is the amplitude spectrum of the training images filtered with the whitening filter. The thin (green) line serves as a reference: this is the amplitude spectrum of whitened ideal input which has an amplitude spectrum proportional to 1/frequency. Due to the limited input size, there is a natural cutoff at higher frequencies. (Since the size of the images is 16×16, the largest frequency is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e190" xlink:type="simple"/></inline-formula>.) The whitening filter: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e191" xlink:type="simple"/></inline-formula>, where the cutoff frequency is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e192" xlink:type="simple"/></inline-formula>. The variances of the plots are due the artifacts caused by the rectangular sampling lattice. For comparison purposes the plots are rescaled onto <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e193" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.g006" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s3b">
        <title>Biological implementation of RPCA based sparse coding</title>
        <p>The qualitative agreement between the filtering properties of the early stages of vision and our two-stage algorithm may allow us to attempt to map the algorithm onto the neural substrate by linking the different computational functions to anatomical areas.</p>
        <p>An important property of our model is that prefiltering requires a dual representation of the stimuli, which assumption is not in line with the current thinking of hierarchical sensory processing (e.g. <xref ref-type="bibr" rid="pcbi.1002372-Riesenhuber1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Cadieu1">[46]</xref>), which often comprises alternating filter and pooling operations. So how can we reconcile the assumption on dual representation with single stream models?</p>
        <p>Since RPCA implies dynamic interaction between the two emerging representations of the typical (global) and atypical (local) features, decomposition requires either a recurrent network with distinct sub-populations of neurons or two layers with feedforward and feedback connections. As retina does not receive feedback modulations from downstream layers, DoG like filtering of the retinal ganglion cells is not a consequence of RPCA, but it may be explained as a facilitating approximation – as we argued about whitening above – before decomposition. LGN, on the other hand, receives massive amount of feedback from V1. Having learned the filters during early development, it can be assumed that LGN neurons can represent a proxy to the <italic>atypical</italic> features of single stimuli. This representation still contains information about the typical features (since clear decomposition of natural signals is unlikely, due to scale-free statistics). In turn, V1 has a two-fold role in processing. It holds the approximation of the global features extracted from the LGN output and it recodes or re-represents the atypical features in an overcomplete sparse form. A candidate for the first task could be a class of V1 interneurons characterized by large, global receptive fields with weak or no orientation selectivity (e.g. <xref ref-type="bibr" rid="pcbi.1002372-Cardin1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Liu1">[48]</xref>). While it is possible to learn the low-frequency typical parts of new stimulus sets, RFs do not need to be continuously updated as they comprise the most typical correlations of natural images (short term adaptation to quick changes is still required). The second task of overcomplete recoding is then realized by simple cells. This setting thus allows for the alternating substraction of RPCA (<xref ref-type="table" rid="pcbi-1002372-t001">Table 1</xref>) by the interaction between inhibitory neurons and simple cells in V1 and the neurons in LGN.</p>
        <p>In summary, this paper presents a novel two-stage algorithm for efficient overcomplete sparse coding. The proposed robust extraction of low-frequency or typical correlations as a prefiltering step has a few remarkable properties that make the algorithm plausible as an important model of neural information processing. First, it supports the formation of overcomplete sparse codes by effectively controlling the transformation matrices (the synaptic weights) and reducing the number of active synapses. Second, the inclusion of RPCA could significantly facilitate perception as it allows the completion of the typical components even if a part of the stimuli is missing (undersampling, occlusion, cf. exact matrix completion). Since these properties may be beneficial for the nervous system, it would be interesting to see if our algorithm could be realized by biologically plausible neural computations.</p>
      </sec>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <p>In this section we briefly present the algorithmic constituents of the subspace cross entropy method used to make overcomplete sparse codes. We also give a short algorithmic description of the RPCA implementation used in the simulations. Finally, details of the training data and the fitting methods are presented.</p>
      <sec id="s4a">
        <title>OSC Part I: Subspace Pursuit method, (SP)</title>
        <p>Subspace Pursuit algorithms have been independently proposed in <xref ref-type="bibr" rid="pcbi.1002372-Dai1">[23]</xref> and <xref ref-type="bibr" rid="pcbi.1002372-Needell2">[49]</xref>. These methods assume that at most <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e194" xlink:type="simple"/></inline-formula> components are sufficient to represent the input. The methods enlarge the subset of candidate features (“candidate subspace”) by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e195" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002372-Dai1">[23]</xref> (or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e196" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002372-Needell2">[49]</xref>) features and then decrease their number back to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e197" xlink:type="simple"/></inline-formula> at every iteration. The method of <xref ref-type="bibr" rid="pcbi.1002372-Dai1">[23]</xref> is as follows (the pseudocode is given in <xref ref-type="table" rid="pcbi-1002372-t002">Table 2</xref>).</p>
        <table-wrap id="pcbi-1002372-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.t002</object-id><label>Table 2</label><caption>
            <title>The pseudocode of the Subspace Pursuit method.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002372-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.t002" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">input:</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e198" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e199" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% sparsity and signal</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e200" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% max iteration number</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e201" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e202" xlink:type="simple"/></inline-formula> column dictionary</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">initialization:</td>
                <td align="left" colspan="1" rowspan="1"/>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e203" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% index set of maximal amplitude elements with set size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e204" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e205" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% sub-matrix belonging to index set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e206" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e207" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% compute residual</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">optimization:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e208" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e209" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e210" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% iteration main loop</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e211" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% index set for expansion</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e212" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% increase set size to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e213" xlink:type="simple"/></inline-formula>)</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e214" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% compute projections</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e215" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% new index set of size <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e216" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e217" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% inserting sub-matrix of index set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e218" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e219" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% compute residual</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e220" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% finish is residual is zero</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e221" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% check for improvement</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e222" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% no new iteration</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e223" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% use previous index set</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e224" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">end loop</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">output:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e225" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% indices of optimal representation</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt102">
              <label/>
              <p>The goal is to represent the input with minimal reconstruction error using <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e226" xlink:type="simple"/></inline-formula> basis only <xref ref-type="bibr" rid="pcbi.1002372-Dai1">[23]</xref>. SP differs from other iterative greedy methods in the incremental refinement of the selected basis subset. First, a representation is generated with the help of the full basis set (using pseudoinverse computations). During iteration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e227" xlink:type="simple"/></inline-formula> basis are selected based on the amplitude of the corresponding coordinates of the representation. The resulting residual (difference between the original input and the approximation obtained by projecting the representation onto the input space) is then again projected back to the representation space and another set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e228" xlink:type="simple"/></inline-formula> basis are chosen. The two selected subsets are then fused (<italic>expansion</italic>) and the resulting expanded set is used again to project the original input onto the representation space. Finally a new set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e229" xlink:type="simple"/></inline-formula> basis are selected by the amplitude of the corresponding coordinates of the projection (<italic>shrinkage</italic>). Iteration stops when the norm of the residual does not decrease anymore. Notation: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e230" xlink:type="simple"/></inline-formula> denotes a sub-matrix of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e231" xlink:type="simple"/></inline-formula> where index set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e232" xlink:type="simple"/></inline-formula> contains the indices of the selected columns. The index set of the first <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e233" xlink:type="simple"/></inline-formula> sorted components of a vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e234" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e235" xlink:type="simple"/></inline-formula>.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>First, a candidate representation is generated using all basis, then a subset of basis is selected that corresponds to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e236" xlink:type="simple"/></inline-formula> largest components of the representation. This initial selection is then iteratively refined: the residual (that is the difference between the input and its current approximation) is calculated and mapped onto the representation space using the entire basis set again. Then – similar to the initial step – another <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e237" xlink:type="simple"/></inline-formula> basis are selected based on amplitude of the corresponding components of the mapped residual. The original input is then projected again to the representation space using a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e238" xlink:type="simple"/></inline-formula> element basis set formed by fusing the two basis subsets. Finally <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e239" xlink:type="simple"/></inline-formula> basis vectors are selected again that correspond to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e240" xlink:type="simple"/></inline-formula> largest components of the projection (basis shrinkage). The iteration stops when the norm of the residual is sufficiently small. SP has superior speed, scaling and reconstruction accuracy over other iterative methods by directly refining the subset of reconstructing (active) components at <italic>each</italic> iteration. Its native shortcomings, though, are the heavy use of the costly encoding transformation of the residuals at each iteration and the preset number of active coding units.</p>
      </sec>
      <sec id="s4b">
        <title>OSC Part II: Cross-Entropy method, CEM</title>
        <p>CEM is a global optimization technique <xref ref-type="bibr" rid="pcbi.1002372-Rubinstein1">[27]</xref> that finds the solution in the following form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e241" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e242" xlink:type="simple"/></inline-formula> is a general objective function.</p>
        <p>While most optimization algorithms maintain a single candidate solution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e243" xlink:type="simple"/></inline-formula> at each time step, CEM maintains a <italic>distribution</italic> over possible solutions. From this distribution, solution candidates are drawn at random. By continuous modification of the sampling distribution, random guess becomes a very efficient optimization method.</p>
        <p>One may start by drawing many samples from a fixed distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e244" xlink:type="simple"/></inline-formula> and then selects the best samples as an estimation of the optimum. The efficiency of this random guess depends on the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e245" xlink:type="simple"/></inline-formula> from which the samples are drawn. After drawing a number of samples from distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e246" xlink:type="simple"/></inline-formula>, we may not be able to give an acceptable approximation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e247" xlink:type="simple"/></inline-formula>, but we may still obtain a <italic>better sampling distribution</italic>. The basic idea of CEM is that it selects the best few samples, and modifies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e248" xlink:type="simple"/></inline-formula> so that it becomes more similar to the empirical distribution of the selected samples. CEM resembles the estimation-of-distribution evolutionary methods (see e.g. <xref ref-type="bibr" rid="pcbi.1002372-Muehlenbein1">[50]</xref>) and as a global optimization method, it provably converges to the optimal solution <xref ref-type="bibr" rid="pcbi.1002372-Rubinstein1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Muehlenbein1">[50]</xref>.</p>
        <p>For many parameterized distribution families, the parameters of the minimum cross-entropy distribution can be computed easily from simple statistics of the elite samples. For sparse representations the Bernoulli distribution is of particular interest <xref ref-type="bibr" rid="pcbi.1002372-Olshausen3">[51]</xref>. This particular choice may bring about bias towards solutions where sparse components are drawn independently. Derivations as well as a list of other discrete and continuous distributions with simple update rules can be found in <xref ref-type="bibr" rid="pcbi.1002372-deBoer1">[52]</xref>. Let us note that we have also translated CEM into an online variant in which parameter tuning is realized by neurally plausible local learning <xref ref-type="bibr" rid="pcbi.1002372-Szita1">[29]</xref>. This translation then allowed us to propose a neurally plausible SC method <xref ref-type="bibr" rid="pcbi.1002372-Lrincz3">[28]</xref> in which spikes signal the presence of active components, while rate codes encode the corresponding uncertainty of the given component. Since CEM randomly generates candidate sparse solutions hand, it uses a significantly less number of costly encoding transformations. However it updates the probability of all active components similarly, regardless their individual contributions to the actual reconstruction error.</p>
      </sec>
      <sec id="s4c">
        <title>OSC Part III Subspace Cross-Entropy method, SCE</title>
        <p>Subspace Cross-Entropy method (SCE) is an efficient combination of CEM and SP for overcomplete sparse coding. A detailed description can be found in <xref ref-type="bibr" rid="pcbi.1002372-Lrincz1">[16]</xref> and the pseudocode is given in <xref ref-type="table" rid="pcbi-1002372-t003">Table 3</xref>. SCE inherits the flexibility and synaptic efficiency of CEM as well as the superior speed and scaling properties of SP without their shortcomings. SCE can be realized by inserting an intermediate control step in CEM to individually update the component probabilities based on their contribution to the reconstruction error. Hence the explicit refinement of the feature set via SP is replaced by an implicit modification through component probabilities.</p>
        <table-wrap id="pcbi-1002372-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002372.t003</object-id><label>Table 3</label><caption>
            <title>Pseudo-code of the subspace cross-entropy (SCE) method for Bernoulli distributions.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002372-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.t003" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="2" rowspan="1">required:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e249" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% initial distribution parameters</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e250" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% approximate number of non-zero components</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e251" xlink:type="simple"/></inline-formula> SP <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e252" xlink:type="simple"/></inline-formula> CE</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e253" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% Main loop of Subspace Pursuit iteration</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e254" xlink:type="simple"/></inline-formula>,</td>
                <td align="left" colspan="1" rowspan="1">% Main loop of CE iteration</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"> execute CE iteration</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e255" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% CE optimized index set</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e256" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% compute next residual</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e257" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% check for improvement</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e258" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e259" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e260" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% BU step of Subspace Pursuit</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e261" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% ordered index set of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e262" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e263" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% auxiliary Bernoulli distribution</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e264" xlink:type="simple"/></inline-formula> number of 1 s on average</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e265" xlink:type="simple"/></inline-formula></td>
                <td align="left" colspan="1" rowspan="1">% weigh by residual's norm</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">to improve distribution</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e266" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">% normalize for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e267" xlink:type="simple"/></inline-formula> to draw</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e268" xlink:type="simple"/></inline-formula> number of 1 s on average</td>
              </tr>
              <tr>
                <td align="left" colspan="2" rowspan="1">end loop</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt103">
              <label/>
              <p>For more details, see technical reports <xref ref-type="bibr" rid="pcbi.1002372-Szita1">[29]</xref> and <xref ref-type="bibr" rid="pcbi.1002372-Lrincz1">[16]</xref>.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>Since the resulting algorithm is not a greedy method, the algorithm is called as Subspace Cross Entropy (SCE) method without the term ‘Pursuit’.</p>
      </sec>
      <sec id="s4d">
        <title>Robust Principal Component Analysis</title>
        <p>An efficient implementation of RPCA algorithm rephrases the optimization problem of (5) by means of the augmented Lagrangian with the following objective function <xref ref-type="bibr" rid="pcbi.1002372-Cands2">[33]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e269" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e270" xlink:type="simple"/></inline-formula> denotes the current residual after subtracting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e271" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e272" xlink:type="simple"/></inline-formula>. The efficiency stems from the fact that both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e273" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e274" xlink:type="simple"/></inline-formula> subproblems have simple solutions. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e275" xlink:type="simple"/></inline-formula> denote <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e276" xlink:type="simple"/></inline-formula>, which can be applied componentwise on matrices. For matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e277" xlink:type="simple"/></inline-formula>, let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e278" xlink:type="simple"/></inline-formula> denote the singular value thresholding operator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e279" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e280" xlink:type="simple"/></inline-formula> is any singular value decomposition. The corresponding pseudocode is given in <xref ref-type="table" rid="pcbi-1002372-t001">Table 1</xref>.</p>
      </sec>
      <sec id="s4e">
        <title>Training data and fitting</title>
        <p>The algorithms were trained on 16×16, normalized (zero mean and 1 std) patches extracted from a public database (<ext-link ext-link-type="uri" xlink:href="http://www.cis.hut.fi/projects/ica/data/images/" xlink:type="simple">http://www.cis.hut.fi/projects/ica/data/images/</ext-link>). For the temporal studies, inputs were generated by concatenating 16 normalized patches of size 8×8 extracted from randomly selected parts of publicly available videos (‘football(b)’, ‘garden’, ‘ice’, ‘tempete’, ‘crowd_run’, ‘sunflower’, ‘tractor’; <ext-link ext-link-type="uri" xlink:href="http://media.xiph.org/video/derf/" xlink:type="simple">http://media.xiph.org/video/derf/</ext-link>). To speed up calculations, batch learning (50000 samples for static stimuli and 25000 samples for the sequences) was applied to learn the low dimensional subspace of RPCA in the preprocessing stage. On the other hand, to learn the over-complete sparse basis (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e281" xlink:type="simple"/></inline-formula>-fold over-completeness), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e282" xlink:type="simple"/></inline-formula> samples have been used. RPCA was run in MATLAB. All other transformations were performed on a cluster of 17 Sony PlayStation 3 consoles in Linux environment using in-house C++ implementation of published algorithms of SVD <xref ref-type="bibr" rid="pcbi.1002372-Golub1">[53]</xref> and CE <xref ref-type="bibr" rid="pcbi.1002372-Rubinstein1">[27]</xref>. The obtained filters were matched with Gabor filters <xref ref-type="bibr" rid="pcbi.1002372-Rodieck1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002372-Jones1">[38]</xref> in order to characterize the spatial structures. The Gabor filter parameters are as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e283" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e284" xlink:type="simple"/><label>(10)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e285" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e286" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e287" xlink:type="simple"/></inline-formula> denote the center of the patch, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e288" xlink:type="simple"/></inline-formula> is the orientation of the normal to the parallel stripes of the Gabor function, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e289" xlink:type="simple"/></inline-formula> is the frequency and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e290" xlink:type="simple"/></inline-formula> is the phase of the cosine factor, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e291" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002372.e292" xlink:type="simple"/></inline-formula> specify the ellipticity of the Gaussian envelope. Fitting was done in MATLAB using the nonlinear least squares optimization function (nsqnonlin(.)) designed for large scale problems. For each parameter value the optimization algorithm was run 20 times with random initialization and the best solution was kept.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>We are grateful to Zoltán Kisvárday for helpful discussions on the properties of the interneuron groups of the primary visual cortex. We thank the reviewers for all valuable suggestions that have greatly abetted the development of these ideas.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002372-Barlow1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>             <year>1961</year>             <article-title>Possible principles underlying the transformation of sensory messages.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group>             <source>Sensory Communication</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>217</fpage>             <lpage>234</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Atick1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name></person-group>             <year>1992</year>             <article-title>Could information theory provide an ecological theory of sensory processing?</article-title>             <source>Network</source>             <volume>3</volume>             <fpage>213</fpage>             <lpage>251</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Dong1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>DW</given-names></name><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name></person-group>             <year>1995</year>             <article-title>Temporal decorrelation: a theory of lagged and nonlagged responses in the lateral geniculate nucleus.</article-title>             <source>Network</source>             <volume>6</volume>             <fpage>159</fpage>             <lpage>178</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Bell1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>1995</year>             <article-title>An information-maximization approach to blind separation and blind deconvolution.</article-title>             <source>Neural Comput</source>             <volume>7</volume>             <fpage>1129</fpage>             <lpage>1159</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Olshausen1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>             <year>1996</year>             <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>607</fpage>             <lpage>609</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Doi1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doi</surname><given-names>E</given-names></name><name name-style="western"><surname>Balcan</surname><given-names>DC</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2007</year>             <article-title>Robust coding over noisy overcomplete channels.</article-title>             <source>IEEE Trans Image Process</source>             <volume>16</volume>             <fpage>442</fpage>             <lpage>452</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Graham1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Graham</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Chandler</surname><given-names>DM</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>             <year>2006</year>             <article-title>Can the theory of “whitening” explain the centersurround properties of retinal ganglion cell receptive fields?</article-title>             <source>Vision Res</source>             <volume>46</volume>             <fpage>2901</fpage>             <lpage>2913</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Ringach1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ringach</surname><given-names>DL</given-names></name></person-group>             <year>2002</year>             <article-title>Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual cortex.</article-title>             <source>J Neurophysiol</source>             <volume>88</volume>             <fpage>455</fpage>             <lpage>463</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-A1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>A</surname><given-names>H</given-names></name><name name-style="western"><surname>Otte</surname><given-names>S</given-names></name><name name-style="western"><surname>Callaway</surname><given-names>E</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>2010</year>             <article-title>Metabolic cost as a unifying principle governing neuronal biophysics.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>107</volume>             <fpage>12329</fpage>             <lpage>12334</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Lennie1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lennie</surname><given-names>P</given-names></name></person-group>             <year>2003</year>             <article-title>The cost of cortical computation.</article-title>             <source>Curr Biol</source>             <volume>13</volume>             <fpage>493</fpage>             <lpage>497</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Laughlin1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>JC</given-names></name></person-group>             <year>1998</year>             <article-title>The metabolic cost of neural information.</article-title>             <source>Nat Neurosci</source>             <volume>1</volume>             <fpage>36</fpage>             <lpage>41</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Berkes1">
        <label>12</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name><name name-style="western"><surname>White</surname><given-names>B</given-names></name><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>No evidence for active sparsification in the visual cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Schuurmans</surname><given-names>D</given-names></name><name name-style="western"><surname>Lafferty</surname><given-names>J</given-names></name><name name-style="western"><surname>Williams</surname><given-names>CKI</given-names></name><name name-style="western"><surname>Culotta</surname><given-names>A</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems 22</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>108</fpage>             <lpage>116</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Fldik1">
        <label>13</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name></person-group>             <year>2002</year>             <article-title>Sparse coding in the primate cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Arbib</surname><given-names>MA</given-names></name></person-group>             <source>The Handbook of Brain Theory and Neural Networks</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press. second edition</publisher-name>             <fpage>1064</fpage>             <lpage>1068</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Doi2">
        <label>14</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doi</surname><given-names>E</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <year>2005</year>             <article-title>Relations between the statistical regularities of natural images and the response properties of the early visual system.</article-title>             <fpage>1</fpage>             <lpage>8</lpage>             <comment>In: Proceedings of the Japanese Cognitive Science Society, Special interest group of Pattern Recognition and Perception Model; 28 July 2005; Kyoto Japan</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Essen1">
        <label>15</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Essen</surname><given-names>DCV</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>C</given-names></name></person-group>             <year>1995</year>             <article-title>Information processing strategies and pathways in the primate retina and visual cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Zornetzer</surname><given-names>SF</given-names></name><name name-style="western"><surname>Davis</surname><given-names>JL</given-names></name><name name-style="western"><surname>Lau</surname><given-names>C</given-names></name></person-group>             <source>Introduction to Neural and Electronic Networks</source>             <publisher-loc>Orlando</publisher-loc>             <publisher-name>Academic Press</publisher-name>             <fpage>45</fpage>             <lpage>76</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Lrincz1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lörincz</surname><given-names>A</given-names></name><name name-style="western"><surname>Palotai</surname><given-names>Z</given-names></name><name name-style="western"><surname>Szirtes</surname><given-names>G</given-names></name></person-group>             <year>2012</year>             <article-title>Sparse and silent coding in neural circuits.</article-title>             <source>Neurocomputing</source>             <volume>79</volume>             <fpage>115</fpage>             <lpage>124</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Widrow1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Widrow</surname><given-names>B</given-names></name><name name-style="western"><surname>Lehr</surname><given-names>MA</given-names></name></person-group>             <year>1990</year>             <article-title>Thirty years of adaptive neural networks: Perceptron, madaline, and backpropagation.</article-title>             <source>Proc IEEE Inst Electr Electron Eng</source>             <volume>78</volume>             <fpage>1415</fpage>             <lpage>1442</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Lrincz2">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lörincz</surname><given-names>A</given-names></name></person-group>             <year>2009</year>             <article-title>Hebbian constraint on the resolution of the Homunculus fallacy leads to a network that searches for hidden cause-effect relationships.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Goertzel</surname><given-names>B</given-names></name><name name-style="western"><surname>Hitzler</surname><given-names>P</given-names></name><name name-style="western"><surname>Hutter</surname><given-names>M</given-names></name></person-group>             <source>2nd Conference on Artificial General Intelligence</source>             <publisher-name>AGI-2009</publisher-name>             <fpage>126</fpage>             <lpage>131</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Natarajan1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Natarajan</surname><given-names>B</given-names></name></person-group>             <year>1995</year>             <article-title>Sparse approximate solutions to linear systems.</article-title>             <source>SIAM J Sci Comput</source>             <volume>24</volume>             <fpage>227</fpage>             <lpage>234</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Tropp1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tropp</surname><given-names>JA</given-names></name></person-group>             <year>2004</year>             <article-title>Greed is good: algorithmic results for sparse approximation.</article-title>             <source>IEEE Trans Inf Theory</source>             <volume>50</volume>             <fpage>2231</fpage>             <lpage>2242</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Needell1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Needell</surname><given-names>D</given-names></name><name name-style="western"><surname>Vershynin</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit.</article-title>             <source>Found Comput Math</source>             <volume>9</volume>             <fpage>317</fpage>             <lpage>334</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Donoho1">
        <label>22</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donoho</surname><given-names>DL</given-names></name><name name-style="western"><surname>Tsaig</surname><given-names>Y</given-names></name><name name-style="western"><surname>Drori</surname><given-names>I</given-names></name><name name-style="western"><surname>Starck</surname><given-names>J</given-names></name></person-group>             <year>2006</year>             <article-title>Sparse solution of underdetermined linear equations by stagewise orthogonal matching pursuit.</article-title>             <comment>Technical Report 2006-02, Stanford University</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Dai1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>W</given-names></name><name name-style="western"><surname>Milenkovic</surname><given-names>O</given-names></name></person-group>             <year>2009</year>             <article-title>Subspace pursuit for compressive sensing signal reconstruction.</article-title>             <source>IEEE Tran Inf Theo</source>             <volume>55</volume>             <fpage>2230</fpage>             <lpage>2249</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Mallat1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mallat</surname><given-names>S</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z</given-names></name></person-group>             <year>1993</year>             <article-title>Matching pursuits with time-frequency dictionaries.</article-title>             <source>IEEE Trans Signal Process</source>             <volume>41</volume>             <fpage>3397</fpage>             <lpage>3415</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cands1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Candès</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Wakin</surname><given-names>M</given-names></name></person-group>             <year>2008</year>             <article-title>An introduction to compressive sampling.</article-title>             <source>IEEE Signal Processing Mag</source>             <volume>25</volume>             <fpage>21</fpage>             <lpage>30</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Pati1">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pati</surname><given-names>YC</given-names></name><name name-style="western"><surname>Rezaiifar</surname><given-names>R</given-names></name><name name-style="western"><surname>Krishnaprasad</surname><given-names>PS</given-names></name></person-group>             <year>1993</year>             <article-title>Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition.</article-title>             <fpage>40</fpage>             <lpage>44</lpage>             <comment>In: Conference Record of The Twenty- Seventh Asilomar Conference on Signals, Systems and Computers; 1–3 November, 1993; Pacific Grove, California, United States</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Rubinstein1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rubinstein</surname><given-names>RY</given-names></name></person-group>             <year>1999</year>             <article-title>The cross-entropy method for combinatorial and continuous optimization.</article-title>             <source>Method Comput Appl Prob</source>             <volume>2</volume>             <fpage>127</fpage>             <lpage>190</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Lrincz3">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lörincz</surname><given-names>A</given-names></name><name name-style="western"><surname>Palotai</surname><given-names>Z</given-names></name><name name-style="western"><surname>Szirtes</surname><given-names>G</given-names></name></person-group>             <year>2008</year>             <article-title>Spike-based cross-entropy method for reconstruction.</article-title>             <source>Neurocomputing</source>             <volume>71</volume>             <fpage>3635</fpage>             <lpage>3639</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Szita1">
        <label>29</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Szita</surname><given-names>I</given-names></name><name name-style="western"><surname>Lörincz</surname><given-names>A</given-names></name></person-group>             <year>2008</year>             <comment>Online variants of the cross-entropy method. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/0801.1988" xlink:type="simple">http://arxiv.org/abs/0801.1988</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Vincent1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vincent</surname><given-names>BT</given-names></name><name name-style="western"><surname>Baddeley</surname><given-names>RJ</given-names></name></person-group>             <year>2003</year>             <article-title>Synaptic energy efficiency in retinal processing.</article-title>             <source>Vision Res</source>             <volume>43</volume>             <fpage>1283</fpage>             <lpage>1290</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Simoncelli1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representation.</article-title>             <source>Annu Rev Neurosci</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Jolliffe1">
        <label>32</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jolliffe</surname><given-names>IT</given-names></name></person-group>             <year>2002</year>             <source>Principal Component Analysis</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name> <!--===== Restructure page-count as size[@units="page"] =====--><size units="page">487</size>           </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cands2">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Candès</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Li</surname><given-names>X</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y</given-names></name><name name-style="western"><surname>Wright</surname><given-names>J</given-names></name></person-group>             <year>2011</year>             <article-title>Robust principal component analysis?</article-title>             <source>J Assoc Comp Mach</source>             <volume>58</volume>             <fpage>1</fpage>             <lpage>37</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Zhou1">
        <label>34</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style="western"><surname>Wright</surname><given-names>J</given-names></name><name name-style="western"><surname>Li</surname><given-names>X</given-names></name><name name-style="western"><surname>Candès</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Y</given-names></name></person-group>             <year>2010</year>             <article-title>Stable principal component pursuit.</article-title>             <fpage>1518</fpage>             <lpage>1522</lpage>             <comment>In: Proceedings of IEEE International Symposium on Information Theory (ISIT 2010); 13–18 June 2010; Austin, Texas, United States</comment>             <comment>DOI:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/ISIT.2010.5513535" xlink:type="simple">10.1109/ISIT.2010.5513535</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cands3">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Candès</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Recht</surname><given-names>B</given-names></name></person-group>             <year>2008</year>             <article-title>Exact matrix completion via convex optimization.</article-title>             <source>Found Comput Math</source>             <volume>9</volume>             <fpage>717</fpage>             <lpage>772</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Rodieck1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rodieck</surname><given-names>RW</given-names></name></person-group>             <year>1965</year>             <article-title>Quantitative analysis of cat retinal ganglion cell response to visual stimuli.</article-title>             <source>Vision Res</source>             <volume>5</volume>             <fpage>583</fpage>             <lpage>601</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cai1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cai</surname><given-names>D</given-names></name><name name-style="western"><surname>DeAngelis</surname><given-names>GC</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>RD</given-names></name></person-group>             <year>1997</year>             <article-title>Spatiotemporal receptive field organization in the lateral geniculate nucleus of cats and kittens.</article-title>             <source>J Neurophysiol</source>             <volume>78</volume>             <fpage>1045</fpage>             <lpage>1061</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Jones1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>JP</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>L</given-names></name></person-group>             <year>1987</year>             <article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex.</article-title>             <source>J Neurophysiol</source>             <volume>58</volume>             <fpage>1233</fpage>             <lpage>1258</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Lcke1">
        <label>39</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lücke</surname><given-names>J</given-names></name></person-group>             <year>2007</year>             <article-title>A dynamical model for receptive field self-organization in V1 cortical columns.</article-title>             <fpage>389</fpage>             <lpage>398</lpage>             <comment>In: Proceedings of International Conference of Artificial Neural Networks, 13–17 September 2007; Porto Portugal. Springer. LNCS 4669</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Rehn1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rehn</surname><given-names>M</given-names></name><name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name></person-group>             <year>2007</year>             <article-title>A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields.</article-title>             <source>J Comput Neurosci</source>             <volume>22</volume>             <fpage>135</fpage>             <lpage>146</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-DeAngelis1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>DeAngelis</surname><given-names>GC</given-names></name><name name-style="western"><surname>Ohzawa</surname><given-names>I</given-names></name><name name-style="western"><surname>Freeman</surname><given-names>RD</given-names></name></person-group>             <year>1995</year>             <article-title>Receptive-field dynamics in the central visual pathways.</article-title>             <source>Trends in Neurosci</source>             <volume>18</volume>             <fpage>451</fpage>             <lpage>458</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Szatmry1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Szatmáry</surname><given-names>B</given-names></name><name name-style="western"><surname>Lörincz</surname><given-names>A</given-names></name></person-group>             <year>2001</year>             <article-title>Independent component analysis of temporal sequences subject to constraints by LGN inputs yields all the three major cell types of the primary visual cortex.</article-title>             <source>J Comput Neurosci</source>             <volume>11</volume>             <fpage>241</fpage>             <lpage>248</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Olshausen2">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name></person-group>             <year>2002</year>             <article-title>Sparse Codes and Spikes.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name></person-group>             <source>Probabilistic Models of the Brain: Perception and Neural Function</source>             <publisher-name>MIT Press</publisher-name>             <fpage>257</fpage>             <lpage>272</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Chennubhotla1">
        <label>44</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chennubhotla</surname><given-names>C</given-names></name><name name-style="western"><surname>Jepson</surname><given-names>AD</given-names></name></person-group>             <year>2001</year>             <article-title>Sparse PCA: Extracting multi-scale structure from data.</article-title>             <comment>In: Proceedings of IEEE International Conference on Computer Vision; 1: 641–647; 7–14 July, 2001; Vancouver, British Columbia, Canada</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Riesenhuber1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>             <year>1999</year>             <article-title>Hierarchical models of object recognition in cortex.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>1019</fpage>             <lpage>1025</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cadieu1">
        <label>46</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cadieu</surname><given-names>C</given-names></name><name name-style="western"><surname>Olshausen</surname><given-names>B</given-names></name></person-group>             <year>2009</year>             <article-title>Learning transformational invariants from natural movies.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Koller</surname><given-names>D</given-names></name><name name-style="western"><surname>Schuurmans</surname><given-names>D</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems 21</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>209</fpage>             <lpage>216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Cardin1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cardin</surname><given-names>JA</given-names></name><name name-style="western"><surname>Palmer</surname><given-names>LA</given-names></name><name name-style="western"><surname>Contreras</surname><given-names>D</given-names></name></person-group>             <year>2007</year>             <article-title>Stimulus feature selectivity in excitatory and inhibitory neurons in primary visual cortex.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>10333</fpage>             <lpage>10344</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Liu1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>B</given-names></name><name name-style="western"><surname>Li</surname><given-names>P</given-names></name><name name-style="western"><surname>Li</surname><given-names>Y</given-names></name><name name-style="western"><surname>Sun</surname><given-names>YJ</given-names></name><name name-style="western"><surname>Yanagawa</surname><given-names>Y</given-names></name></person-group>             <year>2009</year>             <article-title>Visual receptive field structure of cortical inhibitory neurons revealed by two-photon imaging guided recording.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>10520</fpage>             <lpage>10532</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Needell2">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Needell</surname><given-names>D</given-names></name><name name-style="western"><surname>Tropp</surname><given-names>JA</given-names></name></person-group>             <year>2008</year>             <article-title>Cosamp: Iterative signal recovery from incomplete and inaccurate samples.</article-title>             <source>Appl Computational Harmon Anal</source>             <volume>26</volume>             <fpage>301</fpage>             <lpage>321</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Muehlenbein1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Muehlenbein</surname><given-names>H</given-names></name></person-group>             <year>1998</year>             <article-title>The equation for response to selection and its use for prediction.</article-title>             <source>Evol Comput</source>             <volume>5</volume>             <fpage>303</fpage>             <lpage>346</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Olshausen3">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name></person-group>             <year>1997</year>             <article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title>             <source>Vision Res</source>             <volume>37</volume>             <fpage>3311</fpage>             <lpage>3325</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-deBoer1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Boer</surname><given-names>PT</given-names></name><name name-style="western"><surname>Kroese</surname><given-names>DP</given-names></name><name name-style="western"><surname>Mannor</surname><given-names>S</given-names></name><name name-style="western"><surname>Rubinstein</surname><given-names>RY</given-names></name></person-group>             <year>2004</year>             <article-title>A tutorial on the cross-entropy method.</article-title>             <source>Ann Oper Res</source>             <volume>134</volume>             <fpage>19</fpage>             <lpage>67</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002372-Golub1">
        <label>53</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Golub</surname><given-names>GH</given-names></name><name name-style="western"><surname>Loan</surname><given-names>CV</given-names></name></person-group>             <year>1996</year>             <source>Matrix Computation, 3rd edition</source>             <publisher-loc>Baltimore</publisher-loc>             <publisher-name>Johns Hopkins University Press</publisher-name>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>