<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01450</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006968</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological processes</subject><subj-group><subject>Sleep</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological processes</subject><subj-group><subject>Sleep</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Markov models</subject><subj-group><subject>Hidden Markov models</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Muscle electrophysiology</subject><subj-group><subject>Electromyography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Animal management</subject><subj-group><subject>Animal performance</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>SPINDLE: End-to-end learning from EEG/EMG to extrapolate animal sleep scoring across experimental settings, labs and species</article-title>
<alt-title alt-title-type="running-head">Extrapolating animal sleep scoring across experimental domains</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4773-3573</contrib-id>
<name name-style="western">
<surname>Miladinović</surname> <given-names>Đorđe</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1010-2516</contrib-id>
<name name-style="western">
<surname>Muheim</surname> <given-names>Christine</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bauer</surname> <given-names>Stefan</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Spinnler</surname> <given-names>Andrea</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Noain</surname> <given-names>Daniela</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4608-1374</contrib-id>
<name name-style="western">
<surname>Bandarabadi</surname> <given-names>Mojtaba</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7906-4714</contrib-id>
<name name-style="western">
<surname>Gallusser</surname> <given-names>Benjamin</given-names></name>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9794-2480</contrib-id>
<name name-style="western">
<surname>Krummenacher</surname> <given-names>Gabriel</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Baumann</surname> <given-names>Christian</given-names></name>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Adamantidis</surname> <given-names>Antoine</given-names></name>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Brown</surname> <given-names>Steven A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Buhmann</surname> <given-names>Joachim M.</given-names></name>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, ETH Zurich, Zürich, Switzerland</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Max Planck Institute for Intelligent Systems, Tübingen, Germany</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Chronobiology and Sleep Research Group, University of Zurich, Zürich, Switzerland</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Biomedical Sciences, Washington State University, Spokane, Washington, United States of America</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Neurology, University Hospital Zurich, Zürich, Switzerland</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Department of Neurology, University of Bern, Bern, Switzerland</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Winsky-Sommerer</surname> <given-names>Raphaelle</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Surrey, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">djordjem@ethz.ch</email> (ĐM); <email xlink:type="simple">steven.brown@pharma.uzh.ch</email> (SAB)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>18</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>4</issue>
<elocation-id>e1006968</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>3</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Miladinović et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006968"/>
<abstract>
<p>Understanding sleep and its perturbation by environment, mutation, or medication remains a central problem in biomedical research. Its examination in animal models rests on brain state analysis via classification of electroencephalographic (EEG) signatures. Traditionally, these states are classified by trained human experts by visual inspection of raw EEG recordings, which is a laborious task prone to inter-individual variability. Recently, machine learning approaches have been developed to automate this process, but their generalization capabilities are often insufficient, especially across animals from different experimental studies. To address this challenge, we crafted a convolutional neural network-based architecture to produce domain invariant predictions, and furthermore integrated a hidden Markov model to constrain state dynamics based upon known sleep physiology. Our method, which we named SPINDLE (Sleep Phase Identification with Neural networks for Domain-invariant LEearning) was validated using data of four animal cohorts from three independent sleep labs, and achieved average agreement rates of 99%, 98%, 93%, and 97% with scorings from five human experts from different labs, essentially duplicating human capability. It generalized across different genetic mutants, surgery procedures, recording setups and even different species, far exceeding state-of-the-art solutions that we tested in parallel on this task. Moreover, we show that these scored data can be processed for downstream analyzes identical to those from human-scored data, in particular by demonstrating the ability to detect mutation-induced sleep alteration. We provide to the scientific community free usage of SPINDLE and benchmarking datasets as an online server at <ext-link ext-link-type="uri" xlink:href="https://sleeplearning.ethz.ch" xlink:type="simple">https://sleeplearning.ethz.ch</ext-link>. Our aim is to catalyze high-throughput and well-standardized experimental studies in order to improve our understanding of sleep.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Machine learning-based approaches hold great promise to pave the way for high-throughput animal sleep monitoring. With the novel developments of gene-engineering techniques and the proliferation of experimental sleep studies, the need for the automation and cross-lab standardization of sleep scoring becomes more imminent. Traditionally, the classification of electroencephalographic (EEG) signatures is done by trained human experts via visual inspection. Here we present a novel algorithm based upon neural networks to automatically generate accurate and physiologically plausible predictions. Performed experiments demonstrate that the proposed solution offers de facto human level performance, is more accurate than any other approach to date (93-99% accurate compared to multiple trained human scorers), and functions across different genetic mutants, surgery procedures, recording setups and even different species. Moreover, our method was capable of detecting mutation-induced changes in sleeping patterns. To allow for its widespread adaptation, we make our framework freely available through the provision of an online server and an easy to use interface. This community tool will both contribute to the standardization of experimental studies and enhance scientific understanding of sleep.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="14"/>
<table-count count="4"/>
<page-count count="30"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-04-30</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data underlying the results presented in the study are available at <ext-link ext-link-type="uri" xlink:href="https://sleeplearning.ethz.ch/" xlink:type="simple">https://sleeplearning.ethz.ch/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology</italic> Methods paper.</p>
</disp-quote>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The importance of sleep in humans and animals is a widely studied and intriguing topic in medical research [<xref ref-type="bibr" rid="pcbi.1006968.ref001">1</xref>]. Across all organisms with neurons—from Aplysia “sea slugs” to man—sleep-like states can be identified [<xref ref-type="bibr" rid="pcbi.1006968.ref002">2</xref>], and in mammalian phyla these basic states share characteristic synchronous neuronal oscillations accompanied by partial or total cessation of motor activity. Until today, electroencephalogram and electromyogram (EEG and EMG) recordings still provide the most accurate data to describe and monitor sleep and wake. At least three major vigilance states can be identified: wake (with low-amplitude high-frequency beta and gamma EEG oscillation between 15-30 Hz and 30-100Hz, as well as extensive EMG activity), non-rapid eye movement sleep (NREM, characterized by large-amplitude delta EEG waves 0.1-4Hz and low or no EMG activity), and rapid eye movement sleep (REM, with predominant theta activity between 6-9Hz and low EMG activity). The relative abundance of these states is governed by both an endogenous 24-hour circadian clock consolidating sleep mostly to day or night, and a sleep homeostat that directs sleep in proportion to the intensity and duration of prior waking experience [<xref ref-type="bibr" rid="pcbi.1006968.ref003">3</xref>].</p>
<p>Both for the understanding of sleep itself and to study the pathological significance of altered sleep, the identification of individual episodes of sleep and wake across the day based upon EEG/EMG recordings represents a crucial first step. For animals such as rodents EEG/EMG are normally first segmented into 1-8 sec epochs, and are then epoch-by-epoch categorized into the three cardinal vigilance states. Based on the derived scorings, sleep researchers are able to describe the dynamic regulation of sleep, its evolution over time, and the intensity of various EEG oscillations in each vigilance state (its “spectral composition”). Traditionally, this initial classification has been done manually. However, visual inspection-based sleep scoring is a laborious and ambiguous process that requires constant focus of well-trained human experts. Manual sleep scoring is also highly prone to inter-individual variability, and 90-95% agreement in distinguishing vigilance states is usual across human experts (as our study suggests). This data annotation bias is a potential source of inconsistencies between experimental sleep studies.</p>
<p>In past years, numerous approaches have been developed with the aim of automating sleep scoring procedures for human and non-human species. Classical methods transform epochs into hand-designed feature vectors that enable simple discrimination between vigilance states. Manual feature extraction from EEG/EMG is usually based on the domain knowledge already applied in visual sleep scoring. Most popularly, feature vectors are formed from energies of standard frequency bands of EEG power spectrum, such as delta <italic>δ</italic>(0.1–4Hz), theta <italic>θ</italic>(6–9Hz), sigma <italic>σ</italic>(10–15Hz) and beta <italic>β</italic>(15–30Hz) [<xref ref-type="bibr" rid="pcbi.1006968.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref007">7</xref>], or sometimes more fine-grained binning of the spectrum is applied [<xref ref-type="bibr" rid="pcbi.1006968.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref010">10</xref>]. Alternatively, features are extracted directly from the temporal domain [<xref ref-type="bibr" rid="pcbi.1006968.ref008">8</xref>]. Various machine learning methods are then employed to learn to map derived features to vigilance states i.e. to score sleep. Depending on how these methods utilize annotated data, learning procedures are performed in a supervised [<xref ref-type="bibr" rid="pcbi.1006968.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref009">9</xref>] or unsupervised [<xref ref-type="bibr" rid="pcbi.1006968.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref010">10</xref>] fashion.</p>
<p>In a wider context, following the first groundbreaking applications of deep neural networks (DNN) to audio [<xref ref-type="bibr" rid="pcbi.1006968.ref011">11</xref>] and image [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>] modeling, DNNs have been very successfully applied to a sequence of related real-life time-series classification tasks such as video activity recognition [<xref ref-type="bibr" rid="pcbi.1006968.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref014">14</xref>], text classification [<xref ref-type="bibr" rid="pcbi.1006968.ref015">15</xref>] and automatic speech recognition (ASR) [<xref ref-type="bibr" rid="pcbi.1006968.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref016">16</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref018">18</xref>]. The latter example provides a particularly pertinent analogy to EEG sleep classification: instead of mapping audio to a sequence of words, one maps EEG/EMG to a sequence of vigilance states. Both problems encounter similar challenges related to subject and environmental changes. While in sleep scoring different animals exhibit different oscillatory activity patterns, in ASR audio patterns differ across speakers due to different accents, noise level, recording device or other voice characteristics.</p>
<p>Generalizing from these other domains, primarily from harvesting the discriminative power of end-to-end DNN architectures, another class of sleep scoring methods has recently emerged. DNNs are either trained on top of manually extracted features [<xref ref-type="bibr" rid="pcbi.1006968.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref019">19</xref>] or the discriminative features are via end-to-end training learned directly from raw EEG/EMG [<xref ref-type="bibr" rid="pcbi.1006968.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref022">22</xref>] or their time-frequency transforms [<xref ref-type="bibr" rid="pcbi.1006968.ref023">23</xref>]. Popular deep architectures include convolutional neural networks (CNNs) [<xref ref-type="bibr" rid="pcbi.1006968.ref022">22</xref>], recurrent neural networks (RNNs) [<xref ref-type="bibr" rid="pcbi.1006968.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref019">19</xref>], the combination of the two [<xref ref-type="bibr" rid="pcbi.1006968.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref024">24</xref>], or deep belief networks (DBNs) [<xref ref-type="bibr" rid="pcbi.1006968.ref020">20</xref>] which additionally involve unsupervised pre-training procedure.</p>
<p>Albeit undoubtedly useful to sleep researchers for (semi-)automated sleep scoring, the prediction accuracy of existing methods is still not equal to that of human experts. Even more importantly, current animal sleep scoring solutions have major difficulties to generalize pattern recognition across animals from different experimental settings and labs. Cross-subject variations in EEG/EMG sleep patterns normally originate from experimental differences such as signal-to-noise ratio, EEG derivation and electrode placement, genetic background, drug application, disease models, or differing lab strains and animal species e.g. rats vs. mice [<xref ref-type="bibr" rid="pcbi.1006968.ref025">25</xref>]. Due to these signal variabilities, designing robust features by hand is an extremely challenging task. Contrary to approaches attempting to manually construct consistent features, end-to-end architectures have potential to learn robust discriminative features de novo. However, to our knowledge, end-to-end learning frameworks were applied only in the context of human sleep, and furthermore reported solutions showed no significant improvements over the classical hand-derived feature based methods [<xref ref-type="bibr" rid="pcbi.1006968.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref021">21</xref>].</p>
<p>In this paper, we have devised a novel framework—SPINDLE (Sleep Phase Identification with Neural networks for Domain-invariant LEearning) drawing some inspiration from the traditional hybrid DNN-HMM models [<xref ref-type="bibr" rid="pcbi.1006968.ref011">11</xref>] which combine deep neural networks (DNN) and hidden Markov models (HMM) in automatic speech recognition (ASR) [<xref ref-type="bibr" rid="pcbi.1006968.ref011">11</xref>]. In particular, we operated on time-frequency domain and used similar preprocessing procedures to the ones used for extracting Mel frequency cepstrum coefficients (MFCC) [<xref ref-type="bibr" rid="pcbi.1006968.ref017">17</xref>], generalized to multiple heterogeneous channels. Our approach to solving time-frequency fluctuations using a CNN is also motivated by similar modeling ideas in ASR for dealing with speaker and environmental variability [<xref ref-type="bibr" rid="pcbi.1006968.ref026">26</xref>]. Finally, we used a hidden Markov model (HMM) to describe vigilance state dynamics and suppress physiologically impossible transitions, similarly to DNN-HMM ASR systems which constrain the output space alleviating infeasible language constructions. SPINDLE overcomes aforementioned performance and generalization issues in automatic sleep scoring for animals: the model was trained only on two wildtype mice, and was then evaluated on 12 mice and 8 rats from four animal cohorts of three independent labs, rendering accuracies of 99%, 98%, 93% and 97% in signal areas where human experts agreed. When compared to the individual human experts, SPINDLE showed practically equal agreement rate to the one human experts had between themselves, both for artifacts and vigilance states. Specifically, our main contributions are:</p>
<list list-type="order">
<list-item>
<p>Through the spectral profile analysis we explain why current solutions do not generalize well across experimental domains without additional parameter calibration. This severely limits their applicability. We then elaborate why the proposed CNN-HMM architecture is suitable for this task.</p>
</list-item>
<list-item>
<p>We present SPINDLE—a novel ASR-inspired computational method for fast, accurate and physiologically plausible sleep scoring in animals. In terms of predictive performance, our method is vastly superior to existing ones and is fully comparable to trained human experts.</p>
</list-item>
<list-item>
<p>We disclose a diverse double scored data set (14 mice and 8 rats, annotated by five human experts, rendering 950.400 labels in total) assembled from EEG/EMG recordings produced in three separate sleep labs. The data may be used in the future for benchmarking purposes.</p>
</list-item>
<list-item>
<p>By validating the performance of SPINDLE on the collected data, we demonstrate that without any re-training or fine tunning, our model achieves high predictive accuracy on subjects with different EEG/EMG characteristics, originating from different experimental conditions, labs, and even species.</p>
</list-item>
<list-item>
<p>Furthermore, we show that SPINDLE is capable of identifying statistically significant alterations of sleep patterns. We illustrate this by comparing a wild and the corresponding genetically modified mice strain, deriving the same conclusions as scores by two independent human experts.</p>
</list-item>
<list-item>
<p>We contribute with a publicly available free web service for simple and quick classification of EEG/EMG animal recordings.</p>
</list-item>
</list>
<sec id="sec002">
<title>SPINDLE method overview</title>
<p>The SPINDLE method presented here (sketched in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>) is designed to achieve high predictive performance preserved across different experimental settings and labs. Its architecture is carefully crafted in an end-to-end fashion around a convolutional neural network (CNN) which operates on top of the preprocessed time-frequency channels of EEG/EMG. We exploit the ability of the CNN to learn highly discriminative and translation-invariant features, as this allows us to remain agnostic to changes in sleep patterns, in both time and frequency dimension. On top of the CNN, a hidden Markov model (HMM) describes vigilance state transition dynamics and suppresses physiologically infeasible vigilance state transitions when applicable. To account for artifacts, SPINDLE contains an additional CNN with binary output: artifact or non-artifact, which is combined with the predictions of vigilance states to determine the artifact types (steps (d) and (g) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref> respectively). For a detailed explanation, please refer to the Materials and Methods section further below.</p>
<fig id="pcbi.1006968.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Conceptual overview of the SPINDLE framework.</title>
<p><bold>(a)</bold> Measured EEG activity may vary depending on where the electrodes are placed. Assumed input in our setting are two EEG channels and one EMG channel. EMG signal is recorded on the neck muscle (not depicted for simplicity). <bold>(b)</bold> Raw signals are processed by windowed Fourier transforms applied on overlapping frames. The output of the preprocessing are time-frequency representations of EEG/EMG which are additionally preprocessed. <bold>(c)</bold> Three two-dimensional spectrograms are then sectioned into epochs which correspond to 4 sec intervals. Each epoch is independently processed by the two CNNs. <bold>(d)</bold> The first CNN estimates whether the evaluated epoch is an artifact. <bold>(e)</bold> The second CNN estimates the probability of each vigilance state. <bold>(f)</bold> The sequence of estimated vigilance state probabilities is then corrected using the Viterbi decoding algorithm and predetermined transition matrix of HMM which encodes the transition rules. If an epoch is not designated as an artifact, the most probable vigilance state is assigned. <bold>(g)</bold> If an epoch is marked as an artifact, the most probable vigilance state determines the type of the artifact: WAKE-artifact, NREM-artifact or REM-artifact. NREM/N, non-rapid eye movement; REM/R, rapid eye movement; WAKE/W, wakefulness; CNN, convolutional neural network; HMM hidden Markov model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g001" xlink:type="simple"/>
</fig>
<p>SPINDLE was tested on data produced in three independent sleep labs: BrownLab (<ext-link ext-link-type="uri" xlink:href="http://www.sbrownlab.com" xlink:type="simple">www.sbrownlab.com</ext-link>), TidisLab (<ext-link ext-link-type="uri" xlink:href="http://tidis-lab.org/" xlink:type="simple">http://tidis-lab.org/</ext-link>) and BaumannLab (<ext-link ext-link-type="uri" xlink:href="http://www.sleep.uzh.ch/en/research-groups/group-baumann.html" xlink:type="simple">http://www.sleep.uzh.ch/en/research-groups/group-baumann.html</ext-link>). The collected data consists of a number of rodent EEG/EMG recordings acquired during sleep studies performed with varying experimental paradigms. The recordings were clustered into four animal cohorts with similar characteristics as summarized in <xref ref-type="table" rid="pcbi.1006968.t001">Table 1</xref>, and evaluated separately.</p>
<table-wrap id="pcbi.1006968.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.t001</object-id>
<label>Table 1</label>
<caption>
<title>Collected data overview.</title>
<p>Presented are the notable properties of EEG/EMG animal recordings produced in our study. All recordings were segmented into 4 sec time intervals (epochs) and then annotated rendering 21600 × 2 labels per animal. Table columns for each cohort and lab depict: (a) the number of wildtypes; (b) the number of mutants; (c) rodent specie; (d) the sampling rate of the recording device; (e) the derivation of 2 EEG signals with respect to the placement of corresponding EEG electrodes; (f) the derivation of EMG signal; (g) the number of human experts who scored the data; (h) the duration of each animal recording within given cohort; and lastly (i) the degree of signal corruption taken as the average percentage of artifacts computed from the scorings of the corresponding experts. The cohort C was scored by an expert from BaumannLab, as well as by an expert from BrownLab. All other cohorts were scored by experts from the same lab. Data acquisition is for each animal cohort explained in detail in Materials and Methods.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006968.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Cohort</th>
<th align="center">Lab</th>
<th align="center">Wild</th>
<th align="center">Mutant</th>
<th align="center">Specie</th>
<th align="center">S.Rate</th>
<th align="center">EEG Derivations</th>
<th align="center">EMG Derivation</th>
<th align="center">Experts</th>
<th align="center">Duration</th>
<th align="center">Artifacts</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="background-color:#EBEBEB">A</td>
<td align="center" style="background-color:#EBEBEB">BrownLab</td>
<td align="center" style="background-color:#EBEBEB">4</td>
<td align="center" style="background-color:#EBEBEB">0</td>
<td align="center" style="background-color:#EBEBEB">mice</td>
<td align="center" style="background-color:#EBEBEB">128Hz</td>
<td align="center" style="background-color:#EBEBEB">1 frontal, 1 parietal</td>
<td align="center" style="background-color:#EBEBEB">neck</td>
<td align="center" style="background-color:#EBEBEB">2</td>
<td align="center" style="background-color:#EBEBEB">24h</td>
<td align="center" style="background-color:#EBEBEB">15.2%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">B</td>
<td align="center" style="background-color:#D2D2D2">BrownLab</td>
<td align="center" style="background-color:#D2D2D2">0</td>
<td align="center" style="background-color:#D2D2D2">4</td>
<td align="center" style="background-color:#D2D2D2">mice</td>
<td align="center" style="background-color:#D2D2D2">128Hz</td>
<td align="center" style="background-color:#D2D2D2">1 frontal, 1 parietal</td>
<td align="center" style="background-color:#D2D2D2">neck</td>
<td align="center" style="background-color:#D2D2D2">2</td>
<td align="center" style="background-color:#D2D2D2">24h</td>
<td align="center" style="background-color:#D2D2D2">19.2%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">C</td>
<td align="center" style="background-color:#EBEBEB">BaumannLab</td>
<td align="center" style="background-color:#EBEBEB">8</td>
<td align="center" style="background-color:#EBEBEB">0</td>
<td align="center" style="background-color:#EBEBEB">rats</td>
<td align="center" style="background-color:#EBEBEB">200Hz</td>
<td align="center" style="background-color:#EBEBEB">2 parietals</td>
<td align="center" style="background-color:#EBEBEB">neck</td>
<td align="center" style="background-color:#EBEBEB">1+ 1</td>
<td align="center" style="background-color:#EBEBEB">24h</td>
<td align="center" style="background-color:#EBEBEB">21.3%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">D</td>
<td align="center" style="background-color:#D2D2D2">TidisLab</td>
<td align="center" style="background-color:#D2D2D2">6</td>
<td align="center" style="background-color:#D2D2D2">0</td>
<td align="center" style="background-color:#D2D2D2">mice</td>
<td align="center" style="background-color:#D2D2D2">512Hz</td>
<td align="center" style="background-color:#D2D2D2">1 frontal, 1 parietal</td>
<td align="center" style="background-color:#D2D2D2">neck</td>
<td align="center" style="background-color:#D2D2D2">2</td>
<td align="center" style="background-color:#D2D2D2">24h</td>
<td align="center" style="background-color:#D2D2D2">≈0%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec003" sec-type="results">
<title>Results</title>
<sec id="sec004">
<title>Analysis of scoring variability across human experts</title>
<p>One of the major issues of visual inspection is the intrinsic subjectivity of human experts in data annotation, especially in ambiguous cases when signal patterns do not clearly adhere to the predefined scoring rules. For example, during the transition between vigilance states, it is not always clear where the actual state change occurs. Taking this into consideration is particularly relevant for the validation of an automated sleep scoring method. To this end, we analyzed the inter-expert scoring agreement in evaluation of identical EEG/EMG data (see <xref ref-type="fig" rid="pcbi.1006968.g002">Fig 2</xref>). To estimate the coherence between human experts, we first measured their agreement in regions that no expert identified as “artifacts”—EEG/EMG perturbations related to environmental interference rather than changes in brain state. We then computed the accuracy from the corresponding 3 × 3 vigilance state submatrices from <xref ref-type="fig" rid="pcbi.1006968.g002">Fig 2</xref>. When comparing human experts from the same lab, the estimated agreement rate of sleep scoring in non-artifact data was 95-96%, while the inter-lab agreement was about 90%. On the other hand, the disagreement between human experts in classification of artifacts was notably higher, as the figure indicates. To measure this, we calculated the ratio between the number of epochs marked as corrupted by both experts and the number of epochs marked as corrupted by at least one of the two experts:
<disp-formula id="pcbi.1006968.e001"><alternatives><graphic id="pcbi.1006968.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mo>_</mml:mo> <mml:mi>s</mml:mi> <mml:mi>c</mml:mi> <mml:mi>o</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>g</mml:mi> <mml:mo>_</mml:mo> <mml:mi>a</mml:mi> <mml:mi>g</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>e</mml:mi> <mml:mi>m</mml:mi> <mml:mi>e</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mo>_</mml:mo> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>f</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mo>_</mml:mo> <mml:mi>u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>i</mml:mi> <mml:mi>o</mml:mi> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mi>e</mml:mi> <mml:mi>r</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
These numbers provide rough estimates of the expected accuracy bounds of a hypothetical sleep scoring method comparable to human experts in terms of the predictive performance.</p>
<fig id="pcbi.1006968.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Intra-lab and inter-lab human expert agreement.</title>
<p>Confusion matrices derived from the twofold annotation procedure of EEG/EMG data with number of common epochs shown at each intersection, and overall percentage agreement calculated above. We evaluated the agreement of human experts from the same lab (intra-lab agreement), but we also compared the scorings of a BrownLab human expert with the scorings of a BaumannLab human expert on the cohort C (inter-lab agreement). The agreements were computed per-cohort, for non-artifact and artifact data separately, and again when taking all epochs into account.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Analysis and adaptation to variability of spectral profiles</title>
<p>To identify the key obstacles towards robust cross-subject sleep classification, we analyzed the fluctuations of EEG in epochs classified as belonging to the same vigilance state. In the context of our problem, ideally, for each vigilance state we would have signal patterns which are consistent (i) across epochs within the same subject; (ii) across subjects within the same animal cohort; (iii) across subjects from different animal cohorts analyzed under different experimental conditions. To explore the variability across these categories, for each animal and for each vigilance state we separately averaged EEG frequency spectra over all epochs, and then compared these measures within and across cohorts (<xref ref-type="fig" rid="pcbi.1006968.g003">Fig 3</xref>). Whether we applied coarse-grained histogram binning according to the commonly used frequency bands (<xref ref-type="fig" rid="pcbi.1006968.g003">Fig 3</xref>, middle column) or finer binning (<xref ref-type="fig" rid="pcbi.1006968.g003">Fig 3</xref>, right column), spectral energy was differently distributed among frequency bands for different animal cohorts. For example, even though the figure indicates the existence of certain patterns in sleep state signatures i.e. a prominent peak at around 7Hz characteristic of REM sleep, this cannot be simply interpreted as a rule due to high cross-epoch, cross-animal and cross-cohort variabilities [<xref ref-type="bibr" rid="pcbi.1006968.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref027">27</xref>]. This is arguably the main reason why the classical methods which base their features on energies of different frequency bands of power spectrum do not generalize well. The feature vectors of equal vigilance states are highly inconsistent across subjects, especially if animals have significantly different backgrounds.</p>
<fig id="pcbi.1006968.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Spectral profiles.</title>
<p>For each animal, the averaged frequency spectrum of the EEG recording (its spectral profile) was computed per vigilance class. Each plot in the left column is related to one of the 4 animal cohorts and consists of the mean spectral profile curve and the corresponding standard deviation (a half of it). All curves are normalized relative to the total power of the signal. The middle and the right column respectively represent coarse-grained (following the classical delta, theta, sigma and beta bands) and fine-grained histogram binning applied to the raw spectral profiles, with bars representing summed spectral power in each bin.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g003" xlink:type="simple"/>
</fig>
<p>To overcome these variations, SPINDLE employs a preprocessing procedure to increase the consistency of spectral patterns within the samples of the same vigilance class. The effects of preprocessing are illustrated in <xref ref-type="fig" rid="pcbi.1006968.g004">Fig 4</xref>. On one hand, the log transformation attenuates the discrepancies in magnitudes, and on the other the typical zero mean/unit variance standardization emphasizes the differences between vigilance states. The core characteristic of SPINDLE, however, is its ability to adapt to the variations of sleep state pattern variations in the frequency axis. This flexibility is achieved through translational invariance, an intrinsic property of CNNs. Whenever the frequency spectrum of evaluated data sample deviates from a hypothetically expected spectral pattern in terms of small shifts of relevant peaks, the CNN absorbs these shifts through the convolutional and max-pooling layers (see <xref ref-type="sec" rid="sec020">Materials and methods</xref>).</p>
<fig id="pcbi.1006968.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Effects of preprocessing to spectral profiles.</title>
<p>Top row shows per vigilance state spectral profiles normalized relative to the total signal power, but only after the log transformation was applied. Relative differences in amplitudes between cohorts are attenuated (compared to <xref ref-type="fig" rid="pcbi.1006968.g003">Fig 3</xref>). Bottom row shows log transformed curves after being per frequency component standardized to emphasize the differences between vigilance states.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>SPINDLE—Qualitative analysis</title>
<p>Before providing a rigorous statistical performance evaluation of SPINDLE, we illustrate its general applicability in <xref ref-type="fig" rid="pcbi.1006968.g005">Fig 5</xref>, where we visually compare the scorings of two human experts from different sleep labs with the predictions of our method on identical portion of an EEG/EMG recording from the cohort C. The figure shows that the agreement between the predictions of SPINDLE and the corresponding experts is visually appealing and also sheds light on some common sources of disagreements in the sleep scoring procedure. When vigilance state is constant, the predictions are mainly in agreement, but during the transitions between vigilance states, disagreements are frequent both between human expert scorers and between human and automatically generated scorings. The figure also shows that artifacts are another common source of disagreements. Finally, it is illustrated why time-frequency representation is useful for understanding sleep dynamics i.e. it is easy to notice the correlation between the spectral patterns in the spectrogram and the corresponding vigilance states.</p>
<fig id="pcbi.1006968.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Qualitative analysis of SPINDLE.</title>
<p>150 epochs were extracted from an animal from the cohort C, where we found the automated classification to be more challenging, to qualitatively compare the predictions of SPINDLE to the scorings of two experts from different labs. The first three signals from top represent the input, two EEG and an EMG. The spectrogram in the middle is a time-frequency representation of one of the EEG signals. The bottom three plots are the hypnograms, the first one derived from the scorings of one human expert, the second one derived from the predictions of SPINDLE and the third one derived from the scorings of the other human expert.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>SPINDLE—Quantitative analysis</title>
<p>In a comprehensive quantitative study we evaluated different performance aspects of SPINDLE. For this purpose, the data set (previously summarized in <xref ref-type="table" rid="pcbi.1006968.t001">Table 1</xref>) was separated into training and testing subsets. There was no overlap between training and testing sets in any of our experiments. The training set consisted of 2 wildtype mice taken from the cohort A, while the validation was performed on the rest of the data i.e. 20 remaining rodents from different labs, strains, and species. Splitting the data set in this way enabled us to test the main premise of this paper: the robustness of our method holds for different experimental settings and labs without any additional model adaptation. By training SPINDLE only on wildtypes we were able to investigate how well it generalizes across the subjects of the same kind (two other wildtypes from the same cohort A), genetically mutated animals (4 mice from the cohort B), different animal species (the rats from the cohort C), and different sleep labs (comparing across cohorts A/B, C, and D).</p>
<p>First, to diminish the effect of subjectivity in manual sleep scoring, we evaluated the predictions against human expert scoring intersection—epochs in which two human experts agree on the label, since all animal recordings were double scored by two individuals. Here, the epochs in which two human experts disagreed did not have any influence upon the performance evaluation. Secondly, to avoid discarding hard-to-score signal regions, we additionally analyzed the performance with respect to the human experts individually. Finally, as we mention above, artifacts represent a major source of difficulty for human expert scorers (recall <xref ref-type="fig" rid="pcbi.1006968.g002">Fig 2</xref>). To evaluate SPINDLE more accurately in this respect, we then considered “clean” regions and artifacts separately.</p>
<p>More concretely, we first measured the agreement of vigilance state predictions (given by the output of step (f) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>) with the corresponding human expert scorings only for the epochs not marked as artifacts (by any of the two human experts). To evaluate the quality of artifact detection, the 4-category scorings were re-labeled into non-artifacts and artifacts, and then compared to the corresponding predictions (given by the output of step (d) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>). In addition, we investigated the benefits of applying hidden Markov model (HMM) based post-processing, comparing the predictions of HMM (the output of step (f) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>) against the predictions of the convolutional neural network (CNN) (the output of step (e) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>). Evaluations were conducted according to the usual classification metrics, first of overall Accuracy <italic>AC</italic>, and then for each vigilance state in terms of Precision <italic>PR</italic><sup>(<italic>C</italic>)</sup>, Recall <italic>RC</italic><sup>(<italic>C</italic>)</sup> and F1-score <italic>F</italic>1<sup>(<italic>C</italic>)</sup> for each vigilance state C, in percentages:
<disp-formula id="pcbi.1006968.e002"><alternatives><graphic id="pcbi.1006968.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi> <mml:mi>C</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:mi>P</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mi>N</mml:mi></mml:mrow> <mml:mrow><mml:mo>#</mml:mo> <mml:mi>s</mml:mi> <mml:mi>a</mml:mi> <mml:mi>m</mml:mi> <mml:mi>p</mml:mi> <mml:mi>l</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mn>100</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="right"><mml:mrow><mml:mspace width="1.em"/><mml:mi>P</mml:mi> <mml:msup><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:msup><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:msup><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:msup><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mn>100</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:msup><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>T</mml:mi> <mml:msup><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mi>T</mml:mi> <mml:msup><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>F</mml:mi> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mn>100</mml:mn> <mml:mspace width="1.em"/></mml:mrow></mml:mtd> <mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi> <mml:msup><mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mo>·</mml:mo> <mml:mi>P</mml:mi> <mml:msup><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>·</mml:mo> <mml:mi>R</mml:mi> <mml:msup><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:msup><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>R</mml:mi> <mml:msup><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mn>100</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>TP</italic><sup>(<italic>C</italic>)</sup>, <italic>FP</italic><sup>(<italic>C</italic>)</sup> and <italic>FN</italic><sup>(<italic>C</italic>)</sup> are for vigilance class <italic>C</italic> the numbers of true positives, false positives and false negatives respectively. <italic>TP</italic> and <italic>TN</italic> represent the total number of true positives and true negatives.</p>
<p><xref ref-type="table" rid="pcbi.1006968.t002">Table 2</xref> demonstrates the predictive performance of SPINDLE with respect to these metrics (excluding artifacts as described above). We compared the predictions of SPINDLE against the scoring intersection of corresponding human experts. The evaluation was performed with and without HMM-based postprocessing which additionally enforced physiological constraints on vigilance state transitions. Although CNN generates impressive performance alone (top rows), HMM leads to additional improvements (bottom rows), most notably in identification of the REM phase which is usually more challenging to score [<xref ref-type="bibr" rid="pcbi.1006968.ref010">10</xref>]. SPINDLE hence showed that injecting sleep domain knowledge into the model may induce very positive effects on the predictive performance. Across all wildtype and mutant mouse species and across labs, overall accuracies exceeded 97%. Across species, overall accuracy remained at 93%, demonstrating the generalization capabilities of SPINDLE.</p>
<table-wrap id="pcbi.1006968.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.t002</object-id>
<label>Table 2</label>
<caption>
<title>Predicting vigilance states—Agreement analysis.</title>
<p>The evaluation was performed with and without the application of HMM based post-processing (the outputs of steps (f) and (e) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref> respectively). The predictive power is quantified with respect to the global accuracy, and for each vigilance state separately with respect to the precision, recall and F1-score according to <xref ref-type="disp-formula" rid="pcbi.1006968.e002">Eq 2</xref>.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006968.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="center" colspan="2">CNN predictions</td>
<td align="center" colspan="3">WAKE</td>
<td align="center" colspan="3">NREM</td>
<td align="center" colspan="3">REM</td>
</tr>
<tr>
<td align="left">Cohort</td>
<td align="center">Accuracy</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">A</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.5%</td>
<td align="center" style="background-color:#D2D2D2">100 ± 0.3%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.4%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.3%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.8%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.4%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.6%</td>
<td align="center" style="background-color:#D2D2D2">98 ± 1.6%</td>
<td align="center" style="background-color:#D2D2D2">97 ± 2.6%</td>
<td align="center" style="background-color:#D2D2D2">98 ± 1.2%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">B</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.3%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 2.1%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.1%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.7%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 0.9%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.1%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.1%</td>
<td align="center" style="background-color:#EBEBEB">96 ± 2.2%</td>
<td align="center" style="background-color:#EBEBEB">95 ± 1.9%</td>
<td align="center" style="background-color:#EBEBEB">96 ± 0.9%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">C</td>
<td align="center" style="background-color:#D2D2D2">92 ± 3.2%</td>
<td align="center" style="background-color:#D2D2D2">80 ± 10.0%</td>
<td align="center" style="background-color:#D2D2D2">97 ± 1.5%</td>
<td align="center" style="background-color:#D2D2D2">87 ± 6.0%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 1.0%</td>
<td align="center" style="background-color:#D2D2D2">94 ± 3.4%</td>
<td align="center" style="background-color:#D2D2D2">96 ± 1.8%</td>
<td align="center" style="background-color:#D2D2D2">86 ± 5.2%</td>
<td align="center" style="background-color:#D2D2D2">70 ± 16.8%</td>
<td align="center" style="background-color:#D2D2D2">76 ± 12.2%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">D</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.3%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.0%</td>
<td align="center" style="background-color:#EBEBEB">99 ± 0.7%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.6%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 2.7%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.1%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.2%</td>
<td align="center" style="background-color:#EBEBEB">96 ± 3.5%</td>
<td align="center" style="background-color:#EBEBEB">79 ± 23.0%</td>
<td align="center" style="background-color:#EBEBEB">85 ± 17.7%</td>
</tr>
<tr>
<td align="center" colspan="2">CNN+HMM</td>
<td align="center" colspan="3">Wake</td>
<td align="center" colspan="3">NREM</td>
<td align="center" colspan="3">REM</td>
</tr>
<tr>
<td align="left">Cohort</td>
<td align="center">Accuracy</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
<td align="center">Precision</td>
<td align="center">Recall</td>
<td align="center">F1-Score</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">A</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.4%</td>
<td align="center" style="background-color:#D2D2D2">100 ± 0.3%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.4%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.3%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.8%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.4%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 0.6%</td>
<td align="center" style="background-color:#D2D2D2">98 ± 1.5%</td>
<td align="center" style="background-color:#D2D2D2">97 ± 2.5%</td>
<td align="center" style="background-color:#D2D2D2">98 ± 1.1%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">B</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.4%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 2.1%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.0%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.7%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 0.9%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.1%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.1%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 2.4%</td>
<td align="center" style="background-color:#EBEBEB">96 ± 2.0%</td>
<td align="center" style="background-color:#EBEBEB">96 ± 0.8%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">C</td>
<td align="center" style="background-color:#D2D2D2">93 ± 3.4%</td>
<td align="center" style="background-color:#D2D2D2">81 ± 10.6%</td>
<td align="center" style="background-color:#D2D2D2">98 ± 1.5%</td>
<td align="center" style="background-color:#D2D2D2">88 ± 6.4%</td>
<td align="center" style="background-color:#D2D2D2">99 ± 1.0%</td>
<td align="center" style="background-color:#D2D2D2">94 ± 3.6%</td>
<td align="center" style="background-color:#D2D2D2">96 ± 1.8%</td>
<td align="center" style="background-color:#D2D2D2">94 ± 3.0%</td>
<td align="center" style="background-color:#D2D2D2">75 ± 17.7%</td>
<td align="center" style="background-color:#D2D2D2">82 ± 13.0%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">D</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.3%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.0%</td>
<td align="center" style="background-color:#EBEBEB">99 ± 0.8%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 0.6%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 2.8%</td>
<td align="center" style="background-color:#EBEBEB">98 ± 1.0%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 1.2%</td>
<td align="center" style="background-color:#EBEBEB">97 ± 3.3%</td>
<td align="center" style="background-color:#EBEBEB">81 ± 23.0%</td>
<td align="center" style="background-color:#EBEBEB">86 ± 17.0%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The corresponding confusion matrices computed by comparing the label intersection of two human experts to our predictions are given in <xref ref-type="fig" rid="pcbi.1006968.g006">Fig 6</xref> and show how relatively few mislabeled epochs are confused across different vigilance states. For completeness sake, the figure also shows the confusion with respect to artifact identification. The final agreement calculated when artifacts are taken into account is presented in addition.</p>
<fig id="pcbi.1006968.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Comparison against expert intersection—Confusion matrices.</title>
<p>Our predictions were for each cohort independently compared to the annotation intersection of two human experts. Presented are the corresponding confusion matrices. The total and vigilance state scoring agreement was calculated with respect to <xref ref-type="disp-formula" rid="pcbi.1006968.e002">Eq 2</xref>, whereas the artifact scoring agreement was calculated as described in <xref ref-type="disp-formula" rid="pcbi.1006968.e001">Eq 1</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g006" xlink:type="simple"/>
</fig>
<p>Next, we compared the predictions of SPINDLE against the scorings of individual human experts. Doing so enabled us to (i) include into our analysis the epochs in which two experts disagreed; (ii) investigate the potential of SPINDLE to generate predictions which are indistinguishable from the scorings produced by human experts: ideally, the agreement between a human expert and SPINDLE would be close to the agreement between two human experts. The results of the analysis given in <xref ref-type="table" rid="pcbi.1006968.t003">Table 3</xref> are more than encouraging and show that in terms of the global scoring agreement i.e. the accuracy, SPINDLE is perfectly comparable to human experts. The agreement rate between the predictions of SPINDLE and each expert is close to the agreement rate between two corresponding experts.</p>
<table-wrap id="pcbi.1006968.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.t003</object-id>
<label>Table 3</label>
<caption>
<title>Predicting vigilance states—Comparison against individual human experts.</title>
<p>The table shows global agreement rate measured by comparing (a) individual experts between themselves; (b) individual experts with SPINDLE; (c) the scoring intersection of two experts with the predictions of SPINDLE. The evaluation was performed on each cohort separately and only non-artifactual epochs were taken into account.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006968.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="2"/>
<th align="center" colspan="4">Cohort</th>
</tr>
<tr>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="background-color:#EBEBEB">Expert 1 vs Expert 2</td>
<td align="char" char="." style="background-color:#EBEBEB">96.9%</td>
<td align="char" char="." style="background-color:#EBEBEB">94.5%</td>
<td align="char" char="." style="background-color:#EBEBEB">90.0%</td>
<td align="char" char="." style="background-color:#EBEBEB">94.1%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">Expert 1 vs SPINDLE</td>
<td align="char" char="." style="background-color:#D2D2D2">97.7%</td>
<td align="char" char="." style="background-color:#D2D2D2">96.0%</td>
<td align="char" char="." style="background-color:#D2D2D2">89.5%</td>
<td align="char" char="." style="background-color:#D2D2D2">94.9%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">Expert 2 vs SPINDLE</td>
<td align="char" char="." style="background-color:#EBEBEB">96.7%</td>
<td align="char" char="." style="background-color:#EBEBEB">93.7%</td>
<td align="char" char="." style="background-color:#EBEBEB">88.7%</td>
<td align="char" char="." style="background-color:#EBEBEB">94.5%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">Expert intersection vs SPINDLE</td>
<td align="char" char="." style="background-color:#D2D2D2">99.3%</td>
<td align="char" char="." style="background-color:#D2D2D2">98.1%</td>
<td align="char" char="." style="background-color:#D2D2D2">92.8%</td>
<td align="char" char="." style="background-color:#D2D2D2">97.4%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Finally, in a separate set of experiments we evaluated the predictive performance of SPINDLE in identifying artifacts. The subjectivity in distinguishing artifacts from clean epochs is generally known to be overwhelming, and similar conclusions may be derived from <xref ref-type="fig" rid="pcbi.1006968.g002">Fig 2</xref>. To ensure a fair performance estimation we computed the agreement rate with each human expert separately, and then compared it to the agreement rate between the two human experts. <xref ref-type="table" rid="pcbi.1006968.t004">Table 4</xref> suggests that SPINDLE’s predictions are, in terms of global agreement as defined by <xref ref-type="disp-formula" rid="pcbi.1006968.e001">Eq 1</xref>, de facto equal to that of human experts.</p>
<table-wrap id="pcbi.1006968.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.t004</object-id>
<label>Table 4</label>
<caption>
<title>Predicting artifacts—Comparison against individual human experts.</title>
<p>The table shows global agreement rate in artifact detection (evaluated with respect to <xref ref-type="disp-formula" rid="pcbi.1006968.e001">Eq 1</xref>) by comparing (a) individual experts between themselves; (b) individual experts with SPINDLE; (c) only the epochs marked as artifacts by both experts to the artifact predictions of SPINDLE. Note that the cohort D was omitted since it contained practically no epochs labeled as artifacts.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006968.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="2"/>
<th align="center" colspan="3">Cohort</th>
</tr>
<tr>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" style="background-color:#EBEBEB">Expert 1 vs Expert 2</td>
<td align="char" char="." style="background-color:#EBEBEB">29.2%</td>
<td align="char" char="." style="background-color:#EBEBEB">22.5%</td>
<td align="char" char="." style="background-color:#EBEBEB">32.1%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">Expert 1 vs SPINDLE</td>
<td align="char" char="." style="background-color:#D2D2D2">24.6%</td>
<td align="char" char="." style="background-color:#D2D2D2">19.5%</td>
<td align="char" char="." style="background-color:#D2D2D2">40.3%</td>
</tr>
<tr>
<td align="left" style="background-color:#EBEBEB">Expert 2 vs SPINDLE</td>
<td align="char" char="." style="background-color:#EBEBEB">36.1%</td>
<td align="char" char="." style="background-color:#EBEBEB">51.8%</td>
<td align="char" char="." style="background-color:#EBEBEB">32.9%</td>
</tr>
<tr>
<td align="left" style="background-color:#D2D2D2">Expert intersection vs SPINDLE</td>
<td align="char" char="." style="background-color:#D2D2D2">35.7%</td>
<td align="char" char="." style="background-color:#D2D2D2">42.5%</td>
<td align="char" char="." style="background-color:#D2D2D2">59.1%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec008">
<title>Comparison to other approaches</title>
<p>We compared SPINDLE to three previously reported approaches, using the same data identically split into training and testing sets as already described. Analysis was performed only on the epochs where human experts agreed on the label. Since other algorithms lack dedicated artifact detection and analysis subroutines, corrupted epochs were not taken into account. The following three methods were used as our baselines:</p>
<sec id="sec009">
<title>FASTER [<xref ref-type="bibr" rid="pcbi.1006968.ref010">10</xref>]</title>
<p>An unsupervised learning approach which uses nonparametric density estimation clustering on top of manually extracted features. The features in FASTER are derived from a comprehensively binned EEG/EMG power spectra, and are further compressed via principal component analysis. The training data were used for optimizing the hyperparameters according to the procedure described by the authors. Since FASTER was originally applied to 8 seconds long epochs, we down-sampled our scoring resolution from 4 to 8 seconds and then kept only the new larger epochs which contained two equal labels. This way we ensured that FASTER is not at any disadvantage due to the different data annotation setup, furthermore even giving it some advantage by discarding many of the state transition epochs which are hard to score.</p>
</sec>
<sec id="sec010">
<title>SCOPRISM [<xref ref-type="bibr" rid="pcbi.1006968.ref006">6</xref>]</title>
<p>SCOPRISM uses two features: (i) the ratio between EEG spectral power of theta <italic>θ</italic>(6-9Hz) and delta <italic>δ</italic>(0.5-4Hz) frequency ranges; (ii) the root mean squared error of the EMG signal. The two-dimensional feature space is separated according to threshold values which are learned from data. The sleep scoring of each epoch is further refined, following the results of the scoring draft in adjacent epochs. We optimized the thresholds with respect to the training set and kept them fixed during the evaluation on the testing set. SCOPRISM is originally designed for 4 seconds epochs thus no further parameter adaptation was required.</p>
</sec>
<sec id="sec011">
<title>Autoscore [<xref ref-type="bibr" rid="pcbi.1006968.ref009">9</xref>]</title>
<p>This method extracts features from power spectra with respect to the logarithmic distribution. Classification in Autoscore is performed using naive Bayes classifier. The feature vectors are smoothed time-wise using a Gaussian convolution to reduce noise. The method eliminates epochs using a 1-40Hz band-pass filter of the EMG signal. This artifact removal approach produced poor overlap with the artifacts from our data set so it was not further considered. The method was set to produce sleep scoring predictions for 4 second epochs.</p>
<p>Since the competing methods were designed to operate on 1EEG/1EMG setup (rather than 2EEG/1EMG), for each approach we explored the following variations: (a) using only the first EEG signal; (b) using only the second EEG signal; (c) using the average of two EEG signals; (d) averaging the features extracted from two EEG signals; (e) forming the feature vector by combining the features extracted from two EEG signals. In each case, the alternative performing best for the other algorithm was used in our analysis.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006968.g007">Fig 7</xref> presents the summary of the comparative analysis. The experiments showed remarkable reduction in both—the error rates and the corresponding standard deviation, which indicates that SPINDLE is much more accurate, and furthermore is more robust than the previous solutions. Particularly appealing is its superiority in detecting REM sleep which is generally considered more difficult to identify. It is also worthwhile considering the computational intensity of the competing methods (bottom right side of <xref ref-type="fig" rid="pcbi.1006968.g007">Fig 7</xref>). SPINDLE provides impressive predictive performance in a reasonable time frame.</p>
<fig id="pcbi.1006968.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Comparative analysis of SPINDLE.</title>
<p>SPINDLE was compared against three other state-of-the-art solutions (FASTER [<xref ref-type="bibr" rid="pcbi.1006968.ref010">10</xref>], SCOPRISM [<xref ref-type="bibr" rid="pcbi.1006968.ref006">6</xref>] and Autoscore [<xref ref-type="bibr" rid="pcbi.1006968.ref009">9</xref>]). Evaluations were performed for each animal separately and the results were grouped per cohort (top four figures). The global error rate was measured as <italic>ER</italic> = 100 − <italic>AC</italic>, and for each vigilance state separately the class specific error rate was measured as <italic>CER</italic><sup>(<italic>C</italic>)</sup> = 100 − <italic>F</italic>1<sup>(<italic>C</italic>)</sup>, where <italic>AC</italic> and <italic>F</italic>1<sup>(<italic>C</italic>)</sup> are defined in <xref ref-type="disp-formula" rid="pcbi.1006968.e002">Eq 2</xref>. The evaluation of errors was performed on the scoring intersection of two human raters and did not take corrupted epochs into account. Execution times for scoring of 24 hour EEG/EMG animal recordings are given at the right bottom figure.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g007" xlink:type="simple"/>
</fig>
<p>Furthermore, we analyzed the prediction overlap between SPINDLE and FASTER, to understand their agreement with respect to the ground truth annotations. The analysis is given in <xref ref-type="fig" rid="pcbi.1006968.g008">Fig 8</xref>. Note again that in order to ensure a consistent comparison, we down-sampled ground truth and SPINDLE’s predictions to 8 second epochs, and discarded epochs which contained: (i) artefacts; (ii) inconsistent scoring between human experts; (iii) label confusion as a product of down-sampling.</p>
<fig id="pcbi.1006968.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Scoring agreement comparison of FASTER and SPINDLE on 8 second intervals.</title>
<p>The overlap was measured with respect to all epochs from all animal cohorts, and additionally with respect to each vigilance state individually.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g008" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec012">
<title>SPINDLE—Output analysis</title>
<p>After epoch-by-epoch classification, sleep data is typically pooled across each vigilance state, and then quantitatively evaluated with respect to various parameters of sleep architecture, sleep timing and EEG spectral power. Therefore, we next compared such quantitative outputs when calculated using classifications from SPINDLE or from human scorers. To that end, we analyzed the performance of the downstream analysis applied to the predictions of SPINDLE to investigate its capability to (i) predict major parameters of the sleep architecture; (ii) detect sleep alteration induced by a genetic mutation performed on the cohort B, with respect to the cohort A.</p>
<sec id="sec013">
<title>Sleep architecture analysis</title>
<p>We computed and then compared the parameters of sleep architecture from all the three sources of vigilance state scorings, for each cohort separately. The goal was to evaluate the predictive power of SPINDLE in terms of several products of the output analysis: the fraction of vigilance states, bout duration, number of bouts and number of transitions. The resulting plots are given in <xref ref-type="fig" rid="pcbi.1006968.g009">Fig 9</xref>. Visually, it is clear that in most of the cases SPINDLE produced appealing predictions which represent a good balance between the two corresponding experts, thus potentially enabling less biased analysis. To statistically quantify the discrepancy between the predictions of SPINDLE and the human experts, we performed unpaired Student T-test between each pair of the corresponding distributions from <xref ref-type="fig" rid="pcbi.1006968.g009">Fig 9</xref>, comparing human experts and predictions. Statistical significance level was set at <italic>p</italic> &lt; 0.05. Out of 48 independent output analysis shown in the figure, only in two cases the significance was reported between the predictions and both human experts, while the difference between two experts was non-significant: REM bout duration in cohort D (<italic>p</italic> = 0.016 and <italic>p</italic> = 0.004), and the number of REM bouts in the cohort C (both <italic>p</italic> &lt; &lt;0.01). In all other cases, our predictions were significantly close to at least one of the human experts. Furthermore, there were cases when the output of two human experts was significantly different, while SPINDLE’s predictions were not significantly different from any of the two: wake and NREM sleep fractions in the cohort A (<italic>p</italic> = 0.011 and <italic>p</italic> = 0.009), wake bout duration in the cohort C (<italic>p</italic> = 0.019), wake and NREM number of bouts in the cohort B (<italic>p</italic> = 0.011 and <italic>p</italic> = 0.032), and wake to NREM transition and vice versa in the cohort B (<italic>p</italic> = 0.013 and <italic>p</italic> = 0.009). These results clearly demonstrate that SPINDLE has great potential to improve the cross-expert/lab consistency in sleep classification.</p>
<fig id="pcbi.1006968.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Predicting parameters of sleep architecture.</title>
<p>Fraction of sleep (top row), bout duration (second row), number of bouts (third row), and number of sleep transitions (bottom row). Given are the box plots computed from the evaluation of these parameters per hour for each cohort, using data from each human scorer and from SPINDLE output. W→N, transition from WAKE to NREM; N→W, transition from NREM to WAKE; N→R, transitions from NREM to REM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>Detecting mutation-induced differences between cohorts</title>
<p>We investigated the capability of SPINDLE to detect significant differences in animal sleep patterns induced by experimental factors, thus emulating a real-life study. To that end, we post-processed the scorings of human experts and the predictions of SPINDLE to compare the cohorts A and B. The two cohorts were chosen because the corresponding animals had the same background, the only difference being a mutation of a gene in the mutant cohort B. We calculated the cohort differences with respect to sleep timing (<xref ref-type="fig" rid="pcbi.1006968.g010">Fig 10</xref>) and EEG power spectra (<xref ref-type="fig" rid="pcbi.1006968.g011">Fig 11</xref>). Both figures suggest that statistically significant discrepancies between cohorts were successfully identified by SPINDLE, when comparing to the ones identified by two human experts.</p>
<fig id="pcbi.1006968.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Detecting mutation-induced cohort differences in sleep timing.</title>
<p>Fraction of NREM sleep, REM sleep and wakefulness per 2 hour intervals across 24 hours, in the cohorts A (wildtypes) and B (mutants). Results were evaluated from the scorings of the corresponding two experts and SPINDLE. The prediction curves were calculated for two different values of artifact threshold applied on the probabilistic output of the corresponding CNN (see step (d) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>). 0.5 is the default threshold, and indicates that only epochs with &gt; 50% confidence of being non-corrupted were kept in the analysis. Similar procedure was applied for 0.7 artifact threshold. For measuring of the overall statistical significance, two-way ANOVA (marked as A) was used. ▲ <italic>p</italic> &lt; 0.05 regions explain statistical differences of the corresponding 0.5Hz frequency bins measured using a two-tailed T-test with equal variances. The curves represent mean ± SEM. ZT, zeitgeber time.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g010" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006968.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Detecting mutation-induced cohort differences in EEG spectra.</title>
<p>EEG spectral power density plots of the cohort A (wildtypes) and B (mutants) during NREM, REM and wakefulness. Results were evaluated from the scorings of the corresponding two experts and SPINDLE. The prediction curves were calculated for two different values of artifact threshold applied on the probabilistic output of the corresponding CNN (see step (d) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>). 0.5 is the default threshold, and indicates that only epochs with &gt; 50% confidence of being non-corrupted were kept in the analysis. Similar procedure was applied for 0.7 artifact threshold. For measuring of the overall statistical significance, two-way ANOVA (marked as A) was used. ▲ <italic>p</italic> &lt; 0.05 regions explain statistical differences of the corresponding 0.5Hz frequency bins measured using a two-tailed T-test with equal variances. The curves represent mean ± SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g011" xlink:type="simple"/>
</fig>
<p>In addition, the figures illustrate the relevance of identifying artifactual data, especially when detecting significant statistical differences in EEG spectral power density cohort profiles. In particular, we performed equal output analysis for two different cut-off values applied on the probabilistic predictions generated by the CNN in step (d) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>. We showed that the strictness of artifact rejection criteria may have influence on designating the areas where the differences between compared cohorts appear. This is a relevant aspect to be considered when aiming for standardized and coherent systematic sleep studies. Since the artifacts are a major source of sleep scoring disagreement among human experts, it is often hard to find a common ground. However, the probabilistic nature of the SPINDLE framework could be useful in this regard, as it could offer some flexibility in adapting to the experiment-specific artifact rejection criteria.</p>
</sec>
</sec>
<sec id="sec015">
<title>SPINDLE web service</title>
<p>Entire framework described in our study was integrated into a web service which can be found at <ext-link ext-link-type="uri" xlink:href="https://sleeplearning.ethz.ch" xlink:type="simple">https://sleeplearning.ethz.ch</ext-link>. This service ensures an easy access to the sleep researchers around the world and offers a possibility of accurate evaluation of EEG/EMG recordings in no time. Furthermore, we are continuously improving our framework aiming to create a self-sufficient environment for large scale animal analysis e.g. which would include output analysis in addition. Lastly, we would like to emphasize the two following aspects (i) the adaptable artifact threshold functionality; (ii) further technical considerations.</p>
<sec id="sec016">
<title>Adaptable artifact threshold</title>
<p>Due to the particularly high variations and subject-specific bias in designating epochs corrupted by artifacts, we decided to provide users with a certain flexibility in identifying artifactual epochs. To that end, SPINDLE offers an additional optional functionality which allows users to tune the artifact sensitivity by modifying the threshold from the output of the step (d) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>, therefore taking advantage of the probabilistic output of the corresponding CNN. A practical example of how this functionality may be utilized was mentioned earlier and is illustrated in Figs <xref ref-type="fig" rid="pcbi.1006968.g010">10</xref> and <xref ref-type="fig" rid="pcbi.1006968.g011">11</xref>. Note that SPINDLE remains parameter-free, and that this flexibility is introduced to ensure a smoother road towards standardization with respect to the sensitive and not well defined rules for artifact identification.</p>
</sec>
<sec id="sec017">
<title>Technical considerations</title>
<p>To ensure the successful utilization of the proposed framework we note down several technical aspects. First of all, even though our model was trained on a commonly used 2EEG/1EMG setup (employed in the generation of all the data presented here), we also enable users to use SPINDLE with 1EEG/1EMG experimental setting. We do that by multiplying the corresponding EEG channel. This ensures that the sleep labs with a different recording setup are still able to use SPINDLE. It is also worth noting, that data format must adhere precisely to the one described in detail at the above mentioned web platform.</p>
</sec>
<sec id="sec018">
<title>Computation</title>
<p>The programming code underlying the sleep scoring server was written in torch (torch.ch) platform and will be made available through the server. To run the same instance of it locally no special requirements are necessary. We used the server framework for all our experiments. To speed up the learning and evaluation we used vectorized computation on a <italic>GeForce GTX TITAN X</italic> GPU card.</p>
</sec>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>We described a new data processing architecture for fast, accurate and physiologically plausible automated sleep scoring of animal EEG/EMG recordings. SPINDLE is based on end-to-end learning and is capable of learning robust features which generalize across domains. From the robust benchmarking procedures that we employed we concluded that SPINDLE (i) showed de facto human level performance in the quality of sleep classification; (ii) significantly outperformed current state-of-the-art solutions; (iii) was able to preserve predictive performance across animals from different experimental settings, labs and species, without additional parameter calibration; (iv) was able to detect mutation-induced sleep alteration. Our extensive statistical evaluation suggests that the proposed method is in every way the equal of human experts, and holds great promise to improve cross-lab standardization of sleep analysis.</p>
<p>A part of SPINDLE’s success lies in the use of convolutional neural networks (CNNs). CNNs, a variant of deep neural networks, have enjoyed great success, particularly in the domain of natural signal processing [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref016">16</xref>]. They revolutionized the field of computer vision with their unprecedented success on image recognition tasks [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>]. Here, we use them to combat a fundamental problem that has plagued automated sleep scoring: feature variance. Classical approaches based on manually crafted features are likely to fail when generalized across experiments simply because class-specific features or feature combinations may differ across data samples, or mutants, or species. Thus, because these models form their feature vectors from the energies of certain frequency bands, even small shifts of the spectral peaks may cause a different distribution of spectral energy with respect to the histogram bins, ruining predictive performance. All three methods that we tested alongside our own arguably suffer from this spectral pattern inconsistency issue related to manual feature extraction.</p>
<p>By contrast, a CNN extrapolates well across spectral profile variations much as a human expert would do. Furthermore, SPINDLE employs a hidden Markov model layer to describe probabilistic transitions between vigilance states. Thus, impossible transitions—for example wake to REM, known not to occur naturally—can be artificially suppressed by the user. Provided the assumptions about these physiological constraints hold, HMM generates physiologically plausible prediction sequences leading to improved performance.</p>
<p>Unlike other existing solutions which lack a principled way for distinguishing epochs corrupted by artifacts, SPINDLE employs an additional CNN to offer a dedicated computational tool for that task. The identification of artifacts from data is known to be quite challenging due to very large discrepancies in their annotation when different human experts are compared. Nevertheless, we showed that SPINDLE is performing well on this task as well, again reaching human expert level. Furthermore, to still provide some dose of flexibility in this highly biased and human expert-dependent procedure, our framework offers additional functionality which allows users to adapt the degree of artifact rejection. Our framework remains parameter-free and this optional functionality exist to ensure smoother convergence towards full standardization of the sleep scoring procedure.</p>
<p>There are some relevant design decisions worth of noting. Firstly, in our work we used <italic>out-of-the box CNN</italic>, and not any of the commonly used architectures such as AlexNet [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>] or ResNet [<xref ref-type="bibr" rid="pcbi.1006968.ref028">28</xref>]. We empirically found that increasing the depth and width of our network did not led to any improvements, and hence found no reason to e.g. add additional layers to the main architecture. Our hypothesis is that the structural complexity of “spectrograms” is not comparable to the structural complexity of natural images, thus there were additional benefits in having an increased complexity in our CNN architecture. Secondly, in contrast to some related work in the domain of human sleep scoring [<xref ref-type="bibr" rid="pcbi.1006968.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006968.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref024">24</xref>], our final architecture does not include memory models i.e. recurrent neural networks (RNNs). We found that their inclusion did not lead to any performance improvements and moreover was harmful to across-domain generalization. Our tests indicated that RNN tends to overfit to subject-specific dynamics and hence makes it difficult to extrapolate knowledge across environments. On the other hand, even though not mutually exclusive, HMMs offer more fine-grained control over the transition dynamics. Secondly, we utilized separate CNNs for artifacts and vigilance states for the following reasons: (i) semantically, the patterns in data are different—vigilance states are recognized by somewhat predefined rules while the artifacts are data outliers; (ii) the described setup allows us to identify three different types of artifacts, thus more faithfully emulating visual scoring procedure; (iii) we found that the proposed framework leads to better predictive performance.</p>
<p>Finally, as a part of our future work, we intend to investigate the capabilities of and re-fine our method further in subsequent studies possibly involving other experimental paradigms such as sleep deprivation for example, which might possibly cause other types of distortions of spectral profiles. Another interesting aspect would involve including more human scorers and consequently applying more principled ways of combining their knowledge (e.g. see [<xref ref-type="bibr" rid="pcbi.1006968.ref029">29</xref>]).</p>
</sec>
<sec id="sec020" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec021">
<title>Experimental data acquisition</title>
<p>We detail the steps in collecting the data previously summarized in <xref ref-type="table" rid="pcbi.1006968.t001">Table 1</xref>. In total, 4 animal cohorts containing 22 animals were acquired from 3 independent sleep labs. Animal studies were performed by authorized researchers according to all applicable laws and regulations of the cantons of Zürich (BrownLab, BaumannLab) and Bern (TidisLab), and were each approved by the relevant cantonal authorities. Each sleep recording consists of a pair of EEG signals and an EMG signal simultaneously recorded. Manual labeling of wake-sleep states based on EEG/EMG signals was performed by trained experts from the corresponding sleep labs on all consecutive 4 second epochs. Raw EEG traces were visually inspected offline and scored in three vigilance states, wakefulness, NREM sleep and REM sleep. Wakefulness was defined based on increased EMG activity for more than 50% of the epoch duration. NREM sleep was defined by reduced EMG activity and increased EEG power in &lt; 4Hz frequency ranges. REM sleep was characterized based on low EEG power in &gt; 4Hz oscillations and high EEG power in 6-9Hz frequency bands and intermediate muscle tone. Unclear stages or technical artifacts were excluded and subsequently labeled as artifacts.</p>
<sec id="sec022">
<title>BrownLab data set</title>
<p>It contains sleep recordings of two cohorts: the cohort A containing 4 wildtype mice and the cohort B containing 4 mice with a genetic mutation. All mice were kept in Macrolon cages (36x20x35cm) with food and water ad libitum, maintained at a 12 hour light-dark cycle (light onset 07.00 AM) in normal cages prior to surgery, and then in open-top cages with counterbalanced swivel-attached cables during and between sleep recordings. For the EEG recordings, mice were implanted epidurally with gold-plated miniature screws (0.9mm diameter) under constant isofluran inhalation anesthesia. Analgesia was given i.p. at 0.1mg/kg during the surgery. The coordinates of EEG electrodes were as follows; frontal derivation was placed 1mm anterior to bregma, 1.5mm lateral to mid-line, the parietal derivation was placed 3mm posterior to bregma and 2mm lateral to mid-line. The reference was placed over the cerebellum (2mm posterior to lambda on the midline). Two gold wires (0.2mm diameter) were inserted bilaterally in the neck muscle for EMG recordings. The screws were connected to stainless steel wires and fixed on the skull with acrylic dental concrete. The mice were tethered to a swivel throughout the entire experiment and were allowed to recover for 4 to 7 days before any further handling. The EEG and EMG signals were amplified (amplification factor, 2000), filtered (highpass filter: –3 dB at 0.016 Hz; low-pass filter: –3 dB at 40 Hz) sampled with 512 Hz, digitally filtered [EEG: low-pass finite impulse response (FIR) filter, 25 Hz; EMG: bandpass FIR filter, 20–50 Hz or 10-30Hz], and stored with a resolution of 128 Hz. Before each recording, the EEG and EMG channels were calibrated with a 10 Hz, 300 μV peak-to-peak sine wave.</p>
</sec>
<sec id="sec023">
<title>BaumannLab data set</title>
<p>It contains sleep recordings of the cohort C which consists of 8 rats. The rats were implanted with EEG/EMG electrodes for recording of vigilance states using a protocol slightly modified from [<xref ref-type="bibr" rid="pcbi.1006968.ref030">30</xref>]. Briefly, four stainless steel miniature screws (Hasler, Switzerland), one pair for each hemisphere, were inserted bilaterally into the rats’ skull following specific stereotactic coordinates: the anterior electrodes were implanted 3mm posterior to bregma and 2mm lateral to the midline, and the posterior electrodes 6mm posterior to bregma and 2mm lateral to the midline. For monitoring of muscle tone, a pair of gold wires served as EMG electrodes and was inserted into the rats’ neck muscle. All electrodes were connected to stainless steel wires, further connected to a head piece (Farnell, #M80-8540842, Switzerland) and fixed to the skull with dental cement. Bilateral 24 hour EEG/EMG signals were recorded in freely-moving rats. For this purpose, the animals were transferred to special recording cages with food and water available ad libitum, where they had an adaptation period of two days before recordings took place. EEG and EMG were sampled at 200 Hz, signals were amplified, filtered and converted into analog-to-digital signals. Hardware EMBLA and Somnologica-3 software (Medcare Flaga, Iceland) were used. Activity in the 50-Hz band was discarded from the analysis because of power line artifacts.</p>
</sec>
<sec id="sec024">
<title>TidisLab data set</title>
<p>It contains sleep recordings of the cohort D.<italic>C57Bl6 mice</italic> were used, 11-14 weeks of age, from Charles Rivers Laboratories, Germany. Animals were housed in individual custom-designed polycarbonate cages at constant temperature (22 ± 1 °C), humidity (30–40%), and circadian cycle (12 hour light-dark cycle, lights on at 08:00). Food and water were available ad libitum. Animals were treated according to protocols and guidelines approved by the Veterinary office of the Canton of Bern, Switzerland (License number BE 113/13). Animals were housed in IVC cages in groups of 2–5 before instrumentation. After implantation, all mice were housed individually. Animals were habituated to the recording cable in their open-top home cages (300 × 170 mm). Animals were anaesthetized in isoflurane in oxygen and mounted in a stereotaxic frame. Saline 10ml/kg and meloxicam 5mg/kg were given subcutaneously. The skin on the head was shaved and aseptically prepared, and lidocaine 2mg/kg infused subcutaneously at the incision site. A single longitudinal midline incision was made from the level of the lateral canthus of the eyes to the lambda skull suture. Two stainless steel screws were placed in the skull over the parietal cortex to measure EEGs and two bare-ended wires sutured to the trapezius muscle of the neck to record EMG. The implant was stabilized using a methyl methacrylate cement and the animal allowed to recover in the home cage on top of a heating mat. Animals were allowed a minimum of 5 days to recover before starting recordings. Habituation to the cables was performed up to 8 hours per day until the animals had nested and resumed a normal sleep-wake cycle. For recordings, mice were connected to the AM system and data sampled at 512 Hz. For each mouse, 24 hour baseline sleep was recorded, while animal was allowed to move freely in the cage. Recorded EEG/EMG signals were down-sampled to 128 Hz, after applying a low pass filter (Chebyshev Type I, order 8, low pass edge frequency of 50 Hz, passband ripple of 0.05 dB) to prevent aliasing.</p>
</sec>
</sec>
<sec id="sec025">
<title>Data preprocessing</title>
<p>The data preprocessing module (step (c) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>) serves primarily to form the input for the convolutional neural networks (CNNs). We subject EEG/EMG signals to a sequence of transformations which enhance the learning process and consequently improve classification performance. The transformations are performed per animal and are meant to diminish non-informative differences in subject specific spectral patterns. The preprocessing procedure is illustrated in <xref ref-type="fig" rid="pcbi.1006968.g012">Fig 12</xref>. Both EEG signals are first resampled to the frequency of 128Hz to neutralize the differences in sampling rates coming from different recording devices. Resampled EEGs are then transformed into the time-frequency domain by applying fast Fourier transform on overlapping frames of size 256 (corresponds to 2 seconds) with steps of size 16. Hamming windows were applied to reduce edge effects. Power spectral density (PSD) in time-frequency representation is estimated as squared magnitude of the Fourier transform. Each of the two dimensional spectrograms constructed from EEG signals is treated as a separate feature map on top of which the CNNs convolve. This is analogous to the well-known CNN image classification architectures where the input consists of 3 RGB channels [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>]. EEG spectrograms are additionally band-pass filtered (0.5-24Hz), as we experimentally determined that classification performance remains unaffected. Both time-frequency channels are then transformed to log scale and finally each channel is per frequency component standardized (zero mean / unit variance).</p>
<fig id="pcbi.1006968.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Data preprocessing and CNN input preparation.</title>
<p>The figure depicts the creation of the three-channel two-dimensional input for the CNNs. In step (a) raw time series of EEG/EMG signals are separately transformed into the corresponding time-frequency domain (power spectrum density is computed) via a sequence of short Fourier transforms applied to overlapping Hamming windows. In step (b) EEG signals are band-pass filtered (0.5–24<italic>Hz</italic>) and EMG power is integrated over frequency range (0.5-30Hz) resulting in one-dimensional representation of muscle activity change over time. Furthermore, one-dimensional representation of EMG is converted into the two-dimensional one by a multiplication of the signal. Finally, in step (c) the data is log transformed and standardized per frequency component.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g012" xlink:type="simple"/>
</fig>
<p>The EMG signal on the other hand carries the information about muscle activity of evaluated subjects. The total energy of EMG indicates the activity of the corresponding muscle. To decrease the noise we compute signal energy by integrating PSD over a limited frequency band (0.5-30Hz). In other words, we sum up the rows in our time-frequency representation within the given frequency range (see <xref ref-type="fig" rid="pcbi.1006968.g012">Fig 12</xref>). This leaves us with one-dimensional signal which measures the change in muscle activity over time. However, in order to form a consistent input for CNN with respect to two-dimensional representations of EEG, we introduce an additional dimension by repeating the signal as illustrated in the figure. This way of forming input is beneficial because in each time instance the CNN filters can relate the total EMG signal power with spectral patterns in different regions of the frequency axis.</p>
</sec>
<sec id="sec026">
<title>CNN architecture and training</title>
<p>A Convolutional Neural Networks (CNN) [<xref ref-type="bibr" rid="pcbi.1006968.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref031">31</xref>] is an artificial neural network most commonly composed of a sequence of (i) convolutional layers which learn high level signal representations; (ii) pooling layers which increase the translational feature invariance; (iii) dense layers which learn high-level feature combinations in a discriminative manner; and (iv) a softmax layer which generates class probabilities. Details of these layers follow below. For a more thorough introduction to CNN we refer the reader to [<xref ref-type="bibr" rid="pcbi.1006968.ref032">32</xref>].</p>
<sec id="sec027">
<title>Convolutional layer</title>
<p>The convolutional layer implements a filter operation <italic>h</italic> of size <italic>m</italic> × <italic>m</italic>. The resulting value at some neuron <italic>x</italic><sub><italic>ij</italic></sub> in layer <italic>ℓ</italic> can be written as:
<disp-formula id="pcbi.1006968.e003"><alternatives><graphic id="pcbi.1006968.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>b</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:msub><mml:mi>h</mml:mi> <mml:mrow><mml:mi>a</mml:mi> <mml:mi>b</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>β</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>h</italic><sub><italic>ab</italic></sub> is the weight of the filter in the point (<italic>a</italic>, <italic>b</italic>) of the <italic>m</italic> × <italic>m</italic> grid, <inline-formula id="pcbi.1006968.e004"><alternatives><graphic id="pcbi.1006968.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>*</mml:mo> <mml:mo>*</mml:mo></mml:mrow> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> are the corresponding outputs of the previous layer and <inline-formula id="pcbi.1006968.e005"><alternatives><graphic id="pcbi.1006968.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mi>β</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the bias term. After the filter, a non-linear function <italic>σ</italic> is applied to all the neurons in layer <italic>ℓ</italic> independently: <inline-formula id="pcbi.1006968.e006"><alternatives><graphic id="pcbi.1006968.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. We used the well-established rectifier linear unit (ReLU) [<xref ref-type="bibr" rid="pcbi.1006968.ref033">33</xref>]: <inline-formula id="pcbi.1006968.e007"><alternatives><graphic id="pcbi.1006968.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec028">
<title>Max-pooling layer</title>
<p>The max-pooling layer down-samples the previous layer by choosing only the maximal value of non-overlapping rectangles of size <italic>m</italic><sub><italic>h</italic></sub> × <italic>m</italic><sub><italic>w</italic></sub>, see the blue cones in <xref ref-type="fig" rid="pcbi.1006968.g013">Fig 13</xref> for an illustration. The max-pooling layer provides translation-invariance and reduces the number of parameters of the CNN.</p>
<fig id="pcbi.1006968.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Sleep scoring CNN architecture.</title>
<p>Presented are the architectural details of the CNN which estimates the probability distribution over vigilance states for the target E<sub><italic>t</italic></sub>. Input to the CNN is formed as shown in <xref ref-type="fig" rid="pcbi.1006968.g012">Fig 12</xref>. The CNN operates over four neighboring epochs E<sub><italic>t</italic>−2</sub>, E<sub><italic>t</italic>−1</sub>, E<sub><italic>t</italic>+1</sub> and E<sub><italic>t</italic>+2</sub> to capture the contextual information. Illustrated CNN consists of two max-pooling layers (depicted in blue), one convolutional layer (depicted in green), and two fully-connected layers (depicted in red). At the very end, a softmax layer outputs class probabilities. The dimensions of the first max-pooling layer are (width, height) = (2, 3) with the corresponding strides (2, 3). The dimensions of the second max-pooling layer are (2, 2) with the corresponding strides (2, 2). The dimensions of the convolutional kernel are (3, 3) with the corresponding strides (1, 1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g013" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec029">
<title>Dense layer</title>
<p>The last layer in a CNN before the softmax activation function is usually a fully-connected dense layer that connects all the neurons in the previous layer. The resulting value at some neuron <inline-formula id="pcbi.1006968.e008"><alternatives><graphic id="pcbi.1006968.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> in the layer <italic>ℓ</italic> can be written as:
<disp-formula id="pcbi.1006968.e009"><alternatives><graphic id="pcbi.1006968.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>ℓ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>β</mml:mi> <mml:mi>i</mml:mi> <mml:mi>ℓ</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>α</italic><sub><italic>ij</italic></sub> is the weight corresponding to output <inline-formula id="pcbi.1006968.e010"><alternatives><graphic id="pcbi.1006968.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> of the previous layer, and <inline-formula id="pcbi.1006968.e011"><alternatives><graphic id="pcbi.1006968.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msubsup><mml:mi>β</mml:mi> <mml:mi>i</mml:mi> <mml:mi>ℓ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the bias term.</p>
</sec>
<sec id="sec030">
<title>Softmax layer</title>
<p>The last layer is a softmax layer, given by <xref ref-type="disp-formula" rid="pcbi.1006968.e012">Eq 5</xref> where <italic>C</italic> is one of <italic>K</italic> classes and <italic>y</italic><sup>(<italic>C</italic>)</sup> the output of the previous layer with respect to the class <italic>C</italic>. It is used to get normalized class probabilities:
<disp-formula id="pcbi.1006968.e012"><alternatives><graphic id="pcbi.1006968.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<p>In our work we adopted this kind of convolutional neural network architecture. Discriminative features are learned in an end-to-end fashion from three-channel time-frequency signals (the output in <xref ref-type="fig" rid="pcbi.1006968.g012">Fig 12</xref>). Apart from the number of output units, design-wise, two CNNs in our framework are equivalent, thus here we refer only to the CNN used for the classification of vigilance states (step (e) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>). The architectural details are given in <xref ref-type="fig" rid="pcbi.1006968.g013">Fig 13</xref>. To encapsulate the contextual information the CNN convolves over the target epoch, but also over the surrounding neighbor epochs, two from each side. The variability of spectral profiles (previously discussed in Results) is naturally solved through the discriminative learning of translation invariant features. Namely, by convolving and max-pooling over the frequency domain, the CNN becomes agnostic to small shifts in spectral energy distribution which appear when comparing different animals. On the other hand, by operating over time domain we become agnostic to where the spectral patterns appear within the evaluated region of the input signal.</p>
<p>Weight learning was performed via back-propagation using the optimization algorithm called Adam [<xref ref-type="bibr" rid="pcbi.1006968.ref034">34</xref>]. The weight decay rates of first and second moment were set to 0.9 and 0.99 as suggested in the original paper. Prior to back-propagation, weights were randomly initialized as described in [<xref ref-type="bibr" rid="pcbi.1006968.ref035">35</xref>]. To account for the class imbalance (e.g. see the top row in <xref ref-type="fig" rid="pcbi.1006968.g009">Fig 9</xref>) the Adam optimizer was governed by a class-weighted cross-entropy loss function. Given the observation <italic>x</italic> and the true class <italic>C</italic> ∈ {<italic>C</italic><sub>1</sub> = <italic>WAKE</italic>, <italic>C</italic><sub>2</sub> = <italic>NREM</italic>, <italic>C</italic><sub>3</sub> = <italic>REM</italic>} the corresponding loss is calculated as:
<disp-formula id="pcbi.1006968.e013"><alternatives><graphic id="pcbi.1006968.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>loss</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>3</mml:mn></mml:msubsup> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>3</mml:mn></mml:munderover> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>f</italic>(<italic>C</italic>, <italic>x</italic>) is the output of the last fully-connected layer corresponding to class <italic>C</italic> and <italic>w</italic>(<italic>C</italic>) is the weight of that class. Class weights <italic>w</italic>(<italic>C</italic><sub><italic>i</italic></sub>) were for each mini-batch independently set with respect to the class sample ratio within that mini-batch. Final gradient was computed as the normalized sum of individual losses within the mini-batch:
<disp-formula id="pcbi.1006968.e014"><alternatives><graphic id="pcbi.1006968.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>batch</mml:mtext><mml:mo>_</mml:mo><mml:mtext>loss</mml:mtext><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>C</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mi>M</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>M</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>M</mml:mi></mml:munderover> <mml:mtext>loss</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>C</mml:mi> <mml:mi>i</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Learning rate was set to 5 ⋅ 10<sup>−5</sup> and each mini-batch contained M = 100 samples. To further regularize the learning procedure we applied the dropout with the probability of 50% to the fully connected layers and allocated 10% of the data from the training set for early stopping. Overall, our CNN contained 6.8M parameters in total, and converged on the held-out set (10%) already after 5 full iterations over the training data.</p>
</sec>
</sec>
<sec id="sec031">
<title>Constraining transition dynamics with HMM</title>
<p>To obtain a finer-grained modeling control over the dynamics of vigilance state transitions we utilize a hidden Markov model (HMM). Broadly speaking, HMMs are tools for representing probability distributions over sequences of indirectly observable states. A first-order HMMs is fully specified by the probability distribution of the initial state <italic>P</italic>(<italic>S</italic><sub>1</sub>), the matrix of transition probabilities between neighboring states <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic>−1</sub>) and the output model defined by the emission likelihoods <italic>P</italic>(<italic>Y</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic></sub>) where <italic>Y</italic><sub><italic>t</italic></sub> is the indirect observation of variable <italic>S</italic><sub><italic>t</italic></sub>. In the context of our problem, the hidden state in some moment <italic>t</italic> is the vigilance state of the brain <italic>S</italic><sub><italic>t</italic></sub> ∈ {<italic>WAKE</italic>, <italic>REM</italic>, <italic>NREM</italic>}, while the observation <italic>Y</italic><sub><italic>t</italic></sub> is the corresponding region of EEG/EMG signal (in <xref ref-type="fig" rid="pcbi.1006968.g013">Fig 13</xref> that would be <italic>epoch</italic><sub><italic>t</italic>−2</sub> to <italic>epoch</italic><sub><italic>t</italic>+2</sub>) from which hypothetically the state <italic>S</italic><sub><italic>t</italic></sub> can be inferred. Our goal in the training time is to find the optimal parameters λ = [<italic>P</italic>(<italic>Y</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic></sub>), <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic>−1</sub>), <italic>P</italic>(<italic>S</italic><sub>1</sub>)] of the HMM, and then to use learned parameters later in the test time to obtain the most probable sequence of vigilance states given input signal: <inline-formula id="pcbi.1006968.e015"><alternatives><graphic id="pcbi.1006968.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:munder><mml:mtext>argmax</mml:mtext> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:munder> <mml:mspace width="0.277778em"/><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:mo>|</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mtext>EEG/EMG</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where N is the number of epochs.</p>
<p>Firstly, to find the indirect observation emission likelihoods <italic>P</italic>(<italic>Y</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic></sub>) we use the probabilities generated by the discriminatively trained CNN and we treat them as the model output. Namely, CNN produces posterior probabilities over states <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>|<italic>Y</italic><sub><italic>t</italic></sub>) which can be used for computing HMM emission likelihoods <italic>P</italic>(<italic>Y</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic></sub>). They are connected by the following equation which is derived directly from the Bayes rule:
<disp-formula id="pcbi.1006968.e016"><alternatives><graphic id="pcbi.1006968.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<p>Secondly, without loss of generality and any substantial effect to the predictive performance we may assume that all initial states <italic>P</italic>(<italic>S</italic><sub>1</sub>) are equal. Finally, the last component of the HMM is the probability transition matrix <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic>−1</sub>) which is of size 3 × 3 in our case. In order to generate physiologically feasible prediction sequences, when applicable, using the probability transition matrix we can embed the domain knowledge on infeasible vigilance state transitions. An illustration is given in <xref ref-type="fig" rid="pcbi.1006968.g014">Fig 14</xref>. Whenever it is known in advance that certain transitions are not valid such as <italic>REM</italic> → <italic>NREM</italic> and <italic>WAKE</italic> → <italic>REM</italic> [<xref ref-type="bibr" rid="pcbi.1006968.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006968.ref037">37</xref>] which is the case for all the recordings in our data set, we can zero out the corresponding entries in the probability transition matrix. Since there is an additional constraint that the rows of the transition matrix must sum up to 1, this leaves us with effectively only four remaining free parameters. These can be set to be of equal value, or tuned additionally to improve the smoothing of the vigilance state sequence estimates. Having specified the model, new posterior probabilities over vigilance states are generated as follows:
<disp-formula id="pcbi.1006968.e017"><alternatives><graphic id="pcbi.1006968.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006968.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where we used <xref ref-type="disp-formula" rid="pcbi.1006968.e016">Eq 8</xref> to obtain the last equality. Since the CNN is trained in a class-balanced way which compensates for an unequal class ratio in the training set, the prior probabilities of states <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>) may be assumed to be equal. Finally, using the specified HMM model we may now simply apply Viterbi decoding to find the most probable sequence of vigilance states (the output of step (f) in <xref ref-type="fig" rid="pcbi.1006968.g001">Fig 1</xref>).</p>
<fig id="pcbi.1006968.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006968.g014</object-id>
<label>Fig 14</label>
<caption>
<title>CNN-HMM for constraining state transitions.</title>
<p>The figure illustrates how the HMM is used on top of the CNN to enforce prediction sequences which adhere to physiological constraints. In particular, we disallow <italic>REM</italic> → <italic>NREM</italic> and <italic>WAKE</italic> → <italic>REM</italic> vigilance state transitions. The constraints are encoded through the transition probability matrix of the HMM, and the observation likelihoods are implicitly calculated by the CNN.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006968.g014" xlink:type="simple"/>
</fig>
<p>In summary, the HMM allows us to combine the CNN estimates used for computing emission likelihoods <italic>P</italic>(<italic>Y</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic></sub>) (<italic>observation modeling</italic>) with the knowledge encoding capability of the HMM probability transition matrix <italic>P</italic>(<italic>S</italic><sub><italic>t</italic></sub>|<italic>S</italic><sub><italic>t</italic>−1</sub>) (<italic>dynamics modeling</italic>) to produce more plausible and consequently more accurate prediction sequences. It is also worth noting that the commonly advocated problem of HMMs that the observations are assumed to be independent (given state) is treated through the inclusion of neighboring epochs into the convolving field of the CNN (recall <xref ref-type="fig" rid="pcbi.1006968.g013">Fig 13</xref>).</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We are particularly grateful to the following people for their valuable advice and assistance with data collection and experiments: Lucas Imbach, Goncalves Moreira Carlos, and members of the Information Science and Engineering group from ETH Zurich. We would also like to thank Björn Orri Sæmundsson for his help in building up the web server, and Seijin Kobayashi for his help with the method design.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006968.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mignot</surname> <given-names>E</given-names></name>. <article-title>Why we sleep: the temporal organization of recovery</article-title>. <source>PLoS biology</source>. <year>2008</year> <month>Apr</month> <day>29</day>;<volume>6</volume>(<issue>4</issue>):<fpage>e106</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0060106" xlink:type="simple">10.1371/journal.pbio.0060106</ext-link></comment> <object-id pub-id-type="pmid">18447584</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vorster</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Krishnan</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Cirelli</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lyons</surname> <given-names>LC</given-names></name>. <article-title>Characterization of sleep in Aplysia californica</article-title>. <source>Sleep</source>. <year>2014</year> <month>Sep</month> <day>1</day>;<volume>37</volume>(<issue>9</issue>):<fpage>1453</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5665/sleep.3992" xlink:type="simple">10.5665/sleep.3992</ext-link></comment> <object-id pub-id-type="pmid">25142567</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borbély</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Daan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wirz-Justice</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Deboer</surname> <given-names>T</given-names></name>. <article-title>The two-process model of sleep regulation: a reappraisal</article-title>. <source>Journal of sleep research</source>. <year>2016</year> <month>Apr</month> <day>1</day>;<volume>25</volume>(<issue>2</issue>):<fpage>131</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/jsr.12371" xlink:type="simple">10.1111/jsr.12371</ext-link></comment> <object-id pub-id-type="pmid">26762182</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rempe</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Clegern</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Wisor</surname> <given-names>JP</given-names></name>. <article-title>An automated sleep-state classification algorithm for quantifying sleep timing and sleep-dependent dynamics of electroencephalographic and cerebral metabolic parameters</article-title>. <source>Nature and science of sleep</source>. <year>2015</year>;<volume>7</volume>:<fpage>85</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2147/NSS.S84548" xlink:type="simple">10.2147/NSS.S84548</ext-link></comment> <object-id pub-id-type="pmid">26366107</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kohtoh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Taguchi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Matsumoto</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wada</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>HUANG</surname> <given-names>ZL</given-names></name>, <name name-style="western"><surname>Urade</surname> <given-names>Y</given-names></name>. <article-title>Algorithm for sleep scoring in experimental animals based on fast Fourier transform power spectrum analysis of the electroencephalogram</article-title>. <source>Sleep and Biological Rhythms</source>. <year>2008</year> <month>Jul</month> <day>1</day>;<volume>6</volume>(<issue>3</issue>):<fpage>163</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1479-8425.2008.00355.x" xlink:type="simple">10.1111/j.1479-8425.2008.00355.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bastianini</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Berteotti</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gabrielli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Del Vecchio</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Amici</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Alexandre</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Scammell</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Gazea</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Martire</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Silvani</surname> <given-names>A</given-names></name>. <article-title>SCOPRISM: a new algorithm for automatic sleep scoring in mice</article-title>. <source>Journal of neuroscience methods</source>. <year>2014</year> <month>Sep</month> <day>30</day>;<volume>235</volume>:<fpage>277</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2014.07.018" xlink:type="simple">10.1016/j.jneumeth.2014.07.018</ext-link></comment> <object-id pub-id-type="pmid">25092499</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yaghouby</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>O’Hara</surname> <given-names>BF</given-names></name>, <name name-style="western"><surname>Sunderam</surname> <given-names>S</given-names></name>. <article-title>Unsupervised Estimation of Mouse Sleep Scores and Dynamics Using a Graphical Model of Electrophysiological Measurements</article-title>. <source>International journal of neural systems</source>. <year>2016</year> <month>Jun</month>;<volume>26</volume>(<issue>04</issue>):<fpage>1650017</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1142/S0129065716500179" xlink:type="simple">10.1142/S0129065716500179</ext-link></comment> <object-id pub-id-type="pmid">27121993</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Supratak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Matthews</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>Y</given-names></name>. <article-title>Mixed neural network approach for temporal sleep stage classification</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>. <year>2018</year> <month>Feb</month>;<volume>26</volume>(<issue>2</issue>):<fpage>324</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNSRE.2017.2733220" xlink:type="simple">10.1109/TNSRE.2017.2733220</ext-link></comment> <object-id pub-id-type="pmid">28767373</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rytkönen</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Zitting</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Porkka-Heiskanen</surname> <given-names>T</given-names></name>. <article-title>Automated sleep scoring in rats and mice using the naive Bayes classifier</article-title>. <source>Journal of neuroscience methods</source>. <year>2011</year> <month>Oct</month> <day>30</day>;<volume>202</volume>(<issue>1</issue>):<fpage>60</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2011.08.023" xlink:type="simple">10.1016/j.jneumeth.2011.08.023</ext-link></comment> <object-id pub-id-type="pmid">21884727</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sunagawa</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Séi</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shimba</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Urade</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>HR</given-names></name>. <article-title>FASTER: an unsupervised fully automated sleep staging method for mice</article-title>. <source>Genes to Cells</source>. <year>2013</year> <month>Jun</month> <day>1</day>;<volume>18</volume>(<issue>6</issue>):<fpage>502</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/gtc.12053" xlink:type="simple">10.1111/gtc.12053</ext-link></comment> <object-id pub-id-type="pmid">23621645</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dahl</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Mohamed</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Jaitly</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Senior</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vanhoucke</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nguyen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sainath</surname> <given-names>TN</given-names></name>, <name name-style="western"><surname>Kingsbury</surname> <given-names>B</given-names></name>. <article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2012</year> <month>Nov</month>;<volume>29</volume>(<issue>6</issue>):<fpage>82</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MSP.2012.2205597" xlink:type="simple">10.1109/MSP.2012.2205597</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref012">
<label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. InAdvances in neural information processing systems 2012 (pp. 1097-1105).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Karpathy A, Toderici G, Shetty S, Leung T, Sukthankar R, Fei-Fei L. Large-scale video classification with convolutional neural networks. InProceedings of the IEEE conference on Computer Vision and Pattern Recognition 2014 (pp. 1725-1732).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Donahue J, Anne Hendricks L, Guadarrama S, Rohrbach M, Venugopalan S, Saenko K, Darrell T. Long-term recurrent convolutional networks for visual recognition and description. InProceedings of the IEEE conference on computer vision and pattern recognition 2015 (pp. 2625-2634).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Kim Y. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882. 2014 Aug 25.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref016">
<label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Graves A, Mohamed AR, Hinton G. Speech recognition with deep recurrent neural networks. InAcoustics, speech and signal processing (icassp), 2013 ieee international conference on 2013 May 26 (pp. 6645-6649). IEEE.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref017">
<label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Yu, Dong, and Li Deng. AUTOMATIC SPEECH RECOGNITION. SPRINGER LONDON Limited, 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Chan W, Jaitly N, Le Q, Vinyals O. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. InAcoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on 2016 Mar 20 (pp. 4960-4964). IEEE.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hsu</surname> <given-names>YL</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>YT</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>CY</given-names></name>. <article-title>Automatic sleep stage recurrent neural classifier using energy features of EEG signals</article-title>. <source>Neurocomputing</source>. <year>2013</year> <month>Mar</month> <day>15</day>;<volume>104</volume>:<fpage>105</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neucom.2012.11.003" xlink:type="simple">10.1016/j.neucom.2012.11.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Längkvist</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Karlsson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Loutfi</surname> <given-names>A</given-names></name>. <article-title>Sleep stage classification using unsupervised feature learning</article-title>. <source>Advances in Artificial Neural Systems</source>. <year>2012</year> <month>Jul</month> <day>24</day>;<volume>2012</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Supratak</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>Y</given-names></name>. <article-title>DeepSleepNet: A model for automatic sleep stage scoring based on raw single-channel EEG</article-title>. <source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source>. <year>2017</year> <month>Nov</month>;<volume>25</volume>(<issue>11</issue>):<fpage>1998</fpage>–<lpage>2008</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNSRE.2017.2721116" xlink:type="simple">10.1109/TNSRE.2017.2721116</ext-link></comment> <object-id pub-id-type="pmid">28678710</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sors</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bonnet</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mirek</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vercueil</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Payen</surname> <given-names>JF</given-names></name>. <article-title>A convolutional neural network for sleep stage scoring from raw single-channel EEG</article-title>. <source>Biomedical Signal Processing and Control</source>. <year>2018</year> <month>Apr</month> <day>30</day>;<volume>42</volume>:<fpage>107</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bspc.2017.12.001" xlink:type="simple">10.1016/j.bspc.2017.12.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Bashivan P, Rish I, Yeasin M, Codella N. Learning representations from EEG with deep recurrent-convolutional neural networks. arXiv preprint arXiv:1511.06448. 2015 Nov 19.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhao M, Yue S, Katabi D, Jaakkola TS, Bianchi MT. Learning sleep stages from radio signals: A conditional adversarial architecture. InInternational Conference on Machine Learning 2017 Jul 17 (pp. 4100-4109).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franken</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Malafosse</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tafti</surname> <given-names>M</given-names></name>. <article-title>Genetic variation in EEG activity during sleep in inbred mice</article-title>. <source>American Journal of Physiology-Regulatory, Integrative and Comparative Physiology</source>. <year>1998</year> <month>Oct</month> <day>1</day>;<volume>275</volume>(<issue>4</issue>):<fpage>R1127</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/ajpregu.1998.275.4.R1127" xlink:type="simple">10.1152/ajpregu.1998.275.4.R1127</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abdel-Hamid</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Mohamed</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Jiang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Penn</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>D</given-names></name>. <article-title>Convolutional neural networks for speech recognition</article-title>. <source>IEEE/ACM Transactions on audio, speech, and language processing</source>. <year>2014</year> <month>Oct</month>;<volume>22</volume>(<issue>10</issue>):<fpage>1533</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASLP.2014.2339736" xlink:type="simple">10.1109/TASLP.2014.2339736</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tafti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Petit</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chollet</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Neidhart</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>De Bilbao</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kiss</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Franken</surname> <given-names>P</given-names></name>. <article-title>Deficiency in short-chain fatty acid <italic>β</italic>-oxidation affects theta oscillations during sleep</article-title>. <source>Nature genetics</source>. <year>2003</year> <month>Jul</month>;<volume>34</volume>(<issue>3</issue>):<fpage>320</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ng1174" xlink:type="simple">10.1038/ng1174</ext-link></comment> <object-id pub-id-type="pmid">12796782</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>ANSARI</surname> <given-names>Amir Hossein</given-names></name>, <etal>et al</etal>. <article-title>Weighted Performance Metrics for Automatic Neonatal Seizure Detection Using Multiscored EEG Data</article-title>. <source>IEEE journal of biomedical and health informatics</source>, <year>2018</year>, <volume>22</volume>. Jg., Nr. <issue>4</issue>, S. <fpage>1114</fpage>–<lpage>1123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/JBHI.2017.2750769" xlink:type="simple">10.1109/JBHI.2017.2750769</ext-link></comment> <object-id pub-id-type="pmid">28910781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baumann</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Kilic</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Petit</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Werth</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hermann</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Tafti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bassetti</surname> <given-names>CL</given-names></name>. <article-title>Sleep EEG changes after middle cerebral artery infarcts in mice: different effects of striatal and cortical lesions</article-title>. <source>Sleep</source>. <year>2006</year> <month>Oct</month> <day>1</day>;<volume>29</volume>(<issue>10</issue>):<fpage>1339</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/sleep/29.10.1339" xlink:type="simple">10.1093/sleep/29.10.1339</ext-link></comment> <object-id pub-id-type="pmid">17068988</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>Convolutional networks for images, speech, and time series</article-title>. <source>The handbook of brain theory and neural networks</source>. <year>1995</year> <month>Apr</month>;<volume>3361</volume>(<issue>10</issue>):<fpage>1995</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref032">
<label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <source>Deep learning</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT press</publisher-name>; <year>2016</year> <month>Nov</month> <day>18</day>.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. InProceedings of the 27th international conference on machine learning (ICML-10) 2010 (pp. 807-814).</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma DP, Ba J. dam: A method for stochastic optimization. ICLR, 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">LeCun Y, Bottou L, Orr GB, Müller KR. Efficient backprop. InNeural networks: Tricks of the trade 1998 (pp. 9-50). Springer, Berlin, Heidelberg.</mixed-citation>
</ref>
<ref id="pcbi.1006968.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Benington</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Heller</surname> <given-names>HC</given-names></name>. <article-title>REM-sleep timing is controlled homeostatically by accumulation of REM-sleep propensity in non-REM sleep</article-title>. <source>American Journal of Physiology-Regulatory, Integrative and Comparative Physiology</source>. <year>1994</year> <month>Jun</month> <day>1</day>;<volume>266</volume>(<issue>6</issue>):<fpage>R1992</fpage>–<lpage>2000</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/ajpregu.1994.266.6.R1992" xlink:type="simple">10.1152/ajpregu.1994.266.6.R1992</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006968.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Borb</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Achermann</surname> <given-names>P</given-names></name>. <article-title>Sleep homeostasis and models of sleep regulation</article-title>. <source>Journal of biological rhythms</source>. <year>1999</year> <month>Dec</month>;<volume>14</volume>(<issue>6</issue>):<fpage>559</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/074873099129000894" xlink:type="simple">10.1177/074873099129000894</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>