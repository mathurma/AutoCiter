<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1005993</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-01760</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Review</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Plant science</subject><subj-group><subject>Plant anatomy</subject><subj-group><subject>Leaves</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Flowering plants</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Plant science</subject><subj-group><subject>Plant anatomy</subject><subj-group><subject>Flowers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computer vision</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data acquisition</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Fruits</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Automated plant species identification—Trends and future directions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2631-1531</contrib-id>
<name name-style="western">
<surname>Wäldchen</surname>
<given-names>Jana</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rzanny</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7204-3972</contrib-id>
<name name-style="western">
<surname>Seeland</surname>
<given-names>Marco</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mäder</surname>
<given-names>Patrick</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Thuringia, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Thuringia, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bucksch</surname>
<given-names>Alexander</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Georgia Warnell School of Forestry and Natural Resources, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">jwald@bgc-jena.mpg.de</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>5</day>
<month>4</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>4</issue>
<elocation-id>e1005993</elocation-id>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Wäldchen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1005993"/>
<abstract>
<p>Current rates of species loss triggered numerous attempts to protect and conserve biodiversity. Species conservation, however, requires species identification skills, a competence obtained through intensive training and experience. Field researchers, land managers, educators, civil servants, and the interested public would greatly benefit from accessible, up-to-date tools automating the process of species identification. Currently, relevant technologies, such as digital cameras, mobile devices, and remote access to databases, are ubiquitously available, accompanied by significant advances in image processing and pattern recognition. The idea of automated species identification is approaching reality. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Plant identification is not exclusively the job of botanists and plant ecologists. It is required or useful for large parts of society, from professionals (such as landscape architects, foresters, farmers, conservationists, and biologists) to the general public (like ecotourists, hikers, and nature lovers). But the identification of plants by conventional means is difficult, time consuming, and (due to the use of specific botanical terms) frustrating for novices. This creates a hard-to-overcome hurdle for novices interested in acquiring species knowledge. In recent years, computer science research, especially image processing and pattern recognition techniques, have been introduced into plant taxonomy to eventually make up for the deficiency in people's identification abilities. We review the technical status quo on computer vision approaches for plant species identification, highlight the main research challenges to overcome in providing applicable tools, and conclude with a discussion of open and future research thrusts.</p>
</abstract>
<funding-group>
<funding-statement>We are funded by the German Ministry of Education and Research (BMBF) grants: 01LC1319A and 01LC1319B (<ext-link ext-link-type="uri" xlink:href="https://www.bmbf.de/" xlink:type="simple">https://www.bmbf.de/</ext-link>); the German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) grant: 3514 685C19 (<ext-link ext-link-type="uri" xlink:href="https://www.bmub.bund.de/" xlink:type="simple">https://www.bmub.bund.de/</ext-link>); and the Stiftung Naturschutz Thüringen (SNT) grant: SNT-082-248-03/2014 (<ext-link ext-link-type="uri" xlink:href="http://www.stiftung-naturschutz-thueringen.de/" xlink:type="simple">http://www.stiftung-naturschutz-thueringen.de/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="2"/>
<page-count count="19"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>One of the most obvious features of organic life is its remarkable diversity [<xref ref-type="bibr" rid="pcbi.1005993.ref001">1</xref>]. Despite the variation of organisms, a more experienced eye soon discerns that organisms can be grouped into taxa. Biology defines taxa as formal classes of living things consisting of the taxon's name and its description [<xref ref-type="bibr" rid="pcbi.1005993.ref002">2</xref>]. The assignment of an unknown living thing to a taxon is called identification [<xref ref-type="bibr" rid="pcbi.1005993.ref003">3</xref>]. This article specifically focuses on plant identification, which is the process of assigning an individual plant to a taxon based on the resemblance of discriminatory and morphological plant characters, ultimately arriving at a species or infraspecific name. These underlying characters can be qualitative or quantitative. Quantitative characters are features that can be counted or measured, such as plant height, flower width, or the number of petals per flower. Qualitative characters are features such as leaf shape, flower color, or ovary position. Individuals of the same species share a combination of relevant identification features. Since no two plants look exactly the same, it requires a certain degree of generalization to assign individuals to species (or, in other words, assign objects to a fuzzy prototype).</p>
<p>The world inherits a very large number of plant species. Current estimates of flowering plant species (angiosperms) range between 220,000 [<xref ref-type="bibr" rid="pcbi.1005993.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref005">5</xref>] and 420,000 [<xref ref-type="bibr" rid="pcbi.1005993.ref006">6</xref>]. Given the average 20,000 word vocabulary of an educated native English speaker, even teaching and learning the "taxon vocabulary" of a restricted region becomes a long-term endeavor [<xref ref-type="bibr" rid="pcbi.1005993.ref007">7</xref>]. In addition to the complexity of the task itself, taxonomic information is often captured in languages and formats hard to understand without specialized knowledge. As a consequence, taxonomic knowledge and plant identification skills are restricted to a limited number of persons today.</p>
<p>The dilemma is exacerbated since accurate plant identification is essential for ecological monitoring and thereby especially for biodiversity conservation [<xref ref-type="bibr" rid="pcbi.1005993.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref009">9</xref>]. Many activities, such as studying the biodiversity of a region, monitoring populations of endangered species, determining the impact of climate change on species distribution, payment of environmental services, and weed control actions are dependent upon accurate identification skills [<xref ref-type="bibr" rid="pcbi.1005993.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref010">10</xref>]. With the continuous loss of biodiversity [<xref ref-type="bibr" rid="pcbi.1005993.ref011">11</xref>], the demand for routine species identification is likely to further increase, while at the same time, the number of experienced experts is limited and declining [<xref ref-type="bibr" rid="pcbi.1005993.ref012">12</xref>].</p>
<p>Taxonomists are asking for more efficient methods to meet identification requirements. More than 10 years ago, Gaston and O’Neill [<xref ref-type="bibr" rid="pcbi.1005993.ref013">13</xref>] argued that developments in artificial intelligence and digital image processing will make automatic species identification based on digital images tangible in the near future. The rich development and ubiquity of relevant information technologies, such as digital cameras and portable devices, has brought these ideas closer to reality. Furthermore, considerable research in the field of computer vision and machine learning resulted in a plethora of papers developing and comparing methods for automated plant identification [<xref ref-type="bibr" rid="pcbi.1005993.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>]. Recently, deep learning convolutional neural networks (CNNs) have seen a significant breakthrough in machine learning, especially in the field of visual object categorization. The latest studies on plant identification utilize these techniques and achieve significant improvements over methods developed in the decade before [<xref ref-type="bibr" rid="pcbi.1005993.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1005993.ref023">23</xref>].</p>
<p>Given these radical changes in technology and methodology and the increasing demand for automated identification, it is time to analyze and discuss the status quo of a decade of research and to outline further research directions. In this article, we briefly review the workflow of applied machine learning techniques, discuss challenges of image based plant identification, elaborate on the importance of different plant organs and characters in the identification process, and highlight future research thrusts.</p>
</sec>
<sec id="sec002">
<title>Machine learning for species identification</title>
<p>From a machine learning perspective, plant identification is a supervised classification problem, as outlined in <xref ref-type="fig" rid="pcbi.1005993.g001">Fig 1</xref>. Solutions and algorithms for such identification problems are manifold and were comprehensively surveyed by Wäldchen and Mäder [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>] and Cope et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>]. The majority of these methods are not applicable right away but rather require a training phase in which the classifier learns to distinguish classes of interest. For species identification, the training phase (orange in <xref ref-type="fig" rid="pcbi.1005993.g001">Fig 1</xref>) comprises the analysis of images that have been independently and accurately identified as taxa and are now used to determine a classifier's parameters for providing maximum discrimination between these trained taxa. In the application phase (green in <xref ref-type="fig" rid="pcbi.1005993.g001">Fig 1</xref>), the trained classifier is then exposed to new images depicting unidentified specimens and is supposed to assign them to one of the trained taxa.</p>
<fig id="pcbi.1005993.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005993.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Fundamental steps of supervised machine learning for image-based species identification.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005993.g001" xlink:type="simple"/>
</fig>
<p>Images are usually composed of millions of pixels with associated color information. This information is too extensive and cluttered to be directly used by a machine learning algorithm. The high dimensionality of these images is therefore reduced by computing feature vectors, i.e., a quantified representation of the image that contains the relevant information for the classification problem. During the last decade, research on automated species identification mostly focused on the development of feature detection, extraction, and encoding methods for computing characteristic feature vectors. Initially, designing and orchestrating such methods was a problem-specific task, resulting in a model customized to the specific application, e.g., the studied plant parts like leaves or flowers. For example, Wu et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref024">24</xref>] employ a processing chain comprised of image binarization to separate background and the leaf, image denoising, contour detection, and eventually extracting geometrical derivations of 12 leaf shape features. The approach was evaluated on 32 species and delivered an identification accuracy of 90%. However, this approach could only deal with species differing largely in their leaf shapes. Jin et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref025">25</xref>] propose leaf tooth features extracted after binarization, segmentation, contour detection, and contour corner detection. The proposed method achieved an average classification rate of around 76% for the eight studied species but is not applicable to species with no significant appearances of leaf teeth [<xref ref-type="bibr" rid="pcbi.1005993.ref019">19</xref>]. The sole step from an image to a feature vector, however, typically required about 90% of the development time and extensive expert knowledge [<xref ref-type="bibr" rid="pcbi.1005993.ref019">19</xref>].</p>
<p>Model-free approaches aim to overcome the described limitations of model-based approaches. They do not employ application-specific knowledge and therefore promise a higher degree of generalization across different classes, i.e., species and their organs. The core concept of model-free approaches is the detection of characteristic interest points and their description using generic algorithms, such as scale-invariant feature transform (SIFT), speeded-up robust features (SURF), and histogram of gradients (HOG). These descriptors capture visual information in a patch around each interest point as orientation of gradients and have been successfully used for manifold plant classification studies, e.g., [<xref ref-type="bibr" rid="pcbi.1005993.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1005993.ref028">28</xref>]. Seeland et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>] comparatively evaluate alternative parts of a model-free image classification pipeline for plant species identification. They found the SURF detector in combination with the SIFT local shape descriptor to be superior over other detector–descriptor combinations. For encoding interest points, in order to form an characteristic image descriptor for classification, they found the Fisher Kernel encoding to be superior.</p>
<p>The next obvious step in automated plant species identification and many other machine learning problems was removing an explicit decision about features to be described entirely. In the last years, deep learning CNNs have seen a significant breakthrough in computer vision due to the availability of efficient and massively parallel computing on graphics processing units (GPUs) and the availability of large-scale image data necessary for training deep CNNs with millions of parameters [<xref ref-type="bibr" rid="pcbi.1005993.ref019">19</xref>]. In contrast to model-based and model-free techniques, CNNs do not require explicit and hand-crafted feature detection and extraction steps. Instead, both become part of the iterative training process, which automatically discovers a statistically suitable image representation (similar to a feature vector) for a given problem. The fundamental concept of deep learning is a hierarchical image representation composed of building blocks with increasing complexity per layer. In a similar way, nature is compositional, i.e., small units form larger units, and each aggregation level increases the diversity of the resulting structure (<xref ref-type="fig" rid="pcbi.1005993.g002">Fig 2</xref>). Such hierarchical representations achieve classification performances that were mostly unachievable using shallow learning methods with or without hand-crafted features (see <xref ref-type="table" rid="pcbi.1005993.t001">Table 1</xref>).</p>
<fig id="pcbi.1005993.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005993.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Botanists' (left) versus computer vision (right) description of flowers.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005993.g002" xlink:type="simple"/>
</fig>
<table-wrap id="pcbi.1005993.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005993.t001</object-id>
<label>Table 1</label> <caption><title>Increasing classification accuracy achieved with evolving machine learning approaches on popular plant species benchmark datasets.</title></caption>
<alternatives>
<graphic id="pcbi.1005993.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005993.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify"/>
<th align="justify" colspan="2">Model-based approach</th>
<th align="justify" colspan="2">Model-free approach</th>
<th align="justify" colspan="2">Deep learning</th>
</tr>
<tr>
<th align="justify">Dataset</th>
<th align="justify">Accuracy</th>
<th align="justify">Author</th>
<th align="justify">Accuracy</th>
<th align="justify">Author</th>
<th align="justify">Accuracy</th>
<th align="justify">Author</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify">Swedish leaf</td>
<td align="justify">82.0%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref030">30</xref>]</td>
<td align="justify">93.7%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref026">26</xref>]</td>
<td align="justify">99.8%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref018">18</xref>]</td>
</tr>
<tr>
<td align="justify">Flavia</td>
<td align="justify">90.3%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref024">24</xref>]</td>
<td align="justify">95.9%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref027">27</xref>]</td>
<td align="justify">99.7%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref031">31</xref>]</td>
</tr>
<tr>
<td align="justify">Leafsnap</td>
<td align="justify">73.0%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref014">14</xref>]</td>
<td align="justify">72.6%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref020">20</xref>]</td>
<td align="justify">97.6%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref020">20</xref>]</td>
</tr>
<tr>
<td align="justify">ICL</td>
<td align="justify">83.8%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref032">32</xref>]</td>
<td align="justify">91.3%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref032">32</xref>]</td>
<td align="justify">93.9%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref021">21</xref>]</td>
</tr>
<tr>
<td align="justify">Oxford Flower 17</td>
<td align="justify">-</td>
<td align="justify">-</td>
<td align="justify">91.8%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>]</td>
<td align="justify">96.6%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref022">22</xref>]</td>
</tr>
<tr>
<td align="justify">Oxford Flower 102</td>
<td align="justify">-</td>
<td align="justify">-</td>
<td align="justify">90.2%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref028">28</xref>]</td>
<td align="justify">96.6%</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref023">23</xref>]</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec003">
<title>Challenges in image-based taxa identification</title>
<p>In providing a reliable and applicable automated species identification process, researchers need to consider the following main challenges: (a) a vast number of taxa to be discriminated from one another; (b) individuals of the same species that vary hugely in their morphology; (c) different species that are extremely similar to one another; (d) specimen or other objects that are not covered by the trained classifier; and (e) large variation induced by the image acquisition process in the field.</p>
<sec id="sec004">
<title>Large number of taxa to be discriminated</title>
<p>The world exhibits a very large number of plant species. Distinguishing between a large number of classes is inherently more complex than distinguishing between just a few and typically requires substantially more training data to achieve satisfactory classification performance. Even when restricting the focus to the flora of a region, thousands of species need to be supported. For example, the flora of the German state of Thuringia exhibits about 1,600 flowering species [<xref ref-type="bibr" rid="pcbi.1005993.ref033">33</xref>]. Similarly, when restricting the focus to a single genus, this may still contain many species, e.g., the flowering plant genus <italic>Dioscorea</italic> aggregates over 600 species [<xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>]. Only a few studies with such large numbers of categories have been conducted so far. For example, the important "ImageNet Large Scale Visual Recognition Challenge 2017" involves 1,000 categories that cover a wide variety of objects, animals, scenes, and even some abstract geometric concepts such as a hook or a spiral [<xref ref-type="bibr" rid="pcbi.1005993.ref034">34</xref>].</p>
</sec>
<sec id="sec005">
<title>Large intraspecific visual variation</title>
<p>Plants belonging to the same species may show considerable differences in their morphological characteristics depending on their geographical location and different abiotic factors (e.g., moisture, nutrition, and light condition), their development stage (e.g., differences between a seedling and a fully developed plant), the season (e.g., early flowering stage to a withered flower), and the daytime (e.g., the flower is opening and closing during the day). These changes in morphological characteristics can occur on the scale of individual leaves (e.g., area, width, length, shape, orientation, and thickness), flowers (e.g., size, shape, and color), and fruits but may also affect the entire plant. Examples of visual differences of flowers during the daytime and the season are given in <xref ref-type="fig" rid="pcbi.1005993.g003">Fig 3</xref>. In addition to the spatial and temporal variation, the shape of leaves and flowers may vary continuously or discretely along a single individual. For example, the leaf shape of field scabious (<italic>Knautia arvensis</italic>), a common plant in grassy places, ranges from large entire or dentate lanceolate ground leafs over deeply lobed and almost pinnate stem leafs to small and again lanceolate and entire upper stem leafs. Furthermore, diseases commonly affect the surface of leaves, ranging from discoloration to distinct marking, while insects often alter a leaf's shape by consuming parts of it. Some of this variation is systematic, particularly the allometric scaling of many features, but much variation is also idiosyncratic, reflecting the expression of individual genotypic and phenotypic variation related to the factors mentioned.</p>
<fig id="pcbi.1005993.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005993.g003</object-id>
<label>Fig 3</label>
<caption>
<title/>
<p>Visual variation of <italic>Lapsana communis</italic>'s flower throughout the day from two perspectives (left) and visual variation of <italic>Centaurea pseudophrygia</italic>'s flower throughout the season and flowering stage (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005993.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Small interspecific visual variation</title>
<p>Closely related species may be extremely similar to one another. Even experienced botanists are challenged to safely distinguish species that can be identified only by almost invisible characteristics [<xref ref-type="bibr" rid="pcbi.1005993.ref035">35</xref>]. Detailed patterns in the form of particular morphological structures may be crucial and may not always be readily captured, e.g., in images of specimens. For example, the presence of flowers and fruits is often required for an accurate discrimination between species with high interspecific similarity, but these important characteristics are not present during the whole flowering season and therefore are missing in many images. Furthermore, particular morphological structures which are crucial for discrimination may not be captured in an image of a specimen, even when the particular organ is visible (e.g., the number of stamens or ovary position in the flower).</p>
</sec>
<sec id="sec007">
<title>Rejecting untrained taxa</title>
<p>An automated taxon identification approach not only needs to be able to match an individual specimen to one of the known taxa, but should also be able to reject specimens that belong to a taxon that was not part of the training set. In order to reject unknown taxa, the classification method could produce low classification scores across all known classes for "new" taxa. However, aiming for a classifier with such characteristics conflicts with the goal of tolerating large intraspecific variation in classifying taxa. Finding a trade-off between sensitivity and specificity is a particular challenge in classifier design and training.</p>
</sec>
<sec id="sec008">
<title>Variation induced by the acquisition process</title>
<p>Further variation is added to the images through the acquisition process itself. Living plants represent 3D objects, while images capture 2D projections, resulting in potentially large differences in shape and appearance, depending on the perspective from which the image is taken. Furthermore, image-capturing typically occurs in the field with limited control of external conditions, such as illumination, focus, zoom, resolution, and the image sensor itself [<xref ref-type="bibr" rid="pcbi.1005993.ref002">2</xref>]. These variations are especially relevant for an automated approach in contrast to human perception.</p>
</sec>
</sec>
<sec id="sec009">
<title>Status quo</title>
<p>In the last decade, research in computer vision and machine learning has stimulated manifold methods for automated plant identification. Existing image-based plant identification approaches differ in three main aspects: (a) the analyzed plant organs, (b) the analyzed organ characters, and (c) the complexity of analyzed images. An extensive overview of studied methods is given by Wäldchen and Mäder [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>] and is briefly summarized below.</p>
<sec id="sec010">
<title>Relevant organs for automated identification</title>
<p>Above the ground, plants may be composed of four visible organ types: stem, leaf, flower, and fruit. In a traditional identification process, people typically consider the plant as a whole, but also the characteristics of one or more of these organs to distinguish between taxa. In case of automated identification, organ characteristics were analyzed separately, too. For the following reasons one image alone is typically not sufficient: (a) organs may differ in scale and cannot be depicted in detail along with the whole plant or other organs; and (b) different organs require different optimal image perspectives (e.g., leaves are most descriptive from the top, while the stem is better depicted from the side).</p>
<p>A majority of previous studies solely utilized the <bold>leaf</bold> for discrimination [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. The reason is a more methodological one, rather than meaning that leaves are a more discriminative part of plants from a botanical perspective. On the contrary, manual identification of plants in the vegetative state is considered much more challenging than in the flowering state. From a computer vision perspective, leaves have several advantages over other plant morphological structures, such as flowers, stems, or fruits. Leaves are available for examination throughout most of the year. They can easily be collected, preserved, and imaged due to their planar geometric properties. These aspects simplify the data acquisition process [<xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>] and have made leaves the dominantly studied plant organ for automated identification methods in the past. In situ top-side leaf images in front of a natural background were shown to be the most effective nondestructive type of image acquisition [<xref ref-type="bibr" rid="pcbi.1005993.ref036">36</xref>]. Leaves usually refer only to broad leaves, while needles were neglected or treated separately.</p>
<p>Often, the visually most prominent and perceivable part of a plant is its <bold>flower</bold>. Traditional identification keys intensively refer to flowers and their parts for determination. In contrast, previous studies on automated identification rarely used flowers for discrimination [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. Typically, flowers are only available during the blooming season, i.e., a short period of the year. Due to being complex 3D objects, there is a considerable number of variations in viewpoint, occlusions, and scale of flower images compared to leaf images. If captured in their habitat, images of flowers vary due to lighting conditions, time, date, and weather. All these aspects make flower-based classification a challenging task. However, accurate, automated identification supporting a realistic number of taxa will hardly be successful without the analysis of flowers.</p>
<p>Towards a more mature automated identification approach, solely analyzing one organ will often not be sufficient, especially when considering all the challenges discussed in the previous section. Therefore, more recent research started exploring <bold>multi-organ-based plant identification</bold>. The Cross Language Evaluation Forum (ImageCLEF) conference has organized a challenge dedicated to plant identification since 2011. The challenge is described as plant species retrieval based on multi-image plant observation queries and is accompanied by a dataset containing different organs of plants since 2014. Participating in the challenge, Joly et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref037">37</xref>] proposed a multiview approach that analyzes up to five images of a plant in order to identify a species. This multiview approach allows classification at any period of the year, as opposed to purely leaf-based or flower-based approaches that rely on the supported organ to be visible. Initial experiments demonstrate that classification accuracy benefits from the complementarities of the different views, especially in discriminating ambiguous taxa [<xref ref-type="bibr" rid="pcbi.1005993.ref037">37</xref>]. A considerable burden in exploring this research direction is acquiring the necessary training data. However, by using mobile devices and customized apps (e.g., Pl@ntNet [<xref ref-type="bibr" rid="pcbi.1005993.ref038">38</xref>], Flora Capture [<xref ref-type="bibr" rid="pcbi.1005993.ref039">39</xref>]), it is possible to quickly capture multiple images of the same plant observed at the same time, by the same person, and with the same device. Each image, being part of such an observation, can be labeled with contextual metadata, such as the displayed organ (e.g., plant, branch, leaf, fruit, flower, or stem), time and date, and geolocation, as well as the observer.</p>
<p>It is beneficial if training images cover a large variety of scenarios, i.e., different organs from multiple perspective and at varying scale. This helps the model to learn adequate representations under varying circumstances. Furthermore, images of the same organ acquired from different perspectives often contain complementary visual information, improving accuracy in observation-based identification using multiple images. A <bold>structured observation</bold> approach with well defined image conditions (e.g., Flora Capture) is beneficial for finding a balance between a tedious observation process acquiring every possible scenario and a superficial acquisition that misses the characteristic images required for training.</p>
</sec>
<sec id="sec011">
<title>Relevant characters for automated identification</title>
<p>A plant and its organs (i.e., objects in computer vision) can be described by various characters, such as color, shape, growing position, inflorescence of flowers, margin, pattern, texture, and vein structure of the leaves. These characters are extensively used for traditional identification, with many of them also being studied for automated identification. Previous research proposed numerous methods for describing general as well as domain-specific characteristics. Extensive overviews of the utilized characteristics, as well as of the methods used for capturing them in a formal description, are given by Wäldchen and Mäder [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>] and Cope et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>].</p>
<p><bold>Leaf shape</bold> is the most studied characteristic for plant identification. A plethora of methods for its description can be found in previous work [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>]. Also, most traditional taxonomic keys involve leaf shape for discrimination, the reason being that, although species' leaf shape differs in detail, general shape types can easily be distinguished by people. However, while traditional identification categorizes leaf shape into classes (e.g., ovate, oblique, oblanceolate), computerized shape descriptors either analyze the contour or the whole region of a leaf. Initially, basic geometric descriptors, such as aspect ratio, rectangularity, circularity, and eccentricity, were used to describe a shape. Later, more sophisticated descriptions, such as center contour distance, Fourier descriptors, and invariant moments, were intensively studied [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref017">17</xref>].</p>
<p>In addition to the shape characteristic, various researchers also studied <bold>leaf texture</bold>, described by methods like Gabor filters, gray-level co-occurrence matrices (GLCM), and fractal dimensions [<xref ref-type="bibr" rid="pcbi.1005993.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1005993.ref042">42</xref>]. Although texture is often overshadowed by shape as the dominant or more discriminative feature for leaf classification, it has been demonstrated to be of high discriminative power and complementary to shape information [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref043">43</xref>]. In particular, leaf texture captures leaf venation information as well as any eventual directional characteristics, and more generally allows describing fine nuances or micro-texture at the leaf surface [<xref ref-type="bibr" rid="pcbi.1005993.ref044">44</xref>]. Furthermore, leaf texture analysis allows to classify a plant by having only a portion of a leaf available without depending, e.g., on the shape of the full leaf or its color. Therefore, texture analysis can be beneficial for botanists and researchers that aim to identify damaged plants.</p>
<p>The <bold>vein structure</bold> as a leaf-specific characteristic also played a subordinate role in previous studies. Venation extraction is not trivial, mainly due to a possible low contrast between the venation and the rest of the leaf blade structure [<xref ref-type="bibr" rid="pcbi.1005993.ref045">45</xref>]. Some authors have simplified the task by using special equipment and treatments that render images with more clearly identified veins (mainly chemical leaf clarification) [<xref ref-type="bibr" rid="pcbi.1005993.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref046">46</xref>]. However, this defeats the goal of having users get an automated identification for specimens that they have photographed with ordinary digital cameras.</p>
<p><bold>Leaf color</bold> is considered a less discriminative character than shape and texture. Leaves are mostly colored in some shade of green that varies greatly under different illumination [<xref ref-type="bibr" rid="pcbi.1005993.ref044">44</xref>], creating a low interclass color variability. In addition, there is high intraclass variability. For example, the leaves belonging to the same species or even the same plant can present a wide range of colors depending on the season and the plant's overall condition (e.g., nutrients and water). Regardless of the aforementioned complications, color may still contribute to plant identification, e.g., for considering leaves that exhibit an extraordinary hue [<xref ref-type="bibr" rid="pcbi.1005993.ref044">44</xref>]. However, further investigation on the leaf color character is required.</p>
<p>While the shape of the leaves is of very high relevance, <bold>flower shape</bold> has hardly been considered so far. Interestingly, flower shape is an important characteristic in the traditional identification process. It is dividing plants into families and genera and is thereby considerably narrowing the search space for identification. However, previous attempts for describing flower shape in a computable form did not find it to be very discriminative [<xref ref-type="bibr" rid="pcbi.1005993.ref047">47</xref>]. A major reason is the complex 3D structure of flowers, which makes its shape vary depending on the perspective from which an image was taken. Furthermore, flower petals are often soft and flexible, which is making them bend, curl or twist and letting the shape of the same flower appear very differently. A flower's shape also changes throughout the season [<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>] and with its age to the extent where petals even fall off [<xref ref-type="bibr" rid="pcbi.1005993.ref048">48</xref>], as visualized in <xref ref-type="fig" rid="pcbi.1005993.g003">Fig 3</xref>.</p>
<p><bold>Flower color</bold> is a more discriminative character [<xref ref-type="bibr" rid="pcbi.1005993.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref049">49</xref>]. Many traditional field guides divide plants into groups according to their flower color. For automated identification, color has been mostly described by color moments and color histograms [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. Due to the low dimensionality and the low computational complexity of these descriptors, they are also suitable for real-time applications. However, solely analyzing color characters, without, e.g., considering flower shape, cannot classify flowers effectively [<xref ref-type="bibr" rid="pcbi.1005993.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref049">49</xref>]. Flowers are often transparent to some degree, i.e., the perceived color of a flower differs depending on whether the light comes from the back or the front of the flower. Since flower images are taken under different environmental conditions, the variation in illumination is greatly affecting analysis results. This motivated the beneficial usage of photometric invariant color characters [<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref050">50</xref>].</p>
<p>Various previous studies showed that no single character may be sufficient to separate all desired taxa, making character selection and description a challenging problem. For example, whilst leaf shape may be sufficient to distinguish some taxa, others may look very similar to each other but have differently colored leaves or texture patterns. The same applies to flowers, where specimens of the same color may differ in their shape or texture. Therefore, various studies do not only consider one type of character but use a <bold>combination of characteristics</bold> for describing leaves and flowers [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. The selection of characteristics is always specific for a certain set of taxa and might not be applicable to others. Meaningful characters for, e.g., flower shape can only be derived if there are flowers of sufficient size and potentially flat structure. The same applies to leaf shape and texture. This reflects a fundamental drawback of shallow learning methods using hand-crafted features for specific characters.</p>
</sec>
<sec id="sec012">
<title>Deep learning</title>
<p>Deep artificial neural networks automate the critical feature extraction step by learning a suitable representation of the training data and by systematically developing a robust classification model. Since about 2010, extensive studies with folded neural networks have been conducted on various computer vision problems. In 2012, for the first time a deep learning network architecture with eight layers (AlexNet) won the prestigious ImageNet Challenge (ILSVRC) [<xref ref-type="bibr" rid="pcbi.1005993.ref051">51</xref>]. In the following years, the winning architectures grew in depth and provided more sophisticated mechanisms that centered around the design of layers, the skipping of connections, and on improving gradient flow. In 2015, ResNet [<xref ref-type="bibr" rid="pcbi.1005993.ref052">52</xref>] won ILSVRC with a 152 layer architecture and reached a top-5 classification error of 3.6%, being better than human performance (5.1%) [<xref ref-type="bibr" rid="pcbi.1005993.ref034">34</xref>]. As for many object classification problems, CNNs produce promising and constantly improving results on automated plant species identification. One of the first studies on plant identification utilizing CNNs is Lee et al.'s [<xref ref-type="bibr" rid="pcbi.1005993.ref053">53</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref054">54</xref>] leaf classifier that uses the AlexNet architecture pretrained on the ILSVRC2012 dataset and reached an average accuracy of 99.5% on a dataset covering 44 species. Zhang et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref055">55</xref>] used a six-layer CNN to classify the Flavia dataset and obtained an accuracy of 94,69%. Barre et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref019">19</xref>] further improved this result by using a 17-layer CNN and obtained an accuracy of 97.9%. Eventually, Sun et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref031">31</xref>] study the ResNet architecture and found a 26-layer network to reach best performance with 99.65% on the Flavia dataset. Simon et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref056">56</xref>] used CNNs (AlexNet and VGG19) for feature detection and extraction inside a part constellation modeling framework. Using Support Vector Machine (SVM) as classifier, they achieved 95.34% on the Oxford Flowers 102 dataset. <xref ref-type="table" rid="pcbi.1005993.t001">Table 1</xref> contrasts the best previously reported classification results of model-based, model-free and CNN-based approaches on benchmark plant image datasets. A comparison shows that CNN classification performance was unachievable using traditional and shallow learning approaches.</p>
</sec>
<sec id="sec013">
<title>Training data and benchmarks</title>
<p>Merely half of the previous studies on automated plant identification evaluated the proposed method with established benchmark datasets allowing for replication of studies and comparison of methods (see <xref ref-type="table" rid="pcbi.1005993.t002">Table 2</xref>). The other half solely used proprietary leaf image datasets not available to the public [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>].</p>
<table-wrap id="pcbi.1005993.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1005993.t002</object-id>
<label>Table 2</label> <caption><title>Overview of previously studied benchmark datasets.</title></caption>
<alternatives>
<graphic id="pcbi.1005993.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1005993.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Dataset</th>
<th align="justify">Author</th>
<th align="justify"># Species</th>
<th align="justify"># Images</th>
<th align="justify">Acquisition</th>
<th align="justify">Background</th>
<th align="justify">Organs</th>
<th align="justify">Life form</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify">Swedish leaf</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref030">30</xref>]</td>
<td align="justify">15</td>
<td align="justify">1,125</td>
<td align="justify">scan</td>
<td align="justify">plain</td>
<td align="justify">leaves</td>
<td align="justify">trees</td>
</tr>
<tr>
<td align="justify">Flavia</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref024">24</xref>]</td>
<td align="justify">32</td>
<td align="justify">1,907</td>
<td align="justify">scan + photo</td>
<td align="justify">plain</td>
<td align="justify">leaves</td>
<td align="justify">trees</td>
</tr>
<tr>
<td align="justify">Leafsnap</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref058">58</xref>]</td>
<td align="justify">185</td>
<td align="justify">30,866</td>
<td align="justify">scan + photo</td>
<td align="justify">plain</td>
<td align="justify">leaves</td>
<td align="justify">trees</td>
</tr>
<tr>
<td align="justify">ICL</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref059">59</xref>]</td>
<td align="justify">220</td>
<td align="justify">17,032</td>
<td align="justify">scan + photo</td>
<td align="justify">plain</td>
<td align="justify">leaves</td>
<td align="justify">herb, tree</td>
</tr>
<tr>
<td align="justify">Oxford Flower 17</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref048">48</xref>]</td>
<td align="justify">17</td>
<td align="justify">1,360</td>
<td align="justify">photo</td>
<td align="justify">natural</td>
<td align="justify">flower</td>
<td align="justify">herbs</td>
</tr>
<tr>
<td align="justify">Jena Flower 30</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>]</td>
<td align="justify">30</td>
<td align="justify">1,479</td>
<td align="justify">photo</td>
<td align="justify">natural</td>
<td align="justify">flower</td>
<td align="justify">herbs</td>
</tr>
<tr>
<td align="justify">Oxford Flower 102</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref049">49</xref>]</td>
<td align="justify">102</td>
<td align="justify">8,189</td>
<td align="justify">photo</td>
<td align="justify">natural</td>
<td align="justify">flower</td>
<td align="justify">herbs</td>
</tr>
<tr>
<td align="justify">PlantCLEF16</td>
<td align="justify">[<xref ref-type="bibr" rid="pcbi.1005993.ref060">60</xref>]</td>
<td align="justify">1,000</td>
<td align="justify">113,205</td>
<td align="justify">photo</td>
<td align="justify">natural</td>
<td align="justify">fruit, flower, leaves, stem</td>
<td align="justify">herb, tree, fern</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The images contained in these datasets (proprietary as well as benchmark) fall into three categories: scans, pseudo-scans, and photos. While scan and pseudo-scan categories correspond respectively to leaf images obtained through scanning and photography in front of a simple background, the photo category corresponds to leaves or flowers photographed on natural background. The majority of utilized leaf images are scans and pseudo-scans [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. Typically fresh material, i.e., simple, healthy, and not degraded leaves, were collected and imaged in the lab. This fact is interesting since it considerably simplifies the classification task. If the object of interest is imaged against a plain background, the often necessary segmentation for distinguishing foreground and background can be performed in a fully automated way with high accuracy.</p>
<p>Leaves imaged in the natural environment, as well as degraded leaves largely existing in nature, such as deformed, partial, overlapped, and compounded leaves (leaves consisting of two or more leaflets born on the same leafstalk), are largely avoided in the current studies. Segmenting the leaf with natural background is particularly difficult when the background shows a significant amount of overlapping, almost unicolor elements. This is often unavoidable when imaging leaves in their habitat. Interferences around the target leaves, such as small stones and ruderals may create confusion between the boundaries of adjacent leaves. Compound leaves are particularly difficult to recognize and existing studies that are designed for the recognition of simple leaves can hardly be applied directly to compound leaves. This is backed up by the variation of a compound leaf—it is not only caused by morphological differences of leaflets, but also by changes in the leaflet number and arrangements [<xref ref-type="bibr" rid="pcbi.1005993.ref057">57</xref>].</p>
<p>The lower part of <xref ref-type="table" rid="pcbi.1005993.t002">Table 2</xref> shows benchmark datasets containing flower images. The images of the Oxford Flower 17 and 102 datasets have been acquired by searching the internet and by selecting images of species with substantial variation in shape, scale, and viewpoint. The PlantCLEF2015/2016 dataset consists of images with different plant organs or plant views (i.e., entire plant, fruit, leaf, flower, stem, branch, and leaf scan). These images were submitted by a variety of users of the mobile Pl@ntNet application. The recently published Jena Flower 30 dataset [<xref ref-type="bibr" rid="pcbi.1005993.ref029">29</xref>] contains images acquired in the field as top-view flower images using an Apple iPhone 6 throughout an entire flowering season. All images of these flower benchmark datasets are photos taken in the natural environment.</p>
</sec>
<sec id="sec014">
<title>Applicable identification tools</title>
<p>Despite intensive and elaborate research on automated plant species identification, only very few studies resulted in approaches that can be used by the general public, such as Leafsnap [<xref ref-type="bibr" rid="pcbi.1005993.ref061">61</xref>] and Pl@ntNet [<xref ref-type="bibr" rid="pcbi.1005993.ref037">37</xref>]. Leafsnap, developed by researchers from Columbia University, the University of Maryland, and the Smithsonian Institution, was the first widely distributed electronic field guide. Implemented as a mobile app, it uses computer vision techniques for identifying tree species of North America from photographs of their leaves on plain background. The app retrieves photos of leaves similar to the one in question. However, it is up to the user to make the final decision on what species matches the unknown one. LeafSnap achieves a top-1 recognition rate of about 73% and a top-5 recognition rate of 96.8% for 184 tree species [<xref ref-type="bibr" rid="pcbi.1005993.ref061">61</xref>]. The app has attracted a considerable number of downloads but has also received many critical user reviews [<xref ref-type="bibr" rid="pcbi.1005993.ref062">62</xref>] due to its inability to deal with cluttered backgrounds and within-class variance.</p>
<p>Pl@ntNet is an image retrieval and sharing application for the identification of plants. It is being developed in a collaboration of four French research organizations (French agricultural research and international cooperation organization [Cirad], French National Institute for Agricultural Research [INRA], French Institute for Research in Computer Science and Automation [Inria], and French National Research Institute for Sustainable Development [IRD]) and the Tela Botanica network. It offers three front-ends, an Android app, an iOS app, and a web interface, each allowing users to submit one or several pictures of a plant in order to get a list of the most likely species in return. The application is becoming more and more popular. The application has been downloaded by more than 3 million users in about 170 countries. It was initially restricted to a fraction of the European flora (in 2013) and has since been extended to the Indian Ocean and South American flora (in 2015) and the North African flora (in 2016). Since June 2015, Pl@ntNet applies deep learning techniques for image classification. The network is pretrained on the ImageNet dataset and periodically fine-tuned on steadily growing Pl@ntNet data. Joly et al. [<xref ref-type="bibr" rid="pcbi.1005993.ref063">63</xref>] evaluated the Pl@ntNet application, which supported the identification of 2,200 species at that time, and reported a 69% top-5 identification rate for single images. We could not find published evaluation results on the current performance of the image-based identification engine. However, reviews request better accuracy [<xref ref-type="bibr" rid="pcbi.1005993.ref015">15</xref>]. We conclude that computer vision solutions are still far from replacing the botanist in extracting plant characteristic information for identification. Improving the identification performance in any possible way remains an essential objective for future research. The following sections summarize important current research directions.</p>
</sec>
</sec>
<sec id="sec015">
<title>Open problems and future directions</title>
<sec id="sec016">
<title>Utilizing latest machine learning developments</title>
<p>While the ResNet architecture is still state-of-the-art, evolutions are continuously being proposed, (e.g., [<xref ref-type="bibr" rid="pcbi.1005993.ref064">64</xref>]). Other researchers work on alternative architectures like ultra-deep (FractalNet) [<xref ref-type="bibr" rid="pcbi.1005993.ref065">65</xref>] and densely connected (DenseNet) [<xref ref-type="bibr" rid="pcbi.1005993.ref066">66</xref>] networks. These architectures have not yet been evaluated for plant species identification. New architectures and algorithms typically aim for higher classification accuracy, which is clearly a major goal for species identification; however, there are also interesting advances in reducing the substantial computational effort and footprint of CNN classifiers. For example, SqueezeNet [<xref ref-type="bibr" rid="pcbi.1005993.ref067">67</xref>] achieves accuracy comparable to AlexNet but with 50 times fewer parameters and a model that is 510 times smaller. Especially when aiming for identification systems that run on mobile devices, these developments are highly relevant and should be evaluated in this context.</p>
<p>Current studies still mostly operate on the small and nonrepresentative datasets used in the past. Only a few studies train CNN classifiers on large plant image datasets, demonstrating their applicability in automated plant species identification systems [<xref ref-type="bibr" rid="pcbi.1005993.ref068">68</xref>]. Given the typically "small" amounts of available training data and the computational effort for training a CNN, transfer learning has become an accepted procedure (meaning that a classifier will be pretrained on a large dataset, e.g., ImageNet, before the actual training begins). The classifier will then only be fine-tuned to the specific classification problem by training of a small number of high-level network layers proportional to the amount of available problem-specific training data. Researchers argue that this method is superior for problems with ≤ 1 M training images. Most previous studies on plant species identification utilized transfer learning, (e.g., [<xref ref-type="bibr" rid="pcbi.1005993.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref069">69</xref>]). Once a sufficiently large plant dataset has been acquired, it would be interesting to compare current classification results with those of a plant identification CNN solely trained on images depicting plant taxa.</p>
<p>Another approach tackling the issue of small datasets is using data augmentation schemes, commonly including simple modifications of images, such as rotation, translation, flipping, and scaling. Using augmentation for improving the training process has become a standard procedure in computer vision. However, the diversity that can be reached with traditional augmentation schemes is relatively small. This motivates the use of synthetic data samples, introducing more variability and enriching the dataset, in order to improve the training process. A promising approach in this regard are Generative Adversarial Networks (GANs) that are able to generate high-quality, realistic, natural images [<xref ref-type="bibr" rid="pcbi.1005993.ref070">70</xref>].</p>
<p>Without the complicated and time-consuming process for designing an image analysis pipeline, deep learning approaches can also be applied by domain experts directly, i.e., botanists and biologists with only a basic understanding of the underlying machine learning concepts. Large-scale organizations provide a competing and continuously improving set of openly available machine learning frameworks, such as Caffe2, MXNet, PyTorch, and TensorFlow. Developments like Keras specifically target newcomers in machine learning and provide add-ons to these frameworks that aim to simplify the setup of experiments and the analysis of results. Furthermore, it is mostly common practice that researchers make their models and architectures publicly available (model zoos), increasing visibility in their field but also facilitating their application in other studies.</p>
</sec>
<sec id="sec017">
<title>Creating representative benchmarks</title>
<p>Todays benchmark datasets are limited both in the number of species and in the number of images (see <xref ref-type="table" rid="pcbi.1005993.t002">Table 2</xref>) due to the tremendous effort for either collecting fresh specimens and imaging them in a lab or for taking images in the field. Taking a closer look at datasets, it becomes obvious that they were created with an application in computer vision and machine learning in mind. They are typically created by only a few people acquiring specimens or images in a short period of time, from a limited area, and following a rigid procedure for their imaging. As a result, the plants of a given species in those datasets are likely to represent only a few individual plants grown closely together at the same time. Considering the high variability explained before, these datasets do not reflect realistic conditions.</p>
<p>Using such training data in a real-world identification application has little chance to truly classify new images collected at different periods, at different places, and acquired differently [<xref ref-type="bibr" rid="pcbi.1005993.ref063">63</xref>]. Towards real-life applications, studies should utilize more realistic images, e.g., containing multiple, overlapped, and damaged leaves and flowers. Images should have real, complex backgrounds and should be taken under different lighting conditions. Large-scale, well-annotated training datasets with representative data distribution characteristics are crucial for the training of accurate and generalizable classifiers. This is especially true for the training of Deep Convolutional Neural Networks that require extensive training data to properly tune the large set of parameters. The research community working on the ImageNet dataset [<xref ref-type="bibr" rid="pcbi.1005993.ref071">71</xref>] and the related benchmark is particularly important in this regard. ImageNet aims to provide the most comprehensive and diverse coverage of the image world. It currently contains more than 14 million images categorized according to a hierarchy of almost 22,000 English nouns. The average number of training images per category is in the range of 600 and 1,200, being considerable larger than any existing plant image collection.</p>
<p>First efforts have been made recently to create datasets that are specifically designed for machine learning purposes—a huge amount of information, presorted in defined categories. The PlantCLEF plant identification challenge initially provided a dataset containing 71 tree species from the French Mediterranean area depicted in 5,436 images in 2011. This dataset has grown to 113,205 pictures of herb, tree, and fern specimens belonging to 1,000 species living in France and the neighboring countries in 2016. Encyclopedia Of Life (EOL) [<xref ref-type="bibr" rid="pcbi.1005993.ref072">72</xref>], being the world's largest data centralization effort concerning multimedia data for life on earth, currently provides about 3.8 million images for 1.3 million taxa. For angiosperms, there are currently 1.26 million images, but only 68% of them are reviewed and trusted with respect to the identified taxa [<xref ref-type="bibr" rid="pcbi.1005993.ref073">73</xref>].</p>
</sec>
<sec id="sec018">
<title>Crowdsourcing training data</title>
<p>Upcoming trends in crowdsourcing and citizen science offer excellent opportunities to generate and continuously update large repositories of required information. Members of the public are able to contribute to scientific research projects by acquiring or processing data while having few prerequisite knowledge requirements. Crowdsourcing has benefited from Web 2.0 technologies that have enabled user-generated content and interactivity, such as wiki pages, web apps, and social media. iNaturalist and Pl@ntNET already successfully acquire data through such channels [<xref ref-type="bibr" rid="pcbi.1005993.ref037">37</xref>]. Plant image collections that acquire data through crowdsourcing and citizen science projects today often suffer from problems that prevent their effective use as training and benchmark data. First, the number of images per species in many datasets follows a <bold>long-tail distribution</bold>. Thousands of images are acquired for prominent taxa, while less prominent and rare taxa are represented by only a few and sometimes no images at all. The same fact applies to the number of images per organ per taxon. While prominent organs such as the flower of angiosperms are well populated, other organs such as fruits are often underrepresented or even missing. Second, collections contain a high degree of <bold>image and tag heterogeneity</bold>. As we elaborated in our discussion of identification challenges, the acquisition process is a main contributor of image variability. In a crowdsourcing environment, this fact is even exacerbated since contributors with very different backgrounds, motivations, and equipment contribute observations. Image collections today contain many examples not sufficient for an unambiguous identification of the displayed taxon. They may be too blurry or lack details. Collections also suffer from problems such as heterogeneous organ tags (e.g., "leaf" versus "leaves" versus "foliage"), manifold plant species synonyms used alternatively, and evolving and concurrent taxonomies. Third, nonexpert observations are more likely to contain <bold>image and metadata noise</bold>. Image noise refers to problems such as highly cluttered images, other plants depicted along with the intended species, and objects not belonging to the habitat (e.g., fingers or insects). Metadata noise refers to problems such as wrongly identified taxa, wrongly labeled organs, imprecise or incorrect location information, and incorrect observation time and date.</p>
<p>These problems show that crowdsourced content deserves more effort for maintaining sufficient data quality. An examination of a small number of randomly sampled images from the Pl@ntNET initiative and their taxa attributions indicated that misclassifications are in the range of 5% to 10%. In a first attempt to overcome these problems, Pl@ntNET introduced a star-based quality rating for each image and uses a community based review system for taxon annotations, whereas EOL offers a "trusted" tag for each taxon that has been identified within an image by an EOL curator. We argue that multimedia data should be based on common data standards and protocols, such as the Darwin Core [<xref ref-type="bibr" rid="pcbi.1005993.ref074">74</xref>], and that a rigorous review system and quality control workflows should be implemented for community based data assessment.</p>
</sec>
<sec id="sec019">
<title>Analyzing the context of observations</title>
<p>We argue that it is hard to develop a plant identification approach for the worlds estimated 220,000 to 420,000 angiosperms that solely relies on image data. Additional information characterizing the context of a specimen should be taken into consideration. Today, mobile devices allow for high quality images acquired in well choreographed and adaptive procedures. Through software specifically developed for these devices, users can be guided and trained in acquiring characteristic images in situ. Given that mobile devices can geolocalize themselves, acquired data can be spatially referenced with high accuracy allowing to retrieve context information, such as topographic characteristics, climate factors, soil type, land-use type, and biotope. These factors explaining the presence or absence of species are already used to predict plant distribution and should also be considered for their identification. Temporal information, i.e., the date and the time of an observation, could allow adaptation of an identification approach to species' seasonal variations. For example, the flowering period can be of high discriminative power during an identification. Furthermore, recorded observations in public repositories (e.g., Global Biodiversity Information Facility GBIF) can provide valuable hypotheses as to which species are to expect or not to expect at a given location. Finally, additional and still-emerging sensors built into mobile devices allow for measuring environmental variables, such as temperature and air pressure. The latest cameras can acquire depth maps of specimens along with an image and provide additional characteristics of an observation and its context further supporting the identification.</p>
</sec>
<sec id="sec020">
<title>From taxa-based to character-based training</title>
<p>In automated species identification, researchers solely aim to classify on the species level so far. An alternative approach could be classifying plant characteristics (e.g., leaf shape categories, leaf position, flower symmetry) and linking them to plant character databases such as the TRY Plant Trait Database [<xref ref-type="bibr" rid="pcbi.1005993.ref075">75</xref>] for identifying a wide range of taxa. In theory, untrained taxa could be identified by recognizing their characters. So far, it is uncertain whether automated approaches are able to generalize uniform characters from nonuniform visual information. Characters that are shared across different taxa are often differently developed per taxon, making their recognition a particular challenge.</p>
</sec>
<sec id="sec021">
<title>Utilizing the treasure of herbarium specimens</title>
<p>Herbaria all over the world have invested large amounts of money and time in collecting samples of plants. Rather than going into the field for taking images or for collecting specimens anew, it would be considerably less expensive to use specimens of plants that have already been identified and conserved. Today, over 3,000 herbaria in 165 countries possess over 350 million specimens, collected in all parts of the world and over several centuries [<xref ref-type="bibr" rid="pcbi.1005993.ref076">76</xref>]. Currently, many are undertaking large-scale digitization projects to improve their access and to preserve delicate samples. For example, in the USA, more than 1.8 million imaged and georeferenced vascular plant specimens are digitally archived in the iDigBio portal, a nationally funded and primary aggregator of museum specimen data [<xref ref-type="bibr" rid="pcbi.1005993.ref076">76</xref>]. This activity is likely going to be expanded over the coming decade. We can look forward to a time when there will be huge repositories of taxonomic information, represented by specimen images, accessible publicly through the internet. However, very few previous researchers utilized herbaria sheets for generating a leaf image dataset [<xref ref-type="bibr" rid="pcbi.1005993.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1005993.ref077">77</xref>–<xref ref-type="bibr" rid="pcbi.1005993.ref079">79</xref>]. On the other hand, analyzing herbaria specimens may not be suitable for training identification approaches applied in a real environment [<xref ref-type="bibr" rid="pcbi.1005993.ref069">69</xref>]. The material is dried, and thereby, original colors change drastically. Furthermore, all herbaria specimens are imaged flattened on a plain homogeneous background, altering their structure and arrangement. In conclusion, more research on the detection and extraction of characteristics from herbaria specimens is required. It is also an open research question (how to train classifiers on herbaria specimens that are applicable on fresh specimens).</p>
</sec>
<sec id="sec022">
<title>Interdisciplinary collaborations</title>
<p>Twelve years ago, Gaston and O’Neill [<xref ref-type="bibr" rid="pcbi.1005993.ref013">13</xref>] argued that developing successful identification approaches requires novel collaborations between biologists and computer scientists with personnel that have significant knowledge of both biology and computing science. Interestingly, automated plant species identification is still mostly driven by academics specialized in computer vision, machine learning, and multimedia information retrieval. Very few studies were conducted by interdisciplinary groups of biologists and computer scientists in the previous decade [<xref ref-type="bibr" rid="pcbi.1005993.ref016">16</xref>]. Research should move towards more interdisciplinary endeavors. Biologists can apply machine learning methods more effectively with the help of computer scientists, and the latter are able to gain the required exhaustive understanding of the problem they are tacking by working with the former.</p>
</sec>
</sec>
<sec id="sec023">
<title>A vision of automated identification in the wild</title>
<p>We envision identification systems that enable users to take images of specimens in the field with a mobile device's built-in camera system, which are then analyzed by an installed application to identify the taxon or to at least get a list of candidate taxa. This approach is convenient, since the identification requires no work from the user except for taking an image and browsing through the best matching species. Furthermore, minimal expert knowledge is required, which is especially important given the ongoing shortage of skilled botanists. An accurate automated identification system also enables nonexperts with only limited botanical training and expertise to contribute to the survey of the world's biodiversity. Approaching trends and technologies, such as augmented reality, data glasses, and 3D-scans, give such applications a long-term research and application perspective. Furthermore, large-character datasets can be generated automatically (for instance, by taking measurements from thousands of specimens across a single taxon). We cannot only derive more accurate descriptions of a species and its typical character expressions, but also study the statistical distribution of each character, including variance and skew. Furthermore, image processing provides the possibility to extract not only the linear measurements typical of botanical descriptions (leaf length, leaf width, petal length, etc.), but also more sophisticated and precise descriptions such as mathematical models of leaf shapes.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1005993.ref001"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Darwin Charles R. On the origin of species by means of natural selection, or the preservation of favoured races in the struggle for life. Murray, London. 1859.</mixed-citation></ref>
<ref id="pcbi.1005993.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Remagnino</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mayo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wilkin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cope</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kirkup</surname> <given-names>D</given-names></name>. <source>Computational Botany: Methods for Automated Species Identification</source>. <publisher-name>Springer</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hagedorn</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Rambold</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Martellos</surname> <given-names>S</given-names></name>. <chapter-title>Types of identification keys</chapter-title>. In: <source>Tools for Identifying Biodiversity: Progress and Problems</source>. <publisher-name>EUT Edizioni Università di Trieste</publisher-name>; <year>2010</year>. pp. <fpage>59</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scotland</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Wortley</surname> <given-names>AH</given-names></name>. <article-title>How many species of seed plants are there?</article-title> <source>Taxon</source>. <year>2003</year>;<volume>52</volume>(<issue>1</issue>):<fpage>101</fpage>–<lpage>104</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mora</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tittensor</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Adl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Simpson</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Worm</surname> <given-names>B</given-names></name>. <article-title>How many species are there on Earth and in the ocean?</article-title> <source>PLoS Biol</source>. <year>2011</year>;<volume>9</volume>(<issue>8</issue>):<fpage>e1001127</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1001127" xlink:type="simple">10.1371/journal.pbio.1001127</ext-link></comment> <object-id pub-id-type="pmid">21886479</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Govaerts</surname> <given-names>R</given-names></name>. <article-title>How Many Species of Seed Plants Are There?</article-title> <source>Taxon</source>. <year>2001</year>;<volume>50</volume>(<issue>4</issue>):<fpage>1085</fpage>–<lpage>1090</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goulden</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nation</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>J</given-names></name>. <article-title>How large can a receptive vocabulary be?</article-title> <source>Applied Linguistics</source>. <year>1990</year>;<volume>11</volume>(<issue>4</issue>):<fpage>341</fpage>–<lpage>363</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farnsworth</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Chu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kress</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Neill</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Best</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Pickering</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Next-generation field guides</article-title>. <source>BioScience</source>. <year>2013</year>;<volume>63</volume>(<issue>11</issue>):<fpage>891</fpage>–<lpage>899</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elphick</surname> <given-names>CS</given-names></name>. <article-title>How you count counts: the importance of methods research in applied ecology</article-title>. <source>Journal of Applied Ecology</source>. <year>2008</year>;<volume>45</volume>(<issue>5</issue>):<fpage>1313</fpage>–<lpage>1320</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Austen</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Bindemann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Roberts</surname> <given-names>DL</given-names></name>. <article-title>Species identification by experts and non-experts: comparing images from field guides</article-title>. <source>Scientific Reports</source>. <year>2016</year>;<volume>6</volume>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ceballos</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ehrlich</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Barnosky</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Garca</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pringle</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>TM</given-names></name>. <article-title>Accelerated modern human–induced species losses: Entering the sixth mass extinction</article-title>. <source>Science advances</source>. <year>2015</year>;<volume>1</volume>(<issue>5</issue>):<fpage>e1400253</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/sciadv.1400253" xlink:type="simple">10.1126/sciadv.1400253</ext-link></comment> <object-id pub-id-type="pmid">26601195</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref012"><label>12</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hopkins</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Freckleton</surname> <given-names>R</given-names></name>. <chapter-title>Declines in the numbers of amateur and professional taxonomists: implications for conservation</chapter-title>. In: <source>Animal Conservation forum</source>. vol. <volume>5</volume>. <publisher-name>Cambridge University Press</publisher-name>; <year>2002</year>. p. <fpage>245</fpage>–<lpage>249</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>O’Neill</surname> <given-names>MA</given-names></name>. <article-title>Automated species identification: why not?</article-title> <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2004</year>;<volume>359</volume>(<issue>1444</issue>):<fpage>655</fpage>–<lpage>667</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2003.1442" xlink:type="simple">10.1098/rstb.2003.1442</ext-link></comment> <object-id pub-id-type="pmid">15253351</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref014"><label>14</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kumar</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Belhumeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Biswas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kress</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Lopez</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <chapter-title>Leafsnap: A Computer Vision System for Automatic Plant Species Identification</chapter-title>. In: <name name-style="western"><surname>Fitzgibbon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lazebnik</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sato</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schmid</surname> <given-names>C</given-names></name>, editors. <source>Computer Vision–ECCV 2012. Lecture Notes in Computer Science</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2012</year>. p. <fpage>502</fpage>–<lpage>516</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joly</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bonnet</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Goëau</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Barbe</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Selmi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Champ</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>A look inside the Pl@ ntNet experience</article-title>. <source>Multimedia Systems</source>. <year>2016</year>;<volume>22</volume>(<issue>6</issue>):<fpage>751</fpage>–<lpage>766</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wäldchen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mäder</surname> <given-names>P</given-names></name>. <article-title>Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review</article-title>. <source>Archives of Computational Methods in Engineering</source>. <year>2017</year>; <fpage>1</fpage>–<lpage>37</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cope</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Corney</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Remagnino</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wilkin</surname> <given-names>P</given-names></name>. <article-title>Plant species identification using digital morphometrics: A review</article-title>. <source>Expert Systems with Applications</source>. <year>2012</year>;<volume>39</volume>(<issue>8</issue>):<fpage>7562</fpage>–<lpage>7573</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref018"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Pawara P, Okafor E, Schomaker L, Wiering M. Data Augmentation for Plant Classification. In: Proceedings of International ConferenceAdvanced Concepts for Intelligent Vision Systems. Springer International Publishing; 2017. pp. 615–626.</mixed-citation></ref>
<ref id="pcbi.1005993.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barré</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Stöver</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>KF</given-names></name>, <name name-style="western"><surname>Steinhage</surname> <given-names>V</given-names></name>. <article-title>LeafNet: A computer vision system for automatic plant species identification</article-title>. <source>Ecological Informatics</source>. <year>2017</year>; <volume>40</volume>: <fpage>50</fpage>–<lpage>56</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref020"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Pawara P, Okafor E, Surinta O, Schomaker L, Wiering M. Comparing Local Descriptors and Bags of Visual Words to Deep Convolutional Neural Networks for Plant Recognition. In: Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods. ICPRAM; 2017. pp. 479–486.</mixed-citation></ref>
<ref id="pcbi.1005993.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>ming Kan</surname> <given-names>J</given-names></name>. <article-title>Improved deep belief networks and multi-feature fusion for leaf identification</article-title>. <source>Neurocomputing</source>. <year>2016</year>;<volume>216</volume>(<issue>Supplement C</issue>):<fpage>460</fpage>–<lpage>467</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xie</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>XY</given-names></name>, <name name-style="western"><surname>Yan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>CL</given-names></name>. <article-title>SDE: A Novel Selective, Discriminative and Equalizing Feature Representation for Visual Recognition</article-title>. <source>International Journal of Computer Vision</source>. <year>2017</year>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xie</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>XY</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>CL</given-names></name>. <article-title>LG-CNN: From local parts to global discrimination for fine-grained recognition</article-title>. <source>Pattern Recognition</source>. <year>2017</year>;<volume>71</volume>:<fpage>118</fpage>–<lpage>131</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Wu SG, Bao FS, Xu EY, Wang YX, Chang YF, Xiang QL. A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network. In: Proceedings of the IEEE International Symposium on Signal Processing and Information Technology, 2007. pp. 11–16.</mixed-citation></ref>
<ref id="pcbi.1005993.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jin</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hou</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>F</given-names></name>. <article-title>A Novel Method of Automatic Plant Species Identification Using Sparse Representation of Leaf Tooth Features</article-title>. <source>PLoS ONE</source>. <year>2015</year>;<volume>10</volume>(<issue>10</issue>):<fpage>e0139482</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0139482" xlink:type="simple">10.1371/journal.pone.0139482</ext-link></comment> <object-id pub-id-type="pmid">26440281</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Xiao XY, Hu R, Zhang SW, Wang XF. HOG-based Approach for Leaf Classification. In: Proceedings of the Advanced Intelligent Computing Theories and Applications, and 6th International Conference on Intelligent Computing. ICIC'10. Berlin, Heidelberg: Springer-Verlag; 2010. pp.149–155.</mixed-citation></ref>
<ref id="pcbi.1005993.ref027"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Nguyen QK, Le TL, Pham NH. Leaf based plant identification system for Android using SURF features in combination with Bag of Words model and supervised learning. In: Proceedings of the International Conference on Advanced Technologies for Communications (ATC); 2013. pp. 404–407.</mixed-citation></ref>
<ref id="pcbi.1005993.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koniusz</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Yan</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Mikolajczyk</surname> <given-names>K</given-names></name>. <article-title>Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2017</year>;<volume>39</volume>(<issue>2</issue>):<fpage>313</fpage>–<lpage>326</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2016.2545667" xlink:type="simple">10.1109/TPAMI.2016.2545667</ext-link></comment> <object-id pub-id-type="pmid">27019477</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seeland</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rzanny</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Alaqraa</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wäldchen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mälder</surname> <given-names>P</given-names></name>. <article-title>Plant species classification using flower images—A comparative study of local feature representations</article-title>. <source>PLOS ONE</source>. <year>2017</year>;<volume>12</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref030"><label>30</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Söderkvist</surname> <given-names>O</given-names></name>. <source>Computer Vision Classification of Leaves from Swedish Trees</source>. <publisher-name>Department of Electrical Engineering, Computer Vision, Linköping Universityping University</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>H</given-names></name>. <article-title>Deep Learning for Plant Identification in Natural Environment</article-title>. <source>Computational intelligence and neuroscience</source>. <year>2017</year>;<volume>2017</volume>:<fpage>7361042</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2017/7361042" xlink:type="simple">10.1155/2017/7361042</ext-link></comment> <object-id pub-id-type="pmid">28611840</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref032"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Wang Z, Lu B, Chi Z, Feng D. Leaf Image Classification with Shape Context and SIFT Descriptors. In: Proceedings of the International Conference on Digital Image Computing Techniques and Applications (DICTA), 2011. pp. 650–654.</mixed-citation></ref>
<ref id="pcbi.1005993.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Zündorf</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Günther</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Korsch</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Westhus</surname> <given-names>W</given-names></name>. <source>Flora von Thüringen</source>. <publisher-name>Weissdorn</publisher-name>, <publisher-loc>Jena</publisher-loc>. <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russakovsky</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Krause</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Satheesh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>–<lpage>252</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref035"><label>35</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Müller</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ritz</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Welk</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Wesche</surname> <given-names>K</given-names></name>. <source>Rothmaler-Exkursionsflora von Deutschland: Gefäßpflanzen: Kritischer Ergänzungsband</source>. <publisher-name>Springer-Verlag</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rzanny</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Seeland</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wäldchen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mäder</surname> <given-names>P</given-names></name>. <article-title>Acquiring and preprocessing leaf images for automated plant identification: understanding the tradeoff between effort and information gain</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>97</fpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref037"><label>37</label><mixed-citation publication-type="other" xlink:type="simple">Joly A, Goëau H, Bonnet P, Bakić V, Barbe J, Selmi S, et al. Interactive plant identification based on social image data. Ecological Informatics. 2014;23:22–34.</mixed-citation></ref>
<ref id="pcbi.1005993.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Pl@ntNet; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://identify.plantnet-project.org/" xlink:type="simple">https://identify.plantnet-project.org/</ext-link>. 1st October 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">The Flora Incognita Project; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="http://floraincognita.com" xlink:type="simple">http://floraincognita.com</ext-link>. 1st October 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref040"><label>40</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cope</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Remagnino</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Barman</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wilkin</surname> <given-names>P</given-names></name>. <chapter-title>Plant Texture Classification Using Gabor Co-occurrences</chapter-title>. In: <name name-style="western"><surname>Bebis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Boyle</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Parvin</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Koracin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Chung</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hammound</surname> <given-names>R</given-names></name>, <etal>et al</etal>., editors. <source>Advances in Visual Computing. vol. 6454 of Lecture Notes in Computer Science</source>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>2010</year>. p. <fpage>669</fpage>–<lpage>677</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Casanova</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Mesquita Sa Junior</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Bruno</surname> <given-names>OM</given-names></name>. <article-title>Plant leaf identification using Gabor wavelets</article-title>. <source>International Journal of Imaging Systems and Technology</source>. <year>2009</year>;<volume>19</volume>(<issue>3</issue>):<fpage>236</fpage>–<lpage>243</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Backes</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Casanova</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bruno</surname> <given-names>OM</given-names></name>. <article-title>Plant leaf identification based on volumetric fractal dimension</article-title>. <source>International Journal of Pattern Recognition and Artificial Intelligence</source>. <year>2009</year>;<volume>23</volume>(<issue>06</issue>):<fpage>1145</fpage>–<lpage>1160</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chaki</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Parekh</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bhattacharya</surname> <given-names>S</given-names></name>. <article-title>Plant leaf recognition using texture and shape features with neural classifiers</article-title>. <source>Pattern Recognition Letters</source>. <year>2015</year>;<volume>58</volume>:<fpage>61</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yanikoglu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Aptoula</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tirkaz</surname> <given-names>C</given-names></name>. <article-title>Automatic plant identification from photographs</article-title>. <source>Machine Vision and Applications</source>. <year>2014</year>;<volume>25</volume>(<issue>6</issue>):<fpage>1369</fpage>–<lpage>1383</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruno</surname> <given-names>OM</given-names></name>, <name name-style="western"><surname>de Oliveira Plotze</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Falvo</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>de Castro</surname> <given-names>M</given-names></name>. <article-title>Fractal dimension applied to plant identification</article-title>. <source>Information Sciences</source>. <year>2008</year>;<volume>178</volume>(<issue>12</issue>):<fpage>2722</fpage>–<lpage>2733</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilf</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chikkerur</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Little</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Wing</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Serre</surname> <given-names>T</given-names></name>. <article-title>Computer vision cracks the leaf code</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>(<issue>12</issue>):<fpage>3305</fpage>–<lpage>3310</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hsu</surname> <given-names>TH</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>LH</given-names></name>. <article-title>An interactive flower image recognition system</article-title>. <source>Multimedia Tools and Applications</source>. <year>2011</year>;<volume>53</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref048"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Nilsback ME, Zisserman A. A Visual Vocabulary for Flower Classification. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2006. pp. 1447–1454.</mixed-citation></ref>
<ref id="pcbi.1005993.ref049"><label>49</label><mixed-citation publication-type="other" xlink:type="simple">Nilsback ME, Zisserman A. Automated Flower Classification over a Large Number of Classes. In: Proceedings of the IEEE Indian Conference on Computer Vision, Graphics and Image Processing. 2008. pp. 722–729.</mixed-citation></ref>
<ref id="pcbi.1005993.ref050"><label>50</label><mixed-citation publication-type="other" xlink:type="simple">Seeland M, Rzanny M, Alaqraa N, Thuille A, Boho D, Wäldchen J, et al. Description of Flower Colors for Image based Plant Species Classification. In: Proceedings of the 22nd German Color Workshop (FWS). Ilmenau, Germany: Zentrum für Bild- und Signalverarbeitung e.V; 2016. pp.145–1154.</mixed-citation></ref>
<ref id="pcbi.1005993.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title>. In: <source>Advances in neural information processing systems</source>. <year>2012</year>. pp. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref052"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: Proceedings of the of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. pp. 770–778.</mixed-citation></ref>
<ref id="pcbi.1005993.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Lee SH, Chan CS, Wilkin P, Remagnino P. Deep-plant: Plant identification with convolutional neural networks. In: Proceedings of the IEEE International Conference on Image Processing. 2015. pp. 452–456.</mixed-citation></ref>
<ref id="pcbi.1005993.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Chan</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Mayo</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Remagnino</surname> <given-names>P</given-names></name>. <article-title>How deep learning extracts and learns leaf features for plant classification</article-title>. <source>Pattern Recognition</source>. <year>2017</year>;<volume>71</volume>:<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref055"><label>55</label><mixed-citation publication-type="other" xlink:type="simple">Zhang C, Zhou P, Li C, Liu L. A convolutional neural network for leaves recognition using data augmentation. In: Proceedings of the IEEE International Conference on Computer and Information Technology. 2015; 2143–2150.</mixed-citation></ref>
<ref id="pcbi.1005993.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Simon M, Rodner E. Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks. In: Proceedings of IEEE International Conference on Computer Vision. 2015. pp. 1143–1151.</mixed-citation></ref>
<ref id="pcbi.1005993.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhao</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chan</surname> <given-names>SSF</given-names></name>, <name name-style="western"><surname>Cham</surname> <given-names>WK</given-names></name>, <name name-style="western"><surname>Chu</surname> <given-names>LM</given-names></name>. <article-title>Plant identification using leaf shapes? A pattern counting approach</article-title>. <source>Pattern Recognition</source>. <year>2015</year>;<volume>48</volume>(<issue>10</issue>):<fpage>3203</fpage>–<lpage>3215</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agarwal</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Belhumeur</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Feiner</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jacobs</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kress</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Ramamoorthi</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>First steps toward an electronic field guide for plants</article-title>. <source>Taxon</source>. <year>2006</year>;<volume>55</volume>(<issue>3</issue>):<fpage>597</fpage>–<lpage>610</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hu</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jia</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Ling</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>D</given-names></name>. <article-title>Multiscale Distance Matrix for Fast Plant Leaf Recognition</article-title>. <source>IEEE Transactions on Image Processing</source>, <year>2012</year>;<volume>21</volume>(<issue>11</issue>):<fpage>4667</fpage>–<lpage>4672</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2012.2207391" xlink:type="simple">10.1109/TIP.2012.2207391</ext-link></comment> <object-id pub-id-type="pmid">22875247</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref060"><label>60</label><mixed-citation publication-type="other" xlink:type="simple">Goëau H, Bonnet P, Joly A. Plant identification in an open-world. In: CLEF 2016 Conference and Labs of the Evaluation forum. 2016. pp. 428–439</mixed-citation></ref>
<ref id="pcbi.1005993.ref061"><label>61</label><mixed-citation publication-type="other" xlink:type="simple">Kumar Mishra P, Kumar Maurya S, Kumar Singh R, Kumar Misra A. A semi automatic plant identification based on digital leaf and flower images. In: Proceedings of International Conference on Advances in Engineering, Science and Management, 2012. pp. 68–73.</mixed-citation></ref>
<ref id="pcbi.1005993.ref062"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Leafsnap; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://itunes.apple.com/us/app/leafsnap/id430649829" xlink:type="simple">https://itunes.apple.com/us/app/leafsnap/id430649829</ext-link>. 1st October 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref063"><label>63</label><mixed-citation publication-type="other" xlink:type="simple">Joly A, Müller H, Goëau H, Glotin H, Spampinato C, Rauber A, et al. Lifeclef: Multimedia life species identification. In: Proceedings of ACM Workshop on Environmental Multimedia Retrieval; 2014. pp. 1–7.</mixed-citation></ref>
<ref id="pcbi.1005993.ref064"><label>64</label><mixed-citation publication-type="other" xlink:type="simple">Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep Networks with Stochastic Depth. In: Proceedings of European Conference on Computer Vision. 2016. pp. 646–661.</mixed-citation></ref>
<ref id="pcbi.1005993.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larsson</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Maire</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shakhnarovich</surname> <given-names>G</given-names></name>. <article-title>FractalNet: Ultra-Deep Neural Networks without Residuals</article-title>. <source>CoRR</source>. <year>2016</year>;abs/1605.07648.</mixed-citation></ref>
<ref id="pcbi.1005993.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>. <article-title>Densely Connected Convolutional Networks</article-title>. <source>CoRR</source>. <year>2016</year>;abs/1608.06993.</mixed-citation></ref>
<ref id="pcbi.1005993.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iandola</surname> <given-names>FN</given-names></name>, <name name-style="western"><surname>Moskewicz</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Ashraf</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Han</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Dally</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Keutzer</surname> <given-names>K</given-names></name>. <article-title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</article-title>. <source>CoRR</source>. <year>2016</year>;abs/1602.07360.</mixed-citation></ref>
<ref id="pcbi.1005993.ref068"><label>68</label><mixed-citation publication-type="other" xlink:type="simple">Goëau H, Bonnet P, Joly A. Plant Identification Based on Noisy Web Data: the Amazing Performance of Deep Learning. In: Workshop Proceedings of Conference and Labs of the Evaluation Forum (CLEF 2017). 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carranza-Rojas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goeau</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bonnet</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mata-Montero</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Joly</surname> <given-names>A</given-names></name>. <article-title>Going deeper in the automated identification of Herbarium specimens</article-title>. <source>BMC Evolutionary Biology</source>. <year>2017</year>;<volume>17</volume>(<issue>1</issue>):<fpage>181</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s12862-017-1014-z" xlink:type="simple">10.1186/s12862-017-1014-z</ext-link></comment> <object-id pub-id-type="pmid">28797242</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref070"><label>70</label><mixed-citation publication-type="other" xlink:type="simple">Odena, A., Olah, C., Shlens, J. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585.</mixed-citation></ref>
<ref id="pcbi.1005993.ref071"><label>71</label><mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L, Li K, Li F. ImageNet: A large-scale hierarchical image database. In: Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2009. pp. 248–255.</mixed-citation></ref>
<ref id="pcbi.1005993.ref072"><label>72</label><mixed-citation publication-type="other" xlink:type="simple">Encyclopedia of Life (EOL); 2017. Available from: <ext-link ext-link-type="uri" xlink:href="http://eol.org/statistics" xlink:type="simple">http://eol.org/statistics</ext-link>. 6th July 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref073"><label>73</label><mixed-citation publication-type="other" xlink:type="simple">Encyclopedia of Life (EOL); 2017. Available from: <ext-link ext-link-type="uri" xlink:href="http://eol.org/pages/282/media" xlink:type="simple">http://eol.org/pages/282/media</ext-link>. 6th July 2017</mixed-citation></ref>
<ref id="pcbi.1005993.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wieczorek</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bloom</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Guralnick</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Blum</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Döring</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Giovanni</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Darwin Core: An Evolving Community-Developed Biodiversity Data Standard</article-title>. <source>PLOS ONE</source>. <year>2012</year>;<volume>7</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0029715" xlink:type="simple">10.1371/journal.pone.0029715</ext-link></comment> <object-id pub-id-type="pmid">22238640</object-id></mixed-citation></ref>
<ref id="pcbi.1005993.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kattge</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Diaz</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lavorel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Prentice</surname> <given-names>IC</given-names></name>, <name name-style="western"><surname>Leadley</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bönisch</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>TRY–a global database of plant traits</article-title>. <source>Global change biology</source>. <year>2011</year>;<volume>17</volume>(<issue>9</issue>):<fpage>2905</fpage>–<lpage>2935</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willis</surname> <given-names>CG</given-names></name>, <name name-style="western"><surname>Ellwood</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Primack</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Pearson</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Gallinat</surname> <given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Old Plants, New Tricks: Phenological Research Using Herbarium Specimens</article-title>. <source>Trends in ecology &amp; evolution</source>. <year>2017</year>;<volume>32</volume>:<fpage>531</fpage>–<lpage>546</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corney</surname> <given-names>DPA</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Wilkin</surname> <given-names>P</given-names></name>. <article-title>Automatic extraction of leaf characters from herbarium specimens</article-title>. <source>Taxon</source>. <year>2012</year>;<volume>61</volume>(<issue>1</issue>):<fpage>231</fpage>–<lpage>244</lpage>.</mixed-citation></ref>
<ref id="pcbi.1005993.ref078"><label>78</label><mixed-citation publication-type="other" xlink:type="simple">Grimm J, Hoffmann M, Stöver B, Müller K, Steinhage V. Image-Based Identification of Plant Species Using a Model-Free Approach and Active Learning. In: Proceedings of Annual German Conference on AI. 2016. pp. 169–176.</mixed-citation></ref>
<ref id="pcbi.1005993.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Unger</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Merhof</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Renner</surname> <given-names>S</given-names></name>. <article-title>Computer vision applied to herbarium specimens of German trees: testing the future utility of the millions of herbarium specimen images for automated identification</article-title>. <source>BMC Evolutionary Biology</source>. <year>2016</year>;<volume>16</volume>(<issue>1</issue>):<fpage>248</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s12862-016-0827-5" xlink:type="simple">10.1186/s12862-016-0827-5</ext-link></comment> <object-id pub-id-type="pmid">27852219</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>