<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00319</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002236</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Evolutionary modeling</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Cognitive neuroscience</subject>
              <subj-group>
                <subject>Cognition</subject>
                <subject>Consciousness</subject>
                <subject>Decision making</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied mathematics</subject>
            <subj-group>
              <subject>Complex systems</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Integrated Information Increases with Fitness in the Evolution of Animats</article-title><alt-title alt-title-type="running-head">Evolution of Integrated Information</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Edlund</surname>
            <given-names>Jeffrey A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Chaumont</surname>
            <given-names>Nicolas</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Hintze</surname>
            <given-names>Arend</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Koch</surname>
            <given-names>Christof</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff5">
            <sup>5</sup>
          </xref>
          <xref ref-type="aff" rid="aff6">
            <sup>6</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Tononi</surname>
            <given-names>Giulio</given-names>
          </name>
          <xref ref-type="aff" rid="aff7">
            <sup>7</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Adami</surname>
            <given-names>Christoph</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="aff" rid="aff8">
            <sup>8</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Computation &amp; Neural Systems, California Institute of Technology, Pasadena, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Keck Graduate Institute of Applied Life Sciences, Claremont, Pasadena, California, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>BEACON Center for the Study of Evolution in Action, Michigan State University, East Lansing, Michigan, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Computer Science and Engineering, Michigan State University, East Lansing, Michigan, United States of America</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea</addr-line>       </aff><aff id="aff6"><label>6</label><addr-line>Allen Institute for Brain Sciences, Seattle, Washington, United States of America</addr-line>       </aff><aff id="aff7"><label>7</label><addr-line>Department of Psychiatry, University of Wisconsin, Madison, Wisconsin, United States of America</addr-line>       </aff><aff id="aff8"><label>8</label><addr-line>Microbiology &amp; Molecular Genetics, Michigan State University, East Lansing, Michigan, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Graham</surname>
            <given-names>Lyle J.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Université Paris Descartes, Centre National de la Recherche Scientifique, France</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">adami@kgi.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JAE AH CK GT CA. Performed the experiments: JAE AH. Analyzed the data: JAE AH. Contributed reagents/materials/analysis tools: NC. Wrote the paper: JAE CK GT CA.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>20</day>
        <month>10</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>10</issue><elocation-id>e1002236</elocation-id><history>
        <date date-type="received">
          <day>8</day>
          <month>3</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>3</day>
          <month>9</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Edlund et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>One of the hallmarks of biological organisms is their ability to integrate disparate information sources to optimize their behavior in complex environments. How this capability can be quantified and related to the functional complexity of an organism remains a challenging problem, in particular since organismal functional complexity is not well-defined. We present here several candidate measures that quantify information and integration, and study their dependence on fitness as an artificial agent (“animat”) evolves over thousands of generations to solve a navigation task in a simple, simulated environment. We compare the ability of these measures to predict high fitness with more conventional information-theoretic processing measures. As the animat adapts by increasing its “fit” to the world, information integration and processing increase commensurately along the evolutionary line of descent. We suggest that the correlation of fitness with information integration and with processing measures implies that high fitness requires both information processing as well as integration, but that information integration may be a better measure when the task requires memory. A correlation of measures of information integration (but also information processing) and fitness strongly suggests that these measures reflect the functional complexity of the animat, and that such measures can be used to quantify functional complexity even in the absence of fitness data.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Intelligent behavior encompasses appropriate navigation in complex environments that is achieved through the integration of sensorial information and memory of past events to create purposeful movement. This behavior is often described as “complex”, but universal ways to quantify such a notion do not exist. Promising candidates for measures of functional complexity are based on information theory, but fail to take into account the important role that memory plays in complex navigation. Here, we study a different information-theoretic measure called “integrated information”, and investigate its ability to reflect the complexity of navigation that uses both sensory data and memory. We suggest that measures based on the integrated-information concept correlate better with fitness than other standard measures when memory evolves as a key element in navigation strategy, but perform as well as more standard information processing measures if the robots navigate using a purely reactive sensor-motor loop. We conclude that the integration of information that emanates from the sensorial data stream with some (short-term) memory of past events is crucial to complex and intelligent behavior and speculate that integrated information–to the extent that it can be measured and computed–might best reflect the complexity of animal behavior, including that of humans.</p>
      </abstract><funding-group><funding-statement>This work was funded in part by the Paul G. Allen Family Foundation, by the National Science Foundation's Frontiers in Integrative Biological Research grant FIBR-0527023, NSF's BEACON Center for the Study of Evolution in Action under contract No. DBI-0939454, and by the WCU (World Class University) program through the National Research Foundation of Korea funded by the Ministry of Education, Science and Technology (R31-10008). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript</funding-statement></funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Complexity is visible in most scientific disciplines: mathematicians, physicists, biologists, chemists, engineers and social scientists all developed measures to characterize the complexity that they perceive in their systems, borrowing tools from each other but rarely if ever agreeing on a measure that could be used by all of them. Because the objects that each of these disciplines are most concerned with are so different, ranging from mathematical problems and computer programs over physical, chemical, or biological structures, to systems and networks of interacting agents, a convergence of quantitative measures of complexity is perhaps not likely. However, a universal framework that would be capable of adapting its notion to the specific discipline it is applied to would be a welcome trend. Complexity measures abound, but exhaustive reviews are few. A good introduction to the dynamical systems approach to complexity is Ref. <xref ref-type="bibr" rid="pcbi.1002236-Badii1">[1]</xref>, but it does not cover biological applications. The overviews <xref ref-type="bibr" rid="pcbi.1002236-Adami1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Adami3">[4]</xref> focus on the complexity of biological sequences but not on their structure, and mostly ignore the complexity of networks. Neural complexity measures are reviewed in <xref ref-type="bibr" rid="pcbi.1002236-Sporns1">[5]</xref>.</p>
      <p>Among the different measures of complexity, some attempt to quantify the structure <xref ref-type="bibr" rid="pcbi.1002236-Lfgren1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Ahnert1">[13]</xref>, others the sequence giving rise to that structure <xref ref-type="bibr" rid="pcbi.1002236-Kolmogorov1">[14]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Adami4">[19]</xref>, and others again the function of the sequence or system <xref ref-type="bibr" rid="pcbi.1002236-McShea1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Hazen1">[22]</xref>. All these studies attempt to capture “that which increases when self-organizing systems organize themselves” <xref ref-type="bibr" rid="pcbi.1002236-Bennett1">[23]</xref> (a non-exhaustive list is presented in Ref. <xref ref-type="bibr" rid="pcbi.1002236-Lloyd1">[24]</xref>). Increasingly, measures based on information theory are being used to quantify the complexity of living systems, because information provides its owner an obvious fitness advantage compared to those without information by conferring the ability to make predictions about the environment they operate in <xref ref-type="bibr" rid="pcbi.1002236-Taylor1">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Rivoire1">[27]</xref>. In particular Rivoire and Leibler <xref ref-type="bibr" rid="pcbi.1002236-Rivoire1">[27]</xref> study statistical measures based in information theory that maximize the fitness of agents that respond to variable environments, but they do not study evolution. Information-theoretic measures of complexity are reviewed in <xref ref-type="bibr" rid="pcbi.1002236-Bonchev1">[28]</xref> and applications to graphs in <xref ref-type="bibr" rid="pcbi.1002236-Dehmer1">[29]</xref>.</p>
      <p>Here, we study how information-theoretic measures of complexity could be applied to capture the complexity of nervous systems <xref ref-type="bibr" rid="pcbi.1002236-Sporns1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Koch1">[30]</xref>, or more generally speaking, any structure controlling a perception-action cycle. In the absence of any well accepted definition of complexity, we study the correlation of different measures to <italic>organismal fitness</italic>, following the intuition that a well-defined measure of control structure complexity should increase during adaptation <xref ref-type="bibr" rid="pcbi.1002236-McShea1">[20]</xref>. Fitness is a quantitative measure that predicts the long-term success of a lineage <xref ref-type="bibr" rid="pcbi.1002236-Haldane1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-MaynardSmith1">[32]</xref>, and is given by the <italic>expected number of offspring</italic> of an average representative with the given genotype. Unfortunately, this is only a quantitative measure for the simplest of organisms where the expected number of offspring can be determined from the replication rate, or in direct competition experiments (see, e.g., <xref ref-type="bibr" rid="pcbi.1002236-Lenski1">[33]</xref>). For more complex organisms, relative fitness can only be estimated in hindsight, and cannot be used as a proxy for organism complexity. However, if we evolve control structures <italic>in silico</italic> where complete fitness information is available, we can use fitness (within a niche) as an independent arbiter of putative information-based measures of complexity: any measure that does not increase as the organism learns to exploit its environment is unlikely to reflect complex information processing. Because in this type of evolution experiment the number of offspring is directly proportional–on average–to the <italic>performance</italic> of the organism in a task critical to its survival, we here study the correlation of complexity directly with performance or function.</p>
      <p>Note that because fitness necessarily refers to the environment (it measures how well the organism “fits” its niche by exploiting the niche's attributes), fitness cannot be used to compare organism complexity across niches (such as attempting to compare an elephant and an ant in terms of their fitness), but it does reveal functional differences between types that are due to efficiencies of exploiting the same environment. For biological organisms that occupy the same niche, that is, “make a living” in the same manner, relative fitness should correlate with relative functional complexity. Is it true that given a constant environment the more complex organism is necessarily more fit? Answering this question in the affirmative clearly biases our notion of complexity: only useful characters are deemed complex, useless ones are not. While such a bias may be restrictive for structural complexity, it is not so for information-theoretic measures of complexity, as information (if it can be used to reduce uncertainty) will <italic>always</italic> be useful: if it were not, it should be called entropy instead <xref ref-type="bibr" rid="pcbi.1002236-Taylor1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-DonaldsonMatasci1">[34]</xref>.</p>
      <sec id="s1a">
        <title>Predictive information</title>
        <p>Perhaps the best known information-based measure of functional complexity is “predictive information” <xref ref-type="bibr" rid="pcbi.1002236-Bialek1">[35]</xref>, which quantifies the amount of information that can be extracted from sensorial data in order to select actions that are useful to the organism. In this manner, predictive information is able to separate out those features of the sensorial data that are relevant for behavior, and quantifies the amount of information processed by the organism. Predictive information has also been proposed as a measure of complexity of function <xref ref-type="bibr" rid="pcbi.1002236-Bialek1">[35]</xref>.</p>
        <p>If we describe a control network's input variables (“sensors”, or “stimuli”) at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e001" xlink:type="simple"/></inline-formula> by the random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e002" xlink:type="simple"/></inline-formula> and the output variables (“motors”, or “response”) at that time by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e003" xlink:type="simple"/></inline-formula>, then the shared information (used for prediction) is <xref ref-type="bibr" rid="pcbi.1002236-Bialek1">[35]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e004" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e005" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e006" xlink:type="simple"/></inline-formula> are the probability distributions of the sensor and response variables at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e007" xlink:type="simple"/></inline-formula>, respectively, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e008" xlink:type="simple"/></inline-formula> is the joint probability distribution of the sensor and response variables “in the future and the present” <xref ref-type="bibr" rid="pcbi.1002236-Bialek1">[35]</xref> (we use the binary logarithm throughout and assume that the network evolves along discrete time steps). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e009" xlink:type="simple"/></inline-formula> characterizes the capacity of the control system to predict the future one time step ahead, using the present sensorial information. Essentially, it quantifies the correlation between inputs and outputs, and can be thought of as the Kullback-Leibler divergence (or relative entropy) between the full probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e010" xlink:type="simple"/></inline-formula> and the product of the independent ones, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e011" xlink:type="simple"/></inline-formula>.</p>
        <p>Note that for Markov processes, the one-step shared entropy (1) is equal to the shared entropy between the entire past and the entire future (see <xref ref-type="bibr" rid="pcbi.1002236-Ay1">[36]</xref>, Appendix A.1), while this is not true for processes that can use memory. Predictive information was previously used to characterize the complexity of autonomous robot behavior without memory in Ref. <xref ref-type="bibr" rid="pcbi.1002236-Ay1">[36]</xref> (see also <xref ref-type="supplementary-material" rid="pcbi.1002236.s002">Text S1</xref>). If the control structure is not purely reactive and uses information encoded in internal nodes to integrate sensorial information streams, we will need complexity measures that move beyond predictive information <xref ref-type="bibr" rid="pcbi.1002236-Rivoire1">[27]</xref>.</p>
      </sec>
      <sec id="s1b">
        <title>Integrated information</title>
        <p>A fundamental and unique design principle of nervous systems is their extraordinary degree of integration among highly-specialized modules <xref ref-type="bibr" rid="pcbi.1002236-Sporns1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Felleman1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Hagmann1">[38]</xref>. Functional integration is achieved by an extended network of intra- and inter-areal connections, and is reflected in dynamically shifting patterns of synchronization. A precise way to measure a system's capacity to integrate information was developed recently <xref ref-type="bibr" rid="pcbi.1002236-Tononi1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>, and applied to small, simple example networks. This measure, called <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e012" xlink:type="simple"/></inline-formula> and measured in bits, is based on the notion that information integration is achieved by architectural designs that give rise to a single, functionally unified complex (high integration) while ensuring that such a complex has a very large repertoire of discriminable states (high information). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e013" xlink:type="simple"/></inline-formula> captures to what extent, informationally, the whole is more than the sum of its parts, and cannot therefore be reduced to those parts. In this sense, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e014" xlink:type="simple"/></inline-formula> represents the synergy of the system. Before introducing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e015" xlink:type="simple"/></inline-formula> proper, we define a few related quantities.</p>
        <p>In order to study information integration, we have to define the information processed by the entire network, not just the sensors and motors as in Eq. (1). Let us represent the system as a joint random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e016" xlink:type="simple"/></inline-formula>, where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e017" xlink:type="simple"/></inline-formula> represent the <italic>elements</italic> of the system (the nodes of a control structure, such as a neuronal network). The random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e018" xlink:type="simple"/></inline-formula> evolves as the system progresses forward in time, i.e., <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e019" xlink:type="simple"/></inline-formula>, and each variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e020" xlink:type="simple"/></inline-formula> is described by a probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e021" xlink:type="simple"/></inline-formula> to be found in states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e022" xlink:type="simple"/></inline-formula> (here, we will restrict ourselves to binary random variables). At the same time, each node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e023" xlink:type="simple"/></inline-formula> of the system has a time progression <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e024" xlink:type="simple"/></inline-formula>, and each variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e025" xlink:type="simple"/></inline-formula> is described by a probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e026" xlink:type="simple"/></inline-formula>. In the following, we formally define measures of information integration through <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e027" xlink:type="simple"/></inline-formula> time steps (from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e028" xlink:type="simple"/></inline-formula>), but later focus on the computationally more accessible integration through a single average step from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e029" xlink:type="simple"/></inline-formula>.</p>
        <p>The amount of information that is processed by the <italic>entire system</italic> through <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e030" xlink:type="simple"/></inline-formula> time steps is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e031" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e033" xlink:type="simple"/></inline-formula> are the probability distributions of the system at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e034" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e035" xlink:type="simple"/></inline-formula> respectively, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e036" xlink:type="simple"/></inline-formula> is their joint distribution. This measure reduces to the predictive information Eq. (1) for Markov processes connecting only sensor and response nodes, that is, if there are no internal (or hidden) variables.</p>
        <p>One way to measure information integration is to ask how much information is processed by the system above and beyond what is processed by the individual nodes or groups of nodes (modules). To do this, we introduce a <italic>partition</italic> of the network into <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e037" xlink:type="simple"/></inline-formula> parts, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e038" xlink:type="simple"/></inline-formula>, where each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e039" xlink:type="simple"/></inline-formula> is a part of the network: a non-empty set of nodes with no overlap between parts that completely tile the network. We can then define a quantity that measures how much the information processed by the entire network is more than the information processed by all the parts in this particular partition as follows.</p>
        <p>Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e040" xlink:type="simple"/></inline-formula> be the information processed by the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e041" xlink:type="simple"/></inline-formula> part as the system progresses from time 0 to time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e042" xlink:type="simple"/></inline-formula>. Then, the synergistic information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e043" xlink:type="simple"/></inline-formula> processed by the network <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e044" xlink:type="simple"/></inline-formula> given a partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e045" xlink:type="simple"/></inline-formula> quantifies the extent to which the entire processed information is a sum of the information processed by the system's parts, and is calculated as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e046" xlink:type="simple"/><label>(3)</label></disp-formula>From an information-theoretic point of view, the synergistic information measures the excess amount of information that can be encoded in a “multiple access” channel with correlated sources and a joint decoder <xref ref-type="bibr" rid="pcbi.1002236-Slepian1">[41]</xref> over and above what each of the individual channels (the parts of the partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e047" xlink:type="simple"/></inline-formula>) can encode separately. A measure related to the synergistic information is the “effective information” <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e048" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e049" xlink:type="simple"/><label>(4)</label></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e050" xlink:type="simple"/></inline-formula> is the conditional entropy of partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e051" xlink:type="simple"/></inline-formula> given the state of that partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e052" xlink:type="simple"/></inline-formula> time steps later, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e053" xlink:type="simple"/></inline-formula> is the conditional entropy of the entire system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e054" xlink:type="simple"/></inline-formula> at time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e055" xlink:type="simple"/></inline-formula> given the state of that system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e056" xlink:type="simple"/></inline-formula> steps later (see also <xref ref-type="supplementary-material" rid="pcbi.1002236.s004">Text S3</xref>). The quantity (4) is the average over network states at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e057" xlink:type="simple"/></inline-formula> (states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e058" xlink:type="simple"/></inline-formula>) of the quantity called the “effective information across a partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e059" xlink:type="simple"/></inline-formula>” in Ref. <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>. If the probability distribution governing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e060" xlink:type="simple"/></inline-formula> is uniform (maximum entropy), the two measures agree: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e061" xlink:type="simple"/></inline-formula>, but they are different in general (see <xref ref-type="supplementary-material" rid="pcbi.1002236.s004">Text S3</xref>). Below, we will mostly use Eq. (4).</p>
        <p>In order to determine how a network integrates information, we should look for a partition that <italic>minimizes</italic> (4), because it is easy to find a high value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e062" xlink:type="simple"/></inline-formula> by assigning different parts to nodes that are strongly correlated. In essence, looking for the partition that minimizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e063" xlink:type="simple"/></inline-formula> is tantamount to searching for the groups of nodes that are separated from other groups of nodes by a weak informational link <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>. To find this partition, expression (4) needs to be normalized because otherwise the partition that minimizes (4) will almost always be the one that divides a network of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e064" xlink:type="simple"/></inline-formula> parts into one with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e065" xlink:type="simple"/></inline-formula> parts and a single other node <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>. We define the “Minimum Information Partition” (or ‘MIP’) as that partition that minimizes a <italic>normalized</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e066" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e067" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e068" xlink:type="simple"/></inline-formula> is the maximum entropy of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e069" xlink:type="simple"/></inline-formula>th partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e070" xlink:type="simple"/></inline-formula>. If the neurons are binary, then <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e071" xlink:type="simple"/></inline-formula> is just the number of neurons in partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e072" xlink:type="simple"/></inline-formula>. Armed with this definition of the MIP, our measure of information integration is:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e073" xlink:type="simple"/><label>(6)</label></disp-formula>Note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e074" xlink:type="simple"/></inline-formula> represents the average (over all possible final states of the network) of the state-dependent quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e075" xlink:type="simple"/></inline-formula> defined previously <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>, and the subscript <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e076" xlink:type="simple"/></inline-formula> reminds us that the integration is measured from an initial probability distribution at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e077" xlink:type="simple"/></inline-formula> that is uniform.</p>
        <p>The measure can be adapted to characterize the information integration across a single time step simply by defining<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e078" xlink:type="simple"/><label>(7)</label></disp-formula>with a commensurately defined MIP:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e079" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e080" xlink:type="simple"/></inline-formula> is the maximum entropy of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e081" xlink:type="simple"/></inline-formula>th partition at time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e082" xlink:type="simple"/></inline-formula>. Note that we have omitted an index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e083" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e084" xlink:type="simple"/></inline-formula> as defined in Eq. (7) as we assume that for large <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e085" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e086" xlink:type="simple"/></inline-formula> becomes stationary: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e087" xlink:type="simple"/></inline-formula>. This MIP, just as the one defined by Balduzzi and Tononi <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>, divides the network into disjoint parts that are maximally informationally disparate–those parts that are most independent. As defined here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e088" xlink:type="simple"/></inline-formula> is equivalent to the recently defined <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e089" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002236-Barrett1">[42]</xref>, because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e090" xlink:type="simple"/></inline-formula> is based on the reduction (at time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e091" xlink:type="simple"/></inline-formula>) in the Shannon entropy based on the empirical entropy at time step <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e092" xlink:type="simple"/></inline-formula>, not on the reduction from the maximum entropy at time step 0 as in <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>. Thus, our Eq. (7) is equivalent to Eq. (29) in Ref. <xref ref-type="bibr" rid="pcbi.1002236-Barrett1">[42]</xref> (with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e093" xlink:type="simple"/></inline-formula> replaced with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e094" xlink:type="simple"/></inline-formula> ), except that we search all partitions rather than just bi-partitions, and the normalization factor of Barrett and Seth uses the largest of the actual entropies of the parts. Because we will measure information integration for time series generated by a moving animat, we will use Eq. (7) to quantify the animat's complexity in what follows.</p>
        <p>If networks are small, it is possible to find the MIP by brute-force testing all possible partitions. The number of partitions of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e095" xlink:type="simple"/></inline-formula> nodes is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e096" xlink:type="simple"/></inline-formula>th Bell number, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e097" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002236-Rota1">[43]</xref>. Searching across all partitions is exceedingly expensive and scales faster than exponential. For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e098" xlink:type="simple"/></inline-formula> = 5, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e099" xlink:type="simple"/></inline-formula> = 115,975, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e100" xlink:type="simple"/></inline-formula>. For networks of realistic size, search heuristics will be the only way to find the MIP: for the nematode <italic>C. elegans</italic>, for example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e101" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1002236-Koch1">[30]</xref>, and the number of partitions of this network is the absurdly large number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e102" xlink:type="simple"/></inline-formula>. Here, the largest networks we analyze have 12 nodes, but we have been able to calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e103" xlink:type="simple"/></inline-formula> for networks with up to 18 nodes using a fast exact algorithm that does not store all the partitions.</p>
      </sec>
      <sec id="s1c">
        <title>Main complex</title>
        <p>A system composed of a large network together with a single disconnected unit will always have <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e104" xlink:type="simple"/></inline-formula>, because minimizing over all partitions finds the informational disconnect between the network and the disconnected node, and the minimum effective information between these parts is zero <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>. A measure that captures information processing that is synergistic without being trivial can be obtained by defining the network's computational <italic>proper complex</italic> <xref ref-type="bibr" rid="pcbi.1002236-Tononi2">[44]</xref> as a subset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e105" xlink:type="simple"/></inline-formula> of (joint) random variables within the system <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e106" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e107" xlink:type="simple"/></inline-formula>) that maximizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e108" xlink:type="simple"/></inline-formula> over all subsets <italic>and</italic> supersets, that is:</p>
        <p>If <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e109" xlink:type="simple"/></inline-formula> is defined as the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e110" xlink:type="simple"/></inline-formula> of subset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e111" xlink:type="simple"/></inline-formula>, then<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e112" xlink:type="simple"/><label>(9)</label></disp-formula>Each network can have several (proper) complexes, with smaller complexes of higher <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e113" xlink:type="simple"/></inline-formula> embedded within larger complexes of lower <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e114" xlink:type="simple"/></inline-formula>. We define the <italic>proper main complex</italic> as the subset associated with largest <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e115" xlink:type="simple"/></inline-formula> values over all subsets of the entire system. We denote the information integration in the proper main complex as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e116" xlink:type="simple"/></inline-formula>. A simple network with MIP and main complex identified is shown in <xref ref-type="fig" rid="pcbi-1002236-g001">Fig. 1</xref>.</p>
        <fig id="pcbi-1002236-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Exemplar MIP and main complex.</title>
            <p><bold>A</bold>: The logical units are AND gates with multiple outputs (each output is the AND of the two inputs). <bold>B</bold>: A network of seven such units (877 distinct possible partitions). The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e117" xlink:type="simple"/></inline-formula> for the entire system (solid lines) is a bi-partition, and the main complex (dashed line, shaded area) consists of five units. We compute <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e118" xlink:type="simple"/></inline-formula> bits for the entire network, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e119" xlink:type="simple"/></inline-formula> bits.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g001" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s1d">
        <title>Other integration measures</title>
        <p>Among all possible partitions, the “atomic partition” that partitions the network into its individual nodes, plays an important role. For example, we can define the information processed by the network above and beyond the information processed by the individual nodes as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e120" xlink:type="simple"/><label>(10)</label></disp-formula>where the first term is the total processed information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e121" xlink:type="simple"/></inline-formula>, defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e122" xlink:type="simple"/><label>(11)</label></disp-formula>The negative Eq. (10) has previously been used to quantify the redundancy of information processing of a neural network <xref ref-type="bibr" rid="pcbi.1002236-Atick1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Nadal1">[46]</xref>, see also <xref ref-type="bibr" rid="pcbi.1002236-Schneidman1">[47]</xref>. Incidentally, Barlow has long argued that reducing redundancy (and thus compressing the sensorial information stream maximally) is the main purpose of the structure of the sensorial information-processing system <xref ref-type="bibr" rid="pcbi.1002236-Barlow1">[48]</xref>, and we would then, if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e123" xlink:type="simple"/></inline-formula> is fixed, expect a maximization of fitness to go hand-in-hand with a minimization of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e124" xlink:type="simple"/></inline-formula> and therefore a maximization of redundancy.</p>
        <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e125" xlink:type="simple"/></inline-formula> measures the shared entropy between the system at adjacent time points, and is a useful measure to determine whether an increase in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e126" xlink:type="simple"/></inline-formula> is due solely to increased information processing by the entire network (resulting in an increased <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e127" xlink:type="simple"/></inline-formula>) rather than the effective integration of that information. Writing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e128" xlink:type="simple"/></inline-formula> for each node <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e129" xlink:type="simple"/></inline-formula>, we see that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e130" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e131" xlink:type="simple"/></inline-formula> is the number of individual nodes in the network and where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e132" xlink:type="simple"/><label>(13)</label></disp-formula>This quantity has been called “multi-information” <xref ref-type="bibr" rid="pcbi.1002236-Schneidman1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-McGill1">[49]</xref>), and was used as a measure of brain complexity called “integration” in <xref ref-type="bibr" rid="pcbi.1002236-Tononi3">[50]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Lungarella2">[52]</xref>, where the sum was over the components of a network rather than the nodes. Thus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e133" xlink:type="simple"/></inline-formula> is an “atomic” form of the Tononi-Sporns-Edelman (TSE)-complexity <xref ref-type="bibr" rid="pcbi.1002236-Tononi3">[50]</xref>. Note that none of the measures discussed in this section should depend on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e134" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e135" xlink:type="simple"/></inline-formula> is large enough because we assume that at large times the probability distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e136" xlink:type="simple"/></inline-formula> becomes stationary.</p>
        <p>The first part in Eq. (12) is nothing but the effective information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e137" xlink:type="simple"/></inline-formula> (4), but for the “atomic partition”, that is, the partition where each part is given by the individual nodes in the entire network and for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e138" xlink:type="simple"/></inline-formula>. Thus,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e139" xlink:type="simple"/><label>(14)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e140" xlink:type="simple"/><label>(15)</label></disp-formula>Eq. (15) may be a particularly useful measure to approximate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e141" xlink:type="simple"/></inline-formula> when a search for MIPs is computationally infeasible. It has previously been introduced under the name “stochastic information” by Ay <xref ref-type="bibr" rid="pcbi.1002236-Ay2">[53]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-Ay4">[55]</xref>. However, it is neither an upper nor a lower bound on <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e142" xlink:type="simple"/></inline-formula>. Because of its construction (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e143" xlink:type="simple"/></inline-formula>), it incorporates elements of information processing (the excess information processed, in time, by the system above and beyond the information processed by each of the nodes) as well as integration. In other words, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e144" xlink:type="simple"/></inline-formula> encompasses both temporal and spatial synergies of the network.</p>
      </sec>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>In order to test how different measures of functional complexity change as a system adapts to function in its world, we evolve controllers for animats <xref ref-type="bibr" rid="pcbi.1002236-Wilson1">[56]</xref> that have to solve a task that requires sensory-motor coordination as well as memory. Ay and coworkers tested predictive information Eq. (1) as a measure of system complexity when evolving a simulated autonomous robot to solve a simple maze, and found that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e145" xlink:type="simple"/></inline-formula> reflects the performance of the robot <xref ref-type="bibr" rid="pcbi.1002236-Ay1">[36]</xref>. Lungarella and coworkers used information-based complexity measures to understand how appropriate motor action of embodied agents shapes the signal structure perceived by the agent's sensors <xref ref-type="bibr" rid="pcbi.1002236-Lungarella1">[51]</xref>, and studied the information flow through the control structures <xref ref-type="bibr" rid="pcbi.1002236-Lungarella2">[52]</xref>. Klyubin and coworkers used mutual information between an agent's starting position and a representation of this information in the agent's memory to evolve sensorimotor control structures, and used measures of synergy to study whether the positional information could be factorized within the sensors <xref ref-type="bibr" rid="pcbi.1002236-Klyubin1">[57]</xref>.</p>
      <sec id="s2a">
        <title>Description of evolutionary system</title>
        <p>Our animats are embodied controllers with six binary sensors and two (binary) actuators, as well as four internal bits that can be used for memory or processing (<xref ref-type="fig" rid="pcbi-1002236-g002">Fig. 2</xref> and <xref ref-type="sec" rid="s4">Methods</xref>).</p>
        <fig id="pcbi-1002236-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Embodied virtual agent (animat) with six sensors, two actuators, and four internal nodes.</title>
            <p>The complete animat is described by 12 bits: three front sensors (red triangles; # 0,1 &amp; 2), two lateral collision detectors (blue triangles; # 4 &amp; 5), and a single “door” sensor (magenta, #3) that relays the direction of the next opening in the maze (but only while standing in the door). The actuators (trapezoids; # 10 &amp; 11) encode the actions “move left, move right, move forward, do nothing”. The internal nodes (circles; # 6–9) can potentially store states used for internal processing.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g002" xlink:type="simple"/>
        </fig>
        <p>The controllers are stochastic Markov networks (see, e.g., <xref ref-type="bibr" rid="pcbi.1002236-Koller1">[58]</xref>), that is, networks of random variables with the Markov property, where edges between nodes encode arbitrary fuzzy logic gates. As such, the edges could represent simple binary logic gates or more complex computational units. Because these networks actually encode <italic>decisions</italic>, strictly speaking they are encoding discrete-time stochastic Markov decision processes (MDPs). Fundamentally, our Markov networks are related to the hierarchical temporal memory (HTM) model of neocortical function <xref ref-type="bibr" rid="pcbi.1002236-Hawkins1">[59]</xref>–<xref ref-type="bibr" rid="pcbi.1002236-George2">[61]</xref> and the HMAX algorithm <xref ref-type="bibr" rid="pcbi.1002236-Riesenhuber1">[62]</xref>, except that the organization of our stochastic Markov networks need not be strictly hierarchical because it is determined via genetic evolution rather than top-down design (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
        <p>In what follows, the edges connecting the random variables are implemented as <italic>Hidden Markov Gates</italic> (HMGs). Each such gate is a probabilistic finite state machine defined by its input/output structure and state transition probabilities (see <xref ref-type="fig" rid="pcbi-1002236-g003">Fig. 3A</xref>). For example, if ‘100’ was applied to the input state of the HMG in <xref ref-type="fig" rid="pcbi-1002236-g003">Fig. 3A</xref>, ‘11’ is the output with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e146" xlink:type="simple"/></inline-formula> [<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e147" xlink:type="simple"/></inline-formula>], while an input ‘111’ generates ‘01’ with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e148" xlink:type="simple"/></inline-formula> [<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e149" xlink:type="simple"/></inline-formula>], and so forth. Such a gate can also be represented as its dual graph, where the signal lines become the nodes of the Markov network, and the edges between them represent the computation performed by the HMG (<xref ref-type="fig" rid="pcbi-1002236-g003">Fig. 3B</xref>). In this representation, arrows indicate causal influence via an HMG, so in <xref ref-type="fig" rid="pcbi-1002236-g003">Fig. 3B</xref> for example, variable 4 is influenced by variables 1,2, and 3 (as is variable 3), while variables 1 and 2 only have outgoing arrows: they only influence variables 3 and 4 but are not affected by any other variable.</p>
        <fig id="pcbi-1002236-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Hidden Markov Gate representation.</title>
            <p><bold>A</bold>: An HMG with three binary input and two output Markov variables, where one of the outputs is fed back into the HMG (a hidden variable). The state transition table has <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e150" xlink:type="simple"/></inline-formula> entries that are determined by genetic evolution (see <xref ref-type="sec" rid="s4">Methods</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref>). In the gate shown, bit three is a hidden state and can be used to implement a one-bit memory. In principle, the probabilities in the HMG transition table can also be tuned via reinforcement learning using a signal from the environment (“World Feedback”). However, this capacity is not utilized in the present work. <bold>B</bold>: The “dual” representation of this gate, where the Markov variables are nodes, and the gate connects these via edges. This network is obtained by drawing a directed edge between bits that affect each other causally via the logic gate. Because bit 3 feeds back to itself, for example, it is given the same identifier and there is a directed arrow from bit 3 to itself as well as bit 3 to bit 4. See <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002236.s001">Fig. S1</xref> for details on the genetic encoding and network visualization of HMGs.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g003" xlink:type="simple"/>
        </fig>
        <p>The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e151" xlink:type="simple"/></inline-formula> probabilities of an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e152" xlink:type="simple"/></inline-formula>-input and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e153" xlink:type="simple"/></inline-formula>-output state transition table, as well as how each HMG is connected to other gates, is encoded within a genome that, when read by an interpreter, creates the network (see <xref ref-type="sec" rid="s4">Methods</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002236.s001">Figure S1</xref>, as well as Ref. <xref ref-type="bibr" rid="pcbi.1002236-Hintze1">[63]</xref> for a similar structure). Populations of genomes are evolved using a standard Genetic Algorithm (but without crossover, see <xref ref-type="sec" rid="s4">Methods</xref>). To calculate the fitness of each genome, the controller generated from the sequence is transplanted into the animat shown in <xref ref-type="fig" rid="pcbi-1002236-g002">Fig. 2</xref> and tested on its ability to traverse a maze that consists of repeated vertical walls at varying distance to each other, with a single door placed at random locations within the wall. Within each door, a “beacon” indicates the direction to follow for the shortest path to the next door, but this information is erased the moment the animat emerges from the door. Thus, in order to use this information, it has to be stored in memory for later usage. The actual maze has at least 26 walls to traverse before the maze repeats. A section of a typical maze along with an adapted animat's trajectory as well as the states of the memory and motor bits are shown in <xref ref-type="fig" rid="pcbi-1002236-g004">Fig. 4</xref>. <xref ref-type="supplementary-material" rid="pcbi.1002236.s005">Videos S1</xref> to <xref ref-type="supplementary-material" rid="pcbi.1002236.s007">S3</xref> show several movies that depict the motion of the animat, at different evolutionary stages, traveling through the maze.</p>
        <fig id="pcbi-1002236-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Maze structure and animat trajectory.</title>
            <p>Part of one of the test mazes, along with the trajectory of an adapted animat as well as a view of the animat's brain (the four internal nodes 6–9, top four pixels in each animat location) and the motor outputs (bottom two pixels). A bit set to ‘1’ is indicated in green, while blue indicates a bit set to ‘0’. The value of the sensory bits can be inferred from the animat's location. The downward pointing arrow inside a door reminds us that the animat would perceive a ‘1’ on its door sensor at that location (indicating that the next door will be found to the right of the animat's position). If the door is straight ahead or to the left, the door sensor will be set to ‘0’. The animat's goal is to move as far across the maze as possible (see <xref ref-type="sec" rid="s4">Methods</xref>). Note that this representation does not show when the animat is stationary (waits) or retraces its path.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g004" xlink:type="simple"/>
        </fig>
        <p>In each of 64 independent evolution experiments, a population of 300 <italic>initially random</italic> genomes (encoding random controllers, see <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref>) was evolved for 50,000 generations each. We calculate fitness (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e154" xlink:type="simple"/></inline-formula>) and control fitness <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e155" xlink:type="simple"/></inline-formula> both for the highest fitness animats at every generation and for genomes on the line of descent (LOD) of the last common ancestor of the population that existed at generation 50,000 (see <xref ref-type="sec" rid="s4">Methods</xref>). The control fitness <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e156" xlink:type="simple"/></inline-formula> tests the performance of the controller on ten randomly generated mazes that the animat has never before encountered (see <xref ref-type="sec" rid="s4">Methods</xref>), in order to test whether the animat evolved the navigation principles or simply adapted to a particular instance of the problem.</p>
        <p>The LOD recapitulates the evolutionary history of the population, and allows a reconstruction of the path taken mutation by mutation. <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref> shows the evolution of fitness and control fitness for one of 64 experiments [panel (A) shows the fitness on the LOD, while panel (B) shows the corresponding fitness of the best in the population of 300 individuals]. Notice that in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5B</xref> the fitness of the fittest individual is almost always larger than the control fitness for the same individual, while the fitness on the LOD instead scatters around the control fitness, as seen in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5A</xref>. There is a good reason for this difference: in any population, an animat can be fit by chance through having correctly “guessed” the next door position repeatedly. The control fitness removes this element of chance by testing the individual on ten randomly generated mazes. The individuals on the LOD on the other hand are there for a reason and not by chance: their genes have proven themselves in later generations. In the run depicted in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref> (see also the movies <xref ref-type="supplementary-material" rid="pcbi.1002236.s005">Video S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002236.s006">S2</xref>, <xref ref-type="supplementary-material" rid="pcbi.1002236.s007">S3</xref>), the animat evolved a sophisticated (but not perfect) algorithm to navigate the maze, including the use of memory around generation 15,000 to store the doorway bit until the animat reaches the next wall. The wiring diagram of the animat at generation 49,000 is depicted in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6A</xref>. The animat uses only internal node 9 as memory, whose permanency is ensured using auto-feedback. The other nodes are connected but have no fitness impact whatsoever at this time, as determined by a knock-out analysis (see <xref ref-type="sec" rid="s4">Methods</xref>, data not shown), but may have been useful earlier on. The controller contains a total of 17 HMGs, but only nine HMGs (including two pairs of redundant HMGs) are responsible for this wiring. Of the nine useful HMGs, five have three inputs and one output, the other four HMGs are NOT gates. Note that if more than one HMG output serves as input for another HMG, their values are combined using an OR gate. The animat uses the information from the 3-bit retina, the lateral sensors, as well as the conditional information from the door beacon (sensor 3 in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6A</xref>) effectively by integrating this information within the decision machinery for navigation. The central hub is the network's memory: internal bit 9 is set to 0 if the door beacon is detected in the “on” state (b3 = 1) in a doorway, and to 1 if not. The value of bit 9 is maintained until the animat reaches the next wall. (The value of the door bit itself is erased from the sensor after the animat passes through the door, and therefore cannot be accessed by simply re-reading that value.) At that point the value of bit 9 determines if the animat goes left (b9 = 1) or right (b9 = 0). Once the animat is moving along a wall, bit 9 is set to 1 and the animat continues moving in the same direction keeping in mind the value of the left motor (b11). In a sense, the motor bit b11 is also used as memory here, as indicated by the auto-feedback. If bit 5 indicates an obstacle to the right, bit 11 is set, which forces bit 10 off in turn. If bit 4 indicates an obstacle to the left on the other hand, bit 9 is set to 0 which causes bit 11 to turn off and bit 10 to turn on. Once the animat is in front of the next doorway, it moves forward through the door. Thus, we see that this animat effectively uses the integration of different streams of information (door sensor, retina, lateral sensors, and current state of motion) to compute behavior that is appropriate in the given environment most of the time. Reaching 88% of “maximal” fitness is fairly remarkable, as a hand-written optimal controller reaches only 93% of maximal fitness (data not shown) because we force our controllers to be minimally stochastic.</p>
        <fig id="pcbi-1002236-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Fitness evolution on the line of descent and in the population.</title>
            <p><bold>A</bold>: Fitness (blue line) and control fitness (green line) for genotypes on the LOD. <bold>B</bold>: Fitness and control fitness for the same run as shown in (A), but for the fittest individual in the population in each generation. Colors as in (A).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g005" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002236-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Two evolved HMG networks.</title>
            <p>The shapes represent the 9 Markov variables (bits) at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e157" xlink:type="simple"/></inline-formula> that are active in the network (bits 6, 7, and 8 are connected to the network, but are not functional at generation 49,000 and not rendered here). The central feed-forward circuit for navigation is rendered in bold arrows. Color codes and numbering as in <xref ref-type="fig" rid="pcbi-1002236-g002">Fig. 2</xref>. <bold>A</bold>: The network evolved in our focus experiment that achieved 88% of possible fitness. <bold>B</bold>: Another network that evolved in an independent run, and that implements a variant of the hierarchical temporary memory algorithm that creates an expectation of future sensory signals. In contrast to the controller that evolved in panel (A), this one uses a feed-back strategy between memory and motors. This controller achieves 74% of maximal fitness within a random maze environment.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g006" xlink:type="simple"/>
        </fig>
        <p>In another run that achieved a fitness of 74%, a related but fundamentally different algorithm evolved to achieve almost the same functionality (wiring depicted in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6B</xref>). The central part of this algorithm, which is a version of the “hierarchical temporal memory algorithm” <xref ref-type="bibr" rid="pcbi.1002236-Hawkins1">[59]</xref>, is implemented by a feedback loop between the motors 10 and 11 and internal bit 9 (bold arrows in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6B</xref>), as opposed to the feed-forward loop seen in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6A</xref>. Because the animat can read from its motor bits, it can keep track of how it is currently moving, and make decisions based on this state as well as the state of the internal variable bit 9. Temporal memory is achieved by creating a basic expectation (bit 9 set to one) of encountering a door beacon that will be pointing it to the left (bit 3 = 0). If instead it encounters a door pointing to the right (bit 3 = 1), it changes that expectation (bit 9 = 0) and maintains it in memory until it moves in the correct (right) direction. Once this happens, the expectation is changed back to anticipating a beacon pointing it to the left, but the animat does not immediately react to this expectation because bit 9 is ignored as long as the animat moves to the right.</p>
        <p>Let us now look at our information-theoretic constructions as a function of evolutionary time. The quantity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e158" xlink:type="simple"/></inline-formula> is expensive to calculate so they and other measures were calculated along the LOD of each population every 500 generations up to generation 50,000. Each genome was evaluated by testing the controller it spawns for 1,000 world-time steps in 10 control mazes (each tested 10 times, see <xref ref-type="sec" rid="s4">Methods</xref>) in order to even out chance achievements (animates can achieve high fitness by chance due to the stochastic nature of their controllers). We show the evolution of three information integration and three information processing measures over time (for the same run whose fitness evolution is depicted in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref>) in <xref ref-type="fig" rid="pcbi-1002236-g007">Fig. 7</xref>. As fitness increases, all measures we plot here increase at first, but quickly become stagnant when fitness flattens out (see <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref>). Important changes are apparent in all measures when the capacity to use the door beacon for navigation emerges around generation 15,000. To see differences in the measure's abilities to predict fitness, we need to analyze how well these complexity proxies correlate with fitness across our set of 64 runs.</p>
        <fig id="pcbi-1002236-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Information-based measures of complexity.</title>
            <p><bold>A</bold>: Three <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e159" xlink:type="simple"/></inline-formula> related measures of information integration for genomes on the LOD of the same run as shown in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref>. Blue line: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e160" xlink:type="simple"/></inline-formula> defined in (15), green: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e161" xlink:type="simple"/></inline-formula> defined in (13) and red: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e162" xlink:type="simple"/></inline-formula>. <bold>B</bold>: Three information processing measures for the same experiment as (A): Blue: total information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e163" xlink:type="simple"/></inline-formula> (11), green: atomic information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e164" xlink:type="simple"/></inline-formula> (12), and red: predictive information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e165" xlink:type="simple"/></inline-formula> (1).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g007" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2b">
        <title>Statistics</title>
        <p>In order to test whether fitness correlates with a complexity proxy, we calculate the (nonparametric) Spearman rank correlation coefficient of the “final” fitness (the fitness of the genome at generation 49,000 on the LOD, see <xref ref-type="sec" rid="s4">Methods</xref>) with the value of that variable measured at generation 49,000. We chose generation 49,000 as final time because the organism from this generation is guaranteed to represent the common line of descent of the 300 individuals in any particular run (see <xref ref-type="sec" rid="s4">Methods</xref>). While we have correlation data of fitness with each variable along the LOD every 500 generations for each run, these points are not independent, and therefore cannot be used in order to assess the statistical significance of the correlation. The correlation of final fitness (across 64 independent samples) with each of the different information-theoretical candidates for functional complexity is shown in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>. Note that the highest control fitness achieved across the 64 runs is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e166" xlink:type="simple"/></inline-formula>, or almost 90% of perfect performance (see <xref ref-type="sec" rid="s4">Methods</xref> for our definition of fitness). That data point (for the run shown in <xref ref-type="fig" rid="pcbi-1002236-g005">Fig. 5</xref>, giving rise to the controller depicted in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6A</xref>) is indicated in red in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>. The run that evolved the controller shown in <xref ref-type="fig" rid="pcbi-1002236-g006">Fig. 6B</xref> is colored green in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>.</p>
        <fig id="pcbi-1002236-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Correlation of information-based measures of complexity with fitness.</title>
            <p><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e167" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e168" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e169" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e170" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e171" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e172" xlink:type="simple"/></inline-formula> plotted against <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e173" xlink:type="simple"/></inline-formula> (as a percentage of optimal fitness) using the final fitness (generation 49,000) on the LOD trajectory for 64 independent runs. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e174" xlink:type="simple"/></inline-formula> indicates Spearman's rank correlation coefficient.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.g008" xlink:type="simple"/>
        </fig>
        <p>For all measures, we observe positive and highly significant correlations with fitness (<xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref> and <xref ref-type="table" rid="pcbi-1002236-t001">Table 1</xref>). The best correlation is achieved for the integrated information measure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e175" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e176" xlink:type="simple"/></inline-formula>), followed by the information integration across the atomic partition <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e177" xlink:type="simple"/></inline-formula> (Spearman's <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e178" xlink:type="simple"/></inline-formula>), while the correlation with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e179" xlink:type="simple"/></inline-formula> is weaker (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e180" xlink:type="simple"/></inline-formula>). Likewise, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e181" xlink:type="simple"/></inline-formula>, which does not attempt to separate out the integration of different streams of information correlates only weakly with fitness (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e182" xlink:type="simple"/></inline-formula>). The atomic processed information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e183" xlink:type="simple"/></inline-formula> [Eq. (12), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e184" xlink:type="simple"/></inline-formula>] and the integration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e185" xlink:type="simple"/></inline-formula> both contribute to the strong correlation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e186" xlink:type="simple"/></inline-formula> with fitness, as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e187" xlink:type="simple"/></inline-formula> is a sum of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e188" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e189" xlink:type="simple"/></inline-formula> as per Eq. (14). The integration measures <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e190" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e191" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e192" xlink:type="simple"/></inline-formula> also correlate well with each other (data not shown). The difference in the correlation coefficients for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e193" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e194" xlink:type="simple"/></inline-formula> is highly significant (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e195" xlink:type="simple"/></inline-formula> in a Fisher r-to-z transformation test).</p>
        <table-wrap id="pcbi-1002236-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002236.t001</object-id><label>Table 1</label><caption>
            <title>Spearman's rank correlation coefficients (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e196" xlink:type="simple"/></inline-formula>) and significance (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e197" xlink:type="simple"/></inline-formula>-value) between different candidate measures of functional complexity with “final fitness”, using the values achieved at generation 49K of the LOD (an approxiSmation of the most recent common ancestor) for 64 independent runs.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002236-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e198" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e199" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e200" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e201" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e202" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e203" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e204" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">0.937</td>
                <td align="left" colspan="1" rowspan="1">0.784</td>
                <td align="left" colspan="1" rowspan="1">0.776</td>
                <td align="left" colspan="1" rowspan="1">0.553</td>
                <td align="left" colspan="1" rowspan="1">0.335</td>
                <td align="left" colspan="1" rowspan="1">0.63</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e205" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e206" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e207" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e208" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e209" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e210" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e211" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>A clear separation between runs that achieved high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e212" xlink:type="simple"/></inline-formula> of maximal fitness) and low fitness (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e213" xlink:type="simple"/></inline-formula>) is apparent in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>, indicating the difference between controllers that can or cannot access the information in the door beacon, which in turn requires the evolution of at least a single bit of memory. However, while it is not possible to achieve fitness in excess of 70% without using the information from the door beacon, one run utilized this information without exceeding 60% fitness, as determined via a knock-out analysis of the Markov variables.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have characterized several different information-theoretic measures in terms of their ability to reflect the complexity of information processing and integration in discrete dynamical systems. In order to discuss non-trivial examples of networks that are functional, we evolved computational networks that control an animat's behavior in a maze, and tested whether an increase in appropriate behavior is correlated with the putative proxies for complexity. The “brains” we evolved capture the essence of what it means to be successful in the maze environment: they can navigate arbitrary mazes of the type they are confronted with, and perform equally well with random versions of mazes that they never encountered during their evolution. In particular, they integrate the sensory information from several sources appropriately, and when they evolve memory they are able to implement it in a variety of ways, including a variant of the hierarchical temporal memory paradigm <xref ref-type="bibr" rid="pcbi.1002236-Hawkins1">[59]</xref>. We find that a standard measure that has been used to characterize complex robot behavior in the past <xref ref-type="bibr" rid="pcbi.1002236-Ay1">[36]</xref>, the predictive information <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e214" xlink:type="simple"/></inline-formula>, usually correlates well with fitness but sometimes fails to do so. We found examples where the failure to be predictive of fitness is associated with the evolution of memory (for example, the run indicated in green in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>), but also examples where this is not the case (e.g., the run that achieved the highest fitness, shown in red in <xref ref-type="fig" rid="pcbi-1002236-g008">Fig. 8</xref>). We hypothesize that when memory emerges, the integration of information from memory with the other signal streams is best reflected by measures of information integration such as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e215" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e216" xlink:type="simple"/></inline-formula>. Indeed, it is possible to show under fairly general assumptions that measures like <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e217" xlink:type="simple"/></inline-formula> can maximize fitness under the condition that no other information is used by an agent (such as acquired or inherited information <xref ref-type="bibr" rid="pcbi.1002236-Rivoire1">[27]</xref>, see also <xref ref-type="supplementary-material" rid="pcbi.1002236.s002">Text S1</xref>). Thus, while we expect that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e218" xlink:type="simple"/></inline-formula> performs worse and worse as a predictor of complex function as more and more memory is utilized for navigation, in some cases <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e219" xlink:type="simple"/></inline-formula> turns out to perform very well counter to this expectation. It is currently unclear what is at the origin of this difference in predictive performance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e220" xlink:type="simple"/></inline-formula>.</p>
      <p>That <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e221" xlink:type="simple"/></inline-formula> ultimately has to fail as a predictor of fitness when memory is used can be seen in the limiting case of navigating entirely by memory. In that case, any correlation between sensory inputs and motor actions would be purely accidental, in particular if the sensory data that ultimately predict appropriate motor actions are not in the immediate past. However, a non-Markovian version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e222" xlink:type="simple"/></inline-formula> that takes sensorial data from more distant time steps into account could conceivably perform well even in this case.</p>
      <p>On the other hand, measures of information integration could still be elevated even when navigating by memory, as the motor units are driven by streams of information emanating from within, rather than without. However, as sensorial information is not integrated, measures of information integration should be lower when navigating entirely by memory as opposed to navigating via sensors complemented by memory. At the same time, a brain that dreams rather than acts has vanishing predictive information (as the sensor inputs as well as the motor units have vanishing entropy). Yet, integrated information could still be high, and thus reflect complex information processing in the brain even in the absence of behavior. In this respect, measures of integrated information are a good candidate for a quantitative measure of consciousness, as advocated earlier <xref ref-type="bibr" rid="pcbi.1002236-Tononi1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Balduzzi1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Tononi2">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1002236-Tononi3">[50]</xref>. We note, however, that evolving functional networks with high <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e223" xlink:type="simple"/></inline-formula> is not easy. For our 12-bit controllers, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e224" xlink:type="simple"/></inline-formula> bits almost always, and the main complex is significantly smaller than the network size, usually only comprising sensor and motor variables, and occasionally the memory bit when it is used.</p>
      <p>Clearly, how useful <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e225" xlink:type="simple"/></inline-formula> is as a measure of brain complexity let alone consciousness rests on testing it on more complex networks that enable complex behavior in simulated environments that are both deep and broad. Evolving networks that rely heavily on memory, and that have the capacity to observe <italic>their own state</italic> <xref ref-type="bibr" rid="pcbi.1002236-Adami5">[64]</xref> and integrate that information with the sensorial stream, would be particularly useful in this respect. Ultimately, we expect that measures of information integration can then turn into predictors of fitness or function rather than the other way around. Indeed, the functional complexity of biological organisms (measured in terms of fitness) can only be estimated in the rarest of cases when we have a full understanding of what makes an organism successful in its particular niche.</p>
      <p>In future work, we hope to evolve animats in more complex environments that require more broad and versatile use of memory, to thoroughly test the hypothesis that information integration measures outperform pure processing measures such as predictive information in complex tasks. Furthermore, we plan to test whether animats evolve <italic>information matching</italic> <xref ref-type="bibr" rid="pcbi.1002236-Tononi4">[65]</xref>, that is, whether the integrated informational structure generated by an adapted complex fits, or matches, the informational structure of its environment. As it is possible to determine in detail how information about the world is represented within the Markov brains of these animats, the evolution of such creatures should demonstrate that evolution can move beyond representation-free AI <xref ref-type="bibr" rid="pcbi.1002236-Brooks1">[66]</xref> towards autonomous intelligence.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Agent embodiment</title>
        <p>Of the six sensors shown in <xref ref-type="fig" rid="pcbi-1002236-g002">Fig. 2</xref>, three are obstacle detection bits (binary sensors that indicate that an obstacle is in front of it (bits 0–2 encode front, left-front, and right-front, respectively), as well as a “door beacon” (bit 3) that indicates whether the <italic>next</italic> opening will be to the right (bit 3 = ‘1’) or else in front or left (bit 3 = ‘0’) of the opening that the animat is currently passing through. This bit can be used to navigate more successfully in the maze, by keeping this bit in memory and integrating this information with the other sensors. The next opening-direction information is not available after the animat passes through the previous opening. Because the animat cannot turn, it is important to detect whether an animat has hit a lateral wall. Detectors 4 and 5 each return ‘1’ if there is a wall to the left or right respectively.</p>
        <p>For example, in <xref ref-type="fig" rid="pcbi-1002236-g004">Fig. 4</xref>, the opening-direction bit (bit 3 = ‘1’) in the door just after the starting location indicates that the subsequent opening is to the right. After reaching this door and stepping through it, the sensor bit is set to bit 3 = ‘0’ indicating that the next opening is to the left or in front (in this case, in front). Therefore, efficiently navigating the maze requires memorizing this bit when the animat is in an opening and acting on that information until the animat can see the next opening. Two output bits (motors) control each animat's movement: the animat moves right if only bit 10 is on, left if only bit 11 is on, and forward if both are on. The animat has four internal bits (circles 6–9 in <xref ref-type="fig" rid="pcbi-1002236-g002">Fig. 2</xref>) that it can use for information memorization and integration.</p>
      </sec>
      <sec id="s4b">
        <title>Hidden Markov gates</title>
        <p>The table depicted within each HMG in <xref ref-type="fig" rid="pcbi-1002236-g003">Fig. 3</xref> represents the gate's function in terms of a stochastic finite state machine. The binary state of each HMG's inputs corresponds to a row in its probability table. These probabilities are encoded within the genes that specify the network, as described in <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref>. To determine how those probabilities generate an output from an input, first a random number between 0 and the sum of the elements of that row is generated. Comparing this random number to the cumulative sum of the numbers in that row selects an element in the row whose column index corresponds to the binary state of that gate's outputs. The OR operator is used to combine the outputs from multiple gates which output to the same bit.</p>
      </sec>
      <sec id="s4c">
        <title>Genetic encoding of network structure</title>
        <p>Networks are encoded within circular genomes that are given by a sequence of unsigned characters [0,255]. Each gene encodes a single HMG and its connection to other gates via the Markov variables, as well as the state-transition probabilities that define the gate. Details about the interpretation of the genome and its translation into a network are given in <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref>. Each HMG can have at most 4 inputs, and at most 3 outputs. If more than one HMG writes into a single Markov variable, these outputs are combined via an OR operation but we allow at most 3 write-attempts into a single Markov variable. If in the sequential interpretation of the genome an HMG requests to write to a variable that already has three connections, that HMG's connection will instead be routed to the nearest available variable. The same restrictions exist if an HMG tries to read from a variable that already has 3 read connections.</p>
      </sec>
      <sec id="s4d">
        <title>Fitness calculation</title>
        <p>The animat's fitness in a maze <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e226" xlink:type="simple"/></inline-formula> is determined by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e227" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e228" xlink:type="simple"/></inline-formula> is the number of time steps (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e229" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e230" xlink:type="simple"/></inline-formula> is the shortest path distance to the last doorway in the maze from the animat's position at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e231" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e232" xlink:type="simple"/></inline-formula> is the maximum shortest path from all locations in the maze to the last doorway, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e233" xlink:type="simple"/></inline-formula> is the number of times the animat has passed the last doorway. The maze environment is periodic so that if the animat goes past the end of the maze, the environment is the same as the beginning of the maze.</p>
        <p>The stochastic nature of the controller implies that the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e234" xlink:type="simple"/></inline-formula> measured in one run through a single maze <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e235" xlink:type="simple"/></inline-formula> may not be a reliable estimate of the genome's fitness because chance decisions could lead to either too high or too low fitness. We therefore define the animat's selection fitness by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e236" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e237" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e238" xlink:type="simple"/></inline-formula> stochastic realization of the animat's fitness in maze <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e239" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e240" xlink:type="simple"/></inline-formula> is the maximum fitness attainable in that maze. The geometric mean of 10 evaluations helps to ensure the reproducibility of the animat's fitness, and make it a better predictor of the long-term success of the lineage it represents.</p>
        <p>In order to ensure that the genomes have evolved the ability to navigate through <italic>general</italic> mazes of this type (rather than adapting to a single particular maze), a set of 10 control mazes are used to calculate the <italic>control fitness</italic>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e241" xlink:type="simple"/><label>(18)</label></disp-formula>The control fitness uses the arithmetic rather than the geometric mean in order to better track performance. The geometric mean in Eq. (17) allows for the elimination of controllers that ever completely fail at a single instance (as fitness is then multiplied by zero). The arithmetic mean in Eq. (18) is a better numerical indicator for the power of the strategy, as a single failure does not result in a vanishing control fitness.</p>
      </sec>
      <sec id="s4e">
        <title>Evolution and Genetic Algorithm</title>
        <p>64 populations of 300 individuals were evolved for 50,000 generations. For the purpose of selection, a single maze was randomly generated for each run, given a set of boundary conditions. Every 100 generations a new maze was generated for each run so that the animats would not adapt to a specific instance of the problem. To implement selection, the top three individuals from each generation (the elite) were copied into the next generation without mutation, unless their fitness was determined to be zero after re-testing. The remaining places in the population were filled by roulette-wheel selection with mutations <xref ref-type="bibr" rid="pcbi.1002236-Michalewicz1">[67]</xref>. This implies that the number of offspring that any parent places into the next generation is proportional to the relative fitness advantage (or disadvantage) it holds with respect to the average population fitness. However, no individual could place more than 10 offspring into the next generation. The genomes (described in <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002236.s001">Fig. S1</xref>) were changed via a variety of processes from generation to generation. Single loci were copied with a probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e242" xlink:type="simple"/></inline-formula>, deleted with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e243" xlink:type="simple"/></inline-formula>, a random value inserted after a loci with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e244" xlink:type="simple"/></inline-formula>, replaced with a uniformly drawn random number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e245" xlink:type="simple"/></inline-formula> with probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e246" xlink:type="simple"/></inline-formula>, or increased/decreased by a random number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e247" xlink:type="simple"/></inline-formula> (restricted to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e248" xlink:type="simple"/></inline-formula> if necessary) with probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e249" xlink:type="simple"/></inline-formula>. Whole genes where duplicated with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e250" xlink:type="simple"/></inline-formula>, deleted with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e251" xlink:type="simple"/></inline-formula>, and a random gene inserted with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e252" xlink:type="simple"/></inline-formula>.</p>
        <p>Finally, all mutation rates were normalized such that the whole genome mutation rate is equal to one change per genome per generation on average. This has the consequence that the “expressed” genome fraction (fraction with functioning start codon giving rise to HMGs connected to the main network) decreases with evolutionary time. Around 50,000 generations, the amount of expressed genes is of the order of 15% of the total genome size (on average about 200 of 3,000 loci are expressed in an evolved genotype).</p>
      </sec>
      <sec id="s4f">
        <title>Knock-out analysis</title>
        <p>In order to determine the importance and role of individual variables in the brain's operation, we perform “knock-outs” on the variables to test their effect on the Markov animat's performance. Four types of per-bit knockouts were used: replace the value that the variable takes on by ‘always read 0’, ‘always read 1’, ‘always write 0’, and ‘always write 1’. Some brains use variables with fixed values on purpose, in order to select certain rows from the probability tables with certainty. Such variables can be detected when only one of the two read-knockouts (read-zero or read-one) reduce the fitness of the controller. Variables that actually store and/or process information will lead to reduced fitness by both knockouts. Motor variables that are not read from are unaffected by the read knockouts but are affected by the write knockouts. Similarly, write-knockouts from sensor variables do not affect fitness, while read-knockouts do.</p>
        <p>To determine the function of individual HMGs, first each HMG was deleted from the controller to see if it changed the fitness. This identified unique important HMGs, but sometimes the results were masked by redundant HMGs. Then, each entry in the probability table for each HMG was “knocked out” by replacing the corresponding allele by zero or 255 [see Eq. (1) of <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref> for the effect of this replacement]. This data combined with the input distribution for each HMG was used to determine the role of any particular HMG in the brain, and how it worked together with the other HMGs to control the animat.</p>
      </sec>
      <sec id="s4g">
        <title>Line of descent</title>
        <p>For each run the line of descent (LOD) was obtained <xref ref-type="bibr" rid="pcbi.1002236-Lenski2">[68]</xref> by tracing back the fittest organism in the population backwards towards the randomly constructed ancestral sequence used to begin each experiment (encoding on average 12 HMGs, see <xref ref-type="supplementary-material" rid="pcbi.1002236.s003">Text S2</xref>). Seen from the point of view of the ancestral sequence, each following generation creates a branching tree with some lines eventually becoming extinct and other branches surviving. Because we simulate an asexual population in a single niche, only a single line of descent can ultimately remain because of competitive exclusion between members of the same species <xref ref-type="bibr" rid="pcbi.1002236-Hardin1">[69]</xref>. This line can be identified from following the lineage back from <italic>any</italic> of the 300 organism present in the final generation (generation 50,000) back to the origins (300 individual lines of descent). Going back ten generations, say, to 49,990, there will be fewer lines because some lines coalesced going backwards (branched going forward). The further backwards one moves on this “tree of descent”, the more lines coalesce until the last common ancestor (LCA) of the entire population that was alive at generation 50,000 has been reached. Because of the single-niche environment, the 300 lines coalesce very quickly, and are virtually guaranteed to have coalesced to a single line when going back to generation 49,000, which is the “final” generation we study in our simulations, and defines the “final fitness”. The organism at generation 49,000 of the LOD is not guaranteed to be the LCA, but it is guaranteed to be the ancestor of all organisms present in the final generation. Thus, the LOD records the evolutionary history of the experiment mutation by mutation, and allows us to reconstruct the evolutionary path that led to the adapted type. Fitness as well as complexity measures were calculated for organisms on the LOD every 500 generations.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002236.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Genetic encoding of animat controllers. <bold>A</bold>: In this example, two HMGs encoded by two genes can read from and write to several of the 12 Markov variables, indexed 0–11. The top row shows the Markov variables at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e253" xlink:type="simple"/></inline-formula> that the HMGs can read from while the row below shows how the HMGs write into those variables to update their state at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e254" xlink:type="simple"/></inline-formula>. <bold>B</bold>: The genome is a circular sequence of loci that carry unsigned integers <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e255" xlink:type="simple"/></inline-formula> and encode the input output structure of each HMG as well as the connectivity between them and the state transition tables that determine each HMG's function. Colors denote different functional sections of the gene. <bold>C</bold>: Causal influence of the Markov variables induced by the two HMGs. Presence of an arrow between variables <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e256" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e257" xlink:type="simple"/></inline-formula> implies that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e258" xlink:type="simple"/></inline-formula> may change the state of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e259" xlink:type="simple"/></inline-formula> in a single time step. Absence of an arrow implies that the variables cannot influence each other within a single time step.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s002" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Relationship between different forms of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e260" xlink:type="simple"/></inline-formula></p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s003" xlink:type="simple">
        <label>Text S2</label>
        <caption>
          <p>Genetic encoding of network structure and function.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s004" xlink:type="simple">
        <label>Text S3</label>
        <caption>
          <p>Relationship between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e261" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e262" xlink:type="simple"/></inline-formula>.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s005" mimetype="Video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s005" xlink:type="simple">
        <label>Video S1</label>
        <caption>
          <p>This movie shows the trajectory of an evolved animat traveling through the maze after 2,000 generations of evolution in the top panel, and the inner workings of its Markov network brain in the lower panel. At this point in evolutionary history, the animat has learned to move forward whenever it stands in front of an opening, but otherwise performs a random walk. The fitness at this time point is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e263" xlink:type="simple"/></inline-formula> of maximal. The animat in the maze is depicted with a triangle, and the trail it leaves reflects the activation pattern of its four internal nodes and its motor outputs, as described in <xref ref-type="fig" rid="pcbi-1002236-g004">Fig. 4</xref>. The brain state (lower panel) shows all HMGs (U0–U10) and the probabilities in the state-transition tables as percentages (colored in shades of gray). Input bits (labeled iB) and output bits (labeled oB) are green if true and blue if false. The red element in each table indicates the element of the table selected at that time step based on the values of the input bits and the probabilities in that row. In other words, a table element turning red indicates which state of the HMG was selected as a function of the input. This is akin to a pattern of neuronal firings as a function of the inputs.</p>
          <p>(MP4)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s006" mimetype="Video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s006" xlink:type="simple">
        <label>Video S2</label>
        <caption>
          <p>The trajectory and brain states of an evolved animat at generation 14,000. At this point, the animat has acquired the capacity to maintain a direction of travel and move opposite to the direction indicated by the lateral contact sensor. Its movement with respect to the door opening is still random. The fitness at this time point is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e264" xlink:type="simple"/></inline-formula> of maximal.</p>
          <p>(MP4)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002236.s007" mimetype="Video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002236.s007" xlink:type="simple">
        <label>Video S3</label>
        <caption>
          <p>The trajectory and brain states of an evolved animat at generation 49,000. By this time, the animat has evolved the capacity to use the information provided by the door beacon by storing it in bit 9, and move purposefully in the indicated direction after emerging from the previous door. Because of its high fitness, the animat traverses the maze five times, but does not always take the same trajectory every time, illustrating the stochasticity of its decisions. The fitness at this time point is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002236.e265" xlink:type="simple"/></inline-formula> of maximal.</p>
          <p>(MP4)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank David Balduzzi, Virgil Griffith, Nikhil J. Joshi, Sang Wan Lee, and Jory Schossau, and acknowledge detailed and insightful reviewer comments.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002236-Badii1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Badii</surname><given-names>R</given-names></name><name name-style="western"><surname>Politi</surname><given-names>A</given-names></name></person-group>             <year>1997</year>             <source>Complexity: Hierarchical structures and scaling in physics, volume 6 of <italic>Cambridge Nonlinear Science</italic> Series</source>             <publisher-loc>Cambridge, UK</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Adami1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2002</year>             <article-title>What is complexity?</article-title>             <source>Bioessays</source>             <volume>24</volume>             <fpage>1085</fpage>             <lpage>1094</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Adami2">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2004</year>             <article-title>Information theory in molecular biology.</article-title>             <source>Phys Life Rev</source>             <volume>1</volume>             <fpage>3</fpage>             <lpage>22</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Adami3">
        <label>4</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2009</year>             <article-title>Biological complexity and biochemical information.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Meyers</surname><given-names>RA</given-names></name></person-group>             <source>Encyclopedia of Complexity and Systems Science</source>             <publisher-name>Springer Verlag</publisher-name>             <fpage>489</fpage>             <lpage>511</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Sporns1">
        <label>5</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2011</year>             <source>Networks of the Brain</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lfgren1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Löfgren</surname><given-names>L</given-names></name></person-group>             <year>1977</year>             <article-title>Complexity of description of systems: A foundational study.</article-title>             <source>Int J Gen Sys</source>             <volume>3</volume>             <fpage>197</fpage>             <lpage>214</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Chaitin1">
        <label>7</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chaitin</surname><given-names>GJ</given-names></name></person-group>             <year>1979</year>             <article-title>Toward a mathematical definition of life.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Levine</surname><given-names>RD</given-names></name><name name-style="western"><surname>Tribus</surname><given-names>M</given-names></name></person-group>             <source>The Maximum Entropy Formalism</source>             <publisher-name>MIT Press</publisher-name>             <fpage>477</fpage>             <lpage>498</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Papentin1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Papentin</surname><given-names>F</given-names></name></person-group>             <year>1980</year>             <article-title>On order and complexity i: General considerations.</article-title>             <source>J theor Biol</source>             <volume>87</volume>             <fpage>421</fpage>             <lpage>456</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Papentin2">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Papentin</surname><given-names>F</given-names></name></person-group>             <year>1982</year>             <article-title>On order and complexity ii: Application to chemical and biochemical structures.</article-title>             <source>J theor Biol</source>             <volume>95</volume>             <fpage>225</fpage>             <lpage>245</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Thomas1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Thomas</surname><given-names>RDK</given-names></name><name name-style="western"><surname>Reif</surname><given-names>WE</given-names></name></person-group>             <year>1993</year>             <article-title>The skeleton space: A finite set of organic designs.</article-title>             <source>Evolution</source>             <volume>47</volume>             <fpage>341</fpage>             <lpage>360</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Thomas2">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Thomas</surname><given-names>RD</given-names></name><name name-style="western"><surname>Shearman</surname><given-names>RM</given-names></name><name name-style="western"><surname>Stewart</surname><given-names>GW</given-names></name></person-group>             <year>2000</year>             <article-title>Evolutionary exploitation of design options by the first animals with hard skeletons.</article-title>             <source>Science</source>             <volume>288</volume>             <fpage>1239</fpage>             <lpage>1242</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Soloveichik1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Soloveichik</surname><given-names>D</given-names></name><name name-style="western"><surname>Winfree</surname><given-names>E</given-names></name></person-group>             <year>2006</year>             <article-title>Complexity of self-assembled shapes.</article-title>             <source>SIAM J Comput</source>             <volume>36</volume>             <fpage>1544</fpage>             <lpage>1569</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ahnert1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ahnert</surname><given-names>SE</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>IG</given-names></name><name name-style="western"><surname>Fink</surname><given-names>TMA</given-names></name><name name-style="western"><surname>Doye</surname><given-names>JPK</given-names></name><name name-style="western"><surname>Louis</surname><given-names>AA</given-names></name></person-group>             <year>2010</year>             <article-title>Self-assembly, modularity, and physical complexity.</article-title>             <source>Phys Rev E</source>             <volume>82</volume>             <fpage>026117</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Kolmogorov1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kolmogorov</surname><given-names>A</given-names></name></person-group>             <year>1965</year>             <article-title>Three approaches to the quantitative definition of information.</article-title>             <source>Probl Inf Trans</source>             <volume>1</volume>             <fpage>4</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lempel1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lempel</surname><given-names>A</given-names></name><name name-style="western"><surname>Ziv</surname><given-names>J</given-names></name></person-group>             <year>1976</year>             <article-title>On the complexity of finite sequences.</article-title>             <source>IEEE Trans Inform Theory</source>             <volume>22</volume>             <fpage>75</fpage>             <lpage>81</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ebeling1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ebeling</surname><given-names>W</given-names></name><name name-style="western"><surname>Jimenez-Montano</surname><given-names>M</given-names></name></person-group>             <year>1980</year>             <article-title>On grammars, complexity, and information measures of biological macromolecules.</article-title>             <source>Math Biosci</source>             <volume>52</volume>             <fpage>53</fpage>             <lpage>71</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Li1">
        <label>17</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>M</given-names></name><name name-style="western"><surname>Vitanyi</surname><given-names>P</given-names></name></person-group>             <year>1997</year>             <source>An introduction to Kolmogorov complexity and its applications</source>             <publisher-loc>New York, NY</publisher-loc>             <publisher-name>Springer Verlag</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-GellMann1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gell-Mann</surname><given-names>M</given-names></name><name name-style="western"><surname>Lloyd</surname><given-names>S</given-names></name></person-group>             <year>1996</year>             <article-title>Information measures, effective complexity, and total information.</article-title>             <source>Complexity</source>             <volume>2</volume>             <fpage>44</fpage>             <lpage>52</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Adami4">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name><name name-style="western"><surname>Cerf</surname><given-names>NJ</given-names></name></person-group>             <year>2000</year>             <article-title>Physical complexity of symbolic sequences.</article-title>             <source>Physica D</source>             <volume>137</volume>             <fpage>62</fpage>             <lpage>69</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-McShea1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McShea</surname><given-names>DW</given-names></name></person-group>             <year>2000</year>             <article-title>Functional complexity in organisms: Parts as proxies.</article-title>             <source>Biol Philos</source>             <volume>15</volume>             <fpage>641</fpage>             <lpage>668</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Szostak1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Szostak</surname><given-names>JW</given-names></name></person-group>             <year>2003</year>             <article-title>Functional information: Molecular messages.</article-title>             <source>Nature</source>             <volume>423</volume>             <fpage>689</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Hazen1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hazen</surname><given-names>RM</given-names></name><name name-style="western"><surname>Griffin</surname><given-names>PL</given-names></name><name name-style="western"><surname>Carothers</surname><given-names>JM</given-names></name><name name-style="western"><surname>Szostak</surname><given-names>JW</given-names></name></person-group>             <year>2007</year>             <article-title>Functional information and the emergence of biocomplexity.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>104</volume>             <fpage>8574</fpage>             <lpage>8581</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Bennett1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bennett</surname><given-names>C</given-names></name></person-group>             <year>1995</year>             <article-title>Universal computation and physical dynamics.</article-title>             <source>Physica D</source>             <volume>86</volume>             <fpage>268</fpage>             <lpage>273</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lloyd1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lloyd</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>Measures of complexity: A nonexhaustive list.</article-title>             <source>IEEE Control Syst Mag N Y</source>             <volume>21</volume>             <fpage>7</fpage>             <lpage>8</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Taylor1">
        <label>25</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Taylor</surname><given-names>S</given-names></name><name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2007</year>             <article-title>Information and fitness.</article-title>             <comment>eprint arXiv.org:0712.4382</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Polani1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Polani</surname><given-names>D</given-names></name></person-group>             <year>2009</year>             <article-title>Information: currency of life?</article-title>             <source>HFSP J</source>             <volume>3</volume>             <fpage>307</fpage>             <lpage>316</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Rivoire1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rivoire</surname><given-names>O</given-names></name><name name-style="western"><surname>Leibler</surname><given-names>S</given-names></name></person-group>             <year>2011</year>             <article-title>The value of information for populations in varying environments.</article-title>             <source>J Stat Phys</source>             <volume>142</volume>             <fpage>1124</fpage>             <lpage>1166</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Bonchev1">
        <label>28</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bonchev</surname><given-names>DG</given-names></name></person-group>             <year>2009</year>             <article-title>Information theoretic complexity measures.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Meyers</surname><given-names>RA</given-names></name></person-group>             <source>Encyclopedia of Complexity and Systems Science</source>             <publisher-name>Springer Verlag</publisher-name>             <fpage>4820</fpage>             <lpage>4838</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Dehmer1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dehmer</surname><given-names>M</given-names></name><name name-style="western"><surname>Mowshowitz</surname><given-names>A</given-names></name></person-group>             <year>2011</year>             <article-title>A history of graph entropy measures.</article-title>             <source>Inf Sci</source>             <volume>181</volume>             <fpage>57</fpage>             <lpage>78</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Koch1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name></person-group>             <year>1999</year>             <article-title>Complexity and the nervous system.</article-title>             <source>Science</source>             <volume>284</volume>             <fpage>96</fpage>             <lpage>98</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Haldane1">
        <label>31</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haldane</surname><given-names>JBS</given-names></name></person-group>             <year>1932</year>             <source>The Causes of Evolution</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Longmans</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-MaynardSmith1">
        <label>32</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maynard Smith</surname><given-names>J</given-names></name></person-group>             <year>1969</year>             <source>Evolutionary Genetics</source>             <publisher-loc>Oxford, UK</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lenski1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lenski</surname><given-names>R</given-names></name><name name-style="western"><surname>Rose</surname><given-names>M</given-names></name><name name-style="western"><surname>Simpson</surname><given-names>S</given-names></name><name name-style="western"><surname>Tadler</surname><given-names>S</given-names></name></person-group>             <year>1991</year>             <article-title>Long-term experimental evolution in <italic>Escherichia coli</italic>. I. Adaptation and divergence during 2,000 generations.</article-title>             <source>Am Nat</source>             <volume>138</volume>             <fpage>1315</fpage>             <lpage>1341</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-DonaldsonMatasci1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Donaldson-Matasci</surname><given-names>MC</given-names></name><name name-style="western"><surname>Bergstrom</surname><given-names>CT</given-names></name><name name-style="western"><surname>Lachmann</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>The fitness value of information.</article-title>             <source>Oikos</source>             <volume>119</volume>             <fpage>219</fpage>             <lpage>230</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Bialek1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name><name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name></person-group>             <year>2001</year>             <article-title>Predictability, complexity, and learning.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>2409</fpage>             <lpage>2463</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ay1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name><name name-style="western"><surname>Bertschinger</surname><given-names>N</given-names></name><name name-style="western"><surname>Der</surname><given-names>R</given-names></name><name name-style="western"><surname>Guettler</surname><given-names>F</given-names></name><name name-style="western"><surname>Olbrich</surname><given-names>E</given-names></name></person-group>             <year>2008</year>             <article-title>Predictive information and explorative behavior of autonomous robots.</article-title>             <source>Eur Phys J B</source>             <volume>63</volume>             <fpage>329</fpage>             <lpage>339</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Felleman1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>             <year>1991</year>             <article-title>Distributed hierarchical processing in the primate cerebral cortex.</article-title>             <source>Cereb Cortex</source>             <volume>1</volume>             <fpage>1</fpage>             <lpage>47</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Hagmann1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hagmann</surname><given-names>P</given-names></name><name name-style="western"><surname>Cammoun</surname><given-names>L</given-names></name><name name-style="western"><surname>Gigandet</surname><given-names>X</given-names></name><name name-style="western"><surname>Meuli</surname><given-names>R</given-names></name><name name-style="western"><surname>Honey</surname><given-names>CJ</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Mapping the structural core of human cerebral cortex.</article-title>             <source>PLoS Biol</source>             <volume>6</volume>             <fpage>e159</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Tononi1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name></person-group>             <year>2008</year>             <article-title>Consciousness as integrated information: a provisional manifesto.</article-title>             <source>Biol Bull</source>             <volume>215</volume>             <fpage>216</fpage>             <lpage>242</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Balduzzi1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Balduzzi</surname><given-names>D</given-names></name><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name></person-group>             <year>2008</year>             <article-title>Integrated information in discrete dynamical systems: motivation and theoretical framework.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000091</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Slepian1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Slepian</surname><given-names>D</given-names></name><name name-style="western"><surname>Wolf</surname><given-names>JK</given-names></name></person-group>             <year>1973</year>             <article-title>Noiseless coding of correlated information sources.</article-title>             <source>IEEE Trans Inform Theory</source>             <volume>19</volume>             <fpage>471</fpage>             <lpage>480</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Barrett1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name><name name-style="western"><surname>Seth</surname><given-names>AK</given-names></name></person-group>             <year>2011</year>             <article-title>Practical measures of integrated information for time-series data.</article-title>             <source>PLoS Comput Biol</source>             <volume>7</volume>             <fpage>e1001052</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Rota1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rota</surname><given-names>GC</given-names></name></person-group>             <year>1964</year>             <article-title>The number of partitions of a set.</article-title>             <source>Am Math Mon</source>             <volume>7</volume>             <fpage>498</fpage>             <lpage>504</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Tononi2">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name></person-group>             <year>2010</year>             <article-title>Information integration: its relevance to brain function and consciousness.</article-title>             <source>Arch Ital Biol</source>             <volume>148</volume>             <fpage>299</fpage>             <lpage>322</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Atick1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name></person-group>             <year>1992</year>             <article-title>Could information theory provide an ecological theory of sensory processing.</article-title>             <source>Network</source>             <volume>3</volume>             <fpage>213</fpage>             <lpage>251</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Nadal1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name><name name-style="western"><surname>Parga</surname><given-names>N</given-names></name></person-group>             <year>1994</year>             <article-title>Nonlinear neurons in the low noise limit: A factorial code maximizes information transfer.</article-title>             <source>Network</source>             <volume>5</volume>             <fpage>565</fpage>             <lpage>581</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Schneidman1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name><name name-style="western"><surname>Still</surname><given-names>S</given-names></name><name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2003</year>             <article-title>Network information and connected correlations.</article-title>             <source>Phys Rev Lett</source>             <volume>91</volume>             <fpage>238701</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Barlow1">
        <label>48</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name></person-group>             <year>1961</year>             <article-title>Possible principles underlying the transformation of sensory messages.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group>             <source>Sensory Communication</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>217</fpage>             <lpage>234</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-McGill1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McGill</surname><given-names>WJ</given-names></name></person-group>             <year>1954</year>             <article-title>Multivariate information transmission.</article-title>             <source>Psychometrika</source>             <volume>19</volume>             <fpage>97</fpage>             <lpage>116</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Tononi3">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>G</given-names></name></person-group>             <year>1994</year>             <article-title>A measure for brain complexity: Relating functional segregation and integration in the nervous system.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>91</volume>             <fpage>5033</fpage>             <lpage>5037</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lungarella1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name><name name-style="western"><surname>Pegors</surname><given-names>T</given-names></name><name name-style="western"><surname>Bullwinkle</surname><given-names>D</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2005</year>             <article-title>Methods for quantifying the informational structure of sensory and motor data.</article-title>             <source>Neuroinformatics</source>             <volume>3</volume>             <fpage>243</fpage>             <lpage>262</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lungarella2">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name></person-group>             <year>2006</year>             <article-title>Mapping information ow in sensorimotor networks.</article-title>             <source>PLoS Comput Biol</source>             <volume>2</volume>             <fpage>1301</fpage>             <lpage>1312</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ay2">
        <label>53</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name></person-group>             <year>2001</year>             <article-title>Information geometry on complexity and stochastic interaction.</article-title>             <comment>Preprint 95/2001, Max Planck Institut for Mathematics in the Sciences, Leipzig</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ay3">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name><name name-style="western"><surname>Wennekers</surname><given-names>T</given-names></name></person-group>             <year>2003</year>             <article-title>Temporal infomax leads to almost deterministic dynamical systems.</article-title>             <source>Neurocomputing</source>             <volume>52-4</volume>             <fpage>461</fpage>             <lpage>466</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Ay4">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ay</surname><given-names>N</given-names></name><name name-style="western"><surname>Wennekers</surname><given-names>T</given-names></name></person-group>             <year>2003</year>             <article-title>Dynamical properties of strongly interacting markov chains.</article-title>             <source>Neural Netw</source>             <volume>16</volume>             <fpage>1483</fpage>             <lpage>1497</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Wilson1">
        <label>56</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wilson</surname><given-names>SW</given-names></name></person-group>             <year>1991</year>             <article-title>The animat path to AI.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Meyer</surname><given-names>JA</given-names></name><name name-style="western"><surname>Wilson</surname><given-names>S</given-names></name></person-group>             <source>From Animals to Animats</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>15</fpage>             <lpage>21</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Klyubin1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Klyubin</surname><given-names>AS</given-names></name><name name-style="western"><surname>Polani</surname><given-names>D</given-names></name><name name-style="western"><surname>Nehaniv</surname><given-names>CL</given-names></name></person-group>             <year>2007</year>             <article-title>Representations of space and time in the maximization of information ow in the perception-action loop.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2387</fpage>             <lpage>2432</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Koller1">
        <label>58</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koller</surname><given-names>D</given-names></name><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name></person-group>             <year>2009</year>             <source>Probabilistic Graphical Models</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Hawkins1">
        <label>59</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name><name name-style="western"><surname>Blakeslee</surname><given-names>S</given-names></name></person-group>             <year>2004</year>             <source>On Intelligence</source>             <publisher-loc>New York, NY</publisher-loc>             <publisher-name>Henry Holt and Co</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-George1">
        <label>60</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>George</surname><given-names>D</given-names></name><name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name></person-group>             <year>2005</year>             <article-title>A hierarchical Bayesian model of invariant pattern recognition in the visual cortex.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Prokhorov</surname><given-names>D</given-names></name></person-group>             <source>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</source>             <publisher-name>IEEE, volume 3</publisher-name>             <fpage>1812</fpage>             <lpage>1817</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-George2">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>George</surname><given-names>D</given-names></name><name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Towards a mathematical theory of cortical micro-circuits.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000532</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Riesenhuber1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name></person-group>             <year>1999</year>             <article-title>Hierarchical models of object recognition in cortex.</article-title>             <source>Nature Neuroscience</source>             <volume>2</volume>             <fpage>1019</fpage>             <lpage>1025</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Hintze1">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hintze</surname><given-names>A</given-names></name><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2008</year>             <article-title>Evolution of complex modular biological networks.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e23</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Adami5">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2006</year>             <article-title>What do robots dream of?</article-title>             <source>Science</source>             <volume>314</volume>             <fpage>1093</fpage>             <lpage>4</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Tononi4">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Edelman</surname><given-names>G</given-names></name></person-group>             <year>1996</year>             <article-title>A complexity measure for selective matching of signals by the brain.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>93</volume>             <fpage>3422</fpage>             <lpage>3427</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Brooks1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brooks</surname><given-names>R</given-names></name></person-group>             <year>1991</year>             <article-title>Intelligence without representation.</article-title>             <source>Artif Intell</source>             <volume>47</volume>             <fpage>139</fpage>             <lpage>159</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Michalewicz1">
        <label>67</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Michalewicz</surname><given-names>Z</given-names></name></person-group>             <year>1999</year>             <source>Genetic Algorithms+Data Structures = Evolution Programs</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer Verlag</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Lenski2">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lenski</surname><given-names>RE</given-names></name><name name-style="western"><surname>Ofria</surname><given-names>C</given-names></name><name name-style="western"><surname>Pennock</surname><given-names>RT</given-names></name><name name-style="western"><surname>Adami</surname><given-names>C</given-names></name></person-group>             <year>2003</year>             <article-title>The evolutionary origin of complex features.</article-title>             <source>Nature</source>             <volume>423</volume>             <fpage>139</fpage>             <lpage>144</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002236-Hardin1">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hardin</surname><given-names>G</given-names></name></person-group>             <year>1960</year>             <article-title>The competitive exclusion principle.</article-title>             <source>Science</source>             <volume>131</volume>             <fpage>1292</fpage>             <lpage>1297</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>