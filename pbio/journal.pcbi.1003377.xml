<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00258</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003377</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subject>Motor systems</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Interference and Shaping in Sensorimotor Adaptations with Rewards</article-title>
<alt-title alt-title-type="running-head">Sensorimotor Adaptations with Rewards</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Darshan</surname><given-names>Ran</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Leblois</surname><given-names>Arthur</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hansel</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Edmond and Lily Safra Center for Brain Sciences, The Hebrew University, Jerusalem, Israel</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Laboratoire de Neurophysique et Physiologie UMR8119-CNRS Université René Descartes, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>The Alexander Silberman Institute of Life Sciences, The Hebrew University of Jerusalem, Israel</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Torres-Oviedo</surname><given-names>Gelsy</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Pittsburgh, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">david.hansel@univ-paris5.fr</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: RD DH AL. Performed the experiments: RD. Analyzed the data: RD DH AL. Contributed reagents/materials/analysis tools: RD DH AL. Wrote the paper: RD DH AL.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>1</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>9</day><month>1</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>1</issue>
<elocation-id>e1003377</elocation-id>
<history>
<date date-type="received"><day>10</day><month>2</month><year>2013</year></date>
<date date-type="accepted"><day>14</day><month>10</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Darshan et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>When a perturbation is applied in a sensorimotor transformation task, subjects can adapt and maintain performance by either relying on sensory feedback, or, in the absence of such feedback, on information provided by rewards. For example, in a classical rotation task where movement endpoints must be rotated to reach a fixed target, human subjects can successfully adapt their reaching movements solely on the basis of binary rewards, although this proves much more difficult than with visual feedback. Here, we investigate such a reward-driven sensorimotor adaptation process in a minimal computational model of the task. The key assumption of the model is that synaptic plasticity is gated by the reward. We study how the learning dynamics depend on the target size, the movement variability, the rotation angle and the number of targets. We show that when the movement is perturbed for multiple targets, the adaptation process for the different targets can interfere destructively or constructively depending on the similarities between the sensory stimuli (the targets) and the overlap in their neuronal representations. Destructive interferences can result in a drastic slowdown of the adaptation. As a result of interference, the time to adapt varies non-linearly with the number of targets. Our analysis shows that these interferences are weaker if the reward varies smoothly with the subject's performance instead of being binary. We demonstrate how shaping the reward or shaping the task can accelerate the adaptation dramatically by reducing the destructive interferences. We argue that experimentally investigating the dynamics of reward-driven sensorimotor adaptation for more than one sensory stimulus can shed light on the underlying learning rules.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The brain has a robust ability to adapt to external perturbations imposed on acquired sensorimotor transformations. Here, we used a mathematical model to investigate the reward-based component in sensorimotor adaptations. We show that the shape of the delivered reward signal, which in experiments is usually binary to indicate success or failure, affects the adaptation dynamics. We demonstrate how the ability to adapt to perturbations by relying solely on binary rewards depends on motor variability, size of perturbation and the threshold for delivering the reward. When adapting motor responses to multiple sensory stimuli simultaneously, on-line interferences between the motor performance in response to the different stimuli occur as a result of the overlap in the neural representation of the sensory stimuli, as well as the physical distance between them. Adaptation may be extremely slow when perturbations are induced to a few stimuli that are physically different from each other because of destructive interferences. When intermediate stimuli are introduced, the physical distance between neighbor stimuli is reduced, and constructive interferences can emerge, resulting in faster adaptation. Remarkably, adaptation to a widespread sensorimotor perturbation is accelerated by increasing the number of sensory stimuli during training, i.e. learning is faster if one learns more.</p>
</abstract>
<funding-group><funding-statement>This work was carried out within the framework of the France-Israel Laboratory of Neuroscience (LEA-FILNe) and supported by a grant of the France-Israel High Council for Scientific and Technological cooperation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="20"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Transformations that map sensory inputs to motor commands are referred to as sensorimotor mappings <xref ref-type="bibr" rid="pcbi.1003377-Pouget1">[1]</xref>. While sensorimotor mappings are already formed at early stages of development <xref ref-type="bibr" rid="pcbi.1003377-Piaget1">[2]</xref>, they are subject to modifications, since the brain, the body and/or the environment are constantly changing. Plasticity in sensorimotor mappings has been extensively studied in situations where subjects receive sensory feedback during the task, allowing them to correct their motor actions and to adapt to the induced perturbation. These include visuomotor rotation <xref ref-type="bibr" rid="pcbi.1003377-Krakauer1">[3]</xref>, reaching movements under forcefields <xref ref-type="bibr" rid="pcbi.1003377-Thoroughman1">[4]</xref>, adaptation in a smooth pursuit eye movements <xref ref-type="bibr" rid="pcbi.1003377-Chou1">[5]</xref>, prism adaptation <xref ref-type="bibr" rid="pcbi.1003377-Linkenhoker1">[6]</xref>, and pitch perturbation in songbirds <xref ref-type="bibr" rid="pcbi.1003377-Sober1">[7]</xref> and in humans <xref ref-type="bibr" rid="pcbi.1003377-Houde1">[8]</xref>.</p>
<p>Although these studies involve different sensory modalities and different effectors, they are similar in the sense that they all have sensory goals (targets) and a motor gesture is made to reach the target. They consist of three phases namely a standard phase, in which subjects perform the task under regular conditions followed by an adaptation phase, where subjects perform the same task under the perturbed condition and a washout phase during which the perturbation is removed, and the subject readapts toward baseline. Remarkably, in all these three phases, movements display substantial trial to trial variability. Recent theoretical as well as experimental studies suggested that this variability plays a crucial role in sensorimotor learning and adaptation processes <xref ref-type="bibr" rid="pcbi.1003377-Sober2">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Rokni1">[11]</xref>.</p>
<p>Another issue concerns the ability of subjects to generalize the adaptation from one context condition to a different context. This has been investigated by testing how subjects perform upon presentation of sensory stimuli that were not present during the adaptation phase <xref ref-type="bibr" rid="pcbi.1003377-Poggio1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Tanaka1">[13]</xref>. Generalization is usually good for sensory stimuli that are similar to the one used during adaptation and degrades as the sensory stimuli become different <xref ref-type="bibr" rid="pcbi.1003377-Krakauer1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>. Remarkably, subjects can even perform worse than in baseline (negative generalization) for sensory stimuli which are very different from those which was presented to the subject during adaptation. This has been observed, for instance, in motor reaching tasks, when the tested stimulus is presented in a direction which is opposite to the adapted direction <xref ref-type="bibr" rid="pcbi.1003377-Thoroughman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>.</p>
<p>The above mentioned studies implicitly assumed that the neural mechanisms for adaptation are driven by a sensory feedback, which supplies a continuous error signal to the subject. Yet, recent studies show that adaptation is possible even without any sensory feedback, when only a binary reward that informs on a success or a failure of a trial is provided to the subject <xref ref-type="bibr" rid="pcbi.1003377-Tumer1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. Moreover, recent experimental works suggest that reward based mechanisms also affect the adaptation dynamics in sensorimotor tasks even when a sensory feedback is available <xref ref-type="bibr" rid="pcbi.1003377-Shmuelof1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Huang1">[19]</xref>.</p>
<p>However, and not surprisingly, adaptation relying solely on rewards at the end of a trial is more difficult than when a sensory feedback on the performance is provided continuously during the task, as adapting with sensory feedback conveys more information regarding errors. For instance, when visual feedback is available in visuomotor rotation tasks, subjects adapt to large perturbation (e.g. 30 degrees) in a few dozen trials <xref ref-type="bibr" rid="pcbi.1003377-Krakauer1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Paz1">[20]</xref>, while in the absence of such feedback, but with binary (success or a failure) reward feedback, subjects find it notoriously difficult to adapt. Recent studies, nevertheless, have shown that it is possible to adapt to large perturbations relying solely on rewards if the size of the perturbation is slowly increased between rewarded blocks of trials <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Warren1">[21]</xref>. The fact that progressively increasing the amount of perturbation makes it possible to adapt, even when the perturbation is large, is reminiscent of the classical shaping strategy <xref ref-type="bibr" rid="pcbi.1003377-Skinner1">[22]</xref>. In shaping, the difficulty of the task is increased gradually in order to accelerate learning, or to even make it possible. Although shaping is routinely used in laboratories when training animals to perform complex sensorimotor and cognitive tasks <xref ref-type="bibr" rid="pcbi.1003377-Lawrence1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Kangas1">[25]</xref>, it is only in recent years that it started to be explored in a theoretical framework <xref ref-type="bibr" rid="pcbi.1003377-Ng1">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Krueger1">[28]</xref>.</p>
<p>What neural mechanisms could be involved in this reward based learning? Recent experimental evidence <xref ref-type="bibr" rid="pcbi.1003377-Kerr1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Reynolds1">[31]</xref> indicates that rewards modulate local synaptic plasticity via global neuromodulatory signals, e.g. dopamine. When combined with the popular idea that synapses are modified according to Hebbian rules, this leads to the hypothesis that reward signals interact with local neuronal activity to modulate synaptic efficacies <xref ref-type="bibr" rid="pcbi.1003377-Loewenstein1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Reynolds2">[33]</xref>. This theoretical paper aims to provide qualitative as well as quantitative insights into the conditions in which sensorimotor adaptation relying solely on rewards can take place. More specifically, we assume that a local learning rule based on the coactivation of pre and postsynaptic neurons is gated by a binary reward signal is the neural basis for modifications of synaptic efficacies <xref ref-type="bibr" rid="pcbi.1003377-Loewenstein1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Legenstein1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Frmaux1">[35]</xref>.</p>
<p>We focus here on adaptation to a rotation during reaching movements where subjects are asked to move a cursor on a screen to bring it within a circular target while the cursor trajectory is rotated (perturbed) by some angle with respect to the hand trajectory. These perturbation tasks are classically used in behavioral studies of sensorimotor adaptation <xref ref-type="bibr" rid="pcbi.1003377-Krakauer1">[3]</xref>. We consider a simplified network model of this task where adaptation relies solely on binary rewards <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. The simplicity of the model allows us to analytically study several aspects of the adaptation dynamics. Combining these results with numerical simulations enables us to investigate the ways in which the learning dynamics depend on the model parameters. The key question is how the dynamics of adaptation are affected when the task involves multiple targets. Four main findings are reported: interferences can occur when adapting to multiple stimuli, interferences can slow down the adaptation dynamics dramatically, this depends on the (binary, stochastic) reward, and the slow down can be overcome by using shaping strategies.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We consider the classical rotation experiment <xref ref-type="bibr" rid="pcbi.1003377-Krakauer1">[3]</xref> in which a subject has to move a cursor on a screen to bring it within a circular target with a radius of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e001" xlink:type="simple"/></inline-formula>; see <xref ref-type="fig" rid="pcbi-1003377-g001">Figure 1A</xref>. At the beginning of the experiment there is no discrepancy between the movement of the hand and the movement of the cursor. We assume that the subject is able to generate the appropriate hand movement to perform the task correctly. A perturbation is then introduced, so that the cursor trajectory is rotated by an angle <italic>γ</italic> with respect to the hand trajectory. The subject has to adapt his movements to this new condition.</p>
<fig id="pcbi-1003377-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g001</object-id><label>Figure 1</label><caption>
<title>Schematic description of the sensorimotor adaptation task and the model.</title>
<p><bold>A.</bold> The rotation task. From left to right: 1) A circular target (red circle) of radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e002" xlink:type="simple"/></inline-formula> appears on the screen at direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e003" xlink:type="simple"/></inline-formula> (here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e004" xlink:type="simple"/></inline-formula>) to instruct the subject where to move the cursor. 2) The subject moves the cursor, which is invisible to him, toward the target (blue arrow). The only information available to the subject on his performance is the reward, delivered only if the cursor falls within the target. 3) A perturbation is introduced: the cursor is rotated by an angle <italic>γ</italic> with respect to the direction of the subject's hand movement (black arrow). 4) A learning phase follows where the subject progressively adapts to the perturbation, reducing the distance between the cursor endpoint and the target. <bold>B.</bold> Schematic description of the model. When the target appears, the activity profile of the input layer (red neurons) peaks around the target direction. The parameter <italic>ρ</italic> controls the width of the activity profile. The connectivity matrix between the input and the output (blue neurons) layers is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e005" xlink:type="simple"/></inline-formula>. A Gaussian noise with zero mean and a standard deviation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e006" xlink:type="simple"/></inline-formula> is added to the output layer of the network. The two-dimensional output vector rotated by the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e007" xlink:type="simple"/></inline-formula> represents the cursor endpoint. A reward is delivered if the distance between the cursor endpoint and the center of the target is smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e008" xlink:type="simple"/></inline-formula>. The connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e009" xlink:type="simple"/></inline-formula> is then changed according to a reward-modulated plasticity rule (see Eq(8)).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g001" position="float" xlink:type="simple"/></fig>
<p>In the present work, we focus on the case where the subject receives no visual feedback about the trajectory of the cursor. The only information on performance is a reward provided by the experimentalist at the end of a trial, according to the location of the cursor with respect to the desired target.</p>
<p>Our simplified model for a network which generates the reaching movement is depicted in <xref ref-type="fig" rid="pcbi-1003377-g001">Figure 1B</xref>. Its input layer consists of sensory neurons tuned to the location of the target. It has the geometry of a ring: the preferred direction (between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e011" xlink:type="simple"/></inline-formula>) of a neuron corresponds to its location on the ring (see Eq(2)). Hence, when a target appears, the population activity profile in the input layer peaks around a location which is also the target direction. For simplicity we assume that the tuning curves of all the neurons have the same shape. Therefore, the shapes of the population activity profile and the tuning curves are identical. In particular, the tuning width, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e012" xlink:type="simple"/></inline-formula>, is also the width of the activity profile.</p>
<p>The output layer consists of two linear units. Their activity encodes the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e013" xlink:type="simple"/></inline-formula> coordinates of the endpoint of the hand movement in the two dimensional environment. The connectivity matrix implementing the sensorimotor mapping between the input and the output layer is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e014" xlink:type="simple"/></inline-formula>. In addition to their feedfoward inputs from the first layer, the output units also receive a Gaussian noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e015" xlink:type="simple"/></inline-formula> (see Eq(4)), where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e016" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e017" xlink:type="simple"/></inline-formula> of the noise (also referred to hereafter as the <italic>noise level</italic>). The vector representing the endpoint of the cursor is obtained by rotating the output vector of the second layer, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e018" xlink:type="simple"/></inline-formula>, by an angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e019" xlink:type="simple"/></inline-formula> (2×2 rotation matrix- <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e020" xlink:type="simple"/></inline-formula>).</p>
<p>The reward, <italic>R</italic>, delivered at the end of the movement, depends on the distance between the cursor and the target. Unless specified otherwise it is binary: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e021" xlink:type="simple"/></inline-formula> for a successful trial, <italic>i.e.</italic> if the squared distance is smaller than the target size, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e022" xlink:type="simple"/></inline-formula>, otherwise. The target size is controlled by the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e023" xlink:type="simple"/></inline-formula> and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e024" xlink:type="simple"/></inline-formula> is referred to as the target size in the text.</p>
<p>Following trial <italic>t</italic>, the network adapts to the rotation by modifying the connectivity matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e025" xlink:type="simple"/></inline-formula>, according to the reward-gated synaptic plasticity rule <xref ref-type="bibr" rid="pcbi.1003377-Loewenstein1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Williams1">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Werfel1">[38]</xref>:<disp-formula id="pcbi.1003377.e026"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e026" xlink:type="simple"/></disp-formula>where <italic>η</italic> is the learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e027" xlink:type="simple"/></inline-formula> is the noise in the output layer and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e028" xlink:type="simple"/></inline-formula> is the activity of the input layer in response to the presentation of a target in direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e029" xlink:type="simple"/></inline-formula>. We will assume that the initial value of the connectivity matrix is such that without noise, the network performs the task perfectly for all target directions when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e030" xlink:type="simple"/></inline-formula> (See Eq(9)). More details about the model are given in <xref ref-type="sec" rid="s4">Materials and Methods</xref>.</p>
<p>The simplicity of the model allows for analytical calculations in the limit of small targets and a better understanding of the learning dynamics. However, the results reported here are grounded on the assumption of a reward-modulated learning rule and are qualitatively independent of the simplifying assumptions used to construct the model. For instance, as shown in <xref ref-type="supplementary-material" rid="pcbi.1003377.s002">Figure S2</xref>, the results still hold qualitatively in a more complicated network architecture with a different decoding scheme.</p>
<sec id="s2a">
<title>The learning dynamics for one target</title>
<p>We first consider the case where the network has to adapt to a rotation of the cursor when only one target is presented. <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2A</xref> (left) plots the evolution of the error (see <xref ref-type="disp-formula" rid="pcbi.1003377.e426">Eq.(5)</xref>) with the number of trials, hereafter referred to as the learning curve, while the network adapts to an imposed rotation with an angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e031" xlink:type="simple"/></inline-formula>. On the right panel we plotted for the same parameters the learning curve of the directional error, which takes into account only the direction of the movement.</p>
<fig id="pcbi-1003377-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g002</object-id><label>Figure 2</label><caption>
<title>Learning dynamics when the network adapts to the rotation for one target.</title>
<p><bold>A.</bold> An examples of a learning curve for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e032" xlink:type="simple"/></inline-formula>. Left: the error is calculated as the squared distance between the cursor endpoint and the target (see <xref ref-type="disp-formula" rid="pcbi.1003377.e426">Eq. (5)</xref>) and plotted as a function of the trial number. The rotation perturbation is applied on trials following t = 0. For display purposes, only one in four trials is displayed. The solid line represents the error, smoothed with a 100 trials sliding median window. Final error of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e033" xlink:type="simple"/></inline-formula> (mean± SE, computed as explained in <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Dashed purple line: Target size. Right: as in left, but only the directional part of the error is plotted against the trial number. The shaded area corresponds to the target size. <bold>B.</bold> Same as in the left panel of <bold>A.</bold> but with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e034" xlink:type="simple"/></inline-formula> and corresponding final error of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e035" xlink:type="simple"/></inline-formula>. <bold>C.</bold> Same as in the left panel of <bold>A.</bold> but with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e036" xlink:type="simple"/></inline-formula> and a corresponding final error of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e037" xlink:type="simple"/></inline-formula>. <bold>D.</bold> Probability density function (p.d.f.) of the logarithm of the learning duration. The learning duration (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e038" xlink:type="simple"/></inline-formula>) is defined as the number of trials it takes to learn the task (see: <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Target size is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e039" xlink:type="simple"/></inline-formula>. <bold>E.</bold> Trade-off between learning duration and final error. Average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e040" xlink:type="simple"/></inline-formula> distribution (green) and the final error (blue) are plotted against the target size. The shaded area around the averages corresponds to half SD of the distributions. Solid lines: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e041" xlink:type="simple"/></inline-formula>. Dashed lines: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e042" xlink:type="simple"/></inline-formula>. <bold>F.</bold> The probability of getting the first reward, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e043" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003377.e444">Eq. (10)</xref>), <italic>vs.</italic> the noise level, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e044" xlink:type="simple"/></inline-formula> for two values of the target size. In all the panels: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e045" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g002" position="float" xlink:type="simple"/></fig>
<p>The error is large at the beginning of the process and decreases with the number of trials. Importantly, the dynamics strongly depend on the noise. For a low noise level (<xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2A</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e046" xlink:type="simple"/></inline-formula>), the error remains large for many trials and learning is slow. When the noise level is higher (<xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2B</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e047" xlink:type="simple"/></inline-formula>) the error declines faster. However, this comes at the cost of increasing the error after learning: the median of this error, called hereafter the <italic>final error</italic> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>), is larger when the noise level is larger. Similarly, the probability that the network will perform the task successfully, improves more rapidly with the number of trials for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e048" xlink:type="simple"/></inline-formula> than for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e049" xlink:type="simple"/></inline-formula>, but at very long time it is larger in the latter (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e050" xlink:type="simple"/></inline-formula>) than in the former (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e051" xlink:type="simple"/></inline-formula>) case.</p>
<p>The learning curves plotted in <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2A–B</xref> were obtained for particular realizations of the noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e052" xlink:type="simple"/></inline-formula>. To provide a statistical characterization of these dynamics, we estimated the distributions of the logarithm of the learning duration (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e053" xlink:type="simple"/></inline-formula>) over many realizations of the noise (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). As shown in <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2D</xref>, this distribution shifts toward longer learning duration as the noise level decreases.</p>
<p><xref ref-type="fig" rid="pcbi-1003377-g002">Figures 2A and 2C</xref> plot the learning curves for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e054" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e055" xlink:type="simple"/></inline-formula> for the same noise level. The learning is substantially faster for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e056" xlink:type="simple"/></inline-formula> but the final error is larger in this case. This is because when the target size is large, a reward might also be delivered for less precise movement, <italic>i.e.</italic>, for large errors. <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2E</xref> plots the log learning duration and the final error averaged over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e057" xlink:type="simple"/></inline-formula> realizations <italic>vs.</italic> the target size: when increasing the target size, the learning duration rapidly decreases, whereas the final error increases.</p>
<p>When the noise level or the target size are increased, the dynamics are typically faster because the probability of generating rewarded trials at the beginning of the learning is larger. As this probability increases, the time for the network to generate a rewarded trial decreases, leading to more updates in the connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e058" xlink:type="simple"/></inline-formula>; hence the probability of the following trials to be rewarded increases further. This argument can be made more quantitative if one considers how the time to get the first reward depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e059" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e060" xlink:type="simple"/></inline-formula>. It has a geometrical distribution with a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e061" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003377.e444">Eq.(10)</xref>), which is the probability to get the first reward. Lower values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e062" xlink:type="simple"/></inline-formula> increase the expectation time to the first reward, and thereby the learning duration. When the noise level is low and the initial error is larger than the target size, the network explores a small region of the two dimensional space and the probability of getting a reward is small. In contrast, for very large noise the target is missed most of the time. The probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e063" xlink:type="simple"/></inline-formula> therefore varies non-monotonically with the noise level (<xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2F</xref>). The dependency on target size is simpler: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e064" xlink:type="simple"/></inline-formula> increases monotonically with target size, as it is more likely to reach a larger target.</p>
<sec id="s2a1">
<title>Performance depends on the learning rate parameter</title>
<p>Obviously, the number of trials required to adapt also depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e065" xlink:type="simple"/></inline-formula>, which scales the increment in synaptic strength following a rewarded trial. If the rate is too small, the adaptation will be extremely long, even for large noise or big target size. On the other hand, if this rate is too large learning is likely to be impossible.</p>
<p>To analyze how <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e066" xlink:type="simple"/></inline-formula> affects the learning of the task it is convenient to decompose the error at trial <italic>t</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e067" xlink:type="simple"/></inline-formula>, (<xref ref-type="disp-formula" rid="pcbi.1003377.e426">Eq.(5)</xref>) into:<disp-formula id="pcbi.1003377.e068"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e068" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e069" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003377.e428">Eq.(6)</xref>) on trial <italic>t</italic> does not depend on the noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e070" xlink:type="simple"/></inline-formula> (for more details, see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). We therefore refer to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e071" xlink:type="simple"/></inline-formula> as the <italic>noiseless error</italic>. Changes in the noiseless error are due to updates in the connectivity matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e072" xlink:type="simple"/></inline-formula>, and only occur after rewarded trials. In particular, the noiseless error rarely changes at the beginning of learning, when the probability of getting a reward is low (<xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3A</xref>). The two other terms depend on the noise at trial <italic>t</italic>.</p>
<fig id="pcbi-1003377-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g003</object-id><label>Figure 3</label><caption>
<title>Performance and noiseless performance after learning depends on the learning rate.</title>
<p><bold>A.</bold> An example of the variations of the error (blue) and the noiseless error (red) with the number of trials for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e073" xlink:type="simple"/></inline-formula> (purple dashed line), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e074" xlink:type="simple"/></inline-formula> and a normalized learning rate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e075" xlink:type="simple"/></inline-formula>, see <xref ref-type="disp-formula" rid="pcbi.1003377.e459">Eq. (12)</xref>) of 0.3. For display purposes, only one in four trials is displayed. <bold>B.</bold> The performance (blue), <italic>i.e.</italic>, the probability that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e076" xlink:type="simple"/></inline-formula> and the noiseless performance (red), <italic>i.e.</italic>, the probability that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e077" xlink:type="simple"/></inline-formula> are plotted against the normalized learning rate. These quantities were estimated from simulations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e078" xlink:type="simple"/></inline-formula> trials, while excluding the transient learning phase. Note that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e079" xlink:type="simple"/></inline-formula> the noiseless performance is perfect. The standard error of the mean is too small to notice. <bold>C.</bold> Distribution of the noiseless error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e080" xlink:type="simple"/></inline-formula>, at the end of the learning phase. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e081" xlink:type="simple"/></inline-formula>, the support of the distribution is bounded by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e082" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e083" xlink:type="simple"/></inline-formula>, the distribution is uniform for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e084" xlink:type="simple"/></inline-formula> and zero otherwise. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e085" xlink:type="simple"/></inline-formula> the support of the distribution is bounded but extends beyond <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e086" xlink:type="simple"/></inline-formula>. In <bold>B</bold> and <bold>C</bold>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e087" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e088" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g003" position="float" xlink:type="simple"/></fig>
<p>We also define the <italic>noiseless performance</italic> after learning as the probability that the noiseless error will be smaller than the target size at large time. In <xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3A</xref>, the noiseless performance corresponds to the number of trials (red circles) that fall below target size, divided by the number of trials (see also <xref ref-type="sec" rid="s4">Materials and Methods</xref>), when the number of trials is large.</p>
<p><xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3B</xref> plots the performance (blue) and the noiseless performance (red) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e089" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e090" xlink:type="simple"/></inline-formula> <italic>vs.</italic> the <italic>normalized</italic> learning rate, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e091" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e092" xlink:type="simple"/></inline-formula> is a constant; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). The noiseless performance is perfect for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e093" xlink:type="simple"/></inline-formula>. It quickly deteriorates when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e094" xlink:type="simple"/></inline-formula> increases beyond <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e095" xlink:type="simple"/></inline-formula>, until it becomes extremely small around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e096" xlink:type="simple"/></inline-formula>. Performance decreases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e097" xlink:type="simple"/></inline-formula> until it reaches 0 around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e098" xlink:type="simple"/></inline-formula>. Similar qualitatively results were obtained for other values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e099" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e100" xlink:type="simple"/></inline-formula> (results not shown).</p>
<p>To better understand how the noiseless performance changes with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e101" xlink:type="simple"/></inline-formula>, we solved the learning dynamics in the limit of small target size (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e102" xlink:type="simple"/></inline-formula>) analytically. In this limit, the time between rewarded trials diverges. Using the fact that when a trial <italic>t</italic> is rewarded, the noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e103" xlink:type="simple"/></inline-formula>, is uniquely determined in this limit, we computed the trajectory of the noiseless error analytically as a function of the number of rewarded trials; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>. In particular, the noiseless error goes to zero for a large number of trials if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e104" xlink:type="simple"/></inline-formula> is smaller than 2 and diverges for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e105" xlink:type="simple"/></inline-formula> larger than 2.</p>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e106" xlink:type="simple"/></inline-formula> the noiseless error continues to fluctuate with time (as in <xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3A</xref>) in the range (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e107" xlink:type="simple"/></inline-formula>), where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e108" xlink:type="simple"/></inline-formula> depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e109" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e110" xlink:type="simple"/></inline-formula>. This maximal value can be calculated analytically as shown in <xref ref-type="sec" rid="s4">Materials and Methods</xref>:<disp-formula id="pcbi.1003377.e111"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e111" xlink:type="simple"/></disp-formula>The dependency of noiseless performance with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e112" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3B</xref>) stems from this result. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e113" xlink:type="simple"/></inline-formula> the noiseless error is always smaller than the target size (see example in <xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3C</xref>). Therefore the noiseless performance is always 1. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e114" xlink:type="simple"/></inline-formula> the distribution of the noiseless error, can be calculated analytically (the proof is beyond the scope of this paper). It is uniform in the range (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e115" xlink:type="simple"/></inline-formula>) (blue line in <xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3C</xref>). For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e116" xlink:type="simple"/></inline-formula> the noiseless error can be larger than the target size (see example in <xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3C</xref>) and noiseless performance is no longer perfect. In fact as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e117" xlink:type="simple"/></inline-formula> increases, the distribution becomes wider (its SD increases) and noiseless performance decreases monotonously. Finally, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e118" xlink:type="simple"/></inline-formula> the above equation predicts that the support of the noiseless error distribution is unbounded, and simulations show that it becomes wider; hence the probability of getting a reward is substantially smaller than for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e119" xlink:type="simple"/></inline-formula>.</p>
<p>While noiseless performance is always perfect for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e120" xlink:type="simple"/></inline-formula>, performance can be improved by taking smaller values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e121" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3B</xref>). This is because the distribution of the noiseless error is sharper when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e122" xlink:type="simple"/></inline-formula> is smaller. However, decreasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e123" xlink:type="simple"/></inline-formula> has the obvious consequence of increasing the learning duration. <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2</xref> shows that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e124" xlink:type="simple"/></inline-formula> it takes only a few dozen trials to adapt perfectly if the target size is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e125" xlink:type="simple"/></inline-formula>. Nevertheless, for smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e126" xlink:type="simple"/></inline-formula> the number of trials increases dramatically. For instance, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e127" xlink:type="simple"/></inline-formula> this number becomes extremely large (much larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e128" xlink:type="simple"/></inline-formula>) even if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e129" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2a2">
<title>Accelerating the adaptation by shaping the task or the reward</title>
<p>Shaping is a well-known strategy in the context of operant conditioning, which allows a subject to learn difficult tasks in a reasonable amount of time <xref ref-type="bibr" rid="pcbi.1003377-Skinner1">[22]</xref>. In shaping strategies, the difficulty of the task is progressively increased. For a given degree of difficulty, the subject has to learn to perform the task, his performance is monitored, and when it is considered sufficiently satisfactory by the experimentalist, the difficulty of the task is increased. A shaping strategy has recently been successfully applied to allow subjects to learn the sensorimotor rotation task relying solely on a reward signal in the absence of visual feedback <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. In this section, we apply shaping strategies in our model to examine to what extent learning can be facilitated or accelerated.</p>
<p>In the specific case of our sensorimotor adaptation task, the difficulty of the task depends on the target size, the rotation angle and the noise level. For fixed noise level and rotation angle, learning can be shaped by initiating the adaptation process with a large target size and then reducing the size progressively until it becomes as small as desired. This can be implemented as follows. The learning process begins with an initial value of the target size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e130" xlink:type="simple"/></inline-formula>, which is large enough for adaptation to be easy and fast. The target size is kept constant, while monitoring the running average of the reward. When the latter approaches a steady state, the target size is decreased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e131" xlink:type="simple"/></inline-formula> (and the running average of the reward is reinitialized to zero). We repeat this step until the target size reaches the desired value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e132" xlink:type="simple"/></inline-formula>. An example of such a shaping strategy is depicted in <xref ref-type="fig" rid="pcbi-1003377-g004">Figure 4A</xref>. Here we plot the learning curve for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e133" xlink:type="simple"/></inline-formula>, when the adaptation is performed in the presence of very small noise (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e134" xlink:type="simple"/></inline-formula>), starting with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e135" xlink:type="simple"/></inline-formula>. Within fewer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e136" xlink:type="simple"/></inline-formula> trials the network has adapted and reached a performance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e137" xlink:type="simple"/></inline-formula>. In fact, if the adaptation had been performed with fixed value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e138" xlink:type="simple"/></inline-formula>, the probability of getting the first reward in fewer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e139" xlink:type="simple"/></inline-formula> trials would essentially be zero (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e140" xlink:type="simple"/></inline-formula>), making the network unable to adapt without a tremendous number of trials.</p>
<fig id="pcbi-1003377-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g004</object-id><label>Figure 4</label><caption>
<title>Shaping the task allows the network to adapt to a large rotation angle (here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e141" xlink:type="simple"/></inline-formula>) even if the target size and the noise level are extremely small.</title>
<p><bold>A.</bold> Shaping by decreasing the target size, as explained in the text. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e142" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e143" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e144" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e145" xlink:type="simple"/></inline-formula>. Blue: The error is sampled every 3 trials (dots) and smoothed with a 50 trials median sliding window (line) <italic>vs.</italic> the number of trials. Purple: The size of the target. <bold>B.</bold> Reach angle (in degrees) as a function of the trial number when the rotation angle is progressively increased (see <xref ref-type="sec" rid="s2">Results</xref>). The target size is fixed: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e146" xlink:type="simple"/></inline-formula>. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e147" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e148" xlink:type="simple"/></inline-formula>. The rotation angle is increased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e149" xlink:type="simple"/></inline-formula> every 25 trials up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e150" xlink:type="simple"/></inline-formula>. The shaded area corresponds to the target size (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e151" xlink:type="simple"/></inline-formula> around the target center). Inset: the network is unable to follow the gradual rotation for a different realization of the noise with the same parameters. In both panels: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e152" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g004" position="float" xlink:type="simple"/></fig>
<p>Another example of acceleration by shaping is depicted in <xref ref-type="fig" rid="pcbi-1003377-g004">Figure 4B</xref>. Here, as in <xref ref-type="fig" rid="pcbi-1003377-g004">Figure 4A</xref>, the network has to adapt to a rotation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e153" xlink:type="simple"/></inline-formula>. We used similar parameters as in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e154" xlink:type="simple"/></inline-formula>, corresponding to a target with a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e155" xlink:type="simple"/></inline-formula> radius and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e156" xlink:type="simple"/></inline-formula>). Adaptation is performed using a constant target size, but at the beginning of the learning the angle of the rotation is small and is progressively increased with steps of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e157" xlink:type="simple"/></inline-formula> every block of 25 trials. The figure shows that the network adapts in fewer than 200 trials. However, in some of the realizations the network was unable to follow the gradual rotation (see inset). To avoid such cases, one can take smaller rotation steps for longer block of trials, as in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. Another possibility is to monitor the running average reward and to change the rotation angle when the latter approaches a steady state, similarly to what we did with the adaptive target size above.</p>
<p>Binary rewards, as typically used in operant conditioning, provide the subject with a limited amount of information about his performance. For instance, in our model, a binary reward does not convey any information regarding the exact distance between the cursor and the center of the target in case of a miss nor in the case of a success. One way to accelerate adaptation is to shape the reward, <italic>i.e.</italic>, to perform the learning using a reward that depends smoothly on the error. One possibility is to use a deterministic reward given by<disp-formula id="pcbi.1003377.e158"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e158" xlink:type="simple"/><label>(1)</label></disp-formula>where <italic>T</italic> is a <italic>smoothing parameter</italic>. <xref ref-type="fig" rid="pcbi-1003377-g005">Figure 5A</xref> plots the learning curves for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e159" xlink:type="simple"/></inline-formula> (top panel) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e160" xlink:type="simple"/></inline-formula> (bottom panel), for fixed values of target size and noise level (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e161" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e162" xlink:type="simple"/></inline-formula>). The network improves substantially faster in the latter case than in the former. However, after the error has stabilized, it is comparable in both cases. <xref ref-type="fig" rid="pcbi-1003377-g005">Figure 5B</xref> plots the average logarithm of the learning duration as a function of <italic>T</italic>. It shows that the learning duration increases rapidly for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e163" xlink:type="simple"/></inline-formula>, the limit where the reward becomes binary. Note that the learning duration varies non-monotonically with <italic>T</italic> (it is minimum at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e164" xlink:type="simple"/></inline-formula>). This is because the learning duration also increases for large <italic>T</italic> since a reward which is overly smoothed is less informative.</p>
<fig id="pcbi-1003377-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g005</object-id><label>Figure 5</label><caption>
<title>Shaping the reward function accelerates adaptation without impairing performance.</title>
<p><bold>A.</bold> The reward is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e165" xlink:type="simple"/></inline-formula>. Top: learning curve for a reward function that changes abruptly around target size (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e166" xlink:type="simple"/></inline-formula>). Bottom, main panel: learning curve for a gradual reward function (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e167" xlink:type="simple"/></inline-formula>). Note the change in the abscissa scale. Inset: The reward function <italic>vs.</italic> the error. The target size is dashed purple line. <bold>B.</bold> The learning duration and the performance <italic>vs.</italic> the smoothing parameter, <italic>T</italic>. Solid lines: Deterministic smooth reward function as in <bold>A</bold>. Dashed lines: Stochastic binary reward delivered with a probability that depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e168" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s2">Results</xref>). In <bold>A</bold> and <bold>B</bold>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e169" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e170" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g005" position="float" xlink:type="simple"/></fig>
<p>Remarkably, performance remains very close to 0.8 up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e171" xlink:type="simple"/></inline-formula>. Therefore, using a smooth reward with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e172" xlink:type="simple"/></inline-formula> reduces the learning duration substantially without affecting the performance of the network. For <italic>T</italic> above 0.05 performance drops rapidly and the learning duration becomes larger. Hence, in this case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e173" xlink:type="simple"/></inline-formula> is optimal. We found similar behavior for other values of noise level and target size (not shown).</p>
<p>Another way to provide more information to the subject on his performance, still relying on a binary reward as traditionally used in operant conditioning, is to deliver it stochastically with a probability decreasing smoothly with the error. This can also accelerate adaptation as shown in <xref ref-type="fig" rid="pcbi-1003377-g005">Figure 5B</xref> (dashed lines). Here the reward is a Bernoulli random variable with a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e174" xlink:type="simple"/></inline-formula>.</p>
<p>Altogether, our modeling study predicts that reward shaping strategies, <italic>e.g.</italic>, providing a reward that is a smooth function of the error, as well as other shaping strategies, should be efficient in enabling or accelerating such reward-driven sensorimotor adaptation.</p>
</sec><sec id="s2a3">
<title>Generalization error</title>
<p>How does the network generalize the rotation for movement toward targets that were not presented during the adaptation process? To investigate this question we computed the generalization error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e175" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) as a function of the angular distance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e176" xlink:type="simple"/></inline-formula>, between the target to which the network had adapted and a test target to which it did not adapt. For small target size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e177" xlink:type="simple"/></inline-formula> can be calculated analytically (<xref ref-type="disp-formula" rid="pcbi.1003377.e526">Eq. (20)</xref>). <xref ref-type="fig" rid="pcbi-1003377-g006">Figure 6A</xref> plots the results for different widths of the tuning curves, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e178" xlink:type="simple"/></inline-formula>. For narrow tuning curves (dashed-dotted line), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e179" xlink:type="simple"/></inline-formula> is almost one (<italic>i.e.</italic>, perfect generalization) only when the learned and the test targets are very close. When they are far apart, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e180" xlink:type="simple"/></inline-formula> is almost zero. This is because the ability to generalize depends on the overlap, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e181" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003377.e532">Eq. (21)</xref>), between the activity profiles in the input layer of the network upon presentation of the learned and test targets. When the tuning curves are narrow, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e182" xlink:type="simple"/></inline-formula> is substantially different from zero only for very close targets and when they are far it is essentially zero. The range in the angular distance in which the generalization error is positive becomes broader when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e183" xlink:type="simple"/></inline-formula> increases (solid line). However, for wide tuning curves, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e184" xlink:type="simple"/></inline-formula> is negative when the targets are far apart. This means that the network performance on far targets deteriorates compared to what it was before adaptation. Note that for intermediate values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e185" xlink:type="simple"/></inline-formula> the generalization error can vary non-monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e186" xlink:type="simple"/></inline-formula> (dashed lines).</p>
<fig id="pcbi-1003377-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g006</object-id><label>Figure 6</label><caption>
<title>The generalization error (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e187" xlink:type="simple"/></inline-formula>) for a new target (defined as the test target), presented after the network has adapted to one target.</title>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e188" xlink:type="simple"/></inline-formula> is plotted as a function of the angle of the test target after adaptation to a target in direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e189" xlink:type="simple"/></inline-formula>. Perfect generalization is when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e190" xlink:type="simple"/></inline-formula>. Lines: Analytical result for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e191" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003377.e524">Eq.(19)</xref>). Circles: Simulation results for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e192" xlink:type="simple"/></inline-formula>. For clarity, the results are displayed for test targets sampled every 15 degrees. The generalization error was averaged over 200 realizations of the noise. Shaded area represents one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e193" xlink:type="simple"/></inline-formula> around the averages. Gray line: zero <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e194" xlink:type="simple"/></inline-formula>. The mapping between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e195" xlink:type="simple"/></inline-formula> and the half-bandwidth, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e196" xlink:type="simple"/></inline-formula>, is given in <xref ref-type="disp-formula" rid="pcbi.1003377.e412">Eq. (3)</xref>. For instance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e197" xlink:type="simple"/></inline-formula> corresponds to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e198" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e199" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e200" xlink:type="simple"/></inline-formula>. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e201" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e202" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g006" position="float" xlink:type="simple"/></fig>
<p>The generalization error described here reveals possible interactions between the learning processes for two distinct targets, since adapting for a rotation in one target modifies performance toward others. In what follows, we evaluate the impact of such interactions when adapting the reaching movements to two targets simultaneously and dissect the mechanisms underlying <italic>on-line</italic> positive and negative interactions.</p>
</sec></sec><sec id="s2b">
<title>The learning dynamics for two targets</title>
<p>What is the learning dynamics when the subject has to perform the task for two targets ? How does learning the task for one of the targets affect learning the other one? We addressed these questions in numerical simulations, in which two targets were presented at an angular distance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e203" xlink:type="simple"/></inline-formula>, at consecutive times. Similar results were obtained when the targets were presented in a random order with equal probability.</p>
<sec id="s2b1">
<title>Delayed learning</title>
<p>The top panel of <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7A</xref> plots an example of the learning curves when the two targets are presented in opposite directions and the noise level is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e204" xlink:type="simple"/></inline-formula>. Note that since this noise level and the target size are the same as in the bottom panel of <xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2A</xref>, one might expect that learning the task would be fast. Remarkably, this is not the case here. The error for one of the targets decreases in fewer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e205" xlink:type="simple"/></inline-formula> trials, beyond which it keeps fluctuating, most of the time below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e206" xlink:type="simple"/></inline-formula>. The corresponding performance (see <xref ref-type="disp-formula" rid="pcbi.1003377.e577">Eq. (26)</xref>) is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e207" xlink:type="simple"/></inline-formula>. This is in contrast to what happens for the other target, for which the error increases rapidly and keeps fluctuating for the whole duration of the simulation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e208" xlink:type="simple"/></inline-formula> trials) around a mean that is much larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e209" xlink:type="simple"/></inline-formula>. Therefore, in this example, the network is able to adapt in a reasonable amount of time to only one of the targets, in spite of the symmetry of the task with respect to target identity.</p>
<fig id="pcbi-1003377-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g007</object-id><label>Figure 7</label><caption>
<title>Delayed learning for two targets in opposite directions.</title>
<p><bold>A.</bold> Learning curves plotted against the number of trials for each of the targets, sampled every 10 trials. For the target that is learned first (resp. second) the curve is plotted in blue (resp. green). Top: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e210" xlink:type="simple"/></inline-formula>. Middle: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e211" xlink:type="simple"/></inline-formula>. Bottom panel: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e212" xlink:type="simple"/></inline-formula>. <bold>B.</bold> Distribution of learning duration for two opposite targets for different noise levels. Solid lines: The probability density functions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e213" xlink:type="simple"/></inline-formula> (blue) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e214" xlink:type="simple"/></inline-formula> (green) for the two targets (solid lines) where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e215" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e216" xlink:type="simple"/></inline-formula>) is the learning duration for the target that is learned first (resp. second). Dashed lines: Distributions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e217" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e218" xlink:type="simple"/></inline-formula> assuming that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e219" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e220" xlink:type="simple"/></inline-formula> are independent random variables. The distributions were estimated over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e221" xlink:type="simple"/></inline-formula> realizations of the noise. Simulations were long enough for the network to eventually adapt to both targets. Top: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e222" xlink:type="simple"/></inline-formula>. Bottom: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e223" xlink:type="simple"/></inline-formula>. <bold>C.</bold> The average and the SD of the distributions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e224" xlink:type="simple"/></inline-formula>(blue) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e225" xlink:type="simple"/></inline-formula> (green) <italic>vs.</italic> the noise level. <bold>D.</bold> The distribution of the ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e226" xlink:type="simple"/></inline-formula> for the two noise level values in <bold>B</bold>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g007" position="float" xlink:type="simple"/></fig>
<p>Increasing the noise has a dramatic effect, as shown in <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7A</xref>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e227" xlink:type="simple"/></inline-formula> (middle panel), the network is able to learn the task for both targets within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e228" xlink:type="simple"/></inline-formula> trials, but learning the second target is delayed. We term this effect throughout this paper: <italic>delayed learning</italic>. When increasing the noise level further (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e229" xlink:type="simple"/></inline-formula>), the network adapts almost simultaneously to the two targets (bottom panel).</p>
<p>This effect of the noise in suppressing delayed learning is confirmed in <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7B</xref>, where the statistics of the logarithm of the learning durations over many realizations of the noise are depicted. The learning duration for the first (resp. the second) learned target is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e230" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e231" xlink:type="simple"/></inline-formula>). Obviously, the target for which adaptation occurs first depends on the specific realization of the noise. The distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e232" xlink:type="simple"/></inline-formula> (green) is shifted to the right with respect to the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e233" xlink:type="simple"/></inline-formula> (blue), as for each realization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e234" xlink:type="simple"/></inline-formula>, by definition. As a consequence of delayed learning, this shift is larger than would be expected if the task had been learned independently for the two targets (dashed lines). For low noise level this shift is even larger (top panel). <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7C</xref> shows the averages of the distributions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e235" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e236" xlink:type="simple"/></inline-formula> <italic>vs.</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e237" xlink:type="simple"/></inline-formula>. As it was the case for the average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e238" xlink:type="simple"/></inline-formula> for a single target (<xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2C</xref>), these averages increase for low noise levels. However, the increase is faster for the second target.</p>
<p>The delayed learning effect is also clear in <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7D</xref> which plots the distribution of the ratio: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e239" xlink:type="simple"/></inline-formula>, for the same values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e240" xlink:type="simple"/></inline-formula> as in <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7B</xref>. For the highest noise level, in half of the realizations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e241" xlink:type="simple"/></inline-formula>. By contrast, for low noise level in more than half of the realizations the learning of the second target is at least 34 times longer than the first one. Overall, delayed learning is reduced when the noise level is increased.</p>
</sec><sec id="s2b2">
<title>Destructive and constructive interference</title>
<p>This delayed learning can be understood with a geometrical argument, as explained in <xref ref-type="fig" rid="pcbi-1003377-g008">Figure 8</xref>. When the network generates a rewarded trial for one of the targets, it affects the outcome of the second target. Hence, when the targets are in opposite directions, and if the tuning curves are sufficiently broad, this results in an increase in the error of the second target (see also <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7A</xref>). In other words, the learning processes for the two targets interfere <italic>destructively</italic>. As a result, the probability of generating a rewarded trial for the second target is reduced. Note that according to this argument if the targets are sufficiently close, the interference becomes <italic>constructive</italic>.</p>
<fig id="pcbi-1003377-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g008</object-id><label>Figure 8</label><caption>
<title>Geometric intuition for the destructive and constructive interferences.</title>
<p>Following the perturbation, the cursor is rotated with respect to the output of the network, hence inducing a large noiseless error (black vector in panel 1.). The noise in the output layer (green vector in panel 2) helps the network to explore the 2D environment, until the cursor falls inside one of the targets (panel 2). This trial is rewarded and therefore the connectivity matrix is updated, affecting the output of the network for the next trials. This decreases the noiseless error, for the target for which the trial has been rewarded, as the rotated output of the network is now closer to it (by adding the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e242" xlink:type="simple"/></inline-formula>, panel 3). This update moves the rotated output <italic>away</italic> from the target in the opposite direction since the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e243" xlink:type="simple"/></inline-formula> is away from it. This results in an increase in the error, referred to as <italic>destructive interference</italic>. The probability of a rewarded trial for this target is now substantially reduced, delaying learning for that target. A similar effect occurs when the two targets are sufficiently far apart. However, when they are close (panel 5) the interference becomes <italic>constructive</italic>, since after the update of the matrix, the rotated output gets closer to both targets. Note that the overlap, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e244" xlink:type="simple"/></inline-formula>, depends on the width of the tuning curves (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g008" position="float" xlink:type="simple"/></fig>
<p>To further analyze the interference in adaptation to the two targets, we considered the correlations between the errors at consecutive presentations of the targets. For that purpose, we estimated the time dependent correlation coefficient (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e245" xlink:type="simple"/></inline-formula>) of the errors over different realizations (see <xref ref-type="sec" rid="s4"><italic>Materials and Methods</italic></xref>). A destructive interference corresponds to negative correlations, whereas a constructive interference corresponds to positive correlations. <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9A</xref> shows how the sign and the time course of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e246" xlink:type="simple"/></inline-formula> change with the angular distance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e247" xlink:type="simple"/></inline-formula>. For the first few trials, usually none of the presentations of the targets are rewarded and, therefore, the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e248" xlink:type="simple"/></inline-formula> does not change. Hence, during the first trials, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e249" xlink:type="simple"/></inline-formula>. For a sufficiently large number of trials the network adapts to the two targets and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e250" xlink:type="simple"/></inline-formula> reaches some stationary value.</p>
<fig id="pcbi-1003377-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g009</object-id><label>Figure 9</label><caption>
<title>Destructive and constructive interferences are a function of the model parameters.</title>
<p>The correlation coefficient, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e251" xlink:type="simple"/></inline-formula>, characterizes the strength and the nature of the interference during learning of the rotation task for two targets. <bold>A.</bold> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e252" xlink:type="simple"/></inline-formula> for different values of the angular distance between the targets. The interference becomes constructive when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e253" xlink:type="simple"/></inline-formula> decreases. <bold>B.</bold> The extremum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e254" xlink:type="simple"/></inline-formula> over <italic>t</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e255" xlink:type="simple"/></inline-formula>, plotted against <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e256" xlink:type="simple"/></inline-formula> for different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e257" xlink:type="simple"/></inline-formula>. Purple: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e258" xlink:type="simple"/></inline-formula>. Blue: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e259" xlink:type="simple"/></inline-formula>. Green: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e260" xlink:type="simple"/></inline-formula>. The width of the curve was chosen to correspond to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e261" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e262" xlink:type="simple"/></inline-formula>, estimated by bootstrap. Note the slight non-monotonicity for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e263" xlink:type="simple"/></inline-formula>. Inset: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e264" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e265" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e266" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e267" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e268" xlink:type="simple"/></inline-formula> (same color code as for the dots on the main figure in this panel). Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e269" xlink:type="simple"/></inline-formula>. <bold>C–F</bold> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e270" xlink:type="simple"/></inline-formula> is plotted for different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e271" xlink:type="simple"/></inline-formula> (<bold>C</bold>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e272" xlink:type="simple"/></inline-formula> (<bold>D</bold>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e273" xlink:type="simple"/></inline-formula> (<bold>E</bold>) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e274" xlink:type="simple"/></inline-formula> (<bold>F</bold>). In all these figures, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e275" xlink:type="simple"/></inline-formula> was calculated over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e276" xlink:type="simple"/></inline-formula> repetitions. The result was low-pass filtered to suppress fast trial-to-trial fluctuations for the sake of clarity. Consequently, there is a causality artifact around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e277" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e278" xlink:type="simple"/></inline-formula>, although it should be. The standard errors estimated by bootstrap are small and are not plotted.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g009" position="float" xlink:type="simple"/></fig>
<p>The results in <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9A</xref> show that the temporal profiles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e279" xlink:type="simple"/></inline-formula> are qualitatively similar for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e280" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e281" xlink:type="simple"/></inline-formula>, but in the latter case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e282" xlink:type="simple"/></inline-formula> is less negative, indicating a reduction in the destructive interference. By decreasing the angle further to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e283" xlink:type="simple"/></inline-formula> the shape of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e284" xlink:type="simple"/></inline-formula> becomes biphasic. In the latter case the nature of the interference changes during adaptation from constructive to destructive. Finally, for sufficiently small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e285" xlink:type="simple"/></inline-formula>, the interference is always constructive. For the parameters in <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9A</xref>, this is already the case when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e286" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9B</xref> plots the extremum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e287" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e288" xlink:type="simple"/></inline-formula>, against <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e289" xlink:type="simple"/></inline-formula>, for different widths of the tuning curves. For broad and sharp tuning curves, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e290" xlink:type="simple"/></inline-formula> varies monotonously with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e291" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9B</xref>, purple and green lines). For intermediate degrees of tuning (blue line), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e292" xlink:type="simple"/></inline-formula> can display non-monotonous variations with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e293" xlink:type="simple"/></inline-formula> (see also the inset in the figure). In fact, it reveals that the interference can vary non-monotonously with the angular distance, depending on the width of the tuning curves. This non-monotonicity can be grasped from the geometric intuition in <xref ref-type="fig" rid="pcbi-1003377-g008">Figure 8</xref>. The interference is more destructive when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e294" xlink:type="simple"/></inline-formula> is large; however, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e295" xlink:type="simple"/></inline-formula> increases, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e296" xlink:type="simple"/></inline-formula> becomes smaller, making the interference less effective. A more rigorous proof is given in Material and Methods.</p>
<p>Similarly, the interference for fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e297" xlink:type="simple"/></inline-formula> depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e298" xlink:type="simple"/></inline-formula> as the overlap, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e299" xlink:type="simple"/></inline-formula>, becomes smaller when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e300" xlink:type="simple"/></inline-formula> decreases. This is depicted in <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9C</xref>, where we plot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e301" xlink:type="simple"/></inline-formula> in the case of two targets in opposite directions, for three values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e302" xlink:type="simple"/></inline-formula>. Decreasing the width of the tuning curves results in smaller values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e303" xlink:type="simple"/></inline-formula>. For very sharp tuning curves, interferences are minimal and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e304" xlink:type="simple"/></inline-formula> remains very small during the whole learning process. In fact, in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e305" xlink:type="simple"/></inline-formula>, the adaptation process to each of the targets is independent.</p>
<p>Finally, <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9D</xref> displays <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e306" xlink:type="simple"/></inline-formula> for three values of noise level. The same qualitative behavior is observed in all these cases; however, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e307" xlink:type="simple"/></inline-formula> is less negative and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e308" xlink:type="simple"/></inline-formula> recovers faster when the noise is stronger. This is because increasing the noise decorrelates the adaptation process for the different targets, thus reducing the destructive interference. This is in line with the results displayed in <xref ref-type="fig" rid="pcbi-1003377-g007">Figure 7</xref>.</p>
</sec><sec id="s2b3">
<title>Destructive interferences are reduced by shaping the task or the reward</title>
<p>Increasing the target size (<xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9E</xref>), as well as reducing the rotation angle (<xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9F</xref>) reduces <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e309" xlink:type="simple"/></inline-formula>, and hence the destructive interference, when adapting for two targets in opposite directions. Therefore, we expect that shaping strategies which gradually manipulate these parameters can help overcome the delayed learning effect. <xref ref-type="fig" rid="pcbi-1003377-g010">Figure 10</xref> shows that this is indeed the case, when changing the target size adaptively during the adaptation. The running average of the reward signal for each target was monitored <italic>separately</italic> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e310" xlink:type="simple"/></inline-formula> was decreased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e311" xlink:type="simple"/></inline-formula> only when both running averages reached a steady state. In this case, the network adapts to both targets quickly and simultaneously. Similarly, shaping the task by increasing the rotation angle progressively reduces the destructive interference and accelerates the learning (data not shown).</p>
<fig id="pcbi-1003377-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g010</object-id><label>Figure 10</label><caption>
<title>Shaping the task or the reward reduces the delayed learning effect.</title>
<p><bold>A.</bold> Learning curves for two targets in opposite directions. The task is shaped by reducing the target size. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e312" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e313" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e314" xlink:type="simple"/></inline-formula>. The running averages of the reward were monitored for the two targets separately. When both averages reached a steady state the target size was decreased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e315" xlink:type="simple"/></inline-formula>. The error was sampled every 3 trials. <bold>B.</bold> Adaptation with a smooth reward function, <xref ref-type="disp-formula" rid="pcbi.1003377.e158">Eq. (1)</xref>. Top: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e316" xlink:type="simple"/></inline-formula>. Middle: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e317" xlink:type="simple"/></inline-formula>. Bottom: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e318" xlink:type="simple"/></inline-formula>. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e319" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e320" xlink:type="simple"/></inline-formula>. The error was sampled every 10 trials.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g010" position="float" xlink:type="simple"/></fig>
<p>Finally, there is less interference if learning is performed with a reward which depends smoothly on the error (<xref ref-type="disp-formula" rid="pcbi.1003377.e158">Eq. (1)</xref>). As depicted in <xref ref-type="fig" rid="pcbi-1003377-g010">Figure 10B</xref>, this results in a suppression in delayed learning. Increasing the smoothing parameter reduces <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e321" xlink:type="simple"/></inline-formula>. For instance, for the parameters of <xref ref-type="fig" rid="pcbi-1003377-g010">Figure 10B</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e322" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e323" xlink:type="simple"/></inline-formula>, whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e324" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e325" xlink:type="simple"/></inline-formula>. Similar results are found if the reward is binary but stochastic, with a probability that is a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e326" xlink:type="simple"/></inline-formula> (not shown).</p>
</sec></sec><sec id="s2c">
<title>Learning faster by learning more</title>
<p>How does the learning duration, <italic>i.e.</italic>, the time to learn the task for all the presented targets, vary with the number of targets? We simulated the learning of <italic>m</italic> targets, whose directions were <italic>evenly distributed</italic> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e327" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e328" xlink:type="simple"/></inline-formula>. We took a small target size (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e329" xlink:type="simple"/></inline-formula>), so that up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e330" xlink:type="simple"/></inline-formula> non-overlapping targets could be considered (for targets presented on a circle with radius 1).</p>
<p><xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11A</xref> plots the average time to learn the entire task in terms of the total number of target presentations for a fixed noise level and different values of tuning widths. It shows a non-monotonic dependency with the number of targets. This contrasts the monotonically increasing learning duration when targets are learned independently with the same noise level and target size (dashed line).</p>
<fig id="pcbi-1003377-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g011</object-id><label>Figure 11</label><caption>
<title>Adaptation to multiple targets.</title>
<p><bold>A.</bold> Average total number of target presentations required to learn the entire task <italic>vs.</italic> the number of presented targets, <italic>m</italic>. The targets are evenly distributed (between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e331" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e332" xlink:type="simple"/></inline-formula>). Black: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e333" xlink:type="simple"/></inline-formula>. Blue: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e334" xlink:type="simple"/></inline-formula>. Purple: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e335" xlink:type="simple"/></inline-formula>. Green: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e336" xlink:type="simple"/></inline-formula>. Dashed black line corresponds to learning the targets independently from the p.d.f. of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e337" xlink:type="simple"/></inline-formula>, which was estimated from adapting to one target. <bold>B.</bold> Examples of the <italic>noiseless</italic> error during the learning, plotted <italic>vs.</italic> the number of <italic>rewarded</italic> trials. The target direction is color coded. Dashed gray lines: The initial noiseless error for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e338" xlink:type="simple"/></inline-formula>. <bold>B.1</bold> and <bold>B.2</bold> are examples of the noiseless error for narrow tuning curves (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e339" xlink:type="simple"/></inline-formula>) in the case of 3 and 6 targets respectively. The plateau in the noiseless errors indicates that there is no interference between the targets. <bold>B.3</bold> and <bold>B.4</bold> are examples of the noiseless error for wider tuning curves (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e340" xlink:type="simple"/></inline-formula>) in the case of 3 and 6 targets respectively. The increase in the noiseless error above the initial error for some of the targets is the result of the destructive interference between far targets. <bold>C.</bold> The fraction of ordered realizations when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e341" xlink:type="simple"/></inline-formula> as function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e342" xlink:type="simple"/></inline-formula>. Chance level is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e343" xlink:type="simple"/></inline-formula>. An ordered realization is defined as learning the targets in a close-to-far order, as in the example in <bold>B.4</bold>. The statistics were calculated over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e344" xlink:type="simple"/></inline-formula> realizations. For all the results presented in this figure: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e345" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g011" position="float" xlink:type="simple"/></fig><sec id="s2c1">
<title>Narrow tuning curves</title>
<p>When the tuning curves are narrow (black and blue curves) and for small values of <italic>m</italic>, the overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e346" xlink:type="simple"/></inline-formula> is essentially zero; therefore, there is no interference and the network adapts independently to the different targets. An example is depicted in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.1</xref> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e347" xlink:type="simple"/></inline-formula>. In this figure, the <italic>noiseless error</italic> for all three targets is plotted against the number of <italic>rewarded</italic> trials. Independence is indicated by the fact that abrupt changes in the noiseless error for one of the targets do not affect the noiseless error for the other targets. The overlap only becomes significant when the targets are close enough, resulting in constructive interference (see also <xref ref-type="fig" rid="pcbi-1003377-g009">Figure 9A</xref>). In fact, when <italic>m</italic> increases, the adaptation for close targets interferes constructively, as depicted in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.2</xref> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e348" xlink:type="simple"/></inline-formula>. In this example, learning target 1 (see color coding in the figure) does not affect the learning of targets 3, 4 and 5 within the first 200 rewarded trials. It does, however, reduce the noiseless error for the closer targets, <italic>i.e.</italic>, 2 and 6. The constructive interference is also noticeable for the rest of the targets. This constructive interference between close targets facilitates adaptation and explains why the learning duration decreases for larger <italic>m</italic>, and the overall non-monotonicity of the learning duration with <italic>m</italic>.</p>
</sec><sec id="s2c2">
<title>Wide tuning curves</title>
<p>For wider tuning curves, interferences are already present for a small number of targets, but they can be <italic>destructive</italic> when the targets are far apart. For instance, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e349" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e350" xlink:type="simple"/></inline-formula>, improvements for one target result in an increased noiseless error, above the initial error, for the other targets (<xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.3</xref>). However, as in this case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e351" xlink:type="simple"/></inline-formula> is not too large, adaptation is almost independent with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e352" xlink:type="simple"/></inline-formula> (green curve in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11A</xref>). Similar to the case of narrow tuning curves, constructive interference between close targets emerges when <italic>m</italic> is increased. A representative example of adaptation with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e353" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e354" xlink:type="simple"/></inline-formula> is plotted in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.4</xref>. Learning target 1 reduces the noiseless error for the two close targets, whereas the error for the other three targets, which are farther apart, becomes larger than their initial values. In this case, constructive interference among the close targets competes with destructive interference between targets that are far apart.</p>
<p>The drop in the learning duration when increasing <italic>m</italic>, both for wide and narrow tuning curves, implies that learning more targets might be faster than learning only a few. For instance, learning 6 targets for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e355" xlink:type="simple"/></inline-formula> is six times faster than learning only three of them (the 3 that are separated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e356" xlink:type="simple"/></inline-formula>).</p>
</sec><sec id="s2c3">
<title>Adaptation is in the close-to-far order when the tuning curves are broad</title>
<p>In <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.4</xref> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e357" xlink:type="simple"/></inline-formula>) the network learned the task in a specific close-to-far order: after it had learned the first target, it learned the two closest targets (separated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e358" xlink:type="simple"/></inline-formula>), and then the far targets (separated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e359" xlink:type="simple"/></inline-formula> and finally the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e360" xlink:type="simple"/></inline-formula> target). Therefore, in this case the targets were learned in an <italic>ordered</italic> way. In contrast, in the example plotted in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11B.2</xref>, the tuning curves are narrow (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e361" xlink:type="simple"/></inline-formula>) and the learning of the targets is not ordered. This difference stems from the fact that broadening the tuning curves increases the amount of both destructive and constructive interference. As a result, by learning one target, the error of the closer targets is already reduced, whereas learning is delayed for the far targets. Increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e362" xlink:type="simple"/></inline-formula> thereby results in more ordered learning. To better characterize how the tuning width controls whether adaptation is ordered or not, we estimated the probability of this occurring as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e363" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11C</xref> depicts the results for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e364" xlink:type="simple"/></inline-formula>. It shows that the fraction of the realizations for which learning is ordered increases monotonically with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e365" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2c4">
<title>Generalization error for multiple targets</title>
<p><xref ref-type="fig" rid="pcbi-1003377-g012">Figure 12A</xref> plots the generalization error after the network has adapted to 2 or 3 targets for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e366" xlink:type="simple"/></inline-formula>. The generalization is essentially one for all tested targets as soon as the network has adapted for three targets (green line). How does the generalization error depend on <italic>m</italic> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e367" xlink:type="simple"/></inline-formula>? <xref ref-type="fig" rid="pcbi-1003377-g012">Figure 12B</xref> plots the noiseless performance (see <xref ref-type="disp-formula" rid="pcbi.1003377.e576">Eq. (25)</xref>) averaged over all the test targets (denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e368" xlink:type="simple"/></inline-formula>), for different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e369" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e370" xlink:type="simple"/></inline-formula>. For wide tuning curves, as in <xref ref-type="fig" rid="pcbi-1003377-g012">Figure 12A</xref>, learning the task with only 3 targets is sufficient for almost perfect performance on all the test targets (blue line, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e371" xlink:type="simple"/></inline-formula>). Therefore, there is no added value in adapting to more targets as far as generalization is concerned. However, as explained above, this can substantially accelerates learning. In fact, for the parameters used in <xref ref-type="fig" rid="pcbi-1003377-g012">Figure 12A</xref> the average learning duration is about 170 times shorter for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e372" xlink:type="simple"/></inline-formula> than for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e373" xlink:type="simple"/></inline-formula>. When the tuning curves are narrower, the network only generalizes perfectly to all directions for large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e374" xlink:type="simple"/></inline-formula> (green and black lines in <xref ref-type="fig" rid="pcbi-1003377-g012">Fig 12B</xref>). Nevertheless, here it is also advantageous for the network to adapt to more targets than required for perfect generalization, since this can accelerate adaptation.</p>
<fig id="pcbi-1003377-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003377.g012</object-id><label>Figure 12</label><caption>
<title>Generalization error (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e375" xlink:type="simple"/></inline-formula>) and performance when adapting to multiple targets.</title>
<p><bold>A.</bold> The generalization error <italic>vs.</italic> the location of the test targets, estimated from simulations as in <xref ref-type="fig" rid="pcbi-1003377-g006">Figure 6</xref>. Shaded area represents one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e376" xlink:type="simple"/></inline-formula> around the averages. Tuning width: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e377" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The noiseless performance (see Eq(25)), averaged over all the tested targets (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e378" xlink:type="simple"/></inline-formula>) is plotted <italic>vs.</italic> the number of trained targets. See <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details about how this quantity was estimated. Blue: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e379" xlink:type="simple"/></inline-formula>. Green: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e380" xlink:type="simple"/></inline-formula>. Black: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e381" xlink:type="simple"/></inline-formula>. Dashed gray: zero <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e382" xlink:type="simple"/></inline-formula> Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e383" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003377.g012" position="float" xlink:type="simple"/></fig></sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We explored the reward-based component in adaptation processes in a setting in which a subject has to adapt reaching movements to a rotation when the only information available is the location of the target and a binary reward signal indicating success or failure on a trial <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. The subject thus has to adapt to the perturbation by relying solely on the reward. In the framework of a simplified model of a neural network learning the task, we investigated the ways in which the adaptation dynamics depend on the noise level in the network, the target size, the size of the perturbation and the shape of the reward function. The key finding is that if the network has to adapt simultaneously to several target locations, constructive or destructive interferences between the different movements may occur. Such destructive interferences may result in a severe slowdown in the adaptation process, but this slowdown can be mitigated if the reward changes more gradually from a success to a failure around the target.</p>
<p>If the motor variability is not large enough with respect to the target size and the amount of perturbation (<xref ref-type="fig" rid="pcbi-1003377-g002">Figure 2</xref>), it takes a long time for the network to generate rewarded trials and to update its connectivity matrix. This results in slow adaptation and may be the reason why adaptation in the absence of visual feedback is notoriously difficult for subjects when the rotation angle is too large. For example, at the noise level and target size reported in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>, the probability to generate a rewarded trial in less than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e384" xlink:type="simple"/></inline-formula> trials for a rotation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e385" xlink:type="simple"/></inline-formula> is essentially zero.</p>
<p>The time to adapt also depends on the size of the change in synaptic strength on each rewarded trial; i.e., on the learning rate parameter. We showed that perfect adaptation to one target (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e386" xlink:type="simple"/></inline-formula> performance in the absence of noise) is possible only when the (normalized) learning rate is smaller than 1. A high learning rate leads to decreased performance and eventually fully impedes adaptation (<xref ref-type="fig" rid="pcbi-1003377-g003">Figure 3</xref>). Therefore, the extent to which adaptation can be accelerated by choosing a large learning rate is limited.</p>
<p>Adaptation is faster for large noise. On the other hand, if the noise is too large, final performance is impaired. Interestingly, motor areas display high variability at the early stages of learning, which becomes smaller afterward. This has been observed in reaching tasks in monkeys <xref ref-type="bibr" rid="pcbi.1003377-MandelblatCerf1">[39]</xref>, as well as in song acquisition in songbirds <xref ref-type="bibr" rid="pcbi.1003377-lveczky1">[40]</xref>. Our study suggests that this change in noise level during learning can be functionally important to making a compromise between fast adaptation and good performance.</p>
<p>We showed that when adapting to multiple targets, learning the task for one target can impair performance on other targets due to destructive interference. As a result, the probability that the network will generate a rewarded trial for these targets decreases. Therefore, in this case the same noise level that allows exploration of one movement direction is insufficient when adapting to two or more targets, resulting in a delayed learning effect. Interestingly, when the network starts to adapt to the perturbation to the second target, it does not deteriorate the performance of the network on the first target that was already learned. This is because the network keeps generating rewarded trials for the first target and prevents the connectivity matrix from changing in the wrong direction for the first target.</p>
<p>We also showed that there are cases where the interference that occurs when multiple targets are presented is constructive. In fact, the strength and the nature of the interference depend on the similarities in the distance between the targets (the physical stimuli) and in the overlap of the tuning curves (the neural representations of the stimuli). Adding more targets creates constructive interference and therefore can accelerate adaptation.</p>
<sec id="s3a">
<title>Generality of the results</title>
<p>Models of sensorimotor control and learning frequently assume minimizing a squared error function. This is convenient because of analytical or computational simplicity <xref ref-type="bibr" rid="pcbi.1003377-Tanaka1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>. However, it was shown that although these models can be a good approximation they tend to penalize large errors excessively <xref ref-type="bibr" rid="pcbi.1003377-Krding1">[41]</xref>. In contrast, we chose to explore adaptation with a binary reward function, as used in experiments. Our results and predictions stem from the shape of the reward function. Specifically, they do not depend qualitatively on the specific choice of the distance error used, but are based primarily on the fact that the reward function varies sharply with the distance to the target center. The dynamics of the adaptation to more than one target depend on the overlap between the tuning curves of the input neurons. However, the precise shape of the tuning curves is not crucial and the results are unchanged if one replaces the Von Mises function we used here with any other tuning curve function, such as a cosine tuning curve (see e.g. <xref ref-type="disp-formula" rid="pcbi.1003377.e549">Eq. 23</xref>).</p>
<p>As a matter of fact, the results we describe are the outcome of the following: 1) the same system is used to learn the task for several targets, leading to interference which depends on the way in which the targets differ physically as well as in their neuronal representation and 2) learning the task for one target can deteriorate performance on another target such that the information provided by the reward when attempting to learn the task for it becomes small, thereby delaying the learning. These two properties of the learning process are not specific to the simple model we investigated here.</p>
<p>In our model, the latter property stems from the fact that the reward varies sharply with the error. The learning rule we used is part of a general family of gradient-like reinforcement learning rules; i.e., learning rules that on average form a gradient ascent on the reward function <xref ref-type="bibr" rid="pcbi.1003377-Frmaux1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1003377-Fiete2">[37]</xref>. In fact, learning with an on-line Gradient Ascent algorithm with a sigmoidal cost function can result in similar effects (<xref ref-type="supplementary-material" rid="pcbi.1003377.s005">Text S1</xref>; <xref ref-type="supplementary-material" rid="pcbi.1003377.s001">Figure S1</xref>). It might be claimed that plasticity also occurs when no reward is delivered <xref ref-type="bibr" rid="pcbi.1003377-Schultz1">[42]</xref>. Therefore, we also verified that the phenomenology of the model remains qualitatively the same when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e387" xlink:type="simple"/></inline-formula> instead of using a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e388" xlink:type="simple"/></inline-formula> reward function (unpublished data). Note that to avoid a drift of the output vector which occurs when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e389" xlink:type="simple"/></inline-formula>, the synaptic weights must be normalized in this case after each trial. Another extension of our model would be to use a reward prediction error instead of an instantaneous reward; e.g., by subtracting a running average of the reward from the instantaneous reward. Delayed learning also occurs with this type of learning rule (results not shown). In fact, previous works have argued that this modification does not affect most of the qualitative behavior of the algorithm <xref ref-type="bibr" rid="pcbi.1003377-Loewenstein1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Williams1">[36]</xref>. However, it should be noted that in the case of multiple targets, computing the running average of the rewards over all targets is an additional source of interference, as shown recently in <xref ref-type="bibr" rid="pcbi.1003377-Frmaux1">[35]</xref>. To avoid this, the running average of the reward needs to be monitored for each target <italic>separately</italic>.</p>
<p>We focused on the learning dynamics in a feed-forward network of linear neurons with only two layers. We chose this architecture for the sake of simplicity. However, we verified that similar qualitative behaviors in terms of interference and delayed learning occur in a network model in which an intermediate layer consisting of nonlinear neurons was added, and in which a decoder provides the angle of reach movement instead of a vector (<xref ref-type="supplementary-material" rid="pcbi.1003377.s005">Text S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003377.s002">Figure S2</xref> and unpublished data). Note that in the framework of this more complex model, the noise can be unambiguously related to neuronal variability whereas in the simplified two-layer model considered in our paper, the noise is in the decoder.</p>
<p>One limitation of our work is that we did not model the trajectory of the movement and/or the muscle activation patterns needed to produce movements <xref ref-type="bibr" rid="pcbi.1003377-Izawa2">[43]</xref>. However, we expect that delayed learning and interferences also occur in a more detailed model of movement production, such as the one used in Legenstein et al. <xref ref-type="bibr" rid="pcbi.1003377-Legenstein1">[34]</xref>.</p>
</sec><sec id="s3b">
<title>Relation to previous works and predictions</title>
<p>A reward-based component in a sensorimotor task was shown to be involved in adaptation to rotations even when detailed spatial information regarding the error was provided to the subject <xref ref-type="bibr" rid="pcbi.1003377-Shmuelof1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Huang1">[19]</xref>. We investigated the ways in which neural possible mechanisms that reinforce successful actions affect adaptation dynamics. This type of reward-based mechanism was also studied in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>. In this experiment, subjects adapted without visual feedback to a gradually increasing rotation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e390" xlink:type="simple"/></inline-formula> every 40 trials, up to an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e391" xlink:type="simple"/></inline-formula> rotation. Our modeling results are in line with these experiments (<xref ref-type="fig" rid="pcbi-1003377-g004">Figure 4B</xref>). We thus predict that shaping the <italic>reward</italic> also accelerates adaptation.</p>
<p>Besides demonstrating that adaptation relying on rewards is possible by utilizing a gradual rotation paradigm, the Izawa and Shadmehr <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> results suggested that there is no change in the perceived sensory consequences of the motor commands; i.e., there should be no change in a “forward model”. Therefore, in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> adaptation was modeled by an action selection rule. Our model is similar to the latter, as we focused on the reward-based component during adaptation. However, our model differs in that it is value-free, whereas in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> it involved value-based reinforcement learning. Nevertheless, our model can also account for the experimental results reported in <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> for one target (see <xref ref-type="supplementary-material" rid="pcbi.1003377.s005">Text S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003377.s003">Figure S3</xref>). Moreover, it allowed us to investigate the generalization curve and possible interference during adaptation for multiple targets.</p>
<sec id="s3b1">
<title>The learning algorithm</title>
<p>Reward modulated learning rules have been used in previous modeling studies of sensorimotor tasks, such as birdsong acquisition <xref ref-type="bibr" rid="pcbi.1003377-Fiete1">[10]</xref> and motor learning in primates <xref ref-type="bibr" rid="pcbi.1003377-Legenstein1">[34]</xref>. Similar rules have also been implemented in models of decision making <xref ref-type="bibr" rid="pcbi.1003377-Loewenstein1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Frmaux1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Law1">[44]</xref> and association tasks <xref ref-type="bibr" rid="pcbi.1003377-Vladimirskiy1">[45]</xref>. The reward modulated rule we used here is a special case of REINFORCE learning rules. As shown by Williams <xref ref-type="bibr" rid="pcbi.1003377-Williams1">[36]</xref>, REINFORCE learning rules are equivalent on average to a gradient ascent algorithm on the average reward function. In fact, the gradient ascent dynamics with the average reward function (<xref ref-type="disp-formula" rid="pcbi.1003377.e444">Eq.(10)</xref>, averaged over the different movement directions) can be computed analytically. However, for finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e392" xlink:type="simple"/></inline-formula> the actual trajectories can deviate substantially from the gradient ascent trajectory. In particular, delayed learning and the reduction in learning duration with the number of targets occurs for finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e393" xlink:type="simple"/></inline-formula> but these phenomena disappear when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e394" xlink:type="simple"/></inline-formula> (unpublished data).</p>
</sec><sec id="s3b2">
<title>Shaping</title>
<p>Shaping strategies are used to teach subjects to perform operant conditioning tasks in a reasonable amount of time <xref ref-type="bibr" rid="pcbi.1003377-Skinner1">[22]</xref>. They were recently applied in the context of Reinforcement Learning by either increasing the complexity of the task <xref ref-type="bibr" rid="pcbi.1003377-Randlv1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Taylor2">[46]</xref> or by shaping the reward function <xref ref-type="bibr" rid="pcbi.1003377-Ng1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Randlv1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Laud1">[47]</xref>. In the context of our model we showed that adaptation to one target can be accelerated if the target size or the rotation angle are progressively changed. This also reduces destructive interferences, thereby accelerating adaptation to multiple targets as well. We also showed that reward shaping can efficiently suppress destructive interferences and accelerates adaptation without compromising on performance.</p>
<p>To the best of our knowledge there are only a few theoretical works that have addressed shaping strategies in computational models in neuroscience (see e.g. <xref ref-type="bibr" rid="pcbi.1003377-Krueger1">[28]</xref>). Fiete et al. <xref ref-type="bibr" rid="pcbi.1003377-Fiete1">[10]</xref> used an adaptive threshold for reinforcement that adapts to performance. This is equivalent to the adaptive target size used here (<xref ref-type="fig" rid="pcbi-1003377-g004">Figure 4A</xref>). Smooth reward functions have been used in previous models of sensorimotor learning <xref ref-type="bibr" rid="pcbi.1003377-Legenstein1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Frmaux1">[35]</xref>, but the ways in which the shape of the reward function affects learning were not addressed.</p>
</sec><sec id="s3b3">
<title>Interference, delayed learning and generalization</title>
<p>The delayed learning effect exhibited by our network when it adapts to several targets is reminiscent of the slowing down that occurs in the model of birdsong learning in Fiete et al. <xref ref-type="bibr" rid="pcbi.1003377-Fiete3">[48]</xref>. In that model, a gradient ascent on a quadratic error function is performed by the network to learn a time dependent signal. The slowing down is due to destructive interferences in learning different temporal chunks of this signal. In fact, the presentation of multiple targets that involved a target in each trial, can be considered a discrete time dependent signal, and interferences when learning multiple targets can thus be seen as similar to interferences in different temporal chunks of the signal. However, in contrast to Fiete et al. <xref ref-type="bibr" rid="pcbi.1003377-Fiete3">[48]</xref>, our network learns with a stochastic online learning rule, rather than a deterministic batch rule, and a different reward function is utilized.</p>
<p>Fiete et al. <xref ref-type="bibr" rid="pcbi.1003377-Fiete3">[48]</xref> suggested that to avoid interferences the avian brain exploits sparse neural representations. This solution is qualitatively similar to narrowing the tuning curves in our model. Similarly, Tanaka et al. <xref ref-type="bibr" rid="pcbi.1003377-Tanaka1">[13]</xref> showed that narrow tuning curves can explain the independent learning of multiple targets in the context of a visuomotor rotation task with visual feedback. However, narrowing the tuning curves is not the only way to suppress destructive interferences, in that we showed here that they can also be suppressed by increasing the noise level, increasing the number of targets, and shaping the task or the reward.</p>
<p>Similarly to previous theoretical works on sensorimotor adaptation, we also showed that the shape of the generalization curve depends on the width of the tuning curves of the input neurons <xref ref-type="bibr" rid="pcbi.1003377-Thoroughman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Tanaka1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Donchin1">[49]</xref>. In <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref> it was shown that generalization in a reward-based rotation task falls to half of its maximum value already at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e395" xlink:type="simple"/></inline-formula> apart from the adapted direction. However, generalization above <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e396" xlink:type="simple"/></inline-formula> was not explored in this study. We therefore did not limit our model to a specific tuning width, as further experiments should be conducted to determine the generalization in the case of adaptation with rewards.</p>
<p>Negative generalization have been experimentally observed, both in adaptation to reaching movements under force-fields <xref ref-type="bibr" rid="pcbi.1003377-Thoroughman1">[4]</xref> and in visuomotor rotations with visual feedback <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>. In the latter study, the authors demonstrated that generalization curves are task-dependent, and showed how subjects negatively generalize the adaptation when targets that are far from the adapted target are presented. In fact, this study showed that generalization curves can even be non-monotonic. We predict here that this can also occur in the case of adaptation without sensory feedback.</p>
<p>As far as we can ascertain,, delayed learning in sensorimotor adaptation has not been reported before. For delayed learning to occur in our model, adaptation to one target needs to impair the performance on other targets and the reward must change abruptly around the target from a success to a failure. Under the assumptions we made, the shape of generalization curves can hint at on-line interferences that can be expected during adaptation. Therefore, because negative generalization was reported in a visuomotor adaptation task when the subject receives a continuous error <xref ref-type="bibr" rid="pcbi.1003377-Taylor1">[14]</xref>, one might expect to find on-line interferences as well when visual feedback is available. However, in this case the error function does not change abruptly with respect to the distance to the target, as subjects are aware of the cursor location. Hence, when subjects receive visual feedback, we do not expect that interferences will result in substantial delayed learning or that learning will accelerate when the number of targets is large. We verified this expectation in the case of a quadratic error <xref ref-type="bibr" rid="pcbi.1003377-Tanaka1">[13]</xref>. In particular, the learning duration increases monotonically with the number of targets and saturates when this number is large (<xref ref-type="supplementary-material" rid="pcbi.1003377.s005">Text S1</xref>; <xref ref-type="supplementary-material" rid="pcbi.1003377.s004">Figure S4</xref>).</p>
<p>On the other hand, in the case of adaptation with binary rewards, we do expect that if there are angles for which generalization is negative, delayed learning will be noticeable, as the reward function changes abruptly from a success to a failure (<xref ref-type="fig" rid="pcbi-1003377-g010">Figure 10</xref>).</p>
</sec></sec><sec id="s3c">
<title>Conclusions and perspectives</title>
<p>The key finding of this theoretical work is that if a reward-modulated learning rule underlies adaptation, interferences are likely to be observed when learning multiple targets with a binary reward. It would be valuable to explore whether such effects occur in reward-based sensorimotor adaptation experiments with multiple sensory stimuli. We predict that for a binary reward function, destructive interferences will be observed if the neurons that encode the stimuli have broad tuning curves. These interferences are a dynamical counterpart of the generalization function and might result in a dramatic slowdown because of the abrupt change in the reward from success to failure around target size. We also predict that adding more targets should accelerate adaptation (<xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11</xref>). From the learning curve of adaptation to one target, the rate and variability in which subjects adapt can be estimated. We predict that at parity of variability, subjects with larger learning rates will tend to display more destructive interferences and therefore slower adaptation to two targets (see <xref ref-type="disp-formula" rid="pcbi.1003377.e549">Eq. (23)</xref>). By contrast,if the tuning curves are very narrow, destructive interferences are unlikely to be found. However, even in this case, when the stimuli are sufficiently close, constructive interferences should be observed. In this case as well, adding more targets should accelerate the adaptation.</p>
<p>Another prediction is that if adaptation is driven by reward modulated plasticity rules similar to the one we used here, smoothing the reward function should reduce interferences. In our model, this stems from the assumption of a reward modulated learning rule and not from the simplifying assumptions we made in constructing the model. Therefore, we suggest that testing this prediction could shed light on the synaptic mechanisms underlying adaptation tasks.</p>
<p>Finally, the location of the reward-based mechanism involved in adaptation could be the cortex-basal-ganglia network. As a matter of fact, there is evidence for the involvement of this network in pitch shift adaptation in songbirds. Although the neural correlates for adaptation in songbirds are unknown, when an auditory feedback is available to songbirds (by using miniature headphones <xref ref-type="bibr" rid="pcbi.1003377-Sober1">[7]</xref>), the anterior frontal pathway, which is the avian homologue of the cortex-basal-ganglia network <xref ref-type="bibr" rid="pcbi.1003377-Gale1">[50]</xref>, is essential for adaptation based solely on binary rewards <xref ref-type="bibr" rid="pcbi.1003377-Tumer1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003377-Andalman1">[16]</xref>. Thus, exploring the behavioral and neural differences in auditory feedback versus binary reward adaptations in pitch shift experiments in songbirds may help reveal the neural mechanisms for reward-based adaptation.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>The task</title>
<p>We consider a motor reaching task (see <xref ref-type="fig" rid="pcbi-1003377-g001">Figure 1A</xref>) in which a subject manually controls the location of a cursor on a screen to bring it within a circular target of radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e397" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003377-Andalman1">[16]</xref>. The target location is characterized by a two dimensional vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e398" xlink:type="simple"/></inline-formula> of norm 1 (we assume that the target is always at distance 1 from the center of the screen) and direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e399" xlink:type="simple"/></inline-formula>. In a standard block of trials, the direction of motion of the cursor and the hand are the same. We assume that the subject is able to perform the task perfectly in this case. In a rotation block of trials a perturbation is introduced: the movement of the cursor on the screen is now rotated by an angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e400" xlink:type="simple"/></inline-formula> with respect to the hand movement. To overcome this perturbation the subject must move his hand in a direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e401" xlink:type="simple"/></inline-formula> with respect to the target. Here we focus on the case where there is no visual feedback (the cursor is not on the screen): the only information the subject receives about his performance is provided by a reward signal delivered by the experimentalist <xref ref-type="bibr" rid="pcbi.1003377-Izawa1">[17]</xref>.</p>
</sec><sec id="s4b">
<title>The network model</title>
<p>We consider a simplified computational model of a network performing this reaching task, see <xref ref-type="fig" rid="pcbi-1003377-g001">Figure 1B</xref>. The input layer of the network encodes the sensory information regarding the direction of the target, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e402" xlink:type="simple"/></inline-formula>. It is composed of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e403" xlink:type="simple"/></inline-formula> directionally tuned neurons labeled by their preferred direction, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e404" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e405" xlink:type="simple"/></inline-formula>. For simplicity, we assume that the shape of the tuning curves is the same for all neurons: upon presentation of a target in direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e406" xlink:type="simple"/></inline-formula> the activity of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e407" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e408" xlink:type="simple"/></inline-formula>. We take:<disp-formula id="pcbi.1003377.e409"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e409" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e410" xlink:type="simple"/></inline-formula> characterizes the width of the tuning curve and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e411" xlink:type="simple"/></inline-formula> is the peak response of a neuron. The width of the tuning curves at half of its maximal activity relative to the baseline (half bandwidth) in this case is:<disp-formula id="pcbi.1003377.e412"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e412" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>The second layer of the network encodes the location of the endpoint of the hand movement. It consists of two output units whose activities, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e413" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e414" xlink:type="simple"/></inline-formula>, represent the two components of the hand position, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e415" xlink:type="simple"/></inline-formula>. Upon presentation of a target in direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e416" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003377.e417"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e417" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e418" xlink:type="simple"/></inline-formula> is the connectivity matrix between the two layers, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e419" xlink:type="simple"/></inline-formula> denotes the <italic>N</italic> dimensional vector of the input layer with components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e420" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e421" xlink:type="simple"/></inline-formula> is a Gaussian noise. The location of the cursor at the end of the movement is related to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e422" xlink:type="simple"/></inline-formula> by a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e423" xlink:type="simple"/></inline-formula> rotation matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e424" xlink:type="simple"/></inline-formula>, of angle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e425" xlink:type="simple"/></inline-formula>. Therefore, the squared distance between the endpoint location of the cursor and the center of the target is:<disp-formula id="pcbi.1003377.e426"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e426" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e427" xlink:type="simple"/></inline-formula>. This quantity will be used to measure the error with which the network performs the reaching task. It is also useful to define the <italic>noiseless</italic> error:<disp-formula id="pcbi.1003377.e428"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e428" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e429" xlink:type="simple"/></inline-formula>. This quantity measures the error if the noise is suppressed.</p>
<p>Upon presentation of a target in a direction <italic>θ</italic> at trial <italic>t</italic>, the network performs the task and a reward <italic>R</italic> is delivered according to the outcome:<disp-formula id="pcbi.1003377.e430"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e430" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>The matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e431" xlink:type="simple"/></inline-formula> is then modified according to a reward-modulated learning rule:<disp-formula id="pcbi.1003377.e432"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e432" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e433" xlink:type="simple"/></inline-formula> is the learning rate. This learning rule can be derived in a REINFORCE framework <xref ref-type="bibr" rid="pcbi.1003377-Williams1">[36]</xref>.</p>
<p>We assume that at the beginning of learning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e434" xlink:type="simple"/></inline-formula>, when there is no rotation, the network is able to perform the reaching task with zero noiseless error for all targets. When all the Fourier components of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e435" xlink:type="simple"/></inline-formula> are non-zero, this constraint fully determines <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e436" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003377.e437"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e437" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e438" xlink:type="simple"/></inline-formula> is the first Fourier component of the tuning curves. To get <xref ref-type="disp-formula" rid="pcbi.1003377.e444">Eq. 10</xref>, one needs to calculate the Fourier expansion of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e439" xlink:type="simple"/></inline-formula> by using the constraint:<disp-formula id="pcbi.1003377.e440"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e440" xlink:type="simple"/></disp-formula>for each of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e441" xlink:type="simple"/></inline-formula> possible target directions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e442" xlink:type="simple"/></inline-formula>. When some of the Fourier coefficients of the tuning curve function are zero, e.g. when using a cosine tuning curves, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e443" xlink:type="simple"/></inline-formula> is determined up to the Fourier coefficient that are irrelevant to the above constraint. This does not affect the learning dynamics.</p>
</sec><sec id="s4c">
<title>Analysis of the model for adaptation to one target</title>
<sec id="s4c1">
<title>Probability to generate a rewarded trial</title>
<p>The probability of generating a rewarded trial given the noiseless error at the end of the previous trial is:<disp-formula id="pcbi.1003377.e444"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e444" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e445" xlink:type="simple"/></inline-formula> is the modified Bessel function of the first kind of order <italic>n</italic> <xref ref-type="bibr" rid="pcbi.1003377-Gradshten1">[51]</xref>. The transition from the second to the third equation is done by a change of variables to polar coordinates, followed by the integration over the angle. Using this equation, we can calculate the probability to get the first reward in a given number of trials for an initial noiseless error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e446" xlink:type="simple"/></inline-formula>. This probability is given by a geometrical distribution with a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e447" xlink:type="simple"/></inline-formula> (defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e448" xlink:type="simple"/></inline-formula> for simplicity). When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e449" xlink:type="simple"/></inline-formula>, the expectation value of this distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e450" xlink:type="simple"/></inline-formula>, diverges for small values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e451" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e452" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c2">
<title>Learning dynamics in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e453" xlink:type="simple"/></inline-formula></title>
<p>In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e454" xlink:type="simple"/></inline-formula>, the probability of a trial to be rewarded decreases and thus the number of trials between rewarded trials diverges (see <xref ref-type="disp-formula" rid="pcbi.1003377.e444">Eq. 10</xref>). However, one can still characterize the dynamics in terms of the evolution of the error as a function of the number of <italic>rewarded</italic> trials. The condition that the network generates the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e455" xlink:type="simple"/></inline-formula> rewarded trial fully determines the noise:<disp-formula id="pcbi.1003377.e456"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e456" xlink:type="simple"/><label>(11)</label></disp-formula>The connectivity matrix is then updated according to:<disp-formula id="pcbi.1003377.e457"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e457" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e458"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e458" xlink:type="simple"/></disp-formula>where the normalized learning rate is defined by:<disp-formula id="pcbi.1003377.e459"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e459" xlink:type="simple"/><label>(12)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e460" xlink:type="simple"/></inline-formula>. Solving the above recursion, one finds:<disp-formula id="pcbi.1003377.e461"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e461" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e462"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e462" xlink:type="simple"/></disp-formula></p>
<p>The error and the squared Frobenius norm of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e463" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e464" xlink:type="simple"/></inline-formula>) are then:<disp-formula id="pcbi.1003377.e465"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e465" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e466"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e466" xlink:type="simple"/></disp-formula>where we use the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e467" xlink:type="simple"/></inline-formula>.</p>
<p>The sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e468" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e469" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e470" xlink:type="simple"/></inline-formula> converge when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e471" xlink:type="simple"/></inline-formula> if:<disp-formula id="pcbi.1003377.e472"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e472" xlink:type="simple"/><label>(13)</label></disp-formula>Their limiting values are then:<disp-formula id="pcbi.1003377.e473"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e473" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e474"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e474" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e475"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e475" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003377.e476"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e476" xlink:type="simple"/><label>(14)</label></disp-formula>Therefore, after enough rewarded trials the noiseless error goes down to zero. Note that there is no need to normalize the connectivity matrix after each update in this case, since in the large <italic>k</italic> limit the norm of the matrix returns to the value it had at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e477" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c3">
<title>The support of the noiseless error distribution is bounded</title>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e478" xlink:type="simple"/></inline-formula> is finite, the noiseless error after a rewarded trial is:<disp-formula id="pcbi.1003377.e479"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e479" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e480" xlink:type="simple"/></inline-formula> is such that the constraint in <xref ref-type="disp-formula" rid="pcbi.1003377.e430">Eq. (7)</xref> holds, <italic>i.e.</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e481" xlink:type="simple"/></inline-formula>. This constraint implies that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e482" xlink:type="simple"/></inline-formula> can be written as:<disp-formula id="pcbi.1003377.e483"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e483" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e484" xlink:type="simple"/></inline-formula> is the unit vector in the direction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e485" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e486" xlink:type="simple"/></inline-formula> is a vector with a maximal norm <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e487" xlink:type="simple"/></inline-formula>. Inserting <xref ref-type="disp-formula" rid="pcbi.1003377.e483">Eq. (16)</xref> into <xref ref-type="disp-formula" rid="pcbi.1003377.e479">Eq. (15)</xref> one finds:<disp-formula id="pcbi.1003377.e488"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e488" xlink:type="simple"/><label>(17)</label></disp-formula></p>
<p>The noiseless error for a large number of trials is a random variable with a probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e489" xlink:type="simple"/></inline-formula> on the support <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e490" xlink:type="simple"/></inline-formula>. For vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e491" xlink:type="simple"/></inline-formula> to be close to the target, the maximum value of the noiseless error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e492" xlink:type="simple"/></inline-formula>, needs to be as small as possible. To estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e493" xlink:type="simple"/></inline-formula>, we compute the realization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e494" xlink:type="simple"/></inline-formula> which maximizes the noiseless error, <xref ref-type="disp-formula" rid="pcbi.1003377.e518">Eq. (18)</xref>, at each rewarded trial <italic>k</italic>.</p>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e495" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e496" xlink:type="simple"/></inline-formula> is maximal if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e497" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e498" xlink:type="simple"/></inline-formula>. One then gets:<disp-formula id="pcbi.1003377.e499"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e499" xlink:type="simple"/></disp-formula>Solving the recursion gives:<disp-formula id="pcbi.1003377.e500"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e500" xlink:type="simple"/></disp-formula>and therefore after a long time we get:<disp-formula id="pcbi.1003377.e501"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e501" xlink:type="simple"/></disp-formula>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e502" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e503" xlink:type="simple"/></inline-formula> in Eq(17) is maximal if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e504" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e505" xlink:type="simple"/></inline-formula>. This leads to:<disp-formula id="pcbi.1003377.e506"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e506" xlink:type="simple"/></disp-formula>Solving the recursion and taking the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e507" xlink:type="simple"/></inline-formula>, one gets that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e508" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003377.e509"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e509" xlink:type="simple"/></disp-formula>and when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e510" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003377.e511"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e511" xlink:type="simple"/></disp-formula>To summarize:<disp-formula id="pcbi.1003377.e512"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e512" xlink:type="simple"/></disp-formula>In particular, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e513" xlink:type="simple"/></inline-formula> the noiseless error is guaranteed to always be smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e514" xlink:type="simple"/></inline-formula> at large time.</p>
</sec><sec id="s4c4">
<title>Generalization error after adaptation to one target</title>
<p>Let us assume that the network has adapted to the rotation of the target presented in direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e515" xlink:type="simple"/></inline-formula>. To measure the ability of the network to generalize to targets in other directions, we calculate the noiseless error for test target (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e516" xlink:type="simple"/></inline-formula>), presented in a direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e517" xlink:type="simple"/></inline-formula> and define the generalization error by:<disp-formula id="pcbi.1003377.e518"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e518" xlink:type="simple"/><label>(18)</label></disp-formula></p>
<p>In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e519" xlink:type="simple"/></inline-formula> (assuming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e520" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e521" xlink:type="simple"/></inline-formula> can be computed analytically, as function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e522" xlink:type="simple"/></inline-formula>. Using <xref ref-type="disp-formula" rid="pcbi.1003377.e479">Eq. (15)</xref> one finds:<disp-formula id="pcbi.1003377.e523"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e523" xlink:type="simple"/></disp-formula>and<disp-formula id="pcbi.1003377.e524"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e524" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e525" xlink:type="simple"/></inline-formula> is:<disp-formula id="pcbi.1003377.e526"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e526" xlink:type="simple"/><label>(20)</label></disp-formula>depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e527" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e528" xlink:type="simple"/></inline-formula> only via <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e529" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e530" xlink:type="simple"/></inline-formula>. Specifically, in the limit of large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e531" xlink:type="simple"/></inline-formula> and when using the tuning curve function in <xref ref-type="disp-formula" rid="pcbi.1003377.e409">Eq. (2)</xref>, one gets:<disp-formula id="pcbi.1003377.e532"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e532" xlink:type="simple"/><label>(21)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e533" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4d">
<title>Adaptation to two targets</title>
<sec id="s4d1">
<title>How does a reward affect the next trial?</title>
<p>Here we consider the case where the network adapts to two targets in the direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e534" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e535" xlink:type="simple"/></inline-formula> presented in alternation. If a rewarded trial occurs for one of the targets, the connectivity matrix is updated, affecting the noiseless error on the next trial when the other target is presented.</p>
<p>This noiseless error can be computed in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e536" xlink:type="simple"/></inline-formula>. It is a good estimate for the noiseless error in the beginning of the adaptation with finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e537" xlink:type="simple"/></inline-formula>, where the error is still big with respect to the target size. Let us assume that on trial <italic>k</italic> a target in direction <italic>θ</italic> is presented and that it is rewarded. This condition fully determines the realization of the noise on trial <italic>k</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e538" xlink:type="simple"/></inline-formula>. The noiseless errors for the two targets following the rewarded trial, denoted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e539" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e540" xlink:type="simple"/></inline-formula>, can be determined analytically. One finds:<disp-formula id="pcbi.1003377.e541"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e541" xlink:type="simple"/><label>(22)</label></disp-formula></p>
<p>If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e542" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e543" xlink:type="simple"/></inline-formula>, that is, the noiseless error for the target that has been rewarded decreases following the update of the connectivity matrix. For the other target (direction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e544" xlink:type="simple"/></inline-formula>), the effect of this update on the noiseless errors depends on the sign of the expression in parentheses in the second equation. If the two targets are in opposite directions, it is always positive and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e545" xlink:type="simple"/></inline-formula>. Thus, while the network performs better for one of the targets it performs worse for the other target. We term this situation <italic>destructive</italic> interference. On the other hand, if the targets are close such that the expression in parentheses is negative, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e546" xlink:type="simple"/></inline-formula>. In other words, if the network improves for one of the targets it also improves for the other target. We term this situation <italic>constructive</italic> interference.</p>
<p>In particular, for the first rewarded trial, using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e547" xlink:type="simple"/></inline-formula>(0), we get:<disp-formula id="pcbi.1003377.e548"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e548" xlink:type="simple"/></disp-formula>where:<disp-formula id="pcbi.1003377.e549"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e549" xlink:type="simple"/><label>(23)</label></disp-formula>We expect a constructive interference for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e550" xlink:type="simple"/></inline-formula> and destructive interferences otherwise. Note that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e551" xlink:type="simple"/></inline-formula> the interference function equals to the generalization error function (<xref ref-type="disp-formula" rid="pcbi.1003377.e526">Eq. (20)</xref>). The transition between the constructive and destructive regimes is given by:<disp-formula id="pcbi.1003377.e552"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e552" xlink:type="simple"/></disp-formula>The quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e553" xlink:type="simple"/></inline-formula>) characterizes the strength of the interference. It can be a non-monotonous function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e554" xlink:type="simple"/></inline-formula>. To show this, we calculate the derivative of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e555" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e556" xlink:type="simple"/></inline-formula>, using <xref ref-type="disp-formula" rid="pcbi.1003377.e541">Eq. (22)</xref>. This derivative changes sign when:<disp-formula id="pcbi.1003377.e557"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e557" xlink:type="simple"/><label>(24)</label></disp-formula>For instance, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e558" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e559" xlink:type="simple"/></inline-formula> the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e560" xlink:type="simple"/></inline-formula> depends non-monotonically on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e561" xlink:type="simple"/></inline-formula>. In other words, for sufficiently narrow tuning curves, the interference varies non-monotonically with the angular difference. However, this non-monotonicity effect can be very small: if the tuning curves are too narrow, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e562" xlink:type="simple"/></inline-formula> quickly reaches zero when increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e563" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4e">
<title>Numerical simulations</title>
<p>In the numerical simulations described in this paper, the input layer consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e564" xlink:type="simple"/></inline-formula> neurons. We normalized the tuning curves (parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e565" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003377.e409">Eq. (2)</xref>) such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e566" xlink:type="simple"/></inline-formula> remains constant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e567" xlink:type="simple"/></inline-formula>) when changing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e568" xlink:type="simple"/></inline-formula>. This was done to guarantee that the time to learn one target does not depend on the tuning width.</p>
<sec id="s4e1">
<title>Learning duration and final error</title>
<p>We define the final error of the network as the median of the error over the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e569" xlink:type="simple"/></inline-formula> trials of the simulation for each realization. We then determine the learning duration, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e570" xlink:type="simple"/></inline-formula>, as the trial number at which the filtered signal (median filter with a window length of 50 trials) crosses a threshold, defined to be 5% above the final error. In order to avoid boundary problems of the filter at time 0 (the discontinuity in the error when we induce the rotation), we calculate the error at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e571" xlink:type="simple"/></inline-formula> while assuming that the cursor is already rotated (even though it did not). In the figures we plot the actual error before the rotation, which is small. Similar results were obtained using a linear filter.</p>
</sec><sec id="s4e2">
<title>Time dependent correlations between the errors for two targets</title>
<p>When the network adapts to a rotation for two targets presented in alternation in consecutive trials, the learning processes for the two targets interfere. This interference can be quantified by considering the correlations between the errors on two consecutive trials:<disp-formula id="pcbi.1003377.e572"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e572" xlink:type="simple"/></disp-formula>The brackets denote the average over repetitions of the adaptation process, which differ by the realization of the noise. Negative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e573" xlink:type="simple"/></inline-formula> indicates that if the network improves for one target it deteriorates for the other target (destructive interference). Positive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e574" xlink:type="simple"/></inline-formula> corresponds to constructive interference.</p>
</sec><sec id="s4e3">
<title>Performance and noiseless performance</title>
<p>We ran long simulations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e575" xlink:type="simple"/></inline-formula> trials to estimate the performance and noiseless performance after the transient learning phase. The performance is given by:<disp-formula id="pcbi.1003377.e576"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e576" xlink:type="simple"/><label>(25)</label></disp-formula>and the noiseless performance is given by:<disp-formula id="pcbi.1003377.e577"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003377.e577" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e578" xlink:type="simple"/></inline-formula> is the Heaviside function and the average is over time, when the transient learning phase was excluded.</p>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003377.s001" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003377.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p>Delayed learning effect with an on-line gradient ascent algorithm. <bold>A.</bold> Delayed learning in a reward function that varies abruptly with the error (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e579" xlink:type="simple"/></inline-formula>). <bold>B.</bold> The delayed learning is reduced for a smoother reward function. (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e580" xlink:type="simple"/></inline-formula>). <bold>C.</bold> The delayed learning almost disappears when the reward function is smoothed even further (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e581" xlink:type="simple"/></inline-formula>). Note the change of scale in the abscissa. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e582" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e583" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e584" xlink:type="simple"/></inline-formula>.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003377.s002" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003377.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p>Delayed learning effect in a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e585" xlink:type="simple"/></inline-formula> rotation for two targets in the 3-layer network. The reach angle (in degrees) is plotted as a function of the trial number. The shaded area corresponds to the target size. Initial conditions as explained in the text. Parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e586" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e587" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e588" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e589" xlink:type="simple"/></inline-formula>.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003377.s003" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003377.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p>Gradual adaptation for an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e590" xlink:type="simple"/></inline-formula> rotation. <bold>A.</bold> Reach angle (in degrees) as a function of the trial number when the rotation angle is increased by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e591" xlink:type="simple"/></inline-formula> every 40 trials up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e592" xlink:type="simple"/></inline-formula>. The shaded area corresponds to the target size (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e593" xlink:type="simple"/></inline-formula> around the target center). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e594" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The generalization error, given as the change in reach angle. The learned target is at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e595" xlink:type="simple"/></inline-formula>. Circles : simulation results. For clarity, the results are displayed for test targets sampled every 2.5 degrees. Solid line: analytical results. Shaded area corresponds to the standard deviation in generalization error in numerical simulations estimated over 100 repetitions. Number of neurons in the input layer: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e596" xlink:type="simple"/></inline-formula>. <bold>C.</bold> The shape of the tuning curves that was used in (<bold>A</bold>) and (<bold>B</bold>): <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e597" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e598" xlink:type="simple"/></inline-formula> is a normalization constant (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e599" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e600" xlink:type="simple"/></inline-formula>.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003377.s004" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003377.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p>Learning duration when adapting to multiple targets varies monotonically with the number of learned targets when using a gradient descent on a quadratic error function. Total number of target presentations required to learn the entire task <italic>vs.</italic> the number of presented targets, <italic>m</italic>. The targets are evenly distributed (between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e601" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e602" xlink:type="simple"/></inline-formula>). Learning duration was calculated as the trial number at which learning curve crossed a threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e603" xlink:type="simple"/></inline-formula>. Color coded as in <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11</xref> in the <xref ref-type="sec" rid="s2">Results</xref>. Black: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e604" xlink:type="simple"/></inline-formula>. Blue: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e605" xlink:type="simple"/></inline-formula>. Purple: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e606" xlink:type="simple"/></inline-formula>. Green: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003377.e607" xlink:type="simple"/></inline-formula>. Dashed black line corresponds to learning the targets independently. Compare with <xref ref-type="fig" rid="pcbi-1003377-g011">Figure 11</xref> in main text.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003377.s005" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003377.s005" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>This document is a supporting text for the supplementary figures.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Yonatan Loewenstein, Dana Barniv, Mehdi Khamassi, German Mato and Carl van Vreeswijk for fruitful discussions and critical reading of the manuscript of this paper. We also thank Itay Novick, David Perkel and Eilon Vaadia for illuminating discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003377-Pouget1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Snyder</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>Computational approaches to sensorimotor transformations</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1192</fpage>–<lpage>1198</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Piaget1"><label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">Piaget J, Cook M (1953) The origin of intelligence in the child. London: Routledge &amp; Kegan Paul.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Krakauer1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krakauer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pine</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Ghilardi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ghez</surname><given-names>C</given-names></name> (<year>2000</year>) <article-title>Learning of visuomotor transformations for vectorial planning of reaching trajectories</article-title>. <source>The Journal of Neuroscience</source> <volume>20</volume>: <fpage>8916</fpage>–<lpage>8924</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Thoroughman1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thoroughman</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2000</year>) <article-title>Learning of action through adaptive combination of motor primitives</article-title>. <source>Nature</source> <volume>407</volume>: <fpage>742</fpage>–<lpage>747</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Chou1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Lisberger</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2002</year>) <article-title>Spatial generalization of learning in smooth pursuit eye movements: implications for the coordinate frame and sites of learning</article-title>. <source>The Journal of Neuroscience</source> <volume>22</volume>: <fpage>4728</fpage>–<lpage>4739</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Linkenhoker1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linkenhoker</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Knudsen</surname><given-names>E</given-names></name> (<year>2002</year>) <article-title>Incremental training increases the plasticity of the auditory space map in adult barn owls</article-title>. <source>Nature</source> <volume>419</volume>: <fpage>293</fpage>–<lpage>296</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Sober1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sober</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Brainard</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Adult birdsong is actively maintained by error correction</article-title>. <source>Nature neuroscience</source> <volume>12</volume>: <fpage>927</fpage>–<lpage>931</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Houde1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Houde</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Sensorimotor adaptation in speech production</article-title>. <source>Science</source> <volume>279</volume>: <fpage>1213</fpage>–<lpage>1216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Sober2"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sober</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Brainard</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Vocal learning is constrained by the statistics of sensorimotor experience</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume>: <fpage>21099</fpage>–<lpage>21103</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Fiete1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiete</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2007</year>) <article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title>. <source>Journal of Neurophysiology</source> <volume>98</volume>: <fpage>2038</fpage>–<lpage>2057</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Rokni1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rokni</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Richardson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bizzi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2007</year>) <article-title>Motor learning with unstable neural representations</article-title>. <source>Neuron</source> <volume>54</volume>: <fpage>653</fpage>–<lpage>666</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Poggio1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bizzi</surname><given-names>E</given-names></name> (<year>2004</year>) <article-title>Generalization in vision and motor control</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>768</fpage>–<lpage>774</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Tanaka1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Krakauer</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Adaptation to visuomotor rotation through interaction between posterior parietal and motor cortical areas</article-title>. <source>Journal of Neurophysiology</source> <volume>102</volume>: <fpage>2921</fpage>–<lpage>2932</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Taylor1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taylor</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hieber</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Ivry</surname><given-names>R</given-names></name> (<year>2013</year>) <article-title>Feedback-dependent generalization</article-title>. <source>Journal of Neurophysiology</source> <volume>109</volume>: <fpage>202</fpage>–<lpage>215</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Tumer1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tumer</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Brainard</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Performance variability enables adaptive plasticity of crystallized adult birdsong</article-title>. <source>Nature</source> <volume>450</volume>: <fpage>1240</fpage>–<lpage>1244</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Andalman1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andalman</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A basal ganglia-forebrain circuit in the songbird biases motor output to avoid vocal errors</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>12518</fpage>–<lpage>12523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Izawa1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izawa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title>. <source>PLoS computational biology</source> <volume>7</volume>: <fpage>e1002012</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Shmuelof1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shmuelof</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Haith</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Delnicki</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mazzoni</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Overcoming motor forgetting through reinforcement of learned actions</article-title>. <source>The Journal of Neuroscience</source> <volume>32</volume>: <fpage>14617</fpage>–<lpage>14621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Huang1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Haith</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mazzoni</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Krakauer</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Rethinking motor learning and savings in adaptation paradigms: model-free memory for successful actions combines with internal models</article-title>. <source>Neuron</source> <volume>70</volume>: <fpage>787</fpage>–<lpage>801</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Paz1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paz</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Boraud</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Natan</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name> (<year>2003</year>) <article-title>Preparatory activity in motor cortex reflects learning of local visuomotor skills</article-title>. <source>Nature Neuroscience</source> <volume>6</volume>: <fpage>882</fpage>–<lpage>890</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Warren1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Warren</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tumer</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Charlesworth</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Brainard</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Mechanisms and time course of vocal learning and consolidation in the adult songbird</article-title>. <source>Journal of Neurophysiology</source> <volume>106</volume>: <fpage>1806</fpage>–<lpage>1821</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Skinner1"><label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">Skinner B (1967) Science and human behavior. Free Press.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Lawrence1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lawrence</surname><given-names>D</given-names></name> (<year>1952</year>) <article-title>The transfer of a discrimination along a continuum</article-title>. <source>Journal of Comparative and Physiological Psychology</source> <volume>45</volume>: <fpage>511</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Terrace1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terrace</surname><given-names>H</given-names></name> (<year>1963</year>) <article-title>Discrimination learning with and without errors</article-title>. <source>Journal of the Experimental Analysis of Behavior</source> <volume>6</volume>: <fpage>1</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Kangas1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kangas</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>A novel touch-sensitive apparatus for behavioral studies in unrestrained squirrel monkeys</article-title>. <source>Journal of Neuroscience Methods</source> <volume>209</volume>: <fpage>331</fpage>–<lpage>6</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Ng1"><label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">Ng A, Harada D, Russell S (1999) Policy invariance under reward transformations: Theory and application to reward shaping. In: Machine learning: proceedings of the Sixteenth International Conference (ICML'99). Morgan Kaufmann Pub, p. 278.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Randlv1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Randløv J (2000) Shaping in reinforcement learning by changing the physics of the problem. In: Proc. 17th International Conf. on Machine Learning. pp. 767–774.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Krueger1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krueger</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>Flexible shaping: How learning in small steps helps</article-title>. <source>Cognition</source> <volume>110</volume>: <fpage>380</fpage>–<lpage>394</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Kerr1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerr</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>J</given-names></name> (<year>2001</year>) <article-title>Dopamine d-1/d-5 receptor activation is required for long-term potentiation in the rat neostriatum in vitro</article-title>. <source>Journal of Neurophysiology</source> <volume>85</volume>: <fpage>117</fpage>–<lpage>124</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Ding1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ding</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Perkel</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>Long-term potentiation in an avian basal ganglia nucleus essential for vocal learning</article-title>. <source>The Journal of Neuroscience</source> <volume>24</volume>: <fpage>488</fpage>–<lpage>494</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Reynolds1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hyland</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>J</given-names></name> (<year>2001</year>) <article-title>A cellular mechanism of reward-related learning</article-title>. <source>Nature</source> <volume>413</volume>: <fpage>67</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Loewenstein1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>103</volume>: <fpage>15224</fpage>–<lpage>15229</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Reynolds2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reynolds</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name> (<year>2002</year>) <article-title>Dopamine-dependent plasticity of corticostriatal synapses</article-title>. <source>Neural Networks</source> <volume>15</volume>: <fpage>507</fpage>–<lpage>521</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Legenstein1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Chase</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>A reward-modulated hebbian learning rule can explain experimentally observed network reorganization in a brain control task</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>8400</fpage>–<lpage>8410</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Frmaux1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Functional requirements for reward-modulated spike-timing-dependent plasticity</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>13326</fpage>–<lpage>13337</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Williams1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>R</given-names></name> (<year>1992</year>) <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title>. <source>Machine learning</source> <volume>8</volume>: <fpage>229</fpage>–<lpage>256</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Fiete2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiete</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances</article-title>. <source>Physical review letters</source> <volume>97</volume>: <fpage>48104</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Werfel1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werfel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2005</year>) <article-title>Learning curves for stochastic gradient descent in linear feedforward networks</article-title>. <source>Neural computation</source> <volume>17</volume>: <fpage>2699</fpage>–<lpage>2718</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-MandelblatCerf1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mandelblat-Cerf</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Paz</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name> (<year>2009</year>) <article-title>Trial-to-trial variability of single cells in motor cortices is dynamically modified during visuomotor adaptation</article-title>. <source>The Journal of Neuroscience</source> <volume>29</volume>: <fpage>15053</fpage>–<lpage>15062</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-lveczky1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ölveczky</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Otchy</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Goldberg</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Aronov</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Changes in the neural control of a complex motor sequence during learning</article-title>. <source>Journal of Neurophysiology</source> <volume>106</volume>: <fpage>386</fpage>–<lpage>397</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Krding1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Körding</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>The loss function of sensorimotor learning</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>101</volume>: <fpage>9839</fpage>–<lpage>9842</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Schultz1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>1998</year>) <article-title>Predictive reward signal of dopamine neurons</article-title>. <source>Journal of neurophysiology</source> <volume>80</volume>: <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Izawa2"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izawa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kondo</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ito</surname><given-names>K</given-names></name> (<year>2004</year>) <article-title>Biological arm motion through reinforcement learning</article-title>. <source>Biological cybernetics</source> <volume>91</volume>: <fpage>10</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Law1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Law</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Reinforcement learning can account for associative and perceptual learning on a visual-decision task</article-title>. <source>Nature Neuroscience</source> <volume>12</volume>: <fpage>655</fpage>–<lpage>663</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Vladimirskiy1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vladimirskiy</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Stimulus sampling as an exploration mechanism for fast reinforcement learning</article-title>. <source>Biological cybernetics</source> <volume>100</volume>: <fpage>319</fpage>–<lpage>330</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Taylor2"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taylor</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stone</surname><given-names>P</given-names></name> (<year>2009</year>) <article-title>Transfer learning for reinforcement learning domains: A survey</article-title>. <source>The Journal of Machine Learning Research</source> <volume>10</volume>: <fpage>1633</fpage>–<lpage>1685</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Laud1"><label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Laud A, DeJong G (2003) The influence of reward on the speed of reinforcement learning: An analysis of shaping. In: Proc. 12th International Conf. on Machine Learning (ICML-2003). volume 20, p. 440.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Fiete3"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiete</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Hahnloser</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Temporal sparseness of the premotor drive is important for rapid learning in a neural network model of birdsong</article-title>. <source>Journal of Neurophysiology</source> <volume>92</volume>: <fpage>2274</fpage>–<lpage>2282</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Donchin1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donchin</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Francis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2003</year>) <article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control</article-title>. <source>The Journal of neuroscience</source> <volume>23</volume>: <fpage>9032</fpage>–<lpage>9045</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Gale1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gale</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Perkel</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Anatomy of a songbird basal ganglia circuit essential for vocal learning and plasticity</article-title>. <source>Journal of chemical neuroanatomy</source> <volume>39</volume>: <fpage>124</fpage>–<lpage>131</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003377-Gradshten1"><label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">Gradshtei˘n I, Ryzhik I, Jeffrey A, Zwillinger D (2007) Table of integrals, series, and products. Elsevier academic press.</mixed-citation>
</ref>
</ref-list></back>
</article>