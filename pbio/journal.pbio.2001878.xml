<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2001878</article-id>
<article-id pub-id-type="publisher-id">pbio.2001878</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Sensory physiology</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Ferrets</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Egocentric and allocentric representations in auditory cortex</article-title>
<alt-title alt-title-type="running-head">Egocentric and allocentric representations in auditory cortex</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1375-7769</contrib-id>
<name name-style="western">
<surname>Town</surname>
<given-names>Stephen M.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Brimijoin</surname>
<given-names>W. Owen</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bizley</surname>
<given-names>Jennifer K.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Ear Institute, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>MRC/CSO Institute of Hearing Research – Scottish Section, Glasgow, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Ungerleider</surname>
<given-names>Leslie</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>National Institute of Mental Health, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">s.town@ucl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>15</day>
<month>6</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2017</year>
</pub-date>
<volume>15</volume>
<issue>6</issue>
<elocation-id>e2001878</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>12</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>8</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Town et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2001878"/>
<abstract>
<p>A key function of the brain is to provide a stable representation of an object’s location in the world. In hearing, sound azimuth and elevation are encoded by neurons throughout the auditory system, and auditory cortex is necessary for sound localization. However, the coordinate frame in which neurons represent sound space remains undefined: classical spatial receptive fields in head-fixed subjects can be explained either by sensitivity to sound source location relative to the head (egocentric) or relative to the world (allocentric encoding). This coordinate frame ambiguity can be resolved by studying freely moving subjects; here we recorded spatial receptive fields in the auditory cortex of freely moving ferrets. We found that most spatially tuned neurons represented sound source location relative to the head across changes in head position and direction. In addition, we also recorded a small number of neurons in which sound location was represented in a world-centered coordinate frame. We used measurements of spatial tuning across changes in head position and direction to explore the influence of sound source distance and speed of head movement on auditory cortical activity and spatial tuning. Modulation depth of spatial tuning increased with distance for egocentric but not allocentric units, whereas, for both populations, modulation was stronger at faster movement speeds. Our findings suggest that early auditory cortex primarily represents sound source location relative to ourselves but that a minority of cells can represent sound location in the world independent of our own position.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>When we hear a sound, we can describe its location relative to ourselves (e.g., “the phone is on my right”) or relative to the world (e.g., “the phone is in the corner”). These descriptions of space are known as egocentric and allocentric, respectively, and illustrate the representation of sound location in reference frames defined either by the observer or the world. We know that neurons in the brain can represent the location of a sound source. However, previous experiments have been performed in static subjects, in which it’s not possible to tell whether spatial tuning reflects sensitivity to the position of the sound relative to the head or in the world. Here, we recorded neurons in the auditory cortex of freely moving ferrets and showed that most cells represent the position of a sound relative to the head, i.e., in an egocentric reference frame. We also recorded a smaller number of allocentric cells that were tuned to the position of a sound in the world, across the movement of the subject. By recording in freely moving animals, we were also able to investigate the neural encoding of sound source distance and the modulation of auditory processing by head movement speed.</p>
</abstract>
<funding-group>
<funding-statement>MRC <ext-link ext-link-type="uri" xlink:href="http://www.mrc.ac.uk/" xlink:type="simple">http://www.mrc.ac.uk/</ext-link> (grant number U135097131). Awarded to WOB. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Human Frontiers Science Foundation <ext-link ext-link-type="uri" xlink:href="http://www.hfsp.org/" xlink:type="simple">http://www.hfsp.org/</ext-link> (grant number RGY0068). Awarded to JKB. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Wellcome Trust <ext-link ext-link-type="uri" xlink:href="https://wellcome.ac.uk/" xlink:type="simple">https://wellcome.ac.uk/</ext-link> (grant number WT098418MA). Awarded to JKB. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. BBSRC <ext-link ext-link-type="uri" xlink:href="http://www.bbsrc.ac.uk/" xlink:type="simple">http://www.bbsrc.ac.uk/</ext-link> (grant number BB/H016813/1). Awarded to JKB. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="0"/>
<page-count count="34"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data are publicly available from figshare (<ext-link ext-link-type="uri" xlink:href="http://doi.org/10.6084/m9.figshare.c.3767741.v1" xlink:type="simple">http://doi.org/10.6084/m9.figshare.c.3767741.v1</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A central role of the brain is to build a model of the world and objects within it that remains stable across changes in sensory input when we move. In hearing, this requires that an observer maintains the identification of an auditory object as they move through an environment. Movement is a critical aspect of sensing [<xref ref-type="bibr" rid="pbio.2001878.ref001">1</xref>] that contributes to sound localization and other auditory behaviors [<xref ref-type="bibr" rid="pbio.2001878.ref002">2</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref007">7</xref>]; however, the neural basis underpinning active hearing and how the brain constructs world-centered sound location remains unknown.</p>
<p>For a moving observer, it is possible to represent sound location either relative to oneself (egocentric representation) or relative to the world through which one moves (allocentric representation). Allocentric representations provide a consistent report of object location across movement of an observer [<xref ref-type="bibr" rid="pbio.2001878.ref008">8</xref>], as well as a common reference frame for mapping information across several observers or multiple sensory systems [<xref ref-type="bibr" rid="pbio.2001878.ref009">9</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref010">10</xref>]. Despite the computational value and perceptual relevance of allocentric representations to hearing, studies of auditory processing have only recently considered the coordinate frames in which sound location is represented [<xref ref-type="bibr" rid="pbio.2001878.ref011">11</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref013">13</xref>]. Both electroencephalography (EEG) and modelling studies hint that sound location might be represented in cortex in both head-centered and head-independent spaces. However, EEG has not yet revealed the precise location of these representations and cannot determine how individual neurons in tonotopic auditory cortex define space.</p>
<p>In static subjects, auditory cortical neurons encode sound azimuth and elevation [<xref ref-type="bibr" rid="pbio.2001878.ref014">14</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>], and localization of sound sources requires an intact auditory cortex [<xref ref-type="bibr" rid="pbio.2001878.ref019">19</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref021">21</xref>]. However, in static subjects with a fixed head position, neural tuning to sound location is ambiguous, as the head and world coordinate frames are fixed in alignment, and so allocentric and egocentric sound location are always equivalent. While it has been largely assumed that cortical neurons represent sound location relative to the head, the spatial coordinate frame in which location is encoded remains to be demonstrated. Furthermore, though the acoustic cues to sound localization are explicitly head-centered, information about head direction, which is necessary to form a world-centered representation, is present at early levels of the ascending auditory system [<xref ref-type="bibr" rid="pbio.2001878.ref022">22</xref>]. Thus, it may be possible for neurons in the auditory system to represent space in an allocentric, world-centered coordinate frame that would preserve sound location across changes in head position and direction.</p>
<p>Here, we resolve the coordinate frame ambiguity of spatial tuning in auditory cortex by recording from neurons in freely moving ferrets. In moving conditions, the head and world coordinate frames are no longer fixed in alignment, so we can determine in which coordinate frame a given cell is most sensitive to sound source location. Our approach reveals head-centered and world-centered units, suggesting that egocentric and allocentric representations coexist in auditory cortex. We also explore the impact of distance from a sound source and the speed of a subject’s movement on spatial tuning in auditory cortex.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We hypothesized that measuring spatial tuning in moving subjects would allow us to distinguish between egocentric (head-centered) and allocentric (world-centered) representations of sound location (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1</xref>). To formalize this theory and develop quantitative predictions about the effects of observer movement on spatial tuning, we first simulated egocentric and allocentric neurons that were tuned to sound locations defined relative to the head (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1a</xref>) and world (independent of the subject), respectively (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1b</xref>). We simulated allocentric and egocentric units using parameters fitted to produce identical spatial receptive fields when tested in the classical condition in which the head is in a fixed location at the center of a speaker ring (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1c and 1d</xref>), illustrating coordinate frame ambiguity. Our simulation also confirmed that when the observer moved freely with a uniform distribution of head directions, spatial tuning should only be apparent in the coordinate frame relevant for neural output (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1e</xref>). Additionally, changes in head direction produced systematic shifts in tuning curves in the coordinate frame that were irrelevant for neural output, while tuning in the relevant coordinate frame was invariant across head direction (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1f</xref>). We subsequently demonstrated that tuning curves of many shapes and preferred locations can theoretically be explained by spatial receptive fields based within an allocentric coordinate frame (<xref ref-type="supplementary-material" rid="pbio.2001878.s001">S1 Fig</xref>). With simulations providing a foundation, we then made recordings in freely moving animals to determine whether the spatial tuning of auditory cortical neurons followed egocentric or allocentric predictions.</p>
<fig id="pbio.2001878.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Simulated receptive fields show that observer movement resolves coordinate frame ambiguity.</title>
<p><bold>a-b</bold>: Simulated neurons with receptive fields tuned to sound location relative to the head (a, Egocentric) or in the world (b, Allocentric). Circles show hypothetical sound sources in a classical speaker ring; black lines indicate axes and origin of the simulated world. <bold>c</bold>: Schematic of world and head coordinate frames (CFs). <bold>d</bold>: Sound-evoked tuning curves according to allocentric and egocentric hypotheses when head and world coordinate frames were aligned. <bold>e-f</bold>: Predictions of allocentric and egocentric hypotheses showing mean spike probability averaged across uniform distributions of head rotation and position (e) and at specific head directions (f). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955210.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955210.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g001" xlink:type="simple"/>
</fig>
<p>To measure spatial tuning in moving subjects, we implanted ferrets (<italic>n</italic> = 5) with multichannel tungsten electrode arrays, allowing the recording of single and multiunit activity during behavior. During neural recording, each ferret was placed in an arena, which the animal explored for water rewards while the surrounding speakers played click sounds (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2a</xref>). To measure the animal’s head position, direction, and speed in the world during exploration (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2b–2f</xref>), we tracked light-emitting diodes (LEDs) placed on the midline of the head (<xref ref-type="supplementary-material" rid="pbio.2001878.s012">S1 Video</xref>). During exploration, click sounds were presented from speakers arranged at 30° intervals between ±90° relative to the arena center, with speaker order and inter-click interval (250–500 ms) varied pseudo-randomly. We also roved the level of clicks between 54 decibel sound pressure level (dB SPL) and 60 dB SPL such that absolute sound level varied both as a function of sound source level and distance between head and speaker, to reduce cues about sound location provided by absolute sound level (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2g and 2h</xref>). Clicks were used as they provided instantaneous energy and thus ensured minimal movement of the animal during stimulus presentation (<xref ref-type="supplementary-material" rid="pbio.2001878.s003">S3 Fig</xref>). The locations (speaker angle) from which the clicks originated were used alone to estimate <bold>allocentric</bold> receptive fields and were used in conjunction with the animal’s head direction and position to measure <bold>egocentric</bold> spatial receptive fields.</p>
<fig id="pbio.2001878.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Experimental design and exploratory behavior in a sound field.</title>
<p><bold>a</bold>: Arena with speakers (filled circles) and water ports (unfilled circles). Shading indicates the sound field generated by a click from the speaker at 0°, calibrated to be 60 dB SPL at the center of the chamber. Stimuli were presented with a pseudorandom interval and order across speakers. <bold>b</bold>: Mean proportion of time in each recording session spent within the arena. <bold>c</bold>: Stimulus angles relative to the head and world for one session that was representative of behavior in all sessions (<italic>n</italic> = 57). <bold>d</bold>: Correlation coefficients (R<sup>2</sup>) between sound angles in head and world coordinate frames across all behavioral sessions. <bold>e-h</bold>: Distributions of head direction, head speed, distance between head and sound source, and the sound level at the animal’s head during behavior. Bars indicate mean ± SEM across sessions. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955258.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955258.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g002" xlink:type="simple"/>
</fig>
<p>We observed that animals moved throughout the arena to collect water (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2b</xref>) and used a range of head directions during exploration (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2e</xref>). In contrast to our initial simulations, the distribution of the animal’s head direction was notably non-uniform, leading to correlations between sound source angle relative to the head and the world (e.g., <xref ref-type="fig" rid="pbio.2001878.g002">Fig 2c</xref>; mean ± SEM R<sup>2</sup> = 0.247 ± 0.0311). This correlation between sound source angles resulted because the animal preferred to orient towards the front of the arena (0°) and thus sounds that were to the right of the animal were more often on the right of the arena than would result from random behavior. The preference of the animal was likely a consequence of the shape of the arena and the location of the water spouts within it. Although the correlation between sound source locations relative to the head and within the world was relatively small, we sought to determine how the animal’s head direction preference affected our experimental predictions.</p>
<p>To assess the influence of real animal behavior on our ability to distinguish coordinate frames, we combined our simulated egocentric and allocentric receptive fields (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1</xref>) with the animal’s head position and direction across each single behavioral testing session (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3a</xref>). This allowed us to calculate the spatial tuning for known allocentric and egocentric receptive fields in both head and world coordinate frames. Our simulation predictions (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1</xref>) demonstrated that for a uniform distribution of head angles, the tuning function of allocentric or egocentric units should be flat when considered in the irrelevant coordinate frame. However, a bias in head location over time would produce spatial modulation in firing rate with location in the irrelevant coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3a</xref>). In order to account for this, we therefore measured residual modulation as the ratio of modulation depth in each coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3</xref>; MD<sub>Irrelevant</sub> / MD<sub>Relevant</sub>). Residual modulation thus represents the degree of indirect spatial tuning in one coordinate frame observed as a by-product of the animal’s behavior combined with spatial tuning in another coordinate frame.</p>
<fig id="pbio.2001878.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Estimating residual modulation.</title>
<p><bold>a</bold>: Example workflow for estimating residual modulation in coordinate frames irrelevant for neural output that result from biases in head direction. Residual modulation was defined as: (MD<sub>Irrelevant</sub> / MD<sub>Relevant</sub>). Estimations performed separately using simulated units for each behavioral session. <bold>b</bold>: Residual modulation was inversely correlated with the standard deviation of head directions (σ). Red, filled lines indicate regression fit and confidence intervals. Dashed lines indicate the data point for the single session in (a). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955291.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955291.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g003" xlink:type="simple"/>
</fig>
<p>Across all behavioral sessions, residual modulation was inversely correlated with variation in the animal’s head direction (expressed as standard deviation) for both egocentric (R<sup>2</sup> = 0.562, <italic>p</italic> = 1.07 x 10<sup>−10</sup>) and allocentric simulated units (R<sup>2</sup> = 0.615, <italic>p</italic> = 3.73 x 10<sup>−12</sup>) (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3b</xref>). This indicated that for real animal behavior, we would not expect to see the complete abolition of tuning but rather spatial tuning in both coordinate frames, with the weaker tuning potentially attributable to the animal’s bias in head direction. In our neural analysis, we thus used the relationship between behavior and residual modulation to provide a statistical framework in which to assess the significance of spatial tuning of real neurons.</p>
<sec id="sec003">
<title>Egocentric and allocentric tuning in auditory cortex</title>
<p>During exploration, we recorded the activity of 186 sound-responsive units (50 single units, 136 multi-units) in auditory cortex (<xref ref-type="supplementary-material" rid="pbio.2001878.s002">S2 Fig</xref>). Electrode arrays were targeted to span the low-frequency areas in which the middle and posterior ectosylvian gyral regions meet, and thus units were sampled from primary auditory cortex and two tonotopically organized secondary fields: the posterior pseudosylvian and posterior suprasylvian fields. We analyzed the firing rates of units in the 50 milliseconds after the onset of each click; this window was wide enough to capture the neural response while being sufficiently short so that the animal’s head moved less than 1 cm (median 4 mm, <xref ref-type="supplementary-material" rid="pbio.2001878.s003">S3 Fig</xref>) and less than 30° (median 12.6°)—the interval between speakers. The time interval between stimuli always exceeded 250 ms.</p>
<p>We identified periods of time during which the animal was facing forwards (± 15° of the arena midline) at the center of the speaker ring (<xref ref-type="supplementary-material" rid="pbio.2001878.s004">S4 Fig</xref>): in this situation, we mimic classic neurophysiological investigations of spatial tuning in which head and world coordinate frames are aligned. In the aligned case, we recorded 92 units that were significantly modulated by sound source location (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4a–4d</xref>: Top left, general linear model [GLM] analysis of deviance, <italic>p</italic> ≤ 0.05) and for which spatial tuning curves computed in head and world coordinate frames were highly correlated (mean ± SEM: R<sup>2</sup> = 0.889 ± 0.0131). We then compared the aligned control condition with all data in which the head and world coordinate frames were free to vary. Compared to the aligned condition, the correlation between egocentric and allocentric tuning curves when coordinate frames were free to vary was significantly lower (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4e</xref>, R<sup>2</sup> = 0.522 ± 0.0294) (paired <italic>t</italic> test: free versus aligned <italic>t</italic><sub>91</sub> = 8.76, <italic>p</italic> &lt; 0.001), and differences in spatial tuning in head and world coordinate frames became visible (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4a–4d</xref>: Bottom left).</p>
<fig id="pbio.2001878.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Spatial tuning of egocentric and allocentric units.</title>
<p><bold>a-d</bold>: Spatial tuning of four example units that were classified as egocentric (a-b) or allocentric (c-d). In each panel, top left: tuning curves calculated for sound angle in head and world coordinate frames (CFs) when both frames were aligned. Bottom left: tuning curves when head and world CFs were free to vary. Top and bottom right: tuning curves plotted at specific head rotations. Model fit refers to the percentage of explainable deviance calculated according to <xref ref-type="fig" rid="pbio.2001878.g006">Fig 6</xref> across all data in which coordinate frames were free to vary. Data for all tuning curves are shown as mean ± SEM firing rates. Dotted lines show the mean background activity measured in the 50 ms before stimulus presentation. <bold>e</bold>: Correlation coefficients between tuning curves in head and world CFs when aligned or free to vary as the animal foraged around the arena. Boxplots show median and inter-quartile range; symbols show coefficients for individual units. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955300.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955300.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g004" xlink:type="simple"/>
</fig>
<p>When animals moved freely through the arena, and head and world coordinate frames were thus dissociated, we observed units consistent with egocentric (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4a and 4b</xref>, <xref ref-type="supplementary-material" rid="pbio.2001878.s005">S5</xref> and <xref ref-type="supplementary-material" rid="pbio.2001878.s006">S6</xref> Figs) and allocentric hypotheses (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c and 4d</xref>, <xref ref-type="supplementary-material" rid="pbio.2001878.s007">S7</xref> and <xref ref-type="supplementary-material" rid="pbio.2001878.s008">S8</xref> Figs). For units consistent with the egocentric hypothesis, spatial receptive fields were more strongly modulated by sound angle in the head coordinate frame than the world coordinate frame. For the unit shown in <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4a</xref>, modulation depth values in the head and world coordinate frames were 28.3% and 10.1%, respectively. In <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4b</xref>, modulation depth was 49.0% in the head coordinate frame and 30.3% in the world coordinate frame. Furthermore, tuning curves for sounds plotted relative to the head remained consistent across head rotation but shifted systematically when plotted relative to the world (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4a and 4b</xref>: Right columns). Both outcomes are highly consistent with our simulation predictions (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1</xref>).</p>
<p>In addition to identifying head-centered spatial tuning across movement, we also found units with spatial tuning that realized the predictions generated by the allocentric hypothesis. These units showed greater modulation depth to sound angle in the world coordinate frame than the head coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c and 4d</xref>, <xref ref-type="supplementary-material" rid="pbio.2001878.s007">S7</xref> and <xref ref-type="supplementary-material" rid="pbio.2001878.s008">S8</xref> Figs): For putative allocentric units, modulation depths for tuning curves were 21.2% and 13.4% in the world and head coordinate frames, respectively, for the unit shown in <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c</xref> and 12.7% and 10.1%, respectively, for the unit shown in <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4d</xref>. For allocentric units, spatial tuning in the world coordinate frame was robust to head rotation, whereas tuning curves expressed relative to the head were systematically shifted when mapped according to head direction (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c and 4d</xref>: Right column).</p>
</sec>
<sec id="sec004">
<title>Modulation depth across coordinate frames</title>
<p>To quantify the observations we made above and systematically compare spatial tuning in world and head coordinate frames, we calculated modulation depth for both tuning curves for each unit. We next asked if modulation depth observed in either head or world coordinate frames was greater than the residual modulation predicted by our earlier simulations (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3b</xref>). A linear regression model developed using simulated receptive fields was used to predict the magnitude of residual tuning for each coordinate frame from the animal’s behavior during the recording of each unit (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5a–5d</xref>). To describe the animal’s behavior across the relevant testing sessions for each neural recording, we calculated the standard deviation of head directions (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5a</xref>). A smaller standard deviation indicates a less uniform range of head-directions and when combined with our regression model (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5b</xref>) would predict higher residual modulation in both coordinate frames. Thus, for a given standard deviation, we could use linear regression to obtain a predicted confidence interval for the residual modulation in head and world coordinate frames arising from allocentric or egocentric tuning, respectively (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5c</xref>). The observed modulation values were calculated for each unit as the ratio of modulation depth in one coordinate frame divided by the other coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5d</xref>), and significance was attributed when test values exceeded the confidence interval of residual modulation.</p>
<fig id="pbio.2001878.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Modulation depth across coordinate frames.</title>
<p><bold>a-d</bold>: Workflow illustrating the use of animal behavior (a: summarized using the standard deviation of head directions during neural testing, σ) and linear regression models (b: see also <xref ref-type="fig" rid="pbio.2001878.g003">Fig 3</xref>) to generate confidence intervals (CIs) for residual modulation (c) that were compared to observed modulation depth values (d), normalized relative to the alternative coordinate frame. Blue vertical lines in (b) show the σ value in (a). <bold>e-f</bold>: Normalized modulation depth observed for each spatially tuned unit compared against the mean residual modulation predicted from behavior in head (e) or world (f) coordinate frames. Bonferroni-corrected statistical criterion (<italic>p</italic> = 5.43 x 10<sup>−4</sup>) is denoted by α. <bold>g</bold>: Modulation depth for all spatially modulated units (<italic>n</italic> = 92) compared in world (MD<sub>World</sub>) and head coordinate frames (MD<sub>Head</sub>) during exploration. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955342.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955342.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g005" xlink:type="simple"/>
</fig>
<p>Across all spatially tuned units, modulation in the head coordinate frame was significantly greater than predicted for 87/92 units (94.6%) (<italic>p</italic> &lt; 0.05, <xref ref-type="fig" rid="pbio.2001878.g005">Fig 5e</xref>); modulation in the world coordinate frame was significant for 19/92 units (20.7%, <xref ref-type="fig" rid="pbio.2001878.g005">Fig 5f</xref>). For 14/92 units (15.2%), modulation depth was significantly greater than expected in both coordinate frames. When Bonferroni corrected for multiple (<italic>n</italic> = 92, α = 5.43 x 10<sup>−4</sup>) comparisons, these numbers dropped to 69/92 units (75%) for modulation in the head coordinate frame, none of which were additionally modulated in a world coordinate frame, and 6/92 units (6.5%) for modulation in the world coordinate frame—none of which showed significant head-centered modulation. With this more-conservative statistical threshold, modulation depths were not significantly greater than expected in either coordinate frame for the remaining 17/92 units (18.5%). Together these observations suggest that response types occupy a continuum from purely egocentric to purely allocentric. The existence of units with significant modulation in both coordinate frames with a less-conservative statistical cutoff, or no significant modulation with corrected threshold, may indicate mixed spatial sensitivity comparable with other reports in auditory cortex [<xref ref-type="bibr" rid="pbio.2001878.ref023">23</xref>].</p>
<p>A key prediction from our simulations with both uniform head-directions (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1</xref>) and actual head-directions (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3a</xref>) was that modulation depth would be greater in the coordinate frame that was relevant for neural activity than the irrelevant coordinate frame (i.e., Head &gt; World for egocentric; World &gt; Head for allocentric). Having demonstrated that modulation within both co-ordinate frames was greater than expected based on the non-uniform sampling of head direction, we compared the modulation depth across coordinate frames for all spatially tuned units (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5g</xref>). For 76/92 units (82.6%), we observed greater modulation depth in the head coordinate frame than the world coordinate frame, indicating a predominance of egocentric tuning and a minority of units (16/92, 17.4%) in which allocentric tuning was strongest.</p>
</sec>
<sec id="sec005">
<title>General linear modelling to define egocentric and allocentric populations</title>
<p>Our analysis of modulation depth indicated the presence of both egocentric and allocentric representations in auditory cortex but also highlighted that the analysis of modulation depth alone was sometimes unable to resolve the coordinate frame in which units encoded sound location. To calculate modulation depth requires that we discretize sound location into distinct angular bins, average neural responses across trials, and thus ignore single trial variation in firing rates. GLMs potentially offer a more-sensitive method, as they permit the analysis of single trial data and allow us to treat sound angle as a continuous variable. We considered two models, which either characterized neural activity as a function of sound source angle relative to the head (GLM<sub>HEAD</sub>) or in the world (GLM<sub>WORLD</sub>). For all units for which at least one GLM provided a statistically significant fit (relative to a constant model, analysis of deviance; <italic>p</italic> &lt; 0.05, 91/92 units), we compared model performance using the Akaike information criterion (AIC)[<xref ref-type="bibr" rid="pbio.2001878.ref024">24</xref>] for model selection. In accordance with the modulation depth analysis, the majority of units were better modelled by sound angle relative to the head than world (72/91 units; 79.1%; 4 animals), consistent with egocentric tuning. However, we also observed a smaller number of units (19/91 units; 20.9%; 3 animals) whose responses were better modelled by sound angle in the world and thus showed stronger representation of allocentric sound location.</p>
<p>To visualize GLM performance and explore egocentric and allocentric tuning further, we plotted a normalized metric of the deviance value usually used to assess model fit. Here, we defined <bold>model fit</bold> as the proportion of explainable deviance (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6a</xref>) in which a test model (e.g., GLM<sub>WORLD</sub>) is considered in the context of GLMs that have no variable predictors of neural activity (a constant model) or use sound angle in both coordinate frames as predictors (a full model). This normalization step is critical in comparing model fit across units, as deviance values alone are unbounded. In contrast, model fit is limited from 0% (indicating the sound angle provides little information about the neuron’s response) to 100% (indicating the sound angle in one coordinate frame accounts for the neuron’s response as well as sound angles in both frames). While the units we recorded formed a continuum in this space, for the purpose of further analysis we defined egocentric and allocentric units according to the coordinate frame (head/world) that provided the best model fit as determined by the AIC above.</p>
<fig id="pbio.2001878.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g006</object-id>
<label>Fig 6</label>
<caption>
<title>General linear modelling of spatial sensitivity.</title>
<p><bold>a</bold>: Calculation of model fit for sound angle relative to the head or world. Raw deviance values were normalized as a proportion of explainable deviance; the change in deviance between a constant and a full model. <bold>b</bold>: Model fit comparisons for all units when the animal was free to move through the arena. <bold>c</bold>: Model preference that indicates the distribution of units across the diagonal line of equality in (b). <bold>d-e</bold>: Model fit and model preference for data when the head and world coordinate frames were aligned. The grey background in (e) shows the distribution of model preference in the freely varying condition for reference. <bold>f</bold>: Change in model fit between freely moving and aligned states for egocentric and allocentric units in head and world coordinate frames. <bold>g-h</bold>: Model fit and model preference for freely moving data when speaker identity was shuffled. Data points in (g) show for each unit the median model fit averaged across 1,000 shuffles. <bold>i</bold>: The change in model fit between unshuffled and shuffled data. <bold>j-k</bold>: Model fit and model preference for freely moving data when the animal’s head direction was shuffled. Data points in (j) show for each unit median model fit averaged across 1,000 shuffles. <bold>l</bold>: The change in model fit between unshuffled and shuffled data. Asterisks with horizontal bars in (f), (i), and (l) indicate significant interactions (two-way anova on change in model fit with shuffle) between coordinate frame (head/world) and unit type (egocentric/allocentric) (<italic>p</italic> &lt; 0.001). Asterisks/n.s. for each bar represent significant/non-significant effects of shuffle on model fit in specific coordinate frames and for specific unit types (red/blue; <italic>t</italic> test, <italic>p</italic> &lt; 0.05). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955360.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955360.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g006" xlink:type="simple"/>
</fig>
<p>Using a GLM-based analysis, we predicted that egocentric units would have a high percentage of the full model fit by sound angles relative to the head and a low model fit for sound angles relative to the world and that allocentric units would show the reciprocal relationship. To test these predictions, we generated a <bold>model preference</bold> score; the model fit for sound angles relative to the head minus the model fit for sound angles in the world. Accordingly, negative values of model preference should identify allocentric units, while positive values should indicate egocentric units. Neurons in which both sound angles relative to the world and head provide high model fit values may represent sounds in intermediary or mixed coordinate frames and would have model preference scores close to zero, as would neurons in which we were unable to disambiguate coordinate frame preference due to non-uniform head angle distributions.</p>
<p>In the space defined by model fit for sound angles relative to the head and world (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6b</xref>), units clustered in opposite areas, supporting the existence of both egocentric and allocentric representations. This clustering was also evident in the model preference scores, which showed a bimodal distribution (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6c</xref>). Repeating this analysis on data in which the head and world coordinate frames were aligned (due to the animals’ position at the center of the speaker ring) demonstrated that model fit values for head and world coordinate frames became more similar, and model preference scores were centered around zero (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6d and 6e</xref>). When we compared the change in model fit with alignment (two-way anova), this was reflected as a significant interaction between coordinate frame (head or world) and unit type (egocentric or allocentric, determined by the coordinate frame that provided best model fit using the AIC, <italic>F</italic><sub>1,178</sub> = 130.1, <italic>p</italic> = 5.71 x 10<sup>−23</sup>). Post hoc comparison confirmed that model fit in the head coordinate frame decreased significantly for egocentric units (Bonferroni corrected for multiple comparisons, <italic>p</italic> = 5.04 x 10<sup>−5</sup>) and increased significantly for allocentric units (<italic>p</italic> = 4.36 x 10<sup>−4</sup>). In contrast, in the world coordinate frame, alignment led to a significant increase in model fit for egocentric units (<italic>p</italic> = 2.22 x 10<sup>−20</sup>) and a non-significant decrease in model fit for allocentric units (<italic>p</italic> = 0.155).</p>
<p>We performed two additional control analyses on the freely moving dataset: firstly, we randomly shuffled the speaker identity while maintaining the same information about the animal’s head direction. Randomizing the speaker identity should affect the ability to model both egocentric and allocentric neurons, and we would therefore predict that model fits for spatially relevant coordinate frames would be worse, and model preference scores would tend to zero (i.e., shuffling would shift model preference scores in the negative direction for egocentric units and the positive direction for allocentric units). As predicted, shuffling speaker identity eliminated clustering of egocentric and allocentric units in the space defined by model fit (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6g</xref>) and led to opposing effects on model preference (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6h and 6i</xref>): Model fit scores for egocentric and allocentric units were both affected by shuffling speaker identity but in opposite directions (unit x coordinate frame interaction, <italic>F</italic><sub>1, 178</sub> = 227.4, <italic>p</italic> = 1.22 x 10<sup>−33</sup>). For egocentric units, the model fit for sound angle relative to the head declined significantly with shuffling (<italic>p</italic> = 7.44 x 10<sup>−41</sup>), while fit for sound angle in the world did not change significantly (<italic>p</italic> = 0.271). For allocentric units, model fit for sound angle in the world declined significantly (<italic>p</italic> = 1.29 x 10<sup>−8</sup>) but was not significantly different in the head coordinate frame (<italic>p</italic> = 0.35). When shuffling stimulus angle (averaging across 1,000 shuffles), we identified 12/19 (63.2%) allocentric and 63/72 (87.5%) egocentric units with model preference scores beyond the 97.5% limits of the shuffled distribution.</p>
<p>Secondly, we shuffled information about the animal’s head direction while maintaining information about speaker identity. This should cause model fit values to decline for sound angle relative to the head for egocentric units and should therefore result in egocentric units shifting their model preference scores towards zero. For allocentric units, the model fit for sound location in the world should be maintained, and we would not predict a systematic change in model preference. This was indeed the case (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6j–6l</xref>; interaction between coordinate frame and unit type: <italic>F</italic><sub>1, 178</sub> = 216.7, <italic>p</italic> = 1.35 x 10<sup>−32</sup>): For egocentric units shuffling head direction significantly reduced model fit in the head coordinate frame (<italic>p</italic> = 1.40 x 10<sup>−27</sup>) and increased model fit in the world coordinate frame (<italic>p</italic> = 2.20 x 10<sup>−31</sup>). For allocentric units, shuffling head direction did not significantly affect model fit in either head (<italic>p</italic> = 0.211) or world coordinate frames (<italic>p</italic> = 0.178).</p>
</sec>
<sec id="sec006">
<title>Egocentric and allocentric units—Population characteristics</title>
<p>For egocentric units that encoded sound location in the head coordinate frame, it was possible to characterize the full extent of tuning curves in 360° around the head (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5a and 5b</xref>), despite our speaker array only spanning 180°. This was possible because the animal’s head direction varied continuously across 360° and so removed the constraints on measurement of spatial tuning imposed by the range of speaker angles used. Indeed, we were able to extend our approach further to characterize super-resolution tuning functions with an angular resolution more precise than the interval between speakers (<xref ref-type="fig" rid="pbio.2001878.g007">Fig 7a and 7b</xref> and <xref ref-type="supplementary-material" rid="pbio.2001878.s009">S9 Fig</xref>). Together these findings show it is possible to use changes in head direction to recover the spatial tuning of individual units with greater detail than would be possible if the subject’s head position and direction remained constant relative to the sound sources. Egocentric units shared spatial receptive field properties typical of previous studies [<xref ref-type="bibr" rid="pbio.2001878.ref015">15</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref016">16</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref025">25</xref>]: units predominantly responded most strongly to contralateral space (<xref ref-type="fig" rid="pbio.2001878.g007">Fig 7c</xref>) with broad tuning widths (<xref ref-type="fig" rid="pbio.2001878.g007">Fig 7e and 7f</xref>) that typify auditory cortical neurons. We also found similar, if slightly weaker, spatial modulation when calculating modulation depth according to Ref. [<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>] (<xref ref-type="fig" rid="pbio.2001878.g007">Fig 7d</xref>).</p>
<fig id="pbio.2001878.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Egocentric unit characteristics.</title>
<p><bold>a-b</bold>: Spatial tuning of an example egocentric unit at multiple angular resolutions. Data are shown as mean ± SEM firing rate plotted in Cartesian (a) or polar (b) coordinates. Triangle indicates the preferred location of unit. Inset (b) illustrates the corresponding head direction onto which spatial tuning can be super-imposed. <bold>c</bold>: Preferred location of all egocentric units (<bold>n</bold> = 72) in left and right auditory cortex. <bold>d</bold>: Modulation depth calculated according to [<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>] across 360° for units in both hemispheres. <bold>e-f</bold>: Tuning width (e) and equivalent rectangular receptive field (ERRF) width (f) for all units. Triangle indicates the preferred location, modulation depth, tuning width, and ERRF of the example unit in (a). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955366.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955366.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g007" xlink:type="simple"/>
</fig>
<p>For allocentric units, we observed a similar contralateral tuning bias in preferred location (<xref ref-type="fig" rid="pbio.2001878.g008">Fig 8a</xref>) to egocentric units and that allocentric units had relatively low modulation depths (<xref ref-type="fig" rid="pbio.2001878.g008">Fig 8b</xref>). These tuning features may not be surprising, as an allocentric receptive field could presumably fall anywhere within or beyond the arena and might therefore be poorly sampled by circular speaker arrangements. If the tuning curves measured here were in fact sampling a more complex receptive field that related to a world-centered coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g001">Fig 1b</xref>), then we would predict that the receptive fields would be correspondingly noisier. Allocentric and egocentric units were recorded at similar cortical depths (<xref ref-type="fig" rid="pbio.2001878.g008">Fig 8c</xref>) and on the same cortical penetrations; egocentric units were recorded on 9 of 13 electrodes (69.2%) on which we also identified allocentric units (<xref ref-type="fig" rid="pbio.2001878.g008">Fig 8d</xref>).</p>
<fig id="pbio.2001878.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Allocentric unit characteristics.</title>
<p><bold>a</bold>: Preferred location of all allocentric units (<italic>n</italic> = 19) in left and right auditory cortex. <bold>b</bold>: Modulation depth calculated across 180° for units in both hemispheres. <bold>c</bold>: Comparison of cortical depth at which egocentric and allocentric units were recorded. Ferret auditory cortex varies in thickness between 1.5 mm and 2 mm, and electrode depths were confirmed histologically (<xref ref-type="supplementary-material" rid="pbio.2001878.s002">S2 Fig</xref>). <bold>d</bold>: Number of cortical penetrations on which we recorded only egocentric units, only allocentric units, or a combination of both (mixed). All 92 spatially tuned units were recorded on 27 unique electrodes, with recorded units being distributed throughout cortex as the electrodes were descended. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955369.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955369.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>Timing of spatial information</title>
<p>Our findings suggested the existence of egocentric and allocentric tuning in auditory cortex. As these descriptions were functionally defined, we hypothesized that differences between allocentric and egocentric tuning should only arise after stimulus presentation. To test this, we analyzed the time course of unit activity in a moving 20 ms window and compared model fit and model preference of egocentric and allocentric units (defined based on the AIC analysis above) using cluster-based statistics to assess significance [<xref ref-type="bibr" rid="pbio.2001878.ref026">26</xref>]. Model fit for sound angles relative to the head was greater for egocentric than allocentric units only between 5 ms and 44 ms after stimulus onset (<xref ref-type="fig" rid="pbio.2001878.g009">Fig 9a</xref>, <italic>p</italic> = 0.001996). Model fit for sound angles in the world was greater for allocentric than egocentric units only between 6 ms and 34 ms after stimulus (<xref ref-type="fig" rid="pbio.2001878.g009">Fig 9b</xref>, <italic>p</italic> = 0.001996). Model preference diverged only in the window between 4 ms and 43 ms after stimulus onset (<xref ref-type="fig" rid="pbio.2001878.g009">Fig 9c</xref>, <italic>p</italic> = 0.001996). We observed no differences between egocentric and allocentric units before stimulus onset or when coordinate frames were aligned (<xref ref-type="supplementary-material" rid="pbio.2001878.s010">S10 Fig</xref>). Thus, the differences between egocentric and allocentric units reflected a stimulus-evoked effect that was only observed when head and world coordinate frames were free to vary.</p>
<fig id="pbio.2001878.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Egocentric and allocentric tuning over time.</title>
<p><bold>a</bold>: Model fit for predicting neural activity from sound angles relative to the head. <bold>b</bold>: Model fit for predicting neural activity from sound angles in the world. <bold>c</bold>: Model preference. Data are shown as mean ± SEM for egocentric and allocentric unit populations. Black lines indicate periods of statistical significance (cluster-based unpaired <italic>t</italic> test, <italic>p</italic> &lt; 0.05). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955372.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955372.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Population representations of space</title>
<p>We next asked how auditory cortical neurons behaved as a population when spatial tuning was compared across head directions. In contrast to individual units, population activity more closely reflects the large-scale signals observed in human studies using EEG to distinguish coordinate frame representations [<xref ref-type="bibr" rid="pbio.2001878.ref011">11</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref012">12</xref>]. To form populations, we took the unweighted mean of normalized firing rates from all units recorded from left (<italic>n</italic> = 64) or right (<italic>n</italic> = 28) hemispheres and compared tuning curves measured at different head directions. As would be expected from the predominance of egocentric units, we found that tuning curves for both left and right auditory cortical populations were consistent within the head coordinate frame but not the world coordinate frame (<xref ref-type="fig" rid="pbio.2001878.g010">Fig 10</xref>). Thus, the allocentric units we find here were sufficiently rare as to be masked in overall population readouts of spatial tuning, potentially accounting for conflicting findings of coordinate frame representations from EEG recordings.</p>
<fig id="pbio.2001878.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Auditory cortical tuning.</title>
<p>Population tuning curves plotted across head direction for mean (±SEM) normalized response of all units in left and right auditory cortex; filled triangles indicate sound angle of maximum response at each head direction. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955222.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955222.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g010" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>Distance modulation of cortical neurons and spatial tuning</title>
<p>Studying auditory processing in moving subjects allowed us to resolve coordinate frame ambiguity so that we could determine the spaces in which neurons represent sound location. However, recording in freely moving subjects also made it possible to go beyond angular measurements of the source location and address how neurons represented the distance of sound sources. Though often overlooked, distance is a critical component of egocentric models of neural tuning, as the acoustic cues indicating sound location such as inter-aural level differences (ILDs) change as sound sources approach the head. For distant sound sources (typically &gt; 1 m), ILDs are small relative to distance-related attenuation of sound; however, ILDs become much larger as sounds approach the head, and source-receiver distance decreases [<xref ref-type="bibr" rid="pbio.2001878.ref027">27</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref028">28</xref>]. Neurons must therefore accommodate for distance-dependent cues to accurately represent sound location across changes in head position. In our study, the distance between head and sound source ranged from 10 cm (the minimum distance imposed by the arena walls) to 40 cm, with only 3.37% of stimuli (mean across 57 test sessions) presented at greater distances (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2g</xref>), and thus stimuli were likely to produce large ILDs [<xref ref-type="bibr" rid="pbio.2001878.ref027">27</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref028">28</xref>].</p>
<p>Spatial tuning was observed at all distances studied in both egocentric (<xref ref-type="fig" rid="pbio.2001878.g011">Fig 11a</xref>) and allocentric units (<xref ref-type="fig" rid="pbio.2001878.g011">Fig 11b</xref>); however, modulation depth increased with distance for egocentric units (ANOVA, F<sub>2,216</sub> = 3.45, <italic>p</italic> = 0.0334). Pairwise post hoc comparisons showed that modulation depth was largest for sounds at the greatest distances (<xref ref-type="fig" rid="pbio.2001878.g011">Fig 11c</xref>), though the only significant difference was found for sounds presented 20 cm to 30 cm and 30 cm to 40 cm away (<italic>t</italic><sub><italic>72</italic></sub> = −3.54, <italic>p</italic> = 0.0279). In contrast, modulation depth did not change significantly with distance for allocentric units (<xref ref-type="fig" rid="pbio.2001878.g011">Fig 11d</xref>, F<sub>2, 57</sub> = 0.962, <italic>p</italic> &gt; 0.1).</p>
<fig id="pbio.2001878.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Spatial tuning across distance.</title>
<p><bold>a-b</bold>: Tuning curves of an egocentric (a) and allocentric (b) unit obtained with sound sources at varying distances from the animal’s head. Bar plots show the number of stimuli and modulation depth for each tuning curve. <bold>c-d</bold>: Distributions of modulation depth measured across distance for egocentric and allocentric units. Asterisk indicates significant pair-wise comparison (Tukey-Kramer corrected, <bold>p</bold> &lt; 0.05). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955375.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955375.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Speed modulation of cortical neurons and spatial tuning</title>
<p>Changes in head position and direction also allowed us to investigate how speed of head movement (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2f</xref>) affected neural activity. Movement is known to affect auditory processing in rodents [<xref ref-type="bibr" rid="pbio.2001878.ref029">29</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref031">31</xref>], but its effects on spatial representations of sound location and also on auditory cortical processing in other phyla such as carnivores remain unknown. Here, we presented click sounds for which dynamic acoustic cues would be negligible, and thus, we could isolate the effects of head movement on neural activity.</p>
<p>To address auditory cortical processing, first, we asked how many of our recorded units (regardless of auditory responsiveness or spatial modulation) showed baseline activity that varied with speed. For each unit, we took all periods of exploration (excluding the 50 ms after each click onset) and calculated the speed of the animal at the time of each action potential. We then discretized the distribution of spike-triggered speeds to obtain spike counts as a function of speed and normalized spike counts by the duration over which each speed range was measured. This process yielded a speed-rate function for baseline activity (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12a</xref>). We then fitted an exponential regression curve to each function (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12b</xref>) and plotted the correlation (R<sup>2</sup>) and regression coefficients (β) of each curve to map the magnitude and direction of association between speed and baseline activity (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12c</xref>). Across the recorded population, we saw both positive and negative correlations representing units for which firing rate increased or decreased, respectively, with speed. However, a significantly larger proportion of the population increased firing rate with speed across all units (<italic>t</italic> test versus 0; <italic>t</italic><sub>308</sub> = 3.77, <italic>p</italic> = 1.97 x 10<sup>−4</sup>). This was also true if we considered only sound-responsive units (<italic>t</italic><sub>267</sub> = 5.15, <italic>p</italic> = 5.17 x 10<sup>−7</sup>) or only spatially tuned units (<italic>t</italic><sub>91</sub> = 4.12, <italic>p</italic> = 8.41 x 10<sup>−5</sup>).</p>
<fig id="pbio.2001878.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2001878.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Speed-related auditory cortical activity and sensory processing.</title>
<p><bold>a</bold>: An example calculation of speed-related modulation of baseline firing of 1 unit using reverse correlation. <bold>b</bold>: An example speed-firing rate function summarized using regression (β) and correlation (R<sup>2</sup>) coefficients for the same unit as (a). <bold>c</bold>: Population distribution of regression and correlation coefficients showing the predominance of units with increasing speed-rate functions (β &gt; 0). <bold>d-e</bold>: Peri-stimulus time histogram of sound-evoked responses for units that were modulated by speed: In (d), activity was enhanced as speed increased from 1 cm s<sup>−1</sup> to 7 cm s<sup>−1</sup> and decreased thereafter. In (e), firing decreased with increasing speeds, although speed-related modulation was smaller relative to sound-evoked activity than (d). <bold>f</bold>: Box plots showing distributions (median and inter-quartile range) of evoked firing rates in response to clicks across speed for all sound-responsive units. <bold>g</bold>: Population distribution of regression coefficients (β) and model fit (analysis of deviance <italic>p</italic> values) for all sound-responsive units. Units for which speed was not a significant predictor of neural activity (<italic>p</italic> &gt; 0.001) are denoted in grey. <bold>h</bold>: Spatial tuning curve for one unit for clicks presented at different head-movement speeds. <bold>i-k</bold>: Change in preferred location (i), modulation depth (j), and min/max firing rates (k) of egocentric and allocentric units as a function of speed. Data for d-e and h-k are shown as mean ± SEM. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955378.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955378.v1</ext-link>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.g012" xlink:type="simple"/>
</fig>
<p>We also observed speed-related modulation of sound-evoked responses in individual units (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12d and 12e</xref>). For each individual unit, we characterized the relationship between head speed and single trial firing rates (averaged over the 50 ms post-stimulus onset) using a GLM that measured both the strength (analysis of deviance, <italic>p</italic> value) and direction (model coefficient, β) of association. Positive β values indicated an increase in firing rate with increasing speed, whereas negative β values indicated a fall in firing rate with increasing speed. Thus, the direction of the relationship between firing rate and speed was summarized by the model coefficient, allowing us to map the effects of movement speed across the population (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12f</xref>). For 199/268 sound-responsive units (74.3%), speed was a significant predictor of firing rate (analysis of deviance versus a constant model, <italic>p</italic> &lt; 0.001); however, the mean coefficient value for movement-sensitive units did not differ significantly from zero (<italic>t</italic><sub>199</sub> = 0.643, <italic>p</italic> = 0.521). This suggests that the sound-responsive population was evenly split between units that increased or decreased firing with speed. We noted that a significantly greater proportion of spatially modulated units (74/92, 80.4%) had sound-evoked responses that were sensitive to speed than units that were either not spatially modulated or for which we had insufficient sample sizes to test spatial modulation (125/176, 71.0%, Chi-squared test, <italic>χ</italic><sup><italic>2</italic></sup> = 3.96, <italic>p</italic> &lt; 0.05). For those 74 spatially modulated and speed-sensitive units, coefficients were mostly larger than zero (mean ± SEM = 2.73 x 10<sup>−5</sup> ± 1.53 x 10<sup>−5</sup>); however, this effect was not statistically significant (<italic>t</italic><sub>73</sub> = 1.80, <italic>p</italic> = 0.076). For the remaining speed-sensitive units, the mean coefficients was closer to zero (mean ± SEM = −5.02 x 10<sup>−6</sup> ± 1.49 x 10<sup>−5</sup>).</p>
<p>Lastly, we asked if speed affected spatial tuning. Spatial tuning could be observed at all speeds of movement in spatially tuned units (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12h</xref>), and the preferred locations of units did not vary systematically with speed (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12i</xref>). For neither egocentric nor allocentric units was there a significant effect of speed in an ANOVA on preferred locations (egocentric: <italic>F</italic><sub>5, 432</sub> = 0.53, <italic>p</italic> = 0.753; allocentric: <italic>F</italic><sub>5, 108</sub> = 1.53, <italic>p</italic> = 0.188). However, we did observe significant changes in modulation depth (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12j</xref>) both for egocentric (<italic>F</italic><sub>5, 432</sub> = 4.91, <italic>p</italic> = 0.0002) and allocentric units (<italic>F</italic><sub>5, 108</sub> = 5.09, <italic>p</italic> = 0.0003), indicating that spatial modulation was greater when the head was moving fastest. Change in modulation depth resulted from both a gradual suppression in minimum firing rates and enhancement in maximum firing rates with speed (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12k</xref>). However, none of these changes in minimum or maximum firing rate were significant in comparisons across speed (ANOVA with speed as the main factor, <italic>p</italic> &gt; 0.5), indicating that it was only through the aggregative change in responses to both preferred and non-preferred locations that modulation depth increased with speed.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Discussion</title>
<p>Here we have shown that by measuring spatial tuning curves in freely moving animals, it is possible to demonstrate the coordinate frames in which neurons define sound location. For the majority of spatially sensitive auditory cortical neurons, we found egocentric tuning that confirms the broadly held but untested assumption that within the central auditory pathway, sound location is represented in head-centered coordinates. We also identified a small number of units with allocentric tuning, whose responses were spatially locked to sound location in the world, suggesting that multiple coordinate frames are represented at the auditory cortical level. These units were consistently identified across different analyses, observed in several subjects, and could be dissociated from simultaneously recorded egocentric receptive fields during the same behavioral sessions. Finally, we explored the dependence of neural activity and spatial tuning on sound source distance and the speed of head movement, demonstrating that both factors can modulate firing rates and spatial tuning in auditory cortex.</p>
<p>Our results show that an animal’s movement can be successfully tracked to measure head-centered egocentric tuning during behavior. Although we used speakers placed at 30° intervals across a range of 180°, we were nonetheless able to characterize spatial tuning of egocentric units around the full circumference of the head (i.e., <xref ref-type="fig" rid="pbio.2001878.g007">Fig 7b</xref>). This illustrates the practical benefit of using moving subjects to characterize head-centered spatial tuning, as the animal’s head rotation generates the additional variation in sound angle relative to the head necessary to fully map azimuthal tuning with a reduced number of sound sources. Furthermore, as the animal’s head direction was continuous, the stimulus angle was also continuous, and thus, it was possible to measure spatial tuning at finer resolutions than that of the speaker ring from which stimuli were presented. In contrast to egocentric tuning, our ability to map allocentric receptive fields was limited by the speaker arrangement that only sparsely sampled world coordinates (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2a</xref>). This may, in part, explain the low number of allocentric units in our population, and denser sampling of the world may reveal unseen allocentric tuning—for example, in the 50.5% (94/186) of units in which we recorded sound-evoked responses that were not spatially modulated. While a full 360° speaker ring may offer a minor improvement in sampling density, the radial organization of the ring remains a suboptimal design for sampling rectangular or irregular environments. To fully explore the shape of allocentric receptive fields will require denser, uniform speaker grids or lattices in environments through which animals can move between sound sources.</p>
<p>A notable property of egocentric units was the relationship between modulation depth of spatial tuning and distance, which was absent in allocentric units. This distance sensitivity may largely be driven by changes in ILDs, although other auditory and non-auditory cues can affect distance perception [<xref ref-type="bibr" rid="pbio.2001878.ref032">32</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref036">36</xref>]. However, modulation depth in our study was lowest for proximal sounds when ILDs would be larger and when other cues such as inter-aural time differences remain relatively constant [<xref ref-type="bibr" rid="pbio.2001878.ref027">27</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref028">28</xref>]. Localization of nearby sound sources (&lt; 1 m) is possible for ferrets and humans [<xref ref-type="bibr" rid="pbio.2001878.ref037">37</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref038">38</xref>], though within the range of distances we considered here, angular error of azimuthal localization in humans increases slightly as sounds approach the head [<xref ref-type="bibr" rid="pbio.2001878.ref037">37</xref>]. Thus, our findings are consistent with human psychophysical performance but suggest that larger localization cues may not always produce better sound localization by neurons in auditory cortex. In the future, it will be critical to validate our findings for sound sources at greater distances where sound localization has been more widely studied.</p>
<p>In addition to recording many egocentric units, we also recorded a small number of allocentric units, supporting the idea that auditory cortex represents sound location in multiple coordinate frames [<xref ref-type="bibr" rid="pbio.2001878.ref023">23</xref>] and parses an auditory scene into distinct objects [<xref ref-type="bibr" rid="pbio.2001878.ref039">39</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref040">40</xref>]. We believe this is the first study to look for world-centered encoding of sound locations at the cellular level. Thus, the units we recorded, while small in number, reflect a novel spatial representation in the auditory system.</p>
<p>A key question is where allocentric units reside in cortex: egocentric and allocentric units were located on the same electrodes and cortical depths in the primary and posterior regions of auditory cortex in which we recorded. However, the low density of electrodes in our arrays, and their placement over a low-frequency border, prevented us from mapping the precise tonotopic boundaries necessary to attribute units to specific cortical subfields [<xref ref-type="bibr" rid="pbio.2001878.ref041">41</xref>]. Future use of denser sampling arrays may enable cortical mapping in behaving animals and thus precise localization of allocentric units. We targeted the low-frequency reversal between primary and posterior auditory cortex, as these areas are likely to be sensitive to inter-aural timing cues, and the animals involved in this work were also trained to discriminate non-spatial features of low-frequency sounds in another study [<xref ref-type="bibr" rid="pbio.2001878.ref042">42</xref>]. Posterior regions may correspond to part of the “what” pathway in auditory processing, whereas the anterior ectosylvian gyrus may correspond to the “where” pathway in which spatial tuning is more extensive [<xref ref-type="bibr" rid="pbio.2001878.ref043">43</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref044">44</xref>]. It is thus likely that spatial tuning to the coordinate frames represented in our population (in which only 49% of units were spatially sensitive) may be more ubiquitous in anterior regions of ferret auditory cortex. Indeed, given that sensorimotor and cross-modal coordinate frame transformations are a key feature of activity in parietal cortex [<xref ref-type="bibr" rid="pbio.2001878.ref010">10</xref>], it is likely that allocentric representations exist beyond auditory cortex.</p>
<p>While the observation of allocentric receptive fields in tonotopic auditory cortex is novel, the existence of allocentric representations has been predicted by behavioral studies in humans [<xref ref-type="bibr" rid="pbio.2001878.ref006">6</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref013">13</xref>]. Furthermore, coordinate transformations occur elsewhere in the auditory system [<xref ref-type="bibr" rid="pbio.2001878.ref023">23</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref045">45</xref>], and behavioral movements can influence auditory subcortical and cortical processing [<xref ref-type="bibr" rid="pbio.2001878.ref029">29</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref030">30</xref>]. Perhaps most importantly, vestibular signals are already integrated into auditory processing at the level of the cochlea nucleus [<xref ref-type="bibr" rid="pbio.2001878.ref046">46</xref>], allowing the distinction between self and source motion [<xref ref-type="bibr" rid="pbio.2001878.ref022">22</xref>]. Auditory-vestibular integration, together with visual, proprioceptive, and motor corollary discharge systems, provides a mechanism through which changes in head direction can partially offset changes in acoustic input during movement to create allocentric representations. At present, it is unclear whether allocentric representations require self-generated movement, and it will be interesting to test if world-centered tuning is present if head direction is systematically varied in stationary animals.</p>
<p>It is also unclear how head position is also integrated into auditory processing. Positional information within the medial temporal lobe (or its carnivore equivalent) is a leading candidate given the connections between entorhinal and auditory cortex [<xref ref-type="bibr" rid="pbio.2001878.ref047">47</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref048">48</xref>]; however, the functional interactions between these areas and their potential contributions to allocentric processing have yet to be addressed. Another critical question relates to the visual (or other sensory) cues that animals may use to orient in the world and define allocentric representations. Given that head and place cells remap in different settings [<xref ref-type="bibr" rid="pbio.2001878.ref049">49</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref050">50</xref>] and that auditory cortex receives somatosensory and visual information through multiple pathways [<xref ref-type="bibr" rid="pbio.2001878.ref048">48</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref051">51</xref>], it will be interesting to determine if changes in visual/tactile environment affect tuning of allocentric receptive fields, and if so, what environmental features anchor auditory processing.</p>
<p>Variation in the animal’s head position with movement also allowed us to study the effects of head movement speed on auditory processing and spatial tuning. In contrast to other studies in freely behaving animals that reported movement-related suppression of activity [<xref ref-type="bibr" rid="pbio.2001878.ref029">29</xref>–<xref ref-type="bibr" rid="pbio.2001878.ref031">31</xref>], we found that neurons tended to fire more strongly when the head moved faster (<xref ref-type="fig" rid="pbio.2001878.g012">Fig 12</xref>). One reason for this difference may lie in the behavior measured: Other investigators have covered a diverse range of actions including locomotion in which both the head and body move, and self-generated sounds are more likely. We only considered the head speed of an animal and did not track the body position that would distinguish head movements from locomotion (which was relatively limited given the size of the animal and the arena). It is thus likely that much of the variation in speed we observed was a product of head movement during foraging without locomotion and thus with relatively little self-generated sound. The behavior of our subjects may therefore present different requirements for auditory–motor integration that result in distinct neural effects.</p>
<p>We also observed that spatial modulation was also greater when the animal was moving faster, which may be consistent with the sharpening of tuning curves during behavioral engagement [<xref ref-type="bibr" rid="pbio.2001878.ref015">15</xref>]. While sharpening of engagement-related spatial tuning was linked to a reduction in spiking responses at untuned locations, we observed nonsignificant decreases in peak and minimum firing rates (which together increased modulation depth), suggesting that the mechanisms underlying speed-related modulation of spatial tuning may be subtly different. At the acoustic level, faster movements provide larger dynamic cues [<xref ref-type="bibr" rid="pbio.2001878.ref002">2</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref003">3</xref>] that improve sound localization abilities in humans [<xref ref-type="bibr" rid="pbio.2001878.ref003">3</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref052">52</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref053">53</xref>] and may explain the increase in modulation depth of units at greater speeds observed here.</p>
<p>In summary, we recorded spatial tuning curves in auditory cortex of freely moving animals to resolve coordinate frame ambiguity. We demonstrated many egocentric units representing location relative to the head and a small number of allocentric units representing sound location relative to the world. We also studied the role of distance and speed in auditory cortical processing. Together, our findings illustrate that auditory cortical processing of sound space may extend to multiple coordinate frames and spatial dimensions such as azimuth and distance, as well as non-auditory variables such as speed of movement.</p>
</sec>
<sec id="sec012" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec013">
<title>Ethics statement</title>
<p>All experimental procedures were approved by local ethical review committees (Animal Welfare and Ethical Review Board) at University College London and The Royal Veterinary College, University of London and performed under license from the UK Home Office (Project License 70/7267) and in accordance with the Animals (Scientific Procedures) Act 1986.</p>
</sec>
<sec id="sec014">
<title>Simulated spatial receptive fields</title>
<sec id="sec015">
<title>Egocentric</title>
<p>Egocentric tuning described the relationship between spike probability (<italic>P</italic>) and sound source angle relative to the midline of the subject’s head (<italic>θ</italic><sub><italic>HS</italic></sub>) and was simulated in Matlab (MathWorks) using a Gaussian function:
<disp-formula id="pbio.2001878.e001">
<alternatives>
<graphic id="pbio.2001878.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>In the example shown in <xref ref-type="fig" rid="pbio.2001878.g001">Fig 1a</xref>, parameters (a = 1.044, b = 0°, and c = 75.7°) were determined by manual fitting to find values for which egocentric and allocentric tuning matched qualitatively. The theta domain was between ±180° binned at 1° intervals, and distance of sound sources was not included in the simulation.</p>
</sec>
<sec id="sec016">
<title>Allocentric</title>
<p>Allocentric tuning describes the relationship between neural output (reported here as spike probability; <italic>P</italic>) and sound source position within the world measured in Cartesian (<italic>x</italic>, <italic>y</italic>) coordinates. Spatial tuning was simulated as the dot product of spike probability vectors returned from functions defined separately for positions on the <italic>x</italic>- and <italic>y</italic>-axes:
<disp-formula id="pbio.2001878.e002">
<alternatives>
<graphic id="pbio.2001878.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>In <xref ref-type="fig" rid="pbio.2001878.g001">Fig 1b</xref>, logistic probability functions were used for both dimensions:
<disp-formula id="pbio.2001878.e003">
<alternatives>
<graphic id="pbio.2001878.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
<disp-formula id="pbio.2001878.e004">
<alternatives>
<graphic id="pbio.2001878.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
With μ = 1,000 mm and s = 400 mm for the <italic>x</italic>-axis, and μ = 0 mm and s = 1,000 mm for the <italic>y</italic>-axis. For both axes, spike probability vectors were generated for domains between ±1,500 mm binned at 2-mm intervals.</p>
</sec>
<sec id="sec017">
<title>Stimulus presentation, head pose, and movement</title>
<p>The position and orientation of the subject’s head within the world was described as a coordinate frame transform composed of a translation vector between origins (indicating the head position) and a rotation matrix between axes (indicating the head direction). Stimuli were presented on each time-step of the simulation from each speaker in a ring at 10° intervals, 1,000 mm from the origin of the world coordinate frame. As the simulation was deterministic, each stimulus was presented to static simulations only once to calculate the response. When simulating motion, a “pirouette” trajectory was constructed in which the subject’s head translated on a circular trajectory (radius = 50 mm; angular speed = 30° per time-step) while simultaneously rotating (angular speed = 10° per time-step) for 7,200 stimulus presentations (<xref ref-type="supplementary-material" rid="pbio.2001878.s013">S2 Video</xref>). For each stimulus presentation, the stimulus angle was calculated relative to both the midline of the head and the vertical axis of the arena (<xref ref-type="supplementary-material" rid="pbio.2001878.s011">S11 Fig</xref>). Simulation responses were quantized in 1° bins.</p>
</sec>
</sec>
<sec id="sec018">
<title>Animals</title>
<p>Subjects were 5 pigmented female ferrets (1–5 years old) trained in a variety of psychophysical tasks that did not involve the stimuli presented or the experimental chamber used in the current study. Each ferret was chronically implanted with Warp-16 microdrives (Neuralynx, MT), housing 16 independently moveable tungsten microelectrodes (WPI Inc., FL) positioned over middle and posterior fields of left or right auditory cortex. Details of the surgical procedures for implantation and confirmation of electrode position are described elsewhere [<xref ref-type="bibr" rid="pbio.2001878.ref054">54</xref>]. The weight and water consumption of all animals were measured throughout the experiment. Regular otoscopic examinations were performed to ensure the cleanliness and health of ferrets’ ears.</p>
<p>Subjects were water-restricted prior to testing, during which time they explored the experimental arena to find freely available sources of water. On each day of testing, subjects received a minimum of 60 ml/kg of water either during testing or supplemented as a wet mash made from water and ground high-protein pellets. Subjects were tested in morning and afternoon sessions on each day for up to 3 days in a week (i.e., a maximum of 6 consecutive testing sessions); on the remaining weekdays, subjects obtained water in performance of other psychophysical tasks. Test sessions lasted between 10 minutes and 50 minutes and were ended when the animal lost interest in exploring the arena.</p>
</sec>
<sec id="sec019">
<title>Experimental design and stimuli</title>
<p>In each test session, a ferret was placed within a D-shaped arena (<xref ref-type="fig" rid="pbio.2001878.g002">Fig 2a</xref>, rectangular section: 35 cm x 30 cm [width x length]; semi-circular section: 17.5 cm radius; 50 cm tall) with 7 speakers positioned at 30° intervals, 26 cm away from a central pillar from which the animal could find water. The periphery of the circular half of the arena was also fitted with spouts from which water could be obtained. Animals were encouraged either to explore the arena by delivery of water at all spouts, or to hold their head at the center spout by restricted water delivery at this location. The arena and speakers were housed within a sound-attenuating chamber lined with 45-mm sound-absorbing foam.</p>
<p>During exploration (<italic>n</italic> = 57 test sessions), clicks were presented from each speaker with random inter-stimulus intervals (250–500 ms). The instantaneous energy of clicks minimized dynamic cues, simplifying neural analysis and comparisons with other work on spatial encoding. Clicks were presented at 60 dB SPL when measured from the center of the arena using a measuring amplifier (Bruel &amp; Kjaer 2636). However, because sound level varied across the arena, we roved sound levels over a ±6 dB range to reduce changes in level arising from differences in position of the head within the sound field. The frequency response of each speaker (Visaton SC 5.9) was measured using golay codes [<xref ref-type="bibr" rid="pbio.2001878.ref055">55</xref>] and compensated for, to produce a flat spectral output between 20 Hz and 20 kHz. Stimulus location and water delivery were independent and subjects were not required to attend to stimuli in order to find water rewards. To avoid characterizing neural responses to the sound of solenoid control signals, stimulus presentation and water reward were delivered in separate, alternating time windows; water was delivered in a short period of 1 to 2 seconds when each solenoid was rapidly opened (100 ms duration) with a 10-second interval between delivery windows in which click stimuli were presented. Sessions typically lasted approximately 15–20 minutes (median = 16.5 minutes; range = 6.15–48.0 minutes) in which several thousand stimuli could be presented (median = 1984; range = 304–3937).</p>
</sec>
<sec id="sec020">
<title>Head tracking</title>
<p>During exploration of the experimental arena, the animal’s head position and orientation were tracked using 2 LEDs (red and green) placed along the midline of the head and recorded using a RV2 video acquisition system (TDT) sampling at 30 frames per second and synchronized with the electrophysiology recording hardware. For each video frame, the red and green LED positions were identified in Matlab from a weighted image in which the channel color of the target LED was positively weighted and all other channels negatively weighted. Each LED position was then taken as the center of the largest cluster of pixels containing the maximum weighted value. To maximize the frame rate of the camera, we recorded with a low-exposure time (10–20 ms). Lower exposure also improved LED identification by reducing signal intensity in the background of each frame.</p>
<p>In cases in which a LED went out of view of the camera (usually due to the roll or pitch of the head, or the recording cables obscuring the LED), the maximum weighted value identified as the LED would be a random point within the arena, resulting either from a weak reflection or image noise. To remove such data, we set a minimum-intensity threshold based on the distribution of maximum values in weighted images across all frames. In cases in which the LED intensity failed to match the specified threshold, the LED position was noted as missing. To compensate for missing data, we estimated LED positions across runs of up to a maximum of 10 frames (333 ms) using spline interpolation. Longer runs of missing data were discarded.</p>
<p>We then mapped each LED position in the image (<italic>M</italic>) into the behavioral arena to give the new position <italic>N</italic> using the transformation:
<disp-formula id="pbio.2001878.e005">
<alternatives>
<graphic id="pbio.2001878.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>M</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
Where <italic>T</italic> is the translation between the origin of the image coordinate frame (i.e., pixel [0,0]) and the origin of the arena coordinate frame (the center of the arena). And, <italic>R</italic> is the three-dimensional rotation matrix describing the rotation between the arena and image coordinate frames.<italic>T</italic> was obtained by manually identifying the pixel closest to the center of the arena (i.e., the equidistant point between all speakers) in a calibration image captured at the start of each test session. <italic>R</italic> was estimated from singular value decomposition using the position and distance between known points in the arena (also identified manually from each calibration image). Here, we estimated a 3D rotation matrix to take into account the position of the camera relative to the arena (i.e., above the arena rather than below). All coordinate frames were represented using the right-hand rule (i.e., positive values for counter-clockwise rotation about the <italic>z</italic>-axis) to ensure consistency with the <italic>atan2</italic> function.</p>
<p>The animal’s head position <inline-formula id="pbio.2001878.e006"><alternatives><graphic id="pbio.2001878.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> was then calculated as the midpoint between the LEDs within the arena and was used to define the origin of the head-centered coordinate frame (<xref ref-type="supplementary-material" rid="pbio.2001878.s011">S11 Fig</xref>). The animal’s head direction (<italic>θ</italic><sub><italic>HA</italic></sub>) was calculated from the two argument arctangent function (atan2) of the vector between LEDs that defined the midline (<italic>y</italic>-axis) of the head-centered coordinate frame <inline-formula id="pbio.2001878.e007"><alternatives><graphic id="pbio.2001878.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The <italic>z</italic>-axis was undefined by the tracking system, as we only measured 2 points (red and green LEDs) with a single camera; this led to ambiguity about the pitch and roll of the head. To compensate for this deficiency, we assumed that when LEDs were visible, the <italic>xy</italic> plane of the head always matched the plane of the arena floor and that the <italic>z</italic>-axis of the head was orthogonal to this plane and oriented towards the camera. Such assumptions are justified by the properties of the tracking system—as the head rolls or pitches away from the assumed conditions, it becomes impossible to identify both LEDs within the image due to the limited angular range of the each diode. Therefore, tracking was impossible (in which case data was discarded) in the same conditions in which our assumptions became untenable.</p>
<p>By using the frame times recorded on the device, it was possible to create a time series of head position and direction within the arena that could be compared to the spiking pattern of neurons. We used the inter-frame interval and change in position of the head origin, smoothed with a 9-point Hann window to calculate the speed of head movement.</p>
</sec>
<sec id="sec021">
<title>Neural recording</title>
<p>Neural activity in auditory cortex was recorded continuously throughout exploration. On each electrode, voltage traces were recorded using TDT System III hardware (RX8 and RZ2) and OpenEx software (Tucker-Davis Technologies, Alachua, FL) with a sample rate of 25 kHz. For extraction of action potentials, data were band-pass filtered between 300 Hz and 5,000 Hz, and motion artifacts were removed using a decorrelation procedure applied to all voltage traces recorded from the same microdrive in a given session [<xref ref-type="bibr" rid="pbio.2001878.ref056">56</xref>]. For each channel within the array, we identified candidate events as those with amplitudes between −2.5 and −6 times the RMS value of the voltage trace and defined waveforms of events using a 32-sample window centered on threshold crossings. Waveforms were then interpolated (128 points) and candidate events combined across sessions within a test run for spike sorting. Waveforms were sorted using MClust (A.D. Redish, University of Minnesota, <ext-link ext-link-type="uri" xlink:href="http://redishlab.neuroscience.umn.edu/MClust/" xlink:type="simple">http://redishlab.neuroscience.umn.edu/MClust/</ext-link>) so that candidate events were assigned to either single unit, multi-unit clusters, or residual hash clusters. Single units were defined as those with less than 1% of inter-spike intervals shorter than 1 millisecond. In total, 331 units were recorded, including 116 single units (35.1%).</p>
</sec>
<sec id="sec022">
<title>Tracking unit identity across recording sessions</title>
<p>Through the experiment, electrodes were descended progressively deeper into cortex at intervals of 50–100 μm to ensure sampling of different neural populations. At most recording sites, we tested animals on multiple sessions (1–6 sessions) across several (1–3) consecutive days. Conducting test sessions over multiple days makes possible the recording of different units at a single recording site over time (i.e., through electrode drift, gliosis, etc.). To constrain our analysis to units with a consistent identity, we tracked the waveform of recorded units across sessions within a test run. Our rationale was that a unit should have a constant waveform shape across test sessions, and any differences in waveform shape should be small relative to differences in the waveforms of units measured on other electrodes or at other depths by the same electrode. Thus, for one test session at a given recording site, we calculated the Euclidean distance matrix between the mean waveform recorded on that session (W<sub>Test</sub>) and the mean waveform recorded on each additional session at the same recording site (D<sub>Test</sub>). We also calculated the distances between W<sub>Test</sub> and the mean waveform recorded for every session at different recording sites (D<sub>Control</sub>). D<sub>Control</sub> provided null distributions for waveform distances between pairs of neurons known to have separate identities (due to the spatial separation between electrodes at recording sites [&gt;50 μm in depth, &gt;500 μm laterally]). For a given waveform, we then calculated the statistical probability of observing distances between test waveform and waveforms <bold>at the same recording site</bold>, given the distribution of distances between test waveforms and waveforms <bold>at other recording sites</bold>. For waveforms exceeding statistical significance (<italic>t</italic> test; <italic>p</italic> &lt; 0.05, Bonferroni corrected for the number of sessions conducted at the recording site), we concluded that the same neuron or neuronal population was recorded.</p>
<p>For runs of test sessions, we took the longest continuous run for which waveform distances were significantly smaller than expected by chance. The majority of units tested more than once could be tracked over all sessions tested (72.4%: 126/174 units), although the number of neurons tracked fell off with time.</p>
</sec>
<sec id="sec023">
<title>Data analysis</title>
<p>During exploration, we characterized sound-evoked responses from auditory cortical units. Each click stimulus and the concomitant neural response could be related to controlled variables determined by the experimental design and measured variables observed from head tracking. Controlled variables were the position of the sound source relative to the arena and sound source level in dB SPL, whereas measured variables were the position and direction of the head relative to the arena, as well as head speed. Controlled and measured variables were combined to determine several experimental parameters: Stimulus position relative to the head was calculated as the vector <inline-formula id="pbio.2001878.e008"><alternatives><graphic id="pbio.2001878.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pbio.2001878.e009">
<alternatives>
<graphic id="pbio.2001878.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula>
Where <inline-formula id="pbio.2001878.e010"><alternatives><graphic id="pbio.2001878.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the vector from arena origin to head origin and <inline-formula id="pbio.2001878.e011"><alternatives><graphic id="pbio.2001878.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the vector from arena origin to the sound source. Stimulus angle relative to the head (<italic>θ</italic><sub><italic>HS</italic></sub>) was calculated by subtracting head direction in the arena (<italic>θ</italic><sub><italic>HA</italic></sub>) from the stimulus angle relative to the origin of the head coordinate frame:
<disp-formula id="pbio.2001878.e012">
<alternatives>
<graphic id="pbio.2001878.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> atan</mml:mtext><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula></p>
<p>The distance between head and stimulus was calculated as the magnitude of <inline-formula id="pbio.2001878.e013"><alternatives><graphic id="pbio.2001878.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>To calculate sound level at the head, sound pressure at the head was calculated by multiplying the pressure measured at the arena origin during calibration (<italic>p</italic><sub><italic>A</italic></sub>) by the ratio of the distances from arena origin to speaker <inline-formula id="pbio.2001878.e014"><alternatives><graphic id="pbio.2001878.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and from head to speaker <inline-formula id="pbio.2001878.e015"><alternatives><graphic id="pbio.2001878.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:
<disp-formula id="pbio.2001878.e016">
<alternatives>
<graphic id="pbio.2001878.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
Where P<sub>H</sub> and P<sub>A</sub> are sound pressures at the head and center of the arena expressed in pascals, and sound level is expressed in dB SPL:
<disp-formula id="pbio.2001878.e017">
<alternatives>
<graphic id="pbio.2001878.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mtext> </mml:mtext><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula></p>
<p>Sound level was calibrated to 60 dB SPL (0.02 Pa) at the center of the arena.</p>
<p>For each variable, we calculated the value at the time of stimulus presentation (i.e., with a lag of 0 ms) and contrasted these values with the spiking responses of neurons. To study encoding of stimulus features (both measured and control variables) by neurons, single trial responses of individual units were summarized as the mean firing rate 0–50 ms after stimulus onset. This window was sufficiently long to characterize the response of units (<xref ref-type="supplementary-material" rid="pbio.2001878.s005">S5</xref> and <xref ref-type="supplementary-material" rid="pbio.2001878.s007">S7</xref> Figs) but also short enough that changes in head direction and position during the analysis window were small (<xref ref-type="supplementary-material" rid="pbio.2001878.s003">S3 Fig</xref>). Sound-responsive units (268/336) were first identified as those with evoked firing rates that differed significantly from background activity measured in the 50 ms before stimulus presentation (GLM analysis of deviance using Poisson distributions and log link function; <italic>p</italic> ≤ 0.05).</p>
<p>Spatially tuned units were then identified using sound-evoked responses collected with the animal at the center of the arena, with the head and world coordinate frames in approximate alignment. For a stimulus presentation to be included in this analysis, the animal’s head origin was required to be within 5 cm of a point 2.5 cm behind the arena center (<xref ref-type="supplementary-material" rid="pbio.2001878.s004">S4 Fig</xref>). The 2.5-cm offset was applied to provide an approximate account for the distance between the animal’s snout and head center. Head direction was also required to be within ±15° of the midline of the arena (i.e., the line of symmetry of the arena, so that the animal was facing forward). Sound-evoked responses under these constraints were then fitted with a GLM (Poisson distribution; log link function) with sound source angle relative to the head binned in 30° intervals as predictor. To ensure adequate data for statistical testing, units were only assessed if responses were recorded for ≥5 stimulus presentations in each angular bin (186/268 units). Units for which sound source angle significantly reduced model deviance (χ<sup>2</sup> distribution, <italic>p</italic> ≤ 0.05) were classed as spatially tuned (92/186 units). While this approach may not identify all spatially informative neurons (some of which may signal sound location by spike timing rather than rate [<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref057">57</xref>] or that may be tuned only to sounds behind the head that were not sampled by speakers in the aligned condition), it identified a subpopulation of spatially sensitive units on which further analysis could be performed.</p>
<p>To calculate spatial tuning curves, analysis was expanded to include all head positions and directions recorded. To calculate world-based tuning curves, mean firing rate across trials (0–50 ms) was plotted for each sound source angle relative to the arena. For head-based tuning curves, sound source angle relative to the head was binned at 30° intervals and mean spike rate plotted as a function of the bin center angle. To study super-resolution tuning of egocentric units (<xref ref-type="fig" rid="pbio.2001878.g005">Fig 5a and 5b</xref>), the bin width used to calculate curves was reduced to 20°, 10°, 5°, 2°, or 1°. To compare spatial tuning of egocentric units with other studies, we also calculated preferred location, modulation depth, tuning width, and equivalent rectangular receptive field (ERRF) width for spatial tuning curves calculated relative to the head across 360°, according to the methods used for awake cats [<xref ref-type="bibr" rid="pbio.2001878.ref015">15</xref>,<xref ref-type="bibr" rid="pbio.2001878.ref018">18</xref>]. For allocentric units, we calculated preferred location and modulation depth for across sound location in the world.</p>
</sec>
<sec id="sec024">
<title>Modulation depth analysis</title>
<p>For each unit, we calculated the depth of spatial modulation for tuning curves in each coordinate frame. Unless otherwise stated, modulation depth (MD) was calculated as:
<disp-formula id="pbio.2001878.e018">
<alternatives>
<graphic id="pbio.2001878.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mrow><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext> min</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mo>×</mml:mo><mml:mtext> </mml:mtext><mml:mn>100</mml:mn></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
Where <italic>x</italic> is the vector of firing rates in response to sounds located in each 30° bin between ±90° either of the world or head coordinate frame.</p>
<p>Modulation depth could also be calculated for simulated neurons using the same equation but with <italic>x</italic> being a vector of spike probabilities. This approach allowed us to calculate modulation depth for simulated allocentric and egocentric units when presented with sounds during observed animal movement (<xref ref-type="fig" rid="pbio.2001878.g003">Fig 3</xref>). In simulations, modulation depth could be calculated in head and world coordinate frames that were either relevant or irrelevant for neural activity, depending on whether the simulation was allocentric or egocentric. We termed modulation depth in the irrelevant coordinate frame <bold>residual modulation</bold> when expressed as a ratio of modulation in the represented coordinate frame:
<disp-formula id="pbio.2001878.e019">
<alternatives>
<graphic id="pbio.2001878.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>For allocentric simulations, the world was relevant and the head was irrelevant; whereas, for egocentric simulations, the head was relevant and the world was irrelevant.</p>
<p>For each test session in which we observed animal behavior, we compared the relationship between the residual modulation calculated during simulations of both allocentric and egocentric units, with the standard deviation of the head directions (σ, <xref ref-type="fig" rid="pbio.2001878.g003">Fig 3</xref>). We fitted a linear regression model to this relationship that was subsequently used to test if observed modulation depth values of real units were significantly greater than the residual modulation expected from the animal’s behavior. The linear regression model was fitted using the <bold>fitlm</bold> function in Matlab (R2015a). For each observed unit, we measured the standard deviation of head angles during neural testing (σ) and, together with the regression models, predicted the 95% or 99.95% (post-Bonferroni correction for 92 units) confidence interval of residual modulation values in the head and in the world coordinate frame. Prediction was performed in Matlab using the <bold>predict</bold> function with the most conservative options selected (simultaneous confidence bounds and prediction for new observations rather than fitted mean values) to give the widest confidence intervals and thus minimize the probability of false positives. If the observed modulation depth of a unit in a particular coordinate frame exceeded the upper bound of the confidence interval for that frame, we identified it as significantly modulated.</p>
</sec>
<sec id="sec025">
<title>GLMs</title>
<p>To compare the relationship between single trial firing rates and sound source angles in the head and world coordinate frames, we fitted the average firing rate on each trial (0–50 ms) with a generalized linear regression model (Matlab, <bold>fitglm</bold> function: Poisson distribution, log link function). For both sound source angles relative to the head and relative to the world, we measured the deviance of models fitted separately with each parameter (D<sub>Test</sub>). The AIC [<xref ref-type="bibr" rid="pbio.2001878.ref024">24</xref>] was used to compare test models and distinguish allocentric and egocentric units as those for which sound source angle relative to the world or head, respectively, provided the best model. For all but 1 unit that was excluded from further analysis, either sound source angle relative to the world or head improved model fit compared to a constant model (analysis of deviance; Bonferroni correction for 2 comparisons; <italic>p</italic> &lt; 0.05).</p>
<p>To visualize GLM performance (<xref ref-type="fig" rid="pbio.2001878.g006">Fig 6</xref>), we calculated <bold>model fit</bold> for each unit and coordinate frame as:
<disp-formula id="pbio.2001878.e020">
<alternatives>
<graphic id="pbio.2001878.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
Where D<sub>const</sub> was the deviance resulting from a constant model, and D<sub>Full</sub> was the deviance resulting from a full linear model that included both sound source angle relative to the head and relative to the world. We compared the model fit for data obtained when the head and world coordinate frames were free to vary and when we restricted data to cases when the head and world coordinate frames were aligned (see above). We also repeated our analysis but with speaker identity or head direction information randomly shuffled between stimulus presentations prior to calculation of spatial tuning curves. Shuffling was repeated 1,000 times and, across shuffles, we calculated the median model fit in head and world coordinate frames. Here we used the median rather than mean of the permuted values across shuffles, which were not always normally distributed. To test the effect of shuffle on model fit of all units, we performed a 2-way anova on change in model fit with shuffle, with coordinate frame (head/world) and unit class (egocentric/allocentric). Post hoc tests were conducted on change in model fit versus 0 (<italic>t</italic> test) with Bonferroni correction for multiple comparisons.</p>
<p>For each analysis in which we calculated model fit, we also calculated <bold>model preference</bold> as:
<disp-formula id="pbio.2001878.e021">
<alternatives>
<graphic id="pbio.2001878.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pbio.2001878.e021" xlink:type="simple"/>
<mml:math display="block" id="M21">
<mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>F</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula></p>
<p>Model preference could thus vary between −100% (better fit for neural data based on sound angle in the world) and +100% (better fit for neural data based on sound angle relative to the head).</p>
<p>For time-based comparison of model performance, we reduced the time over which firing rates were considered (from 50 ms to 20 ms) and repeated the analysis with a window offset by −60 ms to 90 ms after stimulus presentation that moved with a 2-ms interval. Model fit and preference values for allocentric and egocentric units were compared across time using a nonparametric cluster-based statistical test [<xref ref-type="bibr" rid="pbio.2001878.ref026">26</xref>], implemented in Matlab through the FieldTrip toolbox [<xref ref-type="bibr" rid="pbio.2001878.ref058">58</xref>].</p>
</sec>
</sec>
<sec id="sec026">
<title>Supporting information</title>
<supplementary-material id="pbio.2001878.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Allocentric tuning can generate a wide variety of tuning curve shapes and tuned locations.</title>
<p><bold>a</bold>, Variation in tuning shape generated by allocentric simulations tuned to different regions of space and implemented using different spike probability distributions: Simulations (left to right) based on logistic (red), Gaussian (black), Laplace (blue) and uniform (grey) probability density functions. <bold>b</bold>, Shifts in tuning simulated by varying world-based position of peak spike probability. Plots generated from logistic probability density functions. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955390.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955390.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Electrode positions in auditory cortex.</title>
<p><bold>a</bold>, Electrode positions in two representative animals in auditory cortex. Green and black indicate electrodes within or outside (excluded from analysis) Auditory Cortex respectively. Labels show suprasylvian sulcus (sss) and pseudosylvian sulcus (pss). <bold>b</bold>, Cresyl-violet stained electrode tracks.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Head movement during neural analysis window.</title>
<p><bold>a</bold>, Time-displacement plots showing the change in head position across space (black), in each axis of the arena (red and blue) and change in head direction (green) during the 50 millisecond window over which spike rates were analysed. Data shown as mean ± s.e.m with grey lines showing behavior of every session (n = 57). <bold>b</bold>, Box plots of median displacement 50 milliseconds after click onset in each analysis. <bold>c-d</bold> Same as a-b but plotted for 2 milliseconds after click onset. Data available at <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/S3_Fig/4955393" xlink:type="simple">https://figshare.com/articles/S3_Fig/4955393</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Control data showing spatial tuning when head and world coordinate frames aligned.</title>
<p><bold>a</bold>, Distribution of head position (left), head directions (top right) and head speeds (bottom right, n = 57 test sessions, mean ± s.e.m.) for control data which has been filtered for positions at the center of the arena with the head facing forward. <bold>b</bold>, Spatial tuning functions calculated for sound source angle relative to the head and world aligned for filtered data (inset shows tuning for one unit, n = 275 stimuli). R<sup>2</sup> indicates correlation coefficient between functions calculated for each coordinate frame. Histogram shows distribution of correlation coefficients across all spatially tuned units. <bold>c</bold>, Population tuning functions expressed in head centered coordinate frame illustrating contralateral tuning bias for neurons recorded in left and right hemispheres (n = 64 and 28 respectively). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955399.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955399.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Example egocentric raster plot.</title>
<p>Raster plots showing spiking responses of an example egocentric unit (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4b</xref> in the main manuscript). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955402.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955402.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Additional examples of egocentric units.</title>
<p>Two additional example egocentric units in which spatial receptive fields are tuned to sound source location in the head coordinate frame. Data shown as in <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c</xref> of main text with line plots showing mean ± s.e.m. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955417.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955417.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Example allocentric raster plot.</title>
<p>Raster plots showing spiking responses of an example allocentric unit (<xref ref-type="fig" rid="pbio.2001878.g004">Fig 4c</xref> in the main manuscript). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955405.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955405.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Additional examples of allocentric units.</title>
<p>Two additional example allocentric units in which spatial receptive fields are tuned to sound source location in the world coordinate frame. Data shown as in <xref ref-type="fig" rid="pbio.2001878.g004">Fig 4</xref> of main text with line plots showing mean ± s.e.m. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955414.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955414.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Measuring super-resolution spatial tuning.</title>
<p>Spatial tuning curves for three examples of egocentric units for which tuning was observed in 360° around the head and with resolution greater than the interval between speakers (30°). Typically units showed reliable tuning around the head at resolutions as low as 5°, equivalent to using a speaker ring with 72 speakers positioned at equal intervals around the animal’s head in the azimuthal plane. As we used only 7 speakers over a range of 180°, this reflects an order of magnitude (x 10) increase in spatial resolution. Data shown as mean ± s.e.m firing rate in Cartesian or polar coordinates. Scale bars indicate firing rates of 20 Hz. Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955408.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955408.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Temporal differences when coordinate frames were aligned.</title>
<p>Population distinctions when head and world coordinate frames are aligned (in contrast to the main text where frames were free to vary). <bold>a</bold>, Model fit for predicting neural activity from sound angles relative to the head. <bold>b</bold>, Model fit for predicting neural activity from sound angles in the world. <bold>c</bold>, Model preference. Data shown as mean ± s.e.m. for egocentric (blue) and allocentric (red) populations. Populations did not differ significantly at any time point (cluster based paired t-test, p&lt; 0.05). Data available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.4955411.v1" xlink:type="simple">https://doi.org/10.6084/m9.figshare.4955411.v1</ext-link>.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>Geometric definitions.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s012" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s012" xlink:type="simple">
<label>S1 Video</label>
<caption>
<title>Low exposure video example of LEDs during motion tracking (left) that allows projection of head position and direction within the experimental arena (right).</title>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2001878.s013" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pbio.2001878.s013" xlink:type="simple">
<label>S2 Video</label>
<caption>
<title>Pirouette trajectory used to obtain a uniform distribution of head directions in simulations.</title>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Prof Jan Schnupp, Prof David McAlpine, Dr Daniel Bendor, and Dr Peter Keating for discussions of this work.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>AIC</term>
<def><p>Akaike information criterion</p></def>
</def-item>
<def-item><term>CF</term>
<def><p>coordinate frame</p></def>
</def-item>
<def-item><term>CI</term>
<def><p>confidence interval</p></def>
</def-item>
<def-item><term>dB SPL</term>
<def><p>decibel sound pressure level</p></def>
</def-item>
<def-item><term>EEG</term>
<def><p>electroencephalography</p></def>
</def-item>
<def-item><term>ERRF</term>
<def><p>equivalent rectangular receptive field</p></def>
</def-item>
<def-item><term>GLM</term>
<def><p>general linear model</p></def>
</def-item>
<def-item><term>ILD</term>
<def><p>inter-aural level difference</p></def>
</def-item>
<def-item><term>LED</term>
<def><p>light-emitting diode</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2001878.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroeder</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Radman</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Scharfman</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lakatos</surname> <given-names>P</given-names></name>. <article-title>Dynamics of Active Sensing and perceptual selection</article-title>. <source>Curr Opin Neurobiol</source>. <year>2010</year>;<volume>20</volume>(<issue>2</issue>):<fpage>172</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2010.02.010" xlink:type="simple">10.1016/j.conb.2010.02.010</ext-link></comment> <object-id pub-id-type="pmid">20307966</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallach</surname> <given-names>H</given-names></name>. <article-title>The role of head movements and vestibular and visual cues in sound localization</article-title>. <source>Journal of Experimental Psychology</source>. <year>1940</year>;<volume>27</volume>(<issue>4</issue>):<fpage>339</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrett</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Noble</surname> <given-names>W</given-names></name>. <article-title>The contribution of head motion cues to localization of low-pass noise</article-title>. <source>Percept Psychophys</source>. <year>1997</year>;<volume>59</volume>(<issue>7</issue>):<fpage>1018</fpage>–<lpage>26</lpage>. <object-id pub-id-type="pmid">9360475</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wightman</surname> <given-names>FL</given-names></name>, <name name-style="western"><surname>Kistler</surname> <given-names>DJ</given-names></name>. <article-title>Resolution of front-back ambiguity in spatial hearing by listener and source movement</article-title>. <source>J Acoust Soc Am</source>. <year>1999</year>;<volume>105</volume>(<issue>5</issue>):<fpage>2841</fpage>–<lpage>53</lpage>. <object-id pub-id-type="pmid">10335634</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brimijoin</surname> <given-names>WO</given-names></name>, <name name-style="western"><surname>Boyd</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Akeroyd</surname> <given-names>MA</given-names></name>. <article-title>The contribution of head movement to the externalization and internalization of sounds</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>(<issue>12</issue>):<fpage>e83068</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0083068" xlink:type="simple">10.1371/journal.pone.0083068</ext-link></comment> <object-id pub-id-type="pmid">24312677</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brimijoin</surname> <given-names>WO</given-names></name>, <name name-style="western"><surname>Akeroyd</surname> <given-names>MA</given-names></name>. <article-title>The moving minimum audible angle is smaller during self motion than during source motion</article-title>. <source>Front Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>273</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnins.2014.00273" xlink:type="simple">10.3389/fnins.2014.00273</ext-link></comment> <object-id pub-id-type="pmid">25228856</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brimijoin</surname> <given-names>WO</given-names></name>, <name name-style="western"><surname>Akeroyd</surname> <given-names>MA</given-names></name>. <article-title>The role of head movements and signal spectrum in an auditory front/back illusion</article-title>. <source>Iperception</source>. <year>2012</year>;<volume>3</volume>(<issue>3</issue>):<fpage>179</fpage>–<lpage>82</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1068/i7173sas" xlink:type="simple">10.1068/i7173sas</ext-link></comment> <object-id pub-id-type="pmid">23145279</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davison</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>DW</given-names></name>. <article-title>Simultaneous localization and map-building using active vision</article-title>. <source>Ieee T Pattern Anal</source>. <year>2002</year>;<volume>24</volume>(<issue>7</issue>):<fpage>865</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Castellanos</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Neira</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tardos</surname> <given-names>JD</given-names></name>. <article-title>Multisensor fusion for simultaneous localization and map building</article-title>. <source>Ieee T Robotic Autom</source>. <year>2001</year>;<volume>17</volume>(<issue>6</issue>):<fpage>908</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/70.976024" xlink:type="simple">10.1109/70.976024</ext-link></comment></mixed-citation></ref>
<ref id="pbio.2001878.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>YE</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name>. <article-title>A common reference frame for movement plans in the posterior parietal cortex</article-title>. <source>Nat Rev Neurosci</source>. <year>2002</year>;<volume>3</volume>(<issue>7</issue>):<fpage>553</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn873" xlink:type="simple">10.1038/nrn873</ext-link></comment> <object-id pub-id-type="pmid">12094211</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Altmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Wilczek</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kaiser</surname> <given-names>J</given-names></name>. <article-title>Processing of auditory location changes after horizontal head rotation</article-title>. <source>J Neurosci</source>. <year>2009</year>;<volume>29</volume>(<issue>41</issue>):<fpage>13074</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1708-09.2009" xlink:type="simple">10.1523/JNEUROSCI.1708-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19828820</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schechtman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Shrem</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Deouell</surname> <given-names>LY</given-names></name>. <article-title>Spatial localization of auditory stimuli in human auditory cortex is based on both head-independent and head-centered coordinate systems</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>(<issue>39</issue>):<fpage>13501</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1315-12.2012" xlink:type="simple">10.1523/JNEUROSCI.1315-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23015439</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Grootel</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Van Wanrooij</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Van Opstal</surname> <given-names>AJ</given-names></name>. <article-title>Influence of static eye and head position on tone-evoked gaze shifts</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>(<issue>48</issue>):<fpage>17496</fpage>–<lpage>504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5030-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5030-10.2011</ext-link></comment> <object-id pub-id-type="pmid">22131411</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Linear processing of spatial cues in primary auditory cortex</article-title>. <source>Nature</source>. <year>2001</year>;<volume>414</volume>(<issue>6860</issue>):<fpage>200</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35102568" xlink:type="simple">10.1038/35102568</ext-link></comment> <object-id pub-id-type="pmid">11700557</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>CC</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Auditory cortex spatial sensitivity sharpens during task performance</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>(<issue>1</issue>):<fpage>108</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2713" xlink:type="simple">10.1038/nn.2713</ext-link></comment> <object-id pub-id-type="pmid">21151120</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stecker</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Harrington</surname> <given-names>IA</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Location coding by opponent neural populations in the auditory cortex</article-title>. <source>PLoS Biol</source>. <year>2005</year>;<volume>3</volume>(<issue>3</issue>):<fpage>e78</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030078" xlink:type="simple">10.1371/journal.pbio.0030078</ext-link></comment> <object-id pub-id-type="pmid">15736980</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Guard</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Phan</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>TK</given-names></name>. <article-title>Correlation between the activity of single auditory cortical neurons and sound-localization behavior in the macaque monkey</article-title>. <source>J Neurophysiol</source>. <year>2000</year>;<volume>83</volume>(<issue>5</issue>):<fpage>2723</fpage>–<lpage>39</lpage>. <object-id pub-id-type="pmid">10805672</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mickey</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Representation of auditory space by cortical neurons in awake cats</article-title>. <source>J Neurosci</source>. <year>2003</year>;<volume>23</volume>(<issue>25</issue>):<fpage>8649</fpage>–<lpage>63</lpage>. <object-id pub-id-type="pmid">14507964</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malhotra</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Lomber</surname> <given-names>SG</given-names></name>. <article-title>Cortical control of sound localization in the cat: unilateral cooling deactivation of 19 cerebral areas</article-title>. <source>J Neurophysiol</source>. <year>2004</year>;<volume>92</volume>(<issue>3</issue>):<fpage>1625</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01205.2003" xlink:type="simple">10.1152/jn.01205.2003</ext-link></comment> <object-id pub-id-type="pmid">15331649</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Parsons</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Lanyon</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Akerman</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>GE</given-names></name>, <etal>et al</etal>. <article-title>An investigation of the role of auditory cortex in sound localization using muscimol-releasing Elvax</article-title>. <source>Eur J Neurosci</source>. <year>2004</year>;<volume>19</volume>(<issue>11</issue>):<fpage>3059</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.0953-816X.2004.03379.x" xlink:type="simple">10.1111/j.0953-816X.2004.03379.x</ext-link></comment> <object-id pub-id-type="pmid">15182314</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Kacelnik</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Bajo</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Lesions of the auditory cortex impair azimuthal sound localization and its recalibration in ferrets</article-title>. <source>J Neurophysiol</source>. <year>2010</year>;<volume>103</volume>(<issue>3</issue>):<fpage>1209</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00991.2009" xlink:type="simple">10.1152/jn.00991.2009</ext-link></comment> <object-id pub-id-type="pmid">20032231</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wigderson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Yarom</surname> <given-names>Y</given-names></name>. <article-title>Early multisensory integration of self and source motion in the auditory system</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2016</year>;<volume>113</volume>(<issue>29</issue>):<fpage>8308</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1522615113" xlink:type="simple">10.1073/pnas.1522615113</ext-link></comment> <object-id pub-id-type="pmid">27357667</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werner-Reiss</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Kelly</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Trause</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Underhill</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>. <article-title>Eye position affects activity in primary auditory cortex of primates</article-title>. <source>Curr Biol</source>. <year>2003</year>;<volume>13</volume>(<issue>7</issue>):<fpage>554</fpage>–<lpage>62</lpage>. <object-id pub-id-type="pmid">12676085</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>New Look at Statistical-Model Identification</article-title>. <source>Ieee T Automat Contr</source>. <year>1974</year>;<volume>Ac19</volume>(<issue>6</issue>):<fpage>716</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Recanzone</surname> <given-names>GH</given-names></name>. <article-title>Populations of auditory cortical neurons can accurately encode acoustic space across stimulus intensity</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2009</year>;<volume>106</volume>(<issue>14</issue>):<fpage>5931</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0901023106" xlink:type="simple">10.1073/pnas.0901023106</ext-link></comment> <object-id pub-id-type="pmid">19321750</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Oostenveld</surname> <given-names>R</given-names></name>. <article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title>. <source>J Neurosci Methods</source>. <year>2007</year>;<volume>164</volume>(<issue>1</issue>):<fpage>177</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jneumeth.2007.03.024" xlink:type="simple">10.1016/j.jneumeth.2007.03.024</ext-link></comment> <object-id pub-id-type="pmid">17517438</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brungart</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Rabinowitz</surname> <given-names>WM</given-names></name>. <article-title>Auditory localization of nearby sources. Head-related transfer functions</article-title>. <source>J Acoust Soc Am</source>. <year>1999</year>;<volume>106</volume>(<issue>3 Pt 1</issue>):<fpage>1465</fpage>–<lpage>79</lpage>. <object-id pub-id-type="pmid">10489704</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brungart</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Rabinowitz</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Durlach</surname> <given-names>NI</given-names></name>. <article-title>Auditory localization of a nearby point source</article-title>. <source>J Acoust Soc Am</source>. <year>1996</year>;<volume>100</volume>:<fpage>2593</fpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneider</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mooney</surname> <given-names>R</given-names></name>. <article-title>A synaptic and circuit basis for corollary discharge in the auditory cortex</article-title>. <source>Nature</source>. <year>2014</year>;<volume>513</volume>(<issue>7517</issue>):<fpage>189</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature13724" xlink:type="simple">10.1038/nature13724</ext-link></comment> <object-id pub-id-type="pmid">25162524</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williamson</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Hancock</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name>, <name name-style="western"><surname>Polley</surname> <given-names>DB</given-names></name>. <article-title>Locomotion and Task Demands Differentially Modulate Thalamic Audiovisual Processing during Active Search</article-title>. <source>Curr Biol</source>. <year>2015</year>;<volume>25</volume>(<issue>14</issue>):<fpage>1885</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2015.05.045" xlink:type="simple">10.1016/j.cub.2015.05.045</ext-link></comment> <object-id pub-id-type="pmid">26119749</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Xiong</surname> <given-names>XR</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Xiao</surname> <given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>6</issue>):<fpage>841</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3701" xlink:type="simple">10.1038/nn.3701</ext-link></comment> <object-id pub-id-type="pmid">24747575</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mershon</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>EL</given-names></name>. <article-title>Intensity and reverberation as factors in the auditory perception of egocentric distance</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1975</year>;<volume>18</volume>(<issue>6</issue>):<fpage>409</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/BF03204113" xlink:type="simple">10.3758/BF03204113</ext-link></comment></mixed-citation></ref>
<ref id="pbio.2001878.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Traer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>. <article-title>Statistics of natural reverberation enable perceptual separation of sound and space</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2016</year>;<volume>113</volume>(<issue>48</issue>):<fpage>E7856</fpage>–<lpage>E65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1612524113" xlink:type="simple">10.1073/pnas.1612524113</ext-link></comment> <object-id pub-id-type="pmid">27834730</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kopco</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name>. <article-title>Effect of stimulus spectrum on distance perception for nearby sources</article-title>. <source>J Acoust Soc Am</source>. <year>2011</year>;<volume>130</volume>(<issue>3</issue>):<fpage>1530</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.3613705" xlink:type="simple">10.1121/1.3613705</ext-link></comment> <object-id pub-id-type="pmid">21895092</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zahorik</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brungart</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Bronkhorst</surname> <given-names>AW</given-names></name>. <article-title>Auditory distance perception in humans: A summary of past and present research</article-title>. <source>Acta Acustica</source>. <year>2005</year>;<volume>91</volume>:<fpage>409</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kolarik</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Zahorik</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cirstea</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pardhan</surname> <given-names>S</given-names></name>. <article-title>Auditory distance perception in humans: a review of cues, development, neuronal bases, and effects of sensory loss</article-title>. <source>Atten Percept Psychophys</source>. <year>2016</year>;<volume>78</volume>(<issue>2</issue>):<fpage>373</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13414-015-1015-1" xlink:type="simple">10.3758/s13414-015-1015-1</ext-link></comment> <object-id pub-id-type="pmid">26590050</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brungart</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Durlach</surname> <given-names>NI</given-names></name>, <name name-style="western"><surname>Rabinowitz</surname> <given-names>WM</given-names></name>. <article-title>Auditory localization of nearby sources. II. Localization of a broadband source</article-title>. <source>J Acoust Soc Am</source>. <year>1999</year>;<volume>106</volume>(<issue>4 Pt 1</issue>):<fpage>1956</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">10530020</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wood</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Town</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Atilgan</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>GP</given-names></name>, <name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>. <article-title>Acute Inactivation of Primary Auditory Cortex Causes a Sound Localisation Deficit in Ferrets</article-title>. <source>PLoS ONE</source>. <year>2017</year>;<volume>12</volume>(<issue>1</issue>):<fpage>e0170264</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0170264" xlink:type="simple">10.1371/journal.pone.0170264</ext-link></comment> <object-id pub-id-type="pmid">28099489</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>YE</given-names></name>. <article-title>The what, where and how of auditory-object perception</article-title>. <source>Nat Rev Neurosci</source>. <year>2013</year>;<volume>14</volume>(<issue>10</issue>):<fpage>693</fpage>–<lpage>707</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn3565" xlink:type="simple">10.1038/nrn3565</ext-link></comment> <object-id pub-id-type="pmid">24052177</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref040"><label>40</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bregman</surname> <given-names>AS</given-names></name>. <source>Auditory scene analysis</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1990</year>.</mixed-citation></ref>
<ref id="pbio.2001878.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Functional organization of ferret auditory cortex</article-title>. <source>Cereb Cortex</source>. <year>2005</year>;<volume>15</volume>(<issue>10</issue>):<fpage>1637</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhi042" xlink:type="simple">10.1093/cercor/bhi042</ext-link></comment> <object-id pub-id-type="pmid">15703254</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Town</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>KC</given-names></name>, <name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>. <article-title>Neural correlates of perceptual constancy in Auditory Cortex</article-title>. <source>BioRxiv</source>. <year>2017</year>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/102889" xlink:type="simple">https://doi.org/10.1101/102889</ext-link></mixed-citation></ref>
<ref id="pbio.2001878.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Visual-auditory spatial processing in auditory cortical neurons</article-title>. <source>Brain Res</source>. <year>2008</year>;<volume>1242</volume>:<fpage>24</fpage>–<lpage>36</lpage>. <object-id pub-id-type="pmid">18407249</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Bajo</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Cortico-Cortical Connectivity Within Ferret Auditory Cortex</article-title>. <source>J Comp Neurol</source>. <year>2015</year>;<volume>523</volume>(<issue>15</issue>):<fpage>2187</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/cne.23784" xlink:type="simple">10.1002/cne.23784</ext-link></comment> <object-id pub-id-type="pmid">25845831</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groh</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Trause</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Underhill</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Inati</surname> <given-names>S</given-names></name>. <article-title>Eye position influences auditory responses in primate inferior colliculus</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>29</volume>(<issue>2</issue>):<fpage>509</fpage>–<lpage>18</lpage>. <object-id pub-id-type="pmid">11239439</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shore</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Roberts</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Langguth</surname> <given-names>B</given-names></name>. <article-title>Maladaptive plasticity in tinnitus—triggers, mechanisms and treatment</article-title>. <source>Nat Rev Neurol</source>. <year>2016</year>;<volume>12</volume>(<issue>3</issue>):<fpage>150</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrneurol.2016.12" xlink:type="simple">10.1038/nrneurol.2016.12</ext-link></comment> <object-id pub-id-type="pmid">26868680</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Insausti</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Herrero</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Witter</surname> <given-names>MP</given-names></name>. <article-title>Entorhinal cortex of the rat: cytoarchitectonic subdivisions and the origin and distribution of cortical efferents</article-title>. <source>Hippocampus</source>. <year>1997</year>;<volume>7</volume>(<issue>2</issue>):<fpage>146</fpage>–<lpage>83</lpage>. <object-id pub-id-type="pmid">9136047</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Budinger</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Heil</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Scheich</surname> <given-names>H</given-names></name>. <article-title>Functional organization of auditory cortex in the Mongolian gerbil (Meriones unguiculatus). III. Anatomical subdivisions and corticocortical connections</article-title>. <source>Eur J Neurosci</source>. <year>2000</year>;<volume>12</volume>(<issue>7</issue>):<fpage>2425</fpage>–<lpage>51</lpage>. <object-id pub-id-type="pmid">10947821</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muller</surname> <given-names>RU</given-names></name>, <name name-style="western"><surname>Kubie</surname> <given-names>JL</given-names></name>. <article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</article-title>. <source>J Neurosci</source>. <year>1987</year>;<volume>7</volume>(<issue>7</issue>):<fpage>1951</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">3612226</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fyhn</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hafting</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Treves</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Moser</surname> <given-names>EI</given-names></name>. <article-title>Hippocampal remapping and grid realignment in entorhinal cortex</article-title>. <source>Nature</source>. <year>2007</year>;<volume>446</volume>(<issue>7132</issue>):<fpage>190</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature05601" xlink:type="simple">10.1038/nature05601</ext-link></comment> <object-id pub-id-type="pmid">17322902</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Bajo</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>. <article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex</article-title>. <source>Cereb Cortex</source>. <year>2007</year>;<volume>17</volume>(<issue>9</issue>):<fpage>2172</fpage>–<lpage>89</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhl128" xlink:type="simple">10.1093/cercor/bhl128</ext-link></comment> <object-id pub-id-type="pmid">17135481</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>U</surname> <given-names>H.</given-names></name>, <name name-style="western"><surname>Kashino</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hirahara</surname> <given-names>T</given-names></name>. <article-title>The effect of head motion on the accuracy of sound localization</article-title>. <source>Acoustical Science and Technology</source>. <year>2003</year>;<volume>24</volume>(<issue>5</issue>):<fpage>315</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
<ref id="pbio.2001878.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thurlow</surname> <given-names>WR</given-names></name>, <name name-style="western"><surname>Runge</surname> <given-names>PS</given-names></name>. <article-title>Effect of induced head movements on localization of direction of sounds</article-title>. <source>J Acoust Soc Am</source>. <year>1967</year>;<volume>42</volume>(<issue>2</issue>):<fpage>480</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">6075941</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>. <article-title>Auditory cortex represents both pitch judgments and the corresponding acoustic cues</article-title>. <source>Curr Biol</source>. <year>2013</year>;<volume>23</volume>(<issue>7</issue>):<fpage>620</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2013.03.003" xlink:type="simple">10.1016/j.cub.2013.03.003</ext-link></comment> <object-id pub-id-type="pmid">23523247</object-id>;</mixed-citation></ref>
<ref id="pbio.2001878.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Characterization of external ear impulse responses using Golay codes</article-title>. <source>J Acoust Soc Am</source>. <year>1992</year>;<volume>92</volume>(<issue>2 Pt 1</issue>):<fpage>1169</fpage>–<lpage>71</lpage>. <object-id pub-id-type="pmid">1506522</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Musial</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>SN</given-names></name>, <name name-style="western"><surname>Gerstein</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Keating</surname> <given-names>JG</given-names></name>. <article-title>Signal-to-noise ratio improvement in multiple electrode recording</article-title>. <source>J Neurosci Methods</source>. <year>2002</year>;<volume>115</volume>(<issue>1</issue>):<fpage>29</fpage>–<lpage>43</lpage>. <object-id pub-id-type="pmid">11897361</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Clock</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>. <article-title>A panoramic code for sound location by cortical neurons</article-title>. <source>Science</source>. <year>1994</year>;<volume>264</volume>(<issue>5160</issue>):<fpage>842</fpage>–<lpage>4</lpage>. <object-id pub-id-type="pmid">8171339</object-id>.</mixed-citation></ref>
<ref id="pbio.2001878.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oostenveld</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maris</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schoffelen</surname> <given-names>JM</given-names></name>. <article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title>. <source>Comput Intell Neurosci</source>. <year>2011</year>;<volume>2011</volume>:<fpage>156869</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/2011/156869" xlink:type="simple">10.1155/2011/156869</ext-link></comment> <object-id pub-id-type="pmid">21253357</object-id>;</mixed-citation></ref>
</ref-list>
</back>
</article>