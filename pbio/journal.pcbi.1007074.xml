<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00774</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1007074</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Discrete mathematics</subject><subj-group><subject>Combinatorics</subject><subj-group><subject>Permutation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject></subj-group></article-categories>
<title-group>
<article-title>Perturbing low dimensional activity manifolds in spiking neuronal networks</article-title>
<alt-title alt-title-type="running-head">Manifold perturbation in spiking neuronal networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4754-4561</contrib-id>
<name name-style="western">
<surname>Wärnberg</surname> <given-names>Emil</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id>
<name name-style="western">
<surname>Kumar</surname> <given-names>Arvind</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Dept. of Computational Science and Technology, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Dept. of Neuroscience, Karolinska Institutet, Stockholm, Sweden</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>UCL, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">arvkumar@kth.se</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>5</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>31</day>
<month>5</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>5</issue>
<elocation-id>e1007074</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>5</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Wärnberg, Kumar</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1007074"/>
<abstract>
<p>Several recent studies have shown that neural activity <italic>in vivo</italic> tends to be constrained to a low-dimensional manifold. Such activity does not arise in simulated neural networks with homogeneous connectivity and it has been suggested that it is indicative of some other connectivity pattern in neuronal networks. In particular, this connectivity pattern appears to be constraining learning so that only neural activity patterns falling within the intrinsic manifold can be learned and elicited. Here, we use three different models of spiking neural networks (echo-state networks, the Neural Engineering Framework and Efficient Coding) to demonstrate how the intrinsic manifold can be made a direct consequence of the circuit connectivity. Using this relationship between the circuit connectivity and the intrinsic manifold, we show that learning of patterns outside the intrinsic manifold corresponds to much larger changes in synaptic weights than learning of patterns within the intrinsic manifold. Assuming larger changes to synaptic weights requires extensive learning, this observation provides an explanation of why learning is easier when it does not require the neural activity to leave its intrinsic manifold.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>A network in the brain consists of thousands of neurons. A priori, we expect that the network will have as many degrees of freedom as its number of neurons. Surprisingly, experimental evidence suggests that local brain activity is confined to a subspace spanned by ~10 variables. Here, we employ three established approaches to construct spiking neuronal networks that exhibit low-dimensional activity. Using these models we address a specific experimental observation, namely that monkeys easily can elicit any activity within the subspace but have trouble with any activity outside. Specifically, we show that tasks that requires animals to change the network activity outside the subspace would entail large changes in the neuronal connectivity, and therefore, animals are either slow or not able to acquire such tasks.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>StratNeuro Strategic Program in Neuroscience, Sweden</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id>
<name name-style="western">
<surname>Kumar</surname> <given-names>Arvind</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100008444</institution-id>
<institution>Parkinsonfonden</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id>
<name name-style="western">
<surname>Kumar</surname> <given-names>Arvind</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004359</institution-id>
<institution>Vetenskapsrådet</institution>
</institution-wrap>
</funding-source>
<award-id>2018-03118</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8044-9195</contrib-id>
<name name-style="western">
<surname>Kumar</surname> <given-names>Arvind</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>Partial funding from the School of Electrical Engineering and Computer Science, KTH, Parkinsonfonden Sweden, The Strategic Research Area Neuroscience (StratNeuro) program, and Swedish Research Foundation (Vetenskapsrådet) is greatly acknowledged. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-06-20</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The simulation code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/emiwar/SpikingManifolds" xlink:type="simple">https://github.com/emiwar/SpikingManifolds</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The availability of novel experimental methods allows for simultaneous recording of 1000s of neurons and has made it possible to observe the fine structure of temporal evolution of task-related neuronal activity <italic>in vivo</italic>. The multi-unit neuronal activity can be described in terms of an <italic>N</italic> dimensional neural state-space where each axis (typically) corresponds to the firing rate of each neuron. The activity at a particular time corresponds to a point in this space, and the temporal evolution of the neuronal activity constitutes a trajectory. Analysis of such trajectories has revealed that across different brain regions and in different behavioral conditions the neural activity remains low dimensional [<xref ref-type="bibr" rid="pcbi.1007074.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1007074.ref007">7</xref>] (but see [<xref ref-type="bibr" rid="pcbi.1007074.ref008">8</xref>]) such that the dimensionality of the activity is much smaller than number of neurons. That is, the trajectories corresponding to the task-related activity tend to be constrained to a linear subspace (the <italic>intrinsic manifold</italic>) of the state space rather than moving freely in all directions.</p>
<p>The empirical importance of the relation between dimensionality of activity and network connectivity was best illustrated by an experiment involving brain-computer-interface (BCI) learning in monkeys. Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>] showed that animals were able to quickly learn the BCI task when the neural activity mapping was confined to the intrinsic manifold. By contrast, BCI learning was much slower (or non-existent) when the neural activity mapping for the BCI task was outside the intrinsic manifold [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>]. Here, we address the question of why relearning activity patterns is easier within the intrinsic manifold than outside. Note that our goal is to build a conceptual understanding and not to faithfully model every aspect of the experiment.</p>
<p>Given that the neural activity appears to be low dimensional, Gallego et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref009">9</xref>] recently proposed a division of the neural dynamics into a set of <italic>latent variables</italic> and a set of <italic>neural modes</italic>. The neural modes are static over the relevant timescales and are identified as factor loadings or principal components of the neural activity. Each neural mode is associated with a latent variable so that the actual neural activity at any point in time is a sum of the neural modes weighted by the respective latent variable. This description is analogous to the intrinsic manifold described in [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>]: the neural modes correspond to the vectors spanning the intrinsic manifold.</p>
<p>To analyze perturbations of the intrinsic manifold, we first need a neural network that exhibits low-dimensional activity. A trivial solution would be to drive a recurrent network with low-dimensional input. However, this solution does not address the problem of manifold generation and merely assumes that an unspecified upstream network can generate a low-dimensional input. Therefore, here we make the assumption that low dimensionality is enforced by the local circuitry. In particular, we assume that the specific neural modes arise from local circuitry and that structured inputs may influence the values and trajectories of the latent variables but not the neural modes themselves. Following these assumptions, we ask what changes of the local network are required to change the neural modes, i.e. to rotate the intrinsic manifold. In particular, what type of changes in the neural modes requires the largest changes of local synaptic weights? Under the assumption that changing synaptic weights requires application of some learning rule, this may serve as an unbiased proxy for learning difficulty.</p>
<p>A network that exhibits a <italic>D</italic>-dimensional line attractor dynamics is one of the simplest network models that fulfills the above criteria and maintains <italic>D</italic> neural modes. By definition a line attractor will ensure that the activity remains in the <italic>D</italic>-dimensional intrinsic manifold. There are several frameworks for creating line attractors and networks with dynamical latent variables [<xref ref-type="bibr" rid="pcbi.1007074.ref010">10</xref>]. Here, we focus on three popular frameworks: (1) FORCE-learning [<xref ref-type="bibr" rid="pcbi.1007074.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>], (2) The Neural Engineering Framework (NEF; [<xref ref-type="bibr" rid="pcbi.1007074.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref014">14</xref>]) and (3) Efficient Coding networks [<xref ref-type="bibr" rid="pcbi.1007074.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref016">16</xref>]. We show that for all three frameworks, neural modes directly correspond to the <italic>encoders</italic> which are specified when the system is defined. Thus, we identified a feature common to all three frameworks. Importantly, we show that irrespective of the choice of modeling framework, rotating the neural modes within the intrinsic manifold only requires minor changes in the local synaptic weights, while rotating the neural modes outside the intrinsic manifold requires a complete rewiring of the network. This provides a possible explanation for Sadtler et al.’s observation [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>] and offers new insights into the functional role of neural modes.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Neural activity with <italic>D</italic> dimensions can by definition be decomposed into <italic>D</italic> time-independent components called principal components, factor loadings or <italic>neural modes</italic> (<xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1A</xref>). The vector of instantaneous firing rates <bold>a</bold>(<italic>t</italic>) can at any point in time be described as a linear combination of these neural modes <bold>z</bold><sub>1</sub>, <bold>z</bold><sub>2</sub>, …, <bold>z</bold><sub><italic>D</italic></sub> plus some noise <italic>ϵ</italic>(<italic>t</italic>):
<disp-formula id="pcbi.1007074.e001"><alternatives><graphic id="pcbi.1007074.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>D</mml:mi></mml:munderover> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>Z</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
The time-varying coefficients <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) carry computationally relevant information and are therefore referred to as <italic>latent variables</italic> [<xref ref-type="bibr" rid="pcbi.1007074.ref009">9</xref>]. In this work, we measure the <bold>a</bold>(<italic>t</italic>) as the number of spikes per consecutive 50ms-bin. However, note that <bold>a</bold>(<italic>t</italic>) is intuitively similar to the filtered spike trains <bold>u</bold>(<italic>t</italic>) that are defined below.</p>
<fig id="pcbi.1007074.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g001</object-id>
<label>Fig 1</label>
<caption>
<title>A schematic of the architecture for creating neural modes.</title>
<p>(<bold>A</bold>) Low dimensional neural activity can be decomposed into neural modes (for illustration here we consider two neural modes: <bold>z</bold><sub>1</sub> and <bold>z</bold><sub>2</sub>). (<bold>B</bold>) The neural architecture shared among all three frameworks used in this work. A “reservoir” of neurons that are connected with synaptic weights Ω<sub><italic>ij</italic></sub> generates some activity. From this activity <italic>D</italic> = 2 latent variables (<italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub>) are read out using the weights Φ. The values are then fed back into the network using weights <italic>K</italic>. Note that <italic>K</italic> and Φ do not represent any physical entities, but only serves as helper matrices to eventually construct the weight matrix <italic>W</italic> (see <xref ref-type="disp-formula" rid="pcbi.1007074.e007">Eq 5</xref>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g001" xlink:type="simple"/>
</fig>
<p>To explain the relation between neural modes and learning discovered by Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>] we need to construct a network in which we can control the neural modes and the latent variables. We do this by beginning to model the network as a simple memory circuit (<xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>): once presented with some continuous variable, the network should retain this variable in its intrinsic activity so that it can be read out later. We argue that the ability to robustly encode variables in this fashion is analogous to the ability to keep a BCI readout stable. Nevertheless, some BCI tasks may require the readout to not only be stable and static, but dynamic. Therefore, we have structured the rest of the Results as follows: first we show that synaptic weights are similar before and after an inside-manifold perturbation in networks with static latent variables. Second, we show that this result extends to some degree to networks where the latent variables oscillates. Finally, we show that it also holds for a more biologically plausible network.</p>
<sec id="sec003">
<title>Model</title>
<p>We consider networks of <italic>N</italic> leaky integrate-and-fire (LIF) neurons. The sub-threshold membrane voltage for neuron <italic>j</italic> is given by
<disp-formula id="pcbi.1007074.e002"><alternatives><graphic id="pcbi.1007074.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:msub><mml:mi>V</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mtext>leak</mml:mtext></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>τ</italic><sub><italic>m</italic></sub> = <italic>C</italic><sub><italic>m</italic></sub><italic>R</italic><sub><italic>m</italic></sub> is the membrane time constant, <italic>C</italic><sub><italic>m</italic></sub> is the membrane capacitance, <italic>R</italic><sub><italic>m</italic></sub> is the membrane input resistance and <italic>I</italic><sub><italic>j</italic></sub> is the total time-varying synaptic and external input current. Once the membrane voltage passes a threshold <italic>V</italic><sub>th</sub> a spike is emitted and the membrane voltage is reset to <italic>V</italic><sub>reset</sub>.</p>
<p>The challenge is to connect the synapses of the network is such a way so that the network becomes a (<italic>D</italic>-dimensional) line attractor capable of retaining the value of <italic>D</italic> latent variables. That is, how should the network be connected so that the input currents <italic>I</italic><sub><italic>j</italic></sub> (synaptic or otherwise) to each neuron contributes to keeping the latent variables stable?</p>
<p>The general strategy we use for doing this is illustrated in <xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>. For each latent variable we create a corresponding latent readout-node that decodes the variable from the network activity by taking a linear combination of the synaptically filtered spike trains:
<disp-formula id="pcbi.1007074.e003"><alternatives><graphic id="pcbi.1007074.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo>Φ</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e004"><alternatives><graphic id="pcbi.1007074.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the set of weights, <italic>u</italic><sub><italic>j</italic></sub>(<italic>t</italic>) = <italic>H</italic> * ∑<sub><italic>m</italic></sub> <italic>δ</italic>(<italic>t</italic><sub><italic>jm</italic></sub> − <italic>t</italic>) are the filtered spike trains, <italic>H</italic> is a synaptic kernel and <italic>t</italic><sub><italic>jm</italic></sub> is the time of the <italic>m</italic>th spike from neuron <italic>j</italic>.</p>
<p>We can use the read out latent variables <bold>x</bold> to construct an input current to the neurons that feeds the latent variables back into the network:
<disp-formula id="pcbi.1007074.e005"><alternatives><graphic id="pcbi.1007074.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi> <mml:mtext>fb</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mi>K</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e006"><alternatives><graphic id="pcbi.1007074.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>K</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>×</mml:mo> <mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is an arbitrarily chosen matrix (<xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>) and <bold>I</bold><sub>fb</sub> is vector notation for one part of the synaptic currents to the neurons (<xref ref-type="disp-formula" rid="pcbi.1007074.e002">Eq 2</xref>). This artificial construct can in turn be used to find a set of synaptic weights: note that the decoding (<xref ref-type="disp-formula" rid="pcbi.1007074.e003">Eq 3</xref>) and the feed-back (<xref ref-type="disp-formula" rid="pcbi.1007074.e005">Eq 4</xref>) steps are both linear operations, and can thus be combined into one single matrix multiplication
<disp-formula id="pcbi.1007074.e007"><alternatives><graphic id="pcbi.1007074.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi> <mml:mtext>fb</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mi>K</mml:mi> <mml:mo>Φ</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>W</mml:mi> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
From this, we identify <inline-formula id="pcbi.1007074.e008"><alternatives><graphic id="pcbi.1007074.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi>W</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as a matrix of synaptic weights and a tentative solution to the problem of how to connect the network (for a more detailed derivation of this result, see Abbott et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref010">10</xref>]).</p>
<p>In particular, a network connected in this fashion has a basic structure needed for retaining <italic>D</italic> latent variables once they have been set. However, several more details have to be determined before implementing this idea in practice. The most crucial issue is how to find the decoding weights Φ in <xref ref-type="disp-formula" rid="pcbi.1007074.e003">Eq 3</xref>. Moreover, we need to determine whether any additional synapses are required in addition to the ones indicated by <italic>W</italic> in <xref ref-type="disp-formula" rid="pcbi.1007074.e007">Eq 5</xref>. In this paper, we consider three different strategies for finding Φ and designing the connections.</p>
<p>First, we consider the FORCE learning rule [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>] which builds upon the liquid state machine and echo state networks [<xref ref-type="bibr" rid="pcbi.1007074.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref018">18</xref>]. Following this idea, the synapses of the network belong to two sets. The synapses of the first set are chosen randomly, but are normalized so that the input to each neuron is loosely balanced, which makes the network dynamics chaotic [<xref ref-type="bibr" rid="pcbi.1007074.ref019">19</xref>]. The second set of synapses directly corresponds to the matrix <italic>W</italic> above. Thus, for FORCE networks, the current in <xref ref-type="disp-formula" rid="pcbi.1007074.e002">Eq 2</xref> is given by
<disp-formula id="pcbi.1007074.e009"><alternatives><graphic id="pcbi.1007074.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:mi mathvariant="bold">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mo>Ω</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msub><mml:mi mathvariant="bold">I</mml:mi> <mml:mtext>res</mml:mtext></mml:msub></mml:munder> <mml:mo>+</mml:mo> <mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mi>W</mml:mi> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>︸</mml:mo></mml:munder> <mml:msub><mml:mi mathvariant="bold">I</mml:mi> <mml:mtext>fb</mml:mtext></mml:msub></mml:munder></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e010"><alternatives><graphic id="pcbi.1007074.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mo>Ω</mml:mo> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is the matrix corresponding to the first set synapses and <italic>W</italic> = <italic>K</italic>Φ correspond to the second set as above. The first matrix Ω and the encoder weights <italic>K</italic> are both fixed while Φ are subject to an online learning rule (see [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>] and <xref ref-type="sec" rid="sec009">Methods</xref>).</p>
<p>Second, we consider the Neural Engineering Framework (NEF) [<xref ref-type="bibr" rid="pcbi.1007074.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref014">14</xref>]. Unlike the FORCE algorithm, NEF does not require any extra synapses in addition to ones implied by <italic>W</italic>. Instead, each neuron receives an additional stationary driving current so that the total current becomes
<disp-formula id="pcbi.1007074.e011"><alternatives><graphic id="pcbi.1007074.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:mi mathvariant="bold">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>W</mml:mi> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">I</mml:mi> <mml:mtext>drive</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
Similar to the FORCE rule, in the NEF <italic>W</italic> = <italic>K</italic>Φ where <italic>K</italic> is fixed and Φ is learned. In the NEF however, Φ is learned in batch. That is,
<disp-formula id="pcbi.1007074.e012"><alternatives><graphic id="pcbi.1007074.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="4pt"/><mml:mtext>min</mml:mtext></mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mo>′</mml:mo></mml:msup></mml:munder> <mml:mo>⟨</mml:mo> <mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>target</mml:mtext></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mo>′</mml:mo></mml:msup> <mml:mi mathvariant="bold">u</mml:mi> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mo>⟩</mml:mo> <mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>target</mml:mtext></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where the expected value is taken over all permitted values of <bold>x</bold><sub>target</sub> and the corresponding filtered spike trains <bold>u</bold> (see [<xref ref-type="bibr" rid="pcbi.1007074.ref013">13</xref>] and <xref ref-type="sec" rid="sec009">Methods</xref>).</p>
<p>Finally, we consider the Efficient Coding framework [<xref ref-type="bibr" rid="pcbi.1007074.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref016">16</xref>]. Similar to the FORCE learning, there are two sets of synapses, a set for the reservoir and another set for the recurrent connections. In contrast to FORCE learning and the NEF, no learning is required in the Efficient Coding framework. Instead, the readout weights are fixed to
<disp-formula id="pcbi.1007074.e013"><alternatives><graphic id="pcbi.1007074.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mtext>syn</mml:mtext></mml:msub> <mml:msup><mml:mi>K</mml:mi> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
where λ<sub>syn</sub> = 1/<italic>τ</italic><sub>syn</sub>. At the same time, the reservoir weights are fixed to
<disp-formula id="pcbi.1007074.e014"><alternatives><graphic id="pcbi.1007074.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mrow><mml:mo>Ω</mml:mo> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>K</mml:mi> <mml:msup><mml:mi>K</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mtext>syn</mml:mtext> <mml:mn>2</mml:mn></mml:msubsup> <mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
rather than being chosen randomly. Finally, the reservoir synapses are assumed to be instantaneous, i.e. they have a time constant of 0. The input current to neurons are given by
<disp-formula id="pcbi.1007074.e015"><alternatives><graphic id="pcbi.1007074.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:mi mathvariant="bold">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>Ω</mml:mo> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mi>τ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>W</mml:mi> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>W</italic> = λ<sub>syn</sub> <italic>KK</italic><sup><italic>T</italic></sup>, and <bold>u</bold><sub><italic>τ</italic> = 0</sub>(<italic>t</italic>) are unfiltered spike trains.</p>
</sec>
<sec id="sec004">
<title>The neural modes and the decoders are both determined by the encoders</title>
<p>In <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref> we demonstrate the ability of the three frameworks to retain a value once it has been set. We implement a network with <italic>N</italic> = 1000 neurons and <italic>D</italic> = 2 latent variables for each of the three frameworks mentioned above. We verify the implementations using the following protocol: every 500 ms we artificially fix the two latent variables <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> to some arbitrary values during 100 ms (blue areas in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A, 2D and 2G</xref>) and then let the activity evolve freely for the next 400 ms. As intended from the design of the networks, they retain the variables once they have been set, albeit with varying degrees of noise and drift.</p>
<fig id="pcbi.1007074.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The encoders <italic>K</italic> determine the decoders Φ<sup><italic>T</italic></sup> and the neural modes <italic>Z</italic>.</title>
<p>(<bold>A</bold>) Example of a simulation of a spiking network with the FORCE learning rule. The two traces show the two latent variables. During the periods shaded blue, the latent variables are fixed. The spike times of 50 (5% of the) neurons are shown in the background. (<bold>B</bold>) The fraction of spike rate variance explained by each principal component. (<bold>C</bold>) The generalized correlation (see <xref ref-type="sec" rid="sec009">Methods</xref>) between the encoder matrix <italic>K</italic>, decoder matrix Φ<sup><italic>T</italic></sup>, principal components <italic>Z</italic><sub>PCA</sub> and loading matrix <italic>Z</italic><sub>FA</sub>. (<bold>D, E, F</bold>) Same result for the Neural Engineering Framework. (<bold>G, H, I</bold>) Same result for the Efficient Coding framework.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g002" xlink:type="simple"/>
</fig>
<p>For all three frameworks, the spiking pattern is largely determined by the latent variables. In particular, note that the spiking pattern (background of <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A, 2D and 2G</xref>) changes considerably when the values of the latent variables are changed. These changes in firing pattern are due to the divergent connection <italic>K</italic> from the latent variables <bold>x</bold> to the input currents <bold>I</bold> (see <xref ref-type="disp-formula" rid="pcbi.1007074.e005">Eq 4</xref> and <xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>), which are created so that each neuron receives an input that is a linear combination of the latent variables. We expect the firing rates to approximately reflect these input currents, albeit with some distortion due to auxiliary synapses, threshold nonlinearity in the neuron and binning to estimate the firing rate.</p>
<p>To verify this, we estimated the firing rate in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A, 2D and 2G</xref> by counting the number of spikes from each neuron in each consecutive 50 ms interval. We then applied Principal Component Analysis (PCA) on the time series of spike bins. As can be seen in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2B, 2E and 2H</xref> for all our networks two principal components explained the majority of the variance. We then calculated the cosine of the mean principal angle between the subspace spanned by the first two principal components <italic>Z</italic><sub>PCA</sub> and the subspace spanned by the columns of the encoder matrix <italic>K</italic> (this is a generalization of correlation, see <xref ref-type="sec" rid="sec009">Methods</xref>) and found a high degree of similarity (cos <italic>ϕ</italic> ≈ 0.7) for all three frameworks (<xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2C, 2F and 2I</xref>). We also observed a similar degree of similarity with the loading matrix <italic>Z</italic><sub>FA</sub> from Factor Analysis (FA)—indeed, the subspaces found by FA were almost completely equal to the the subspaces found by PCA. Equating the principal components with neural modes we conclude that neural modes are determined by the encoder matrix <italic>K</italic>. Note that all the three frameworks allow us to choose <italic>K</italic> arbitrarily. Thus, they provide a way to construct networks where we can freely select not only the number of neural modes (i.e. dimensionality), but also the specific mode vectors.</p>
<p>The fact that the neural activity is determined by the neural modes further influences the readout weights Φ. Although the specific algorithm for determining Φ differs between the three frameworks, they all have in common that Φ should be an accurate linear transform from the spike trains to the latent variables. If the neural modes <italic>Z</italic> are orthonormal, a latent variable can be read out by taking the inner product between the respective neural mode and the neural activity: <italic>x</italic><sub><italic>i</italic></sub> ≈ <bold>z</bold><sub><italic>i</italic></sub> ⋅ <bold>u</bold>. Therefore, we expect the readout weights to be Φ<sup><italic>T</italic></sup> ≈ <italic>Z</italic> (where the columns of <italic>Z</italic> span the same space as <italic>Z</italic><sub>PCA</sub> and <italic>Z</italic><sub>FA</sub>). For a more formal treatment of this argument, see Salinas and Abbott [<xref ref-type="bibr" rid="pcbi.1007074.ref020">20</xref>].</p>
<p>Given that Φ<sup><italic>T</italic></sup> ≈ <italic>Z</italic> and <italic>Z</italic> ≈ <italic>K</italic> it is safe to assume from transitivity that
<disp-formula id="pcbi.1007074.e016"><alternatives><graphic id="pcbi.1007074.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mo>≈</mml:mo> <mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
For the three frameworks under consideration here the correlation between Φ<sup><italic>T</italic></sup> and <italic>K</italic> is in fact even larger than between <italic>Z</italic> and <italic>K</italic> or between <italic>Z</italic> and Φ<sup><italic>T</italic></sup> (<xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2C, 2F and 2I</xref>). For FORCE learning and the NEF, this is likely because while <italic>Z</italic> is found to be the directions of maximal variance, Φ<sup><italic>T</italic></sup> is found by supervised learning so that the readout match the “real” latent variables as close as possible. For Efficient Coding, the correlation between <italic>K</italic> and Φ<sup><italic>T</italic></sup> is exactly 1 because of <xref ref-type="disp-formula" rid="pcbi.1007074.e013">Eq 9</xref>.</p>
</sec>
<sec id="sec005">
<title>Synaptic modifications required for inside- and outside-manifold perturbations</title>
<p>Now we use the previous networks to address the main issue of this paper: why are some perturbations of the neural modes much easier to learn than others? We know that the neural modes <italic>Z</italic> ≈ <italic>K</italic> for the type of networks we are studying here. Thus, if <italic>Z</italic> is to be perturbed to perform the task, a corresponding perturbation must be applied to the encoders <italic>K</italic>.</p>
<p>Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>] characterized perturbations of the neural modes into <italic>inside-manifold</italic> perturbations and <italic>outside-manifold</italic> perturbations. An inside-manifold perturbation is restricted so that the subspace spanned by the neural modes (the <italic>intrinsic manifold</italic>) is preserved. In other words, even though the neural modes have been changed, the part of the neural state space that is allowed is unchanged. Mathematically, the perturbed neural modes of an inside-manifold perturbation are given by
<disp-formula id="pcbi.1007074.e017"><alternatives><graphic id="pcbi.1007074.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>Z</mml:mi> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e018"><alternatives><graphic id="pcbi.1007074.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Here, we assume that the inside-manifold perturbation is defined to not stretch or skew the subspace, and therefore that <inline-formula id="pcbi.1007074.e019"><alternatives><graphic id="pcbi.1007074.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is an orthogonal matrix. We refer the treatment of non-orthogonal matrices to <xref ref-type="supplementary-material" rid="pcbi.1007074.s001">S1 Text</xref>.</p>
<p>Assuming the <italic>Z</italic> ≈ <italic>K</italic> ≈ Φ<sup><italic>T</italic></sup> holds for any network, it will also hold for the permuted network, i.e. <inline-formula id="pcbi.1007074.e020"><alternatives><graphic id="pcbi.1007074.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:msup><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, we can expect the corresponding changes to encoder and readout matrices to be
<disp-formula id="pcbi.1007074.e021"><alternatives><graphic id="pcbi.1007074.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mi>K</mml:mi> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mspace width="2.em"/><mml:mtext>and</mml:mtext> <mml:mspace width="2.em"/><mml:msup><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mo>≈</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mi>T</mml:mi></mml:msup> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
For all three frameworks the set of (learnable) synaptic weights are given by <italic>W</italic> = <italic>K</italic>Φ. Thus, the new set of synaptic weights after the perturbation is:
<disp-formula id="pcbi.1007074.e022"><alternatives><graphic id="pcbi.1007074.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mi>K</mml:mi> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>K</mml:mi> <mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e023"><alternatives><graphic id="pcbi.1007074.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> follows from orthogonality. Changing the synaptic weights from <italic>W</italic> to <inline-formula id="pcbi.1007074.e024"><alternatives><graphic id="pcbi.1007074.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> would therefore only require minor modifications, if any.</p>
<p>This result does not hold for outside-manifold perturbations. Such a perturbation can be written as
<disp-formula id="pcbi.1007074.e025"><alternatives><graphic id="pcbi.1007074.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mrow><mml:mover accent="true"><mml:mi>Z</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>Z</mml:mi></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e026"><alternatives><graphic id="pcbi.1007074.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is also an orthogonal matrix, although with many more elements than <inline-formula id="pcbi.1007074.e027"><alternatives><graphic id="pcbi.1007074.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (<italic>N</italic><sup>2</sup> vs <italic>D</italic><sup>2</sup>). Following the same steps as above, one arrives at
<disp-formula id="pcbi.1007074.e028"><alternatives><graphic id="pcbi.1007074.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>K</mml:mi> <mml:mo>Φ</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>W</mml:mi> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
In general, <inline-formula id="pcbi.1007074.e029"><alternatives><graphic id="pcbi.1007074.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is not equal to <italic>W</italic> and therefore learning <inline-formula id="pcbi.1007074.e030"><alternatives><graphic id="pcbi.1007074.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> likely requires substantial synaptic changes and hence extensive learning.</p>
<p>To verify this derivation, we created three new networks similar to the ones in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref>. Keeping all other parameters equal, we permuted the two columns of <italic>K</italic> to mimic inside-manifold perturbations, i.e. we applied <xref ref-type="disp-formula" rid="pcbi.1007074.e021">Eq 14</xref> with
<disp-formula id="pcbi.1007074.e031"><alternatives><graphic id="pcbi.1007074.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
We then learned <inline-formula id="pcbi.1007074.e032"><alternatives><graphic id="pcbi.1007074.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> using the respective framework, and calculated <inline-formula id="pcbi.1007074.e033"><alternatives><graphic id="pcbi.1007074.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. For comparison, we created an additional three networks with the same parameters as before, but where the rows of <italic>K</italic> were permuted by switching the first 500 rows with the last 500. That is, we used
<disp-formula id="pcbi.1007074.e034"><alternatives><graphic id="pcbi.1007074.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
to mimic outside-manifold transformation and learned <inline-formula id="pcbi.1007074.e035"><alternatives><graphic id="pcbi.1007074.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007074.e036"><alternatives><graphic id="pcbi.1007074.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> as before. We thus have examples of <italic>W</italic>, <inline-formula id="pcbi.1007074.e037"><alternatives><graphic id="pcbi.1007074.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1007074.e038"><alternatives><graphic id="pcbi.1007074.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for the three frameworks. In <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3</xref>, we show that the element-wise correlation is high between <italic>W</italic> and <inline-formula id="pcbi.1007074.e039"><alternatives><graphic id="pcbi.1007074.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (inside-manifold permutation, light green) but close to zero for <italic>W</italic> and <inline-formula id="pcbi.1007074.e040"><alternatives><graphic id="pcbi.1007074.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (outside-manifold permutation, red). A high correlation indicate that only small changes to synaptic weights are required to change the neural modes within the intrinsic-manifold suggesting that this change may be easier for a biological network to learn. Note that correlations do not capture global additive or multiplicative scaling of the weights. Given that the networks have otherwise identical parameters before and after perturbation, we have no reason to expect any such scaling their weights. Nevertheless, to avoid this possibility, we also calculated the Frobenius norm which also showed a very similar pattern (<xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1 Fig</xref>).</p>
<fig id="pcbi.1007074.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Inside manifold perturbations do not require big changes of synapse weights.</title>
<p>(<bold>A</bold>) Synapse weights (elements of <italic>W</italic>) from one network instantiation of FORCE, compared to re-learning with the permuted encoders <inline-formula id="pcbi.1007074.e041"><alternatives><graphic id="pcbi.1007074.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mover accent="true"><mml:mi>K</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (elements of <inline-formula id="pcbi.1007074.e042"><alternatives><graphic id="pcbi.1007074.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>). (<bold>B</bold>) Same as A, but for an outside-manifold permutation (<inline-formula id="pcbi.1007074.e043"><alternatives><graphic id="pcbi.1007074.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>). (<bold>C</bold>) The element-wise correlations between old (<italic>W</italic>) and new synaptic weights for different types of permutations (see main text for definitions) for all three frameworks. Error bars indicate standard deviation over 30 network instantiations. Note that orthogonal inside manifold perturbations (“Permutation” and “45 degree rotation”) require almost no changes to the weights, non-orthogonal inside manifold perturbations (“Gaussian elements” and “Same element”) require some changes and the outside-manifold permutation requires as much change as a network with unrelated encoders. See also <xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1D Fig</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g003" xlink:type="simple"/>
</fig>
<p>For further comparisons, we introduced four additional inside manifold perturbations: 1. no perturbation at all, just restarting the learning algorithm with the same <italic>K</italic> (<xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, light blue), 2. an orthogonal inside manifold perturbation given by the following transformation (<xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, green):
<disp-formula id="pcbi.1007074.e044"><alternatives><graphic id="pcbi.1007074.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mtext>rot</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>sin</mml:mtext> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext> <mml:mfrac><mml:mi>π</mml:mi> <mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
3. a non-orthogonal perturbation matrix <inline-formula id="pcbi.1007074.e045"><alternatives><graphic id="pcbi.1007074.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msub><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mtext>gaussian</mml:mtext></mml:msub></mml:math></alternatives></inline-formula> where all four elements were independently drawn from <inline-formula id="pcbi.1007074.e046"><alternatives><graphic id="pcbi.1007074.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, dark green), a non-orthogonal matrix <inline-formula id="pcbi.1007074.e047"><alternatives><graphic id="pcbi.1007074.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msub><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mtext>same</mml:mtext></mml:msub></mml:math></alternatives></inline-formula> where all four elements were equal (to a random value) (<xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, darker green). In each of these different types of inside-manifold perturbations, element-wise correlation between <italic>W</italic> and <inline-formula id="pcbi.1007074.e048"><alternatives><graphic id="pcbi.1007074.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> was high, further suggesting that inside manifold perturbation required relatively small change in synaptic weights. By contrast, correlation between <inline-formula id="pcbi.1007074.e049"><alternatives><graphic id="pcbi.1007074.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> corresponding to a network where <italic>K</italic> was redrawn independently and <italic>W</italic> was almost zero (<xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, dark red). These results were independent of the way we measured the similarity between <italic>W</italic> and <inline-formula id="pcbi.1007074.e050"><alternatives><graphic id="pcbi.1007074.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (<xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1 Fig</xref>).</p>
<p>Note that in the frameworks we have used here the learning rule is applied to Φ and only indirectly to <italic>W</italic>. In general Φ is not correlated to either <inline-formula id="pcbi.1007074.e051"><alternatives><graphic id="pcbi.1007074.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> or <inline-formula id="pcbi.1007074.e052"><alternatives><graphic id="pcbi.1007074.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (see <xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1A, S1B and S1C Fig</xref>). The correlation only becomes visible after multiplying <italic>K</italic> and Φ (see <xref ref-type="disp-formula" rid="pcbi.1007074.e022">Eq 15</xref>). Furthermore, note that we have excluded the reservoir weights Ω from the correlation because we model them as being static. For Efficient Coding, we have ignored changes to the fast weights (Ω) because they are just replicates of the regular weights <italic>W</italic>.</p>
</sec>
<sec id="sec006">
<title>Manifold perturbations with dynamical latent variables</title>
<p>In <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref> we considered latent variables that were assigned values by some unspecified external control, and that once set the networks’ sole task was to retain them. In reality the functional role of most local circuits is doubtlessly more complex and likely involves manipulating the latent variables in relation to the inputs.</p>
<p>The static attractor model can easily be extended to allow for some computations on the latent variables. In particular, we can replace the decoder weights Φ with some other weights <italic>Γ</italic> and choose these so that the latent variables are modified before they are fed back into the network (<xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4A</xref>). Notably, if choose
<disp-formula id="pcbi.1007074.e053"><alternatives><graphic id="pcbi.1007074.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mrow><mml:mo>Γ</mml:mo> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>τ</mml:mi> <mml:mtext>syn</mml:mtext></mml:msub> <mml:mi>A</mml:mi> <mml:mo>+</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo> <mml:mo>Φ</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
where <inline-formula id="pcbi.1007074.e054"><alternatives><graphic id="pcbi.1007074.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:mi>A</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mo>×</mml:mo> <mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the dynamical variables will evolve according to
<disp-formula id="pcbi.1007074.e055"><alternatives><graphic id="pcbi.1007074.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow> <mml:mrow><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula></p>
<fig id="pcbi.1007074.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Oscillating latent variables.</title>
<p>(<bold>A</bold>) By slightly modifying the architecture of <xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>, the network can be constructed to perform some computation on the latent variables, for example evolving them as a linear dynamical system (<xref ref-type="disp-formula" rid="pcbi.1007074.e055">Eq 22</xref>). (<bold>B</bold>) The element-wise correlation between <italic>K</italic> and Γ for increasing frequencies <italic>f</italic> = <italic>ω</italic>/2<italic>π</italic>. (<bold>C</bold>) The element-wise correlation between synaptic weights before and after an inside-manifold perturbation (permuting the columns of <italic>K</italic>, solid lines), and before and after and outside-manifold perturbation (permuting the rows, dotted lines).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g004" xlink:type="simple"/>
</fig>
<p>In the Efficient coding framework, <xref ref-type="disp-formula" rid="pcbi.1007074.e053">Eq 21</xref> is explicitly applied to find the new weights Γ. In FORCE and the NEF however, the error function of the learning is changed so that <xref ref-type="disp-formula" rid="pcbi.1007074.e053">Eq 21</xref> will only be applied implicitly. Finding Γ using this implicit method has the benefit that Γ can represent both linear and non-linear transformations of <bold>x</bold> [<xref ref-type="bibr" rid="pcbi.1007074.ref014">14</xref>], thus enabling more complex dynamics than suggested by <xref ref-type="disp-formula" rid="pcbi.1007074.e053">Eq 21</xref>.</p>
<p>Here we first consider the linear case of two-dimensional oscillators, i.e.
<disp-formula id="pcbi.1007074.e056"><alternatives><graphic id="pcbi.1007074.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mrow><mml:mi>A</mml:mi> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mi>ω</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>ω</mml:mi></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
There are three reasons we study oscillating dynamics in lieu of some more complex dynamical system: (1) it is straight-forward to implement comparable dynamics in all three frameworks, (2) the frequency can be continuously varied to demonstrate the difference between fast and slow dynamics, and (3) periodic and quasi-periodic movements are often observed in natural behavior and weakly periodic neural activity patterns have been observed in certain brain regions e.g. motor cortex [<xref ref-type="bibr" rid="pcbi.1007074.ref021">21</xref>].</p>
<p>Following <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2C, 2F and 2I</xref>, we measured the correlation between the encoders <italic>K</italic> and the decoders Γ for different oscillation frequencies to study slow and fast dynamics of the neural modes. We found that the correlation between <italic>K</italic> and Γ decreased with increasing oscillator frequency. We expect this effect because from Eqs <xref ref-type="disp-formula" rid="pcbi.1007074.e056">23</xref> and <xref ref-type="disp-formula" rid="pcbi.1007074.e053">21</xref> we have
<disp-formula id="pcbi.1007074.e057"><alternatives><graphic id="pcbi.1007074.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mrow><mml:mo>Γ</mml:mo> <mml:mo>→</mml:mo> <mml:mo>Φ</mml:mo> <mml:mspace width="4.pt"/><mml:mtext>when</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>ω</mml:mi> <mml:mo>→</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
For the Efficient coding framework, <xref ref-type="disp-formula" rid="pcbi.1007074.e053">Eq 21</xref> is applied explicitly. Therefore, in <xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4B</xref> the Efficient coding trace could also be viewed as the ideal correlation predicted by the theory. The NEF follows this closely, while the FORCE deviates a bit. In any case, seeing that Γ is correlated with <italic>K</italic> for low frequencies, we can conclude that even for dynamical latent-variables inside-manifold perturbations should require less change of the synaptic weights than outside-manifold perturbations. In <xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4C</xref> we verify that this is indeed the case: for low frequencies the synaptic weights after an inside-manifold perturbation are highly correlated with the originals (solid lines) while the correlation is robustly (close to) 0 for outside-manifold perturbations. Thus, for low frequencies, only small changes of the synaptic weights are required to make an inside-manifold perturbation. The procedure for creating <xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4B and 4C</xref> was very similar for creating figure <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3</xref>: the three networks consisted of <italic>N</italic> = 1000 neurons each, and inside and outside-manifold perturbations were given by Eqs <xref ref-type="disp-formula" rid="pcbi.1007074.e031">18</xref> and <xref ref-type="disp-formula" rid="pcbi.1007074.e034">19</xref>, respectively.</p>
<p>Thus, we argue that introducing slow dynamics does not invalidate our claim that inside-manifold perturbations will be easier to learn. If the latent variables reflect motor behaviors, it is reasonable to assume that they vary with behavioral timescales (hundreds of milliseconds), i.e. less than 5-10 Hz. Indeed, the oscillations found by Churchland et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref021">21</xref>] appears to be about 1 Hz. Note that oscillations of latent variables do not in general reflect oscillations of neural activity (<italic>δ</italic>, <italic>α</italic>, <italic>β</italic>, <italic>γ</italic>, etc.).</p>
</sec>
<sec id="sec007">
<title>Networks with sparse and Daleian connectivity</title>
<p>While the networks created up to this point exhibit many biologically plausible features, the matrices of synaptic weights <italic>W</italic> are dense and do not obey Dale’s law [<xref ref-type="bibr" rid="pcbi.1007074.ref022">22</xref>]. Because our claim is centered around the similarity between these matrices before and after the perturbations, it is necessary to verify that adding biological constraints does not nullify the result. Therefore, we here introduce network sparsity and Dale’s law.</p>
<p>We focus in this section on the Neural Engineering Framework because its use of explicit optimization makes it relatively straight-forward to introduce constraints. However, in the NEF (<xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref>) the variable being optimized is the decoders Φ (or, in the non-stationary case, Γ), and not the synaptic weights <italic>W</italic>. Therefore, to directly impose constraints on <italic>W</italic> we changed the optimization procedure to
<disp-formula id="pcbi.1007074.e058"><alternatives><graphic id="pcbi.1007074.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mrow><mml:mi>W</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mrow><mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:mtext>min</mml:mtext></mml:mrow> <mml:mrow><mml:mtext>sign</mml:mtext> <mml:mo>(</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>C</mml:mi></mml:mrow></mml:munder> <mml:mo>⟨</mml:mo> <mml:mrow><mml:mo>∥</mml:mo> <mml:mi>K</mml:mi> <mml:mi>f</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>target</mml:mtext></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi>W</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi mathvariant="bold">u</mml:mi> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mo>⟩</mml:mo> <mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mtext>target</mml:mtext></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
where
<disp-formula id="pcbi.1007074.e059"><alternatives><graphic id="pcbi.1007074.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mrow><mml:mtext>sign</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
Thus, by appropriately choosing <italic>C</italic>, we can constrain <italic>W</italic>. We enforced sparsity and Dale’s law by setting
<disp-formula id="pcbi.1007074.e060"><alternatives><graphic id="pcbi.1007074.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mi>ξ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>ξ</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>j</mml:mi> <mml:mo>≤</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>R</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>≥</mml:mo> <mml:mi>ξ</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>j</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
where <italic>R</italic><sub><italic>ij</italic></sub> is draw randomly from a uniform distribution between 0 and 1, <italic>ξ</italic> controls the sparsity and <italic>ϵ</italic> is the fraction of excitatory neurons. Also note that we have changed the target from <bold>x</bold><sub>target</sub> in <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref> to <italic>f</italic>(<bold>x</bold><sub>target</sub>) for some function <italic>f</italic>(⋅) in <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref>, so that temporal dynamics can be introduced.</p>
<p>We used <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref> to construct a network with <italic>N</italic> = 5000 neurons and <italic>D</italic> = 4 latent variables. The increase in network size helps to prevent the in-degrees from being too small given the sparser connectivity. The dynamics <italic>f</italic>(⋅) were chosen so that the first two latent variables would be the leading and lagging components of a 2 Hz oscillator and the last two would similarly be the lead and lag of a 4 Hz oscillator. Additionally, we took advantage of NEF’s possibility of non-linearities and added a term to <italic>f</italic>(⋅) causing the oscillation amplitude to converge to 1 (see <xref ref-type="sec" rid="sec009">Methods</xref>).</p>
<p>We chose <italic>ϵ</italic> = 0.8, corresponding to 80% excitatory neurons and 20% inhibitory neurons and matching the observed ratio of excitatory and inhibitory neurons in the neocortex [<xref ref-type="bibr" rid="pcbi.1007074.ref023">23</xref>]. To achieve plausible sparsity we set <italic>ξ</italic> = 0.75 meaning only 25% of the connections were allowed. However, even when <italic>C</italic><sub><italic>ij</italic></sub> ≠ 0 it is still allowed for <italic>W</italic><sub><italic>ij</italic></sub> to be arbitrarily close to 0. Excluding connections numerically indistinguishable from 0, the connection probability was 10.8% for excitatory neurons (<italic>j</italic> ≤ 4000) and 19.0% for inhibitory neurons (<italic>j</italic> &gt; 4000). These connection probabilities are well within the biological ranges [<xref ref-type="bibr" rid="pcbi.1007074.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1007074.ref026">26</xref>].</p>
<p>The matrix <italic>W</italic> is shown in <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5A</xref> and exhibits many biologically plausible features in addition to being Daleian and sparse. For instance, the sum of all excitatory inputs matches the sum of all inhibitory inputs to every neuron (<xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5C</xref>). Furthermore, the synaptic weights have a heavy-tail distribution (<xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5D</xref>). Although a closer analysis reveals it to be somewhat leptokurtic (kurtosis is 4.56 for logarithmized weights), the weights distribution is in any case rather close to being log-normal across more than five orders of magnitude. This conforms with recent experimental data suggesting that the weight distributions in biological neural networks are typically heavy-tailed, if not perfectly log-normal [<xref ref-type="bibr" rid="pcbi.1007074.ref027">27</xref>].</p>
<fig id="pcbi.1007074.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The biologically plausible weights matrix <italic>W</italic>.</title>
<p>(<bold>A</bold>) The synaptic weight matrix is sparse and Daleian (neurons with id ≥ 4000 only make inhibitory connections). Only 100 excitatory and 25 inhibitory neurons are shown (<bold>B</bold>) Singular value decomposition of weight matrix <italic>W</italic>. <italic>W</italic> has more than four non-zero singular values, i.e. rank &gt;<italic>D</italic>. (<bold>C</bold>) The sum of incoming excitatory weights to each neuron (one point) is similar to the sum of incoming inhibitory weights, indicating E/I balance. (<bold>D</bold>) Distribution of synaptic weights. The distribution is heavy-tailed but not perfectly log-normal. (<bold>E</bold>) Correlation between <italic>W</italic> and <inline-formula id="pcbi.1007074.e061"><alternatives><graphic id="pcbi.1007074.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (inside manifold perturbation of <italic>K</italic>) and between <italic>W</italic> and <inline-formula id="pcbi.1007074.e062"><alternatives><graphic id="pcbi.1007074.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (outside manifold perturbation). For comparison a third option with a completely independent <italic>K</italic> is also shown. Error bars indicate standard deviation over 23 different permutations.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g005" xlink:type="simple"/>
</fig>
<p>We then exposed this model to the same type of perturbations of the encoders <italic>K</italic> as we did for the previous models. However, while only one non-identity permutation matrix exists for <italic>D</italic> = 2, there are 4! − 1 = 23 non-identity permutations for <italic>D</italic> = 4. Still restricting our choice of perturbations to permutations, there are hence 23 possibilities for <inline-formula id="pcbi.1007074.e063"><alternatives><graphic id="pcbi.1007074.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. We calculated the element-wise correlation between <italic>W</italic> and <inline-formula id="pcbi.1007074.e064"><alternatives><graphic id="pcbi.1007074.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for all 23 perturbations. Similarly, we generalized <xref ref-type="disp-formula" rid="pcbi.1007074.e034">Eq 19</xref> to 4-fold permutations and calculated the element-wise correlation between <italic>W</italic> and <inline-formula id="pcbi.1007074.e065"><alternatives><graphic id="pcbi.1007074.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mover accent="true"><mml:mi>W</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for all 23 non-identity permutations. The correlations for inside-manifold perturbations were significantly higher than outside-manifold perturbations (<xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5E</xref>, <italic>p</italic> = 6.1 ⋅ 10<sup>−7</sup>, dependent samples t-test). Note that the same <italic>C</italic> was used for all weight matrices and that this incurs some structure that increases the correlation. To assess the magnitude of this effect, we redrew 23 independent encoder matrices <italic>K</italic>′, computed the corresponding weight matrices and correlated them with <italic>W</italic>. These correlations were not significantly different than the outside-manifold perturbations (<xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5E</xref>, <italic>p</italic> = 0.12, dependent samples t-test). In other words: although outside-manifold perturbations evince non-zero correlations, they still require changing the synaptic weights almost as much as when learning a completely new set of neural modes.</p>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>Low-dimensionality has recently emerged as a prominent feature of the neuronal activity in the neocortex [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref029">29</xref>]. There is a growing interest in understanding the origin and maintenance of the low-dimensional activity. Several studies have proposed mechanisms and constrains necessary to generate low-dimensional activity in recurrent neural networks [<xref ref-type="bibr" rid="pcbi.1007074.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref031">31</xref>]. An important feature of the low-dimensional activity of the neurons is that animals find it very hard to generate activity that lies outside the intrinsic manifold [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>]. In this article, for the first time, we provide an explanation of this experimental observation.</p>
<p>We used three popular frameworks to design functional networks—Liquid State Machines with FORCE learning, the Neural Engineering Framework and Efficient Coding networks. First we demonstrate that there is a close similarity between the neural modes and a parameter matrix here called the <italic>encoders</italic> (<italic>K</italic>). Assuming the brain employs something akin to these frameworks, we argue that altering the neural modes (<italic>Z</italic>) would require changing the encoders (<italic>K</italic>). The encoders, in turn, are related to the recurrent connectivity within the network (<italic>W</italic>) in such a manner that certain large changes to the encoders do not require similarly large changes of the synaptic weights. This type of changes corresponds to <italic>inside-manifold perturbations</italic> if we follow the terminology used by Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>]. By contrast, <italic>outside-manifold</italic> perturbations require large changes in the recurrent synaptic weights. Because learning is commonly believed to be implemented in terms of synaptic plasticity, our results provide an explanation of the difference in learning difficulty for the two types of perturbations as reported by Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>].</p>
<p>Whether the brain does employ anything like these frameworks remains an open question. However, these frameworks provide more insights about the nature of low-dimensional activity and learning difficulty than other competing models of emergence of neural modes. For instance, although a clustered architecture also results in low dimensional activity [<xref ref-type="bibr" rid="pcbi.1007074.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref006">6</xref>], clusters are not consistent with direct measurements of neural activity [<xref ref-type="bibr" rid="pcbi.1007074.ref006">6</xref>]. Alternatively, one can argue that neural modes could be a consequence of feed-forward connectivity. This would certainly be possible for circuits with strong, low-dimensional inputs and weak or absent lateral connections. However, this merely transfers the problem of creating a low dimensional signal to the afferent circuit. Solving this problem requires structured connections at some stage. The strength of the models we have used is precisely that they intrinsically ensures low-dimensional activity by implicitly creating a subspace attractor pulling the activity back to the space of neural modes [<xref ref-type="bibr" rid="pcbi.1007074.ref014">14</xref>]. In other words, components of the input orthogonal to this subspace are rapidly canceled out.</p>
<p>On the practical side, the architecture is particularly useful for studying neural modes in spiking networks because by using the frameworks described in this article the experimenter is free to explicitly specify any number of latent variables, any particular neural modes and any dynamics of the latent variables. For this purpose, it is important to emphasize that the architecture and the frameworks should be seen as recipes for specifying the synaptic weights and not as anatomical predictions: the readout nodes of <xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref> do not reflect any biological entity, and the helper matrices <italic>K</italic> and Φ do not exist other than as components of the matrix of synaptic weights <italic>W</italic>. This distinction is also crucial for our results: the three frameworks in their simple forms optimize Φ and not <italic>W</italic>, yet our prediction is only that <italic>W</italic> will be similar before and after an inside-manifold perturbation. In fact, Φ and <inline-formula id="pcbi.1007074.e066"><alternatives><graphic id="pcbi.1007074.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mover accent="true"><mml:mo>Φ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> will in general be uncorrelated, which makes it difficult to estimate learning time or convergence rates.</p>
<p>One might expect the matrix of synaptic weights <italic>W</italic> for <italic>any</italic> network with neural modes to be decomposable into two matrices <italic>K</italic> and Φ [<xref ref-type="bibr" rid="pcbi.1007074.ref007">7</xref>]. However, such decomposition can only be done when <italic>W</italic> has rank ≤<italic>D</italic> and as we show in <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5B</xref>, it is possible to construct a network with neural modes but with a non-decomposable weight matrix. Lifting the requirement that the weight matrix must be decomposable is likely favorable when extending this work to networks with a heterogeneous populations of neurons and synapses (for example, it is difficult to decompose a weight matrix if each synapse not only has a different weight but also different time constant). Recently Mastrogiuseppe and Ostojic [<xref ref-type="bibr" rid="pcbi.1007074.ref031">31</xref>] have argued that dense low rank matrices give low dimensional activity. Here, we show that this is not a necessary condition and that balanced and sparse weight matrices can also give rise to low dimensional activity.</p>
<p>Here, we have limited our scope to study a local network in isolation. While sufficient for comparing inside and outside manifold perturbations, this limitation masks the fact that the latent variables are most likely evolving not only as a result of local dynamics (<italic>A</italic> in <xref ref-type="disp-formula" rid="pcbi.1007074.e053">Eq 21</xref> and <italic>f</italic>(⋅) in <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref>), but also due to structured inputs (<xref ref-type="fig" rid="pcbi.1007074.g001">Fig 1B</xref>). Indeed, in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref>, we have exaggerated the strength and accuracy of inputs and postulated that they are strong enough to instantly overwrite the currently encoded variables. In reality, this is most likely not the case. Rather, we expect the time evolution of the latent variables to be a consequence of a deliberate combination of afferent inputs and local feedback (see [<xref ref-type="bibr" rid="pcbi.1007074.ref032">32</xref>] for an example of how a large set of local NEF-circuits can be connected to a functional whole).</p>
<p>The fact that latent variable dynamics can arise from two sources makes it precarious to draw conclusions from <xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4</xref>. For example, it is tempting to assume that inside-manifold perturbations would also be hard to learn during tasks involving high-frequency motions. However, this is not necessarily the case because even in an integrator network it is possible for the latent variables to oscillate if the inputs are oscillating (although they will be low-pass filtered). The prediction of our model is therefore that the difference between inside- and outside-manifold perturbations will be more pronounced in networks where the <italic>local</italic> dynamics imposed on latent variables are slow or stationary, and vanish completely for networks with <italic>intrinsically</italic> rapidly oscillating latent variables (<xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4</xref>). It would be interesting to measure the spectra of the latent variables (and not that of the neuronal activity) to know whether the cortical networks operate in the high or low frequency regimes (<xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4</xref>). Tentative evidence suggests it is in the low frequency range (about 1 Hz) [<xref ref-type="bibr" rid="pcbi.1007074.ref021">21</xref>].</p>
<p>The presence of structured inputs raises the question: what really changes in the network when an inside-manifold perturbation is applied? Consider the networks in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref>. They are designed to have two neural modes and two latent variables. The dynamical equations of the two latent variables are however identical (integrate the inputs) and therefore, once a network has been created, the numbering of neural modes becomes arbitrary. PCA provides one way of assigning order by considering the variance explained be each neural mode, which in turn depends on the variance of the corresponding latent variable. In an integrator network the variance of each latent variable is completely determined by the inputs. Thus, for integrator networks, a permutation of the neural modes becomes equivalent to a permutation of the inputs. Indeed, when <italic>Q</italic> is a permutation matrix, <xref ref-type="disp-formula" rid="pcbi.1007074.e022">Eq 15</xref> states that changing the order of the neural modes does not imply <italic>any</italic> changes in the weight matrix (in the ideal case, or with Efficient Coding)—the only thing changing is the inputs.</p>
<p>To verify that an inside manifold perturbation really does change something meaningful in the local network in a more general setting, one has to consider networks where the dynamical equations are not identical for all latent variables. For example, the latent variables of the network described in <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5</xref> do have non-identical dynamical equations (<xref ref-type="disp-formula" rid="pcbi.1007074.e076">Eq 35</xref>). In such a network we can directly read how inside manifold perturbation alters the latent variable calculated using the <italic>same</italic> decoding weights for the unperturbed and perturbed networks (compare <xref ref-type="fig" rid="pcbi.1007074.g006">Fig 6A and 6B</xref>). It is evident that the order of the latent variables have changed accordingly. Note that there are no structured inputs to this network and the permutation of the latent variables in this case stems completely from changing the local weights of the network. If we allow the readout weights abstractly represent the coefficients of the BCI interface as was done in Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>], <xref ref-type="fig" rid="pcbi.1007074.g006">Fig 6</xref> shows an example of how a modification to the local circuitry can alter the BCI readout in a systematic way. Recently, Golub et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref033">33</xref>] provided some evidence that learning of inside-manifold perturbations may indeed be achieved by reassociation of latent variables with neural modes. With our results we argue that reassociation of latent modes may involves small changes in the network connectivity. It is also possible that the input to the network are altered to achieve inside-manifold perturbations [<xref ref-type="bibr" rid="pcbi.1007074.ref034">34</xref>]. However, changes the input requires changes in the manifold of the activity of an upstream network.</p>
<fig id="pcbi.1007074.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1007074.g006</object-id>
<label>Fig 6</label>
<caption>
<title>A permutation of the columns of <italic>K</italic> results in a permutation of the latent variables.</title>
<p>(<bold>A</bold>) A readout of the four latent variables of the unperturbed network shown in <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5</xref>. (<bold>B</bold>) A readout of the latent variables of one of the perturbed networks (inside manifold permutation 3 4 1 2) using the readout weights from the unperturbed network, which results in a corresponding permutation of the latent variables. Note that this network receives no structured inputs and that the permutation of the latent variables is completely due changes in the local connections.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.g006" xlink:type="simple"/>
</fig>
<p>Although we have suggested one possible explanation of the findings of Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>], our models are not intended to faithfully reproduce all aspects of their experiment. For example, in our simulations we use small networks that do not match the number of neurons in primate motor cortex, and we have simplified the intrinsic manifolds to be 2- or 4-dimensional while the recorded manifolds had around 10 dimensions. The conceptual result however holds for larger manifolds as well (as shown in <xref ref-type="supplementary-material" rid="pcbi.1007074.s004">S3A Fig</xref> for an NEF network). Another discrepancy with the experiments is that extracellular field recordings necessarily only sample a small subset of neurons in the network. By contrast, in our networks we have estimated the dimensionality using all the neurons. Subsampling of neurons does not affect our results, as long as the sampling of neurons represent an independent and identically distributed sample of the network and are independent with respect to the neural modes (as shown in <xref ref-type="supplementary-material" rid="pcbi.1007074.s004">S3B Fig</xref> for NEF network). However, spatial and cell-type inhomogenities (as is often the case with the networks in the brain) might lead to a dependent sampling so that the intrinsic manifold seen by experimenters does not fully match the true one of the network.</p>
<p>We have demonstrated that the difference between inside and outside manifold perturbations is independent of the choice of framework by repeating most simulations in the three frameworks. However, our results should not be considered a fair comparison between the performance of the three frameworks because the parameters and setup were slightly different between the frameworks (see <xref ref-type="sec" rid="sec009">Methods</xref>). Furthermore, for simplicity we used comparatively simple versions of the frameworks. For example we only considered LIF-neurons, although all three frameworks are compatible with more biologically realistic neuron models [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1007074.ref036">36</xref>]. Moreover, online learning rules for the NEF [<xref ref-type="bibr" rid="pcbi.1007074.ref037">37</xref>] and Efficient coding [<xref ref-type="bibr" rid="pcbi.1007074.ref038">38</xref>] have been proposed, but are not considered here. To verify that our claim allows for biological constraints, we created an example by making a rather straight-forward modification of the NEF that yields a biologically plausible weight matrix. Doing this however requires solving <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref> explicitly which is computationally expensive, and more sophisticated methods exists for enforcing Dale’s law in the NEF [<xref ref-type="bibr" rid="pcbi.1007074.ref039">39</xref>].</p>
<p>Elsayed and Cunningham [<xref ref-type="bibr" rid="pcbi.1007074.ref040">40</xref>] recently argued that neural modes might not be unexpected given long-known properties of tuning and correlation of neurons. Indeed, while the columns of <italic>Z</italic> in <xref ref-type="disp-formula" rid="pcbi.1007074.e001">Eq 1</xref> correspond to neural modes, the <italic>rows</italic> of <italic>Z</italic> correspond to preferred directions. That is, if the latent variables happen to be analogous to some behaviorally measurable quantities, neuron <italic>j</italic> will exhibit classical cosine-tuning [<xref ref-type="bibr" rid="pcbi.1007074.ref041">41</xref>] with respect to the angle between the vector given by row <italic>j</italic> of <italic>Z</italic> and the vector of latent variables <bold>x</bold>. This raises the philosophical question of whether we see neural modes because the neurons are tuned, or whether we see neuron tunings because the network has neural modes. In either case, we argue that areas known to exhibit population vector codes are particularly likely to be subject to the kind of architectural framework treated in this work. Notably, one such area is the primary motor cortex [<xref ref-type="bibr" rid="pcbi.1007074.ref041">41</xref>], which is the area Sadtler et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref004">4</xref>] recorded from. In conclusion, the approach we have taken here provides a relation between the synaptic weights and not only neural modes but also the general phenomenon of population vector codes.</p>
</sec>
<sec id="sec009" sec-type="materials|methods">
<title>Methods</title>
<p>We used three different frameworks for constructing the networks: a Liquid State Machine with FORCE learning, the Neural Engineering Framework and Efficient Coding. The purpose of employing all three frameworks is to demonstrate that they all relate to neural modes in a similar fashion. Because we did not choose comparable parameters for the three types of networks, the differences in their performances (in for instance <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref>) are not an indicative of their intrinsic ability.</p>
<p>All three frameworks require some tuning of parameters to achieve acceptable performance. The most important one is the time constant <italic>τ</italic><sub>syn</sub> of the synaptic kernel. For all synapses (except the fast synapses in Efficient Coding), we used a simple exponential kernel
<disp-formula id="pcbi.1007074.e067"><alternatives><graphic id="pcbi.1007074.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e067" xlink:type="simple"/><mml:math display="block" id="M67"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>t</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mi>t</mml:mi> <mml:msub><mml:mi>τ</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>t</mml:mi> <mml:mo>≥</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula>
A longer time constant greatly improved both the smoothness of the synaptic currents <italic>I</italic><sub><italic>j</italic></sub> and the memory of the circuit, thus reducing noise and drift of the latent variables.</p>
<sec id="sec010">
<title>FORCE learning</title>
<p>We implemented the network described by Nicola and Clopath [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>]. We normalized the units of <xref ref-type="disp-formula" rid="pcbi.1007074.e002">Eq 2</xref> so that
<disp-formula id="pcbi.1007074.e068"><alternatives><graphic id="pcbi.1007074.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mrow><mml:msub><mml:mi>R</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mspace width="2.em"/><mml:msub><mml:mi>V</mml:mi> <mml:mtext>th</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="2.em"/><mml:msub><mml:mi>V</mml:mi> <mml:mtext>reset</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mspace width="2.em"/><mml:msub><mml:mi>V</mml:mi> <mml:mtext>leak</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
Setting the reverse potential at the threshold causes the neurons to be tonically active, removing the need of a driving current to start the network.</p>
<p>Following [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>] we set the membrane time constant to <italic>τ</italic><sub><italic>m</italic></sub> = 10 ms, the synaptic time constant to <italic>τ</italic><sub>syn</sub> = 20 ms, and added a <italic>t</italic><sub>ref</sub> = 2 ms refractory time after each spike during which the membrane voltage was clamped to <italic>V</italic><sub>reset</sub>. Each element in the reservoir weight matrix Ω had a 90% probability of being set to 0, and is otherwise drawn from a normal distribution with zero mean and standard deviation
<disp-formula id="pcbi.1007074.e069"><alternatives><graphic id="pcbi.1007074.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e069" xlink:type="simple"/><mml:math display="block" id="M69"><mml:mrow><mml:mi>σ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>G</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula>
where <italic>G</italic> = 10 and <italic>N</italic> = 1000. In addition, the reservoir weights were balanced so that the average input weight to each neuron was 0.</p>
<p>The elements of encoder matrix <italic>K</italic> were drawn uniformly from [−100, 100] and the initial decoders Φ(0) where 0. The helper matrix <italic>P</italic> was initialized to
<disp-formula id="pcbi.1007074.e070"><alternatives><graphic id="pcbi.1007074.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e070" xlink:type="simple"/><mml:math display="block" id="M70"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>5</mml:mn> <mml:mo>·</mml:mo> <mml:msup><mml:mn>10</mml:mn> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>6</mml:mn></mml:mrow></mml:msup> <mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>The decoders and the the helper matrix were subsequently updated every 10th time-step when learning was active, using the following update rules:
<disp-formula id="pcbi.1007074.e071"><alternatives><graphic id="pcbi.1007074.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">e</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1007074.e072"><alternatives><graphic id="pcbi.1007074.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e072" xlink:type="simple"/><mml:math display="block" id="M72"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula>
where <bold>e</bold>(<italic>t</italic>) = <bold>x</bold>(<italic>t</italic>) − <bold>x</bold><sub>target</sub>(<italic>t</italic>) is the error in the current decoding. The helper matrix <inline-formula id="pcbi.1007074.e073"><alternatives><graphic id="pcbi.1007074.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>×</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is an online estimate of the inverse of the correlation matrix of <bold>u</bold>(<italic>t</italic>) (see [<xref ref-type="bibr" rid="pcbi.1007074.ref012">12</xref>]).</p>
<p>The network was simulated for a total of 50 seconds for each experiment, with time-step 0.05 ms. During the first 45 seconds, the network was repeatedly exposed to the 2.5 second long target pattern (the steps shown in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A</xref> and a sine and cosine for <xref ref-type="fig" rid="pcbi.1007074.g004">Fig 4</xref>) while learning (<xref ref-type="disp-formula" rid="pcbi.1007074.e072">Eq 33</xref>) was active. During the last 5 seconds learning was not active and the last 2.5 seconds are shown in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A</xref>.</p>
<p>Learning was made more difficult in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2</xref> because of the instantaneous jumps in the target signal. To compensate for this, the network was first trained without the feedback connection. Instead, the target signal was used as input to the network. The feedback was then gradually reintroduced until the network was able independently keep the variables stable after the imposed jumps of latent variables. Note that because the illustration in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2A</xref> is using the same latent variable values as was used during training, it is not an unbiased performance test.</p>
</sec>
<sec id="sec011">
<title>Neural Engineering Framework</title>
<p>We used the NEF implementation in the simulator package Nengo [<xref ref-type="bibr" rid="pcbi.1007074.ref042">42</xref>] (version 2.6.0). We kept the default parameters of Nengo, but changed the synaptic time constant to <italic>τ</italic><sub>syn</sub> = 10 ms (membrane time constant was left at <italic>τ</italic><sub><italic>m</italic></sub> = 20 ms). Similarly, we used Nengo’s default way of choosing encoders <italic>K</italic> which is to draw each row uniformly from the unit sphere. However, we changed the <italic>max_rates</italic> parameter that controls the scaling of <italic>K</italic> so that the maximal firing rate of each neuron fell between 80 and 120 Hz. From the maximal firing rates the <italic>bias</italic> (<italic>I</italic><sub>drive</sub> in <xref ref-type="disp-formula" rid="pcbi.1007074.e011">Eq 7</xref>) and the <italic>gain</italic> were determined automatically by Nengo for each neuron.</p>
<p>The simulation in Nengo is done in two steps. First, an estimated solution of <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref> is found and used to connect the network. Second, the network is simulated. To estimate the solution of <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref>, Nengo draws a number of <italic>evaluation points</italic> from the space of allowed values of <bold>x</bold><sub>target</sub>. We used Nengo’s default space of allowed values, -1 to 1 for each latent variable, as well Nengo’s default heuristic to determine the number of evaluation points, which gave 2000 points. For each point, Nengo estimates what the expected filtered spike trains <bold>u</bold>(<italic>t</italic>) would be for that point, and then find the least-squares optimal solution to <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref> where the average is taken over the evaluation points.</p>
<p>To simulate fixation of variables during the shaded periods in <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2B</xref>, we connected the <italic>ensemble</italic> in Nengo to a <italic>node</italic> that was in turn connected back to the ensemble. The node was programmed to output the fixed signals during the shaded periods and to simply relay its input otherwise. In the non-shaded periods this is equivalent to recurrently connecting the ensemble to itself. We simulated the network for 2.5 seconds using the default Nengo time-step 1 ms.</p>
</sec>
<sec id="sec012">
<title>Efficient coding</title>
<p>We implemented the network described by Boerlin et al. [<xref ref-type="bibr" rid="pcbi.1007074.ref015">15</xref>]. We normalized the membrane voltage so that <italic>V</italic><sub>leak</sub> = 0 and <italic>R</italic><sub><italic>m</italic></sub> = 1. The threshold was set individually for each neuron to
<disp-formula id="pcbi.1007074.e074"><alternatives><graphic id="pcbi.1007074.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e074" xlink:type="simple"/><mml:math display="block" id="M74"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi> <mml:mtext>th</mml:mtext> <mml:mi>j</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>ν</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>μ</mml:mi> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mi>n</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(34)</label></disp-formula>
where <italic>ν</italic> = 10<sup>−3</sup>, <italic>μ</italic> = 10<sup>−6</sup> and <bold>k</bold><sub><italic>j</italic></sub> is the <italic>j</italic>th row of <italic>K</italic>. No explicit reset is needed after a spike. Instead, the fast reservoir connections includes inhibitory autapses (from Eqs <xref ref-type="disp-formula" rid="pcbi.1007074.e014">10</xref> and <xref ref-type="disp-formula" rid="pcbi.1007074.e015">11</xref>) which decrease <italic>V</italic><sub><italic>m</italic></sub> by <inline-formula id="pcbi.1007074.e075"><alternatives><graphic id="pcbi.1007074.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi>μ</mml:mi> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mi>n</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:msub><mml:mi mathvariant="bold">k</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> after each spike. We used the same membrane time constant <italic>τ</italic><sub><italic>m</italic></sub> = 50 ms as [<xref ref-type="bibr" rid="pcbi.1007074.ref015">15</xref>], but chose a shorter synaptic time constant <italic>τ</italic><sub>syn</sub> = 20 ms ⇒ λ<sub>syn</sub> = 50 Hz.</p>
<p>For <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2G</xref>, the latent variables actually used in the simulation were 100 times bigger than indicated on the axis. However, since the units of the latent variables are arbitrary we scaled it down in the plot to make comparison easier between the frameworks. To create the figure we simulated the network for 2.5 seconds with time-step 0.1 ms.</p>
</sec>
<sec id="sec013">
<title>Customized learning rule for NEF</title>
<p>To demonstrate that our result can generalize to a biologically plausible weight matrix we modified Nengo to solve <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref> instead of <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref> during its building phase. We implemented this modification as a subclass of the <italic>Solver</italic> class in Nengo. Our subclass takes as input a matrix <italic>C</italic> and when called from Nengo, it uses the scipy routine <italic>nnls</italic> to solve the constrained optimization in <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref>. Similarly, when solving <xref ref-type="disp-formula" rid="pcbi.1007074.e012">Eq 8</xref>, the average in <xref ref-type="disp-formula" rid="pcbi.1007074.e058">Eq 25</xref> is estimated over a number of evaluation points. In the biologically plausible case, we explicitly increased the number of evaluation points to 40,000.</p>
<p>We did a few more changes to increase the biological plausibility compared to the previous simulations. First, we increased the network size to <italic>N</italic> = 5000 neurons and the number of latent variables to <italic>D</italic> = 4. We also chose a non-linear dynamical system for the latent variables to follow. Namely, we chose <italic>f</italic>(⋅) so that the first two encoded values would be the leading and lagging components of a 2 Hz oscillator and the last two would similarly be the lead and lag of a 4 Hz oscillator. Additionally, we added a non-linear term causing the amplitude to converge to 1. With these terms combined, the latent variables <bold>x</bold>(<italic>t</italic>) evolves as:
<disp-formula id="pcbi.1007074.e076"><alternatives><graphic id="pcbi.1007074.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e076" xlink:type="simple"/><mml:math display="block" id="M76"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>˙</mml:mo></mml:mover></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>ω</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>)</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>˙</mml:mo></mml:mover></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>ω</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>)</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>˙</mml:mo></mml:mover></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mn>2</mml:mn> <mml:mi>ω</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>4</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mn>3</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>4</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>)</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:msub><mml:mi>x</mml:mi> <mml:mn>4</mml:mn></mml:msub> <mml:mo>˙</mml:mo></mml:mover></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>ω</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi> <mml:mn>3</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mn>4</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt> <mml:mo>)</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula>
We used <italic>α</italic> = 0.2 which is sufficient to help stabilize the dynamics to oscillators with frequencies 2 and 4 Hz even in the absence of structured inputs. In the simulations shown in <xref ref-type="fig" rid="pcbi.1007074.g006">Fig 6</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007074.s003">S2 Fig</xref>, we initialized the latent variables <italic>x</italic><sub>1</sub>…<italic>x</italic><sub>4</sub> to 0.</p>
</sec>
<sec id="sec014">
<title>Simulation tools</title>
<p>Simulation and training of networks designed according to Liquid State Machine and Efficient Coding frameworks were performed using custom scripts written in Julia [<xref ref-type="bibr" rid="pcbi.1007074.ref043">43</xref>]. Networks designed according to the NEF were implemented using NENGO [<xref ref-type="bibr" rid="pcbi.1007074.ref042">42</xref>].</p>
</sec>
<sec id="sec015">
<title>Measuring similarities</title>
<p>In <xref ref-type="fig" rid="pcbi.1007074.g002">Fig 2C, 2F and 2I</xref>, we want to measure the similarity between the subspaces spanned by each pair of matrices (<italic>K</italic>, Φ<sup><italic>T</italic></sup>, <italic>Z</italic><sub>PCA</sub> and <italic>Z</italic><sub>PCA</sub>). To this end we measured the cosine of the mean principal angles (as computed by the scipy routine <italic>subspace_angles</italic>, see also [<xref ref-type="bibr" rid="pcbi.1007074.ref044">44</xref>]). This value ranges from 0 (unrelated, orthogonal space) to 1 (exactly the same space). This measure was also used in <xref ref-type="supplementary-material" rid="pcbi.1007074.s003">S2C Fig</xref>.</p>
<p>In <xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1A, S1B and S1C Fig</xref> on the other hand, we sought to demonstrate that the neural modes have indeed been permuted along with the encoders. This change is happening <italic>within</italic> the manifold/subspace, so angles between subspaces are no longer meaningful. Therefore, specifically for <xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1A, S1B and S1C Fig</xref>, we instead calculated the average of the absolutes of the correlations between the columns:
<disp-formula id="pcbi.1007074.e077"><alternatives><graphic id="pcbi.1007074.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mrow><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mrow><mml:mo>|</mml:mo> <mml:mtext>corr</mml:mtext></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>1000</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>1000</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>+</mml:mo> <mml:mo>|</mml:mo> <mml:mtext>corr</mml:mtext></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>1000</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mn>1000</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(36)</label></disp-formula></p>
<p>Finally, for comparing weight matrices in Figs <xref ref-type="fig" rid="pcbi.1007074.g003">3</xref>–<xref ref-type="fig" rid="pcbi.1007074.g005">5</xref>, we calculated the correlation over all pairs of old and new weights (i.e. element-wise). We have also reported the Frobenius norm of the difference in <xref ref-type="supplementary-material" rid="pcbi.1007074.s002">S1D</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007074.s003">S2D</xref> Figs.</p>
</sec>
</sec>
<sec id="sec016">
<title>Supporting information</title>
<supplementary-material id="pcbi.1007074.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>In this text we describe how inside manifold perturbations would affect the connectivity matrix <italic>W</italic> when the perturbation matrix <inline-formula id="pcbi.1007074.e078"><alternatives><graphic id="pcbi.1007074.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1007074.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is non-orthogonal.</title>
<p>This analysis complements the analysis shown in Eqs <xref ref-type="disp-formula" rid="pcbi.1007074.e017">13</xref>–<xref ref-type="disp-formula" rid="pcbi.1007074.e022">15</xref> in the main text.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007074.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Supplementary to <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3</xref>.</title>
<p>(<bold>A</bold>) Pairwise correlations (see <xref ref-type="sec" rid="sec009">Methods</xref>) between the four matrices before (subscript “orig” for “original”) and after (subscript “pert” for “perturbed”) an inside-manifold permutation for the FORCE network. (<bold>B</bold>) Same as (A), for the NEF. (<bold>C</bold>) Same as (A), for the Efficient coding framework. (<bold>D</bold>) The same data as in <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>, but with the Frobenius (L2) norm of the difference between the matrices instead of correlation.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007074.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Supplementary to <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5</xref>.</title>
<p>(<bold>A</bold>) Spike times of 80 excitatory and 20 inhibitory neurons for the first 2.5 seconds of the simulation. (<bold>B</bold>) The variance explained by the principal components of the binned spike trains. Note that there are four clear dimensions in spite of the fact that the weight matrix has more the four singular values (<xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5B</xref>). (<bold>C</bold>) The cosine of the mean principal angle between the subspace spanned by the columns of each pair of matrices. (<bold>D</bold>) Same data as in <xref ref-type="fig" rid="pcbi.1007074.g005">Fig 5E</xref>, but with Frobenius (L2) norm instead of correlation and with the difference between networks with the same encoders (i.e. no perturbation, red).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1007074.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1007074.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Effect of varying the number of dimensions and number of sampled neurons.</title>
<p>(<bold>A</bold>) Extension of <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref> to higher dimensions. Here we show results for an NEF-network after no perturbation, an inside-manifold permutation and an outside-manifold permutation (other perturbations and networks from <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref> are not shown). Error bars indicate standard deviation across five different realizations of the respective perturbation. As the number of latent variables increases, different realizations of the same network start to differ more. However, this difference is not larger for inside-manifold perturbations than unperturbed reinstantiation. Outside-manifold perturbation require big changes (red line) irrespective of the number of latent variables. This is similar to what was shown in in <xref ref-type="fig" rid="pcbi.1007074.g003">Fig 3C</xref>. (<bold>B</bold>) Same data as shown in <xref ref-type="supplementary-material" rid="pcbi.1007074.s003">S2B Fig</xref>, but when only using a random subset of the neurons to calculate the principal components. For each number, an independent subset of neurons was selected. Note that subsampling in this fashion does not influence the dimensionality as long as <italic>N</italic> ≫ <italic>D</italic>. In particular, note that dimensionality is an estimation of the rank of the matrix of spikes per bin per neuron (number of bins × nubmer of neurons). Subsampling the columns of this matrix can only decrease the rank, i.e. the dimensionality.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1007074.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mazor</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>. <article-title>Transient dynamics versus fixed points in odor representations by locust antennal lobe projection neurons</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>48</volume>(<issue>4</issue>):<fpage>661</fpage>–<lpage>673</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.09.032" xlink:type="simple">10.1016/j.neuron.2005.09.032</ext-link></comment> <object-id pub-id-type="pmid">16301181</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bisley</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Roitman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KD</given-names></name>. <article-title>One-dimensional dynamics of attention and decision making in LIP</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>58</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.01.038" xlink:type="simple">10.1016/j.neuron.2008.01.038</ext-link></comment> <object-id pub-id-type="pmid">18400159</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>BM</given-names></name>. <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature Neurosci</source>. <year>2014</year>;<volume>17</volume>(<issue>11</issue>):<fpage>1500</fpage>–<lpage>1509</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3776" xlink:type="simple">10.1038/nn.3776</ext-link></comment> <object-id pub-id-type="pmid">25151264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sadtler</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Quick</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Golub</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Chase</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <etal>et al</etal>. <article-title>Neural constraints on learning</article-title>. <source>Nature</source>. <year>2014</year>;<volume>512</volume>(<issue>7515</issue>):<fpage>423</fpage>–<lpage>426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature13665" xlink:type="simple">10.1038/nature13665</ext-link></comment> <object-id pub-id-type="pmid">25164754</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mazzucato</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fontanini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>La Camera</surname> <given-names>G</given-names></name>. <article-title>Stimuli reduce the dimensionality of cortical activity</article-title>. <source>Front Syst Neurosci</source>. <year>2016</year>;<volume>10</volume>:<fpage>11</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2016.00011" xlink:type="simple">10.3389/fnsys.2016.00011</ext-link></comment> <object-id pub-id-type="pmid">26924968</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Williamson</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Cowley</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>MA</given-names></name>, <etal>et al</etal>. <article-title>Scaling properties of dimensionality reduction for neural populations and network models</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005141" xlink:type="simple">10.1371/journal.pcbi.1005141</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Murray</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Bernacchia</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Roy</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Constantinidis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <etal>et al</etal>. <article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2016</year>;<volume>114</volume>(<issue>2</issue>):<fpage>394</fpage>–<lpage>399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1619449114" xlink:type="simple">10.1073/pnas.1619449114</ext-link></comment> <object-id pub-id-type="pmid">28028221</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gao</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>. <article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2015</year>;<volume>32</volume>:<fpage>148</fpage>–<lpage>155</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2015.04.003" xlink:type="simple">10.1016/j.conb.2015.04.003</ext-link></comment> <object-id pub-id-type="pmid">25932978</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gallego</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Perich</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Solla</surname> <given-names>SA</given-names></name>. <article-title>Neural manifolds for the control of movement</article-title>. <source>Neuron</source>. <year>2017</year>;<volume>94</volume>(<issue>5</issue>):<fpage>978</fpage>–<lpage>984</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2017.05.025" xlink:type="simple">10.1016/j.neuron.2017.05.025</ext-link></comment> <object-id pub-id-type="pmid">28595054</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Depasquale</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name>. <article-title>Building functional networks of spiking model neurons</article-title>. <source>Nature Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>350</fpage>–<lpage>355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4241" xlink:type="simple">10.1038/nn.4241</ext-link></comment> <object-id pub-id-type="pmid">26906501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>63</volume>(<issue>4</issue>):<fpage>544</fpage>–<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nicola</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>. <article-title>Supervised learning in spiking neural networks with FORCE training</article-title>. <source>Nature Communications</source>. <year>2017</year>;<volume>8</volume>:<fpage>2208</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-01827-3" xlink:type="simple">10.1038/s41467-017-01827-3</ext-link></comment> <object-id pub-id-type="pmid">29263361</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref013">
<label>13</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>CH</given-names></name>. <chapter-title>Neural Engineering</chapter-title>. <source>Computation, representation, and dynamics in neurobiological systems</source>. <publisher-name>MIT Press</publisher-name>; <year>2003</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>. <article-title>A unified approach to building and controlling spiking attractor networks</article-title>. <source>Neural Comput</source>. <year>2005</year>;<volume>17</volume>(<issue>2003</issue>):<fpage>1276</fpage>–<lpage>1314</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0899766053630332" xlink:type="simple">10.1162/0899766053630332</ext-link></comment> <object-id pub-id-type="pmid">15901399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name>. <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>11</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003258" xlink:type="simple">10.1371/journal.pcbi.1003258</ext-link></comment> <object-id pub-id-type="pmid">24244113</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>. <article-title>Efficient codes and balanced networks</article-title>. <source>Nature Neurosci</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>375</fpage>–<lpage>382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4243" xlink:type="simple">10.1038/nn.4243</ext-link></comment> <object-id pub-id-type="pmid">26906504</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>11</issue>):<fpage>2531</fpage>–<lpage>2560</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602760407955" xlink:type="simple">10.1162/089976602760407955</ext-link></comment> <object-id pub-id-type="pmid">12433288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Haas</surname> <given-names>H</given-names></name>. <article-title>Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</article-title>. <source>Science</source>. <year>2004</year>;<volume>304</volume>:<fpage>78</fpage>–<lpage>79</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Vreeswijk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source>. <year>1996</year>;<volume>274</volume>(<issue>5293</issue>):<fpage>1724</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.274.5293.1724" xlink:type="simple">10.1126/science.274.5293.1724</ext-link></comment> <object-id pub-id-type="pmid">8939866</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Vector Reconstruction from Firing Rates</article-title>. <source>J Comput Neurosci</source>. <year>1994</year>;<volume>1</volume>(<issue>1</issue>):<fpage>89</fpage>–<lpage>107</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF00962720" xlink:type="simple">10.1007/BF00962720</ext-link></comment> <object-id pub-id-type="pmid">8792227</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Nuyujukian</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <etal>et al</etal>. <article-title>Neural population dynamics during reaching</article-title>. <source>Nature</source>. <year>2012</year>;<volume>487</volume>(<issue>7405</issue>):<fpage>1</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11129" xlink:type="simple">10.1038/nature11129</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Strata</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Harvey</surname> <given-names>R</given-names></name>. <article-title>Dale’s principle</article-title>. <source>Brain Res Bull</source>. <year>1999</year>;<volume>50</volume>(<issue>5-6</issue>):<fpage>349</fpage>–<lpage>350</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0361-9230(99)00100-8" xlink:type="simple">10.1016/S0361-9230(99)00100-8</ext-link></comment> <object-id pub-id-type="pmid">10643431</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Braitenberg</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Schüz</surname> <given-names>A</given-names></name>. <source>Cortex: statistics and geometry of neural connectivity</source>. <edition>2nd ed</edition>. <publisher-loc>Berlin Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boucsein</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Nawrot</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Schnepel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Aertsen</surname> <given-names>A</given-names></name>. <article-title>Beyond the cortical column: abundance and physiology of horizontal connections imply a strong role for inputs from the surround</article-title>. <source>Front Neurosci</source>. <year>2011</year>;<volume>5</volume>(<issue>32</issue>):<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kätzel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zemelman</surname> <given-names>BV</given-names></name>, <name name-style="western"><surname>Buetfering</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wölfel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Miesenböck</surname> <given-names>G</given-names></name>. <article-title>The columnar and laminar organization of inhibitory connections to neocortical excitatory cells</article-title>. <source>Nature Neurosci</source>. <year>2010</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2687" xlink:type="simple">10.1038/nn.2687</ext-link></comment> <object-id pub-id-type="pmid">21076426</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Petreanu</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Mao</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sternson</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Svoboda</surname> <given-names>K</given-names></name>. <article-title>The subcellular organization of neocortical excitatory connections</article-title>. <source>Nature</source>. <year>2009</year>;<volume>457</volume>(<issue>7233</issue>):<fpage>1142</fpage>–<lpage>1145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature07709" xlink:type="simple">10.1038/nature07709</ext-link></comment> <object-id pub-id-type="pmid">19151697</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mizuseki</surname> <given-names>K</given-names></name>. <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nature Rev Neurosci</source>. <year>2014</year>;<volume>15</volume>(<issue>4</issue>):<fpage>264</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3687" xlink:type="simple">10.1038/nrn3687</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Compressed Sensing, Sparsity, and Dimensionality in Neuronal Information Processing and Data Analysis</article-title>. <source>Annual review of neuroscience</source>. <year>2012</year>;<volume>35</volume>(<issue>1</issue>):<fpage>485</fpage>–<lpage>508</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-062111-150410" xlink:type="simple">10.1146/annurev-neuro-062111-150410</ext-link></comment> <object-id pub-id-type="pmid">22483042</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gao</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Trautmann</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Byron</surname> <given-names>MY</given-names></name>, <name name-style="western"><surname>Santhanam</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ganguli</surname> <given-names>S</given-names></name>. <article-title>A theory of multineuronal dimensionality, dynamics and measurement</article-title>. <source>bioRxiv</source>. <year>2017</year> <month>Jan</month> <volume>1</volume>:<fpage>214262</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/214262" xlink:type="simple">10.1101/214262</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Pyle</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rosenbaum</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name>. <article-title>Circuit Models of Low-Dimensional Shared Variability in Cortical Networks</article-title>. <source>Neuron</source>. <year>2019</year>;<volume>101</volume>(<issue>2</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.11.034" xlink:type="simple">10.1016/j.neuron.2018.11.034</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mastrogiuseppe</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ostojic</surname> <given-names>S</given-names></name>. <article-title>Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>99</volume>(<issue>3</issue>):<fpage>609</fpage>–<lpage>623</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.07.003" xlink:type="simple">10.1016/j.neuron.2018.07.003</ext-link></comment> <object-id pub-id-type="pmid">30057201</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Choo</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Bekolay</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>DeWolf</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>C</given-names></name>, <etal>et al</etal>. <article-title>A Large-Scale Model of the Functioning Brain</article-title>. <source>Science</source>. <year>2012</year>;<volume>338</volume>(<issue>6111</issue>):<fpage>1202</fpage>–<lpage>1205</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1225266" xlink:type="simple">10.1126/science.1225266</ext-link></comment> <object-id pub-id-type="pmid">23197532</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Golub</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Sadtler</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Oby</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Quick</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Ryu</surname> <given-names>SI</given-names></name>, <name name-style="western"><surname>Tyler-Kabara</surname> <given-names>EC</given-names></name>, <etal>et al</etal>. <article-title>Learning by neural reassociation</article-title>. <source>Nature Neuroscience</source>. <year>2018</year>;<volume>21</volume>(<issue>4</issue>):<fpage>607</fpage>–<lpage>616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-018-0095-3" xlink:type="simple">10.1038/s41593-018-0095-3</ext-link></comment> <object-id pub-id-type="pmid">29531364</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Menendez</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>P</given-names></name>. <article-title>Learning low-dimensional inpus for brain-machine interface control</article-title>. <source>CoSYNE Abstract</source>. <year>2019</year>; p. <fpage>III</fpage>–<lpage>43</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Eliasmith C, Gosmann J, Choo X. BioSpaun: A large-scale behaving brain model with complex neurons. arXiv. 2016; p. 1602.05220v1.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwemmer</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Denéve</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>ET</given-names></name>. <article-title>Constructing precisely computing networks with biophysical spiking neurons</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>35</volume>(<issue>28</issue>):<fpage>10112</fpage>–<lpage>10134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4951-14.2015" xlink:type="simple">10.1523/JNEUROSCI.4951-14.2015</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Bekolay T, Kolbeck C, Eliasmith C. Simultaneous unsupervised and supervised learning of cognitive functions in biologically plausible spiking neural networks. In: 35th Annual Conference of the Cognitive Science Society; 2013. p. 169–174.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Brendel W, Bourdoukan R, Vertechi P, Machens CK, Denéve S. Learning to represent signals spike by spike. Arixv. 2017;.</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Parisien</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>. <article-title>Solving the problem of negative synaptic weights in cortical models</article-title>. <source>Neural Comput</source>. <year>2008</year>;<volume>20</volume>(<issue>6</issue>):<fpage>1473</fpage>–<lpage>1494</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2008.07-06-295" xlink:type="simple">10.1162/neco.2008.07-06-295</ext-link></comment> <object-id pub-id-type="pmid">18254696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elsayed</surname> <given-names>GF</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>JP</given-names></name>. <article-title>Structure in neural population recordings: An expected byproduct of simpler phenomena?</article-title> <source>Nature Neuroscience</source>. <year>2017</year>;<volume>20</volume>(<issue>9</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4617" xlink:type="simple">10.1038/nn.4617</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Georgopoulos</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Kettner</surname> <given-names>RE</given-names></name>. <article-title>Neuronal Population Coding of Movement Direction</article-title>. <source>Science</source>. <year>1986</year>;<volume>233</volume>(<issue>4771</issue>):<fpage>1416</fpage>–<lpage>1419</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.3749885" xlink:type="simple">10.1126/science.3749885</ext-link></comment> <object-id pub-id-type="pmid">3749885</object-id></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bekolay</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bergstra</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hunsberger</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dewolf</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Rasmussen</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Nengo: a Python tool for building large-scale functional brain models</article-title>. <source>Front Neuroinfo</source>. <year>2014</year>;<volume>7</volume>(<issue>48</issue>).</mixed-citation>
</ref>
<ref id="pcbi.1007074.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bezanson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Edelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Karpinski</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shah</surname> <given-names>VB</given-names></name>. <article-title>Julia: A Fresh Approach to Numerical Computing</article-title>. <source>SIAM Review</source>. <year>2017</year>;<volume>59</volume>(<issue>1</issue>):<fpage>65</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1137/141000671" xlink:type="simple">10.1137/141000671</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1007074.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Björck</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Golub</surname> <given-names>GH</given-names></name>. <article-title>Numerical Methods for Computing Angles Between Linear Subspaces</article-title>. <source>Mathematics of Computation</source>. <year>1973</year>;<volume>27</volume>(<issue>123</issue>):<fpage>579</fpage>–<lpage>594</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1090/S0025-5718-1973-0348991-3" xlink:type="simple">10.1090/S0025-5718-1973-0348991-3</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>