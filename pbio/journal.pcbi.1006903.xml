<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01140</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006903</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Mental health and psychiatry</subject><subj-group><subject>Mood disorders</subject><subj-group><subject>Depression</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Science policy</subject><subj-group><subject>Science and technology workforce</subject><subj-group><subject>Careers in research</subject><subj-group><subject>Engineers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Engineers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject></subj-group></article-categories>
<title-group>
<article-title>Models that learn how humans learn: The case of decision-making and its disorders</article-title>
<alt-title alt-title-type="running-head">RNN and decision-making</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7633-9225</contrib-id>
<name name-style="western">
<surname>Dezfouli</surname> <given-names>Amir</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7108-2272</contrib-id>
<name name-style="western">
<surname>Griffiths</surname> <given-names>Kristi</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2996-2188</contrib-id>
<name name-style="western">
<surname>Ramos</surname> <given-names>Fabio</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3476-1839</contrib-id>
<name name-style="western">
<surname>Dayan</surname> <given-names>Peter</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8618-7950</contrib-id>
<name name-style="western">
<surname>Balleine</surname> <given-names>Bernard W.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>School of Psychology, UNSW, Sydney, Australia</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Data61, CSIRO, Australia</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Westmead Institute for Medical Research, University of Sydney, Sydney, Australia</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>University of Sydney, Sydney, Australia</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Gatsby Computational Neuroscience Unit, UCL, London, United Kingdom</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Macke</surname> <given-names>Jakob H</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Stiftung caesar, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>Part of this work was conducted while PD was visiting Uber Technologies. The latter played no role in its design, execution or communication.</p>
</fn>
<fn fn-type="other" id="econtrib001">
<p>‡ These authors are joint senior authors on this work.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">bernard.balleine@unsw.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>6</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>11</day>
<month>6</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>6</issue>
<elocation-id>e1006903</elocation-id>
<history>
<date date-type="received">
<day>6</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>2</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Dezfouli et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006903"/>
<abstract>
<p>Popular computational models of decision-making make specific assumptions about learning processes that may cause them to underfit observed behaviours. Here we suggest an alternative method using recurrent neural networks (RNNs) to generate a flexible family of models that have sufficient capacity to represent the complex learning and decision- making strategies used by humans. In this approach, an RNN is trained to predict the next action that a subject will take in a decision-making task and, in this way, learns to imitate the processes underlying subjects’ choices and their learning abilities. We demonstrate the benefits of this approach using a new dataset drawn from patients with either unipolar (n = 34) or bipolar (n = 33) depression and matched healthy controls (n = 34) making decisions on a two-armed bandit task. The results indicate that this new approach is better than baseline reinforcement-learning methods in terms of overall performance and its capacity to predict subjects’ choices. We show that the model can be interpreted using off-policy simulations and thereby provides a novel clustering of subjects’ learning processes—something that often eludes traditional approaches to modelling and behavioural analysis.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Computational models of decision-making provide a quantitative characterisation of the learning and choice processes behind human actions. Designing a computational model is often based on manual engineering with an iterative process to examine the consistency between different aspects of the model and the empirical data. In practice, however, inconsistencies between the model and observed behaviours can remain hidden behind examined summary statistics. To address this limitation, we developed a recurrent neural network (RNNs) as a flexible type of model that can automatically characterize human decision-making processes without requiring tweaking and engineering. To show the benefits of this new approach, we collected data on a decision-making task conducted on subjects with either bipolar or unipolar depression, as well as healthy controls. The results showed that, indeed, important aspects of decision-making remained uncaptured by typical computational models and even their enhanced variants, but were captured by RNNs automatically. Further, we were able to show that the nature of such processes can be unveiled by simulating the model under various conditions. This new approach can be used, therefore, as a standalone model of decision-making or as a baseline model to evaluate how well other candidate models fit observed data.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000925</institution-id>
<institution>National Health and Medical Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>GNT1089270</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8618-7950</contrib-id>
<name name-style="western">
<surname>Balleine</surname> <given-names>Bernard W.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000925</institution-id>
<institution>National Health and Medical Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>GNT1079561</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-8618-7950</contrib-id>
<name name-style="western">
<surname>Balleine</surname> <given-names>Bernard W.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000324</institution-id>
<institution>Gatsby Charitable Foundation</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3476-1839</contrib-id>
<name name-style="western">
<surname>Dayan</surname> <given-names>Peter</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by a grant from the NHMRC, GNT1089270 to BB. BB was supported by a Senior Principal Research Fellowship from the National Health and Medical Research Council of Australia, GNT1079561; <ext-link ext-link-type="uri" xlink:href="https://nhmrc.gov.au" xlink:type="simple">https://nhmrc.gov.au</ext-link>. PD was partly funded by the Gatsby Charitable Foundation; <ext-link ext-link-type="uri" xlink:href="http://www.gatsby.org.uk" xlink:type="simple">http://www.gatsby.org.uk</ext-link>. The funders played no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="2"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-06-21</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data are available within the manuscript and Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A computational model of decision-making is a mathematical function that inputs past experiences—such as chosen actions and the value of rewards—and outputs predictions about future actions [e.g. <xref ref-type="bibr" rid="pcbi.1006903.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref003">3</xref>]. Typically, experimenters develop such models by specifying a set of structural assumptions along with free parameters that allow the model to produce a range of behaviors. The models are then fitted to the observed behaviors in order to obtain the parameter settings that make the model’s predictions as close as possible to the empirical data. Nevertheless, if the actual learning and choice processes used by real human subjects differ from those assumptions, e.g., if a single learning-rate parameter is assumed to update the effects of reward and punishment on action values when they are in fact modulated by different learning-rates, then the model will misfit the data [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref004">4</xref>]. To overcome this problem, computational modelling often involves an iterative process that includes additional analyses to assess assumptions about model behavior, subsequent emendation of the structural features of the model to reduce residual fitting error, then new analyses, and so forth. The final model is that which is simplest and misfits the least. This iterative process has been common practise for model development in domains such as cognitive science, computational psychiatry, and model-based analyses of neural data [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref010">10</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref011">11</xref>]. This approach is, however, limited from two standpoints: (i) It is typically unclear when to stop iterating over models. This is because in each iteration the unexplained variance in the data can be either attributed to the natural randomness of humans actions, which implies that no further model improvement is required, or to the lack of a mechanism in the model to absorb the remaining variance, which implies that further iterations are required. (ii) Even if it is believed that further iterations are required, improving the model will be mostly based on manual engineering in the hope of finding a new mechanism that, when added to the model, provides a better explanation for the data.</p>
<p>Here to address these limitations we consider an alternative approach based on recurrent neural networks (RNNs); a flexible class of models that make minimal assumptions about the underlying learning processes used by the subject and that are known to have sufficient capacity to represent any form of computational process [<xref ref-type="bibr" rid="pcbi.1006903.ref012">12</xref>], including those believed to be behind the behaviour of humans and other animals in a wide range of decision-making, cognitive and motor tasks [<xref ref-type="bibr" rid="pcbi.1006903.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref024">24</xref>]. Since these models are flexible, they can automatically characterize the major behavioral trends exhibited by real subjects without requiring tweaking and engineering. This is achieved by training a network to learn how humans learn [<xref ref-type="bibr" rid="pcbi.1006903.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref028">28</xref>], which involves adjusting the weights in a network so that it can predict the choices that subjects make both during learning and at asymptote. At this point the weights are frozen and the model is simulated on the actual learning task to assess its predictive capacity and to gain insights into the subjects’ behavior. This approach is not prone to the problems mentioned earlier because RNNs can in principle be trained to represent any form of behavioural process without requiring manual engineering; however, a potential problem is that the models are so flexible that they may overfit the data and not generalize in a relevant manner; an issue that we address by using regularisation methods and cross-validation.</p>
<p>To illustrate and evaluate this approach, we focus on a relatively simple decision-making task, involving a two-arm bandit, in which subjects chose between two actions (button presses) that were rewarded probabilistically. To examine the predictive capacity of RNNs under typical and atypical conditions, data from three groups were collected: healthy subjects, and patients with either unipolar or bipolar depression. We found that RNNs were able to learn the subjects’ decision-making strategies more accurately than both baseline reinforcement-learning and logistic regression models. Furthermore, we show that off-policy simulations of the RNN model allowed us to visualize, and thus uncover, the properties of the learning process behind subjects’ actions and that these were inconsistent with the assumptions made by reinforcement-learning treatments. Furthermore, we illustrate how the RNN method can be applied to predict diagnostic categories for different patient populations.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Model and task settings</title>
<sec id="sec004">
<title>RNN</title>
<p>The architecture we used is depicted in <xref ref-type="fig" rid="pcbi.1006903.g001">Fig 1</xref>; it is a particular form of recurrent neural network. The model is composed of an <sc>lstm</sc> layer [Long short-term memory; <xref ref-type="bibr" rid="pcbi.1006903.ref029">29</xref>], which is a recurrent neural network, and an output softmax layer with two nodes (since there are two actions in the task). The inputs to the model on each trial are the previous action and the reward received after taking the action, and the outputs of the model are the probabilities of selecting each action on the next trial. We refer to the framework proposed here as <sc>rnn</sc>.</p>
<fig id="pcbi.1006903.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Structure of the <sc>rnn</sc> model.</title>
<p>The model has an <sc>lstm</sc> layer (shown by red dashed line) which receives the previous action and reward as inputs, and is connected to a softmax layer (shown by a black rectangle) which outputs the probability of selecting each action on the next trial (policy). The <sc>lstm</sc> layer is composed of a set of <sc>lstm</sc> cells (<italic>N</italic><sub><italic>c</italic></sub> cells shown by blue circles), that are connected to each other (shown by green arrows). The outpt of the cells (denoted by <inline-formula id="pcbi.1006903.e001"><alternatives><graphic id="pcbi.1006903.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> for cell <italic>i</italic> at time <italic>t</italic>) are connected to a softmax layer using a set of connections shown by black lines. The free parameters of the model (in both <sc>lstm</sc> and softmax layers) are denoted by Θ, and <inline-formula id="pcbi.1006903.e002"><alternatives><graphic id="pcbi.1006903.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is a metric which represents how well the model fits subjects’ data and is used to adjust the parameters of the model using the maximum-likelihood estimate as the network learns how humans learn.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g001" xlink:type="simple"/>
</fig>
<p>The <sc>lstm</sc> layer is composed of a set of interconnected <sc>lstm</sc> cells, in which each cell can be thought of as a memory unit which maintains and updates a scalar value over time (shown by <inline-formula id="pcbi.1006903.e003"><alternatives><graphic id="pcbi.1006903.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> in <xref ref-type="fig" rid="pcbi.1006903.g001">Fig 1</xref> for the i<italic>th</italic> <sc>lstm</sc> cell at time <italic>t</italic>). On each trial, the value of each cell is updated based on the inputs and on the last value of the other <sc>lstm</sc> cells in the network (including the cell itself), and in this way the <sc>lstm</sc> layer can track relevant information regarding the history of past rewards and actions. Each <sc>lstm</sc> cell outputs its current value (<inline-formula id="pcbi.1006903.e004"><alternatives><graphic id="pcbi.1006903.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>) to the softmax layer through an additional set of connections that determine the influence of the output of each cell on the predictions for the next action (shown by lines connecting them in <xref ref-type="fig" rid="pcbi.1006903.g001">Fig 1</xref>). As a whole, such an architecture is able to <italic>learn</italic> in a decision-making task by tracking the history of past experiences using the <sc>lstm</sc> layer, and then turning this information into subsequent actions through the outputs of the softmax layer.</p>
<p>The way in which a network learns in the task and maps past experiences to future actions is modulated by weights in the network. Here, our aim was to tune the weights so that the network could predict the next action taken by the subjects—given that the inputs to the network were the same as those that the subjects received on the task. This is learning how humans learn, in which the weights are trained to optimise a metric (denoted by <inline-formula id="pcbi.1006903.e005"><alternatives><graphic id="pcbi.1006903.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="fig" rid="pcbi.1006903.g001">Fig 1</xref>) which represents how well the model predicts subjects’ choices. In order to prevent the model from overfitting the data, we used early stopping [a commonly used regularisation method in deep learning; <xref ref-type="bibr" rid="pcbi.1006903.ref030">30</xref>] and used cross-validation to assess the generalization abilities of the model to predict unseen data.</p>
</sec>
<sec id="sec005">
<title>Baseline models</title>
<p>We compared the predictive accuracy of the <sc>rnn</sc> model with classical exemplars from the reinforcement learning (RL) family as well as a logistic regression model. The first baseline RL model was the <italic>Q</italic>-learning model (denoted by <sc>ql</sc>), in which subjects’ choices are determined by learned action values [often called <italic>Q</italic> values; <xref ref-type="bibr" rid="pcbi.1006903.ref031">31</xref>], which are updated based on the experience of reward [as used for example in <xref ref-type="bibr" rid="pcbi.1006903.ref032">32</xref>]. The second baseline model was <italic>Q</italic>-learning with perseveration (denoted by <sc>qlp</sc>), which is similar to <sc>ql</sc> but has an extra parameter that allows for a tendency to stick with the same action for multiple trials (i.e., to perseverate), or sometimes to alternate between the actions [independently of reward effects; <xref ref-type="bibr" rid="pcbi.1006903.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref033">33</xref>]. As we show below, the accounts of subject choices provided by both <sc>ql</sc> and <sc>qlp</sc> were significantly worse than <sc>rnn</sc>, and so we developed a new baseline model that we called generalised <italic>Q</italic>-learning (denoted by <sc>gql</sc>). This model extends <sc>ql</sc> and <sc>qlp</sc> models by learning multiple values for each action using different learning rates, and also by tracking the history of past actions at different time scales. The final baseline model we used was a logistic regression model (denoted by <sc>lin</sc>) in which the probability of taking each action was determined by a linear combination of previous rewards, actions and their interactions [<xref ref-type="bibr" rid="pcbi.1006903.ref033">33</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref034">34</xref>]. See section Computational models in <xref ref-type="sec" rid="sec014">Materials and methods</xref> for more details.</p>
</sec>
<sec id="sec006">
<title>Task and subjects</title>
<p>The instrumental learning task (<xref ref-type="fig" rid="pcbi.1006903.g002">Fig 2</xref>) involved participants choosing between pressing a left (L action) or right (R action) button (self-paced responses) in order to earn food rewards (an M&amp;M chocolate or a BBQ flavoured cracker). The task was divided into 12 different blocks each lasting for 40 seconds and separated by a 12-second inter-block interval. Within each block one of the actions was better than the other in terms of the probability of earning a reward but across blocks the action with the higher reward probability was varied, i.e., in some of the blocks the left action was better while in others the right action was better. The reward probability for the better action was 0.25, 0.125, or 0.08 and the probability of earning reward from the other action was always 0.05 (probabilities were fixed within each block); as such, there were six pairs of reward probabilities and each was repeated twice. 34 uni-polar depression (<sc>depression</sc>), 33 bipolar (<sc>bipolar</sc>) and 34 control (<sc>healthy</sc>) participants (age, gender, IQ and education matched) completed the task. See <xref ref-type="sec" rid="sec014">Materials and methods</xref> for the details.</p>
<fig id="pcbi.1006903.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Structure of the decision-making task.</title>
<p>Subjects had a choice between a left keypress (L) and a right keypress (R), shown by yellow rectangles. Before the choice, no indication was given as to which button was more likely to lead to reward. When the participant made a rewarded choice, the button chosen was highlighted (green) and a picture of the earned reward was presented for 500ms (M&amp;M chocloate in this case). The task was divided into 12 different blocks each lasting for 40 seconds and separated by a 12-second inter-block interval. Within each block actions were self-paced and participants were free to complete as many trials as they could within the 40 second time limit. The probability of earning a reward from each action was varied between the blocks. See the text for more details about the probabilities of earning rewards from actions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g002" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec007">
<title>Performance in the task</title>
<p><xref ref-type="fig" rid="pcbi.1006903.g003">Fig 3</xref> shows the probability of selecting the best action (i.e., the action with the higher reward probability). Results are shown by subject (i.e., <sc>subj</sc>) in the graph. The probability of selecting the better action was significantly higher than the other action in all groups (<sc>healthy</sc> [<italic>η</italic> = 0.270, SE = 0.026, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.149, SE = 0.028, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.119, SE = 0.021, <italic>p</italic> &lt; 0.001]). Comparing <sc>healthy</sc> and <sc>depression</sc> groups revealed that the group x action interaction had a significant effect on the probability of selecting actions [<italic>η</italic> = −0.120, SE = 0.038, <italic>p</italic> = 0.002]. A similar effect was observed when comparing the <sc>healthy</sc> and <sc>bipolar</sc> groups [<italic>η</italic> = −0.150, SE = 0.034, <italic>p</italic> &lt; 0.001]. In summary, these results indicate that all groups were able to direct their actions toward the better choice, however the <sc>depression</sc> and <sc>bipolar</sc> groups were less able to do so compared to the <sc>healthy</sc> group.</p>
<fig id="pcbi.1006903.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Probability of selecting the action with the higher reward probability (averaged over subjects).</title>
<p><sc>subj</sc> refers to the data of the experimental subjects, whereas the remaining columns show simulations of the models trained on the task (on-policy simulations) with the same reward probabilities and for the same number of trials that each subject completed. Each dot represents a subject and error-bars represent 1 SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g003" xlink:type="simple"/>
</fig>
<p>Next, we trained three instances of a <sc>rnn</sc> using the data from each group and then froze the weights of the models and simulated them on-policy in the task (with the same reward probabilities and for the same number of trials that each subject completed). On-policy means that the models completed the task on their own by selecting the actions that they predicted a representative subject would take in each situation. The results of the simulations are shown in <xref ref-type="fig" rid="pcbi.1006903.g003">Fig 3</xref> in the <sc>rnn</sc> column. Similar to the subjects’ data, the probability of selecting the better action was significantly higher than the other action in all the three groups (<sc>healthy</sc> [<italic>η</italic> = 0.192, SE = 0.011, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.058, SE = 0.014, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.074, SE = 0.011, <italic>p</italic> &lt; 0.001]). Therefore, although the structure of <sc>rnn</sc> was initially unaware that the objective of the task was to collect rewards, its actions were directed toward the better key by following the strategy that it learned from the subjects’ actions.</p>
<p>We also trained three instances of each baseline model using the data from each group, and simulated these on the task. <xref ref-type="fig" rid="pcbi.1006903.g003">Fig 3</xref> shows the results of the simulations. As the graph shows, the <sc>lin</sc> model was also able to direct its choices toward the best action by learning the effect of past choices and rewards on the next actions. A similar pattern was observed for <sc>gql</sc>, <sc>qlp</sc> and <sc>ql</sc> models in the figure, which is not surprising as the structure of these models includes value representations which can be used for reward maximization. Thus, all of the models were consistent with the subjects’ performance in terms of being able to find and select the best actions in the task. Further details about the models’ parameters and training can be found in the Supporting information (estimated parameters for <sc>ql</sc>, <sc>qlp</sc> and <sc>gql</sc> models are shown in <xref ref-type="supplementary-material" rid="pcbi.1006903.s023">S3</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s024">S4</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006903.s025">S5</xref> Tables respectively; the negative log-likelihood for each model is reported in <xref ref-type="supplementary-material" rid="pcbi.1006903.s026">S6 Table</xref>. See <xref ref-type="supplementary-material" rid="pcbi.1006903.s028">S8 Table</xref> for the effect of initialisation of the network on the negative log-likelihood of the trained <sc>rnn</sc>. See <xref ref-type="supplementary-material" rid="pcbi.1006903.s027">S7 Table</xref> for the negative log-likelihood when a separate model was fitted to each subject in the case of baseline models. See <xref ref-type="supplementary-material" rid="pcbi.1006903.s003">S3 Text</xref> for the analysis of randomness of choices).</p>
</sec>
<sec id="sec008">
<title>The immediate effect of reward on choice</title>
<p>The analyses in the previous section suggested that <sc>rnn</sc> was able to guide its actions toward the better choices, consistent with subjects’ behavior. However, there are multiple strategies that the models could follow to achieve this, and here we aimed to establish whether the strategy used by the models was similar to that used by the subjects’. We started by investigating the immediate effect of reward on choice. <xref ref-type="fig" rid="pcbi.1006903.g004">Fig 4</xref> shows the effect of earning a reward on the previous trial on the probability of staying on the same action in the next trial. For the subjects (<sc>subj</sc>), earning a reward significantly <italic>decreased</italic> the probability of staying on the same action in the <sc>healthy</sc> and <sc>depression</sc> groups, but not in the <sc>bipolar</sc> group (<sc>healthy</sc> [<italic>η</italic> = 0.112, SE = 0.019, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.111, SE = 0.029, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.030, SE = 0.035, <italic>p</italic> = 0.391]). As the figure shows, the same pattern was observed in <sc>rnn</sc> (<sc>healthy</sc> [<italic>η</italic> = 0.082, SE = 0.006, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.089, SE = 0.013, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.001, SE = 0.010, <italic>p</italic> = 0.887]), which shows that the strategy used by <sc>rnn</sc> was similar to the subjects’ according to this analysis.</p>
<fig id="pcbi.1006903.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Probability of staying on the same action based on whether the previous trial was rewarded (reward) or not rewarded (no reward), averaged over subjects.</title>
<p><sc>subj</sc> shows the data from subjects and results of columns are derived from on-policy simulations of various models on the task. Each dot represents a subject and error-bars represent 1SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g004" xlink:type="simple"/>
</fig>
<p>In contrast, stay probabilities were in the opposite directions in <sc>ql</sc> and <sc>qlp</sc>, i.e., the probability of staying on the same action was <italic>higher</italic> after earning reward (for the case of <sc>qlp</sc>; <sc>healthy</sc> [<italic>η</italic> = −0.028, SE = 0.004, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = −0.039, SE = 0.006, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = −0.054, SE = 0.007, <italic>p</italic> &lt; 0.001]), which differs from the subjects’ data. This pattern was expected from baseline reinforcement-learning models, i.e., <sc>ql</sc> and <sc>qlp</sc>, because, in these models, earning reward increases the value of the taken action, which raises the probability of choosing that action on the next trial. Indeed, this learning process is embedded in the parametric forms of <sc>ql</sc> and <sc>qlp</sc> models, and cannot be reversed no matter what values are assigned to the free-parameters of these models. Therefore, although <sc>qlp</sc> and <sc>ql</sc> were able to find the best action in the task, analyzing the immediate effect of reward showed that their learning processes differed from those used by the subjects’.</p>
<p>To address this limitation of <sc>ql</sc> and <sc>qlp</sc>, we designed <sc>gql</sc> as a baseline model with more relaxed assumptions, in which action values could have an opposite effect on the probability of selecting actions, and so could generate a similar response pattern to the subjects’, as shown in the figure. The <sc>lin</sc> model was also able to produce a pattern similar to the empirical data. This is because in this model actions are determined by previous rewards (and actions) without making assumptions about the direction of the effects. Therefore, <sc>gql</sc> and <sc>lin</sc> appear to be able to model subjects’s choices, at least in terms of the behavioral summaries presented here and in the previous section. Despite this, it remains an open question whether these models can represent all of the behavioural trends in the data, or whether there are some missing trends that were undetected in the summary statistics. In the next section we will answer this question by comparing the prediction capacity of <sc>gql</sc> and <sc>lin</sc> with <sc>rnn</sc>, as a model that has the capacity to capture all of the behavioural trends in the data.</p>
</sec>
<sec id="sec009">
<title>Action prediction</title>
<p>Here our aim was to quantify how well the models predicted the actions chosen by the subjects. We used leave-one-out cross-validation for this purpose in which, at each round, one of the subjects was withheld and the model was trained using the remaining subjects; the trained model was then used to make predictions about the withheld subject. The withheld subject was rotated in each group, yielding 34, 34 and 33 prediction accuracy measures in the <sc>healthy</sc>, <sc>depression</sc>, and <sc>bipolar</sc> groups, respectively.</p>
<p>The results are reported in <xref ref-type="fig" rid="pcbi.1006903.g005">Fig 5</xref>. The left-panel of the figure shows prediction accuracy in terms of <sc>nlp</sc> (negative log-probability; averaged over leave one-out cross-validation folds; lower values are better) and the right-panel shows the percentage of actions predicted correctly (‘%correct’; higher values are better). <sc>nlp</sc> roughly represents how well each model fits the choices of the withheld subject and so, unlike ‘%correct’, takes the certainty of predictions into account. Therefore, we focus on <sc>nlp</sc> in this analysis. Firstly, <sc>lin</sc> had the best <sc>nlp</sc> among the baseline models in all the three groups, although it was not statistically better than <sc>gql</sc>. The <sc>gql</sc> model was the second best among the baseline models and its advantage over <sc>qlp</sc> was statistically significant in the <sc>depression</sc> and <sc>bipolar</sc> groups (<sc>healthy</sc> [<italic>η</italic> = −0.036, SE = 0.020, <italic>p</italic> = 0.086], <sc>depression</sc> [<italic>η</italic> = −0.101, SE = 0.024, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = −0.105, SE = 0.019, <italic>p</italic> &lt; 0.001]). Secondly, <sc>rnn</sc>’s <sc>nlp</sc> was even better than the best baseline model (<sc>lin</sc>) across all groups (<sc>healthy</sc> [<italic>η</italic> = 0.069, SE = 0.017, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.107, SE = 0.012, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.128, SE = 0.012, <italic>p</italic> &lt; 0.001]), showing that <sc>rnn</sc> was able to predict subjects’ choices better than the other models.</p>
<fig id="pcbi.1006903.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Cross-validation results.</title>
<p><bold>(Left-panel)</bold> <sc>nlp</sc> (negative log-probability) averaged across leave-one-out cross-validation folds. Lower values are better. <bold>(Right-panel)</bold> Percentage of actions predicted correctly averaged over cross-validation folds. Error-bars represent 1SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g005" xlink:type="simple"/>
</fig>
<p>The fact that <sc>lin</sc> and <sc>gql</sc> were better than <sc>ql</sc> and <sc>qlp</sc> is not unexpected; we showed in the previous section that the predictions from <sc>ql</sc> and <sc>qlp</sc> were inconsistent with the trial-by-trial behaviour of the subjects. On the other hand, the fact that <sc>rnn</sc> is better than <sc>lin</sc> and <sc>gql</sc> shows that there are some behavioural trends that even <sc>lin</sc> and <sc>gql</sc> failed to capture, although they were consistent with subjects’ choices according to the behavioural summary statistics. In the next sections, we use off-policy simulations of the models to uncover the additional behavioural trends that were captured by <sc>rnn</sc>.</p>
</sec>
<sec id="sec010">
<title>Off-policy simulations</title>
<p>In an off-policy simulation, a model uses information about previous choices and rewards to make predictions about the next action. However, the actual next action used to simulate the model is not derived from these predictions and is derived in some other manner; notably, from human choices. In this way we can control the inputs the model receives and examine how they affect predictions. We were interested, in particular, in establishing how the predictions of the models were affected by the history of previous actions and rewards. As such, we designed a variety of inputs based on the behavioural statistics, fed them into the models, and recorded the predictions of each model in response to each input set (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s002">S2 Text</xref> for more details on how simulation parameters were chosen). Simulations of the models are shown in each row of <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref> for the <sc>healthy</sc> group, in which each panel shows a separate simulation across 30 trials (horizontal axis). For trials 1-10, the action that was fed to the model was R (right action), and for trials 11-30 it was L, i.e., left action (the action fed into the model at each trial is shown in the ribbons below each panel). The rewards associated with these trials varied among simulations (the columns) and are shown by black crosses (x) in the graphs.</p>
<fig id="pcbi.1006903.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Off-policy simulations of all models for group <sc>healthy</sc>.</title>
<p>Each panel shows a simulation of 30 trials (horizontal axis), and the vertical axis shows the predictions of each model on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. Red arrows point to the same trial number in all the simulations and are shown to compare changes in the predictions in that trial between different simulations. The sequence of rewards and actions fed to the model are the same for the panels in each column, but they are different across the columns. See text for the interpretation of the graph.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g006" xlink:type="simple"/>
</fig>
<sec id="sec011">
<title>The effect of reward on choice</title>
<p>Focusing on the <sc>rnn</sc> simulations in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, it can be observed that earning a reward (shown by black crosses) caused a ‘dip’ in the probability of staying with an action, which showed a tendency to switch to the other action. This is consistent with the observation made in <xref ref-type="fig" rid="pcbi.1006903.g004">Fig 4</xref> that the probability of switching increases after reward. We saw a similar pattern in <sc>gql</sc>, in which the contribution of action values to choices can be negative, i.e., higher values can lead to a lower probability of staying with an action (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s001">S1 Text</xref> for more explanation). Similarly, in the <sc>lin</sc> model the effect of reward on the probability of the next action can be negative, which allowed this model to produce the observed pattern. The pattern, however, is reversed in the <sc>ql</sc> and <sc>qlp</sc> models, i.e., the probability of choosing an action increased after a reward due to an increase in action value, which is again consistent with the observations in <xref ref-type="fig" rid="pcbi.1006903.g004">Fig 4</xref>. The effects are rather small in these two models (and may not be clear for the <sc>qlp</sc> model), which is likely because the effect of reward needs to be non-zero in order to enable the model to direct choices toward the best action in the long run. At the same time the effect is in the wrong direction compared to the actual data and therefore it needs to be kept small to minimize the discrepancy of predictions with the actual actions after earning rewards.</p>
<p>The next observation was the effect of the previous reward on the probability of switching after a reward. First we focused on the <sc>rnn</sc> model and on the trials shown by red arrows in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>. The red arrows point to the same trial number, but the number of rewards earned prior to the trial differed. As the figure shows, the probability of switching after reward was lower in the right-panel compared to the left and middle panels. The only difference between simulations is that, in the right panel, two more rewards were earned before the red arrow. Therefore, the figure shows that although the probability of switching was higher after reward, it got smaller as the number of rewards previously earned by an action increased. Indeed, this strategy made subjects switch more often from the inferior action, because rewards were sparse on that action, and switch less from the best action, because it was more frequently rewarded. This reconciles the observations made in Figs <xref ref-type="fig" rid="pcbi.1006903.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006903.g004">4</xref> that, although more responses were made on the better action, the probability of switching after reward was higher. <xref ref-type="fig" rid="pcbi.1006903.g007">Fig 7</xref> shows the same simulations using <sc>rnn</sc> for all groups. Comparing the predictions at the red arrows for the <sc>depression</sc> and <sc>bipolar</sc> groups, we observed a pattern similar to the <sc>healthy</sc> group, although the differences were smaller in the <sc>bipolar</sc> group (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s014">S11 Fig</xref> for the effect of the initialisation of the model).</p>
<fig id="pcbi.1006903.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Off-policy simulations of <sc>rnn</sc> for all groups.</title>
<p>Each panel shows a simulation of 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs, and the red arrows point to the same trial number in all the panels. See text for the interpretation of the graph. Note that the simulation conditions are the same as those shown in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, and the first row here (<sc>healthy</sc> group) is the same as the first row shown in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref> which is shown again for comparison with the other groups.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g007" xlink:type="simple"/>
</fig>
<p>The above observations are consistent with the pattern of choices in the empirical data shown in <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>-left panel, which depicts the probability of staying with an action after earning reward as a function of how many rewards were earned after switching to the action (a similar graph using on-policy simulation of <sc>rnn</sc> is shown in <xref ref-type="supplementary-material" rid="pcbi.1006903.s016">S13 Fig</xref>). In all three groups, the probability of staying with an action (after earning a reward) was significantly higher when more than two rewards were earned previously (&gt;2) compared to when no reward was earned (<sc>healthy</sc> [<italic>η</italic> = 0.148, SE = 0.037, <italic>p</italic> &lt; 0.001], <sc>depression</sc> [<italic>η</italic> = 0.188, SE = 0.045, <italic>p</italic> &lt; 0.001], <sc>bipolar</sc> [<italic>η</italic> = 0.150, SE = 0.056, <italic>p</italic> = 0.012]), which is consistent with the behaviour of the <sc>rnn</sc>.</p>
<fig id="pcbi.1006903.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g008</object-id>
<label>Fig 8</label>
<caption>
<title>The effect of the history of previous rewards and actions on the future choices of the subjects.</title>
<p><bold>(Left-panel)</bold> The probability of staying with an action after earning reward as a function of the number of rewards earned since switching to the current action (averaged over subjects). Each red dot represents the data for one subject. <bold>(Right-panel)</bold> The probability of staying with an action as a function of the number of actions taken since switching to the current action. The red line was obtained using Loess regression (Local Regression), which is a non-parametric regression approach. The grey area around the red line represents the 95% confidence interval. Error-bars represent 1SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g008" xlink:type="simple"/>
</fig>
<p>As shown in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, the <sc>gql</sc> model produced a pattern similar to <sc>rnn</sc> largely because this model tracks multiple values for each action, which allows this model to produce the ‘dip’s after reward, with a magnitude that is sensitive to the number of past rewards (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s001">S1 Text</xref> for details). As such, it is not surprising that <sc>gql</sc> was consistent with the subjects’ choices with respect to the effects of immediate reward. Similarly, the <sc>lin</sc> model was able to produce this pattern, which is again expected as in this model the predicted probabilities are based on rewards earned several trials back (up to 18 trials back), which allows this model to learn about the effect of distant rewards on current actions. In this case, the rewards earned proximal to the action will have a negative effect on selecting the same action—generating the observed ‘dip’ in the probabilities—and distant rewards will have a positive effect on the probabilities of staying on the same action, making the size of the ‘dip’ sensitive to the number of rewards earned in the past (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s009">S6</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s010">S7</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s011">S8</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s012">S9</xref> Figs for the corresponding results for all of the groups based on the <sc>lin</sc>, <sc>gql</sc>, <sc>qlp</sc> and <sc>ql</sc> models respectively).</p>
<p><italic>The effect of repeating an action on choices</italic>. Next, we looked at the effect of the history of actions on choices. Focusing on the <sc>rnn</sc> model in <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, we can see that, in the first 10 trials, the predicted probability of taking R was higher than L; this then reversed over the next 20 trials. This implies that perseveration (i.e., sticking with the previously taken action) was an element of action selection. This is consistent with the fact that the <sc>qlp</sc> model (which has a parameter for perseveration) performed better than the <sc>ql</sc> model in the cross-validation statistics (see <xref ref-type="fig" rid="pcbi.1006903.g005">Fig 5</xref>); and, indeed, <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref> shows <sc>ql</sc>’s inability to reflect this characteristic. Note that it can be seen in <xref ref-type="fig" rid="pcbi.1006903.g003">Fig 3</xref> that the probability of staying with an action was above 50% irrespective of whether a reward was earned on the previous trial or not. This does not, however, provide evidence for perseveration because the trials were not statistically independent. For example, in late training trials a subject might have discovered which action returns more reward on average and, therefore, stayed with that action irrespective of reward and so without necessarily relying on perseveration.</p>
<p>Focusing on <sc>rnn</sc> simulations in the left-panel of <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, we observed that, after switching to action L (after trial 10), the probability of staying with that action gradually decreased; i.e., although there was a high chance the next action would be similar to the previous action, subjects developed a tendency to switch the longer they stayed with an action. To compare this pattern with the empirical data, we calculated the probability of staying with an action as a function of how many times the action had been taken since switching (<xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>:right-panel; similar graphs for <sc>rnn</sc>, <sc>lin</sc> and <sc>gql</sc> on-policy simulations are shown in <xref ref-type="supplementary-material" rid="pcbi.1006903.s016">S13</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s017">S14</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006903.s018">S15</xref> Figs respectively). As the figure shows, for the <sc>healthy</sc> group, the chance of staying with an action decreased as the action was repeated [<italic>η</italic> = −0.005, SE = 0.001, <italic>p</italic> = 0.006], which is consistent with the behaviour of <sc>rnn</sc>. With regard to the baseline models, going back to <xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>, we did not see a similar pattern, although in <sc>gql</sc> there was a small decrement in the probability of staying with an action after earning the first reward.</p>
<p><italic>Symmetric oscillations between actions</italic>. Next, we focussed on the <sc>rnn</sc> simulations in <xref ref-type="fig" rid="pcbi.1006903.g007">Fig 7</xref> in <sc>depression</sc> and <sc>bipolar</sc> groups for which the gap between prediction accuracy of baseline models and <sc>rnn</sc> was largest. As can bee seen in the left-panels, after switching to action L (after trial 10), the probability of staying with that action gradually decreased in the <sc>depression</sc> group whereas, for the <sc>bipolar</sc> group, there was a ‘dip’ around 10 trials after the switching to action L (i.e., around trial 20), and then the policy became flat. With reference to the empirical data, as shown in <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>:right-panel, for the <sc>depression</sc> and <sc>bipolar</sc> groups, the probability of staying with an action immediately after switching to that action was around 50%—60% (shown by the bar at <italic>x</italic> = 0 in <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>:right-panel), i.e., there was a 40%–50% chance that the subject immediately switched back to the previous action. Based on this we expected to see a ‘dip’ in the simulations of the <sc>depression</sc> and <sc>bipolar</sc> groups in <xref ref-type="fig" rid="pcbi.1006903.g007">Fig 7</xref> just after the switch to action L. This was not the case, pointing to an inconsistency between model predictions and the empirical data.</p>
<p>However, <xref ref-type="fig" rid="pcbi.1006903.g007">Fig 7</xref> is based on particular, artificial sequences of actions and rewards. To look more closely at the above effect, we defined a <italic>run</italic> of actions as a sequence of presses on a specifed button without switching to the other button. For example, if the executed actions were L, R, R, L, then the length of the first run was 1 (L), the length of the second run was 2 (R, R), and the length of the third run was 1 (L). <xref ref-type="fig" rid="pcbi.1006903.g009">Fig 9</xref> shows the relationship between consecutive run length, i.e., the length of the current run of actions, as a function of the length of the previous run of actions in the empirical data. The graph show the empirical data (shown by <sc>subj</sc>) and on-policy model simulations. The dashed line in the figure indicates the points at which the current run length was the same as the previous run length. Being close to this line implies that subjects were performing symmetrical oscillations between the two actions, i.e., going back and forth between the two actions while performing an equal number of presses on each button. The data for subjects (shown in <sc>subj</sc> column) shows that, in the <sc>bipolar</sc> group, and to some extent in the <sc>depression</sc> group, a short run triggered a subsequent run of similar brevity (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s006">S3</xref>, <xref ref-type="supplementary-material" rid="pcbi.1006903.s007">S4</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1006903.s008">S5</xref> Figs for raw empirical data). This implies that if a subject performed a run of length 1 that would initiate a sequence of oscillations between the two actions, keeping the stay probabilities low during short runs, consistent with what was seen at <italic>x</italic> = 0 in <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>:right-panel. This effect was not seen in the simulations shown in <xref ref-type="fig" rid="pcbi.1006903.g007">Fig 7</xref>, because the length of the previous run before switching to action L was 10 (there were 10 R actions), and therefore we should not expect the next run to be of length 1, nor should we have actually expected to see a ‘dip’ in policy just after the first switch.</p>
<fig id="pcbi.1006903.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The median number of actions executed sequentially before switching to another action (run of actions) as a function of the length of the previous run of actions (averaged over subjects).</title>
<p>The dotted line shows the points at which the length of the previous and the current run of actions were the same. Note that the median was used instead of the average to illustrate the most common ‘current run length’, instead of the average run length for each subject. The results for actual data are shown in <sc>subj</sc> column, and the remaining columns show the results using the on-policy simulations of the models in the task. Error-bars represent 1SEM.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g009" xlink:type="simple"/>
</fig>
<p>As shown in <xref ref-type="supplementary-material" rid="pcbi.1006903.s015">S12 Fig</xref>, the modal length of runs in the <sc>depression</sc>, and <sc>bipolar</sc> groups was 1 (around 17%, 37%, and 45% of runs were of length 1 in the <sc>healthy</sc>, <sc>depression</sc>, and <sc>bipolar</sc> groups respectively). Given this, and the specific pattern of oscillations in the <sc>depression</sc> and <sc>bipolar</sc> groups, our next question was whether, in the models, a run of length 1 triggered oscillations similar to those observed in the empirical data. We used a combination of off-policy and on-policy model simulations to answer this question; i.e., during the off-policy phase we forced the model to make an oscillation between the two actions, and then allowed the model to select between actions. We expected, in the <sc>healthy</sc> group, that the model would converge on one action, whereas, in the <sc>depression</sc> and <sc>bipolar</sc> groups, we expected the initial oscillations to trigger further switches. Simulations are presented in <xref ref-type="fig" rid="pcbi.1006903.g010">Fig 10</xref>, which shows that the sequence of actions fed to the model for the first 9 (off-policy) trials was:
<disp-formula id="pcbi.1006903.e006"><alternatives><graphic id="pcbi.1006903.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>L</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>L</mml:mtext> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
in which there were two oscillations at the tail of the sequence (R, L, R, L,). The rest of the actions (trials 10-20) were selected based on which action the model assigned the highest probability. Note that in on-policy simulations, actions were typically selected probabilistically according to the probabilities that a model assigned to each action. However, in the on-policy simulations presented in this section, in order to get consistent results across simulations actions were <italic>not</italic> selected probabilistically but were chosen based on which action achieved the highest prediction probability. As the simulation shows, at the beginning, the probability the model assigned to action R was high, but after feeding in the oscillations, the model predicted that the future actions would oscillate in the <sc>depression</sc> and <sc>bipolar</sc> groups, but not in the <sc>healthy</sc> group, consistent with what we expected to observe.</p>
<fig id="pcbi.1006903.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Mixed off-policy and on-policy simulations of the models.</title>
<p>Each panel shows a simulation of 20 trials for which the first nine trials were off-policy and the subsequent trials were on-policy, during which the action with the highest probability was selected. Trials marked with green ribbons were off-policy (actions were fed to the model), whereas the trials marked with blue ribbons were on-policy (actions were selected by the model). The ribbon below each panel shows the actions that were fed to the model (for the first 9 trials), and the actions that were selected by the model (on the subsequent trials). During off-policy trials, the sequence of actions that was fed to the model was R, R, R, R, R, R, L, R, L. See the text for interpretation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.g010" xlink:type="simple"/>
</fig>
<p>Therefore, <sc>rnn</sc> was able to produce symmetrical oscillations and its behaviour was consistent with the subjects’ actions. As <xref ref-type="fig" rid="pcbi.1006903.g010">Fig 10</xref> shows, besides <sc>rnn</sc>, the <sc>lin</sc> and <sc>gql</sc> models were also able to produce length 1 oscillations to some extent (as shown for the <sc>bipolar</sc> group), which could explain why the prediction accuracy achieved by these models was significantly better than <sc>qlp</sc> in the <sc>bipolar</sc> and <sc>depression</sc> groups (<xref ref-type="fig" rid="pcbi.1006903.g005">Fig 5</xref>) in which length 1 oscillations were more common (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s002">S2 Text</xref> for more details). However, as shown in <xref ref-type="fig" rid="pcbi.1006903.g009">Fig 9</xref>, <sc>lin</sc> and <sc>gql</sc> both failed to produce oscillations of longer lengths, whereas <sc>rnn</sc> was able to do so (<xref ref-type="fig" rid="pcbi.1006903.g009">Fig 9</xref> <sc>rnn</sc> column). <sc>gql</sc> failed to produce the oscillations even if we increased the capacity of <sc>gql</sc> to track 10 different values for each action (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s019">S16 Fig</xref>). Similarly, as the simulations show, the <sc>lin</sc> model was not able to produce these oscillations; such oscillations will likely require higher order interaction terms and, although these could be added to the LIN model in principal, in practise they will significantly complicate its transparent interpretation. This failing of the <sc>lin</sc> and <sc>gql</sc> models is particularly problematic in the <sc>depression</sc> and <sc>healthy</sc> groups, because these two groups tended to match the length of consecutive runs of actions. This could partly account for why the cross-validation statistics associated with <sc>rnn</sc> were significantly better than <sc>lin</sc> and <sc>gql</sc> for the <sc>depression</sc> and <sc>bipolar</sc> groups.</p>
<p><italic>Summary of off-policy simulations</italic>. Firstly, we found that a <sc>rnn</sc> model was able to capture the immediate effect of rewards on actions (i.e., the ‘dip’ after rewards), as well as the effect of previous rewards on choices. <sc>gql</sc> and <sc>lin</sc> had a similar ability, which enabled them to reproduce the behavioural summary statistics shown in Figs <xref ref-type="fig" rid="pcbi.1006903.g003">3</xref> and <xref ref-type="fig" rid="pcbi.1006903.g004">4</xref>. Baseline reinforcement-learning models (<sc>qlp</sc> and <sc>ql</sc>) failed to capture either trend. Secondly, <sc>rnn</sc> was able to capture how choices change as an action is chosen repeatedly and sequentially, and also the symmetrical oscillations between actions, neither of which could be detected by any of the baseline models.</p>
</sec>
</sec>
<sec id="sec012">
<title>Diagnostic label prediction</title>
<p>In the previous sections we showed that there are several behavioural trends that baseline models failed to capture. Here we asked whether capturing such behavioural trends in this task is necessary to predict the diagnostic labels of the subjects. We used the leave-one-out cross-validation method based on which, in each run, one of the subjects in each group was withheld, and a <sc>rnn</sc> model was fitted to the rest of the group. This model, along with the versions of the same model fitted to all of the subjects in each of the other two groups, was used to predict the diagnostic label for the withheld subject. This prediction was based on which of the three models provided the best fit (lowest <sc>nlp</sc>) for that subject. As an example, assume that we are interested to predict the diagnostic label for a certain subject—say subject #10—in the <sc>depression</sc> group. We first trained a model using the data of all other subjects in the <sc>depression</sc> group, and we also trained two other models using the data of <sc>healthy</sc> and <sc>bipolar</sc> groups. Then, we evaluated which of these three models can better predict the actions of subject #10 in terms of <sc>nlp</sc> to predict the diagnostic label of the subject.</p>
<p>The results are reported in <xref ref-type="table" rid="pcbi.1006903.t001">Table 1</xref>. Baseline random performance was near 33%. As the table shows, the highest performance was achieved for the <sc>healthy</sc> group of which 64% of subjects were classified correctly. A binomial test indicated that the proportion of correctly classified subjects in the <sc>healthy</sc> group was significantly different than the expected proportion of 0.336 based on random classification (<italic>p</italic> &lt; 0.001 two-sided). On the other hand, in the <sc>depression</sc> group a significant portion of subjects were classified as <sc>healthy</sc>. A binomial test did not indicate that the proportion of correctly classified subjects in the <sc>depression</sc> and <sc>bipolar</sc> groups was significantly different than the expected proportion of 0.336 and 0.326, respectively, based on random classification (<italic>p</italic> &gt; 0.1 two-sided).</p>
<table-wrap id="pcbi.1006903.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.t001</object-id>
<label>Table 1</label>
<caption>
<title>Prediction of diagnostic labels using <sc>rnn</sc>.</title>
<p>The number of subjects in each true- and predicted-label. The numbers inside the parentheses are the percentage of subjects relative to the total number of subjects in each diagnostic group.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006903.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2" rowspan="2"/>
<th align="center" colspan="3">predicted labels</th>
</tr>
<tr>
<th align="center"><sc>healthy</sc></th>
<th align="center"><sc>depression</sc></th>
<th align="center"><sc>bipolar</sc></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="3">true labels</td>
<td align="center"><sc>healthy</sc></td>
<td align="center" style="background-color:#C0C0C0">22 (64%)</td>
<td align="center">8 (23%)</td>
<td align="center">4 (11%)</td>
</tr>
<tr>
<td align="center"><sc>depression</sc></td>
<td align="center">13 (38%)</td>
<td align="center" style="background-color:#C0C0C0">16 (47%)</td>
<td align="center">5 (14%)</td>
</tr>
<tr>
<td align="center"><sc>bipolar</sc></td>
<td align="center">9 (27%)</td>
<td align="center">9 (27%)</td>
<td align="center" style="background-color:#C0C0C0">15 (45%)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The overall correct classification rate of the model was 52%, whereas <sc>lin</sc> achieved 46% accuracy (<xref ref-type="supplementary-material" rid="pcbi.1006903.s021">S1 Table</xref>) and <sc>gql</sc> achieved 50% accuracy (<xref ref-type="supplementary-material" rid="pcbi.1006903.s022">S2 Table</xref>). We conclude that although <sc>lin</sc> and <sc>gql</sc> were unable to accurately characterize behavioural trends in the data, the group differences that were captured by <sc>lin</sc> and <sc>gql</sc> appeared sufficient to guide diagnostic label predictions.</p>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>We used a recurrent neural network to provide a framework for learning a computational model that can characterize human learning processes in decision-making tasks. Unlike previous work, the current approach makes minimal assumptions about these learning processes; we showed that this agnosticism is important in developing an appropriate explanation of the data. In particular, the <sc>rnn</sc> model was able to encode the melange of processes that subjects appeared to use to select actions; it was also able to capture differences between the psychiatric groups. These processes were largely inconsistent with conventional and tailored <italic>Q</italic>-learning models, and were also hidden in the overall performance of subjects on the task. This provided a clear example of how the currently proposed framework can outperform previous approaches.</p>
<p>In general, as we were able to show, this new approach improves upon previous methods from four standpoints: First, it provides a model that was able to predict subjects’ choices without requiring manual engineering; and to do so more accurately than baseline models on test data. Second, the framework contributes to computational modelling by providing a baseline for predictive accuracy; i.e., to the extent that other candidate models failed to generate the performance of <sc>rnn</sc> models, important and accessible behavioural trends would have been missed in the model structure. This is particularly important because the natural randomness of human choice in many scenarios makes it unclear whether the model at hand (e.g., a <italic>Q</italic>-learning model) has reached a limit as to how well those choices can be predicted, or whether it requires further improvements. Without other recourse, conventional treatments tend to relegate model mis-fit to irreducible randomness in choice. Third, based on the framework, a trained model can be regarded as representative of a group’s behaviour, which can then be interrogated in control conditions using off-policy simulations to gain insights into the learning processes behind subject’s choices. Finally, the framework can be used to predict the diagnostic labels of the subjects.</p>
<p>It might be possible to design different variants of <italic>Q</italic>-learning models (e.g., based on the analysis presented above) and obtain more competitive prediction accuracy. For example, although it is non-trivial, it is possible to design a new variant of <sc>gql</sc> able to track oscillatory behaviour such as that described here. Our aim was not to rule out this possibility, but rather to show that the framework can automatically extract learning features from subjects’ actions using learning to learn principles without requiring feature engineering in the models. This is even when those features were initially invisible in task performance metrics.</p>
<p>Our approach inherits these benefits from the field of neural networks [<xref ref-type="bibr" rid="pcbi.1006903.ref035">35</xref>], in which feature engineering has been significantly simplified across various domains [see also for example <xref ref-type="bibr" rid="pcbi.1006903.ref036">36</xref>, for recent developments using probabilitic programming]. However, our approach also inherits the black-box nature of these neural networks, i.e., the lack of an interpretable working mechanism. This might not be an issue in some applications, such as the ones mentioned above; however, this needs to be addressed in other applications in which the aim of the study is actually to obtaining an interpretable working mechanism. Nevertheless, we were able to show that running controlled experiments on the model using off-policy simulations can provide significant insights into the processes that mediate subjects’ choices. An alternative method for interpreting the model is using gradients [<xref ref-type="bibr" rid="pcbi.1006903.ref037">37</xref>], which will be considered in future work. Interpreting neural networks is an active area of research in machine learning [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref038">38</xref>], and the approach proposed here will benefit from further developments in this area.</p>
<p>In particular, although we found that off-policy simulations of the model could be used to gain insights into the model’s working mechanism, off-policy simulations need to be designed manually to determine inputs to the model. Here, we designed the initial off-policy simulations based on the specific questions and hypotheses that we were interested in testing and using overall behavioural statistics (<xref ref-type="fig" rid="pcbi.1006903.g006">Fig 6</xref>; <xref ref-type="supplementary-material" rid="pcbi.1006903.s002">S2 Text</xref>). However, an important aspect of the behavioural process, i.e., the tendency of subjects to oscillate between actions, was not visible in those simulations and, because of this, we had to design another set of inputs to investigate these oscillations (<xref ref-type="fig" rid="pcbi.1006903.g010">Fig 10</xref>). This shows that the choice of off-policy simulation can affect the interpretation of the model’s working mechanism. As such, although <sc>rnn</sc> can be trained automatically and without intuition into the behavioural processes behind actions [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref039">39</xref>], designing off-policy simulations is not automated and requires manual hypothesis generation. Automating this process will require a method that generates representative inputs (and network outputs) that are clearly able to discriminate the differences between the psychiatric groups. The existence of adversarial examples in neural networks [<xref ref-type="bibr" rid="pcbi.1006903.ref040">40</xref>] suggests that this will not be as simple as using the networks to search explicitly for those input sequences that are the most discriminative—representativeness is also critical.</p>
<p>Recurrent neural networks have previously been used to study reward-related decision-making [<xref ref-type="bibr" rid="pcbi.1006903.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref014">14</xref>], perceptual decision-making, performance in cognitive tasks, working-memory [<xref ref-type="bibr" rid="pcbi.1006903.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref020">20</xref>], motor patterns, motor reach and timing [<xref ref-type="bibr" rid="pcbi.1006903.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref024">24</xref>]. Typically, in these studies, an <sc>rnn</sc> is itself trained to perform the task. This is different from the current study in which the aim of training was to generate behaviour similar to the subjects’, even if that were to lead to poor performance on the task. One exception is the study of [<xref ref-type="bibr" rid="pcbi.1006903.ref021">21</xref>] in which a network was trained to generate outputs similar to electromyographic (EMG) signals recorded in behaving animals during a motor reach task. Interestingly, that study found that, even though the model was trained based purely on EMG signals, the internal activity of the model resembled the neural responses recorded from the subjects’ motor cortex. Indeed, we have recently used a similar approach to investigate whether brain activity during decision-making is related to network activity [<xref ref-type="bibr" rid="pcbi.1006903.ref037">37</xref>].</p>
<p>The fact that <sc>rnn</sc> performance was better than the baseline models can be attributed to two factors. Firstly, recurrent neural networks can potentially track a long history of previous events (such as rewards and actions) in order to predict the next actions. Evidence shows that humans tend to find patterns in the history of previous events (e.g., repetitions, alterations)—even if the events are generated randomly—and subsequently use those patterns to guide their choices and therefore it is important that a model be able to represent such inter-dependencies between current actions and past events [e.g., sequential effects; <xref ref-type="bibr" rid="pcbi.1006903.ref041">41</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref047">47</xref>]. Secondly, such past influences might have a non-linear effect on current choices, and therefore it is important that the model be able to track higher-order statistics [<xref ref-type="bibr" rid="pcbi.1006903.ref048">48</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref049">49</xref>] and non-linear effects. For example in the current study a linear logistic regression model (<sc>lin</sc>) was unable to reproduce the symmetrical oscillations between the actions whereas <sc>rnn</sc> could, which shows that non-linear dynamics are necessary to explain the current data.</p>
<p>With regard to predicting subjects’ diagnostic labels, it was perhaps not surprising to find that the model was unable to achieve a high level of classification accuracy. This is because there is a high level of heterogeneity in patients with the same diagnostic label. Heterogeneity, which is well understood in the wide variation in treatments and treatment outcomes in disorders like depression [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref050">50</xref>], is likely also to be reflected in the differing learning and choice abilities of the subjects.</p>
<p>Given a set of models, different (approximate) Bayesian methods can be used for comparing different candidate models in order to find the model that has generated the data (or has the highest probability of being the model that has generated the data). This comparison can be achieved for example by calculating model-evidence, Bayes factors, exceedance probabilities [<xref ref-type="bibr" rid="pcbi.1006903.ref051">51</xref>], or using hierarchical Bayesian model comparison [<xref ref-type="bibr" rid="pcbi.1006903.ref052">52</xref>]. In some other settings, however, the aim is not just to compare a set of models, but to develop new models or to improve them in order to achieve a high out-of-sample prediction accuracy. A natural way to assess such prediction accuracy is to use cross-validation, that we used jointly with early stopping [<xref ref-type="bibr" rid="pcbi.1006903.ref030">30</xref>] to prevent the RNN from overfitting the data. Indeed, it has been suggested that, from a Bayesian perspective, the other quantities such as Akaike information criterion [AIC; <xref ref-type="bibr" rid="pcbi.1006903.ref053">53</xref>], Deviance information criterion [DIC; <xref ref-type="bibr" rid="pcbi.1006903.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1006903.ref055">55</xref>], and Watanabe-Akaike information criterion [WAIC; <xref ref-type="bibr" rid="pcbi.1006903.ref056">56</xref>] can be viewed as approximations to different forms of cross-validation [<xref ref-type="bibr" rid="pcbi.1006903.ref057">57</xref>], which was directly calculated in the current study.</p>
<p>In the model fitting procedure used here, a single model was fitted to all of the subjects in each group, despite possible individual differences within a group. This was partly because we were interested in obtaining a single parameter set for making predictions for the subject withheld in the leave-one-out cross-validation experiments. Even if a mixed-effect model was fitted to the data, a summary of group statistics will be required to make predictions about a new subject. In other applications, one might be interested in estimating parameters for each individual (either network weights or the parameters of the reinforcement-learning models); in this respect using a hierarchical model fitting procedure would be a more appropriate approach, something that has been used previously for reinforcement-learning models [e.g., <xref ref-type="bibr" rid="pcbi.1006903.ref004">4</xref>] and would be an interesting future step for <sc>rnn</sc> models.</p>
<p>Along the same lines, due to its rich set of parameters, a single <sc>rnn</sc> model might be able to learn about and detect individual differences (e.g., differences in the learning-rates of subjects) at an early stage of the task, and then use this information to make predictions about performance on later trials. For example, during the training phase (learning how humans learn), the model might learn that subjects have either a very high or a very low learning-rate. Then, when being evaluated in the actual learning task, the model can use observations from subjects’ choices on early trials to determine whether the learning-rate for that specific subject is high or low, and then utilise that information to make more accurate predictions in latter trials. Determining individual-specific traits in early trials of the task is presumably <italic>not</italic> part of the computational process occurring in the subject’s brain during the task, and is occurring in the model merely to make more accurate predictions. To the extent that the network learns such higher order structure, it is appealing, though difficult, to extract information about such heterogeneity from the recurrent state of the <sc>rnn</sc>. Of course, this implies that the (implicit) inferences that the <sc>rnn</sc> makes about the type of subject might be confounded with the (implicit) inferences that the <sc>rnn</sc> makes about the actual choices—thus it is a model that makes predictions about subjects’ choices using mechanisms that may not necessarily be competent computational models of the way that the subjects themselves make those choices.</p>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec015">
<title>Ethics statement</title>
<p>The study was approved by the University of Sydney ethics committee (HREC #12812). Participants gave informed consent prior to participation in the study.</p>
</sec>
<sec id="sec016">
<title>Participants</title>
<p>34 uni-polar depression (<sc>depression</sc>), 33 bipolar (<sc>bipolar</sc>) and 34 control (<sc>healthy</sc>) participants (age, gender, IQ and education matched) were recruited from outpatient mental health clinics at the Brain and Mind Research Institute, Sydney, and the surrounding community. Participants were aged between 16 and 33 years. Exclusion criteria for both clinical and control groups were history of neurological disease (e.g. head trauma, epilepsy), medical illness known to impact cognitive and brain function (e.g. cancer), intellectual and/or developmental disability and insufficient English for neuropsychological assessment. Controls were screened for psychopathology by a research psychologist via clinical interview. Patients were tested under ‘treatment-as-usual’ conditions, and at the time of assessment, 77% of depressed and 85% of bipolar patients were taking medications (see <xref ref-type="table" rid="pcbi.1006903.t002">Table 2</xref> for breakdown of medication use). The study was approved by the University of Sydney ethics committee. Participants gave informed consent prior to participation in the study.</p>
<table-wrap id="pcbi.1006903.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006903.t002</object-id>
<label>Table 2</label>
<caption>
<title>Demographic and clinical characteristics of participants.</title>
<p><bold>Means (SD)</bold>. HDRS: Hamilton Depression Rating Scale; YMRS: Young Mania Rating Scale; SOFAS: Social and Occupational Functioning Scale; a: <sc>depression</sc> greater than <sc>healthy</sc> and <sc>bipolar</sc>, <italic>p</italic> &lt; 0.05. b: <sc>bipolar</sc> greater than <sc>healthy</sc>, <italic>p</italic> &lt; 0.05. c: <sc>healthy</sc> greater than <sc>depression</sc> and <sc>bipolar</sc>, <italic>p</italic> &lt; 0.05.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006903.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="center"><sc>healthy</sc><break/>(n = 34)</th>
<th align="center"><sc>depression</sc><break/>(n = 34)</th>
<th align="center"><sc>bipolar</sc><break/>(n = 33)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" colspan="4" style="background-color:#C0C0C0">Demographics</td>
</tr>
<tr>
<td align="left">Gender (M:F)</td>
<td align="center">15:19</td>
<td align="center">15:19</td>
<td align="center">9:24</td>
</tr>
<tr>
<td align="left">Age in years</td>
<td align="center">23.6 (4.3)</td>
<td align="center">21.6 (2.5)</td>
<td align="center">23.1 (4.4)</td>
</tr>
<tr>
<td align="left">Predicted IQ</td>
<td align="center">107.3 (7.5)</td>
<td align="center">105.5 (7.9)</td>
<td align="center">106.0 (7.4)</td>
</tr>
<tr>
<td align="left">Eduation</td>
<td align="center">14.3 (3.0)</td>
<td align="center">13.3(1.9)</td>
<td align="center">13.3 (2.4)</td>
</tr>
<tr>
<td align="center" colspan="4" style="background-color:#C0C0C0">Symptoms and History</td>
</tr>
<tr>
<td align="left">Age of onset (years)</td>
<td align="center">-</td>
<td align="center">14.4 (3.8)</td>
<td align="center">15.9 (4.7)</td>
</tr>
<tr>
<td align="left">Duration of illness (years)</td>
<td align="center">-</td>
<td align="center">7.7 (4.3)</td>
<td align="center">6.4 (3.3)</td>
</tr>
<tr>
<td align="left">HDRS</td>
<td align="center">1.5(2.0)</td>
<td align="center">14.1 (7.2)<sup>a</sup></td>
<td align="center">8.9 (6.5)</td>
</tr>
<tr>
<td align="left">YMRS</td>
<td align="center">0.1 (0.4)</td>
<td align="center">2.5 (5.4)</td>
<td align="center">4.6 (5.8)<sup>b</sup></td>
</tr>
<tr>
<td align="left">SOFAS</td>
<td align="center">91.0 (3.5)<sup>c</sup></td>
<td align="center">63.8 (9.2)</td>
<td align="center">65.7 (13.7)</td>
</tr>
<tr>
<td align="center" colspan="4" style="background-color:#C0C0C0">Medication</td>
</tr>
<tr>
<td align="left">Medicated</td>
<td align="center">-</td>
<td align="center">77%</td>
<td align="center">85%</td>
</tr>
<tr>
<td align="left">Anti-depressants</td>
<td align="center">-</td>
<td align="center">71%</td>
<td align="center">41%</td>
</tr>
<tr>
<td align="left">Mood stabilizers/Anti-convulsants</td>
<td align="center">-</td>
<td align="center">9%</td>
<td align="center">73%</td>
</tr>
<tr>
<td align="left">Lithium</td>
<td align="center">-</td>
<td align="center">0%</td>
<td align="center">18%</td>
</tr>
<tr>
<td align="left">Anti-psychotics</td>
<td align="center">-</td>
<td align="center">18%</td>
<td align="center">33%</td>
</tr>
<tr>
<td align="left">Anxiolytics</td>
<td align="center">-</td>
<td align="center">0%</td>
<td align="center">3%</td>
</tr>
<tr>
<td align="center" colspan="4" style="background-color:#C0C0C0">Motivation measures</td>
</tr>
<tr>
<td align="left">Hunger</td>
<td align="center">6.5 (1.7)</td>
<td align="center">6.0 (2.1)</td>
<td align="center">6.0 (2.4)</td>
</tr>
<tr>
<td align="left">Reward Pleasantness</td>
<td align="center">3.1 (1.3)</td>
<td align="center">2.0 (2.0)</td>
<td align="center">2.6 (2.0)</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001">
<p>Duration of illness indicates time since patient first experienced mental health problems, not time since diagnosis.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Demographics and clinical characteristics of the sample are presented in <xref ref-type="table" rid="pcbi.1006903.t002">Table 2</xref>. Levene’s test indicated unequal variances for the HDRS [Hamilton Depression Rating Scale; <xref ref-type="bibr" rid="pcbi.1006903.ref058">58</xref>], YMRS [Young Mania Rating Scale; <xref ref-type="bibr" rid="pcbi.1006903.ref059">59</xref>], SOFAS [Social and Occupational Functional Scale; <xref ref-type="bibr" rid="pcbi.1006903.ref060">60</xref>] and age, thus Welch’s statistic was used for these variables. A one-way ANOVA revealed no differences between groups in age [<italic>F</italic>(2, 98) = 2.48, <italic>p</italic> = 0.09], education [<italic>F</italic>(2, 98) = 1.76, <italic>p</italic> = 0.18], IQ [<italic>F</italic>(2, 94) = 0.47, <italic>p</italic> = 0.62] or gender (<italic>χ</italic><sup>2</sup> = 2.66, <italic>p</italic> = 0.27). There were differences in HDRS [<italic>F</italic>(2, 49.21) = 64.21, <italic>p</italic> &lt; 0.001], YMRS [<italic>F</italic>(2, 43.71) = 12.57, <italic>p</italic> &lt; 0.001], and SOFAS [<italic>F</italic>(2, 41.61) = 169.66, <italic>p</italic> &lt; 0.001]. Bonferroni post-hoc comparisons revealed higher depression scores in <sc>depression</sc> group compared to <sc>bipolar</sc> and <sc>healthy</sc> groups, and higher depression in <sc>bipolar</sc> group compared to <sc>healthy</sc> group. Mania scores were significantly higher in the <sc>bipolar</sc> group compared to the <sc>healthy</sc> group. Both patient groups had significantly lower SOFAS scores compared to the <sc>healthy</sc> group, but did not differ from one another. Age of mental illness onset was younger in the <sc>depression</sc> group compared to the <sc>bipolar</sc> group [<italic>t</italic>(56) = −2.14, <italic>p</italic> = 0.04], however duration of illness did not differ significantly between groups [<italic>t</italic>(56) = 1.25, <italic>p</italic> = 0.22]. There were no differences between groups in pre-test hunger [<italic>F</italic>(2, 79) = 0.54, <italic>p</italic> = 0.59] or average snack rating [<italic>F</italic>(2, 79) = 2.53, <italic>p</italic> = 0.09].</p>
</sec>
<sec id="sec017">
<title>Task</title>
<p>The instrumental learning task (<xref ref-type="fig" rid="pcbi.1006903.g002">Fig 2</xref>) involved participants choosing between pressing a left or right button in order to earn food rewards (an M&amp;M chocolate or a BBQ flavoured cracker). We refer to these two key presses as L and R for left and right button presses respectively. Fourteen <sc>healthy</sc> participants (41.2% of the group) and 13 <sc>bipolar</sc> participants (36.7% of the group) completed the task in an fMRI setting, using a 2 button Lumina response box. The remaining <sc>healthy</sc> and <sc>bipolar</sc> participants, and all <sc>depression</sc> participants, completed the task on a computer with a keyboard, where the “Z” and “?” keys were designated L and R. Although the performance of subjects was higher overall in the fMRI setting [<italic>η</italic> = 0.050, SE = 0.024, <italic>p</italic> = 0.041], the place in which the task was completed had no significant effect on how choices adjusted on a trial-by-trial basis, either on the probability of staying with the same action after earning a reward [<italic>η</italic> = 0.041, SE = 0.054, <italic>p</italic> = 0.45], or after no reward [<italic>η</italic> = 0.030, SE = 0.062, <italic>p</italic> = 0.627]), and, therefore, the data were combined.</p>
<p>During each block, one action was always associated with a higher probability of reward than the other. The best action was varied across blocks, and the probabilities varied between 0.25, 0.125, and 0.08. The probability of reward on the other action always remained at 0.05. Therefore, there were six pairs of reward probabilities and each was repeated twice. Participants were instructed to earn as many points as possible, as they would be given the concomitant number of M&amp;Ms or BBQ flavoured crackers at the end of the session. After a non-rewarded response, a grey circle appeared in the centre of the screen for 250ms, whereas after a rewarded response the key turned green and an image of the food reward earned appeared in the centre of the screen for 500ms. A tally of accumulated winnings remained on the bottom of the screen for the duration of the task. The task began with a 0.25 contingency practice block and a pleasantness rating for each food outcome (-5 to +5). Responding was self-paced during the 12 blocks of training, each 40-s in length. On average participants completed 109.45, 114.91, 102.79 trials per block in <sc>healthy</sc>, <sc>depression</sc>, and <sc>bipolar</sc> groups respectively (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s029">S9 Table</xref> for the average number of trials completed in each reward probability condition in each group). During inter-block intervals (12 seconds) the participants rated how causal each button was in earning rewards. These self-reports (causal ratings) are not used in the modelling analysis presented here.</p>
</sec>
<sec id="sec018">
<title>Computational models</title>
<sec id="sec019">
<title>Notation</title>
<p>The set of available actions is denoted by <inline-formula id="pcbi.1006903.e007"><alternatives><graphic id="pcbi.1006903.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mi mathvariant="script">A</mml:mi></mml:math></alternatives></inline-formula> and the total number of available actions is denoted by <italic>N</italic><sub><italic>a</italic></sub>. Here <inline-formula id="pcbi.1006903.e008"><alternatives><graphic id="pcbi.1006903.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi mathvariant="script">A</mml:mi> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtext>L</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, with L and R referring to left and right key presses respectively (<italic>N</italic><sub><italic>a</italic></sub> = 2). A set of subjects is denoted by <inline-formula id="pcbi.1006903.e009"><alternatives><graphic id="pcbi.1006903.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>, and the total number of trials completed by subject <inline-formula id="pcbi.1006903.e010"><alternatives><graphic id="pcbi.1006903.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> over the whole task (all blocks) is denoted by <inline-formula id="pcbi.1006903.e011"><alternatives><graphic id="pcbi.1006903.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:math></alternatives></inline-formula>. <inline-formula id="pcbi.1006903.e012"><alternatives><graphic id="pcbi.1006903.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> denotes the action taken by subject <italic>s</italic> at trial <italic>t</italic>. The reward earned at trial <italic>t</italic> is denoted by <italic>r</italic><sub><italic>t</italic></sub>, and we use <italic>a</italic><sub><italic>t</italic></sub> to refer to an action taken at time <italic>t</italic>, either by the subjects or the models (in simulations).</p>
</sec>
<sec id="sec020">
<title>Recurrent neural network model (<sc>rnn</sc>)</title>
<p><bold>Architecture</bold>. The architecture used is based on a recurrent neural network model (<sc>rnn</sc>) and is depicted in <xref ref-type="fig" rid="pcbi.1006903.g001">Fig 1</xref>. The model is composed of an <sc>lstm</sc> layer [Long short-term memory; <xref ref-type="bibr" rid="pcbi.1006903.ref029">29</xref>] and an output softmax layer with two nodes (since there are two actions in the task). The inputs to the <sc>lstm</sc> layer are the previous action (<italic>a</italic><sub><italic>t</italic>−1</sub> coded using one-hot transformation) and the reward received after taking action (<italic>r</italic><sub><italic>t</italic>−1</sub> ∈ {0, 1}). The outputs of the softmax are probabilities of selecting each action, which are denoted by <italic>π</italic><sub><italic>t</italic></sub> (<italic>a</italic>; <sc>rnn</sc>) for action <inline-formula id="pcbi.1006903.e013"><alternatives><graphic id="pcbi.1006903.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> at trial <italic>t</italic>.</p>
<p>The <sc>lstm</sc> layer is composed of a set of <sc>lstm</sc> cells (<italic>N</italic><sub><italic>c</italic></sub> cells). Each cell is associated with (i) a cell state denoted by <inline-formula id="pcbi.1006903.e014"><alternatives><graphic id="pcbi.1006903.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> for cell <italic>k</italic> at time <italic>t</italic>, and (ii) cell output denoted by <inline-formula id="pcbi.1006903.e015"><alternatives><graphic id="pcbi.1006903.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>h</mml:mi> <mml:mi>t</mml:mi> <mml:mi>k</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> for cell <italic>k</italic> at time <italic>t</italic>. Cell states and outputs are initially zero and are updated after receiving each input. Let’s define <inline-formula id="pcbi.1006903.e016"><alternatives><graphic id="pcbi.1006903.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as a vector containing cell states for all the cells at time <italic>t</italic> (<inline-formula id="pcbi.1006903.e017"><alternatives><graphic id="pcbi.1006903.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>), and <inline-formula id="pcbi.1006903.e018"><alternatives><graphic id="pcbi.1006903.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:mn>1</mml:mn></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>c</mml:mi> <mml:mi>t</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msubsup> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as a vector containing all the cell outputs at time (<inline-formula id="pcbi.1006903.e019"><alternatives><graphic id="pcbi.1006903.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>). Furthermore, assume that <bold>x</bold><sub><italic>t</italic></sub> is a vector containing inputs to the network at time <italic>t</italic>, i.e., one-hot representation of <italic>a</italic><sub><italic>t</italic></sub> and <italic>r</italic><sub><italic>t</italic></sub> (<inline-formula id="pcbi.1006903.e020"><alternatives><graphic id="pcbi.1006903.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>). The update rules for <bold>c</bold><sub><italic>t</italic></sub> and <bold>h</bold><sub><italic>t</italic></sub> are as follows:
<disp-formula id="pcbi.1006903.e021"><alternatives><graphic id="pcbi.1006903.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">f</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">b</mml:mtext> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">f</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
<disp-formula id="pcbi.1006903.e022"><alternatives><graphic id="pcbi.1006903.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">i</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">b</mml:mtext> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">i</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
<disp-formula id="pcbi.1006903.e023"><alternatives><graphic id="pcbi.1006903.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">o</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">b</mml:mtext> <mml:mi>o</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">o</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
<disp-formula id="pcbi.1006903.e024"><alternatives><graphic id="pcbi.1006903.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mtext mathvariant="bold">f</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>⊙</mml:mo> <mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">i</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>⊙</mml:mo> <mml:mtext>tanh</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>U</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">b</mml:mtext> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
<disp-formula id="pcbi.1006903.e025"><alternatives><graphic id="pcbi.1006903.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mtext mathvariant="bold">o</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>⊙</mml:mo> <mml:mtext>tanh</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">c</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext mathvariant="bold">h</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
in which <italic>σ</italic> refers to the sigmoid function and ⊙ represents the element-wise Hadamard product. The parameters of the <sc>lstm</sc> layer include <inline-formula id="pcbi.1006903.e026"><alternatives><graphic id="pcbi.1006903.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mi>W</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>×</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1006903.e027"><alternatives><graphic id="pcbi.1006903.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi>U</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>×</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1006903.e028"><alternatives><graphic id="pcbi.1006903.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mtext mathvariant="bold">b</mml:mtext> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>The softmax layer takes outputs from the <sc>lstm</sc> layer as its inputs (<bold>h</bold><sub><italic>t</italic></sub>) and provides the probability of selecting each action <italic>π</italic><sub><italic>t</italic></sub> (<italic>a</italic>; <sc>rnn</sc>). The parameter of the softmax layer is <inline-formula id="pcbi.1006903.e029"><alternatives><graphic id="pcbi.1006903.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi>V</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mrow><mml:msub><mml:mi>N</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>×</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, and therefore the parameters of the <sc>rnn</sc> model will be Θ = {<italic>V</italic>, <italic>W</italic><sub><italic>f</italic></sub>, <italic>W</italic><sub><italic>i</italic></sub>, <italic>W</italic><sub><italic>o</italic></sub>, <italic>W</italic><sub><italic>c</italic></sub>, <italic>U</italic><sub><italic>f</italic></sub>, <italic>U</italic><sub><italic>i</italic></sub>, <italic>U</italic><sub><italic>o</italic></sub>, <italic>U</italic><sub><italic>c</italic></sub>, <bold>b</bold><sub><italic>f</italic></sub>, <bold>b</bold><sub><italic>i</italic></sub>, <bold>b</bold><sub><italic>o</italic></sub>, <bold>b</bold><sub><italic>c</italic></sub>}.</p>
<p><bold>Training</bold>. In the training phase (learning how humans learn), the aim is to train weights in the network so that the model learns to predict subjects’ actions given their past observations (i.e., it learns how <italic>they</italic> learn). This can be thought as a variant of a <italic>learning-to-learn</italic> process; albeit, more commonly, the learner is a human facing a series of learning tasks rather than a computer model trying to copy the human on a single task. For this purpose, the objective function for optimising weights in the network
(denoted by Θ) for subject set <inline-formula id="pcbi.1006903.e030"><alternatives><graphic id="pcbi.1006903.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula> is,
<disp-formula id="pcbi.1006903.e031"><alternatives><graphic id="pcbi.1006903.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mtext>log</mml:mtext> <mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
where <inline-formula id="pcbi.1006903.e032"><alternatives><graphic id="pcbi.1006903.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the action selected by subjects <italic>s</italic> at trial <italic>t</italic>, and <italic>π</italic><sub><italic>t</italic></sub> (.; <sc>rnn</sc>) is the probability that model assigns to each action. Note that the policy is conditioned on the previous actions and rewards in each block of training; notation for this is omitted, for simplicity.</p>
<p>Models were trained using the maximum-likelihood (ML) estimation method,
<disp-formula id="pcbi.1006903.e033"><alternatives><graphic id="pcbi.1006903.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:msubsup><mml:mo>Θ</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mtext>ML</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:munder><mml:mtext>max</mml:mtext> <mml:mo>Θ</mml:mo></mml:munder> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
where Θ is a vector containing free-parameters of the model (in both <sc>lstm</sc> and softmax layers). The models were implemented in TensorFlow [<xref ref-type="bibr" rid="pcbi.1006903.ref061">61</xref>] and optimized using Adam optimizer [<xref ref-type="bibr" rid="pcbi.1006903.ref062">62</xref>]. Note that Θ was estimated for each group of subjects separately. Networks with different numbers <italic>N</italic><sub><italic>c</italic></sub> of <sc>lstm</sc> cells (<italic>N</italic><sub><italic>c</italic></sub> ∈ {5, 10, 20}) were considered, and the best model was selected using leave-one-out cross-validation (see below). Early stopping was used for regularization and the optimal number of training iterations was selected using leave-one-out cross-validation.</p>
<p>The total number of free parameters (in both the <sc>lstm</sc> layer and softmax layer) were 190, 580, and 1960 for the networks with 5, 10, and 20 <sc>lstm</sc> cells, respectively. In order to control for the effect of initialization of network weights on the final results, a single random network of each size (5, 10, 20) was generated, and was used to initialize the weights in the network.</p>
<p>After the training phase, the weights in the network were frozen and the trained model was used for three purposes: (i) cross-validation (see below), (ii) on-policy simulations and (iii) off-policy simulations. For cross-validation, the previous actions of the test subject(s) and the rewards experienced by the subject(s) were fed into the model, but unlike the training phase, the weights were not changing and we only recorded the prediction of the model about the next action. Note that even though the weights in the network were fixed, the output of the network changed from trial to trial due to the recurrent nature of these networks.</p>
<p>Due to the small sample size, we used the same set of subjects for testing the model and for the validation of model hyper-parameters (<italic>N</italic><sub><italic>c</italic></sub> and number of optimization iterations). That is, we calculated the prediction accuracy of the model in each group using cross-validation for different numbers of training iterations and different numbers of cells (<xref ref-type="supplementary-material" rid="pcbi.1006903.s004">S1 Fig</xref>) and chose the hyper-parameters (<italic>N</italic><sub><italic>c</italic></sub> and number of optimization iterations) that led to the highest performance (for the comparison with other models). Another alternative to this in-sample hyper-parameter selection was to use the data from two of the groups to obtain the optimal hyper-parameters (number of iterations/cells) for the other group. We found that the prediction accuracies obtained using these two alternatives were similar across the groups. The results reported in the paper are those derived using the estimations based on in-sample hyper-parameter estimations. <xref ref-type="supplementary-material" rid="pcbi.1006903.s020">S17 Fig</xref> shows cross-validation results using the alternative hyper-parameter estimation (using other groups for the estimations) and <xref ref-type="supplementary-material" rid="pcbi.1006903.s030">S10 Table</xref> shows the comparison of the cross-validation results obtained using the two methods.</p>
<p>Other than being used for calculating cross-validation statistics, trained models were used for on-policy and off-policy simulations (with frozen weights). In the on-policy simulations, the model received its own actions and earned rewards as inputs (instead of receiving the action selected by the subjects). In the off-policy simulations, the set of actions and rewards that the model received was fixed and predetermined. The details of these simulations are reported in the Results section.</p>
<p><bold>Model settings</bold>. For the <sc>rnn</sc> model, leave-one-out cross-validation was used to determine the number of cells and optimisation iterations required for the <sc>rnn</sc> model to achieve the highest prediction accuracy. We found that the lowest mean negative log-probability (<sc>nlp</sc>) was achieved by 10 cells in the <sc>lstm</sc> layer and after 1100, 1200 optimisation iterations for the <sc>healthy</sc> and <sc>depression</sc> groups respectively whereas for the <sc>bipolar</sc> group the best <sc>nlp</sc> was achieved by 20 cells and 400 optimisation iterations (see <xref ref-type="supplementary-material" rid="pcbi.1006903.s004">S1 Fig</xref>). These settings were used for making predictions and simulations.</p>
</sec>
<sec id="sec021">
<title>Baseline methods</title>
<p>For our baselines, we used <sc>ql</sc>, <sc>qlp</sc> and <sc>gql</sc>– which are variants and generalizations of <italic>Q</italic>-learning [<xref ref-type="bibr" rid="pcbi.1006903.ref031">31</xref>]—and <sc>lin</sc>, which is a logistic regression model.</p>
<p><sc>ql</sc> <italic>model</italic>. After taking action <italic>a</italic><sub><italic>t</italic>−1</sub> at time <italic>t</italic> − 1, the value of the action, denoted by <italic>Q</italic><sub><italic>t</italic></sub>(<italic>a</italic><sub><italic>t</italic>−1</sub>), is updated as follows,
<disp-formula id="pcbi.1006903.e034"><alternatives><graphic id="pcbi.1006903.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:msub><mml:mi>Q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>ϕ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>ϕ</italic> is the learning-rate and <italic>r</italic><sub><italic>t</italic>−1</sub> is the reward received after taking the action. Given the action values, the probability of taking action <italic>a</italic> ∈ {L, R} in trial <italic>t</italic> is:
<disp-formula id="pcbi.1006903.e035"><alternatives><graphic id="pcbi.1006903.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">ql</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:msub><mml:mi>Q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:msub><mml:mi>Q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>β</italic> &gt; 0 is a free-parameter and controls the contribution of values to the choices (balance between exploration and exploitation). The free-parameters of this variant are <italic>ϕ</italic> and <italic>β</italic>. Note that the probability that the models predict for each action at trial <italic>t</italic> is necessarily based on the data <italic>before</italic> observing the action and reward at trial <italic>t</italic>. Further, since there are only two actions, we can write <italic>π</italic><sub><italic>t</italic></sub>(L; <sc>ql</sc>) = 1 − <italic>π</italic><sub><italic>t</italic></sub>(R; <sc>ql</sc>) = <italic>σ</italic>(<italic>β</italic>(<italic>Q</italic><sub><italic>t</italic></sub>(L) − <italic>Q</italic><sub><italic>t</italic></sub>(R))) where <italic>σ</italic>(⋅) is the standard logistic sigmoid.</p>
<p>Note that since here we are focused on modeling a bandit task, <italic>Q</italic>-values are represented as a function of actions and not states.</p>
<p><sc>qlp</sc> <italic>model</italic>. This model is inspired by the fact that humans and other animals have a tendency to stick with the same action for multiple trials (i.e., perseverate), or sometimes to alternate between the actions [independent of the reward effects; <xref ref-type="bibr" rid="pcbi.1006903.ref033">33</xref>]. We therefore call this model <sc>qlp</sc>, for <italic>Q</italic>-learning with perseveration. In it, action values are updated according to <xref ref-type="disp-formula" rid="pcbi.1006903.e034">Eq 8</xref> and so similarly to the <sc>ql</sc> model, but the probability of selecting actions is,
<disp-formula id="pcbi.1006903.e036"><alternatives><graphic id="pcbi.1006903.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">qlp</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:msub><mml:mi>Q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:msub><mml:mi>Q</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where,
<disp-formula id="pcbi.1006903.e037"><alternatives><graphic id="pcbi.1006903.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>κ</mml:mi></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
Therefore, there is a tendency to select the same action again on the next trial (if <italic>κ</italic> &gt; 0) or switch to the other action (if <italic>κ</italic> &lt; 0). In the specific case that <italic>κ</italic> = 0, the <sc>qlp</sc> model reduces to <sc>ql</sc>. Free-parameters are <italic>ϕ</italic>, <italic>β</italic>, <italic>κ</italic>.</p>
<p><sc>gql</sc> <italic>model</italic>. As we will show in the results section, neither <sc>ql</sc> nor <sc>qlp</sc> fit the behaviour of the subjects in the task. As such, we aimed to develop a baseline model which could at least capture high-level behavioural trends, and we built a generalised <italic>Q</italic>-learning model, <sc>gql</sc>, to compare with <sc>rnn</sc>. In this variant, instead of learning a single action value for each action, the model learns <italic>d</italic> different values for each action, where the difference between the values learned for each action is that they are updated using different learning-rates. The action values for action <italic>a</italic> are denoted by <bold>Q</bold>(<italic>a</italic>), which is a vector of size <italic>d</italic>, and the corresponding learning-rates are denoted by vector Φ of size <italic>d</italic> (<bold>0</bold> ⪯ Φ ⪯ <bold>1</bold>). Based on this, the value of action <italic>a</italic><sub><italic>t</italic>−1</sub> at trial <italic>t</italic> − 1 is updated as follows,
<disp-formula id="pcbi.1006903.e038"><alternatives><graphic id="pcbi.1006903.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn mathvariant="bold">1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊙</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>Φ</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where as mentioned before ⊙ represents the element-wise Hadamard product. For example, if <italic>d</italic> = 2, and Φ = [0.1, 0.05], then the model will learn two different values for each action (L, R actions) with one of the values updated using a learning-rate of 0.1 and the other updated using a learning-rate of 0.05. In the specific case that <italic>d</italic> = 1, the above equation reduces to <xref ref-type="disp-formula" rid="pcbi.1006903.e034">Eq 8</xref> used in <sc>ql</sc> and <sc>qlp</sc> models, in which only a single value is learned for each action.</p>
<p>In the <sc>qlp</sc> model, the current action is affected by the last taken action (perseveration). This property is generalised in the <sc>gql</sc> model by learning the history of previously taken actions instead of just the last action. These action histories are denoted by <bold>H</bold>(<italic>a</italic>) for action <italic>a</italic>. <bold>H</bold>(<italic>a</italic>) is a vector of size <italic>d</italic>, and each entry of this vector tracks the tendency of taking action <italic>a</italic> in the past, i.e., if an element of <bold>H</bold>(<italic>a</italic>) is close to one it means that action <italic>a</italic> was taken frequently in the past and being close to zero implies that the action was taken rarely. In similar fashion to action values, for each action <italic>d</italic> different histories are tracked, each of which is modulated by a separate learning-rate. Learning-rates are represented in vector Ψ of size <italic>d</italic> (<bold>0</bold> ⪯ Ψ ⪯ <bold>1</bold>). Assuming that action <italic>a</italic><sub><italic>t</italic>−1</sub> was taken at trial <italic>t</italic> − 1, <bold>H</bold>(<italic>a</italic>) updates as follows,
<disp-formula id="pcbi.1006903.e039"><alternatives><graphic id="pcbi.1006903.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn mathvariant="bold">1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Ψ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊙</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>Ψ</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn mathvariant="bold">1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>Ψ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>⊙</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable> <mml:mo/> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
Intuitively, according to the above equation, if action <italic>a</italic> was taken on a trial, <bold>H</bold>(<italic>a</italic>) increases (the amount of increase depends on the learning-rate of each entry), and for the rest of the actions, <bold>H</bold>(other actions) will decrease (again the amount of decrement is modulated by the learning rates). For example, if <italic>d</italic> = 2, and Ψ = [0.1, 0.05], it means that for each action two choice tendencies will be learned, one of which is updated by rate 0.1 and the other one by rate 0.05.</p>
<p>Having learned <bold>Q</bold>(<italic>a</italic>) and <bold>H</bold>(<italic>a</italic>) for each action, the next question is how are they combined to guide choice. <italic>Q</italic>-learning models assume that the contribution of values to choices is modulated by parameter <italic>β</italic>. Here, since the model learns multiple values for each action, we assume that each value is weighted by a separate parameter, denoted by vector <bold>B</bold> of size <italic>d</italic>. Similarly, in the <sc>qlp</sc> model the contribution of perseveration to choices is controlled by parameter <italic>κ</italic>, and here we assume that parameter <bold>K</bold> modulates the contribution of previous actions to the current choice. Based on this, the probability of taking action <italic>a</italic> at trial <italic>t</italic> is,
<disp-formula id="pcbi.1006903.e040"><alternatives><graphic id="pcbi.1006903.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mrow><mml:msubsup><mml:mi>π</mml:mi> <mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">gql</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext mathvariant="bold">B</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">K</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext mathvariant="bold">B</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">K</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where “⋅” operator refers to the inner product. Here, we also add extra flexibility to the model by allowing values to interact with the history of previous actions in influencing choices. For example, if <italic>d</italic> = 2, we allow the two learned values for each action to interact with the two learned action histories of each action, leading to four interaction terms, and the contribution of each interaction term to choices is determined by a matrix <bold>C</bold> of size <italic>d</italic> × <italic>d</italic> (<italic>d</italic> = 2 in this example),
<disp-formula id="pcbi.1006903.e041"><alternatives><graphic id="pcbi.1006903.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">gql</mml:mtext> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext mathvariant="bold">B</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">K</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mtext mathvariant="bold">C</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mtext mathvariant="bold">B</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">K</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">H</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mtext mathvariant="bold">C</mml:mtext> <mml:mo>·</mml:mo> <mml:msub><mml:mtext mathvariant="bold">Q</mml:mtext> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
The free-parameters of this model are Φ, Ψ, <bold>B</bold>, <bold>K</bold>, and <bold>C</bold>. In this paper we use models with <italic>d</italic> = 1, 2, 10, which have 5, 12 and 140 free parameters respectively. We used <italic>d</italic> = 2 for the results reported in the main text, since this model setting was able to capture several behavioural trends while still being interpretable. The results using <italic>d</italic> = 1, 10 are reported in the supplementary materials to illustrate the models’ capabilities in extreme cases.</p>
<p><sc>lin</sc> <italic>model</italic>. The probability of taking each action is determined by a history past rewards and actions—up to <italic>J</italic> trials back—using a linear logistic regression model,
<disp-formula id="pcbi.1006903.e042"><alternatives><graphic id="pcbi.1006903.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:mtext>log</mml:mtext> <mml:mfrac><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>L</mml:mtext> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">lin</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>=</mml:mo> <mml:mtext>R</mml:mtext> <mml:mo>;</mml:mo> <mml:mtext mathsize="small">lin</mml:mtext> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ζ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>J</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi>μ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>J</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
Parameter <italic>J</italic> was selected using cross-validation (<xref ref-type="supplementary-material" rid="pcbi.1006903.s005">S2 Fig</xref>), which indicated that <italic>J</italic> = 18 provides the best <sc>nlp</sc> mean, and therefore the model with <italic>J</italic> = 18 was used in the analyses presented in the paper.</p>
<p><italic>Objective function</italic>. The objective function for optimising the models was the same as the one chosen for <sc>rnn</sc>,
<disp-formula id="pcbi.1006903.e043"><alternatives><graphic id="pcbi.1006903.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:mtext>log</mml:mtext> <mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>∈</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mtext mathsize="small">ql</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">qlp</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">gql</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">lin</mml:mtext> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where, as mentioned before, <inline-formula id="pcbi.1006903.e044"><alternatives><graphic id="pcbi.1006903.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is the action selected by subject <italic>s</italic> at trial <italic>t</italic>, and <inline-formula id="pcbi.1006903.e045"><alternatives><graphic id="pcbi.1006903.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>.</mml:mo> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the probability that model <inline-formula id="pcbi.1006903.e046"><alternatives><graphic id="pcbi.1006903.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> assigns to each action. Models were trained using the maximum-likelihood estimation method,
<disp-formula id="pcbi.1006903.e047"><alternatives><graphic id="pcbi.1006903.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mrow><mml:msubsup><mml:mo>Θ</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mtext>ML</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:munder><mml:mtext>max</mml:mtext> <mml:mo>Θ</mml:mo></mml:munder> <mml:mi mathvariant="script">L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where Θ is a vector containing the free-parameters of the models. Optimizations for all models except <sc>lin</sc>, were performed using Adam optimizer [<xref ref-type="bibr" rid="pcbi.1006903.ref062">62</xref>], and using the automatic differentiation method provided in TensorFlow [<xref ref-type="bibr" rid="pcbi.1006903.ref061">61</xref>]. The free-parameters with limited support (<italic>ϕ</italic>, <italic>β</italic>, Φ, Ψ) were transformed to satisfy the constraints. For <sc>lin</sc> model, we used ‘glm’ method in R [<xref ref-type="bibr" rid="pcbi.1006903.ref063">63</xref>] with ‘binomial’ link function to estimate the parameters and to make predictions.</p>
</sec>
<sec id="sec022">
<title>Performance measures</title>
<p>Two different measures were used for quantifying the predictive accuracy of the models. The first measure is the average log-probability of the models’ prediction for the actions taken by subjects. For a group of subjects denoted by <inline-formula id="pcbi.1006903.e048"><alternatives><graphic id="pcbi.1006903.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>, we define negative log-probability (<sc>nlp</sc>) as follows:
<disp-formula id="pcbi.1006903.e049"><alternatives><graphic id="pcbi.1006903.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mrow><mml:mtext mathsize="small">nlp</mml:mtext> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mtext>log</mml:mtext> <mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>∈</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mtext mathsize="small">rnn</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">lin</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">gql</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">ql</mml:mtext> <mml:mo>,</mml:mo> <mml:mtext mathsize="small">qlp</mml:mtext> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
The other measure is the percentage of actions predicted correctly,
<disp-formula id="pcbi.1006903.e050"><alternatives><graphic id="pcbi.1006903.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006903.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mrow><mml:mo>%</mml:mo> <mml:mtext>correct</mml:mtext> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>…</mml:mo> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msub> <mml:mo>⟦</mml:mo> <mml:mtext>arg</mml:mtext> <mml:mspace width="4pt"/><mml:msub><mml:mtext>max</mml:mtext> <mml:mi>a</mml:mi></mml:msub> <mml:msub><mml:mi>π</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">M</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>s</mml:mi></mml:msubsup> <mml:mo>⟧</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="script">T</mml:mi> <mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
where ⟦.⟧ denotes the indicator function. Unlike, ‘%correct’, <sc>nlp</sc> takes the probabilities of predictions into account instead of making binary predictions for the next action. In this way, if the models are certain about wrong predictions <sc>nlp</sc> performance gets penalized, and it gets credit if the models are certain about a correct prediction.</p>
</sec>
<sec id="sec023">
<title>Model selection</title>
<p>Leave-one-out cross-validation was used for comparing different models. At each round, one of the subjects was withheld and the model was trained using the remaining subjects; the trained model was then used to make predictions about the withheld subject. The withheld subject was rotated in each group, yielding 34, 34 and 33 prediction accuracy measures in the <sc>healthy</sc>, <sc>depression</sc>, and <sc>bipolar</sc> groups respectively.</p>
</sec>
</sec>
<sec id="sec024">
<title>Statistical analysis</title>
<p>For the analysis we performed hierarchical linear mixed-effects regression using the lme4 package in R [<xref ref-type="bibr" rid="pcbi.1006903.ref064">64</xref>] and obtained <italic>p</italic>-values for regression coefficients using the lmerTest package [<xref ref-type="bibr" rid="pcbi.1006903.ref065">65</xref>]. Hierarchical mixed-effects models involve random-effects and fixed-effects. Fixed-effects are of primary interest and are estimated directly (fixed-effects estimates are denoted by <italic>η</italic>). Random-effects specify different levels at which the data is collected (e.g., different subjects), i.e., fixed-effects are nested within random-effects in a hierarchical manner. Specific fixed-effects and random-effects used for each analysis are mentioned below for each analysis. For each test we report parameter estimate (<italic>η</italic>), standard error (SE), and <italic>p</italic>-value.</p>
<p>For the analysis presented in section ‘Performance in the task’ the intercept term was the random-effect at the subject level; action (low reward probability = 0, high reward probabilities = 1) was the fixed-effect; the dependent variable was the probability of selecting the action. For the second set of analyses in this section, the intercept term was the random-effect at the subject level; and action (low reward probability = 0, high reward probabilities = 1), groups (<sc>healthy</sc> = 0, <sc>depression</sc> = 1/<sc>bipolar</sc> = 1) and their interaction were fixed-effects; the dependent variable was the probability of selecting the action.</p>
<p>In the analysis in section ‘The immediate effect of reward on choice’ the intercept was the random-effect at the subject level; whether reward was earned on the previous trial was the fixed-effect and the probability of staying on the same action was the dependent variable.</p>
<p>In the analysis presented in section ‘Action prediction’ the intercept term was the random-effect at the cross-validation fold level; model (<sc>gql</sc> = 1, <sc>qlp</sc> = 0 for the first analysis and <sc>lin</sc> = 1, <sc>rnn</sc> = 0 for the second analysis) was the fixed-effect. <sc>nlp</sc> was the dependent variable.</p>
<p>In the analysis presented in section ‘The effect of reward on choice’ the intercept was the random-effect at the subject level; whether zero rewards or more than two rewards were earned previously was fixed-effect. The dependent variable was the probability of
staying with an action.</p>
<p>In the analysis presented in section ‘Task’, the intercept term was the random-effect at the group level (<sc>healthy</sc> or <sc>bipolar</sc>), and the mode of task completion (fMRI setting = 1, computer = 0) was the fixed-effect; the probability of selecting the better key was the dependent variable.</p>
<p>In the analysis presented in section ‘The effect of reward on choice’ the intercept was the random-effect at the subject level; the number of times that an action was repeated since switching to the action was the fixed-effect (between zero to 15 times). The dependent variable was the probability of staying with an action. Note that in <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref>:right-panel in this section, to be consistent with off-policy simulations, only trials on which (i) subjects did not earn a reward on that trial, and (ii) subjects did not earn a reward since switching to the current action, were included in the graph.</p>
<p>For Loess regression [<xref ref-type="bibr" rid="pcbi.1006903.ref066">66</xref>], ‘loess’ method in R was used [<xref ref-type="bibr" rid="pcbi.1006903.ref063">63</xref>].</p>
</sec>
</sec>
<sec id="sec025">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006903.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Behavioural analysis using <sc>gql</sc>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s002" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>The choice of off-policy settings.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s003" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Analysis of randomness of choices.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s004" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Cross-validation results for different numbers of cells and optimization iterations.</title>
<p><bold>(Top-panel)</bold> Percentage of actions predicted correctly averaged over leave-one-out cross-validation folds. <bold>(Bottom-panel)</bold> Mean <sc>nlp</sc> averaged over cross-validation folds. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s005" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Cross-validation results for the <sc>lin</sc> model as a function of number of trials back (<italic>J</italic>).</title>
<p><bold>(Left-panel)</bold> <sc>nlp</sc> (negative log-probability) averaged across leave-one-out cross-validation folds. Lower values are better. <bold>(Right-panel)</bold> Percentage of actions predicted correctly averaged over cross-validation folds. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s006" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Choices of the <sc>healthy</sc> group.</title>
<p>Each row shows the choices of a subject across different blocks (12 blocks).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s007" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Choices of the <sc>depression</sc> group.</title>
<p>Each row shows the choices of a subject across different blocks (12 blocks).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s008" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Choices of the <sc>bipolar</sc> group.</title>
<p>Each row shows the choices of a subject across different blocks (12 blocks).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s009" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Off-policy simulations of <sc>lin</sc>.</title>
<p>Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph. Note that the simulation conditions are same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s010" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Off-policy simulations of <sc>gql</sc> (<italic>d</italic> = 2).</title>
<p>Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph. Note that the simulation conditions are the same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s011" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Off-policy simulations of <sc>qlp</sc>.</title>
<p>Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph. Note that the simulation conditions are the same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s012" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s012" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Off-policy simulations of <sc>ql</sc>.</title>
<p>Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph. Note that the simulation conditions are the same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s013" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s013" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Off-policy simulations of <sc>gql</sc> with <italic>d</italic> = 1.</title>
<p>Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions for each group on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph. Note that the simulation conditions are the same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s014" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s014" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>The effect of the initialisation of the network on the off-policy simulations of <sc>rnn</sc>.</title>
<p>The simulation conditions are the same as those depicted in Figs <xref ref-type="fig" rid="pcbi.1006903.g007">7</xref> and <xref ref-type="fig" rid="pcbi.1006903.g006">6</xref>. Here, 15 different initial networks were generated and optimised and the policies of the models on each trial were averaged. The grey ribbon around the policy shows the standard deviation of the policies. Each panel shows a simulation for 30 trials (horizontal axis), and the vertical axis shows the predictions of each model on each trial. The ribbon below each panel shows the action which was fed to the model on each trial. In the first 10 trials, the action that the model received was R and in the next 20 trials it was L. Rewards are shown by black crosses (x) on the graphs. See text for the interpretation of the graph.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s015" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s015" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Percentage of each run of actions relative to the total number of runs for each subject.</title>
<p>Percentage of each length of run of actions relative to the total number of runs in each subject (averaged over subjects). Red dots represent data for each subject, and error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s016" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s016" xlink:type="simple">
<label>S13 Fig</label>
<caption>
<title><sc>rnn</sc> simulations.</title>
<p>The graph is similar to <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref> but using data from <sc>rnn</sc> simulations (on-policy). <bold>(Left-panel)</bold> Probability of staying with an action after earning reward as a function of the number of actions taken since switching to the current action (averaged over subjects). Each red dot represents the data for each subject. <bold>(Right-panel)</bold> Probability of staying with an actions as a function of the number of actions taken since switching to the current action. The red line was obtained using Loess regression (Local Regression), which is a non-parametric regression approach. The grey area around the red line represents 95% confidence interval. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s017" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s017" xlink:type="simple">
<label>S14 Fig</label>
<caption>
<title><sc>lin</sc> simulations.</title>
<p>The graph is similar to <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref> but using data from <sc>lin</sc> simulations (on-policy). <bold>(Left-panel)</bold> Probability of staying with an action after earning reward as a function of the number of actions taken since switching to the current action (averaged over subjects). Each red dot represents the data for each subject. <bold>(Right-panel)</bold> Probability of staying with an actions as a function of the number of actions taken since switching to the current action. The red line was obtained using Loess regression (Local Regression), which is a non-parametric regression approach. The grey area around the red line represents 95% confidence interval. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s018" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s018" xlink:type="simple">
<label>S15 Fig</label>
<caption>
<title><sc>gql</sc> simulations (<italic>d</italic> = 2).</title>
<p>The graph is similar to <xref ref-type="fig" rid="pcbi.1006903.g008">Fig 8</xref> but using data from <sc>gql</sc> simulations with <italic>d</italic> = 2 (on-policy). <bold>(Left-panel)</bold> Probability of staying with an action after earning reward as a function of the number of actions taken since switching to the current action (averaged over subjects). Each red dot represents the data for each subject. <bold>(Right-panel)</bold> Probability of staying with an actions as a function of the number of actions taken since switching to the current action. The red line was obtained using Loess regression (Local Regression), which is a non-parametric regression approach. The grey area around the red line represents 95% confidence interval. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s019" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s019" xlink:type="simple">
<label>S16 Fig</label>
<caption>
<title><sc>gql</sc> simulations (<italic>d</italic> = 10).</title>
<p>The graph is similar to <xref ref-type="fig" rid="pcbi.1006903.g009">Fig 9</xref> but using data from <sc>gql</sc> simulations with <italic>d</italic> = 10 (on-policy). Median number of actions executed in a row before switching to another action (run of actions) in each subject as a function of the length of the previous run of actions (averaged over subjects). The dotted line shows the points at which the length of the previous and current runs are the same. Note that the median rather than the average was because we aimed to illustrate the most common ‘length of current run’, instead of average run length in each subject. Error-bars represent 1SEM.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s020" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s020" xlink:type="simple">
<label>S17 Fig</label>
<caption>
<title>Cross-validation results.</title>
<p><bold>(Left-panel)</bold> <sc>nlp</sc> (negative log-probability) averaged across leave-one-out cross-validation folds. Lower values are better. <bold>(Right-panel)</bold> Percentage of actions predicted correctly averaged over cross-validation folds. Note that the difference between this figure and <xref ref-type="fig" rid="pcbi.1006903.g005">Fig 5</xref> is that in <xref ref-type="fig" rid="pcbi.1006903.g005">Fig 5</xref> hyper-parameters were obtained using in-sample estimations but here we used the data from two of the groups to obtain the optimal hyper-parameters (number of iterations/cells) for the other group. See text for more information.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s021" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s021" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Prediction of diagnostic labels using <sc>lin</sc>.</title>
<p>Number of subjects for each true- and predicted-label. The numbers inside parentheses are the percentage of subjects relative to the total number of subjects in each diagnostic group.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s022" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s022" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Prediction of diagnostic labels using <sc>gql</sc> (<italic>d</italic> = 2).</title>
<p>Number of subjects for each true- and predicted-label. The numbers inside parentheses are the percentage of subjects relative to the total number of subjects in each diagnostic group.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s023" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s023" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Estimated parameters for <sc>ql</sc> model.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s024" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s024" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Estimated parameters for <sc>qlp</sc> model.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s025" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s025" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Estimated parameters for <sc>gql</sc> model with <italic>d</italic> = 2.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s026" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s026" xlink:type="simple">
<label>S6 Table</label>
<caption>
<title>Negative log-likelihood for each model optimized over all the subjects in each group.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s027" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s027" xlink:type="simple">
<label>S7 Table</label>
<caption>
<title>Negative log-likelihood for each model.</title>
<p>For <sc>rnn</sc> a single model was fitted to the whole group using ML estimation. For baseline methods (<sc>gql</sc>, <sc>qlp</sc>, and <sc>ql</sc>), a separate model was fitted to each subject, and the reported number is the sum of negative log-likelihoods over the whole group.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s028" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s028" xlink:type="simple">
<label>S8 Table</label>
<caption>
<title>Mean and standard deviation of negative log-likelihood for <sc>rnn</sc> over 15 different initialisations of the model and optimised over all the subjects in each group.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s029" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s029" xlink:type="simple">
<label>S9 Table</label>
<caption>
<title>Average number of trials in each pair of reward probabilities in each group.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s030" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s030" xlink:type="simple">
<label>S10 Table</label>
<caption>
<title>Mean of <sc>nlp</sc> derived using in-sample hyper-parameter estimation (in-sample) and using the data of other groups (other-groups).</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006903.s031" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006903.s031" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Supporting data.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006903.ref001">
<label>1</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Diederich</surname> <given-names>A</given-names></name>. <source>Cognitive modeling</source>. <publisher-name>Sage</publisher-name>; <year>2010</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <chapter-title>Trial-by-trial data analysis using computational models</chapter-title>. In: <name name-style="western"><surname>Delgado</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Robbins</surname> <given-names>TW</given-names></name>, editors. <source>Decision Making, Affect, and Learning</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>The neural basis of decision making</article-title>. <source>Annual review of neuroscience</source>. <year>2007</year>;<volume>30</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.29.051605.113038" xlink:type="simple">10.1146/annurev.neuro.29.051605.113038</ext-link></comment> <object-id pub-id-type="pmid">17600525</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zeighami</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bahrami</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Eissa</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Hewedi</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Moustafa</surname> <given-names>AA</given-names></name>. <article-title>Impulse control disorders in Parkinson’s disease are associated withdysfunction in stimulus valuation but not action valuation</article-title>. <source>The Journal of neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>23</issue>):<fpage>7814</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4063-13.2014" xlink:type="simple">10.1523/JNEUROSCI.4063-13.2014</ext-link></comment> <object-id pub-id-type="pmid">24899705</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Stout</surname> <given-names>JC</given-names></name>. <article-title>A contribution of cognitive decision models to clinical assessment:decomposing performance on the Bechara gambling task</article-title>. <source>Psychological assessment</source>. <year>2002</year>;<volume>14</volume>(<issue>3</issue>):<fpage>253</fpage>–<lpage>62</lpage> <object-id pub-id-type="pmid">12214432</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref006">
<label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Dezfouli A, Keramati MM, Ekhtiari H, Safaei H, Lucas C. Understanding Addictive Behavior on the Iowa Gambling Task UsingReinforcement Learning Framework. In: 30th Annual Conference of the Cognitive Science Society; 2007. p.1094–1099.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>. <article-title>Computational psychiatry</article-title>. <source>Trends in cognitive sciences</source>. <year>2012</year>;<volume>16</volume>(<issue>1</issue>):<fpage>72</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2011.11.018" xlink:type="simple">10.1016/j.tics.2011.11.018</ext-link></comment> <object-id pub-id-type="pmid">22177032</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>O’Doherty</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Hampton</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>. <article-title>Model-based fMRI and its application to reward learning and decisionmaking</article-title>. <source>Annals of the New York Academy of sciences</source>. <year>2007</year>;<volume>1104</volume>(<issue>1</issue>):<fpage>35</fpage>–<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1196/annals.1390.022" xlink:type="simple">10.1196/annals.1390.022</ext-link></comment> <object-id pub-id-type="pmid">17416921</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>. <article-title>Dorsal hippocampus contributes to model-based planning</article-title>. <source>Nature neuroscience</source>. <year>2017</year>;<volume>20</volume>(<issue>9</issue>):<fpage>1269</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4613" xlink:type="simple">10.1038/nn.4613</ext-link></comment> <object-id pub-id-type="pmid">28758995</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Acuña</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Schrater</surname> <given-names>P</given-names></name>. <article-title>Structure Learning in Human Sequential Decision-Making</article-title>. <source>PLOS Computational Biology</source>. <year>2010</year>;<volume>6</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Reinforcement learning: the good, the bad and the ugly</article-title>. <source>Current opinion in neurobiology</source>. <year>2008</year>;<volume>18</volume>(<issue>2</issue>):<fpage>185</fpage>–<lpage>96</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2008.08.003" xlink:type="simple">10.1016/j.conb.2008.08.003</ext-link></comment> <object-id pub-id-type="pmid">18708140</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Siegelmann</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Sontag</surname> <given-names>ED</given-names></name>. <article-title>On the computational power of neural nets</article-title>. <source>Journal of computer and system sciences</source>. <year>1995</year>;<volume>50</volume>(<issue>1</issue>):<fpage>132</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/jcss.1995.1013" xlink:type="simple">10.1006/jcss.1995.1013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Reward-based training of recurrent neural networks for cognitive andvalue-based tasks</article-title>. <source>eLife</source>. <year>2017</year>;<volume>6</volume>:<fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.21492" xlink:type="simple">10.7554/eLife.21492</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Nie</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>T</given-names></name>. <article-title>A neural network model for the orbitofrontal cortex and task spaceacquisition during reinforcement learning</article-title>. <source>PLOS Computational Biology</source>. <year>2018</year>;<volume>14</volume>(<issue>1</issue>):<fpage>e1005925</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005925" xlink:type="simple">10.1371/journal.pcbi.1005925</ext-link></comment> <object-id pub-id-type="pmid">29300746</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miconi</surname> <given-names>T</given-names></name>. <article-title>Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks</article-title>. <source>eLife</source>. <year>2017</year>;<volume>6</volume>:<fpage>1</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.20899" xlink:type="simple">10.7554/eLife.20899</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carnevale</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>DeLafuente</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>. <article-title>Dynamic Control of Response Criterion in Premotor Cortex duringPerceptual Detection under Temporal Uncertainty</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>86</volume>(<issue>4</issue>):<fpage>1067</fpage>–<lpage>1077</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.04.014" xlink:type="simple">10.1016/j.neuron.2015.04.014</ext-link></comment> <object-id pub-id-type="pmid">25959731</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Context-dependent computation by recurrent dynamics in prefrontalcortex</article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>(<issue>7474</issue>):<fpage>78</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature12742" xlink:type="simple">10.1038/nature12742</ext-link></comment> <object-id pub-id-type="pmid">24201281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Training Excitatory-Inhibitory Recurrent Neural Networks forCognitive Tasks: A Simple and Flexible Framework</article-title>. <source>PLoS Computational Biology</source>. <year>2016</year>;<volume>12</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004792" xlink:type="simple">10.1371/journal.pcbi.1004792</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>From fixed points to chaos: Three models of delayed discrimination</article-title>. <source>Progress in Neurobiology</source>. <year>2013</year>;<volume>103</volume>:<fpage>214</fpage>–<lpage>222</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pneurobio.2013.02.002" xlink:type="simple">10.1016/j.pneurobio.2013.02.002</ext-link></comment> <object-id pub-id-type="pmid">23438479</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yang</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Song</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Clustering and compositionality of task representations in a neuralnetwork trained to perform many cognitive tasks</article-title>. <source>bioRxiv</source>. <year>2017</year>; p. 183632.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Churchland</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>. <article-title>A neural network that finds a naturalistic solution for theproduction of muscle activity</article-title>. <source>Nature Neuroscience</source>. <year>2015</year>;<volume>18</volume>(<issue>7</issue>):<fpage>1025</fpage>–<lpage>1033</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4042" xlink:type="simple">10.1038/nn.4042</ext-link></comment> <object-id pub-id-type="pmid">26075643</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name>. <article-title>Optimal control of transient dynamics in balanced networks supportsgeneration of complex movements</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>82</volume>(<issue>6</issue>):<fpage>1394</fpage>–<lpage>1406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2014.04.045" xlink:type="simple">10.1016/j.neuron.2014.04.045</ext-link></comment> <object-id pub-id-type="pmid">24945778</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rajan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Harvey</surname> <given-names>CDD</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DWW</given-names></name>. <article-title>Recurrent Network Models of Sequence Generation and Memory</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>(<issue>1</issue>):<fpage>128</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.02.009" xlink:type="simple">10.1016/j.neuron.2016.02.009</ext-link></comment> <object-id pub-id-type="pmid">26971945</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laje</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Buonomano</surname> <given-names>DV</given-names></name>. <article-title>Robust timing and motor patterns by taming chaos in recurrent neuralnetworks</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>7</issue>):<fpage>925</fpage>–<lpage>933</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3405" xlink:type="simple">10.1038/nn.3405</ext-link></comment> <object-id pub-id-type="pmid">23708144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Hochreiter S, Younger AS, Conwell PR. Learning to learn using gradient descent. In: International Conference on Artificial Neural Networks. Springer;2001. p. 87–94.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Wang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo JZ, Munos R, et al. Learning to reinforcement learn. arXiv preprint arXiv:161105763. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Duan Y, Schulman J, Chen X, Bartlett PL, Sutskever I, Abbeel P. RL<sup>2</sup>: Fast Reinforcement Learning via Slow ReinforcementLearning. arXiv preprint arXiv:161102779. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Weinstein A, Botvinick MM. Structure Learning in Motor Control: A Deep Reinforcement LearningModel. arXiv preprint arXiv:170606827. 2017.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Long short-term memory</article-title>. <source>Neural computation</source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>1780</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1997.9.8.1735" xlink:type="simple">10.1162/neco.1997.9.8.1735</ext-link></comment> <object-id pub-id-type="pmid">9377276</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>. <source>Deep Learning</source>. <publisher-name>MIT Press</publisher-name>; <year>2016</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Watkins CJCH. Learning from Delayed Rewards [Ph. D. thesis]. Cambridge University; 1989.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Validation of decision-making models and analysis of decision variables in the rat basal ganglia</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>31</issue>):<fpage>9861</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.6157-08.2009" xlink:type="simple">10.1523/JNEUROSCI.6157-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19657038</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lau</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>Dynamic response-by-response models of matching behavior in rhesusmonkeys</article-title>. <source>Journal of the experimental analysis of behavior</source>. <year>2005</year>;<volume>84</volume>(<issue>3</issue>):<fpage>555</fpage>–<lpage>79</lpage> <object-id pub-id-type="pmid">16596980</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sul</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Huh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jung</surname> <given-names>MW</given-names></name>. <article-title>Role of striatum in updating values of chosen actions</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>47</issue>):<fpage>14701</fpage>–<lpage>14712</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2728-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2728-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19940165</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lecun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lake</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname> <given-names>JB</given-names></name>. <article-title>Human-level concept learning through probabilistic programinduction</article-title>. <source>Science</source>. <year>2015</year>;<volume>350</volume>(<issue>6266</issue>):<fpage>1332</fpage>–<lpage>1338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.aab3050" xlink:type="simple">10.1126/science.aab3050</ext-link></comment> <object-id pub-id-type="pmid">26659050</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Ramos</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name>. <article-title>Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models</article-title>. In: <source>Advances in Neural Information Processing Systems (Neurips)</source>;<year>2018</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Karpathy A, Johnson J, Fei-Fei L. Visualizing and understanding recurrent networks. arXiv preprint arXiv:150602078. 2015.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>. <article-title>Recurrent neural networks as versatile tools of neuroscience research</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2017</year>;<volume>46</volume>:<fpage>1</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2017.06.003" xlink:type="simple">10.1016/j.conb.2017.06.003</ext-link></comment> <object-id pub-id-type="pmid">28668365</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, et al. Intriguing properties of neural networks. arXiv preprint arXiv:13126199. 2013;.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fründ</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Wichmann</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Macke</surname> <given-names>JH</given-names></name>. <article-title>Quantifying the effect of intertrial dependence on perceptualdecisions</article-title>. <source>Journal of vision</source>. <year>2014</year>;<volume>14</volume>(<issue>7</issue>):<fpage>9</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.7.9" xlink:type="simple">10.1167/14.7.9</ext-link></comment> <object-id pub-id-type="pmid">24944238</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howarth</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Bulmer</surname> <given-names>MG</given-names></name>. <article-title>Non-random sequences in visual threshold experiments</article-title>. <source>Quarterly Journal of Experimental Psychology</source>. <year>1956</year>;<volume>8</volume>(<issue>4</issue>):<fpage>163</fpage>–<lpage>171</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/17470215608416816" xlink:type="simple">10.1080/17470215608416816</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lages</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jaworska</surname> <given-names>K</given-names></name>. <article-title>How predictable are “spontaneous decisions” and “hiddenintentions”? Comparing classification results based on previous responses with multivariate pattern analysis of fMRI BOLD signals</article-title>. <source>Frontiers in psychology</source>. <year>2012</year>;<volume>3</volume>:<fpage>56</fpage> <object-id pub-id-type="pmid">22408630</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lages</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Treisman</surname> <given-names>M</given-names></name>. <article-title>A criterion setting theory of discrimination learning that accountsfor anisotropies and context effects</article-title>. <source>Seeing and perceiving</source>. <year>2010</year>;<volume>23</volume>(<issue>5</issue>):<fpage>401</fpage>–<lpage>434</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/187847510X541117" xlink:type="simple">10.1163/187847510X541117</ext-link></comment> <object-id pub-id-type="pmid">21466134</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Senders</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Sowards</surname> <given-names>A</given-names></name>. <article-title>Analysis of response sequences in the setting of a psychophysical experiment</article-title>. <source>The American journal of psychology</source>. <year>1952</year>;<volume>65</volume>(<issue>3</issue>):<fpage>358</fpage>–<lpage>374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/1418758" xlink:type="simple">10.2307/1418758</ext-link></comment> <object-id pub-id-type="pmid">12976561</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Treisman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>TC</given-names></name>. <article-title>A theory of criterion setting with an application to sequentialdependencies</article-title>. <source>Psychological Review</source>. <year>1984</year>;<volume>91</volume>(<issue>1</issue>):<fpage>68</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Verplanck</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Blough</surname> <given-names>DS</given-names></name>. <article-title>Randomized stimuli and the non-independence of successive responsesat the visual threshold</article-title>. <source>The Journal of general psychology</source>. <year>1958</year>;<volume>59</volume>(<issue>2</issue>):<fpage>263</fpage>–<lpage>272</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00221309.1958.9710195" xlink:type="simple">10.1080/00221309.1958.9710195</ext-link></comment> <object-id pub-id-type="pmid">13587937</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Angela</surname> <given-names>JY</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name>. <article-title>Sequential effects: superstition or rational behavior?</article-title> In: <source>Advances in Neural Information Processing Systems (Neurips)</source>;<year>2009</year>. p. <fpage>1873</fpage>–<lpage>1880</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilder</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mozer</surname> <given-names>MC</given-names></name>. <article-title>Sequential effects reflect parallel learning of multipleenvironmental regularities</article-title>. In: <source>Advances in Neural Information Processing Systems (Neurips)</source>;<year>2009</year>. p. <fpage>2053</fpage>–<lpage>2061</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rush</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Trivedi</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Wisniewski</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Nierenberg</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Acute and longer-term outcomes in depressed outpatients requiring one or several treatment steps: A STAR*D report</article-title>. <source>American Journal of Psychiatry</source>. <year>2006</year>;<volume>163</volume>(<issue>11</issue>):<fpage>1905</fpage>–<lpage>1917</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1176/ajp.2006.163.11.1905" xlink:type="simple">10.1176/ajp.2006.163.11.1905</ext-link></comment> <object-id pub-id-type="pmid">17074942</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Penny</surname> <given-names>WD</given-names></name>, <name name-style="western"><surname>Daunizeau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Moran</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Bayesian model selection for group studies</article-title>. <source>NeuroImage</source>. <year>2009</year>;<volume>46</volume>(<issue>4</issue>):<fpage>1004</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.03.025" xlink:type="simple">10.1016/j.neuroimage.2009.03.025</ext-link></comment> <object-id pub-id-type="pmid">19306932</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Piray</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dezfouli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Heskes</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Hierarchical Bayesian inference for concurrent model fitting andcomparison for group studies</article-title>. <source>bioRxiv</source>. <year>2018</year>; p. 393561.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref053">
<label>53</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. In: <name name-style="western"><surname>Parzen</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tanabe</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kitagawa</surname> <given-names>G</given-names></name>, editors. <source>Information Theory andan Extension of the Maximum Likelihood Principle</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer New York</publisher-name>; <year>1998</year>. p. <fpage>199</fpage>–<lpage>213</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Spiegelhalter</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Best</surname> <given-names>NG</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>van der Linde</surname> <given-names>A</given-names></name>. <article-title>Bayesian measures of model complexity and fit (with discussion)</article-title>. <source>Journal of the Royal Statistical Society, Series B</source>. <year>2002</year>;<volume>64</volume>:<fpage>583</fpage>–<lpage>639(57)</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1467-9868.00353" xlink:type="simple">10.1111/1467-9868.00353</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van der Linde</surname> <given-names>A</given-names></name>. <article-title>DIC in variable selection</article-title>. <source>Statistica Neerlandica</source>. <year>2005</year>;<volume>59</volume>(<issue>1</issue>):<fpage>45</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9574.2005.00278.x" xlink:type="simple">10.1111/j.1467-9574.2005.00278.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watanabe</surname> <given-names>S</given-names></name>. <article-title>Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory</article-title>. <source>Journal of Machine Learning Research</source>. <year>2010</year>;<volume>11</volume>(<issue>Dec</issue>):<fpage>3571</fpage>–<lpage>3594</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vehtari</surname> <given-names>A</given-names></name>. <article-title>Understanding predictive information criteria for Bayesian models</article-title>. <source>Statistics and computing</source>. <year>2014</year>;<volume>24</volume>(<issue>6</issue>):<fpage>997</fpage>–<lpage>1016</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11222-013-9416-2" xlink:type="simple">10.1007/s11222-013-9416-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hamilton</surname> <given-names>M</given-names></name>. <article-title>A rating scale for depression</article-title>. <source>Journal of neurology, neurosurgery, and psychiatry</source>. <year>1960</year>;<volume>23</volume>(<issue>1</issue>):<fpage>56</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1136/jnnp.23.1.56" xlink:type="simple">10.1136/jnnp.23.1.56</ext-link></comment> <object-id pub-id-type="pmid">14399272</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Young</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Biggs</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Ziegler</surname> <given-names>VE</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>DA</given-names></name>. <article-title>A rating scale for mania: reliability, validity and sensitivity</article-title>. <source>The British Journal of Psychiatry</source>. <year>1978</year>;<volume>133</volume>(<issue>5</issue>):<fpage>429</fpage>–<lpage>435</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1192/bjp.133.5.429" xlink:type="simple">10.1192/bjp.133.5.429</ext-link></comment> <object-id pub-id-type="pmid">728692</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goldman</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Skodol</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Lave</surname> <given-names>TR</given-names></name>. <article-title>Revising axis V for DSM-IV: a review of measures of socialfunctioning</article-title>. <source>American Journal of Psychiatry</source>. <year>1992</year>;<volume>149</volume>(<issue>9</issue>):<fpage>1148</fpage>–<lpage>1156</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1176/ajp.149.9.1148" xlink:type="simple">10.1176/ajp.149.9.1148</ext-link></comment> <object-id pub-id-type="pmid">1386964</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref061">
<label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. Tensorflow: Large-scale machine learning on heterogeneousdistributed systems. arXiv preprint arXiv:160304467. 2016.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref062">
<label>62</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma DP, Ba J. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:14126980. 2014.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref063">
<label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">R Core Team. R: A Language and Environment for Statistical Computing; 2016. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/" xlink:type="simple">https://www.r-project.org/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bates</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mächler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bolker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>S</given-names></name>. <article-title>Fitting Linear Mixed-Effects Models Using lme4</article-title>. <source>Journal of Statistical Software</source>. <year>2015</year>;<volume>67</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18637/jss.v067.i01" xlink:type="simple">10.18637/jss.v067.i01</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006903.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kuznetsova</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bruun Brockhoff</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Haubo Bojesen Christensen</surname> <given-names>R</given-names></name>. <source>lmerTest:Tests in Linear Mixed Effects Models</source>; <year>2016</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006903.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cleveland</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Devlin</surname> <given-names>SJ</given-names></name>. <article-title>Locally weighted regression: an approach to regression analysis bylocal fitting</article-title>. <source>Journal of the American statistical association</source>. <year>1988</year>;<volume>83</volume>(<issue>403</issue>):<fpage>596</fpage>–<lpage>610</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1988.10478639" xlink:type="simple">10.1080/01621459.1988.10478639</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>