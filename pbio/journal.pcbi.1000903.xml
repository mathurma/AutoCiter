<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-1688R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000903</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Animal Cognition</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Pavlovian-Instrumental Interaction in ‘Observing Behavior’</article-title><alt-title alt-title-type="running-head">‘Observing Behavior’</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Beierholm</surname><given-names>Ulrik R.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group><aff id="aff1">          <addr-line>Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">John Radcliffe Hospital, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">beierh@gatsby.ucl.ac.uk</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: PD. Analyzed the data: URB. Wrote the paper: URB PD. Performed the simulations: URB.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>9</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>9</day><month>9</month><year>2010</year></pub-date><volume>6</volume><issue>9</issue><elocation-id>e1000903</elocation-id><history>
<date date-type="received"><day>19</day><month>1</month><year>2010</year></date>
<date date-type="accepted"><day>26</day><month>7</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Beierholm, Dayan</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Subjects typically choose to be presented with stimuli that predict the existence of future reinforcements. This so-called ‘observing behavior’ is evident in many species under various experimental conditions, including if the choice is expensive, or if there is nothing that subjects can do to improve their lot with the information gained. A recent study showed that the activities of putative midbrain dopamine neurons reflect this preference for observation in a way that appears to challenge the common prediction-error interpretation of these neurons. In this paper, we provide an alternative account according to which observing behavior arises from a small, possibly Pavlovian, bias associated with the operation of working memory.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The theory of Reinforcement Learning (RL) has been influential in explaining basic learning and behavior in humans and other animals, and in accounting for key features of the activity of dopamine neurons. However, perhaps due to this very success, paradigms that challenge RL are at a premium. One case concerns so-called ‘<italic>observing behavior</italic>’, in which, at least in some versions, animals elect to observe cues that are predictive of future rewarding outcomes, although the observations themselves have no direct behavioral relevance. In a recent experiment on observing, the activity of monkey dopaminergic neurons was also found to be incompatible with classic RL. However, as is often the case, this was a task that allowed for potential interactions from a secondary behavioral system in which responses are directly triggered by values. In this paper we show that a model incorporating a next order of refinement associated with such Pavlovian interactions can explain this type of observing behavior.</p>
</abstract><funding-group><funding-statement>Funding was from the Gatsby Charitable Foundation (URB &amp; PD), <ext-link ext-link-type="uri" xlink:href="http://www.gatsby.org.uk/" xlink:type="simple">http://www.gatsby.org.uk/</ext-link>, and the Marie Curie FP7 programme (URB), <ext-link ext-link-type="uri" xlink:href="http://cordis.europa.eu/fp7/people/home_en.html" xlink:type="simple">http://cordis.europa.eu/fp7/people/home_en.html</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Animal behavior all too rarely follows the precepts of simple theories such as normatively optimal choice. Prominent examples of this arise in the florid fancies of Breland &amp; Breland's animal actors <xref ref-type="bibr" rid="pcbi.1000903-Breland1">[1]</xref>, or in the complexities of negative automaintenance or omission schedules <xref ref-type="bibr" rid="pcbi.1000903-Williams1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Dayan1">[4]</xref>. Such failures and irrationalities have been important sources of theory revision and refinement, for instance leading to suggestions about the competition and cooperation of multiple systems of control <xref ref-type="bibr" rid="pcbi.1000903-Balleine1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Dayan2">[7]</xref>, some instrumental and adaptive; others Pavlovian and hard-wired.</p>
<p>In this paper, we study one apparent departure from optimality, namely a type of ‘observing behavior’ <xref ref-type="bibr" rid="pcbi.1000903-Wyckoff1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref>, which has been the subject of a recent important electrophysiological study <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>. In brief, subjects are programmed to receive either a large or small reward, with its size being determined stochastically. When faced with the choice of finding out (by being presented with a suitably distinctive cue) sooner rather than later which of the two rewards they will ultimately receive, subjects prefer to know sooner. A lack of indifference despite the equality of the outcomes has been found to be widely true even if the knowledge cannot influence the outcome, and, at least in other experiments, even if this choice is expensive <xref ref-type="bibr" rid="pcbi.1000903-Wyckoff1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Prokasy1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Daly1">[13]</xref>. In economics, the same anomaly is referred to in terms of “temporal resolution of uncertainty” <xref ref-type="bibr" rid="pcbi.1000903-Kreps1">[14]</xref>, explained by such notions as savoring <xref ref-type="bibr" rid="pcbi.1000903-Caplin1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Lovallo1">[17]</xref>, with subjects enjoying the anticipation of good things to come.</p>
<p>The correct interpretation of this form of observing behavior has been the subject of substantial debate (see, e.g. <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref>). Superficially attractive theories, such as a desire to gain Shannon information <xref ref-type="bibr" rid="pcbi.1000903-Shannon1">[18]</xref> have been dealt fatal blows, for instance with animals preferring to observe <italic>more</italic> even when the number of bits they receive by doing so is <italic>less</italic> (e.g., as the probability of getting the large reward becomes smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e001" xlink:type="simple"/></inline-formula>, <xref ref-type="bibr" rid="pcbi.1000903-Roper1">[12]</xref>).</p>
<p>A recent study on observing behavior in macaques <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> has offered a new perspective on the problem. These authors recorded from putative dopamine neurons in the midbrain whilst monkeys chose to observe. According to a common theory, these neurons report a temporal difference error in predictions of future reward <xref ref-type="bibr" rid="pcbi.1000903-Montague1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Schultz1">[20]</xref> as in reinforcement learning accounts of optimal instrumental choice <xref ref-type="bibr" rid="pcbi.1000903-Sutton1">[21]</xref>. Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> showed that: (a) the macaques did observe; and furthermore (b) the activity of dopamine neurons was associated with the choice they make. However, although the behavior and activity are mutually consistent, observing behavior offers no instrumental benefit and therefore it should also not be associated with any prediction errors. Bromberg-Martin and Hikosaka suggested that this means that the dopamine cells are reporting on some aspects of the benefit of information gathering in addition to aspects of reward.</p>
<p>In this paper, we examine the extent to which this form of observing behavior can be explained by temporal difference learning, coupled with the same mechanism that provides an account of a wide range of departures from normative choice, namely a Pavlovian influence over instrumental actions <xref ref-type="bibr" rid="pcbi.1000903-Dayan1">[4]</xref>. In particular, we assume that subjects only make associative predictions when they are appropriately engaged in the task. If the level of this engagement is influenced by the size of the predictions (the putatively Pavlovian effect), then stimuli predicting certain or deterministic large future rewards (one outcome of an observing choice) will lead to more engagement than stimuli that leave uncertain the magnitude of the future rewards. This idea can be seen as a realization of the suggestion made by Dinsmoor <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref> that the predictions of future reward associated with stimuli influence the attention paid to them. We show that occasional failures of engagement, modeled as a breakdown in the working memory for the representational state, can lead directly to both the preference for observing and the apparently anomalous dopamine activity, without need for any reference to ‘information’. We also examine the various factors that control the strength of observing in this model.</p>
</sec><sec id="s2">
<title>Results</title>
<p>Bromberg-Martin and Hikosaka's experiment (see <xref ref-type="sec" rid="s4">Methods</xref> and <xref ref-type="fig" rid="pcbi-1000903-g001">Figure 1</xref>) involved the most precise conditions for establishing observing behavior. On each trial, thirsty subjects had a 50% chance of receiving a small or large volume of water directly into their mouths. There were three sorts of trials: forced-information, forced-random and free choice. On forced-information trials, the subjects were presented with a single target (C<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e002" xlink:type="simple"/></inline-formula>; just an orange square in the figure) and, after looking at it, would receive one of two cues (S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e003" xlink:type="simple"/></inline-formula>; an orange ‘+’, or S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e004" xlink:type="simple"/></inline-formula>; an orange ‘<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e005" xlink:type="simple"/></inline-formula>’) according to the volume they were to receive in a couple of seconds. On forced-random trials, looking at the single target (C<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e006" xlink:type="simple"/></inline-formula>; green square) led again to one of two cues (S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e007" xlink:type="simple"/></inline-formula>; green ‘*’, or S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e008" xlink:type="simple"/></inline-formula>; green ‘o’). However, either of these could be followed by either small or large rewards; and thus they provided no discriminative information about the forthcoming reward. Finally, on free choice trials, both orange and green targets were provided, and the subjects could choose whether to receive the discriminative (orange) or non-discriminative (green) cues.</p>
<fig id="pcbi-1000903-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g001</object-id><label>Figure 1</label><caption>
<title>Experimental setup for a free-choice trial, similar to Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>.</title>
<p>The monkey performs its choice (C<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e009" xlink:type="simple"/></inline-formula> or C<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e010" xlink:type="simple"/></inline-formula>) according to color, and the discriminating/random stimulus is presented. At the end of the trial either a large (1 ml) or tiny (0.04 ml) amount of water is delivered.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g001" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1000903-g002">Figures 2a;b</xref> show primary behavioral results from the study for two subjects – both gradually expressed a bias towards the discriminative (orange) option in the free-choice trials. As Bromberg-Martin and Hikosaka stressed, under a standard associative learning or temporal difference scheme, there is no difference between the expected reward for the discriminating and non-discriminating option, and so no reason to expect this strong and enduring preference.</p>
<fig id="pcbi-1000903-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g002</object-id><label>Figure 2</label><caption>
<title>Comparing observing in monkeys and the model.</title>
<p>a–b) Observing in two monkeys performing the task, from Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>. The dotted lines correspond to the Clopper-Pearson 95 percent confidence interval. c–d) Two examples of observing produced by the model. The parameters for the two plots differ only by the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e011" xlink:type="simple"/></inline-formula>, the inverse temperature in the softmax. Each session is 480 trials in the simulations (160 choice trials).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g002" xlink:type="simple"/></fig>
<p>We built a model of this which, with one critical exception that we discuss below, involves a standard temporal difference learning algorithm <xref ref-type="bibr" rid="pcbi.1000903-Sutton1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Sutton2">[22]</xref>. Forced-choice and free-choice trials permit learning about the future expected rewards associated with the various targets and stimuli, training the values of the states. Then, on free-choice trials, the selection depends on the relative values, via a softmax function (see <xref ref-type="sec" rid="s4">methods</xref>). <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2c;d</xref> shows the results from simulations of our model, with parameters chosen to match Bromberg-Martin and Hikosaka's two subjects. The model closely matches qualitative features of the monkeys' performances.</p>
<p>In standard models such as this, in which there is a delay between the presentation of cues and the rewards that they predict, an assumption has to be made about the way that the subjects maintain knowledge about their state in the task, and indeed keep time. Many different possibilities have been explored, from delay lines to complex patterns of activity evolving in dynamical recurrent networks (e.g., <xref ref-type="bibr" rid="pcbi.1000903-Sutton3">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Mauk1">[28]</xref>). All of these amount to forms of working memory – and so present the minimal requirement that the subjects continue to be engaged in the task throughout the delay in sufficiently intense a manner as to maintain this ongoing memory. Thus the critical exception to conventional temporal difference learning in our model is to assume that this maintained engagement is influenced by the current predicted value. That is, if the value is high, then engagement is readily maintained; if the value is low, then engagement can be weakened or lost.</p>
<p>Losing engagement is detrimental to the subject in the context of the present task; by analogy with a similarly detrimental effect in negative automaintenance, we consider it a form of Pavlovian misbehavior <xref ref-type="bibr" rid="pcbi.1000903-Dayan1">[4]</xref>. Pavlovian responses are typically elicited in an automatic manner based on appetitive or aversive predictions, and can exert benign or malign influences over the achievement of subjects' apparent goals. Normally, such responses are overt behaviors; here, along with several recent studies <xref ref-type="bibr" rid="pcbi.1000903-OReilly1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Frank1">[30]</xref>, we consider internal responses, associated with the operation of working memory. Mechanistically, these could come, for instance, from the influence dopamine itself exerts on the processes concerned <xref ref-type="bibr" rid="pcbi.1000903-Williams2">[31]</xref>.</p>
<p>In the model, we consider engagement to be lost completely on some trials as a stochastic function of the evolving predicted value. Such losses have the effect of decreasing the subjective value of cues and states associated with lower values below their objective worth; in particular exerting a negative bias on the non-discriminative cues (S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e012" xlink:type="simple"/></inline-formula>; S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e013" xlink:type="simple"/></inline-formula>) compared with the discriminative cue associated with the large reward (S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e014" xlink:type="simple"/></inline-formula>), which will more rarely experience such losses. <xref ref-type="fig" rid="pcbi-1000903-g003">Figure 3</xref> shows the effective probability of disengagement at different timepoints as well as showing the effect this has on the expected reward. Disengagement associated with S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e015" xlink:type="simple"/></inline-formula> is benign, since the outcome on those trials is modelled as being close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e016" xlink:type="simple"/></inline-formula> in any case. Altogether, this creates a bias towards choosing the discriminative option on free-choice trials, as is evident in <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2c;d</xref>.</p>
<fig id="pcbi-1000903-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g003</object-id><label>Figure 3</label><caption>
<title>The mechanics of the model.</title>
<p>a) The probability of disengagement at different timepoints for the task in <xref ref-type="fig" rid="pcbi-1000903-g001">Figure 1</xref> (conditional on having not disengaged at prior timesteps). Similar color convention as in <xref ref-type="fig" rid="pcbi-1000903-g001">Figure 1</xref>. Orange traces are for discriminative trials; green for non-discriminative ones; solid lines for the larger reward or one of the two non-discriminative cues; dashed lines for the smaller reward. b) The total probability of having disengaged by the time of reaching state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e017" xlink:type="simple"/></inline-formula>. c) The expected reward, V, at different timepoints for the TD model with disengagement. For comparison, the expected reward for a traditional TD model without disengagement is shown in transparent colors. Notice that although the chance of disengagement is high for S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e018" xlink:type="simple"/></inline-formula>, it has little effect due to the already low value of this state. By contrast, the moderate engagement for S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e019" xlink:type="simple"/></inline-formula> and S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e020" xlink:type="simple"/></inline-formula> has a larger effect due to their higher associated value.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g003" xlink:type="simple"/></fig>
<p>The difference between the parameters for <xref ref-type="fig" rid="pcbi-1000903-g002">Figures 2c;d</xref> is in the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e021" xlink:type="simple"/></inline-formula> governing the strength of the competition in the softmax (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e022" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e023" xlink:type="simple"/></inline-formula> for <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2c;d</xref> respectively). Monkey V's results are consistent with a larger value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e024" xlink:type="simple"/></inline-formula> than monkey Z; smaller <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e025" xlink:type="simple"/></inline-formula> leads to more stochasticity and a lower overall degree of preference. The asymptotic preference for observing is monotonic in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e026" xlink:type="simple"/></inline-formula>.</p>
<p>Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> also recorded the activity of putative midbrain dopaminergic cells during the performance of the task. <xref ref-type="fig" rid="pcbi-1000903-g004">Figure 4a</xref> shows the activity of an example neuron in the various conditions. The population response is similar (<xref ref-type="fig" rid="pcbi-1000903-g004">Figure 4</xref> of <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>) albeit, as has often been seen, with an initial brief activation to the forced choice non-discriminative case, likely because of generalization <xref ref-type="bibr" rid="pcbi.1000903-Tobler1">[32]</xref>. Firing at the time of the discriminative or non-discriminative cues (marked ‘cue’) and the delivery or non-delivery of reward (‘reward’) is just as expected from the standard interpretation of these neurons, i.e., that they report the temporal difference prediction error in the delivery of future reward <xref ref-type="bibr" rid="pcbi.1000903-Montague1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Schultz1">[20]</xref>.</p>
<fig id="pcbi-1000903-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g004</object-id><label>Figure 4</label><caption>
<title>Comparison of neuronal firing and the modeled TD signal.</title>
<p>a) An example of the firing rate of a single dopamine neuron during forced trials, based on data from <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>. The various trial types are marked on the plot; briefly orange traces are for discriminative trials; green for non-discriminative ones; solid lines for the larger reward, when known (or one of the two non-discriminative cues); dashed lines for the smaller reward (or the other non-discriminative cue). b) The modeled average TD signal at different time points in a trial using the same conventions as in (a). In order to facilitate the visual comparison of model and data in this figure, we truncated the negative part of the modeled TD signal at 25% of the maximal positive response of the neuron.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g004" xlink:type="simple"/></fig>
<p>However, it is their activity at the time of the targets indicating the forced-informative or forced-random trials (marked ‘target’) that is revealing about observing. The target indicating a forced-informative trial was associated with a small but significant phasic increase in activity; whereas that indicating the random cues was followed by a small decrease in the firing rate. Under the temporal difference interpretation of the neurons, this is consistent with the preference exhibited by the monkeys, but not with the objective value of the options.</p>
<p><xref ref-type="fig" rid="pcbi-1000903-g004">Figure 4b</xref> shows modelled dopamine activity in the variable engagement temporal difference model (here, negative prediction errors have been compressed compared with positive ones, see <xref ref-type="sec" rid="s4">methods</xref>; <xref ref-type="bibr" rid="pcbi.1000903-Fiorillo1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Bayer1">[34]</xref>). This shows exactly the same pattern shown in the monkey data. Note that, once the subject has learned the associations and learned the preference for choosing the discriminative option in the free choice trials, these trials will overall be more frequent than the forced-random trials, and so the negative prediction error associated with the latter will be larger than the positive prediction error associated with the former.</p>
<p><xref ref-type="fig" rid="pcbi-1000903-g005">Figure 5</xref> decomposes the modelled responses in the cases that there is successful and failed engagement between cues and reward or non-reward. The most significant effect of the complete failure to engage given an non-discriminative cue, is that if the large reward is provided, then there is a greater response than expected from a 50% prediction. The possibility of using this to test the theory is discussed below.</p>
<fig id="pcbi-1000903-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g005</object-id><label>Figure 5</label><caption>
<title>An illustrative example of the modeled temporal difference signal for each of the four conditions.</title>
<p>The coloured line indicates the regular temporal difference term, with the following color convention: orange represents a discriminating choice, green is the non-discriminating option, while the complete line is for a rewarding trial, the dotted line for a non rewarding trial. The vertically off-set black line represents the temporal difference signal for a failure after the time of the revealing (indicated by the black dotted line) of the stimulus due to Pavlovian dis-engagement. Notice that the dis-engagement is an unlikely event that relatively rarely elicits a dip in the TD signal, whereas, e.g., the delivery of an unexpected reward elicits the typically robust response.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g005" xlink:type="simple"/></fig>
<p>In a version of the task that involved choice between immediate or delayed information about upcoming rewards, Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> further showed that switching the colors of the cues without warning led to a slow reversal of the observing choice (<xref ref-type="fig" rid="pcbi-1000903-g006">Figure 6a;b</xref>). <xref ref-type="fig" rid="pcbi-1000903-g006">Figure 6c;d</xref> shows the same for the model using identical softmax parameters to those in <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2c;d</xref>. The switch in preference evolves at a similarly glacial pace.</p>
<fig id="pcbi-1000903-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g006</object-id><label>Figure 6</label><caption>
<title>Comparison of observing in monkeys and the model for a delayed task.</title>
<p>a–b) The biases of two monkeys performing a version of the observing task in which they were given the choice of receiving immediate or delayed discriminating stimuli, from Bromberg-Martin and Hikosaka. The colors of the choices switched in the session number indicated in the graph. The dotted lines correspond to the Clopper-Pearson 95 percent confidence interval. c–d) Two examples of biasing in switching, similar to Bromberg-Martin and Hikosaka. The parameters for the two plots differ only by the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e027" xlink:type="simple"/></inline-formula> in the softmax (same values as in <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2c;d</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g006" xlink:type="simple"/></fig>
<p>Various other features of observing can be examined through the medium of the model. <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7a;b</xref> show the consequence of the reinforcing outcome being aversive (e.g., an electric shock) rather than appetitive. One key question in this case is whether failure to engage is controlled more by salience or valence. <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7a</xref> shows the former case, for which a prediction of a large punishment also protects engagement (symmetrically with reward; inset plot). In this case, subjects prefer the random to the discriminative cues, since disengagement leads to subjective preference. Such preference for random cues might also come from adding a fixed value to all the potential rewards, thus allowing the moderately large disengagement in S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e028" xlink:type="simple"/></inline-formula> to have a subtractive value on its expected values (Bromberg-Martin, personal communication, 2010). However such an effect would likely be small.</p>
<fig id="pcbi-1000903-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000903.g007</object-id><label>Figure 7</label><caption>
<title>Effect of varying parameters in the model.</title>
<p>a–b) For aversive stimuli (punishment) the shape of the memory retention as a function of the expected value has a large effect on the bias towards observing. A symmetric function (a) leads to less observing, an asymmetric function (b) leads to more observing. The dotted lines indicates the Clopper-Pearson 95 percent confidence interval. c) Given appetitive stimuli, the rate of reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e029" xlink:type="simple"/></inline-formula> can have different effects on the tendency to choose the discriminating option, based on the version of softmax used. The iterative solution to the self-consistency requirement (6) using the softmax from Eq. 5 (black) is plotted, as well as the iterative solution for choices based on the logarithm of the learned value (red). Mean choice bias for Monte carlo simulations (with STD) are overlaid for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e030" xlink:type="simple"/></inline-formula>. d) Given initial starting conditions far from the correct values, initial learning can lead to too strong or weak effects (single runs; initial values of all states are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e031" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e033" xlink:type="simple"/></inline-formula> for the blue and red curves respectively).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.g007" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7b</xref> shows the case in which valence (from appetitive to aversive) determines disengagement, with predictions of punishments leading to more failures of engagement than small rewards. This again supports observing behavior. Unfortunately, experimental tests of the case involving punishment <xref ref-type="bibr" rid="pcbi.1000903-Badia1">[35]</xref> have not enjoyed the precision of the paradigm adopted by Bromberg-Martin and Hikosaka, leaving open the question as to which of these patterns arises.</p>
<p>Another important experimental manipulation has been to vary the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e034" xlink:type="simple"/></inline-formula> of the larger versus the smaller reward. As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e035" xlink:type="simple"/></inline-formula> decreases from 1 towards 0.5 there is an increase in the observing bias (i.e., a greater tendency to choose the discriminative option). Below this, the nature of the bias depends on the assumption about how the choices are generated. A choice rule that depends on the difference in expected values (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e036" xlink:type="simple"/></inline-formula>) leads to a bias that ultimately decreases towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e037" xlink:type="simple"/></inline-formula> as these values themselves decrease towards <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e038" xlink:type="simple"/></inline-formula>. However, the bias is asymmetric about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e039" xlink:type="simple"/></inline-formula> (black curve in <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7c</xref>). If, instead, the choices are based on the ratio of the values (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e040" xlink:type="simple"/></inline-formula>), the choice bias can continue to increase as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e041" xlink:type="simple"/></inline-formula> approaches <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e042" xlink:type="simple"/></inline-formula> (red curve). Just such an increase in observing was shown by Roper and Zentall <xref ref-type="bibr" rid="pcbi.1000903-Roper1">[12]</xref> as reward schedules thinned. While some studies have also manipulated the size of the reward <xref ref-type="bibr" rid="pcbi.1000903-Mitchell1">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Daly2">[38]</xref>, our model does not make any direct predictions about this. It is possible that adaptation would scale the response to the overall sizes of available rewards (as indeed found for phasic dopamine activity in <xref ref-type="bibr" rid="pcbi.1000903-Tobler2">[39]</xref>), and the metrics of this would have to be known in order to make predictions about disengagement.</p>
<p>One extra factor that is important for analysing behavior is that the biases inherent in disengagement are small and develop over a long time-scale, consistent with the stately progress evident in <xref ref-type="fig" rid="pcbi-1000903-g002">Figure 2</xref>. However, this means that the initial course of learning can be subject to significant influence from the initial values ascribed to the different options, leading to biases that are incommensurate with the final, long term, state. <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7d</xref> shows an example. For the blue curve, the initial values of all states are low (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e043" xlink:type="simple"/></inline-formula>), but the probability of a reward is high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e044" xlink:type="simple"/></inline-formula>); for the red curve, the initial values are high (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e045" xlink:type="simple"/></inline-formula>), but the probability of a reward is low (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e046" xlink:type="simple"/></inline-formula>). In the former case, there is substantial initial over-observation; in the latter, initial under-observation.</p>
</sec><sec id="s3">
<title>Discussion</title>
<p>We have provided an account of ‘observing behavior’ that shows how it can arise from a small Pavlovian bias over instrumental behavior associated with disengagement from a task, rather than any aspect of information seeking. Pavlovian biases are rife in decision-making; and accommodating them does not necessitate any further change to the standard underlying theory of the activity of dopaminergic neurons that has not already been suggested to accommodate other data. What we have done here is specify the shape of such an interaction based on disengagement in the task. We intended specifically to capture <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> experiment on macaques. However our results do touch upon other, but emphatically not all, instances of observing in the literature.</p>
<p>Experiments such as <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> into observing are designed to maximize the effects of what is a relatively small anomaly in decision making (compared, for instance, with the more extreme misbehavior evident in negative automaintenance <xref ref-type="bibr" rid="pcbi.1000903-Williams1">[2]</xref> or the schedule task <xref ref-type="bibr" rid="pcbi.1000903-Shidara1">[40]</xref>). Indeed, in this case, the subjects did not have to pay a penalty for observing. Thus, under standard decision-making conditions, we may expect the net effect of disengagement to be modest, leaving near-optimal behavior within the scope of the model.</p>
<p>Dinsmoor <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref> suggested an account of the phenomenon based on his observation of ‘selective observing’, i.e., that the subjects would preferentially focus on stimuli associated with higher probabilities of reward. This idea met some resistance (some of which is contained in the commentary to <xref ref-type="bibr" rid="pcbi.1000903-Dinsmoor1">[9]</xref>), partly based on experimental tests in which the subjects were not able to avoid the low value predictive cues. Our account can be seen as a form of selective observing, but involving internal actions associated with the allocation of engagement and attention, rather than external actions involving preferential looking. It might seem that these accounts are close to Mackintosh's <xref ref-type="bibr" rid="pcbi.1000903-Mackintosh1">[41]</xref> suggestion that attention is preferentially paid to stimuli that are strong predictors of affectively important outcomes. However, in Mackintosh's account, attention particularly influences the speed of learning (the associability of the stimulus) rather than the fact of it (at least in the absence of competing predictors), and so would not have the asymptotic effect that is apparent in the experiments we have discussed.</p>
<p>Another interesting account of observing is Daly and Daly's DMOD <xref ref-type="bibr" rid="pcbi.1000903-Daly3">[42]</xref>, which learns predictions associated with frustration (when reward is expected, but does not arrive), and courage (when reward is actually delivered during a state of frustration). These extra predictions warp the net expected values associated with the different cases in observing, favoring observing responses. The theory underlying DMOD is the original Rescorla-Wagner <xref ref-type="bibr" rid="pcbi.1000903-Rescorla1">[43]</xref> version of the delta rule <xref ref-type="bibr" rid="pcbi.1000903-Widrow1">[44]</xref>, whose substantial modification by Sutton and Barto <xref ref-type="bibr" rid="pcbi.1000903-Sutton4">[45]</xref> to account for secondary conditioning led to the original prediction error treatment of the activity of dopamine neurons in appetitive conditioning <xref ref-type="bibr" rid="pcbi.1000903-Montague1">[19]</xref>. It would be necessary to extend DMOD in a similar way, and to make an assumption about which of its three prediction errors (or other quantities) are reflected in the activity of dopamine neurons, in order to determine its match to the neurophysiological data. The failure of TD models to capture behavioral aspects of frustration is, however, notable.</p>
<p>To some tastes, the most theoretically appealing accounts of observing start from the notion that animals seek to acquire information about the world <xref ref-type="bibr" rid="pcbi.1000903-Berlyne1">[46]</xref>. However, formal informational theories have difficulty with the results of reducing the probability of reward (<xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7c</xref>; <xref ref-type="bibr" rid="pcbi.1000903-Roper1">[12]</xref>), which reduce the uncertainty and the information gained, but increase observing. More informal theories, such as that suggested by <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref> require more precise specification to be tested against accounts such as the one here. The sloth of initial learning and reversal apparent in <xref ref-type="fig" rid="pcbi-1000903-g006">Figure 6</xref> (taking 1200–2400 choice trials, 3000–7000 trials overall) might be considered suggestive evidence against an informational account, since it implies at the very least a nugatory value for the information.</p>
<p>In terms of our account, there are various routes by which predicted values could influence persistent engagement. Failure to engage can be seen as the same sort of malign Pavlovian influence over behavior that is implicated in the poor performance of monkeys in tasks in which they know themselves to be several steps away from reward <xref ref-type="bibr" rid="pcbi.1000903-Shidara1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Dayan3">[47]</xref>. In that paradigm, it is an explicitly informative cue that the reward is disappointingly far away that leads to disengagement; this parallels the disappointment associated with the non-discriminative cue in observing. The most obvious mechanism associated with engagement is the influence of dopamine itself over working memory <xref ref-type="bibr" rid="pcbi.1000903-Williams2">[31]</xref>; however, whether this is the phasic dopamine signal associated with prediction errors for reward <xref ref-type="bibr" rid="pcbi.1000903-Montague1">[19]</xref> or a more tonic dopamine signal associated with a longer term average reward rate <xref ref-type="bibr" rid="pcbi.1000903-Niv1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Niv2">[49]</xref> is not clear. Alternatively, some theories suggest that working memory is controlled by a gating process <xref ref-type="bibr" rid="pcbi.1000903-OReilly1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Frank1">[30]</xref> associated with the basal ganglia, treating internally- and externally directed action in a uniform manner. Dopamine certainly influences the vigor associated with external actions <xref ref-type="bibr" rid="pcbi.1000903-Niv1">[48]</xref>–<xref ref-type="bibr" rid="pcbi.1000903-Salamone1">[50]</xref>; it is therefore reasonable to assume that it might also influence internal engagement.</p>
<p>We specialized our description of the model to the particulars of the experiment conducted by Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>. The most important question for other cases concerns the conditions under which re-engagement occurs. Since disengagement is seemingly rather rare, it is hard to get many hints from this experiment, and we might assume that it is reward delivery itself that causes re-engagement. However in a more general setup (e.g. without reward delivery at fixed time points), a mechanism for re-engagement is necessary. One possible way to do that would be by stochastically re-engaging based on either the reward prediction error or expected value. Such a mechanism of re-engagement could happen at any time point but would be extremely likely to happen at the delivery of reward, as well as for the initiation of a new trial. To be fully generalizable we also need to specify the case for disengagement at the time of an action selection. While in a disengaged state we envision the animal not performing an explicit choice, thus potentially not responding within an allocated time. If a choice is required to progress in the behavioral setup it would happen after an eventual re-engagement.</p>
<p>The model raises some further questions. First, we assumed that the probability of disengagement is a function of the actual prediction. However, it is possible that this function scales with the overall magnitude or scale of possible rewards, making the degree of observing relative rather than absolute. There is a report that phasic dopamine itself scales in an adaptive manner <xref ref-type="bibr" rid="pcbi.1000903-Tobler2">[39]</xref>, , and this would be a natural substrate.</p>
<p>A second issue is whether disengagement is occasioned by the change in predictions associated with the phasic dopamine activity, or the level of the prediction itself. If the former, then in tasks such as the one studied by Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>, where substantial prediction errors only happen with phasic targets and cues, the state could, for instance, just be poorly established in working memory at the outset, because of a weak dopamine signal, and this could lead to a subsequent chance of disengagement. We adopted the simpler scheme in which it is the ongoing predictive value that controls the chance of disengagement. One experiment that hints in the direction of change is that of Spetch et al. <xref ref-type="bibr" rid="pcbi.1000903-Spetch1">[52]</xref> (for a more recent study see <xref ref-type="bibr" rid="pcbi.1000903-Gipson1">[53]</xref>). In this, pigeons were given the choice between a certain (100%) or uncertain (50%, but observed) reward. Surprisingly, the level of engagement to the latter (measured by the number of pecks to the illuminated key) was many times to that of the former, and the pigeons duly made the suboptimal choice. The model presented in this paper does tie engagement to choice in a similar way, but we would be unable to explain such a strong effect. A variant of the model for which engagement is governed by prediction errors rather than predictions would show some contrast effect that could favor the uncertain, but observed, reward. However, it would be hard to explain such a stark contrast.</p>
<p>A third issue is whether disengagement is complete (and stochastic), or partial (and, at least possibly, deterministic). We considered the former case, and indeed, this leads to a straightforward prediction that the histogram of the dopamine response at the time of a delivered reward in the non-discriminative case might have two peaks; one associated with continuing engagement to the point of reward; the other, which would be roughly twice as high, associated with prior disengagement. However, it is also possible that less dramatic changes in engagement occur during the interval between cues and reward. If many individual neural elements are involved in the engagement (for instance in working memory circuits devoted to timing), then some could disengage before others. This might even lead to a non-uniform behavior among different dopamine cells. Unfortunately, the low firing rates of these cells make it hard to discriminate between these various possibilities.</p>
<p>Finally, the question arises as to the computational rationale for value-dependent disengagement. Other instances of Pavlovian misbehavior, such as withdrawal from cues associated with predictions of low values, can find plausible justifications in terms of evolutionary optimality. Disengagement might be seen in the same way, as a Pavlovian spur to exploration <xref ref-type="bibr" rid="pcbi.1000903-AstonJones1">[54]</xref> in the face of poor expected returns.</p>
<p>From the perspective of conditioned reinforcement, our account suggests that the issue that is often studied is not really the one that is critical. Various investigators (see, for instance, the ample discussion in Lieberman et al. 1997 <xref ref-type="bibr" rid="pcbi.1000903-Lieberman1">[55]</xref> about the differences between their findings and those of Fantino and Case 1983 <xref ref-type="bibr" rid="pcbi.1000903-Fantino1">[56]</xref>) have considered whether stimuli like S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e047" xlink:type="simple"/></inline-formula> are conditioned reinforcers because of their association with the reward. For us, S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e048" xlink:type="simple"/></inline-formula> and S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e049" xlink:type="simple"/></inline-formula> and S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e050" xlink:type="simple"/></inline-formula> are all conditioned reinforcers. The key question for observing behavior is instead an apparent concavity: the average worth of two different stimuli associated deterministically with small and large rewards is greater than the worth of a single stimulus associated stochastically with the same outcome statistics (see <xref ref-type="bibr" rid="pcbi.1000903-Wyckoff2">[57]</xref>). It is this non-linearity that demands explanation, and not merely the fact, for instance, of savoring or anticipation of the future reward, which could quite reasonably also be purely linear. Some accounts put the weight of the non-linearity onto the stimulus associated surely with the large reward. By comparison, our account places this emphasis onto the non-discriminative stimuli, suggesting that they are more likely to lead to disengagement. The same is true of other sources of non-linearity, for instance a mechanism that accumulates distress from the prolonged variance/uncertainty in the non-discriminative pathway.</p>
<p>Various versions of the ‘observing task’ have also been tested on humans <xref ref-type="bibr" rid="pcbi.1000903-Lieberman1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Fantino1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Perone1">[58]</xref>. These studies have shown consistent observing behavior, but, partly because of the different reading of the issue of conditioned reinforcement to the one discussed above, have often focused on different questions and methods from those in Bromberg-Martin and Hikosaka <xref ref-type="bibr" rid="pcbi.1000903-BrombergMartin1">[10]</xref>. For instance, one question has been whether subjects would observe if they only ever found out S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e051" xlink:type="simple"/></inline-formula> and never S<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e052" xlink:type="simple"/></inline-formula> – the idea being that conditioned reinforcement could support observing of the latter but not the former. Unfortunately, the answers have been confusing <xref ref-type="bibr" rid="pcbi.1000903-Lieberman1">[55]</xref>, perhaps partly because of issues about how cognitive effects (e.g., expectations of controllability) influence the results. Note, in particular, that we have only modeled observing behavior associated with repeated experience and learning, and not the sort of single-instance decisions that are often used in human cases.</p>
<p>In conclusion we have shown that the often observed effect of ‘observing’, preferring a behaviorally irrelevant discriminating stimulus cue, can readily be explained by a bias caused by Pavlovian misbehavior, putting it in the same category as a range of other suboptimalities. Informational accounts, however seductive, are not necessary.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<p>We model value learning using a modified version of a standard temporal difference model <xref ref-type="bibr" rid="pcbi.1000903-Sutton1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1000903-Sutton2">[22]</xref>. We assume the task can be specified as a Markov process, where the participant estimates the expected long run future reward (value) of each state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e053" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e054" xlink:type="simple"/></inline-formula>, updating it according to<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e055" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e056" xlink:type="simple"/></inline-formula> is the learning rate, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e057" xlink:type="simple"/></inline-formula> is the change in expected value given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e058" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e059" xlink:type="simple"/></inline-formula> is the delivered reward, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e060" xlink:type="simple"/></inline-formula> is the state that follows <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e061" xlink:type="simple"/></inline-formula>. Learning proceeds for all three sorts of trials (forced disc., forced non-disc. and choice trials). The modelled dopamine signal for <xref ref-type="fig" rid="pcbi-1000903-g004">Figures 4</xref> and <xref ref-type="fig" rid="pcbi-1000903-g005">5</xref> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e062" xlink:type="simple"/></inline-formula>.</p>
<p>The only deviation from the standard TD model is in assuming that the correct updating of this system is dependent on maintaining engagement, for instance in working memory. We assume the probability of disengagement of the course of state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e063" xlink:type="simple"/></inline-formula> to be<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e064" xlink:type="simple"/><label>(3)</label></disp-formula>per unit of time (in seconds). Hence, for a given state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e065" xlink:type="simple"/></inline-formula> the probability of a correct updating is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e066" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e067" xlink:type="simple"/></inline-formula> is the amount of time spent in the state (see <xref ref-type="fig" rid="pcbi-1000903-g001">Figure 1</xref>). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e069" xlink:type="simple"/></inline-formula> are fixed parameters. We assume the consequence of disengagement to be the transition to a specific fixed (non-updating) state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e070" xlink:type="simple"/></inline-formula> of value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e071" xlink:type="simple"/></inline-formula> and hence the updating signal for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e072" xlink:type="simple"/></inline-formula> is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e073" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>The system stays in this state, until a reward is delivered at the end of the trial. At this point the system is ‘re-engaged’ creating a TD error relative to the fixed state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e074" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1000903-g005">Figure 5</xref>). We assume that any potential disengagement in the intertrial interval is negated by the initiation of a new trial.</p>
<p>Choice is only possible at one state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e075" xlink:type="simple"/></inline-formula>, between progressing to either state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e076" xlink:type="simple"/></inline-formula> and state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e077" xlink:type="simple"/></inline-formula>. Given the learned values, we assume the subject performs choice <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e078" xlink:type="simple"/></inline-formula> based on the Softmax or Luce choice rule <xref ref-type="bibr" rid="pcbi.1000903-Luce1">[59]</xref><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e079" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>Note that it is straightforward to see that this version of softmax is dependent on the difference in values (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e080" xlink:type="simple"/></inline-formula>), whereas using the logarithm of the value (as in <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7c</xref>) causes the function to be dependent on the ratio of values (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e081" xlink:type="simple"/></inline-formula>).</p>
<p>In the limit without any failures in updating the learned values would approach the true value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e082" xlink:type="simple"/></inline-formula>, where the expectation is taken over states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e083" xlink:type="simple"/></inline-formula>. However with a chance of failure <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e084" xlink:type="simple"/></inline-formula> dependent on the value, the iterative solution in <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7c</xref> can be given by solving<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e085" xlink:type="simple"/><label>(6)</label></disp-formula>numerically.</p>
<p>For all figures we assumed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e087" xlink:type="simple"/></inline-formula>. For <xref ref-type="fig" rid="pcbi-1000903-g002">Figs. 2</xref> and <xref ref-type="fig" rid="pcbi-1000903-g006">6</xref> we used parameters, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e088" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e089" xlink:type="simple"/></inline-formula>. For the aversive stimuli in <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7a–b</xref> we assumed negative reward values. For <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7a</xref> the parameters were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e090" xlink:type="simple"/></inline-formula>. For <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7b</xref> the parameters were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e091" xlink:type="simple"/></inline-formula>. For <xref ref-type="fig" rid="pcbi-1000903-g007">Figure 7d</xref> the parameters were <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000903.e092" xlink:type="simple"/></inline-formula>. To mimic the fact that dopamine neurons have less dynamic range for increases than decreases in firing rate, for <xref ref-type="fig" rid="pcbi-1000903-g004">Figure 4</xref> we truncated the negative responses at −25 percent of the maximal positive response of the neuron.</p>
</sec></body>
<back>
<ack>
<p>We are very grateful to Ethan Bromberg-Martin and Okihide Hikosaka for freely sharing their ideas, thoughts and data, to EB-M for the suggestion about the relationship between Pavlovian disengagement and selective observing and an anonymous reviewer for further advice. We also thank Marc Guitart Masip for helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000903-Breland1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Breland</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Breland</surname><given-names>M</given-names></name>
</person-group>             <year>1961</year>             <article-title>The misbehavior of organisms.</article-title>             <source>Am Psychol</source>             <volume>16</volume>             <fpage>681</fpage>             <lpage>84</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Williams1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Williams</surname><given-names>DR</given-names></name>
<name name-style="western"><surname>Williams</surname><given-names>H</given-names></name>
</person-group>             <year>1969</year>             <article-title>Auto-maintenance in the pigeon: sustained pecking despite contingent non-reinforcement.</article-title>             <source>J Exp Anal Behav</source>             <volume>12</volume>             <fpage>511</fpage>             <lpage>520</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Sheffield1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sheffield</surname><given-names>F</given-names></name>
</person-group>             <year>1965</year>             <article-title>Relation between classical conditioning and instrumental learning.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Prokasy</surname><given-names>W</given-names></name>
</person-group>             <source>Classical conditioning</source>             <publisher-loc>New York, NY</publisher-loc>             <publisher-name>Appelton-Century-Crofts</publisher-name>             <fpage>302</fpage>             <lpage>322</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Dayan1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>
<name name-style="western"><surname>D Daw</surname><given-names>N</given-names></name>
</person-group>             <year>2006</year>             <article-title>The misbehavior of value and the discipline of the will.</article-title>             <source>Neural Netw</source>             <volume>19</volume>             <fpage>1153</fpage>             <lpage>1160</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Balleine1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Balleine</surname><given-names>B</given-names></name>
</person-group>             <year>2005</year>             <article-title>Neural bases of food-seeking: Affect arousal and reward in corticostriatolimbic circuits.</article-title>             <source>Physiol Behav</source>             <volume>86</volume>             <fpage>717</fpage>             <lpage>730</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Daw1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daw</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2005</year>             <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control.</article-title>             <source>Nat Neurosci</source>             <volume>8</volume>             <fpage>1704</fpage>             <lpage>1711</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Dayan2"><label>7</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2008</year>             <article-title>The role of value systems in decision-making.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Engel</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>
</person-group>             <source>Better than Conscious</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>Ernst Strüngmann Forum, MIT Press</publisher-name>             <fpage>51</fpage>             <lpage>70</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Wyckoff1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wyckoff</surname><given-names>LB</given-names></name>
</person-group>             <year>1952</year>             <article-title>The role of observing responses in discrimination learning. Part I.</article-title>             <source>Psychol Rev</source>             <volume>59</volume>             <fpage>431</fpage>             <lpage>442</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Dinsmoor1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dinsmoor</surname><given-names>J</given-names></name>
</person-group>             <year>1983</year>             <article-title>Observing and conditioned reinforcement.</article-title>             <source>Behav Brain Sci</source>             <volume>6</volume>             <fpage>693</fpage>             <lpage>728</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-BrombergMartin1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bromberg-Martin</surname><given-names>ES</given-names></name>
<name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>
</person-group>             <year>2009</year>             <article-title>Midbrain dopamine neurons signal preference for advance information about upcoming rewards.</article-title>             <source>Neuron</source>             <volume>63</volume>             <fpage>119</fpage>             <lpage>126</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Prokasy1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Prokasy</surname><given-names>W</given-names></name>
</person-group>             <year>1956</year>             <article-title>The acquisition of observing responses in the absence of differential external reinforcement.</article-title>             <source>J Comp Physiol Psychol</source>             <volume>49</volume>             <fpage>131</fpage>             <lpage>134</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Roper1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Roper</surname><given-names>KL</given-names></name>
<name name-style="western"><surname>Zentall</surname><given-names>TR</given-names></name>
</person-group>             <year>1999</year>             <article-title>Observing Behavior in Pigeons: The Effect of Reinforcement Probability and Response Cost Using a Symmetrical Choice Procedure.</article-title>             <source>Learn Motiv</source>             <volume>220</volume>             <fpage>201</fpage>             <lpage>220</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Daly1"><label>13</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daly</surname><given-names>H</given-names></name>
</person-group>             <year>1992</year>             <article-title>Preference for unpredictability is reversed when unpredictable nonreward is aversive.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Gormezano</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Wasserman</surname><given-names>E</given-names></name>
</person-group>             <source>Learning and memory: The behavioral and biological substrates</source>             <publisher-loc>Hillsdale, NJ</publisher-loc>             <publisher-name>Erlbaum</publisher-name>             <fpage>81</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Kreps1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kreps</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Porteus</surname><given-names>E</given-names></name>
</person-group>             <year>1978</year>             <article-title>Temporal resolution of uncertainty and dynamic choice theory.</article-title>             <source>Econometrica</source>             <volume>46</volume>             <fpage>185</fpage>             <lpage>200</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Caplin1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Caplin</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Leahy</surname><given-names>J</given-names></name>
</person-group>             <year>2001</year>             <article-title>Psychological Expected Utility Theory and Anticipatory Feelings?</article-title>             <source>Q J Econ</source>             <volume>116</volume>             <fpage>55</fpage>             <lpage>79</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Loewenstein1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Loewenstein</surname><given-names>G</given-names></name>
</person-group>             <year>1987</year>             <article-title>Anticipation and the valuation of delayed consumption.</article-title>             <source>Econ J (London)</source>             <volume>97</volume>             <fpage>666</fpage>             <lpage>684</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Lovallo1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lovallo</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Kahneman</surname><given-names>D</given-names></name>
</person-group>             <year>2000</year>             <article-title>Living with uncertainty: attractiveness and resolution timing.</article-title>             <source>J Behav Decis Mak</source>             <volume>13</volume>             <fpage>179</fpage>             <lpage>190</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Shannon1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shannon</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Weaver</surname><given-names>W</given-names></name>
</person-group>             <year>1949</year>             <source>The mathematical theory of information, volume 97</source>             <publisher-loc>Urbana, IL</publisher-loc>             <publisher-name>University of Illinois Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000903-Montague1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1996</year>             <article-title>A framework for mesencephalic dopamine systems based on predictive hebbian learning.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>1936</fpage>             <lpage>47</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Schultz1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Montague</surname><given-names>P</given-names></name>
</person-group>             <year>1997</year>             <article-title>A neural substrate of prediction and reward.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1593</fpage>          </element-citation></ref>
<ref id="pcbi.1000903-Sutton1"><label>21</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1998</year>             <source>Reinforcement Learning: An Introduction</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000903-Sutton2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
</person-group>             <year>1988</year>             <article-title>Learning to predict by the methods of temporal differences.</article-title>             <source>Mach Learn</source>             <volume>3</volume>             <fpage>9</fpage>             <lpage>44</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Sutton3"><label>23</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>A</given-names></name>
</person-group>             <year>1990</year>             <source>Time-derivative models of Pavlovian reinforcement</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT press</publisher-name>             <fpage>497</fpage>             <lpage>538</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Kehoe1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kehoe</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Schreurs</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Amodei</surname><given-names>N</given-names></name>
</person-group>             <year>1981</year>             <article-title>Blocking acquisition of the rabbit's nictitating membrane response to serial conditioned stimuli.</article-title>             <source>Learn Motiv</source>             <volume>12</volume>             <fpage>92</fpage>             <lpage>108</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Suri1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Suri</surname><given-names>RE</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1999</year>             <article-title>A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task.</article-title>             <source>Neuroscience</source>             <volume>91</volume>             <fpage>871</fpage>             <lpage>890</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Grossberg1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Schmajuk</surname><given-names>N</given-names></name>
</person-group>             <year>1988</year>             <article-title>Neural dynamics of adaptive timing and temporal discrimination during associative learning.</article-title>             <source>Neural Netw</source>             <volume>1</volume>             <fpage>98</fpage>          </element-citation></ref>
<ref id="pcbi.1000903-Ludvig1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ludvig</surname><given-names>EA</given-names></name>
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Kehoe</surname><given-names>EJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>3034</fpage>             <lpage>3054</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Mauk1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mauk</surname><given-names>MD</given-names></name>
<name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name>
</person-group>             <year>2004</year>             <article-title>The neural basis of temporal processing.</article-title>             <source>Annu Rev Neurosci</source>             <volume>27</volume>             <fpage>307</fpage>             <lpage>340</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-OReilly1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>O'Reilly</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Frank</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <article-title>Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>283</fpage>             <lpage>328</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Frank1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frank</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Loughry</surname><given-names>B</given-names></name>
<name name-style="western"><surname>O'Reilly</surname><given-names>R</given-names></name>
</person-group>             <year>2001</year>             <article-title>Interactions between frontal cortex and basal ganglia in working memory: a computational model.</article-title>             <source>Cogn Affect Behav Neurosci</source>             <volume>1</volume>             <fpage>137</fpage>             <lpage>160</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Williams2"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Williams</surname><given-names>GV</given-names></name>
<name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name>
</person-group>             <year>1995</year>             <article-title>Modulation of memory fields by dopamine d1 receptors in prefrontal cortex.</article-title>             <source>Nature</source>             <volume>376</volume>             <fpage>572</fpage>             <lpage>575</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Tobler1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>
<name name-style="western"><surname>Dickinson</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Coding of predicted reward omission by dopamine neurons in a conditioned inhibition paradigm.</article-title>             <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source>             <volume>23</volume>             <fpage>10402</fpage>             <lpage>10</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Fiorillo1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Discrete coding of reward probability and uncertainty by dopamine neurons.</article-title>             <source>Science</source>             <volume>299</volume>             <fpage>1898</fpage>             <lpage>1902</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Bayer1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bayer</surname><given-names>HM</given-names></name>
<name name-style="western"><surname>Glimcher</surname><given-names>PW</given-names></name>
</person-group>             <year>2005</year>             <article-title>Midbrain dopamine neurons encode a quantitative reward prediction error signal.</article-title>             <source>Neuron</source>             <volume>47</volume>             <fpage>129</fpage>             <lpage>141</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Badia1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Badia</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Harsh</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Abbott</surname><given-names>B</given-names></name>
</person-group>             <year>1979</year>             <article-title>Choosing between predictable and unpredictable shock conditions: Data and theory.</article-title>             <source>Psychol Bull</source>             <volume>86</volume>             <fpage>1107</fpage>             <lpage>1131</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Mitchell1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitchell</surname><given-names>KM</given-names></name>
<name name-style="western"><surname>Perkins</surname><given-names>NP</given-names></name>
<name name-style="western"><surname>Perkins</surname><given-names>CC</given-names></name>
</person-group>             <year>1965</year>             <article-title>Conditions affecting acquisition of observing responses in the absence of differential reward.</article-title>             <source>J Comp Physiol Psychol</source>             <volume>60</volume>             <fpage>435</fpage>             <lpage>7</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Levis1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Levis</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Perkins</surname><given-names>CC</given-names></name>
</person-group>             <year>1965</year>             <article-title>Acquisition of observing responses (RO) with water reward.</article-title>             <source>Psychol Rep</source>             <volume>16</volume>             <fpage>114</fpage>          </element-citation></ref>
<ref id="pcbi.1000903-Daly2"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daly</surname><given-names>HB</given-names></name>
</person-group>             <year>1989</year>             <article-title>Preference for unpredictable food rewards occurs with high proportion of reinforced trials or alcohol if rewards are not delayed.</article-title>             <source>J Exp Psychol Anim Behav Process</source>             <volume>15</volume>             <fpage>3</fpage>             <lpage>13</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Tobler2"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>
<name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2005</year>             <article-title>Adaptive coding of reward value by dopamine neurons.</article-title>             <source>Science</source>             <volume>307</volume>             <fpage>1642</fpage>             <lpage>1645</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Shidara1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shidara</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Aigner</surname><given-names>TG</given-names></name>
<name name-style="western"><surname>Richmond</surname><given-names>BJ</given-names></name>
</person-group>             <year>1998</year>             <article-title>Neuronal signals in the monkey ventral striatum related to progress through a predictable series of trials.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>2613</fpage>             <lpage>2625</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Mackintosh1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mackintosh</surname><given-names>NJA</given-names></name>
</person-group>             <year>1975</year>             <article-title>theory of attention: Variations in the associability of stimuli with reinforcement.</article-title>             <source>Psychol Rev</source>             <volume>2</volume>             <fpage>276</fpage>             <lpage>298</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Daly3"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daly</surname><given-names>HB</given-names></name>
<name name-style="western"><surname>Daly</surname><given-names>JT</given-names></name>
</person-group>             <year>1982</year>             <article-title>A Mathematical Model of Reward and Aversive Nonreward: Its Application in Over 30 Appetitive Learning Situations.</article-title>             <source>New York</source>             <volume>11</volume>             <fpage>441</fpage>             <lpage>480</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Rescorla1"><label>43</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rescorla</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Wagner</surname><given-names>A</given-names></name>
</person-group>             <year>1972</year>             <source>Variations in the Effectiveness of Reinforcement and Nonreinforcement</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Classical Conditioning II: Current Research and Theory, Appleton-Century-Crofts</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000903-Widrow1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Widrow</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Hoff</surname><given-names>MJ</given-names></name>
</person-group>             <year>1960</year>             <article-title>Adaptive switching circuits.</article-title>             <source>IRE WESCON Convention Record</source>             <fpage>96</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Sutton4"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>A</given-names></name>
</person-group>             <year>1987</year>             <article-title>A temporal-difference model of classical conditioning.</article-title>             <source>Proc Annu Conf Cogn Sci Soc</source>             <fpage>355</fpage>             <lpage>378</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Berlyne1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berlyne</surname><given-names>D</given-names></name>
</person-group>             <year>1957</year>             <article-title>Uncertainty and conflict - a point of contact between information-theory and behavior-theory concepts.</article-title>             <source>Psychol Rev</source>             <volume>64</volume>             <fpage>329</fpage>             <lpage>339</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Dayan3"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Prospective and retrospective temporal difference learning.</article-title>             <source>Network</source>             <volume>20</volume>             <fpage>32</fpage>             <lpage>46</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Niv1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Duff</surname><given-names>MO</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2005</year>             <article-title>Dopamine, uncertainty and TD learning.</article-title>             <source>Behavioral Brain Function</source>             <volume>1</volume>             <fpage>6</fpage>          </element-citation></ref>
<ref id="pcbi.1000903-Niv2"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Joel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>A normative perspective on motivation.</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>375</fpage>             <lpage>381</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Salamone1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Salamone</surname><given-names>JD</given-names></name>
<name name-style="western"><surname>Correa</surname><given-names>M</given-names></name>
</person-group>             <year>2002</year>             <article-title>Motivational views of reinforcement: implications for understanding the behavioral functions of nucleus accumbens dopamine.</article-title>             <source>Behav Brain Res</source>             <volume>137</volume>             <fpage>3</fpage>             <lpage>25</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Bunzeck1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bunzeck</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Dolan</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Düzel</surname><given-names>E</given-names></name>
</person-group>             <year>2010</year>             <article-title>A common mechanism for adaptive scaling of reward and novelty.</article-title>             <source>Human Brain Mapping</source>             <comment>In press</comment>          </element-citation></ref>
<ref id="pcbi.1000903-Spetch1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Spetch</surname><given-names>ML</given-names></name>
<name name-style="western"><surname>Belke</surname><given-names>TW</given-names></name>
<name name-style="western"><surname>Barnet</surname><given-names>RC</given-names></name>
<name name-style="western"><surname>Dunn</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Pierce</surname><given-names>WD</given-names></name>
</person-group>             <year>1990</year>             <article-title>Suboptimal choice in a percentage-reinforcement procedure: effects of signal condition and terminal-link length.</article-title>             <source>J Exp Anal Behav</source>             <volume>53</volume>             <fpage>219</fpage>             <lpage>234</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Gipson1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gipson</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Alessandri</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Miller</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Zentall</surname><given-names>TR</given-names></name>
</person-group>             <year>2009</year>             <article-title>Preference for 50% reinforcement over 75% reinforcement by pigeons.</article-title>             <source>Learn Behav</source>             <volume>37</volume>             <fpage>289</fpage>             <lpage>298</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-AstonJones1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Aston-Jones</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Cohen</surname><given-names>JD</given-names></name>
</person-group>             <year>2005</year>             <article-title>Adaptive gain and the role of the locus coeruleus-norepinephrine system in optimal performance.</article-title>             <source>J Comp Neurol</source>             <volume>493</volume>             <fpage>99</fpage>             <lpage>110</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Lieberman1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lieberman</surname><given-names>DA</given-names></name>
<name name-style="western"><surname>Cathro</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Nichol</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Watson</surname><given-names>E</given-names></name>
</person-group>             <year>1997</year>             <article-title>The role of S- in human observing behavior: bad news is sometimes better than no news.</article-title>             <source>Learn Motiv</source>             <volume>28</volume>             <fpage>20</fpage>             <lpage>42</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Fantino1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fantino</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Case</surname><given-names>DA</given-names></name>
</person-group>             <year>1983</year>             <article-title>Human observing:maintaned by stimuli correlated with reinforcement but not extinction.</article-title>             <source>Journal of the experimental analysis of behavior</source>             <volume>40</volume>             <fpage>193</fpage>             <lpage>210</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Wyckoff2"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wyckoff</surname><given-names>L</given-names></name>
</person-group>             <year>1959</year>             <article-title>Toward a quantitative theory of secondary reinforcement.</article-title>             <source>Psychol Rev</source>             <volume>66</volume>             <fpage>68</fpage>             <lpage>78</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Perone1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Perone</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Baron</surname><given-names>A</given-names></name>
</person-group>             <year>1980</year>             <article-title>Reinforcement of human observing behavior by a stimulue correlated with extinction or increased effort.</article-title>             <source>J Exp Anal Behav</source>             <volume>34</volume>             <fpage>239</fpage>             <lpage>261</lpage>          </element-citation></ref>
<ref id="pcbi.1000903-Luce1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Luce</surname><given-names>RD</given-names></name>
</person-group>             <year>1959</year>             <article-title>On the possible psychophysical laws.</article-title>             <source>Psychol Rev</source>             <volume>66</volume>             <fpage>81</fpage>             <lpage>95</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>