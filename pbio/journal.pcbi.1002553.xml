<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01387</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002553</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Neural networks</subject>
              <subject>Neurophysiology</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Computer modeling</subject>
          </subj-group>
          <subj-group>
            <subject>Synthetic vision systems</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Geometry</subject>
            <subj-group>
              <subject>Differential geometry</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Modeling Boundary Vector Cell Firing Given Optic Flow as a Cue</article-title><alt-title alt-title-type="running-head">Modeling Boundary Vector Cell Firing</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Raudies</surname>
            <given-names>Florian</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Hasselmo</surname>
            <given-names>Michael E.</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Center for Computational Neuroscience and Neural Technology (CompNet), Boston University, Boston, Massachusetts, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Psychology, Boston University, Boston, Massachusetts, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Maloney</surname>
            <given-names>Laurence T.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">New York University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">fraudies@bu.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: FR MEH. Performed the experiments: FR. Analyzed the data: FR. Contributed reagents/materials/analysis tools: FR. Wrote the paper: FR MEH. Wrote the software for all computational simulations: FR.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>6</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>28</day>
        <month>6</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>6</issue><elocation-id>e1002553</elocation-id><history>
        <date date-type="received">
          <day>19</day>
          <month>9</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>25</day>
          <month>4</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Raudies, Hasselmo</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Boundary vector cells in entorhinal cortex fire when a rat is in locations at a specific distance from walls of an environment. This firing may originate from memory of the barrier location combined with path integration, or the firing may depend upon the apparent visual input image stream. The modeling work presented here investigates the role of optic flow, the apparent change of patterns of light on the retina, as input for boundary vector cell firing. Analytical spherical flow is used by a template model to segment walls from the ground, to estimate self-motion and the distance and allocentric direction of walls, and to detect drop-offs. Distance estimates of walls in an empty circular or rectangular box have a mean error of less than or equal to two centimeters. Integrating these estimates into a visually driven boundary vector cell model leads to the firing patterns characteristic for boundary vector cells. This suggests that optic flow can influence the firing of boundary vector cells.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Over the past few decades a variety of cells in hippocampal structures have been analyzed and their function has been identified. Head direction cells indicate the world-centered direction of the animals head like a compass. Place cells fire in locations associated with visual, auditory, or olfactory cues. Grid cells fill open space like a carpet with their mosaic of firing. Boundary vector cells fire, if a boundary that cannot be passed by the animal appears at a certain distance and world-centered direction. All these cells are players in the navigation game; however, their interaction and linkage to sensory systems like vision and memory is not fully understood. Our model analyzes a potential link between the visual system and boundary vector cells. As part of the visual system, we model optic flow that is available to rats. Optic flow is defined as change of lightness patterns on the retina and contains information about self-motion and environment. This optic flow is used in our model to estimate the distance and direction of boundaries. Our model simulations suggest a link between optic flow and the firing of boundary vector cells.</p>
      </abstract><funding-group><funding-statement>Both authors are supported in part by CELEST, a NSF Science of Learning Center (OMA-0835976). FR acknowledges support from the Office of Naval Research (ONR N00014-11-1-0535) and MEH from the Office of Naval Research (ONR MURI N00014-10-1-0936) and from NIH R01 MH60013 and MH61492. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="17"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Populations of neurons within the entorhinal cortex and subiculum have firing patterns that depend upon the distance and angle of boundaries in the environment, such as barrier walls. Neurons with this pattern of firing are referred to as boundary vector cells (BVCs) <xref ref-type="bibr" rid="pcbi.1002553-OKeefe1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref> or border cells <xref ref-type="bibr" rid="pcbi.1002553-Solstad1">[4]</xref>. The definition of BVCs includes that of border cells. Border cells specifically fire at a short distance to the wall whereas BVCs fire at a short or long distance to the wall. More general BVCs have a tuning for different wall distances. Boundary vector cells were initially proposed based on observations of changes in the firing location of hippocampal place cells caused by changes in the location of barrier walls surrounding the environment <xref ref-type="bibr" rid="pcbi.1002553-OKeefe1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref>. The initial proposal of BVCs was extended in detailed computational models that explicitly predicted the pattern of firing of BVCs that could, in turn, generate the firing pattern of hippocampal place cells <xref ref-type="bibr" rid="pcbi.1002553-Barry1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Hartley1">[6]</xref>. The predictions of these BVC models have been supported by recent experimental data clearly showing neural firing patterns similar to proposed BVCs in the subiculum <xref ref-type="bibr" rid="pcbi.1002553-Barry1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref> and the entorhinal cortex <xref ref-type="bibr" rid="pcbi.1002553-Solstad1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Savelli1">[7]</xref>.</p>
      <p>The firing fields of BVCs at a distance from the walls (Lever et al., 2009) cannot be explained by proximal cues such as those provided by the whisker system. At least three alternative cues could provide the information for distant firing, and these cues are not mutually exclusive. The first possibility is that distance estimates could be retrieved from memory in combination with sensorimotor path integration. This would require the memorization of the entire environment, especially its boundaries. Information about the spatial location of a boundary would be combined with the current spatial position and head direction to estimate distance and direction of that boundary. The current position and head direction of the rat would be estimated from temporally integrated sensorimotor signals. The second possibility is that multiple visual cues on the wall could be used by rats to estimate the normal of the surface and its distance based on the feature's relative size on the projection, requiring knowledge of the absolute size of the feature. However, typical rat experiments lack the presence of distinct visual features, e.g. wallpapers that could be used to estimate the distance of the wall. Therefore, this possibility seems unlikely. A third possibility is the use of optic flow, the varying patterns of light on the retina while the rat is moving. Optic flow could be used for distance and direction estimation of walls based on the following two assumptions: (i) walls are orthogonal to the ground; and (ii) these walls have piecewise smooth surfaces. In this article, we test this “flow-influence” hypothesis by simulating a rat's trajectory in a circular or square box while estimating the distance and direction of walls from optic flow and integrating these estimates into a model of BVC firing. A priori it is unclear if distance and direction estimates extracted from optic flow are accurate enough to support the firing of BVCs. We demonstrate that these estimates are sufficiently accurate, even for drop offs that lack an orthogonal wall.</p>
      <p>Further evidence for our flow-influence hypothesis is provided by the rat brain structures processing visual image motion. For instance, neurons in primary visual cortex are sensitive to visual motion <xref ref-type="bibr" rid="pcbi.1002553-Burne1">[8]</xref>. These neurons are tuned for orientation, spatial frequency, and temporal frequency of gratings <xref ref-type="bibr" rid="pcbi.1002553-Girman1">[9]</xref>. Another example is a hierarchy of visual processing identified in rats based on anatomical differences of brain structure. This hierarchy could have similar functions compared to the hierarchy found in primates <xref ref-type="bibr" rid="pcbi.1002553-Coogan1">[10]</xref> which is thought to extract properties of optic flow necessary for estimating self-motion <xref ref-type="bibr" rid="pcbi.1002553-Tanaka1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Duffy2">[14]</xref>. An alternative pathway that has been explicitly pointed out in the processing of large-field optic flow could go from the retina to the accessory optic system <xref ref-type="bibr" rid="pcbi.1002553-Simpson1">[15]</xref> and from there to the hippocampal formation <xref ref-type="bibr" rid="pcbi.1002553-Wylie1">[16]</xref>. The latter connection has been described for pigeons. We test this optic flow processing hypothesis and demonstrate that a template model can interpret optic flow patterns and decompose them into variables of self-motion, distance, and direction estimates of walls.</p>
      <p>A sketch of our model is shown in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1</xref>. We assume a simulated rat is running in a box, <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1a</xref>. While the rat is running it samples optic flow patterns from the floor and walls. Sampling is from a wide visual field, as shown in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1a</xref>. To model this wide field of view we use a spherical camera model, of which a side-view is shown in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1b</xref>. The rat's eyeball is elevated above the ground and is moving in the forward direction, in this example. During this self-motion, sample points of the ground will have an angular displacement in the spherical camera model. The idea of our model is to match all the angular displacements that occur within the visual field by flow templates. These flow templates contain parameters of self-motion, ground, and wall planes, depicted by the three boxes in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>. Templates of ground and wall are constructed for a specific known head direction and tilt angle of the head, as well as for unknown self-motion parameters. The tilt is the angle between the optical axis and ground. The sensed flow is compared against all of these templates for parameterized self-motion, ground, and wall configurations. In a cascade of steps that detect maximum activity, the model extracts parameters of self-motion and planar surfaces. First, all templates for the ground and wall are compared and a wall-ground segmentation is achieved by selecting the maximum responding template (no. 1 in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>). Note that the wall and ground template space also contains the parameters of self-motion. Second, the ground flow is used with outputs from the self-motion templates to estimate the self-motion parameters (no. 2 in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>). Third, the distance and allocentric direction of walls is computed from the wall flow and the parameters of self-motion (no. 3 in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>). The distance and direction estimates are passed along to the existing BVC model proposed by Burgess <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Hartley1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Barry2">[17]</xref>. A sketch of the BVC model is given in <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1d</xref>. The BVC model uses the allocentric direction of a wall together with its distance. <xref ref-type="sec" rid="s4">Model</xref> cells construct a tuning for allocentric direction and distance along the normal direction of the wall. In sum, our modeling work suggests that distance and direction estimates are extracted from optic flow and shows that when these estimates are then fed into the previously developed BVC model this can explain the characteristic firing of BVC cells as measured experimentally <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>.</p>
      <fig id="pcbi-1002553-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Distance and allocentric direction of walls are encoded in the firing of boundary vector cells.</title>
          <p>Here we use a model to determine if these cells could be influenced by optic flow. <bold>a</bold>) Shows a rat in a box estimating the distance and allocentric direction of a wall from the sensed patterns of light on its retina. <bold>b</bold>) If the rat moves its eye, e.g. by a forward body motion, these sensed patterns of light shift on the surface of the eyeball. This shift can be described as an angular displacement. <bold>c</bold>) Schematic drawing of the proposed template model. This model sets up flow templates for parameters of self-motion in combination with parameters of planes either describing ground or wall. In a cascade of estimation steps (max-operations) the self-motion and parameters that describe ground and walls are estimated by looking for the best match between flow templates and sensed input flow. <bold>d</bold>) We use a box with an arbitrary outline (gray shading) to display the variables used in the boundary vector cell model. These variables are the cell's preferred allocentric direction <italic>Φ<sub>i</sub></italic> and preferred distance <italic>D<sub>i</sub></italic> and the estimated allocentric direction α and estimated distance <italic>d</italic>. All distances are measured with respect to the wall's surface normal.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g001" xlink:type="simple"/>
      </fig>
      <p>We make several assumptions to focus our modeling effort on the estimation of self-motion, distance and direction from optic flow. First, the visual field, across a full range of angles extending 240° horizontally and 120° vertically, is simulated using a spherical camera model that describes the flow of individual features of the visual scene by temporal changes of the azimuth and the elevation angle of these features (see <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1a and 1b</xref>). Second, the simulation computes the analytical spherical flow of visual features in a virtual environment instead of estimating flow from an image stream. Third, if the rat is in a rectangular or circular box the walls are orthogonal to the ground. Fourth, we assume that the rat leverages different mechanisms to segment walls from the ground versus detecting drop-offs. Fifth, the model builds upon template cells that are tuned to optic flow that is generated by a combination of self-motion and an environment; and the environment is modeled as smooth surfaces for ground and walls. Tuning for self-motion has been found for neurons in macaque monkeys' area MST <xref ref-type="bibr" rid="pcbi.1002553-Tanaka1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Duffy2">[14]</xref>. This finding motivated template models of self-motion estimation in macaque monkeys <xref ref-type="bibr" rid="pcbi.1002553-Perrone1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Lappe2">[21]</xref>.</p>
      <p>Several aspects distinguish our model from previously published template models. In our model, self-motion is restricted to curvilinear motions: These are translational motions along the optical axis combined with rotations around the vertical axis (yaw-rotations). Our template model uses a spherical camera model that helps to account for effects in large visual fields in contrast to a pinhole camera that is restricted to a 180° visual field. Another difference from existing template models is the introduction of templates that are tuned to the combination of self-motion and smooth surfaces modeling walls or ground. This extended tuning allows not only for the estimation of self-motion but also for the estimation of the distance of these surfaces. We make no assumption with respect to the shape of the box, e.g. it could be square, rectangular, or circular. Note that the introduction of multiple models for ground and wall surfaces also requires the segmentation of flow into these separate surfaces. For instance, a rectangular box consists of a ground plane surrounded by planar walls whereas each individual optic flow sample has to be identified as either originating from ground or wall. Given analytical flow for a spherical camera model, the flow that is induced by linear or rotational motion of a wall can be distinguished from flow that is induced by the same motion of the ground. Thus, segmentation in our model is achieved by deciding whether the wall or ground flow template fits better to the sensed flow vector. Our model provides several extensions to existing template models and is motivated by the need to test whether physiological findings of boundary vector cell firing can be explained by using optic flow as a distal cue.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>We organize our results into four sections. First, we start with the generation of simulated rat trajectories based on the velocity statistics of recorded rat trajectories. These simulated trajectories are used to generate the analytical spherical flow representing the flow that would occur on the rat retina: This flow is then provided as input to our template model. In the second section, we show examples of the wall-ground segmentation and detection of drop-offs with our template model. Third, examples for the estimation of distance and direction of walls are shown, together with the error statistics of distance estimates for an entire simulated rat trajectory. In the fourth section we link our template model to the BVC model and show the resulting firing of model cells compared against data <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>.</p>
      <sec id="s2a">
        <title>Simulated rat trajectories that model rat locomotion</title>
        <p>Our aim is to simulate the rat's body movement in an environment similar to the one used in the study of Lever et al. <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>. Therefore, we computed the movement statistics of available rat trajectories in circular <xref ref-type="bibr" rid="pcbi.1002553-Hafting1">[22]</xref> and square boxes <xref ref-type="bibr" rid="pcbi.1002553-Sargolini1">[23]</xref>. Linear velocities are fit by a Rayleigh distribution and rotational velocities by a normal distribution. Values of these fits are reported in <xref ref-type="table" rid="pcbi-1002553-t001">Table 1</xref>. For these values we generated rat trajectories that matched these velocity distributions. Values of the match are reported again in <xref ref-type="table" rid="pcbi-1002553-t001">Table 1</xref>.</p>
        <table-wrap id="pcbi-1002553-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.t001</object-id><label>Table 1</label><caption>
            <title>Matching statistics of recorded and simulated rat trajectories.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002553-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Reference</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Files</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Environment</italic>
                  <xref ref-type="table-fn" rid="nt101">(a)</xref>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Rayleigh distribution b (cm/sec)</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Normal distribution μ (°/sec)</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Normal distribution σ (°/sec)</italic>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">Hafting et al. 2005</td>
                <td align="left" colspan="1" rowspan="1">Hafting_Fig2c_Trial1, Hafting_Fig2c_Trial2, rat_10925</td>
                <td align="left" colspan="1" rowspan="1">Circular, diameter 180 cm</td>
                <td align="left" colspan="1" rowspan="1">16.99</td>
                <td align="left" colspan="1" rowspan="1">−2.48</td>
                <td align="left" colspan="1" rowspan="1">350.58</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Ours</td>
                <td align="left" colspan="1" rowspan="1">‘CircularCage.mat’</td>
                <td align="left" colspan="1" rowspan="1">Circular, diameter 79 cm</td>
                <td align="left" colspan="1" rowspan="1">16.44</td>
                <td align="left" colspan="1" rowspan="1">0.31</td>
                <td align="left" colspan="1" rowspan="1">355.35</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Sargolini et al. 2006</td>
                <td align="left" colspan="1" rowspan="1">11084-03020501_t2c1, 11084-03020501_t2c2, 11084-10030502_t1c1, 11084-10030502_t1c2, 11084-10030502_t1c6, 11084-10030502_t3c7, 11084-10030502_t4c1, 11138-11040509_t5c1, 11207-11060502_t6c2, 11207-11060502_t6c3, 11207-11060502_t6c4, 11207-11060502_t6c5, 11207-16060501_t7c1, 11207-21060503_t8c1, 11207-27060501_t1c3, 11343-08120502_t8c2</td>
                <td align="left" colspan="1" rowspan="1">Square length 50 cm</td>
                <td align="left" colspan="1" rowspan="1">13.25</td>
                <td align="left" colspan="1" rowspan="1">0.62</td>
                <td align="left" colspan="1" rowspan="1">337.93</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Ours</td>
                <td align="left" colspan="1" rowspan="1">‘SquareCage.mat’</td>
                <td align="left" colspan="1" rowspan="1">Square length 62 cm</td>
                <td align="left" colspan="1" rowspan="1">13.02</td>
                <td align="left" colspan="1" rowspan="1">−0.03</td>
                <td align="left" colspan="1" rowspan="1">330.12</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">Ours</td>
                <td align="left" colspan="1" rowspan="1">‘SquareCageWithWall.mat’</td>
                <td align="left" colspan="1" rowspan="1">Square length 62 cm</td>
                <td align="left" colspan="1" rowspan="1">12.95</td>
                <td align="left" colspan="1" rowspan="1">1.89</td>
                <td align="left" colspan="1" rowspan="1">331.07</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label>(a)</label>
              <p>Note that the dimensions of the boxes are quite different due to the different sized boxes used in different labs and experiments.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>For the generation of rat trajectories we combined a deterministic algorithm with a random component. We randomly generate a linear or rotational velocity that follows a Rayleigh or normal distribution, respectively. As the deterministic component we calculate a rotation that turns the rat to continue to walk parallel to the wall. This turn happens only if the rat is closer than 2 cm to the wall and its head direction is smaller than 90° with respect to the normal vector of the wall. <xref ref-type="fig" rid="pcbi-1002553-g002">Figure 2</xref> contains the pseudo-code for this method.</p>
        <fig id="pcbi-1002553-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Shows the pseudo-code for the generation of simulated rat trajectories.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g002" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002553-g003">Figure 3</xref> shows characteristics of our simulated rat trajectories. The Panel 3a shows the Rayleigh distribution of linear velocity (or speed) and Panel 3b shows the normal distribution of rotational velocity for the data for a rat in a circular box. Panels 3c and 3d show fragments of the first minute and of the first five minutes of the simulated trajectory. The second row, Panels 3e–3h, shows the same properties for simulated trajectories in a square box.</p>
        <fig id="pcbi-1002553-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Replication of the velocity statistics of recorded rat trajectories using simulated rat trajectories.</title>
            <p><bold>a</bold>) The linear velocities of the rat's body motion are fitted by a Rayleigh distribution. <bold>b</bold>) The yaw-rotational velocities are fitted by a normal distribution. <bold>c</bold>) Shows the first minute of the simulated rat trajectory and <bold>d</bold>) the first five minutes of the same trajectory in a circular box (79 cm diameter). The panels <bold>e</bold>) and <bold>f</bold>) show the fits for linear and rotational velocity for a simulation in a squared box (62 cm×62 cm). The first minute and the first five minutes of the simulated trajectory are shown in <bold>g</bold>) and <bold>h</bold>), respectively.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g003" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2b">
        <title>Wall-ground segmentation and drop-off detection</title>
        <p>Before the distance of walls can be estimated, flow samples of walls have to be segmented from flow samples of the ground. This is accomplished in the first stage of our model, see <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>. Examples of the segmentation are shown in <xref ref-type="fig" rid="pcbi-1002553-g004">Figure 4c</xref> for a circular box and <xref ref-type="fig" rid="pcbi-1002553-g004">Figure 4g</xref> for a square box.</p>
        <fig id="pcbi-1002553-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Examples of wall-ground segmentation and drop-off detection by our model.</title>
            <p><bold>a</bold>) Depicts a circular box of diameter 79 cm and a 50 cm high wall with the camera at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e001" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e002" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e003" xlink:type="simple"/></inline-formula>, with orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e004" xlink:type="simple"/></inline-formula>, and self-motion <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e005" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e006" xlink:type="simple"/></inline-formula>. <bold>b</bold>) Shows the same circular box with walls removed to simulate a platform. <bold>c</bold>) Wall-ground segmentation estimated by our model based on the analytical flow shown as black arrows in a). <bold>d</bold>) Drop-off detection based on the flow discontinuities. Note that distant boundary locations are not detected, but these usually do not play a role for behavior. <bold>e</bold>) Shows a square box 62 cm×62 cm with a 50 cm high wall. <bold>f</bold>) Shows the same box as in e) with walls removed. <bold>g</bold>) Estimated wall-ground segmentation for the square box. <bold>h</bold>) Detected drop-off at close distance. In all examples the camera had the same position, orientation, and self-motion as mentioned in a). All boxes are described by a triangular mesh.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g004" xlink:type="simple"/>
        </fig>
        <p>In case of drop-offs our model employs a different mechanism by detecting the flow transition from large to small magnitude. Examples of this detection are shown in <xref ref-type="fig" rid="pcbi-1002553-g004">Figure 4d</xref> for a circular box and <xref ref-type="fig" rid="pcbi-1002553-g004">Figure 4h</xref> for a square box. Note that in these examples the drop-off, indicated by the red dots, is not completely detected. The detection shows gaps where the flow differences are not large enough to be picked up by our mechanism. However, these gaps appear for very distant points of the ground-plane and will not directly influence steering for the rat. In contrast, drop-offs that are close to the rat generate large flow differences that are picked up by our model mechanism and which are potential threats for the rat.</p>
      </sec>
      <sec id="s2c">
        <title>Estimation of wall distances, irrespective of their surface</title>
        <p>Instead of modeling specific surface types, like curved and planar, we approximate arbitrary surfaces locally by planes. This allows us to use the same model for curved walls of a circular box or planar walls of a square box. <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5</xref> shows examples of distance estimates. For instance, in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5a</xref> distances are depicted by the magenta colored arrows that closely match up with the boundary of the box. The Panel 5d shows values of the 2D matching function when comparing the sensed flow to flow templates for walls of a certain allocentric direction and distance. The normalized match value is encoded in gray-values whereas black encodes a low match and white encodes a high match. In the example of <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5d</xref> (circular environment) the maximum is at ≈80° to the right and 20 cm distance. This maximum together with all responses that are within a 70% range of the maximum are displayed in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5a</xref> by magenta arrows. Further examples are shown in the 2<sup>nd</sup> and 3<sup>rd</sup> column of <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5</xref>. Note that wall distances are estimated for both curved and planar walls with the same mechanism, as shown in the examples in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5a–c</xref>.</p>
        <fig id="pcbi-1002553-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Examples and error statistics for the allocentric direction and distance estimation of walls for different boxes.</title>
            <p><bold>a</bold>) Top-view of a circular box with diameter 79 cm and 50 cm high walls. The rat's position is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e007" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e008" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e009" xlink:type="simple"/></inline-formula> and the camera coordinate system has the orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e010" xlink:type="simple"/></inline-formula> in the xz-plane depicted by the red and green arrows. <bold>b</bold>) Top-view of the same configuration as in a) for a square box with 62 cm×62 cm with 50 cm high walls. <bold>c</bold>) An additional wall has been added inside the square box of b) and the rat's position changed to be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e011" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e012" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e013" xlink:type="simple"/></inline-formula> and orientation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e014" xlink:type="simple"/></inline-formula>. In all cases the camera moved by the linear velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e015" xlink:type="simple"/></inline-formula> and the rotational yaw-velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e016" xlink:type="simple"/></inline-formula>. <bold>d</bold>) Match values for distance and allocentric direction of walls in the circular environment (shown in a) provided by our template model. Low match values are encoded by black and high match values by white. <bold>e</bold>) Shows the match values for the square box (shown in b) with same encoding as used in d). Multiple separate regions of high intensity with their peak encode multiple walls as shown in this example. <bold>f</bold>) Match values for the box with interior wall (shown in c). In the last row the mean distance errors over all estimates from 20 min long simulated rat trajectories are shown depending on the position of the rat. <bold>g</bold>) Distance errors for the circular and square box <bold>h</bold>) both range within two centimeters. <bold>i</bold>) For the box with interior wall the mean distance error ranges within six centimeters.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g005" xlink:type="simple"/>
        </fig>
        <p>Our template model allows for the estimation of distance and direction to multiple walls. In <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5b and 5c</xref> distances to two walls are estimated. These are represented by their individual matching high intensity regions in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5e and 5f</xref>. For instance, for the square box in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5b</xref>, high intensity regions appear in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5e</xref> at 0° and 90° allocentric direction representing the left and upper wall, respectively. In case of the curved wall in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5a</xref>, each segment of the wall is represented by a wall-model. High matching values appear around 80° allocentric direction in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5d</xref>. This shows that our model generalizes to non-planar walls.</p>
        <p>In addition to these examples of single distance estimates, we evaluated the distance error systematically for each sample point of our simulated rat trajectories of approximately 20 min duration that include 60,000 sample points for the 50 Hz sampling frequency. We compute the mean distance error for each location, computed for all distance estimates made at that location. Note, that this error measure is, mostly, independent of the actual distance to the wall since all positions provide at least two different distances to walls, excluding the center in the circular box or square box. Here, we assume larger distance estimates for the center and smaller ones for areas close to the wall as our model tends to estimate distance to closer walls rather than farther walls. For the circular box, the mean error is largest in its center; see <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5g</xref>. For the square box the mean distance error is approximately homogenous and smaller, with a value of about one centimeter; see <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5h</xref>. For the square box with an intrinsic wall the mean distance error has a maximum of 6 cm, occurring at the inner side of the narrow passages at each end of the intrinsic wall; see <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5i</xref>. Next, we will integrate these distance estimates for allocentric directions into the boundary vector cell model.</p>
      </sec>
      <sec id="s2d">
        <title>Boundary vector cells might be influenced by optic flow</title>
        <p>Optic flow could be one cue to support firing of boundary vector cells (BVCs) that fire for walls being present at a specific distance and allocentric direction. Distant firing distinguishes BVCs from border cells <xref ref-type="bibr" rid="pcbi.1002553-Solstad1">[4]</xref>. So far, our template model provides distance and direction estimates of walls, given the allocentric head direction which we assume is available, e.g., from the head direction cell system. The head direction cell system encodes the head direction in an allocentric representation <xref ref-type="bibr" rid="pcbi.1002553-Taube1">[24]</xref>. In our model simulation we assume the head direction and position given by ground-truth values for every sample point. We use the ground-truth head direction to estimate the wall direction in allocentric angular coordinates. Ground-truth positions are used to spatially register the firing of cells in the model. In the corresponding experiment of rats foraging in a box this ground-truth location is given by tracking the rat's position reconstructed from video recording of a light-emitting diode attached to the rat. Ground-truth position values are not provided to our template model of brain mechanisms for the estimation of self-motion or wall distances. When the model produces inconsistent distance estimates, the plotting of these estimates in relationship to ground-truth position appears as noisy plots of firing. Such firing lacks the consistent tuning properties for allocentric direction and distance toward the wall that is characteristic of data on the firing of boundary vector cells <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>.</p>
        <p>We compare the data of recordings of BVCs <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref> and simulations of the BVC model <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref> based on ground-truth input to our visually driven model of BVCs. <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6a</xref> shows the square box used for this simulation together with the occupancy of the simulated rat in this box. <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6b</xref> shows the firing of the BVC model when supplied with ground truth input. <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6c</xref> shows experimental data from recordings of recorded BVCs and <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6d</xref> shows our visually driven BVC model based on optic flow input. The firing fields of our visually driven BVC model are more restricted in location than the experimental data or the firing of the original BVC model that uses ground-truth input. In the original BVC model only four distance and direction values are used to update the firing of a model cell. For our visually driven BVC model more than four distance and direction estimates are used to update the firing of a model cell, see e.g., the number of magenta arrows in <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5a–c</xref>. Because there are more estimates incorporated into the visually driven BVC model its firing fields appear more restricted compared to those of the original BVC model. Simulations and data for a circular box are shown in <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6e–g</xref>. Again, our simulated firing fields appear more localized than the firing of recorded cells. Our model assumes analytically defined flow. However, in case of flow detected from an image sequence, distance estimates could be more erroneous leading to the firing observed in recorded data.</p>
        <fig id="pcbi-1002553-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Boundary vector cell (BVC) responses (rate maps) for a square (top) and circular (bottom) box for the model and from data.</title>
            <p><bold>a</bold>) Shows the square box and the occupancy that is high at edges for the simulated rat trajectory. <bold>b</bold>) Rate maps for the BVC model using ground-truth distance and direction of walls. For the model we used eight allocentric directions ranging from east, east-north, north, … to south-east combined with the three distance tunings 2 cm, 10 cm, and 25 cm. High firing is encoded as red color and low as blue color. This color encoding is the same for all plots showing firing rate maps. White numbers are the individual scaling parameters for each plot similar to the firing rate scaling used for plotting the experimental data. <bold>c</bold>) Example of recorded BVCs. These firing maps have been redrawn from Lever et al, J. of Neurosci. 29, 2009 from their <xref ref-type="fig" rid="pcbi-1002553-g003">Figure 3</xref> on page 9774 <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>. The numbers in black denote the firing rate of the cells. <bold>d</bold>) The BVC model receives estimates about allocentric direction and distance from our template model. <bold>e</bold>) Shows the circular box and occupancy of the simulated trajectory. <bold>f</bold>) Shows the rate maps of the BVC model that uses distance and direction estimates of our template model. <bold>g</bold>) Data from recorded BVCs <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g006" xlink:type="simple"/>
        </fig>
        <p>Aside from using a square and circular box we provide additional simulations with a wall inserted inside of the square box and in another simulation we removed all walls to model a platform. <xref ref-type="fig" rid="pcbi-1002553-g007">Figure 7b</xref> shows the BVC firing of our simulation and 7c the corresponding experimental data. The important observation from this simulation is that the BVC firing in the model is not tied to a specific wall of allocentric direction and distance but to <italic>any</italic> wall of an allocentric direction and distance. In our simulation, firing appears also next to the inserted wall. In the same way our model cells would adapt to wall changes in the environment as shown in another experiment which involved nesting two boxes, a small one in a bigger one. After some time the smaller box is quickly removed in that experiment. Then firing of BVC shifts its absolute position in the larger box to resemble the same distant tuning that it had in the small box <xref ref-type="bibr" rid="pcbi.1002553-Savelli1">[7]</xref>. Our model would produce results consistent with this experiment.</p>
        <fig id="pcbi-1002553-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Boundary vector cell (BVC) responses in a box with an additional interior wall and for a platform.</title>
            <p><bold>a</bold>) Square box with an additional interior wall and the occupancy of the simulated rat trajectory. <bold>b</bold>) Simulation of the BVC model using estimates from our template model. The tuning of model cells is the same as in <xref ref-type="fig" rid="pcbi-1002553-g006">Figure 6</xref>. <bold>c</bold>) Data of recorded BVC. Note that model and recorded BVCs respond to any wall of a certain distance and allocentric direction and not only, e.g. to the exterior walls of a box. <bold>d</bold>) Circular platform together with the occupancy of the simulated rat trajectory. For reasons of comparison we use the same trajectory as in the simulation with a circular box. <bold>e</bold>) Shows the rate map of model BVC supplied with estimates about distance and direction of walls. <bold>f</bold>) Data from recorded cells. Firing maps of BVC have been redrawn from Lever et al, J. of Neurosci. 29, 2009 from their <xref ref-type="fig" rid="pcbi-1002553-g003">Figure 3</xref> on page 9774 <xref ref-type="bibr" rid="pcbi.1002553-Lever1">[3]</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g007" xlink:type="simple"/>
        </fig>
        <p>For a platform, the drop-off is detected as a local discontinuity in flow direction and speed. Once the elevation of the drop-off is determined, it is converted into a distance estimate. All distance estimates are fed into the BVC model with their response values. <xref ref-type="fig" rid="pcbi-1002553-g007">Figure 7e</xref> shows the simulation results and 7f the corresponding experimental data. As in previous cases, the BVC in the model is more clearly restricted in location compared to experimental data. This greater restriction in location might differ if the optic flow signal were detected from visual input instead of analytically defined flow that is used in the simulation. In particular, flow detected from visual input would be noisier, and this would influence the accuracy of detection of flow discontinuities.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>This paper presents a template model for scene-segmentation and the estimation of geometric properties of the environment, namely the distance and allocentric direction of walls and drop-offs. Distance estimates of our model in empty boxes are accurate within a two-centimeter-range; for a square box with an inserted intrinsic wall the error is higher at locations at the inner edge of the narrow passages created at either end of the intrinsic wall. When these distance and direction estimates are integrated into the boundary vector cell model <xref ref-type="bibr" rid="pcbi.1002553-Barry1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Hartley1">[6]</xref>, the typical firing patterns found in experimental data on boundary vector cells can be observed.</p>
      <p>Template models for the estimation of ego-motion have been used mainly as a model for self-motion estimation in primates <xref ref-type="bibr" rid="pcbi.1002553-Perrone1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Lappe2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Royden1">[25]</xref>. All these models used a pinhole camera model. In contrast, here we used a spherical camera model to simulate the large visual field of rats. Existing template models account for general self-motion sometimes restricted by visual fixation, which allows the translational motion to be compensated by a rotation in order to keep a single point stationary in the visual field <xref ref-type="bibr" rid="pcbi.1002553-Perrone2">[19]</xref>. No previously published template model provides a link to estimate environmental variables such as distance toward walls. Thus, our model is novel for defining an extended template space and for combining this with voting that allows for the estimation of multiple walls. An advantage of such a voting technique is the more robust estimation and compact description of the surrounding environment in contrast to reconstructing a depth map with variable depths for every single flow vector as suggested by others, e.g. by Perrone &amp; Stone <xref ref-type="bibr" rid="pcbi.1002553-Perrone2">[19]</xref>.</p>
      <p>Most studies on firing properties of hippocampal structures in rats focus on visual cues in general, e.g. a cue card, but not on optic flow as such. For instance, visual cues influence the orientation and firing location of hippocampal place cells <xref ref-type="bibr" rid="pcbi.1002553-Muller1">[26]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Knierim1">[28]</xref>. In neurophysiological recording data on place and head direction cell firing, landmark cues have been shown to dominate over idiothetic cues (e.g. path integration of self-motion information) if the mismatch between cues is smaller than 45°. Above 45° mismatch, the hippocampal representation of place cell firing reorganizes and head direction cell firing is dominated by idiothetic cues <xref ref-type="bibr" rid="pcbi.1002553-Knierim1">[28]</xref>. When deprived from vision and audition the majority of place cells (11 out of 15) lose their spatially consistent firing. Instead their firing pattern rotates with the associated arm of a multi-armed maze <xref ref-type="bibr" rid="pcbi.1002553-Hill1">[29]</xref>. Entorhinal lesions had a similar effect to vision deprivation. Sixteen out of 17 place cells lost their spatially consistent firing <xref ref-type="bibr" rid="pcbi.1002553-Miller1">[30]</xref>. Grid cells rotate their firing with visual landmark cues <xref ref-type="bibr" rid="pcbi.1002553-Hafting1">[22]</xref>. Combining results from the lesion and sensory deprivation study suggests a role of visual and auditory sensory signals in spatially consistent firing. However, none of the existing studies focused on optic flow as the only cue for spatially consistent firing.</p>
      <p>Although visual input provides a rich set of information, other cues might be important for BVC firing, as well. Vestibular information and visual motion influences hippocampal place cell firing <xref ref-type="bibr" rid="pcbi.1002553-Sharp1">[31]</xref>. Vestibular inputs are used to find a path back to the home location, especially in the dark <xref ref-type="bibr" rid="pcbi.1002553-Wallace1">[32]</xref>. Head direction cells are regulated by the vestibular system <xref ref-type="bibr" rid="pcbi.1002553-Brown1">[33]</xref>. In blind rats place cell firing occurs and in four out of 15 cells the firing is spatially consistent <xref ref-type="bibr" rid="pcbi.1002553-Wallace1">[32]</xref>. The consistent firing in four cells provides evidence for the use of idiothetic cues such as path integration in order to maintain a stable representation of the self in the environment model <xref ref-type="bibr" rid="pcbi.1002553-Best1">[34; page 466]</xref>. Idiothetic cues like path integration and external cues like landmarks interact to regulate place field firing in rats on a running track foraging while cues are brought into mismatch by spatially shifting the goal location <xref ref-type="bibr" rid="pcbi.1002553-Gothard1">[35]</xref>. These various examples show that cues other than vision are important to maintain the firing of place cells.</p>
      <p>Three alternative technical solutions are possible for the segmentation of walls from ground, and subsequently the estimation of self-motion and environment variables. These are: RANSAC <xref ref-type="bibr" rid="pcbi.1002553-Fischler1">[36]</xref>, m-functions <xref ref-type="bibr" rid="pcbi.1002553-Boyd1">[37]</xref>, or the expectation maximization (EM) algorithm <xref ref-type="bibr" rid="pcbi.1002553-Dempster1">[38]</xref>. RANSAC could be based on a model for flow of the ground while treating flow samples from walls as outliers assuming that the majority of flow samples originate from the ground. Once the segmentation is achieved all the points identified as outliers can be used to estimate the distance and direction of walls. An integration of Equations (3) and (4) into convex m-functions leads to a non-linear optimization problem. An embedding into the EM algorithm with Gaussian mixture models leads again to a non-linear optimization. Overall, the segmentation of walls from ground is a challenging and computationally expensive task.</p>
      <p>For real-life images the quality of flow based segmentation depends largely on the quality of the detected flow and the dissimilarity between flow templates or flow vectors at the drop-off. Since we do not know the quality of detected flow for real-life images we study simulated noise superimposed on the analytically defined flow. Examples with additive Gaussian noise in each component of the flow with a signal-to-noise ratio of approximately 70 dB leads to larger errors in distance and direction estimates (see <xref ref-type="supplementary-material" rid="pcbi.1002553.s001">Figure S1</xref>). A major source for this error is insufficient segmentation. Since the segmentation is based on local information, a single flow vector, it is strongly influenced by noise. This could be compensated by adding a neighborhood function into the process of segmentation that assumes neighboring points belong to the same planar model, either wall or ground. Another problem is a close similarity between flow templates if matching a noisy input flow. Therefore, the dissimilarity or “distance” between templates should be maximized in the sense of the proposed matching functions in order to match noisy input flow to the correct flow template. For drop-offs the dissimilarity between flows at the drop-off versus everywhere else in the flow field matters. If discontinuities within the flow due to noise become too large false detections happen. This can be only compensated for with context information, e.g. providing extended curve models for the drop-off in the spherical camera model that could be fitted as an entire curve ranging from −120° to +120° azimuth angle for the parameterization of our spherical camera model. So far, these extensions have not been realized in the current model and are the subject of future work.</p>
      <p>Further properties of our model are the logarithm used in the matching function and the model's capability to incorporate tilt angles. Choosing a logarithmic sampling and the logarithm of motion speeds to compare input flow vectors and template flow vectors makes sense for a first-person perspective from an ecological and behavioral point of view. Typically, objects' distances that are far do not have to be represented with a high sampling, e.g. of centimeter-precision, because they are not reachable or are not potential obstacles. A logarithmic sampling of distance values also has an effect on the comparison between flow vectors of different distance. For optic flow generated by translational self-motion the length of flow vectors is inversely proportional to the distance of a sample point in 3D space. By transforming these distances using a logarithm we put more emphasis on short flow vectors that relate to points that are close to the rat. <xref ref-type="supplementary-material" rid="pcbi.1002553.s002">Figure S2</xref> shows a comparison between a matching function that uses the logarithm of the speed and their difference or only the difference of speeds without the logarithm. In both cases the speeds are computed from the input flow vectors and template flow vectors. The matching that includes the logarithm appears clearer over the entire range of depths compared to directly using the difference of speeds. Note that the speed difference that does not involve the logarithm can be adjusted only to accommodate a small depth range with clear tuning. This concept of using a logarithmic sampling and logarithmic scale to compare speeds could be used even more broadly by mechanisms that afford an ecological solution, e.g. if only a limited small number of samples are available.</p>
      <p>Another property of our model is the incorporation of non-zero tilt angles. In such configurations the optical axis is not parallel to the ground. The normal vector that describes the wall or ground becomes dependent on the head direction. In our model this head direction is assumed to be given, e.g. by the vestibular cues captured by the head direction cell system, as is the tilt angle (see also <xref ref-type="fig" rid="pcbi-1002553-g001">Figure 1c</xref>). Then our model constructs flow templates for this given tilt and head direction. Simulation results for BVC firing look similar to the ones of <xref ref-type="fig" rid="pcbi-1002553-g005">Figure 5</xref> and <xref ref-type="fig" rid="pcbi-1002553-g006">6</xref> as shown in <xref ref-type="supplementary-material" rid="pcbi.1002553.s003">Figure S3</xref>; however, distance errors at large distances are slightly increased. Note that for the positive 30° tilt more flow samples originate from the ground which could give an explanation for the increase in the measured distance error. This is especially the case at large distances to the wall. These two properties of our model, the logarithm of speeds used in the matching function and the non-zero tilt angle that introduces a dependency on head direction, are important for the distance estimation and generalization to other configurations of varying tilt.</p>
      <p>Our current model has several limitations. So far, our model responds only to visible walls and drop-offs; however, place cells that may be driven by BVCs also respond in the presence of transparent walls <xref ref-type="bibr" rid="pcbi.1002553-Muller1">[26]</xref>. Furthermore, this model does not work in the dark since our model relies on optic flow, the changes of light patterns on the retina. Another limitation of our model is the restriction of self-motion to curvilinear path motion. Such motions exclude pitch and roll rotations and translational motions that are not parallel to the ground. These limitations could be relaxed by modeling more degrees of freedom in the template model. However, such an extension will increase the number of flow templates. Furthermore, it remains unclear if detecting the separation between ground and wall is still possible for such an extended model in the way it is possible for curvilinear motion. Another restriction of our model is the assumption about analytical, noise-free flow. In reality, flow has to be estimated from light changes and flow estimates would contain errors. To address these limitations future work could include other systems, such as distance estimates from binocular vision, a landmark system along with a triangulation strategy, sensorimotor integration and memory to operate in the dark, or the suggested neighborhood function to improve segmentation given noisy, detected flow.</p>
      <p>Information about self-motion and environment structure that is extracted by our model from optic flow could be useful for other cell types as well. Grid cells can be generated by temporally integrated linear and rotational velocities that are estimated from optic flow <xref ref-type="bibr" rid="pcbi.1002553-Raudies1">[39]</xref>. Such integration allows for a reasonable estimate of the rat's position in the environment for a short duration, less than a minute with a temporal sampling frequency of 50 Hz. Optic flow can provide the information about short paths and, thus, has the potential to contribute to the place cell firing, a firing tied to specific allocentric spatial location in the environment. The integration of rotational yaw velocities can provide a head-direction signal, again for the time frame of about a minute. Furthermore, there may be an indirect effect as boundary vector cells might influence the firing of grid cells and place cells. Recent studies suggest that BVCs may function as an independent system from grid cells, as inactivation of the medial septum with muscimol causes a loss of grid cell spatial periodicity with sparing of some cells that look like BVCs, and sparing of the spatial firing response of place cells <xref ref-type="bibr" rid="pcbi.1002553-Brandon1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Koenig1">[41]</xref>. Thus, optic flow may provide input to cell populations in entorhinal cortex, subiculum, and hippocampus.</p>
      <p>Following our “flow-influence” hypothesis our model would predict cells with sensitivity to large flow fields. However, instead of in hippocampal or related areas, we assume this sensitivity to exist in sensory related areas, such as the primary visual area or higher level visual cortical areas or the accessory optic system. In primates these sensitivities have been found in area MT and MSTd <xref ref-type="bibr" rid="pcbi.1002553-Tanaka1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1002553-Duffy2">[14]</xref>. The spatially integrative behavior of cells can be tested by using motion stimuli of different retinal size while measuring the response from our hypothetical motion cells. Then there should be an effect on firing rate coupled to retinal stimulus size. Furthermore, the “flow-influence” hypothesis for BVC is supported by our modeling work. An experiment testing this hypothesis would record BVCs from subiculum while the animal is passively watching the visual input of a simulated trajectory. To only provide optic flow cues the displayed stimulus would consist of a random dot texture as used in virtual environment setups for humans and should be compared with performance when viewing a display that consists mainly of object outlines that provide visual cues other than optic flow <xref ref-type="bibr" rid="pcbi.1002553-Warren1">[42]</xref>. This passive watching setup should be compared to the freely moving animal while recording from the same BVC – this might be difficult to achieve but testing of virtual environments with stationary animals has been done <xref ref-type="bibr" rid="pcbi.1002553-Harvey1">[43]</xref>. Our modeling work would predict that BVC firing will be observed during the passive watching setup; however, we assume it would be nosier, due to the lack of other cues and the prediction that multimodal sensory cues are usually integrated by BVCs during normal behavior.</p>
    </sec>
    <sec id="s4">
      <title>Model</title>
      <p>We divide the explanation of our template model into the following steps: First, we define the spherical image flow model for curvilinear self-motion. In the second step, flow templates are defined for a ground-plane and planar walls. Third, tuning or matching functions for the comparison between input flow and template flow are defined. Then we summarize all computational steps in an algorithm. Fourth, to interpret and visualize the representation of our template model we define a read-out method. Fifth, a description for the integration of estimated distance and direction values into the boundary vector cell model is given. Parameters of the spherical camera model, template model, and boundary vector cell model are summarized in <xref ref-type="table" rid="pcbi-1002553-t002">Table 2</xref>.</p>
      <table-wrap id="pcbi-1002553-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.t002</object-id><label>Table 2</label><caption>
          <title>Parameters of the models and their values used in the simulations.</title>
        </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002553-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.t002" xlink:type="simple"/><table>
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <td align="left" colspan="1" rowspan="1">
                <italic>Description of parameter</italic>
              </td>
              <td align="left" colspan="1" rowspan="1">
                <italic>Identifier and value</italic>
              </td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" colspan="1" rowspan="1">
                <bold>Spherical camera model</bold>
              </td>
              <td align="left" colspan="1" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Horizontal field of view</td>
              <td align="left" colspan="1" rowspan="1">240°</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Vertical field of view</td>
              <td align="left" colspan="1" rowspan="1">120°</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Eye-height above ground</td>
              <td align="left" colspan="1" rowspan="1">3.5 cm</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Horizontal resolution</td>
              <td align="left" colspan="1" rowspan="1">80 samples or 400 samples<xref ref-type="table-fn" rid="nt102">a</xref></td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Vertical resolution</td>
              <td align="left" colspan="1" rowspan="1">40 samples or 200 samples<xref ref-type="table-fn" rid="nt102">a</xref></td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Minimum distance to samples</td>
              <td align="left" colspan="1" rowspan="1">0 cm</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Maximum distance to samples</td>
              <td align="left" colspan="1" rowspan="1">1000 cm</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">
                <bold>Template model</bold>
              </td>
              <td align="left" colspan="1" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation for ground samples</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e017" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation for wall samples</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e018" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Interval for linear velocities</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e019" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Samples for linear velocities</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e020" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation for rotational velocity</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e021" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Interval for rotational velocities</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e022" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Samples for rotational velocities</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e023" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation for speed tuning used for walls</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e024" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation for direction tuning used for walls</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e025" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Offset for direction tuning of walls</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e026" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Interval for walls' angles</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e027" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Samples for walls' angles</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e028" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Interval for walls' distances<xref ref-type="table-fn" rid="nt103">b</xref></td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e029" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Samples for walls' distances</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e030" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">
                <bold>Boundary vector cell model</bold>
              </td>
              <td align="left" colspan="1" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation of distance tuning</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e031" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Parameters of distance-dependent tuning</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e032" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Samples for distances</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e033" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Standard deviation of angular tuning</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e034" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Allocentric direction samples</td>
              <td align="left" colspan="1" rowspan="1">
                <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e035" xlink:type="simple"/></inline-formula>
              </td>
            </tr>
          </tbody>
        </table></alternatives><table-wrap-foot>
          <fn id="nt102">
            <label>a</label>
            <p>We use the latter increased sampling for the simulation with drop-offs only to increase the sampling of distances that is coupled to the number of elevations.</p>
          </fn>
          <fn id="nt103">
            <label>b</label>
            <p>The interval is sampled at a logarithmic scale.</p>
          </fn>
        </table-wrap-foot></table-wrap>
      <sec id="s4a">
        <title>Spherical image flow model</title>
        <p>The spherical image flow model for instantaneous motion through a rigid stationary environment is <xref ref-type="bibr" rid="pcbi.1002553-Rieger1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Calow1">[45]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e036" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e037" xlink:type="simple"/></inline-formula> denotes the azimuth angle and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e038" xlink:type="simple"/></inline-formula> the elevation angle. Azimuth is measured from the z-axis pointing forward along the optical axis in the xz-plane. Elevation is measured from z′-axis in the yz′-plane where z′ denotes the z-axis that is rotated by the azimuth angle. This definition uses a left-handed coordinate system. The 3D linear velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e039" xlink:type="simple"/></inline-formula> and the 3D rotational velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e040" xlink:type="simple"/></inline-formula> cause temporal changes for azimuth <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e041" xlink:type="simple"/></inline-formula> and elevation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e042" xlink:type="simple"/></inline-formula> assuming a differential motion model that neglects higher order temporal differences, like accelerations <xref ref-type="bibr" rid="pcbi.1002553-Goldstein1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-LonguetHiggins1">[47]</xref>. The super-index ‘t’ denotes the vector-transpose. The distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e043" xlink:type="simple"/></inline-formula> is the length toward a 3D sample point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e044" xlink:type="simple"/></inline-formula> in Cartesian coordinates.</p>
        <p>In the simulations we assume that the rat is moving tangent to the recorded trajectory in the 2D plane. This assumption reduces the six degrees of freedom of the model to two degrees of freedom: The linear velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e045" xlink:type="simple"/></inline-formula> along the optical axis (z-axis) and the rotational velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e046" xlink:type="simple"/></inline-formula> around the y-axis (yaw-rotation). Thus, Equation 1 reduces to a model of visual image motion for curvilinear self-motion:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e047" xlink:type="simple"/><label>(2)</label></disp-formula>In this Equation 2 the distance variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e048" xlink:type="simple"/></inline-formula> is very general and can be different for every image location defined by the azimuth angle <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e049" xlink:type="simple"/></inline-formula> and elevation angle <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e050" xlink:type="simple"/></inline-formula>. To constrain this variable further, we define a model of a ground plane and planar walls. <xref ref-type="fig" rid="pcbi-1002553-g008">Figure 8</xref> visualizes this simplified spherical flow model with only two degrees of freedom together with the definition of the camera system.</p>
        <fig id="pcbi-1002553-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Drawing of the spherical camera model and an analytical flow vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e051" xlink:type="simple"/></inline-formula> that arises if the entire model is moving by the linear velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e052" xlink:type="simple"/></inline-formula> and rotating around the y-axis by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e053" xlink:type="simple"/></inline-formula>.</title>
            <p>For this paper we use the left-handed-coordinate system with the x-axis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e054" xlink:type="simple"/></inline-formula> pointing to the right, the y-axis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e055" xlink:type="simple"/></inline-formula> pointing upward, and the z-axis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e056" xlink:type="simple"/></inline-formula> pointing forward. The location <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e057" xlink:type="simple"/></inline-formula> is described by the angles <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e058" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e059" xlink:type="simple"/></inline-formula> together with its distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e060" xlink:type="simple"/></inline-formula> from the origin. Our spherical model describes the flow vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e061" xlink:type="simple"/></inline-formula> by its angular, temporal differentials <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e063" xlink:type="simple"/></inline-formula> - not depicted in the drawing for clarity.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g008" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4b">
        <title>Flow template for a ground-plane</title>
        <p>In Hessian normal form a plane is described by its unit normal vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e064" xlink:type="simple"/></inline-formula> and distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e065" xlink:type="simple"/></inline-formula>. This distance is measured along the normal. Plugging the plane definition into the projection function for the spherical camera model defined in azimuth angle and elevation angle results in the definition of the 3D point distance:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e066" xlink:type="simple"/><label>(3)</label></disp-formula>For a ground-plane with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e067" xlink:type="simple"/></inline-formula> for zero-tilt <italic>γ</italic> = 0 and distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e068" xlink:type="simple"/></inline-formula> as eye-height above the ground this ground-plane model simplifies to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e069" xlink:type="simple"/></inline-formula>. For a tilt angle <italic>γ</italic>≠0 the normal vector is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e070" xlink:type="simple"/></inline-formula> which depends now also on the allocentric camera or head direction <italic>φ</italic>. This normal vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e071" xlink:type="simple"/></inline-formula> can be computed, e.g., by using Rodrigues rotation equation and rotating the normal vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e072" xlink:type="simple"/></inline-formula> around the axis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e073" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4c">
        <title>Flow templates for planar walls</title>
        <p>The depth function for planar walls assumes a wall to be defined by the normal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e074" xlink:type="simple"/></inline-formula> that is rotated according to the allocentric direction <italic>φ</italic> of the rat's head which results in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e075" xlink:type="simple"/></inline-formula> with the angle <italic>α</italic> being the allocentric direction of the wall. For a tilt angle <italic>γ</italic>≠0 the wall's normal vector is described by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e076" xlink:type="simple"/><label>(4)</label></disp-formula>For this definition the order of rotations is crucial: First, we rotate for the wall's direction <italic>α</italic>, second for the tilt angle <italic>γ</italic>, and third by the allocentric direction of the rat's head <italic>φ</italic>.</p>
        <p>The distance function from Equation (3) with the corresponding normal vectors is plugged into Equation (2) to define the template flows for curvilinear self-motions defined by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e077" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e078" xlink:type="simple"/></inline-formula>. This results in the constrained flow equation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e079" xlink:type="simple"/><label>(5)</label></disp-formula>For the normal vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e080" xlink:type="simple"/></inline-formula> the corresponding model for a ground-plane or wall-plane is plugged in. Tuning functions are employed to compare single flow vectors of the template flows against its corresponding vectors of the input flow. These tuning functions are described next.</p>
      </sec>
      <sec id="s4d">
        <title>Optimizations and tuning functions of the template model</title>
        <p>Input flow is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e081" xlink:type="simple"/></inline-formula> and is compared against the template flow for walls <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e082" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e083" xlink:type="simple"/></inline-formula>, the template flow for the ground-plane where the latter two parameters of distance and angle are dropped. Our first goal is to segment the flows into samples from ground or wall. To derive a flow constraint that is independent of the rotational velocity <italic>ω<sub>y</sub></italic> but depends on the distances <italic>D</italic>, we multiply the Equation (5) by the vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e084" xlink:type="simple"/></inline-formula>. This provides the following tuning functions for segmentation. First, the tuning function for potential sample points of the ground-plane is:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e085" xlink:type="simple"/><label>(6)</label></disp-formula></p>
        <p>Second, the tuning function for potential sample points of walls is defined by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e086" xlink:type="simple"/><label>(7)</label></disp-formula>In this tuning function we use the mean velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e087" xlink:type="simple"/></inline-formula> that is computed over all <italic>m</italic> velocity samples. For the wall-ground segmentation we use the following decisions to define the set of wall samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e088" xlink:type="simple"/></inline-formula> and the set of ground samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e089" xlink:type="simple"/></inline-formula>.</p>
        <p>Then we continue with the ground samples to estimate the linear velocity of the rat by using the tuning function:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e090" xlink:type="simple"/><label>(8)</label></disp-formula>This function in Equation (8) defines matches between the input flow and the template flows for the linear velocity samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e091" xlink:type="simple"/></inline-formula>. Matches are computed over all samples that have been identified to originate from the ground. This provides the overall similarity between the input flow and a template flow.</p>
        <p>Next, we compute the rotational velocity from ground samples. For this computation we use the following tuning function which computes the Euclidean distance between input flow and template flow:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e092" xlink:type="simple"/><label>(9)</label></disp-formula>In this Equation (9), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e093" xlink:type="simple"/></inline-formula> is the estimated linear velocity from Equation (8), e.g. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e094" xlink:type="simple"/></inline-formula>.</p>
        <p>In the last step we estimate wall distances for a given allocentric direction and use the already estimated linear and rotational velocity from Equation (8) and (9). For this distance and direction estimation we use the tuning function as defined by Perrone <xref ref-type="bibr" rid="pcbi.1002553-Perrone1">[18]</xref>. Note, this function has not been used for any of the previous problems due to optimizing for the rotational velocity in Equations (6)–(8) which uses a constraint that is independent of rotational velocities and in Equation (9) because the rotational velocity is independent of the depth and, thus, a more elaborate log-distance tuning as suggested by Perrone <xref ref-type="bibr" rid="pcbi.1002553-Perrone1">[18]</xref> for the length of flow vectors is not necessary. But now, since we estimate the distance of walls this distance tuning is crucial. Perrone's tuning model starts with a transformation of flow vectors from Cartesian into polar coordinates, whereas the radius is associated with the speed of an image location. In this polar representation the matching function is defined as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e095" xlink:type="simple"/><label>(10)</label></disp-formula>The Equation (10) combines the log-speed tuning, the first factor, with the direction tuning, the second factor, by multiplication. The angular difference in Equation (10) is denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e096" xlink:type="simple"/></inline-formula>. This assumes that the two tunings for motion speed and direction are independent <xref ref-type="bibr" rid="pcbi.1002553-Perrone1">[18]</xref>. The already estimated linear velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e097" xlink:type="simple"/></inline-formula> and rotational velocity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e098" xlink:type="simple"/></inline-formula> are used to define self-motion specific flow templates in Equation (10).</p>
        <p>Our extended model can be summarized into the following four steps. First, we compute a wall-ground segmentation by using the tuning functions from Equation (6) and (7). The segmentation is determined by whether a flow vector fits better to a ground template vector from Equation (6) or a wall template vector from Equation (7) while sampling all possible linear velocities in Equation (6) and all possible allocentric directions and possible distances for a wall in Equation (7). Therefore, the segmentation is computed without knowing the parameters of self-motion. But once the segmentation into wall-ground is known we use ground samples to estimate linear and rotational velocity in step two and three, respectively. For estimating linear velocity we use the tuning function from Equation (8) and for rotational velocity the tuning function from Equation (9). In the fourth step, we estimate distance and allocentric direction of walls using the known segmentation, linear, and rotational velocity. A pseudo-code of the algorithm is provided in the <xref ref-type="fig" rid="pcbi-1002553-g009">Figure 9</xref>.</p>
        <fig id="pcbi-1002553-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Shows the pseudo-code for wall-ground segmentation, and estimation of self-motion, distance, and direction of walls.</title>
            <p>The constructor “<monospace>MatrixValueLinear(min,max,num)</monospace>” provides a linear equidistant sampling between “<monospace>min</monospace>” and “<monospace>max</monospace>” of “<monospace>num</monospace>” samples. In contrast, the constructor “<monospace>MatrixValuesLog(min,max,num)</monospace>” implements a logarithmic sampling between “<monospace>min</monospace>” and “<monospace>max</monospace>” with “<monospace>num</monospace>” samples. The function “<monospace>matchGround</monospace>” implements Equation (6) and “<monospace>matchWall</monospace>” implements Equation (7). Both functions are used to compute a wall-ground segmentation. The linear velocity is estimated using Equation (8). Further, “<monospace>matchRotation</monospace>” implements Equation (9) and “<monospace>matchSpeedDirection</monospace>” implements Equation (10). The readout functions “<monospace>readout1D</monospace>” and “<monospace>readout2D</monospace>” are defined in Code-box 3.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g009" xlink:type="simple"/>
        </fig>
        <p>So far, we have not described how velocities, distances, and directions are estimated given the activity from evaluating the residual functions in Equation (8), (9), and (10). Such a description follows.</p>
      </sec>
      <sec id="s4e">
        <title>Read-out of matching functions</title>
        <p>The matching functions in Equation (8), (9), and (10) depend on different stimulus parameters. For instance, the function of Equation (8) depends on linear velocity samples, whereas the function of Equation (10) depends on distance and direction of walls. Our read-out distinguishes between 1D and 2D functions. For a 1D function our read-out method uses a weighted sum with two percent of all argument values that are centered on the maximum. For the 2D match function of Equation (10) we use a different method. Our read-out mechanism selects all matches with their value being within the 70% range with respect to the maximum match. These match values together with their respective arguments, in the above example the linear velocities, are passed along to the distance error calculation or BVC model. The calculation of distance errors takes the direction arguments and computes the ground-truth distance for this direction. Then this ground-truth distance is subtracted from the estimate. The absolute value is computed for this difference to define the distance error.</p>
        <p>If the boundary vector cell model is the next stage, arguments about distance and max read-out directions are passed together with their activation. Distance and direction are integrated into the existing BVC model and we weigh each BVC activity by the match value provided by our template model. This is described in more detail next. A pseudo-code of the 1D and 2D readout method is given in <xref ref-type="fig" rid="pcbi-1002553-g010">Figure 10</xref>.</p>
        <fig id="pcbi-1002553-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>Shows pseudo-code for the interpretation of match values.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g010" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4f">
        <title>Integration of wall's direction and distance estimates into the boundary vector cell model</title>
        <p>The boundary vector cell (BVC) model was described in detail elsewhere <xref ref-type="bibr" rid="pcbi.1002553-Burgess1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Hartley1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002553-Barry2">[17]</xref>. Here, we only repeat the main model equation to show how our estimated variables are integrated. The distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e099" xlink:type="simple"/></inline-formula> with its allocentric direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e100" xlink:type="simple"/></inline-formula> of a wall leads to the activation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e101" xlink:type="simple"/><label>(11)</label></disp-formula>We assume the <italic>i</italic>-th BVC is tuned to the distance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e102" xlink:type="simple"/></inline-formula> and the allocentric direction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e103" xlink:type="simple"/></inline-formula>. Normalized match values of the template model are included in the firing of a BVC. These match values are the third factor of the product in Equation (11). The indices <italic>k</italic> and <italic>j</italic> range over all read out activations from our model that are above 70% of the maximum activity. Parameter values for this Equation (11) and all other equations are reported in <xref ref-type="table" rid="pcbi-1002553-t002">Table 2</xref>.</p>
      </sec>
      <sec id="s4g">
        <title>Detection of drop-offs</title>
        <p>Drop-offs are detected with a center-surround filter applied to the speeds of the flow field. This detection method assumes that the azimuth and elevation angles are arranged on a regular sample grid that is associated with pixels. In our model example we use 400 horizontal samples and 200 vertical samples. To detect the flow discontinuity at the drop-off we apply a center-surround filter kernel to the length of the flow vectors and detect its maximum response. Formally, this is expressed by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e104" xlink:type="simple"/><label>(12)</label></disp-formula>In some cases of small flow discontinuities at the transition from flow at the horizon to the flow of the background this detection does not provide valid values indicated by a small maximum response, see also the pseudo-code in <xref ref-type="fig" rid="pcbi-1002553-g011">Figure 11</xref>.</p>
        <fig id="pcbi-1002553-g011" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002553.g011</object-id>
          <label>Figure 11</label>
          <caption>
            <title>Shows pseudo-code for the method that detects drop-offs.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.g011" xlink:type="simple"/>
        </fig>
        <p>The detected elevation angle of the drop-off is then converted into a distance estimate assuming the eye-height <italic>h</italic>, tilt angle <italic>γ</italic>, and head direction <italic>φ</italic> are known:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e105" xlink:type="simple"/><label>(13)</label></disp-formula></p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002553.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Examples of wall-ground segmentations, distance, and direction estimation of walls for analytical flow superimposed with additive, independent Gaussian noise. Flow with noise is defined as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002553.e106" xlink:type="simple"/></inline-formula> with <italic>N<sub>θ</sub></italic> and <italic>N<sub>φ</sub></italic> drawn from a normal distribution with zero mean and <italic>σ<sub>n</sub></italic> as standard deviation. This type of noise strongly influences the quality of the segmentation and, thus, influences the other estimations based on this segmentation. In both examples the rat is positioned at <italic>x<sub>0</sub></italic> = 20 cm, <italic>y<sub>0</sub></italic> = 3.5 cm, <italic>z<sub>0</sub></italic> = 15 cm, has the head direction <italic>φ</italic> = 23°, and the self-motion <italic>v<sub>z</sub></italic> = 19 cm/sec and <italic>ω<sub>y</sub></italic> = 25°/sec. <bold>a</bold>) Square box with the coordinate system. The distance estimates are indicted by magenta arrows. <bold>b</bold>) Spherical flow field with noise (<italic>σ<sub>n</sub></italic> = 5°/sec) and wall-ground segmentation. <bold>c</bold>) 2D match values as calculated from samples that are indicated as originating from a wall, these are the blue dots in b). Low match values are encoded by low intensity and high match values by a high intensity, see also the inset with the color code. <bold>d–g</bold>) 1D match values for the linear and rotational self-motion and the direction and distance of walls. The latter two curves are computed as the maximum response from the 2D match value function of c), whereas the maximum is computed for the dimension not shown. The estimated self-motion is <italic>v<sub>z,est</sub></italic> = 19 cm/sec and <italic>ω<sub>y,est</sub></italic> = 24°/sec and the mean distance error is 1.15 cm with a standard deviation of 0.52 cm. <bold>h</bold>) Circular box with coordinate system and distance estimates of the wall depicted by the magenta arrows. <bold>i</bold>) Spherical flow with noise (<italic>σ<sub>n</sub></italic> = 15°/sec) and wall-ground segmentation for this circular box. <bold>j</bold>) 2D match values. <bold>k–n</bold>) Match value functions for velocities of the camera and direction and distance of walls. The estimated self-motion is <italic>v<sub>z,est</sub></italic> = 19 cm/sec and <italic>ω<sub>y,est</sub></italic> = 25°/sec and the mean distance error is 3.03 cm with a standard deviation of 1.29 cm. The distance error in the circular cage is higher due to the assumption about a planar approximation for each segment of the curved wall.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002553.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>Taking the logarithm of the speeds of input and template flow vectors as the argument for a Gaussian function provides a “sharper” tuning compared to taking only the difference of speeds without the logarithm. <bold>a</bold>) Shows the spherical flow field and the detected wall-ground segmentation. <bold>b</bold>) 2D match values for the matching using the log-speed difference shows three clearly distinctive high intensity regions. These are better visible in the break-down into 1D curves <bold>c</bold>) for the wall's direction taking the maximum of all distances and <bold>d</bold>) the wall's distance taking the maximum of all directions. The mean error in distance estimates is 2.76 cm with a standard deviation of 1.81 cm. <bold>e</bold>) 2D match values for the matching using the speed difference without applying the logarithm. Compared to b) the matching occurs fuzzy, also visible in the break-down in the matching for <bold>f</bold>) the walls' directions and <bold>g</bold>) the walls' distances. In this case the mean distance error is 3.74 cm with a standard deviation of 3.85 cm and, thus, higher than in b). This example uses a rectangular box 250 cm×280 cm with 50 cm high walls and the rat's position is <italic>x<sub>0</sub></italic> = 0 cm, <italic>y<sub>0</sub></italic> = 3.5 cm, <italic>z<sub>0</sub></italic> = 10 cm with the head direction <italic>φ</italic> = 15°.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002553.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002553.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p>A tilt angle of 30°, that is the angle between the optical axes compared to the horizontal parallel to the ground, does no change firing fields qualitatively; however, the error at larger distances is larger compared to a zero tilt angle. <bold>a</bold>) Simulated BVC firing for a square box with distance and direction estimates for walls provided from our template model. <bold>b</bold>) Mean distance error in the two-centimeter range. <bold>c</bold>) Simulated BVC firing for the same square box as in a) but with an additional intrinsic wall. Firing appears at any wall of a specific distance and allocentric direction the cell is tuned for. <bold>d</bold>) Mean distance error in the range of zero to six centimeters. <bold>e</bold>) Simulated BVC firing for a circular box. <bold>f</bold>) Mean distance error in the two-centimeter range.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors appreciate the time and effort of two anonymous reviewers and their helpful comments. Special thanks go to Ennio Mingolla for helpful comments during drafting figures and discussions of the model and to Andrew Browning for comments on an earlier version of the manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002553-OKeefe1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name></person-group>             <year>1996</year>             <article-title>Geometric determinants of the place fields of hippocampal neurons.</article-title>             <source>Nature</source>             <volume>381</volume>             <fpage>425</fpage>             <lpage>428</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Barry1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barry</surname><given-names>C</given-names></name><name name-style="western"><surname>Lever</surname><given-names>C</given-names></name><name name-style="western"><surname>Hayman</surname><given-names>R</given-names></name><name name-style="western"><surname>Hartley</surname><given-names>T</given-names></name><name name-style="western"><surname>Burton</surname><given-names>S</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>The boundary vector cell model of place cell firing and spatial memory.</article-title>             <source>Rev Neurosci</source>             <volume>17</volume>             <fpage>71</fpage>             <lpage>97</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Lever1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lever</surname><given-names>C</given-names></name><name name-style="western"><surname>Burton</surname><given-names>S</given-names></name><name name-style="western"><surname>Jeewajee</surname><given-names>A</given-names></name><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name></person-group>             <year>2009</year>             <article-title>Boundary vector cells in the subiculum of the hippocampal formation.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>9771</fpage>             <lpage>9777l</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Solstad1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Solstad</surname><given-names>T</given-names></name><name name-style="western"><surname>Boccara</surname><given-names>CN</given-names></name><name name-style="western"><surname>Kropff</surname><given-names>E</given-names></name><name name-style="western"><surname>Moser</surname><given-names>MB</given-names></name><name name-style="western"><surname>Moser</surname><given-names>EI</given-names></name></person-group>             <year>2008</year>             <article-title>Representation of geometric borders in the entorhinal cortex.</article-title>             <source>Science</source>             <volume>332</volume>             <fpage>1865</fpage>             <lpage>1868</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Burgess1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name><name name-style="western"><surname>Donnett</surname><given-names>JG</given-names></name><name name-style="western"><surname>Jeffery</surname><given-names>KJ</given-names></name><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name></person-group>             <year>1997</year>             <article-title>Robotic and neuronal simulation of the hippocampus and rat navigation.</article-title>             <source>Phil Trans R Soc Lond B</source>             <volume>352</volume>             <fpage>1535</fpage>             <lpage>1543</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Hartley1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hartley</surname><given-names>T</given-names></name><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name><name name-style="western"><surname>Lever</surname><given-names>C</given-names></name><name name-style="western"><surname>Cacucci</surname><given-names>F</given-names></name><name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name></person-group>             <year>2000</year>             <article-title>Modeling place fields in terms of the cortical inputs to the hippocampus.</article-title>             <source>Hippocampus</source>             <volume>10</volume>             <fpage>269</fpage>             <lpage>379</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Savelli1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Savelli</surname><given-names>F</given-names></name><name name-style="western"><surname>Yoganarasimha</surname><given-names>D</given-names></name><name name-style="western"><surname>Knierim</surname><given-names>JJ</given-names></name></person-group>             <year>2008</year>             <article-title>Influence of boundary removal on the spatial representation of the medial entorhinal cortex.</article-title>             <source>Hippocampus</source>             <volume>18</volume>             <fpage>1270</fpage>             <lpage>1282</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Burne1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burne</surname><given-names>RA</given-names></name><name name-style="western"><surname>Parnavelas</surname><given-names>JG</given-names></name><name name-style="western"><surname>Lin</surname><given-names>CS</given-names></name></person-group>             <year>1984</year>             <article-title>Response properties of neurons in the visual cortex of the rat.</article-title>             <source>Exp Brain Res</source>             <volume>53</volume>             <fpage>374</fpage>             <lpage>383</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Girman1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Girman</surname><given-names>SV</given-names></name><name name-style="western"><surname>Sauve</surname><given-names>Y</given-names></name><name name-style="western"><surname>Lund</surname><given-names>RD</given-names></name></person-group>             <year>1999</year>             <article-title>Receptive field properties of single neurons in rat primary visual cortex.</article-title>             <source>J Neurophysiol</source>             <volume>82</volume>             <fpage>301</fpage>             <lpage>311</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Coogan1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Coogan</surname><given-names>TA</given-names></name><name name-style="western"><surname>Burkhalter</surname><given-names>A</given-names></name></person-group>             <year>1993</year>             <article-title>Hierarchical organization of areas in rat visual cortex.</article-title>             <source>J Neurosci</source>             <volume>13</volume>             <fpage>3749</fpage>             <lpage>3772</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Tanaka1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name><name name-style="western"><surname>Saito</surname><given-names>H</given-names></name></person-group>             <year>1989</year>             <article-title>Analysis of motion of the visual field by direction, expansion/contraction, and rotation cells clustered in the dorsal part of the medial superior temporal area of the macaque monkey.</article-title>             <source>J Neurophysiol</source>             <volume>62</volume>             <fpage>626</fpage>             <lpage>641</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Graziano1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Graziano</surname><given-names>MSA</given-names></name><name name-style="western"><surname>Andersen</surname><given-names>RA</given-names></name><name name-style="western"><surname>Snowden</surname><given-names>RJ</given-names></name></person-group>             <year>1994</year>             <article-title>Tuning of MST neurons to spiral motions.</article-title>             <source>J Neurosci</source>             <volume>14</volume>             <fpage>54</fpage>             <lpage>67</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Duffy1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Duffy</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Wurtz</surname><given-names>RH</given-names></name></person-group>             <year>1995</year>             <article-title>Response of monkey MST neurons to optic flow stimuli with shifted centers of motion.</article-title>             <source>J Neurosci</source>             <volume>15</volume>             <fpage>5192</fpage>             <lpage>5208</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Duffy2">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Duffy</surname><given-names>CJ</given-names></name></person-group>             <year>1998</year>             <article-title>MST neurons respond to optic flow and translational movement.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>1816</fpage>             <lpage>1827</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Simpson1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Simpson</surname><given-names>JI</given-names></name></person-group>             <year>1984</year>             <article-title>The accessory optic system.</article-title>             <source>Annu Rev Neurosci</source>             <volume>7</volume>             <fpage>13</fpage>             <lpage>41</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Wylie1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wylie</surname><given-names>RWD</given-names></name><name name-style="western"><surname>Glover</surname><given-names>RG</given-names></name><name name-style="western"><surname>Aitchison</surname><given-names>JD</given-names></name></person-group>             <year>1999</year>             <article-title>Optic flow input to the hippocampal formation from the accessory optic system.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>5514</fpage>             <lpage>5527</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Barry2">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barry</surname><given-names>C</given-names></name><name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name></person-group>             <year>2007</year>             <article-title>Learning in a geometric model of place cell firing.</article-title>             <source>Hippocampus</source>             <volume>17</volume>             <fpage>786</fpage>             <lpage>800</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Perrone1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Perrone</surname><given-names>J</given-names></name></person-group>             <year>1992</year>             <article-title>Model for the computation of self-motion in biological systems.</article-title>             <source>J Opt Soc Am A</source>             <volume>9</volume>             <fpage>177</fpage>             <lpage>192</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Perrone2">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Perrone</surname><given-names>J</given-names></name><name name-style="western"><surname>Stone</surname><given-names>LS</given-names></name></person-group>             <year>1994</year>             <article-title>A model of self-motion estimation within primate extrastriate visual cortex.</article-title>             <source>Vision Res</source>             <volume>34</volume>             <fpage>2917</fpage>             <lpage>2938</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Lappe1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lappe</surname><given-names>M</given-names></name><name name-style="western"><surname>Rauschecker</surname><given-names>J</given-names></name></person-group>             <year>1993</year>             <article-title>A neuronal network for the processing of optic flow from ego-motion in man and higher mammels.</article-title>             <source>Neural Comput</source>             <volume>5</volume>             <fpage>374</fpage>             <lpage>391</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Lappe2">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lappe</surname><given-names>M</given-names></name></person-group>             <year>1998</year>             <article-title>A model of the combination of optic flow and extraretinal eye movement signals in primate extrastriate visual cortex - Neural model of self-motion from optic flow and extraretinal cues.</article-title>             <source>Neural Netw</source>             <volume>11</volume>             <fpage>397</fpage>             <lpage>414</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Hafting1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hafting</surname><given-names>T</given-names></name><name name-style="western"><surname>Fyhn</surname><given-names>M</given-names></name><name name-style="western"><surname>Molden</surname><given-names>S</given-names></name><name name-style="western"><surname>Moser</surname><given-names>MB</given-names></name><name name-style="western"><surname>Moser</surname><given-names>IM</given-names></name></person-group>             <year>2005</year>             <article-title>Microstructure of a spatial map in the entorhinal cortex.</article-title>             <source>Nature</source>             <volume>436</volume>             <fpage>801</fpage>             <lpage>806</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Sargolini1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sargolini</surname><given-names>F</given-names></name><name name-style="western"><surname>Fyhn</surname><given-names>M</given-names></name><name name-style="western"><surname>Hafting</surname><given-names>T</given-names></name><name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name><name name-style="western"><surname>Witter</surname><given-names>MP</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>Conjunctive representation of position, direction, and velocity in entorhinal cortex.</article-title>             <source>Science</source>             <volume>312</volume>             <fpage>758</fpage>             <lpage>726</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Taube1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Taube</surname><given-names>JS</given-names></name><name name-style="western"><surname>Muller</surname><given-names>RU</given-names></name><name name-style="western"><surname>Ranck</surname><given-names>JB</given-names><suffix>Jr</suffix></name></person-group>             <year>1990</year>             <article-title>Head direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis.</article-title>             <source>J Neurosci</source>             <volume>10</volume>             <fpage>420</fpage>             <lpage>435</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Royden1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Royden</surname><given-names>CS</given-names></name></person-group>             <year>1997</year>             <article-title>Mathematical analysis of motion-opponent mechanisms used in the determination of heading and depth.</article-title>             <source>J Opt Soc Am A Opt Image Sci Vis</source>             <volume>14</volume>             <fpage>2128</fpage>             <lpage>2143</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Muller1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Muller</surname><given-names>RU</given-names></name><name name-style="western"><surname>Kubie</surname><given-names>JL</given-names></name></person-group>             <year>1987</year>             <article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells.</article-title>             <source>J Neurosci</source>             <volume>7</volume>             <fpage>1951</fpage>             <lpage>1968</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Jeffery1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jeffery</surname><given-names>KJ</given-names></name><name name-style="western"><surname>O'Keefe</surname><given-names>JM</given-names></name></person-group>             <year>1999</year>             <article-title>Learned interaction of visual and idiothetic cues in the control of place field orientation.</article-title>             <source>Exp Brain Res</source>             <volume>127</volume>             <fpage>151</fpage>             <lpage>161</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Knierim1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Knierim</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Kudrimoti</surname><given-names>HS</given-names></name><name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name></person-group>             <year>1998</year>             <article-title>Interactions between idiothetic cues and external landmarks in the control of place cells and head direction cells.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>425</fpage>             <lpage>446</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Hill1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hill</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Best</surname><given-names>PJ</given-names></name></person-group>             <year>1981</year>             <article-title>Effects of deafness and blindness on the spatial correlates of hippocampal unit activity in the rat.</article-title>             <source>Exp Neurology</source>             <volume>74</volume>             <fpage>204</fpage>             <lpage>217</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Miller1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>VM</given-names></name><name name-style="western"><surname>Best</surname><given-names>PJ</given-names></name></person-group>             <year>1980</year>             <article-title>Spatial correlates of hippocampal unit activity are altered by lesions of the fornix and entorhinal cortex.</article-title>             <source>Brain Res</source>             <volume>194</volume>             <fpage>311</fpage>             <lpage>323</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Sharp1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sharp</surname><given-names>PE</given-names></name><name name-style="western"><surname>Blair</surname><given-names>HT</given-names></name><name name-style="western"><surname>Etkin</surname><given-names>D</given-names></name><name name-style="western"><surname>Tzanetos</surname><given-names>DB</given-names></name></person-group>             <year>1995</year>             <article-title>Influences of vestibular and visual motion information on the spatial firing patterns of hippocampal place cells.</article-title>             <source>J of Neurosci</source>             <volume>15</volume>             <fpage>173</fpage>             <lpage>189</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Wallace1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wallace</surname><given-names>DG</given-names></name><name name-style="western"><surname>Hinse</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Pellis</surname><given-names>SM</given-names></name><name name-style="western"><surname>Whishaw</surname><given-names>IQ</given-names></name></person-group>             <year>2002</year>             <article-title>Vestibular information is required for dead reckoning in the rat.</article-title>             <source>J of Neurosci</source>             <volume>22</volume>             <fpage>10009</fpage>             <lpage>10017</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Brown1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>JE</given-names></name><name name-style="western"><surname>Yates</surname><given-names>BJ</given-names></name><name name-style="western"><surname>Taube</surname><given-names>JS</given-names></name></person-group>             <year>2002</year>             <article-title>Does the vestibular system contribute to head direction cell activity in the rat?</article-title>             <source>Physiol Behavi</source>             <volume>77</volume>             <fpage>743</fpage>             <lpage>748</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Best1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Best</surname><given-names>PJ</given-names></name><name name-style="western"><surname>White</surname><given-names>AM</given-names></name><name name-style="western"><surname>Miani</surname><given-names>A</given-names></name></person-group>             <year>2001</year>             <article-title>Spatial processing in the brain: The activity of hippocampal place cells.</article-title>             <source>Ann Rev Neurosci</source>             <volume>24</volume>             <fpage>459</fpage>             <lpage>486</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Gothard1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gothard</surname><given-names>KM</given-names></name><name name-style="western"><surname>Skaggs</surname><given-names>WE</given-names></name><name name-style="western"><surname>McNaughton</surname><given-names>L</given-names></name></person-group>             <year>1996</year>             <article-title>Dynamics of mismatch correction in the hippocampal ensemble code of space: Interaction between path integration and environmental cues.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>8027</fpage>             <lpage>8040</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Fischler1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fischler</surname><given-names>M</given-names></name><name name-style="western"><surname>Bolles</surname><given-names>R</given-names></name></person-group>             <year>1981</year>             <article-title>Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography.</article-title>             <source>Com of the ACM</source>             <volume>24</volume>             <fpage>381</fpage>             <lpage>395</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Boyd1">
        <label>37</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Boyd</surname><given-names>S</given-names></name><name name-style="western"><surname>Vandenberghe</surname><given-names>L</given-names></name></person-group>             <year>2004</year>             <source>Convex Optimization</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Dempster1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dempster</surname><given-names>A</given-names></name><name name-style="western"><surname>Laird</surname><given-names>N</given-names></name><name name-style="western"><surname>Rubin</surname><given-names>D</given-names></name></person-group>             <year>1977</year>             <article-title>Maximum likelihood from incomplete data via the EM algorithm.</article-title>             <source>J Roy Stat Soc, Ser B</source>             <volume>39</volume>             <fpage>1</fpage>             <lpage>38</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Raudies1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Raudies</surname><given-names>F</given-names></name><name name-style="western"><surname>Mingolla</surname><given-names>E</given-names></name><name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group>             <year>2012</year>             <article-title>Modeling the influence of optic flow on grid cell firing in the absence of other cues.</article-title>             <source>J Comput Neurosci</source>             <comment>E-pub ahead of print</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Brandon1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brandon</surname><given-names>MP</given-names></name><name name-style="western"><surname>Bogaard</surname><given-names>AR</given-names></name><name name-style="western"><surname>Libby</surname><given-names>CP</given-names></name><name name-style="western"><surname>Connerney</surname><given-names>MA</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>K</given-names></name><etal/></person-group>             <year>2011</year>             <article-title>Reduction of theta rhythm dissociates grid cell spatial periodicity from directional tuning.</article-title>             <source>Science</source>             <volume>332</volume>             <fpage>595</fpage>             <lpage>599</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Koenig1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koenig</surname><given-names>J</given-names></name><name name-style="western"><surname>Linder</surname><given-names>AN</given-names></name><name name-style="western"><surname>Leutgeb</surname><given-names>JK</given-names></name><name name-style="western"><surname>Leutget</surname><given-names>S</given-names></name></person-group>             <year>2011</year>             <article-title>The spatial periodicity of grid cells is not sustained during reduced theta oscillations.</article-title>             <source>Science</source>             <volume>332</volume>             <fpage>592</fpage>             <lpage>595</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Warren1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>WH</given-names></name><name name-style="western"><surname>Kay</surname><given-names>BA</given-names></name><name name-style="western"><surname>Zosh</surname><given-names>WD</given-names></name><name name-style="western"><surname>Duchon</surname><given-names>AP</given-names></name><name name-style="western"><surname>Sahuc</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>Optic flow is used to control human walking.</article-title>             <source>Nat Neurosci</source>             <volume>4</volume>             <fpage>213</fpage>             <lpage>216</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Harvey1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Harvey</surname><given-names>CD</given-names></name><name name-style="western"><surname>Collman</surname><given-names>F</given-names></name><name name-style="western"><surname>Dombeck</surname><given-names>DA</given-names></name><name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name></person-group>             <year>2009</year>             <article-title>Intracellular dynamics of hippocampal place cells during virtual navigation.</article-title>             <source>Nature</source>             <volume>461</volume>             <fpage>941</fpage>             <lpage>946</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Rieger1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rieger</surname><given-names>JH</given-names></name></person-group>             <year>1983</year>             <article-title>Information in optical flows induced by curved paths of observation.</article-title>             <source>J Opt Soc Am</source>             <volume>73</volume>             <fpage>339</fpage>             <lpage>344</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Calow1">
        <label>45</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Calow</surname><given-names>D</given-names></name><name name-style="western"><surname>Krüger</surname><given-names>N</given-names></name><name name-style="western"><surname>Wörgötter</surname><given-names>F</given-names></name></person-group>             <year>2004</year>             <article-title>Statistics of optic flow for self-motion through natural scenes.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Ilg</surname><given-names>UJ</given-names></name><name name-style="western"><surname>Blüthoff</surname><given-names>HH</given-names></name><name name-style="western"><surname>Mallot</surname><given-names>HA</given-names></name></person-group>             <source>Proceedings of Dynamic Perception</source>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>IOS Press, Akademische Verlagsgesellschaft Aka GmBH</publisher-name>             <fpage>133</fpage>             <lpage>138</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-Goldstein1">
        <label>46</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Goldstein</surname><given-names>H</given-names></name><name name-style="western"><surname>Poole</surname><given-names>C</given-names></name><name name-style="western"><surname>Safko</surname><given-names>J</given-names></name></person-group>             <year>2001</year>             <source>Classical mechanics. 3rd edition</source>             <publisher-name>Addison Wesley</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002553-LonguetHiggins1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Longuet-Higgins</surname><given-names>H</given-names></name><name name-style="western"><surname>Prazdny</surname><given-names>K</given-names></name></person-group>             <year>1980</year>             <article-title>The interpretation of a moving retinal image.</article-title>             <source>Proc R Soc Lond B Biol Sci</source>             <volume>208</volume>             <fpage>385</fpage>             <lpage>397</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>