<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004316</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-02095</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Emerging Object Representations in the Visual System Predict Reaction Times for Categorization</article-title>
<alt-title alt-title-type="running-head">Emerging Object Representations Predict Reaction Times</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ritchie</surname>
<given-names>J. Brendan</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tovar</surname>
<given-names>David A.</given-names>
</name>
<xref rid="aff004" ref-type="aff"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Carlson</surname>
<given-names>Thomas A.</given-names>
</name>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Philosophy, University of Maryland, College Park, Maryland, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Perception in Action Research Centre, Department of Cognitive Science, Macquarie University, Sydney, New South Wales, Australia</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>ARC Centre of Excellence in Cognition and its Disorders, Macquarie University, Sydney, New South Wales, Australia</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>School of Medicine, Vanderbilt University, Nashville, Tennessee, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname>
<given-names>Matthias</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: TAC DAT. Performed the experiments: JBR DAT. Analyzed the data: JBR DAT TAC. Wrote the paper: JBR TAC.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">j.brendan.w.ritchie@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>6</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>6</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>6</issue>
<elocation-id>e1004316</elocation-id>
<history>
<date date-type="received">
<day>24</day>
<month>11</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>4</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Ritchie et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004316" xlink:type="simple"/>
<abstract>
<p>Recognizing an object takes just a fraction of a second, less than the blink of an eye. Applying multivariate pattern analysis, or “brain decoding”, methods to magnetoencephalography (MEG) data has allowed researchers to characterize, in high temporal resolution, the emerging representation of object categories that underlie our capacity for rapid recognition. Shortly after stimulus onset, object exemplars cluster by category in a high-dimensional activation space in the brain. In this emerging activation space, the decodability of exemplar category varies over time, reflecting the brain’s transformation of visual inputs into coherent category representations. How do these emerging representations relate to categorization behavior? Recently it has been proposed that the distance of an exemplar representation from a categorical boundary in an activation space is critical for perceptual decision-making, and that reaction times should therefore correlate with distance from the boundary. The predictions of this distance hypothesis have been born out in human inferior temporal cortex (IT), an area of the brain crucial for the representation of object categories. When viewed in the context of a time varying neural signal, the optimal time to “read out” category information is when category representations in the brain are most decodable. Here, we show that the distance from a decision boundary through activation space, as measured using MEG decoding methods, correlates with reaction times for visual categorization during the period of peak decodability. Our results suggest that the brain begins to read out information about exemplar category at the optimal time for use in choice behaviour, and support the hypothesis that the structure of the representation for objects in the visual system is partially constitutive of the decision process in recognition.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Recognizing an object in the world (e.g. a cat) takes just a fraction of a second. Recent advances in neuroscience have allowed researchers to measure the emergence and dynamics of time-varying neural signals that allow us to quickly recognize objects visually. At each moment in time, these neural signals can be characterized as patterns of neural activity that cluster categorically in a high-dimensional activation space. Within this space, a boundary can be drawn between the clusters of activity patterns, which can then be used to discriminate object categories (e.g. cats vs. cars). Based on which side of the boundary a pattern falls, researchers can guess (or “decode”) the category membership of an object that an observer is viewing. In the present study, we provide evidence that at the time when category clusters are best separated in activation space—that is, the time when category information is best suited to be “read out” from the brain’s signals—the structure of activation space can be used to predict behaviour. Our results provide insights into both when and how the brain’s representational architecture supports rapid object recognition.</p>
</abstract>
<funding-group>
<funding-statement>This research was supported by an Australian Research Council Future Fellowship, FT120100816 (TAC). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>Files containing processed and analyzed MEG data, as well as summary statistics for the behavioral data, for each subject, are available through DRYAD. DOI: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5061/dryad.fv8g8" xlink:type="simple">10.5061/dryad.fv8g8</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>When recognizing objects the brain does not take its time. Although object recognition is one of the most computationally difficult feats performed by the visual system, it is carried out in an ultra-rapid fashion, with relative ease and high fidelity [<xref rid="pcbi.1004316.ref001" ref-type="bibr">1</xref>–<xref rid="pcbi.1004316.ref002" ref-type="bibr">2</xref>]. Some of the most convincing evidence for ultra-rapid recognition comes from behavioural research. Using saccadic eye-movements, subjects can reliably categorize object exemplars as quickly as 120 ms post-stimulus onset [<xref rid="pcbi.1004316.ref003" ref-type="bibr">3</xref>–<xref rid="pcbi.1004316.ref004" ref-type="bibr">4</xref>] and faces as fast as 100 ms [<xref rid="pcbi.1004316.ref005" ref-type="bibr">5</xref>]. The rapidity of saccadic reaction times for categorization suggests that information about stimulus category must be available in the brain very shortly after stimulus onset [<xref rid="pcbi.1004316.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1004316.ref006" ref-type="bibr">6</xref>].</p>
<p>The application of multi-variate pattern analysis (MVPA), or “decoding”, methods to time-series data has allowed researchers to characterize the emergence and dynamics of time-varying neuronal activity associated with objects in the brain. Concordant with the early availability of category information observed in behavioural research, information about object category can be decoded as early as 60–100 ms after stimulus onset [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref010" ref-type="bibr">10</xref>]. While decoding onset is largely stable across categories, MEG decoding studies have found that peak decoding varies with the hierarchical organization of object categories; classifiers trained to discriminate subordinate categories (e.g. face, body) peak earlier in their performance relative to ones trained to discriminate superordinate categories (e.g. animacy) [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>]. These results suggest that peak decoding indexes the optimal time to read out information about stimulus categories [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>], and that time-resolved decoding provides a method for revealing the representational dynamics of the brain [<xref rid="pcbi.1004316.ref011" ref-type="bibr">11</xref>].</p>
<p>The classifiers used in decoding analysis rely on boundaries through high-dimensional activation spaces that separate patterns of activity for object exemplars based on category (e.g. faces vs. houses). Classifier performance is better when an activation space is organized along clear categorical dimensions. For example, object category is highly decodable from the activity patterns in human and monkey inferior temporal cortex (IT), a region strongly implicated in representing object categories [<xref rid="pcbi.1004316.ref012" ref-type="bibr">12</xref>], but only moderately decodable in early visual cortex [<xref rid="pcbi.1004316.ref013" ref-type="bibr">13</xref>], which encodes low-level image features (e.g. edges and colors). With time-resolved decoding, classifier performance improves at time points when activation spaces are better organized along categorical dimensions. If these spaces indeed reflect underlying representational dynamics, then an important question is when and how these emergent activation spaces are used by the brain in a task related manner.</p>
<p>One approach to forging a link between decodability and behaviour holds that a boundary that separates object exemplars based on category membership (e.g. animate vs. inanimate) in an activation space reflects a <italic>decision</italic> boundary for behavioural categorization (<xref rid="pcbi.1004316.g001" ref-type="fig">Fig 1A</xref>; [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>]). Signal detection theory [<xref rid="pcbi.1004316.ref015" ref-type="bibr">15</xref>] suggests that evidence close to a decision boundary is more ambiguous, while evidence far from the boundary is less ambiguous. Since decision time tends to increase with the quality of evidence for an observer, and ambiguity is one dimension of evidence quality, a simple consequence of this familiar picture from classic psychophysics is that reaction times will correlate negatively with distance from a decision boundary: the farther an object representation is from the decision boundary through the space, the less ambiguous the evidence, and the faster the reaction times [<xref rid="pcbi.1004316.ref016" ref-type="bibr">16</xref>–<xref rid="pcbi.1004316.ref018" ref-type="bibr">18</xref>]. Using fMRI, Carlson et al. [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>] tested this distance hypothesis using an activation-space for objects constructed from patterns of activity in human IT. They found that distance from a categorical boundary for animacy through the activation space of the region negatively correlated with RTs, suggesting that object representations form part of the decision-process for visual categorization (cf. [<xref rid="pcbi.1004316.ref019" ref-type="bibr">19</xref>]).</p>
<fig id="pcbi.1004316.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Distances from a decision boundary through activation space can be used to predict reaction times (RT).</title>
<p>(A) A hypothetical 2D activation space for human IT representing animate and inanimate object exemplars. Activation patterns for individual exemplars are projected onto a discriminant axis, which differentiates patterns based on animacy. A decision boundary placed along the axis allows for classification of animate and inanimate exemplars. Gaussian distributions along the discriminant axis reflect “decision noise”. Exemplar representations closer to the boundary produce more ambiguous evidence compared to exemplar representations far from the boundary. An implication of classic signal detection theory is that RTs will correlate negatively with distance from the boundary. (B) A hypothetical emergent activation space for animate vs. inanimate object exemplars as would be revealed using MEG decoding methods. Stimulus onset is the time the stimulus is presented. The decoding onset (dashed line) is the first time point that a classifier trained to discriminate between animate and inanimate examplars performs significantly above chance. Peak decoding (gray box) is the optimal time point to read out information about stimulus category. Clusters depict the hypothetical 2D activation spaces at notable points in the decoding time-course.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g001" position="float" xlink:type="simple"/>
</fig>
<p>In the present study, we show <italic>when</italic> distances from a boundary through a high-dimensional activation space is predictive of reaction times, in order to reveal a link between the time-varying signals revealed by MEG decoding and behaviour. If peak decoding does indeed index the optimal time to read-out information about object category, then a plausible hypothesis is that peak decoding is the time at which the brain constructs a representation of the stimulus that is predictive of reaction time behaviour (<xref rid="pcbi.1004316.g001" ref-type="fig">Fig 1B</xref>). We tested this hypothesis by examining the relationship between emergent activation spaces for objects in the brain, measured using MEG, and reaction times for object categorization. Controlling for potential task-related and motor confounds, our study shows that reaction times begin to correlate with representational distances during peak decoding, and that the relationship between representational distance and reaction times in general follows the time-course of decoding. Our results provide support for the hypothesis that the brain reads out information when sensory evidence is optimal for making a decision about object category.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Subjects were shown a series of images while their brain activity was recorded using MEG. Each image depicted either an animate or inanimate object from a set of twenty-four object exemplars (<xref rid="pcbi.1004316.g002" ref-type="fig">Fig 2</xref>). Superimposed onto each image was a letter at fixation. We focused on the distinction between animate vs. inanimate exemplars because it is reliably decoded with MEG [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>], and because it was the same distinction relied on in previous work that measured the correlation between representational distance and reaction time [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>]. In separate blocks of trials subjects either actively categorized images as animate or inanimate (<italic>categorization</italic> task), or responded whether the letter at fixation was a vowel or consonant (<italic>distracted viewing</italic> task), while passively viewing the exemplar stimuli (<xref rid="pcbi.1004316.g002" ref-type="fig">Fig 2</xref>).</p>
<fig id="pcbi.1004316.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Behavioral paradigm.</title>
<p>Trial structure for the two experimental tasks. On each trial a letter (vowel or consonant) was superimposed on the fixation circle in the centre of each object exemplar. When performing the <italic>categorization</italic> task subjects judged whether the exemplar stimulus was animate or inanimate, while during the <italic>distracted viewing</italic> task subjects judged whether the letter was a vowel or consonant. After subjects responded the fixation circle briefly flashed green (correct response) or red (incorrect response or no response) to provide trial-by-trial feedback on performance.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g002" position="float" xlink:type="simple"/>
</fig>
<sec id="sec003">
<title>Behavioral performance</title>
<p>Reaction time (RT) and choice behaviour were recorded on each trial during the scanning session. Overall subjects performed well at both tasks. Mean accuracy for the categorization task was 91.8% (<italic>SD</italic> = 5.6%) correct, and 87.2% (<italic>SD</italic> = 8.5%) correct for the distracted viewing task. The mean RTs were 469 ms (<italic>SD</italic> = 82 ms) for the categorization task, and 516 ms (<italic>SD</italic> = 96 ms) for the distracted viewing task.</p>
</sec>
<sec id="sec004">
<title>Decoding animacy from MEG time series</title>
<p>We first replicated previous studies showing that animate vs. inanimate object exemplars can be decoded on a trial-by-trial basis from the MEG time-series data [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>]. Linear discriminate analysis (LDA; [<xref rid="pcbi.1004316.ref020" ref-type="bibr">20</xref>]) was used at each time point to classify the category of the exemplar (animate or inanimate) displayed to the subject. <xref rid="pcbi.1004316.g003" ref-type="fig">Fig 3</xref> shows decoding performance as a function of time for the MEG data from the categorization and distracted viewing tasks epoched from -100 to 600 ms post-stimulus onset (reported in <italic>d’</italic>). Performance was similar for both data sets. The decoding onset (i.e. the first time point at which decoding is above chance) was 60 ms post-stimulus for both tasks. Following the onset, there was a broad peak in decoding with two smaller peaks. The first smaller peak at 140 ms was the same for both task data sets. The second smaller peak occurred at 220 and 240 ms for the distracted viewing and categorization task respectively. Our main theoretical interest was the time of peak decoding–the optimal time for decision information to be “read out” from the brain’s representation of the stimulus. To define this time-period, we calculated the mean decoding performance for the two smaller of peaks (<italic>d’</italic> = .61), and then tested all time points to determine those that were not significantly different from this mean peak value (see <xref rid="sec013" ref-type="sec">Methods</xref>). The time points from 120–240 ms were not significantly different from the peak value. In what follows, we will refer to this period as the <italic>period of peak decoding</italic>, which is highlighted as a grey region in <xref rid="pcbi.1004316.g003" ref-type="fig">Fig 3</xref>.</p>
<fig id="pcbi.1004316.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g003</object-id>
<label>Fig 3</label>
<caption>
<title>MEG decoding for animacy.</title>
<p>Mean classifier performance (<italic>d’</italic>) for both the categorization task (red) and distracted viewing task data (orange) plotted over time. Shaded regions above and below the mean lines indicate +/- 1 <italic>SEM</italic> across subjects. Color-coded asterisks indicate time points at which classifier performance was significantly above chance, using a Wilcoxon signed rank test (* = false discovery rate (FDR) adjusted <italic>p</italic> &lt; 0.05). Decoding onset for both data sets (60ms) is indicated by a dashed vertical line. The period of peak decoding is indicated by a gray box extending 120–240 ms post-stimulus onset. The bar along the x-axis indicates the stimulus duration.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g003" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Representational distance predicts reaction times for categorization throughout the decoding time-course</title>
<p>We sought to test whether distance from a decision boundary through an activation space constructed at the period of peak decoding would predict RTs for categorization. For each time point we constructed a multidimensional activation space from the MEG sensor data (see <xref rid="sec013" ref-type="sec">Methods</xref>). In the activation space, the average activity patterns of individual exemplar representations were the exemplars’ coordinates. LDA was used to calculate a discriminate axis and decision boundary for animacy through this space. The individual exemplar activation patterns were then projected onto the discriminate axis, and we computed the distance of the exemplar pattern from the decision boundary (see <xref rid="pcbi.1004316.g001" ref-type="fig">Fig 1A and 1B</xref>). The procedure was done separately for the categorization and distracted viewing task MEG data sets, yielding “representational distances” (one for each object exemplar) for each task at each time point.</p>
<p>To study the relationship between RT behaviour and the emerging representation of the stimuli in the brain, the time varying distances of the object exemplars from the representational boundary for animacy were correlated with RT performance on the categorization task. To reduce noise and increase statistical power, we performed a fixed-effect analysis, utilizing the average representational distances at each time-point and normalized median RTs across subjects. Based on our primary hypothesis, we predicted that RTs for the categorization task would correlate negatively with representational distances during the period of peak decoding. Contrary to our expectations, the correlation time-course was significant at multiple time-points from decoding onset onwards (<xref rid="pcbi.1004316.g004" ref-type="fig">Fig 4</xref>). Several of these time points were before (&lt; 120 ms) or after (&gt; 240 ms) peak decoding, while the correlation notably failed to achieve significance in the middle of the period of peak decoding (160–180 ms).</p>
<fig id="pcbi.1004316.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Time-series correlation between representational distances from the animacy boundary and categorization task reaction times.</title>
<p>The time-varying rank-order correlation (Spearman’s ρ) between the average object exemplar representational distance and average reaction time across subjects for the categorization task (red), distracted viewing task (orange), and cross-over between the tasks (blue), in which representational distances from the distracted viewing task were correlated with the median RTs for the categorisation task. Color-coded asterisks indicate time points at which a correlation between distance and RT achieves significance (* = FDR adjusted <italic>p</italic> &lt; 0.05). The decoding onset is indicated by dashed vertical line (60 ms). The period of peak decoding is indicated by the gray shaded region extending from 120–240 ms post-stimulus onset. The bar along the x-axis indicates the stimulus duration.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g004" position="float" xlink:type="simple"/>
</fig>
<p>We next reasoned that median RTs for the distracted viewing task should <italic>not</italic> correlate with distances from an animacy boundary, since there is no reason to believe that distance from such a boundary should be predictive for categorizing letters as vowels or consonants. To test this null prediction, we grouped RTs for the distracted viewing task based on object exemplar, so that decision time reflected the time for observers to categorize the co-occuring letters as vowels and consonants. Based on such a grouping, there should be no relationship between median RTs and distance from the animacy boundary. As expected, at no time point did the correlation between distractor task RTs and distances from the animacy boundary achieve significance (<xref rid="pcbi.1004316.g004" ref-type="fig">Fig 4</xref>).</p>
<p>Carlson et al. [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>] correlated representational distances from IT, measured using fMRI while subjects passively viewed object exemplars, with RTs for object categorization, which were obtained from a separate group of subjects. Even though the fMRI subjects were not performing a categorization task, representational distances still correlated with RTs. The findings of Carlson et al. suggest that the structure of neural representations in the visual system driving categorization behaviour is not wholly task-dependent. We reasoned that even though subjects “passively” viewed exemplars while performing the distracted viewing task the representational distances constructed from the distracted viewing task MEG data would still be predictive of RTs for the “active” categorization task. We measured the correlation between representational distances for the distracted viewing task with the median RTs for the categorization task. This <italic>cross-over</italic> correlation followed a similar trajectory to the categorization task correlation time-course (<xref rid="pcbi.1004316.g004" ref-type="fig">Fig 4</xref>), achieving significance for nearly the complete time-course from decoding onset onwards, and again with the notable exception of 160–180 ms during the period of peak decoding. These findings show that the emerging representation that predicts RTs is a core representation that is constructed by the visual system even during passive viewing, and thus must be independent of task-specific decision or motor processing (see <xref rid="sec013" ref-type="sec">Methods</xref>).</p>
<p>In order to quantify the relationship between decoding and the representational distance-RT correlations, we correlated both decoding time-courses with both significant representational distance-RT correlation time-courses. We observed a significant negative correlation between each pair of decoding and correlation time-courses (<xref rid="pcbi.1004316.g005" ref-type="fig">Fig 5</xref>), reflecting the fact that as classifier performance increased there was an increase in the negative correlation between representational distance and RTs. Thus the relationship between representational distance and RTs appears to track the time-course of decoding.</p>
<fig id="pcbi.1004316.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Matrix of correlations between decoding time-courses and representational distance-RT correlation time-courses.</title>
<p>The “heat” of each tile reflects the strength of the rank-order correlation (Spearman’s ρ) between a decoding time-course and a representational distance-RT correlation time-course. Asterisks indicate significant correlations (* = <italic>p</italic> &lt; .01; ** = <italic>p</italic> &lt; .001).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g005" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Animate exemplars drive relationship between representational distance and reaction times for categorization</title>
<p>There is evidence that animate and inanimate objects are represented differently in the ventral visual stream of humans and monkeys [<xref rid="pcbi.1004316.ref013" ref-type="bibr">13</xref>, <xref rid="pcbi.1004316.ref021" ref-type="bibr">21</xref>–<xref rid="pcbi.1004316.ref022" ref-type="bibr">22</xref>]. In their study, Carlson et al. [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>] found that the correlation between distance and RTs that they observed was driven entirely by animate exemplars, with no significant correlation between representational distance and RTs for inanimate exemplar stimuli. Thus we sought to determine whether the correlations we observed between representational distance and RTs were also driven by the data for the animate exemplars.</p>
<p>We again measured the categorization task and cross-over correlation time-courses, this time analysing the data separately for animate and inanimate objects (<xref rid="pcbi.1004316.g006" ref-type="fig">Fig 6A and 6B</xref>). The animate correlations showed a consistent negative pattern, achieving significance around the period of peak decoding, and at later time-points (≥ 380 ms). When time-averaged (0–600 ms), the animacy correlations were significantly greater than their inanimate counterparts, though the latter correlations were also significant (<xref rid="pcbi.1004316.g007" ref-type="fig">Fig 7</xref>). However, only the animate exemplar correlation time-courses were significantly correlated with the decoding time-courses (<xref rid="pcbi.1004316.g005" ref-type="fig">Fig 5</xref>). Thus it appears that the animate examplars are indeed driving the time-varying relationship between representational distances and RTs. Furthermore, given that the mean RT for the categorization task was 469 ms, the significant time points after the period of peak decoding likely reflect the continued representation of animate exemplars after a decision was already made by observers (or would have been made, in the case of the cross-over correlation). Thus our primary prediction is borne out by the correlation time-courses for animate exemplars: information about (animate) object categories is being read-out during the period of peak decoding (<xref rid="pcbi.1004316.g006" ref-type="fig">Fig 6</xref>).</p>
<fig id="pcbi.1004316.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Time varying representational distance for individual exemplars separated by object category.</title>
<p>Time varying (A) categorization task (red) and (B) cross-over (blue) rank-order correlations (Spearman’s ρ) for animate and inanimate exemplar stimuli. The color-coded asterisks in the top row of plots indicate time points at which there is a significant correlation between distances and RTs (* = FDR adjusted <italic>p</italic> &lt; 0.05). The bottom row of plots displays the distances from animacy decision boundaries at each time point, computed for both the categorization task and distractor task data sets. Each time-varying line depicts the representational distance for an individual exemplar stimulus, at each 20 ms time point (either animate or inanimate). The color of each line is based on the rank-order of the median RT for each exemplar (rank is always within category). In all plots, decoding onset is indicated by dashed vertical line (60 ms). The period of peak decoding is indicated by a gray box extending 120–240 ms post-stimulus onset. The bar along the x-axis indicates the stimulus duration.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g006" position="float" xlink:type="simple"/>
</fig>
<fig id="pcbi.1004316.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Time-averaged (0–600 ms) correlations between representational distance and RTs for both animate and inanimate object exemplars.</title>
<p>Mean time-averaged categorization (red) and cross-over (blue) correlations (Spearman’s ρ) between representational distances and RTs. Asterisks indicate significant comparisons (Wilcoxon Signed Rank Test; * = <italic>p</italic> &lt; .01; ** = <italic>p</italic> &lt; .001). Error bars indicate +/- 1 <italic>SEM</italic> across time-points.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g007" position="float" xlink:type="simple"/>
</fig>
<p>The lower plots of <xref rid="pcbi.1004316.g006" ref-type="fig">Fig 6A and 6B</xref> show the averaged representational distances for animate and inanimate stimuli, color-coded by the rank-order categorization task RTs. Qualitatively, the representational distances for animate exemplars appear more separable than those for inanimate exemplars, increasing in relative distance considerably around the time of peak decoding. Notably, the distances for animate exemplars collapse at 160–180 ms, which corresponds to a slight dip, or “valley”, during the period of peak decoding (<xref rid="pcbi.1004316.g003" ref-type="fig">Fig 3</xref>), and is the point when the correlation time-courses for mean representational distance and median RT lose significance (<xref rid="pcbi.1004316.g004" ref-type="fig">Fig 4</xref>). This “peak and valley” pattern in classifier performance is also found in the results of other studies that have decoded exemplars based on animacy [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>]. The collapse in distances at 160–180 ms appears to explain why classifier performance remains high at these time points, but the correlation time-courses are not significant: at that time there is substantial relative distance between category (as reflected in distance from the decision boundary, or 0 on the y-axes), but minimal relative distance within category, resulting in a poorer ordering with respect to rank RTs.</p>
</sec>
<sec id="sec007">
<title>Latency and amplitude of sensory peaks do not predict reaction times for categorization</title>
<p>One question is whether the observed relationship between representational distance and RTs might track evoked responses observable using more conventional analyses. For example, Philiastides and Sajda [<xref rid="pcbi.1004316.ref023" ref-type="bibr">23</xref>] found that large amplitudes in difference waveforms coincided with their best decoding time-windows. Similarly, in our data differences in the amplitude of the evoked responses for animate and inanimate exemplars are evident in the grand average scalp topographies at the decoding peaks (<xref rid="pcbi.1004316.s001" ref-type="supplementary-material">S1 Fig</xref>). Furthermore, difference waveforms were significant both at peak decoding, and at ~ 380 ms onward (<xref rid="pcbi.1004316.s002" ref-type="supplementary-material">S2 Fig</xref>). Thus we asked whether the timing or magnitude of evoked responses might also predict RTs for categorization. To address this question, we isolated the peak latency and amplitude of early sensory peaks (&lt; 160 ms post-stimulus onset) for each individual exemplar from the grand averaged categorization and distracted viewing task MEG data. We then correlated these peak latencies and amplitudes with the median normalized RTs. If RTs reflect peak latency, we expect a positive correlation (greater latency predicting slower RTs), while if RTs reflect peak amplitude, we expect a negative correlation (greater amplitude predicting faster RTs). Neither correlation was significant, for either the categorization task or distracted viewing task peaks, even when separated by animacy (<xref rid="pcbi.1004316.g008" ref-type="fig">Fig 8</xref>). For comparison with the lower plots of <xref rid="pcbi.1004316.g006" ref-type="fig">Fig 6</xref>, <xref rid="pcbi.1004316.s003" ref-type="supplementary-material">S3 Fig</xref> also depicts the sensory peaks for each exemplar, separated by animacy, and color-coded for rank-order RT. As can be seen in the plots, the peak waveforms exhibit no clear ordering with respect to rank RTs. Given these null results, the observed relationship between representational distance and RTs does not appear to straightforwardly track properties of sensory peaks in the MEG evoked response.</p>
<fig id="pcbi.1004316.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004316.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Correlations between sensory peak latencies and amplitudes and categorization task RTs.</title>
<p>Scatter plots for the rank-order (A) categorization and (B) cross-over correlations (Spearman’s <italic>ρ</italic>) between peak latency and peak amplitude and median normalized RTs for the categorization task. (A) Red circles indicate animate exemplar data points, while red rings indicate inanimate data points. (B) Blue circles indicate animate exemplar data points, while blue rings indicate inanimate data points. Peak amplitude scale is in units of 10<sup>−14</sup> T. For comparison also plotted are the correlations between representational distance and RTs at 120 ms. Asterisks indicate significant correlations (* = <italic>p</italic> &lt; .01; ** = <italic>p</italic> &lt; .001).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004316.g008" position="float" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Discussion</title>
<p>In the present study we showed when and how the emerging representation of objects in the brain, as revealed using MEG decoding, can be related to categorization behaviour. Following previous work with fMRI showing that representational distance from a category boundary in the activation space of human IT predicts RTs for object categorization [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>], we sought to determine at what time representational distances predicted RT behaviour. The period of peak decoding provides the optimal period at which to read-out information about stimulus category. Thus we hypothesized that representational distances and RTs would exhibit the same relationship during this period, and predicted that RTs would negatively correlate with distance from a decision boundary through the activation spaces at the time of peak classifier performance. We found that representational distances and RTs correlated at most time-points following decoding onset. It was only when the data was separated by animacy that our prediction was supported: RTs began to negatively correlate with representational distance during the period of peak decoding, with the time-varying correlation tracking the time-course of decoding more generally. Our results have implications for (i) research on ultra-rapid object categorization, (ii) how time-resolved decoding results are interpreted to make inferences about neural representations, as well as for (iii) hypotheses concerning the possible neural loci of perceptual decision-making in vision.</p>
<sec id="sec009">
<title>Implications for ultra-rapid categorization</title>
<p>The fact that ultra-rapid categorization can be performed with some reliability suggests that information about stimulus category is available shortly after stimulus presentation. Congruent with this conjecture, information about object category can be decoded 60–100 ms after stimulus onset. However the availability of such information at such early latencies does not show, by itself, that this information is being used by subjects in a task related manner. In the present study, classifiers trained on the categorization task and distractor task data both achieved significant above chance performance as soon as 60 ms after stimulus onset, in line with previous MEG decoding experiments [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>, <xref rid="pcbi.1004316.ref024" ref-type="bibr">24</xref>]. When data was pooled across subjects, the first time point at which we observed a significant negative correlation between RTs and distance from an animacy boundary was decoding onset. Such an early latency for the predicted relationship between distance and RTs is consistent with existing psychophysical research showing reliable saccadic RTs at very short latencies of less than 200 ms [<xref rid="pcbi.1004316.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1004316.ref005" ref-type="bibr">5</xref>]. An enticing prospect is that future research might use the present distance approach to MEG decoding, in conjunction with behavioural measures, to further characterize the temporal dynamics of ultra-rapid object categorization. For example, there is evidence that RTs for superordinate categories (e.g. animal or non-animal) can be significantly faster than those for subordinate categories (e.g. dog or cat) [<xref rid="pcbi.1004316.ref025" ref-type="bibr">25</xref>–<xref rid="pcbi.1004316.ref026" ref-type="bibr">26</xref>]. In contrast, while MEG decoding onset does not appear to vary with object category, classifier performance peaks earlier for subordinate relative to superordinate categories [<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>–<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>]. Future research could use our distance approach to reconcile the apparent tension between these behavioural and decoding results.</p>
</sec>
<sec id="sec010">
<title>Implications for the interpretation of time-resolved decoding</title>
<p>It has been claimed that time-resolved decoding methods reveal the temporal dynamics of “representational geometries” in the brain [<xref rid="pcbi.1004316.ref011" ref-type="bibr">11</xref>, <xref rid="pcbi.1004316.ref027" ref-type="bibr">27</xref>]. However significant above chance classifier performance only warrants the inference that some information about a stimulus or task condition is available, but not that it is being used by the brain [<xref rid="pcbi.1004316.ref028" ref-type="bibr">28</xref>]. In this respect, the dynamics of the correlations between distance and RTs that we report are notable. The few MEG decoding results to date consistently show a “peak and valley” topography when a classifier is used to discriminate between animate and inanimate exemplars, as well as between other categories. From decoding onset classifier performance climbs to an initial peak, followed by a dip into a high altitude valley, before rising to a second peak. This can be seen in Carlson et al. ([<xref rid="pcbi.1004316.ref007" ref-type="bibr">7</xref>], Fig 3E), and Cichy, Pantazis and Oliva ([<xref rid="pcbi.1004316.ref008" ref-type="bibr">8</xref>], Fig 2A), as well as the present results (<xref rid="pcbi.1004316.g003" ref-type="fig">Fig 3</xref>). It is noteworthy that the “valley” in decoding performance aligns with an approximate point, 160–180 ms, at which there is a collapse in the representational distances (<xref rid="pcbi.1004316.g006" ref-type="fig">Fig 6A and 6B</xref>), and a corresponding lack of correlation between distance and RTs (Figs <xref rid="pcbi.1004316.g004" ref-type="fig">4</xref>, <xref rid="pcbi.1004316.g006" ref-type="fig">6A and 6B</xref>). We have highlighted this qualitative feature of our results because it suggests that although a classifier is able to use information latent in the patterns of neural activity at this time, the brain itself might not be using this information. Decodability reflects classifier performance, while the correlation between representational distance and RTs reflects human performance, and so the latter likely provides a better proxy of when in time object representations have emerged, or been “untangled” [<xref rid="pcbi.1004316.ref029" ref-type="bibr">29</xref>]. At the same time, the fact that the correlations between distance and RTs closely track classifier performance (<xref rid="pcbi.1004316.g005" ref-type="fig">Fig 5</xref>) provides some evidence for the idea that the brain is indeed a decoder of its own neural signals [<xref rid="pcbi.1004316.ref011" ref-type="bibr">11</xref>].</p>
</sec>
<sec id="sec011">
<title>Implications for neural models of perceptual-decision making</title>
<p>While our study shows that representational distance predicts RT behaviour, one question is how to connect this spatial measure, distance, to the temporal dynamics of the decision process. Such a connection can be established using sequential analysis models, which have used to uncover the neural loci of perceptual-decision making [<xref rid="pcbi.1004316.ref030" ref-type="bibr">30</xref>, <xref rid="pcbi.1004316.ref031" ref-type="bibr">31</xref>]. These models have been related to neural activity, in humans and monkeys, using a variety of stimuli, tasks, and recording methods, including cellular recordings, fMRI, and EEG/MEG [<xref rid="pcbi.1004316.ref032" ref-type="bibr">32</xref>–<xref rid="pcbi.1004316.ref038" ref-type="bibr">38</xref>]. While varying in their details, all sequential analysis models characterize differences in RT as resulting from variation in the evidence accumulation (or “drift”) rate toward a decision threshold. In order to make a connection between the evidence accumulation process and the brain’s population codes (as reflected in the patterns of neural activity clustered in high-dimensional activation spaces), Carlson et al. [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>] implemented a simple sequential analysis model to show that distance from a decision boundary in human IT results in differences in accumulation rate to a decision threshold. If drift rate is the only free parameter of the model, it can be fixed based on distance, since the starting point of the accumulation process is in effect equivalent to the decision boundary as specified by signal detection theory ([<xref rid="pcbi.1004316.ref039" ref-type="bibr">39</xref>–<xref rid="pcbi.1004316.ref040" ref-type="bibr">40</xref>]). When fixed in this way, drift rate scales with distance from a decision boundary: shorter distances entail slower drift rates, and longer distances entail faster drift rates. Thus, sequential analysis models, a prominent theoretical fixture in the decision-making literature, provide a bridge from representational distances to the temporal dynamics of the decision processes.</p>
<p>We did not apply a sequential analysis model to our data. However, as discussed by Carlson et al. [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>], it follows analytically from the observed correlations between distance and RTs that such an application is possible. For example, with our data time averaged distances during peak decoding could be used to set the drift rate parameter for each individual object exemplar, resulting in a positive correlation between drift rate and RTs (cf. [<xref rid="pcbi.1004316.ref041" ref-type="bibr">41</xref>]). In so far as the present experiment provides further evidence that representational distance predicts RTs, our results supports the hypothesis that representational distance determines the quality of evidence that feeds into the evidence accumulation process. Although the correlations we report likely reflects multiple brain areas rather than a single decision variable accumulating over time, we do believe our results also offers an important perspective on how decision variables are implemented in the brain; namely, as a trajectory through high-dimensional activation spaces reflecting the transformation of information over time [<xref rid="pcbi.1004316.ref029" ref-type="bibr">29</xref>,<xref rid="pcbi.1004316.ref039" ref-type="bibr">39</xref>]. More broadly our findings support the idea that representing is partially constitutive of the decision process for categorization [<xref rid="pcbi.1004316.ref014" ref-type="bibr">14</xref>].</p>
</sec>
<sec id="sec012">
<title>Summary</title>
<p>Motivated by recent findings, we sought to determine how the time-varying signal for objects, as identified by time-resolved decoding methods, can be related to behaviour. Previous research has shown that distance from a partitioning through a high-dimensional activation space can be used to predict RTs. We reasoned that since peak-decoding indexes the optimal time for reading-out information regarding stimulus category, it would be during the period of peak decoding that we would witness a relationship between distance and decision. In line with our expectations, RTs negatively correlated with distance from decision boundaries during the period of peak decodability, but only when our data was separated by object category. Our results provide evidence that the time course of decoding indeed reflects the emergence of representations for objects in the visual system. Furthermore, they also give credence to the thesis that representing and deciding do not necessarily reflect a clean partitioning between sensory evidence and its evaluation, but are instead fused during the process of categorization.</p>
</sec>
</sec>
<sec id="sec013" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec014">
<title>Ethics statement</title>
<p>The research in this study was approved by the Institutional Review Board at the University of Maryland, College Park.</p>
</sec>
<sec id="sec015">
<title>Participants</title>
<p>Thirty subjects from the University of Maryland, College Park, participated in the experiment (15 female; mean age = 21.1). All subjects had normal or correct to normal vision and were compensated financially for participating. The MEG data of one subject was corrupted, so only the data of the remaining 29 subjects was analysed.</p>
</sec>
<sec id="sec016">
<title>Stimuli and tasks</title>
<p>Stimuli were twenty-four segmented natural images of objects, consisting of a heterogeneous mix of animate and inanimate exemplars (12 animate, 12 inanimate): human and animal faces and bodies, and artificial and natural objects. Experiments were run on a Dell desktop PC computer running Matlab (Natick, MA). Stimuli were displayed on a translucent screen located 30 cm above participants in the MEG chamber. On the display, the stimuli were approximately 4 degrees of visual angle. Superimposed onto each image was a small fixation circle (0.4 degrees of visual angle) containing a letters drawn from the following set of vowels and consonants: vowels = ‘A’, ‘E’, ‘I’, ‘O’,’U’; consonants = ‘R’, ‘N’, ‘X’, ‘S’, ‘G’.</p>
<p>Subjects performed one of two tasks on alternating runs of the experiment (<xref rid="pcbi.1004316.g002" ref-type="fig">Fig 2</xref>). The <italic>categorization</italic> task required subjects to respond whether the exemplar was animate or inanimate (i.e. whether it was “capable of moving on its own volition”). The <italic>distracted viewing</italic> task required subjects to respond whether the character in the fixation circle was a vowel or consonant. Subjects were instructed to respond as quickly and accurately as possible while performing the tasks.</p>
<p>To remove any potential confounds associated with motor activity, the mapping between object category/letter type and response alternated on each run of each task. For example, if on the first categorization task run subjects responded with the left button for animate stimuli, and right button for inanimate stimuli, then on the next categorization task run the mapping would be reversed. Subject’s choice and RT data were collected for each trial. After responding, subjects were given feedback on their performance: if subjects responded correctly the fixation circle flashed green; if the subjects responded incorrectly, or failed to respond during the response period, then the fixation circle would flash red (<xref rid="pcbi.1004316.g002" ref-type="fig">Fig 2</xref>).</p>
<p>Trials were structured as follows (<xref rid="pcbi.1004316.g002" ref-type="fig">Fig 2</xref>). Each stimulus was presented for 500 ms, and subjects had 1000 ms (including the stimulus duration) to respond. The inter-stimulus interval for each trial was randomly selected from the range 900–1200 ms. Each image was presented 8 times in each run, in random order, resulting in 192 trials per run. Subjects performed 8 runs, with 4 categorization runs, and 4 distractor runs, resulting in 768 trials per task. At the end of each run subjects were provided feedback on their response accuracy for the run (percentage correct).</p>
</sec>
<sec id="sec017">
<title>MEG data acquisition and preprocessing</title>
<p>The neuromagnetic signal of the subjects was recorded using a 160 channel (157 recording; 3 reference) whole-head axial gradiometer (KIT, Kanazawa, Japan). Signals were digitized at 1000Hz, and filtered online from 0.1 to 200 Hz using first-order RC filters. Offline, time-shifted principle component analysis (TSPCA) was used to denoise the data [<xref rid="pcbi.1004316.ref042" ref-type="bibr">42</xref>]. TSPCA filters the data using the reference channels to estimate environmental noise. After denoising the data, trials were epoched from 100 ms pre-stimulus to 600 ms post-stimulus. Eye-movement artefacts were removed using an automated algorithm in Matlab. The average rejection rate of trials due to eye-movements was 2.1% with 2.6% <italic>SD</italic> across subjects.</p>
<p>For the MEG decoding analyses, PCA with a threshold of retaining 99% of the variance was used to reduce the dimensionality of the datasets. The sampling rate was reduced to 50Hz to increase signal to noise, resulting in 36 time-points with a 20 ms resolution. The data was downsampled using the decimate function in Matlab, which first applies a low-pass Chebyshev Type I filter. Filtering when downsampling introduces a latency offset (estimated by simulation to be 20 ms), which was corrected for after downsampling. For the conventional time-series analyses, the data was downsampled to 500 Hz and low-pass filtered (Butterworth) at 40 Hz offline using SPM8 ([<xref rid="pcbi.1004316.ref043" ref-type="bibr">43</xref>]), resulting in time-points with a 2 ms resolution and negligible latency offset. No off-line high-pass filtering was applied to the data for either the decoding or conventional analyses.</p>
</sec>
<sec id="sec018">
<title>Sliding time-window decoding analysis</title>
<p>The decoding analysis was run separately for the categorization task and distracted viewing task MEG datasets. For each set of data, we used a naïve Bayes implementation of linear discriminate analysis (LDA, [<xref rid="pcbi.1004316.ref020" ref-type="bibr">20</xref>]) to perform single-trial classification of animate and inanimate objects from the scalp topography at each time point. Generalization of the classifier was evaluate using k-fold cross validation with a 9:1 training to test ratio. In this procedure, the neuromagnetic data for all trials of a task were randomly assigned to 10 bins approximately equal in size. Nine of the bins were pooled to train the classifier, and the trials in the remaining bin were used to test the classifier. This procedure was repeated 10 times such that each trial was tested on exactly once. The decoding analysis was run on each time point to measure the time varying decoding performance for animacy. Classifier performance is reported in terms of <italic>d’</italic>. Mean classifier performance at each time-point was tested for significance using the Wilcoxon signed rank test. To correct for multiple comparisons we also computed the false discovery rate (FDR) adjusted <italic>p</italic>-values with α = 0.05. The same correction was performed for all other tests of statistical significance that involved multiple comparisons (i.e. testing at each time point).</p>
<p>To quantify the period of peak decoding, we first calculated the mean value of the two peaks in classifier performance for the categorization task and distractor task data (<italic>d’</italic> = .61). We then tested whether classifier performance at each time-point was significantly different from this peak value using the Wilcoxon signed rank test (FDR adjusted <italic>p</italic> &lt; .05). The period of peak decoding was defined as all time-points at which at least one classifier time-course was not significantly different than the peak performance value.</p>
</sec>
<sec id="sec019">
<title>Distances in activation space</title>
<p>An activation space is an <italic>N</italic>-dimensional space with the number of dimensions determined by the number of components retained after dimensionality reduction using PCA (see above). LDA was used to compute a discriminant axis for animacy in the activation space constructed for each 20 ms time-point. The mean activation pattern for each exemplar was projected onto the discriminant axis. A naïve Bayes classifier was then used to compute a decision boundary for animacy for each MEG dataset. Individual exemplar distances were computed as the absolute value of the Euclidean distance of a pattern of activity for an exemplar from the decision boundary. The exemplar distances were computed at each time point providing a time varying measure of the distance of object exemplar representations from the decision boundary. The partitioning of the activation space generated by LDA is not perfect. Although an exemplar is animate, a pattern of activity for the exemplar might fall on the inanimate side of the partition, since LDA and the classifier only provide the best (not perfect) linear partitioning of a space. To ensure that each time point had equal data for computing the correlations, classification accuracy was not taken into consideration when computing the distances. The above procedure was carried out separately for the two data sets, yielding a set of categorization task distances for each time-point, and a set of distracted viewing task distances for each time-point.</p>
</sec>
<sec id="sec020">
<title>Correlating distances in activation space with reaction times</title>
<p>RTs from the categorization and distracted viewing tasks were analysed after individual subjects’ RT were normalized and pooled. We measured the rank-order (Spearman’s <italic>ρ</italic>) correlations between the normalized median RTs and the representational distances at each of the 36 time points. We measured three different correlations between distance and RTs: (i) the correlation between object categorization task RTs and distances computed from the categorization task time-series data (the <italic>categorization</italic> correlation); (ii) the correlation between distracted viewing task RTs pooled by object exemplar and distances computed from the distracted viewing task time-series data, and (iii) the correlation between the categorization task RTs and the distances computed from the distractor task time-series data (the <italic>cross-over</italic> correlation). We also measured the categorization and cross-over correlations separately for animate and inanimate exemplars. These comparisons produced correlation time-courses which were also correlated (Spearman’s <italic>ρ</italic>) with the decoding time-courses, or time-averaged.</p>
</sec>
<sec id="sec021">
<title>Analysis of difference waveforms and sensory peaks</title>
<p>To determine whether a substantial difference between evoked responses for animate and inanimate exemplars might coincide with the period of peak decoding, we generated difference waveforms (animate - inanimate) from both the categorization task and distracted viewing task MEG data. First, using the grand averaged data, we isolated the five sensors that showed the largest (positive) amplitude in the difference waveforms at the same peak times that were used to define the period of peak decoding (categorization task: 140 and 240 ms; distracted viewing task: 140 and 220 ms). The approximate location of these sensors can be seen in <xref rid="pcbi.1004316.s001" ref-type="supplementary-material">S1 Fig</xref>. This resulted in four sets of five sensors, one for each local decoding peak. Second, we averaged the data from each set of five sensors for each individual subject, and tested for significance at each time-point using the Wilcoxon signed rank test (correcting for multiple comparisons using FDR).</p>
<p>To test whether sensory peak latencies or amplitudes might predict RTs, we first isolated the five sensors that showed the largest (positive) amplitude -100–160 ms post stimulus onset for animate exemplars, using the grand averaged data. The approximate location of these sensors can be seen in <xref rid="pcbi.1004316.s001" ref-type="supplementary-material">S1 Fig</xref>. Next, again using the grand averaged data, we averaged the data from these five sensors for each individual exemplar, and calculated the latency and amplitude of the maximum peak -100–160 ms. The sensory peaks isolated by this procedure are depicted in <xref rid="pcbi.1004316.s003" ref-type="supplementary-material">S3 Fig</xref>. Finally, we then measure the rank-order correlations (Spearman’s <italic>ρ</italic>) between the peak latencies and amplitudes and median normalized RTs.</p>
<p>All channel selection for these analyses was done using an automated search for the maximum amplitude of the evoked responses within the pre-defined time-windows.</p>
</sec>
</sec>
<sec id="sec022">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004316.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004316.s001" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Grand averaged scalp topographies at peak decoding.</title>
<p>Plots show the grand average scalp topographies for animate and inanimate exemplars, as well as their difference (animate - inanimate), at each local decoding peak. Black dots indicate the sensors selected for isolating data for further analysis (see: <xref rid="sec002" ref-type="sec">Results</xref>; <xref rid="sec013" ref-type="sec">Methods</xref>; and Figs <xref rid="pcbi.1004316.g008" ref-type="fig">8</xref>, <xref rid="pcbi.1004316.s002" ref-type="supplementary-material">S2</xref> and <xref rid="pcbi.1004316.s003" ref-type="supplementary-material">S3</xref>). All amplitude scales are in units of 10<sup>−14</sup> T.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004316.s002" xlink:href="info:doi/10.1371/journal.pcbi.1004316.s002" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Difference waveforms with maximum amplitudes at decoding peaks.</title>
<p>Each waveform depicts the grand average data from five sensors that had maximum amplitude at the local decoding peaks. Lighter colored waveforms have maximum amplitude at the first peak (140 ms), while darker colored waveforms have maximum amplitude at the second peak (categorization task: 240 ms; distracted viewing task: 220 ms). All response amplitude scales are in units of 10<sup>−14</sup> T. Color-coded asterisks indicate time points at which the amplitude of the difference waveforms achieved significance based on a Wilcoxon signed rank test (* = FDR adjusted <italic>p</italic> &lt; 0.05). The decoding onset is indicated by dashed vertical line (60 ms). The period of peak decoding is indicated by the gray shaded region extending from 120–240 ms post-stimulus onset. The bar along the x-axis indicates the stimulus duration.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004316.s003" xlink:href="info:doi/10.1371/journal.pcbi.1004316.s003" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Sensory peak waveforms for object exemplars.</title>
<p>Each waveform depicts the grand averaged data for an individual exemplar, from five sensors with maximum amplitude for animate exemplars -100–160 ms post-stimulus onset. Each plot contains the waveform for animate or inanimate exemplars, from the categorization task or distracted viewing task MEG data. The color of each waveform is based on the rank-order of the median normalized RT for each exemplar (rank is always within category). All response amplitude scales are in units of 10<sup>−14</sup> T. The decoding onset is indicated by dashed vertical line (60 ms). The period of peak decoding is indicated by the gray shaded region extending from 120–240 ms post-stimulus onset. The bar along the x-axis indicates the stimulus duration.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We would like to thank the research assistants of the (former) Maryland Vision Science Lab who helped to collect and analyse some of the data for this experiment.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004316.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potter</surname> <given-names>MC</given-names></name>. <article-title>Short-term conceptual memory for pictures</article-title>. <source>J Exp Psychol Hum Learn</source>. <year>1976</year>;<volume>2</volume>: <fpage>509</fpage>–<lpage>522</lpage>. <object-id pub-id-type="pmid">1003124</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name>. <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>: <fpage>520</fpage>–<lpage>522</lpage>. <object-id pub-id-type="pmid">8632824</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirchner</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>. <article-title>Ultra-rapid object detection with saccadic eye movements: visual processing speed revisited</article-title>. <source>Vision Res</source>. <year>2006</year>;<volume>46</volume>: <fpage>1762</fpage>–<lpage>1776</lpage>. <object-id pub-id-type="pmid">16289663</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>C-T</given-names></name>, <name name-style="western"><surname>Crouzet</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>. <article-title>At 120 msec you can spot the animal but you don’t yet know it’s a dog</article-title>. <source>J Cogn Neurosci</source>. <year>2014</year>;<volume>27</volume>: <fpage>141</fpage>–<lpage>149</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004316.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crouzet</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Kirchner</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>. <article-title>Fast saccades toward faces: face detection in just 100 ms</article-title>. <source>J Vis</source>. <year>2010</year>;<volume>10</volume>: <fpage>16</fpage>. <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/10/4/16.short" xlink:type="simple">http://www.journalofvision.org/content/10/4/16.short</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/10.9.16" xlink:type="simple">10.1167/10.9.16</ext-link></comment> <object-id pub-id-type="pmid">21187350</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname> <given-names>SJ</given-names></name>. <article-title>The speed of categorization in the human visual system</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>: <fpage>168</fpage>–<lpage>170</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.04.012" xlink:type="simple">10.1016/j.neuron.2009.04.012</ext-link></comment> <object-id pub-id-type="pmid">19409262</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tovar</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Alink</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>. <article-title>Representational dynamics of object vision: the first 1000 ms</article-title>. <source>J Vis</source>. <year>2013</year>;<volume>13</volume>: <fpage>1</fpage>. <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/13/10/1.short" xlink:type="simple">http://www.journalofvision.org/content/13/10/1.short</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/13.14.1" xlink:type="simple">10.1167/13.14.1</ext-link></comment> <object-id pub-id-type="pmid">24297775</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cichy</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Pantazis</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Oliva</surname> <given-names>A</given-names></name>. <article-title>Resolving human object recognition in space and time</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>: <fpage>455</fpage>–<lpage>462</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3635" xlink:type="simple">10.1038/nn.3635</ext-link></comment> <object-id pub-id-type="pmid">24464044</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hung</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title>. <source>Science</source>. <year>2005</year>;<volume>310</volume>: <fpage>863</fpage>–<lpage>866</lpage>. <object-id pub-id-type="pmid">16272124</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Agam</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>. <article-title>Timing, timing, timing: fast decoding of object information from intracranial field potentials in human visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>: <fpage>281</fpage>–<lpage>290</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.02.025" xlink:type="simple">10.1016/j.neuron.2009.02.025</ext-link></comment> <object-id pub-id-type="pmid">19409272</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>King J-</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dehaene</surname> <given-names>S</given-names></name>. <article-title>Characterizing the dynamics of mental representations the temporal generalization method</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>: <fpage>203</fpage>–<lpage>210</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2014.01.002" xlink:type="simple">10.1016/j.tics.2014.01.002</ext-link></comment> <object-id pub-id-type="pmid">24593982</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Logothetis</surname> <given-names>NK</given-names></name>, <name name-style="western"><surname>Sheinberg</surname> <given-names>DL</given-names></name> <article-title>Visual object recognition</article-title>. <source>Annu Rev Neurosci</source>. <year>1996</year>;<volume>19</volume>: <fpage>577</fpage>–<lpage>621</lpage>. <object-id pub-id-type="pmid">8833455</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Esteky</surname> <given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>: <fpage>1126</fpage>–<lpage>1141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.10.043" xlink:type="simple">10.1016/j.neuron.2008.10.043</ext-link></comment> <object-id pub-id-type="pmid">19109916</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlson</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Ritchie</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Durvasula</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>J</given-names></name>. <article-title>Reaction time for object categorization is predicted by representational distance</article-title>. <source>J Cogn Neurosci</source>. <year>2014</year>;<volume>26</volume>: <fpage>132</fpage>–<lpage>142</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/jocn_a_00476" xlink:type="simple">10.1162/jocn_a_00476</ext-link></comment> <object-id pub-id-type="pmid">24001004</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <chapter-title>Signal detection theory and psychophysics</chapter-title>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1966</year>.</mixed-citation></ref>
<ref id="pcbi.1004316.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pike</surname> <given-names>R</given-names></name>. <article-title>Response latency models for signal detection</article-title>. <source>Psychol Rev</source>. <year>1973</year>;<volume>80</volume>: <fpage>53</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">4689203</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>, <name name-style="western"><surname>Maddox</surname> <given-names>WT</given-names></name>. <article-title>A response time theory of separability and integrality in speeded classification</article-title>. <source>J Math Psychol</source>. <year>1994</year>;<volume>38</volume>: <fpage>423</fpage>–<lpage>466</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004316.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunovan</surname> <given-names>KE</given-names></name>, <name name-style="western"><surname>Tremel</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Wheeler</surname> <given-names>ME</given-names></name>. <article-title>Prior probability and feature predictability interactively bias perceptual decisions</article-title>. <source>Neuropsychologia</source>. <year>2014</year>;<volume>61</volume>: <fpage>210</fpage>–<lpage>221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuropsychologia.2014.06.024" xlink:type="simple">10.1016/j.neuropsychologia.2014.06.024</ext-link></comment> <object-id pub-id-type="pmid">24978303</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>White</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Mumford</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Poldrack</surname> <given-names>RA</given-names></name>. <article-title>Perceptual criteria in the human brain</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>: <fpage>16716</fpage>–<lpage>16724</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1744-12.2012" xlink:type="simple">10.1523/JNEUROSCI.1744-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23175825</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref020"><label>20</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Duda</surname> <given-names>RO</given-names></name>, <name name-style="western"><surname>Hart</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Stork</surname> <given-names>DG</given-names></name>. <chapter-title>Pattern classification</chapter-title>. <edition>2nd ed.</edition> <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>2001</year>.</mixed-citation></ref>
<ref id="pcbi.1004316.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Esteky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Mirpour</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>K</given-names></name>. <article-title>Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</article-title>. <source>J Neurophysiol</source>. <year>2007</year>;<volume>97</volume>: <fpage>4296</fpage>–<lpage>4309</lpage>. <object-id pub-id-type="pmid">17428910</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Konkle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Caramazza</surname> <given-names>A</given-names></name>. <article-title>Tripartite organization of the ventral visual stream by animacy and object size</article-title>. <source>J Neurosci</source>. <year>2013</year>;<volume>33</volume>: <fpage>10235</fpage>–<lpage>10242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0983-13.2013" xlink:type="simple">10.1523/JNEUROSCI.0983-13.2013</ext-link></comment> <object-id pub-id-type="pmid">23785139</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Temporal characterization of the neural correlates of perceptual decision making in the human brain</article-title>. <source>Cereb Cortex</source>. <year>2006</year>;<volume>16</volume>: <fpage>509</fpage>–<lpage>518</lpage>. <object-id pub-id-type="pmid">16014865</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isik</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Meyers</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Leibo</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>The dynamics of invariant object recognition in the human visual system</article-title>. <source>J Neurophysiol</source>. <year>2014</year>;<volume>111</volume>: <fpage>91</fpage>–<lpage>102</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00394.2013" xlink:type="simple">10.1152/jn.00394.2013</ext-link></comment> <object-id pub-id-type="pmid">24089402</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Delorme</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Marlot</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Thorpe</surname> <given-names>S</given-names></name>. <article-title>A limit to the speed of processing in ultra-rapid visual categorization of novel natural scenes</article-title>. <source>J Cogn Neurosci</source>. <year>2001</year>;<volume>13</volume>: <fpage>171</fpage>–<lpage>180</lpage>. <object-id pub-id-type="pmid">11244543</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poncet</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname> <given-names>M</given-names></name>. <article-title>Stimulus duration and diversity do not reverse the advantage for superordinate-level representations: the animal is seen before the bird</article-title>. <source>Eur J Neurosci</source>. <year>2014</year>;<volume>39</volume>: <fpage>1508</fpage>–<lpage>1516</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/ejn.12513" xlink:type="simple">10.1111/ejn.12513</ext-link></comment> <object-id pub-id-type="pmid">24617612</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kievit</surname> <given-names>RA</given-names></name>. <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year>;<volume>17</volume>: <fpage>401</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2013.06.007" xlink:type="simple">10.1016/j.tics.2013.06.007</ext-link></comment> <object-id pub-id-type="pmid">23876494</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Dang</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>NG</given-names></name>. <article-title>Only some spatial patterns of fMRI response are read out in task performance</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>: <fpage>685</fpage>–<lpage>686</lpage>. <object-id pub-id-type="pmid">17486103</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname> <given-names>DD</given-names></name>. <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn Sci</source>. <year>2007</year>;<volume>11</volume>: <fpage>333</fpage>–<lpage>341</lpage>. <object-id pub-id-type="pmid">17631409</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>The neural basis of decision making</article-title>. <source>Annu Rev Neurosci</source>. <year>2007</year>;<volume>30</volume>: <fpage>535</fpage>–<lpage>574</lpage>. <object-id pub-id-type="pmid">17600525</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Kiani</surname> <given-names>R</given-names></name>. <article-title>Decision making as a window on cognition</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>80</volume>: <fpage>791</fpage>–<lpage>806</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2013.10.047" xlink:type="simple">10.1016/j.neuron.2013.10.047</ext-link></comment> <object-id pub-id-type="pmid">24183028</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roitman</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name>. <article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title>. <source>J Neurosci</source>. <year>2002</year>;<volume>22</volume>: <fpage>9475</fpage>–<lpage>9489</lpage>. <object-id pub-id-type="pmid">12417672</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heitz</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Schall</surname> <given-names>JD</given-names></name>. <article-title>Neural mechanisms of speed-accuracy tradeoff</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>76</volume>: <fpage>616</fpage>–<lpage>628</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2012.08.030" xlink:type="simple">10.1016/j.neuron.2012.08.030</ext-link></comment> <object-id pub-id-type="pmid">23141072</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Shenoy</surname> <given-names>KV</given-names></name>, <name name-style="western"><surname>Newsome</surname> <given-names>WT</given-names></name>. <article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title>. <source>Nature</source>. <year>2013</year>;<volume>503</volume>: <fpage>78</fpage>–<lpage>84</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12742" xlink:type="simple">10.1038/nature12742</ext-link></comment> <object-id pub-id-type="pmid">24201281</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Marrett</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bandettini</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Ungerleider</surname> <given-names>LG</given-names></name>. <article-title>A general mechanism for perceptual decision-making in the human brain</article-title>. <source>Nature</source>. <year>2004</year>;<volume>431</volume>: <fpage>859</fpage>–<lpage>862</lpage>. <object-id pub-id-type="pmid">15483614</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Pleskac</surname> <given-names>TJ</given-names></name>. <article-title>Neural correlates of evidence accumulation in a perceptual decision task</article-title>. <source>J Neurophysiol</source>. <year>2011</year>;<volume>106</volume>: <fpage>2383</fpage>–<lpage>2398</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.00413.2011" xlink:type="simple">10.1152/jn.00413.2011</ext-link></comment> <object-id pub-id-type="pmid">21849612</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ploran</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Velonova</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Donaldson</surname> <given-names>DI</given-names></name>, <name name-style="western"><surname>Petersen</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Wheeler</surname> <given-names>ME</given-names></name>. <article-title>Evidence accumulation and the moment of recognition: dissociating perceptual recognition processes using fMRI</article-title>. <source>J Neurosci</source>. <year>2007</year>;<volume>27</volume>: <fpage>11912</fpage>–<lpage>11924</lpage>. <object-id pub-id-type="pmid">17978031</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Connell</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Dockree</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Kelly</surname> <given-names>SP</given-names></name>. <article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title>. <source>Nat Neurosci</source>. <year>2012</year>;<volume>15</volume>: <fpage>1729</fpage>–<lpage>1735</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3248" xlink:type="simple">10.1038/nn.3248</ext-link></comment> <object-id pub-id-type="pmid">23103963</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname> <given-names>FG</given-names></name>. <article-title>A stochastic version of general recognition theory</article-title>. <source>J Math Psychol</source>. <year>2000</year>;<volume>44</volume>: <fpage>310</fpage>–<lpage>329</lpage>. <object-id pub-id-type="pmid">10831374</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>. <article-title>Theoretical interpretations of the speed and accuracy of positive and negative responses</article-title>. <source>Psychol Rev</source>. <year>1985</year>;<volume>92</volume>: <fpage>212</fpage>–<lpage>225</lpage>. <object-id pub-id-type="pmid">3991839</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Ratcliff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sajda</surname> <given-names>P</given-names></name>. <article-title>Neural representation of task difficulty and decision making during perceptual categorization: a timing diagram</article-title>. <source>J Neurosci</source>. <year>2006</year>;<volume>26</volume>: <fpage>8965</fpage>–<lpage>8975</lpage>. <object-id pub-id-type="pmid">16943552</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Cheveigne</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name>. <article-title>Denoising based on time-shifted PCA</article-title>. <source>J Neurosci Meth</source>. <year>2007</year>;<volume>165</volume>: <fpage>297</fpage>–<lpage>305</lpage>. <object-id pub-id-type="pmid">17624443</object-id></mixed-citation></ref>
<ref id="pcbi.1004316.ref043"><label>43</label><mixed-citation publication-type="other" xlink:type="simple">Litvak V, Mattout J, Kiebel S, Phillips C, Henson R, Kilner J, et al. EEG and MEG data analysis in SPM8. Comput Intell Neurosci. 2011;2011. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/2011/852961" xlink:type="simple">http://dx.doi.org/10.1155/2011/852961</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>