<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00803</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003985</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Auditory cortex</subject></subj-group></subj-group></subj-group><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject></subj-group></subj-group><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering and technology</subject><subj-group><subject>Acoustical engineering</subject><subj-group><subject>Noise control</subject></subj-group></subj-group><subj-group><subject>Signal processing</subject><subj-group><subject>Audio signal processing</subject><subject>Noise reduction</subject><subject>Speech signal processing</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Segregating Complex Sound Sources through Temporal Coherence</article-title>
<alt-title alt-title-type="running-head">Segmenting Cluttered Auditory Scenes through Temporal Coherence</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Krishnan</surname><given-names>Lakshmi</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>Mounya</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>Shihab</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Electrical and Computer Engineering, University of Maryland, College Park, Maryland, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, Maryland, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department Etudes Cognitive, Ecole Normale Superieure, Paris, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname><given-names>Michael</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Case Western Reserve University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">lakshmik@umd.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: LK SS ME. Performed the experiments: LK. Analyzed the data: LK SS. Wrote the paper: SS LK. Reviewed the manuscript: ME.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>12</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>18</day><month>12</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>12</issue>
<elocation-id>e1003985</elocation-id>
<history>
<date date-type="received"><day>7</day><month>5</month><year>2014</year></date>
<date date-type="accepted"><day>14</day><month>10</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Krishnan et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>A new approach for the segregation of monaural sound mixtures is presented based on the principle of temporal coherence and using auditory cortical representations. Temporal coherence is the notion that perceived sources emit coherently modulated features that evoke highly-coincident neural response patterns. By clustering the feature channels with coincident responses and reconstructing their input, one may segregate the underlying source from the simultaneously interfering signals that are uncorrelated with it. The proposed algorithm requires no prior information or training on the sources. It can, however, gracefully incorporate cognitive functions and influences such as memories of a target source or attention to a specific set of its attributes so as to segregate it from its background. Aside from its unusual structure and computational innovations, the proposed model provides testable hypotheses of the physiological mechanisms of this ubiquitous and remarkable perceptual ability, and of its psychophysical manifestations in navigating complex sensory environments.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Humans and many animals can effortlessly navigate complex sensory environments, segregating and attending to one desired target source while suppressing distracting and interfering others. In this paper, we present an algorithmic model that can accomplish this task with no prior information or training on complex signals such as speech mixtures, and speech in noise and music. The model accounts for this ability relying solely on the temporal coherence principle, the notion that perceived sources emit coherently modulated features that evoke coincident cortical response patterns. It further demonstrates how basic cortical mechanisms common to all sensory systems can implement the necessary representations, as well as the adaptive computations necessary to maintain continuity by tracking slowly changing characteristics of different sources in a scene.</p>
</abstract>
<funding-group><funding-statement>This work was partially supported by an NIH grant R01 DC007657 and an Advanced ERC 295603 (to SS), and R01AG036424-01 (to ME). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts><custom-meta-group><custom-meta id="data-availability" xlink:type="simple"><meta-name>Data Availability</meta-name><meta-value>The authors confirm that all data underlying the findings are fully available without restriction. Data are from the TIMIT database that may be accessed from <ext-link ext-link-type="uri" xlink:href="https://catalog.ldc.upenn.edu/LDC93S1" xlink:type="simple">https://catalog.ldc.upenn.edu/LDC93S1</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Humans and animals can attend to a sound source and segregate it rapidly from a background of many other sources, with no learning or prior exposure to the specific sounds. For humans, this is the essence of the well-known <italic>cocktail party problem</italic> in which a person can effortlessly conduct a conversation with a new acquaintance in a crowded and noisy environment <xref ref-type="bibr" rid="pcbi.1003985-Bregman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Cherry1">[2]</xref>. For frogs, songbirds, and penguins, this ability is vital for locating a mate or an offspring in the midst of a loud chorus <xref ref-type="bibr" rid="pcbi.1003985-Bee1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Singh1">[4]</xref>. This capacity is matched by comparable object segregation feats in vision and other senses <xref ref-type="bibr" rid="pcbi.1003985-Henderson1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Jones1">[6]</xref>, and hence understanding it will shed light on the neural mechanisms that are fundamental and ubiquitous across all sensory systems.</p>
<p>Computational models of auditory scene analysis have been proposed in the past to disentangle source mixtures and hence capture the functionality of this perceptual process. The models differ substantially in flavor and complexity depending on their overall objectives. For instance, some rely on prior information to segregate a specific target source or voice, and are usually able to reconstruct it with excellent quality <xref ref-type="bibr" rid="pcbi.1003985-Kristjansson1">[7]</xref>. Another class of algorithms relies on the availability of multiple microphones and the statistical independence among the sources to separate them, using for example ICA approaches or beam-forming principles <xref ref-type="bibr" rid="pcbi.1003985-Comon1">[8]</xref>. Others are constrained by a single microphone and have instead opted to compute the spectrogram of the mixture, and then to decompose it into separate sources relying on heuristics, training, mild constraints on matrix factorizations <xref ref-type="bibr" rid="pcbi.1003985-Smaragdis1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-King1">[11]</xref>, spectrotemporal masks <xref ref-type="bibr" rid="pcbi.1003985-Cooke1">[12]</xref>, and gestalt rules <xref ref-type="bibr" rid="pcbi.1003985-Bregman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Brown1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Shao1">[14]</xref>. A different class of approaches emphasizes the biological mechanisms underlying this process, and assesses both their plausibility and ability to replicate faithfully the psychoacoustics of stream segregation (with all their strengths and weaknesses). Examples of the latter approaches include models of the auditory periphery that explain how simple tone sequences may stream <xref ref-type="bibr" rid="pcbi.1003985-Hartmann1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-McCabe1">[17]</xref>, how pitch modulations can be extracted and used to segregate sources of different pitch <xref ref-type="bibr" rid="pcbi.1003985-Stark1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-Clark1">[20]</xref>, and models that handle more elaborate sound sequences and bistable perceptual phenomena <xref ref-type="bibr" rid="pcbi.1003985-Ellis1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Mill1">[21]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-VonDerMalsburg1">[23]</xref>. Finally, of particular relevance here are algorithms that rely on the notion that features extracted from a given sound source can be bound together by correlations of intrinsic coupled oscillators in neural networks that form their connectivity online <xref ref-type="bibr" rid="pcbi.1003985-VonDerMalsburg1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Wang1">[24]</xref>. It is fair to say, however, that the diversity of approaches and the continued strong interest in this problem suggest that no algorithm has yet achieved sufficient success to render the “cocktail party problem" solved from a theoretical, physiological, or applications point of view.</p>
<p>While our approach echoes some of the implicit or explicit ideas in the above-mentioned algorithms, it differs fundamentally in its overall framework and implementation. It is based on the notion that perceived sources (sound streams or objects) emit features, that are modulated in strength in a largely temporally coherent manner and that they evoke highly correlated response patterns in the brain. By clustering (or grouping) these responses one can reconstruct their underlying source, and also segregate it from other simultaneously interfering signals that are uncorrelated with it.</p>
<p>This simple principle of <italic>temporal coherence</italic> has already been shown to account experimentally for the perception of sources (or streams) in complex backgrounds <xref ref-type="bibr" rid="pcbi.1003985-Shamma1">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-Teki1">[28]</xref>. However, this is the first detailed computational implementation of this idea that demonstrates how it works, and why it is so effective as a strategy to segregate spectrotemporally complex stimuli such as speech and music. Furthermore, it should be emphasized that despite apparent similarities, the idea of temporal coherence differs fundamentally from previous efforts that invoked correlations and synchronization in the following ways <xref ref-type="bibr" rid="pcbi.1003985-Wang2">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003985-Almonte1">[33]</xref>: (1) coincidence here refers to that among modulated feature channels due to slow stimulus power (envelope) fluctuations, and not to any <italic>intrinsic</italic> brain oscillations; (2) coincidences are strictly done at cortical time-scales of a few hertz, and not at the fast pitch or acoustic frequency rates often considered; (3) coincidences are measured among modulated cortical features and perceptual attributes that usually occupy well-separated channels, unlike the crowded frequency channels of the auditory spectrogram; (4) coincidence must be measured over multiple time-scales and not just over a single time-window that is bound to be too long or too short for a subset of modulations; and finally (5) the details we describe later for how the coincidence matrices are exploited to segregate the sources are new and are critical for the success of this effort. For all these reasons, the simple principle of temporal coherence is not easily implementable. Our goal here is to show how to do so using plausible cortical mechanisms able to segregate realistic mixtures of complex signals.</p>
<p>As we shall demonstrate, the proposed framework mimics human and animal strategies to segregate sources with no prior information or knowledge of their properties. The model can also gracefully utilize available cognitive influences such as attention to, or memory of specific attributes of a source (e.g., its pitch or timbre) to segregate it from its background. We begin with a sketch of the model stages, with emphasis on the unique aspects critical for its function. We then explore how separation of feature channel responses and their temporal continuity contribute to source segregation, and the potential helpful role of perceptual attributes like pitch and location in this process. Finally, we extend the results to the segregation of complex natural signals such as speech mixtures, and speech in noise or music.</p>
</sec><sec id="s2">
<title>Results</title>
<p>The temporal coherence algorithm consists of an auditory model that transforms the acoustic stimulus to its cortical representation (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1A</xref>). A subsequent stage computes a coincidence matrix (C-matrices in <xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>) that summarizes the pair-wise coincidences (or correlations at zero-lag) between all pairs of responses making up the cortical representation. A final auto-encoder network is then used to decompose the coincidence matrix into its different streams. The use of the cortical representation here is extremely important as it provides a multiresolution view of the signal's spectral and temporal features, and these in turn endow the process with its robust character. Details of these auditory transformations are described elsewhere <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>, and summarized in <bold><xref ref-type="sec" rid="s4">Methods</xref></bold> below for completeness.</p>
<fig id="pcbi-1003985-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g001</object-id><label>Figure 1</label><caption>
<title>The temporal coherence model consists of two stages.</title>
<p>(A) Transformation of sound into a cortical representation <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>: It begins with a computation of the auditory spectrogram (left panel), followed by an analysis of its spectral and temporal modulations in two steps (middle and right panels, respectively): a multi-scale (or a multi-bandwidth) wavelet analysis along the spectral dimension to create the frequency-scale responses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e001" xlink:type="simple"/></inline-formula>, followed by a wavelet analysis of the <italic>modulus</italic> of these outputs to create the final cortical outputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e002" xlink:type="simple"/></inline-formula> (right panel). (B) <italic>Coincidence and clustering</italic>: The cortical outputs at each time-step are used to compute a family of coincidence matrices (left panel). Each matrix (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e003" xlink:type="simple"/></inline-formula>) is the outer product of the cortical outputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e004" xlink:type="simple"/></inline-formula> (i.e., separately for each modulation rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e005" xlink:type="simple"/></inline-formula>). The C-matrices are then stacked (middle panel) and simultaneously decomposed by a nonlinear auto-encoder network (right panel) into two principal components corresponding to the foreground and background masks which are used to segregate the cortical response.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g001" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Extracting streams from the coincidence matrices</title>
<p>The critical information for identifying the perceived sources is contained in the instantaneous coincidence among the feature channel pairs as depicted in the C-matrices (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>). At each modulation rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e006" xlink:type="simple"/></inline-formula>, the coincidence matrix at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e007" xlink:type="simple"/></inline-formula> is computed by taking the outer product of all cortical frequency-scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e008" xlink:type="simple"/></inline-formula> outputs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e009" xlink:type="simple"/></inline-formula>). Such a computation effectively estimates simultaneously the "average coincidence" over the time window implicit in each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e010" xlink:type="simple"/></inline-formula> rate, i.e., at different temporal resolutions, thus retaining both short- and long-term coincidence measures crucial for segregation. Intuitively, the idea is that responses from pairs of channels that are strongly positively correlated should belong to the same stream, while channels that are uncorrelated or anti-correlated should belong to different streams. This decomposition need not be all-or-none, but rather responses of a given channel can be parceled to different streams in proportion to the degree of the average coincidence it exhibits with the two streams. This intuitive reasoning is captured by a factorization of the coincidence matrix into two uncorrelated streams by determining the direction of maximal incoherence between the incoming stimulus patterns. One such factorization algorithm is a nonlinear principal component analysis (nPCA) of the C-matrices <xref ref-type="bibr" rid="pcbi.1003985-Kramer1">[35]</xref>, where the principal eigenvectors correspond to masks that select the channels that are positively correlated within a stream, and parcel out the others to a different stream. This procedure is implemented by an auto-encoder network with two rectifying linear hidden units corresponding to foreground and background streams as shown in <xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref> (right panel). The weights computed in the output branches of each unit are associated with each of the two sources in the input mixture, and the number of hidden units can be automatically increased if more than two segregated streams are anticipated. The nPCA is preferred over a linear PCA because the former assigns the channels of the two (often anti-correlated) sources to different eigenvectors, instead of combining them on opposite directions of a single eigenvector <xref ref-type="bibr" rid="pcbi.1003985-Nair1">[36]</xref>.</p>
<p>Another key innovation in the model implementation is that the nPCA decomposition is performed not directly on the input data from the cortical model (which are modulated at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e011" xlink:type="simple"/></inline-formula> rates), but rather on the columns of the C-matrices whose entries are either stationary or vary slowly regardless of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e012" xlink:type="simple"/></inline-formula> rates of the coincident channels. These common and slow dynamics enables stacking <italic>all</italic> C-matrices into one large matrix decomposition (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>). Specifically, the columns of the stacked matrices are applied (as a batch) to the auto-encoder network at each instant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e013" xlink:type="simple"/></inline-formula> with the aim of computing weights that can reconstruct them while minimizing the mean-square reconstruction error. Linking these matrices has two critical advantages: It ensures that the pair of eigenvectors from each matrix decomposition is consistently labeled across all matrices (e.g., source 1 is associated with eigenvector 1 in all matrices); It also couples the eigenvectors and balances their contributions to the minimization of the MSE in the auto-encoder. The weight vectors thus computed are then applied as masks on the cortical outputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e014" xlink:type="simple"/></inline-formula>. This procedure is repeated at each time step as the coincidence matrices evolve with the changing inputs.</p>
</sec><sec id="s2b">
<title>Role of feature separation, temporal continuity, and pitch in source segregation</title>
<p>The separation of feature responses on different channels and their temporal continuity are two important properties of the model that allow temporal coherence to segregate sources. Several additional perceptual attributes can play a significant role including pitch, spatial location, and timbre. Here we shall focus on pitch as an example of such attributes.</p>
<sec id="s2b1">
<title>Feature separation</title>
<p>This refers to the notion that for two sounds to be segregated, it is necessary (but insufficient) that their features induce responses in mostly different auditory channels. Temporal coherence then serves to bind the coincident channels and segregate them as one source. For example, the tone sequences of <xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2A, B</xref> are well separated at the start, and are alternating and hence non-coincident. The sequences therefore quickly stream apart perceptually and become two segregated streams of high and low tones <xref ref-type="bibr" rid="pcbi.1003985-Bregman1">[1]</xref>. When the tones approach each other and their responses interact (as in <xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2B</xref>), the channels become more coherent and the segregation fails, as is evident by the middle tones becoming momentarily attenuated in the two segregated sequences <xref ref-type="bibr" rid="pcbi.1003985-Shamma1">[25]</xref>.</p>
<fig id="pcbi-1003985-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g002</object-id><label>Figure 2</label><caption>
<title>Stream segregation of tone sequences and complexes.</title>
<p>Top row of panels represent the "mixture" audio whose two segregated streams are depicted in the middle and bottom rows. (A) The classic case of the well-separated alternating tones (top panel) becoming rapidly segregated into two streams (middle and bottom panels). (B) Continuity of the streams causes the crossing alternating tone sequences (top) to bounce maintaining an upper and a lower stream (middle and bottom panels). (C) Continuity also helps a stream maintain its integrity despite a transient synchronization with another tone. (D) When a sequence of tone complexes becomes desynchronized by more than 40 ms (top panel), they segregate into different streams despite a significant overlap (middle and bottom panels).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2b2">
<title>Temporal continuity</title>
<p>The relatively slow dynamics of the cortical rate-filters (tuned at 2–16 Hz) confer this important property on streams. Specifically, the C-matrix entries inherit the dynamics of their rate-filters and hence change only as fast as the rate of their inputs, exhibiting an <italic>inertia</italic> or continuity. This explains why a tone sequence of rapidly alternating tones across two frequency channels splits into two streams each composed of slowly changing or stationary tones. By contrast, when a tone sequence changes its frequencies slowly, a stream can track the slow change and maintain the ongoing organization (as demonstrated by the slowly varying upper and lower frequency streams of the “bouncing-tone" sequence in <xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2B</xref>). Another example is when a new distant-frequency tone suddenly appears in a sequence, the C-matrix entries cannot track it rapidly enough causing the sequence to segregate and form a new stream that perceptually pops-out of the ongoing background (<xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2C</xref>). Finally, the bandpass character of cortical rate-filtering enhances the response to tone onsets (relative to their sustained portions), and hence repeated desynchronization of <italic>onsets</italic> is sufficient to segregate tone sequences despite extensive overlap as seen in <xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2D</xref>. These same phenomena are commonly seen with mixtures of more complex signals such as speech and music where the continuity of different streams is maintained despite transient synchronization and overlap.</p>
</sec><sec id="s2b3">
<title>How pitch contributes to segregation</title>
<p>Harmonic complexes evoke pitch percepts at their fundamental and are commonly found in speech and music (see <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref> for details). <xref ref-type="fig" rid="pcbi-1003985-g003">Fig. 3A</xref> illustrates how two such alternating complexes with different pitches (500 Hz and 630 Hz) form two streams. Aside from the spectral channels, we also plot the pitch of the complexes alternating below the spectrograms. The pitch estimates are computed with a harmonic-template algorithm <xref ref-type="bibr" rid="pcbi.1003985-Shamma2">[37]</xref>, and mapped to an array of channels tuned to different values (see <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref> for details), e.g., as in the pitch-selective neurons reported in the inferior colliculus or the auditory cortex <xref ref-type="bibr" rid="pcbi.1003985-Bendor1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Langner1">[39]</xref>. We refer to the activity of this pitch-ordered array of channels as a pitch-gram. These pitch channels are exploited in the coincidence matrix computations in an analogous way to the channels of the auditory spectrograms. That is, they are simply augmented to the spectral channels to create a larger feature vector that is used to compute a correspondingly larger coincidence matrix. The additional pitch channels contribute to the segregation of the alternating complexes of <xref ref-type="fig" rid="pcbi-1003985-g003">Fig.3A</xref>. Thus, despite having some closely spaced harmonics (1890, 2000 Hz), the two complexes are sufficiently different in pitch (and in other spectral components) that they produce largely uncorrelated responses in their pitch and spectral channels and hence can be readily segregated. The C-matrices in this simulation utilize all spectral and pitch channels. Note however, that not all these channels are necessary as comparable segregation can be achieved based only on a subset of channels. For example, since the pitch channel responses are correlated with their own spectral harmonics, it is sufficient to compute the nPCA decomposition only on the columns of the pitch channels in the C-matrices (see <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref> for more details) to segregate the two complex sequences. Similarly, using coincidences between spectral scale-frequency inputs alone also yields similar segregation. In fact, if the pitch range of one harmonic complex is known (e.g., the pitch of the first complex is in the range 450 to 550 Hz), then its stream can be readily extracted by iterating the auto-encoder on the columns of the C-matrix that lie <italic>only</italic> in this pitch range. All these variations illustrate that the C-matrices can be exploited in various ways to segregate sources depending on availability of the different sound attributes, and that even partial information is often sufficient to form the streams and bind all their correlated components together. For example, if the location information is extracted and is available to the C-matrices (analogous to the pitch-grams), then they can be exploited in parallel with, and in a manner exactly analogous to the pitch. Temporal coherence can similarly help segregate speech using co-modulated signals of other modalities as in lip-reading as demonstrated later.</p>
<fig id="pcbi-1003985-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g003</object-id><label>Figure 3</label><caption>
<title>Segregation of harmonic complexes by the temporal coherence model.</title>
<p>(A) A sequence of alternating harmonic complexes (pitches  = 500 and 630 Hz). (B) The complexes are segregated using all spectral and pitch channels. Closely spaced harmonics (1890, 2000 Hz) mutually interact and hence their channels are only partially correlated with the remaining harmonics, becoming weak or may even vanish in the segregated streams.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g003" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s2c">
<title>Segregating speech from mixtures</title>
<p>Speech mixtures share many of the same characteristics already seen in the examples of <xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2</xref> and <xref ref-type="fig" rid="pcbi-1003985-g003">Fig. 3</xref>. For instance, they contain harmonic complexes with different pitches (e.g., males versus females) that often have closely spaced or temporally overlapped components. Speech also possesses other features such as broad bursts of noise immediately followed or preceded by voiced segments (as in various consonant-vowel combinations), or even accompanied by voicing (voiced consonants and fricatives). In all these cases, the syllabic onsets of one speaker synchronize a host of channels driven by the harmonics of the voicing, and that are desynchronized (or uncorrelated) with the channels driven by the other speaker. <xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4A</xref> depicts the clean spectra of two speech utterances (middle and right panels) and their mixture (left panel) illustrating the harmonic spectra and the temporal fluctuations in the speech signal at 3–7 Hz that make speech resemble the earlier harmonic sequences. The pitch tracks associated with each of these panels are shown below them.</p>
<fig id="pcbi-1003985-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g004</object-id><label>Figure 4</label><caption>
<title>Segregation of speech mixtures.</title>
<p>(A) Mixture of two sample utterances (left panel) spoken by a female (middle panel) and male (right panel); pitch tracks of the utterances are shown below each panel. (B) The segregated speech using all C-matrix columns. (C) The segregated speech using only coincidences among the frequency-scale channels (<italic>no pitch</italic> information). (D) The segregated speech using the channels surrounding the pitch channels of the female speaker as the anchor.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g004" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4B</xref> illustrates the segregation of the two speech streams from the mixture using all available coincidence among the spectral (frequency-scale) and pitch channels in the C-matrices. The reconstructed spectrograms are not identical to the originals (<xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4A</xref>), an inevitable consequence of the energetic masking among the crisscrossing components of the two speakers. Nevertheless, with two speakers there are sufficient gaps between the syllables of each speaker to provide clean, unmasked views of the other speaker's signal <xref ref-type="bibr" rid="pcbi.1003985-Viemeister1">[40]</xref>. If more speakers are added to the mix, such gaps become sparser and the amount of energetic masking increases, and that is why it is harder to segregate one speaker in a crowd if they are not distinguished by unique features or a louder signal. An interesting aspect of speech is that the relative amplitudes of its harmonics vary widely over time reflecting the changing formants of different phonemes. Consequently, the saliency of the harmonic components changes continually, with weaker ones dropping out of the mixture as they become completely masked by the stronger components. Despite these changes, speech syllables of one speaker maintain a stable representation of a sufficient number of features from one time instant to the next, and thus can maintain the continuity of their stream. This is especially true of the pitch (which changes only slowly and relatively little during normal speech). The same is true of the spectral region of maximum energy which reflects the average formant locations of a given speaker, reflecting partially the timbre and length of their vocal tract. Humans utilize either of these cues alone or in conjunction with additional cues to segregate mixtures. For instance, to segregate speech with overlapping pitch ranges (a mixture of male speakers), one may rely on the different spectral envelopes (timbres), or on other potentially different features such as location or loudness. Humans can also exploit more complex factors such as higher-level linguistic knowledge and memory as we discuss later.</p>
<p>In the example of <xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4C</xref>, the two speakers of <xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4A</xref> are segregated based on the coincidence of only the spectral components conveyed by the frequency-scale channels. The extracted speech streams of the two speakers resemble the original unmixed signals, and their reconstructions exhibit significantly less mutual interference than the mixture as quantified later.Finally, as we discuss in more detail below, it is possible to segregate the speech mixture based on the pattern of correlations computed with one “anchor” feature such as the pitch channels of the female, i.e., using only the columns of the C-matrix near the female pitch channels as illustrated in <xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4D</xref>.</p>
<p>Exactly the same logic can be applied to any auxiliary function that is co-modulated in the same manner as the rest of the speech signal. For instance, one may “look” at the lip movements of a speaker which open and close in a manner that closely reflects the instantaneous power in the signal (or its envelope) as demonstrated in <xref ref-type="bibr" rid="pcbi.1003985-Chandrasekaran1">[41]</xref>. These two functions (inter-lip distance and the acoustic envelope) can then be exploited to segregate the target speech much as with the pitch channels earlier. Thus, by simply computing the correlation between the lip function (<xref ref-type="fig" rid="pcbi-1003985-g005">Fig. 5B</xref>) or the acoustic envelope (<xref ref-type="fig" rid="pcbi-1003985-g005">Fig. 5C</xref>) with all the remaining channels, an effective mask can be readily computed to extract the target female speech (and the background male speech too). This example thus illustrates how in general any other co-modulated features of the speech signal (e.g., location, loudness, timbre, and visual signals such as lip movements can contribute to segregation of complex mixtures).</p>
<fig id="pcbi-1003985-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g005</object-id><label>Figure 5</label><caption>
<title>Segregation of speech utterances based on auxiliary functions.</title>
<p>(A) Mixture of two sample utterances (right panel) spoken by a female (left panel) and male (middle panel) speakers; (B) The inter-lip distance of the female saying <italic>“twice each day”</italic> used as the anchor to segregate the mixture into its target female (middle panel) and the remaining male speech (bottom panel); (C) The envelope of the female speech “twice each day” used as anchor to segregate the mixture into its target female speaker (middle panel) and the remaining male speech (bottom speech).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g005" position="float" xlink:type="simple"/></fig>
<p>The performance of the model is quantified with a database of 100 mixtures formed from pairs of male-female speech randomly sampled from the TIMIT database (<xref ref-type="fig" rid="pcbi-1003985-g006">Fig. 6</xref>) where the spectra of the clean speech are compared to those of the corresponding segregated versions. The signal-to-noise ratio is computed as<disp-formula id="pcbi.1003985.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003985.e015" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pcbi.1003985.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003985.e016" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<fig id="pcbi-1003985-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g006</object-id><label>Figure 6</label><caption>
<title>Signal to noise ratio.</title>
<p>(<bold>A</bold>) Box plot of the SNR of the segregated speech and the mixture over 100 mixtures from the TIMIT corpus. (<bold>B</bold>) (Top) Notation used for coincidence measures computed between the original and segregated sentences plotted in panels below. (Middle) Distribution of coincidence in the cortical domain between each segregated speech and its corresponding original version (violet) and original interferer (magenta). 100 pairs of sentences from the TIMIT corpus were mixed together with equal power. (Bottom) Scatter plot of difference between correlation of original sentences with each segregated sentence demonstrates that the two segregated sentences correlate well with different original sentences.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g006" position="float" xlink:type="simple"/></fig>
<p>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e017" xlink:type="simple"/></inline-formula> are the cortical representations of the segregated sentences and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e018" xlink:type="simple"/></inline-formula> are the cortical representations of the original sentences and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e019" xlink:type="simple"/></inline-formula> is the cortical representation of the mixture. Average SNR improvement was 6 dB for mixture waveforms mixed at 0 dB.</p>
<p>Another way to demonstrate the effectiveness of the segregation is to compare the match between the segregated samples and their corresponding originals. This is evidenced by the minimal overlap in <xref ref-type="fig" rid="pcbi-1003985-g006">Fig. 6B</xref> (middle panel) across the distributions of the coincidences computed between each segregated sentence and its original version versus the interfering speech. To compare directly these coincidences for each pair of mixed sentences, the difference between coincidences in each mixture are scatter-plotted in the bottom panel. Effective pairwise segregation (e.g., not extracting only one of the mixed sentences) places the scatter points along the diagonal. Examples of segregated and reconstructed audio files can be found in <bold><xref ref-type="supplementary-material" rid="pcbi.1003985.s001">S1 Dataset</xref></bold>.</p>
<sec id="s2c1">
<title>Segregating speech from music and noise</title>
<p>In principle, segregating mixtures does not depend on them being speech or music, but rather that the signals have different spectrotemporal patterns and exhibit a continuity of features. <xref ref-type="fig" rid="pcbi-1003985-g007">Fig. 7A</xref> illustrates the extraction of a speech signal from a highly overlapping temporally modulated street noise background. The same speech sample is extracted from a mixture with music in <xref ref-type="fig" rid="pcbi-1003985-g007">Fig. 7B</xref>. As explained earlier, this segregation (psychoacoustically and in the model) becomes more challenging in the absence of “clean looks”, as when the background is an unmodulated white noise or babble that energetically masks the target speech.</p>
<fig id="pcbi-1003985-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003985.g007</object-id><label>Figure 7</label><caption>
<title>Extraction of speech from noise and music.</title>
<p>(A) Speech mixed with street noise of many overlapping spectral peaks (left panel). The two signals are uncorrelated and hence can be readily segregated and the speech reconstructed (right panel). (B) Extraction of speech (right panel) from a mixture of speech and a sustained oboe melody (left panel).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003985.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s2d">
<title>Attention and memory in streaming</title>
<p>So far, attention and memory have played no direct role in the segregation, but adding them is relatively straightforward. From a computational point of view, attention can be interpreted as a focus directed to one or a few features or feature subspaces of the cortical model which enhances their amplitudes relative to other unattended features. For instance, in segregating speech mixtures, one might choose to attend specifically to the high female pitch in a group of male speakers (<xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4D</xref>), or to attend to the location cues or the lip movements (<xref ref-type="fig" rid="pcbi-1003985-g005">Fig. 5C</xref>) and rely on them to segregate the speakers. In these cases, only the appropriate subset of columns of the C-matrices are needed to compute the nPCA decomposition (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>). This is in fact also the interpretation of the simulations discussed in <xref ref-type="fig" rid="pcbi-1003985-g003">Fig. 3</xref> for harmonic complexes. In all these cases, the segregation exploited only the C-matrix columns marking coincidences of the attended <italic>anchor</italic> channels (pitch, lip, loudness) with the remaining channels.</p>
<p>Memory can also be strongly implicated in stream segregation in that it constitutes <italic>priors</italic> about the sources which can be effectively utilized to process the C-matrices and perform the segregation. For example, in extracting the melody of the violins in a large orchestra, it is necessary to know first what the timbre of a violin is before one can turn the attentional focus to its unique spectral shape features and pitch range. One conceptually simple way (among many) of exploiting such information is to use as ‘template’ the average auto-encoder weights (masks) computed from iterating on clean patterns of a particular voice or instrument, and use the resulting weights to perform an initial segregation of the desired source by applying the mixture to the stored mask directly.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>A biologically plausible model of auditory cortical processing can be used to implement the perceptual organization of auditory scenes into distinct auditory objects (streams). Two key ingredients are essential: (1) a multidimensional cortical representation of sound that explicitly encodes various acoustic features along which streaming can be induced; (2) clustering of the temporally coherent features into different streams. Temporal coherence is quantified by the coincidence between all pairs of cortical channels, slowly integrated at cortical time-scales as described in <xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1</xref>. An auto-encoder network mimicking Hebbian synaptic rules implements the clustering through nonlinear PCA to segregate the sound mixture into a foreground and a background.</p>
<p>The temporal coherence model segregates novel sounds based exclusively on the ongoing temporal coherence of their perceptual attributes. Previous efforts at exploiting explicitly or implicitly the correlations among stimulus features differed fundamentally in the details of their implementation. For example, some algorithms attempted to decompose directly the channels of the spectrogram representations <xref ref-type="bibr" rid="pcbi.1003985-Lee1">[42]</xref> rather than the more distributed multi-scale cortical representations. They either used the fast phase-locked responses available in the early auditory system <xref ref-type="bibr" rid="pcbi.1003985-vonderMalsburg1">[43]</xref>, or relied exclusively on the pitch-rate responses induced by interactions among the unresolved harmonics of a voiced sound <xref ref-type="bibr" rid="pcbi.1003985-Schimmel1">[44]</xref>. Both these temporal cues, however, are much faster than cortical dynamics (&gt;100 Hz) and are highly volatile to the phase-shifts induced in different spectral regions by mildly reverberant environments. The cortical model instead naturally exploits multi-scale dynamics and spectral analyses to define the structure of all these computations as well as their parameters. For instance, the product of the wavelet coefficients (entries of the C-matrices) naturally compute the running-coincidence between the channel pairs, integrated over a time-interval determined by the time-constants of the cortical rate-filters (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1</xref> and <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>). This insures that all coincidences are integrated over time intervals that are commensurate with the dynamics of the underlying signals and that a balanced range of these windows are included to process slowly varying (2 Hz) up to rapidly changing (16 Hz) features.</p>
<p>The biological plausibility of this model rests on physiological and anatomical support for the two postulates of the model: a cortical multidimensional representation of sound and coherence-dependent computations. The cortical representation is the end-result of a sequence of transformations in the early and central auditory system with experimental support discussed in detail in <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>. The version used here incorporates only a frequency (tonotopic) axis, spectrotemporal analysis (scales and rates), and pitch analysis <xref ref-type="bibr" rid="pcbi.1003985-Shamma2">[37]</xref>. However, other features that are pre-cortically extracted can be readily added as inputs to the model such as spatial location (from interaural differences and elevation cues) and pitch of unresolved harmonics <xref ref-type="bibr" rid="pcbi.1003985-Moore1">[45]</xref>.</p>
<p>The second postulate concerns the crucial role of temporal coherence in streaming. It is a relatively recent hypothesis and hence direct tests remain scant. Nevertheless, targeted psychoacoustic studies have already provided perceptual support of the idea that coherence of stimulus-features is <italic>necessary</italic> for perception of streams <xref ref-type="bibr" rid="pcbi.1003985-Micheyl1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Teki1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Micheyl2">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Christiansen1">[47]</xref>. Parallel physiological experiments have also demonstrated that coherence is a critical ingredient in streaming and have provided indirect evidence of its mechanisms through rapidly adapting cooperative and competitive interactions between coherent and incoherent responses <xref ref-type="bibr" rid="pcbi.1003985-Elhilali1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Shamma3">[48]</xref>. Nevertheless, much more remains uncertain. For instance, where are these computations performed? How exactly are the (auto-encoder) clustering analyses implemented? And what exactly is the role of attentive listening (versus pre-attentive processing) in facilitating the various computations? All these uncertainties, however, invoke coincidence-based computations and adaptive mechanisms that have been widely studied or postulated such as coincidence detection and Hebbian associations <xref ref-type="bibr" rid="pcbi.1003985-Sejnowski1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Abbott1">[50]</xref>.</p>
<p>Dimensionality-reduction of the coincidence matrix (through nonlinear PCA) allows us effectively to cluster all correlated channels apart from others, thus grouping and designating them as belonging to distinct sources. This view bears a close relationship to the predictive clustering-based algorithm by <xref ref-type="bibr" rid="pcbi.1003985-Elhilali2">[51]</xref> in which input feature vectors are gradually clustered (or routed) into distinct streams. In both the coherence and clustering algorithms, cortical dynamics play a crucial role in integrating incoming data into the appropriate streams, and therefore are expected to exhibit for the most part similar results. In some sense, the distinction between the two approaches is one of implementation rather than fundamental concepts. Clustering patterns and reducing their features are often (but not always) two sides of the same coin, and can be shown under certain conditions to be largely equivalent and yield similar clusters <xref ref-type="bibr" rid="pcbi.1003985-Duda1">[52]</xref>. Nevertheless, from a biological perspective, it is important to adopt the correlation view as it suggests concrete mechanisms to explore.</p>
<p>Our emphasis thus far has been on demonstrating the ability of the model to perform unsupervised (automatic) source segregation, much like a listener that has no specific objectives. In reality, of course, humans and animals utilize intentions and attention to selectively segregate one source as the foreground against the remaining background. This operational mode would similarly apply in applications in which the user of a technology identifies a target voice to enhance and isolate from among several based on the pitch, timbre, location, or other attributes. The temporal coherence algorithm can be readily and gracefully adapted to incorporate such information and task objectives, as when specific subsets of the C-matrix columns are used to segregate a targeted stream (e.g., <xref ref-type="fig" rid="pcbi-1003985-g003">Fig. 3</xref> and <xref ref-type="fig" rid="pcbi-1003985-g004">Fig. 4</xref>). In fact, our experience with the model suggests that segregation is usually of better quality and faster to compute with attentional priors.</p>
<p>In summary, we have described a model for segregating complex sound mixtures based on the temporal coherence principle. The model computes the coincidence of multi-scale cortical features and clusters the coherent responses as emanating from one source. It requires no prior information, statistics, or knowledge of source properties, but can gracefully incorporate them along with cognitive influences such as attention to, or memory of specific attributes of a target source to segregate it from its background. The model provides a testable framework of the physiological bases and psychophysical manifestations of this remarkable ability. Finally, the relevance of these ideas transcends the auditory modality to elucidate the robust visual perception of cluttered scenes <xref ref-type="bibr" rid="pcbi.1003985-Blake1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003985-Alais1">[54]</xref>.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>The auditory representation</title>
<p>Sound is first transformed into its auditory spectrogram, followed by a cortical spectrotemporal analysis of the modulations of the spectrogram (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1A</xref>) <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>. <italic>Pitch</italic> is an additional perceptual attribute that is derived from the resolved (low-order) harmonics and used in the model <xref ref-type="bibr" rid="pcbi.1003985-Shamma2">[37]</xref>. It is represented as a ‘pitch-gram’ of additional channels that are simply augmented to the cortical spectral channels prior to subsequent rate analysis (see below). Other perceptual attributes such as location and unresolved harmonic pitch can also be computed and represented by an array of channels analogously to the pitch estimates.</p>
<p>The auditory spectrogram, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e020" xlink:type="simple"/></inline-formula>, is generated by a model of early auditory processing <xref ref-type="bibr" rid="pcbi.1003985-Wang4">[55]</xref>, which begins with an affine wavelet transform of the acoustic signal, followed by nonlinear rectification and compression, and lateral inhibition to sharpen features. This results in <italic>F</italic> = 128 frequency channels that are equally spaced on a logarithmic frequency axis over 5.2 octaves.</p>
<p>Cortical spectro-temporal analysis of the spectrogram is effectively performed in two steps <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>: a spectral wavelet decomposition followed by a temporal wavelet decomposition, as depicted in <xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1A</xref>. The first analysis provides multi-scale (multi-bandwidth) views of each spectral slice <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e021" xlink:type="simple"/></inline-formula>, resulting in a 2D <italic>frequency-scale</italic> representation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e022" xlink:type="simple"/></inline-formula>. It is implemented by convolving the spectral slice with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e023" xlink:type="simple"/></inline-formula> complex-valued spectral receptive fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e024" xlink:type="simple"/></inline-formula> similar to Gabor functions, parametrized by spectral tuning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e025" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e026" xlink:type="simple"/></inline-formula>.</p>
<p>The outcome of this step is an array of <italic>F</italic>x<italic>S</italic> frequency-scale channels indexed by frequency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e027" xlink:type="simple"/></inline-formula> and local spectral bandwidth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e028" xlink:type="simple"/></inline-formula> at each time instant <italic>t</italic>. We typically used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e029" xlink:type="simple"/></inline-formula>  = 2 to 5 scales in our simulations (e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e030" xlink:type="simple"/></inline-formula> cyc/oct), producing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e031" xlink:type="simple"/></inline-formula> copies of the spectrogram channels with different degrees of spectral smoothing. In addition, the pitch of each spectrogram frame is also computed (if desired) using a harmonic template-matching algorithm <xref ref-type="bibr" rid="pcbi.1003985-Shamma2">[37]</xref>. Pitch values and saliency were then expressed as a <italic>pitch-gram</italic> (<italic>P</italic>) channels that are appended to the frequency-scale channels (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>).</p>
<p>The cortical rate-analysis is then applied to the modulus of each of the channel outputs in the freq-scale-pitch array by passing them through an array <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e032" xlink:type="simple"/></inline-formula> of modulation-selective filters (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e033" xlink:type="simple"/></inline-formula>), each indexed by its center rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e034" xlink:type="simple"/></inline-formula> which range over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e035" xlink:type="simple"/></inline-formula> Hz in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e036" xlink:type="simple"/></inline-formula> octave steps (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>). This temporal wavelet analysis of the response of each channel is described in detail in <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>. Therefore, the final representation of the cortical outputs (features) is along four axes denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e037" xlink:type="simple"/></inline-formula>. It consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e038" xlink:type="simple"/></inline-formula> coincidence matrices per time frame, each of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e039" xlink:type="simple"/></inline-formula>x(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e040" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>).</p>
<p>The exact choice of all above parameters is not critical for the model in that the performance changes very gradually when the parameters or number of feature channels are altered. All parameter values in the model were chosen based on previous simulations with the various components of the model. For example, the choice of rates (2–32 Hz) and scales (1–8 cyc/oct) reflected their utility in the representation of speech and other complex sounds in numerous previous applications of the cortical model <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>. Thus, the parameters chosen were known to reflect speech and music, but ofcourse could have been chosen differently if the stimuli were drastically different. The least committal choice is to include the largest range of scales and rates that is computationally feasible. In our implementations, the algorithm became noticeably slow when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e041" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e042" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e043" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e044" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b">
<title>Coherence computations and nonlinear principal component analysis</title>
<p>The decomposition of the C-matrices is carried out as described earlier in <xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>. The iterative procedure to learn the auto-encoder weights employs Limited-memory Broyden-Fletcher-Goldfarb-Shannon (L-BFGS) method as implemented in <xref ref-type="bibr" rid="pcbi.1003985-Schmidt1">[56]</xref>. The <italic>output</italic> weight vectors (<xref ref-type="fig" rid="pcbi-1003985-g001">Fig. 1B</xref>) thus computed are subsequently applied as masks on the input channels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e045" xlink:type="simple"/></inline-formula>. This procedure that is repeated every time step using the weights learned in the previous time step as initial conditions to ensure that the assignment of the learned eigenvectors remains consistent over time. Note that the C matrices do not change rapidly, but rather slowly, as fast as the time-constants of their corresponding rate analyses allow (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e046" xlink:type="simple"/></inline-formula>). For example, for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e047" xlink:type="simple"/></inline-formula> Hz filters, the cortical outputs change slowly reflecting a time-constant of approximately 250 ms. More often, however, the C-matrix entries change much slower reflecting the sustained coincidence patterns between different channels. For example, in the simple case of two alternating tones (<xref ref-type="fig" rid="pcbi-1003985-g002">Fig. 2A</xref>), the C-matrix entries reach a steady state after a fraction of a second, and then remain constant reflecting the unchanging coincidence pattern between the two tones. Similarly, if the pitch of a speaker remains relatively constant, then the correlation between the harmonic channels remains approximately constant since the partials are modulated similarly in time. This aspect of the model explains the source of the continuity in the streams. The final step in the model is to invert the <italic>masked</italic> cortical outputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003985.e048" xlink:type="simple"/></inline-formula> back to the sound <xref ref-type="bibr" rid="pcbi.1003985-Chi1">[34]</xref>.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003985.s001" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pcbi.1003985.s001" position="float" xlink:type="simple"><label>S1 Dataset</label><caption>
<p><bold>Example segregation of a male-female mixture.</bold> The female sentence is <italic>‘The clothes dried on a thin wooden rack’</italic>. The male sentence is <italic>‘The juice of lemons makes fine punch’</italic>. <italic>Female_original.wav</italic> is the original female speech. <italic>Male_original.wav</italic> is the original male speech. <italic>Mixture.wav</italic> is the 0 dB mixture speech. <italic>Female_reconstructed.wav</italic> is the segregated female speech and <italic>Male_reconstructed.wav</italic> is the segregated male speech.</p>
<p>(ZIP)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Dr. Chandramouli Chandrasekaran and Dr. Asif Ghazanfar for providing the inter-lip distance data.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003985-Bregman1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">Bregman AS (1990) Auditory Scene Analysis: The Perceptual Organization of Sound. MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Cherry1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cherry</surname><given-names>EC</given-names></name> (<year>1953</year>) <article-title>Some experiments on the recognition of speech, with one and with two ears</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>25</volume>: <fpage>975</fpage>–<lpage>979</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Bee1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bee</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name> (<year>2008</year>) <article-title>The “Cocktail party problem”: What is it? how can it be solved? and why should animal behaviorists study it?</article-title> <source>Journal of comparative psychology</source> <volume>122</volume>: <fpage>235</fpage>–<lpage>251</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Singh1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singh</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2003</year>) <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>114</volume>: <fpage>3394</fpage>–<lpage>3411</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Henderson1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Chanceaux</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>TJ</given-names></name> (<year>2009</year>) <article-title>The influence of clutter on real-world scene search: Evidence from search efficiency and eye movements</article-title>. <source>Journal of Vision</source> <volume>9</volume>: <fpage>32</fpage>–<lpage>40</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Jones1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname><given-names>G</given-names></name> (<year>2013</year>) <article-title>Sensory biology: Listening in the dark for echoes from silent and stationary prey</article-title>. <source>Current Biology</source> <volume>23</volume>: <fpage>R249</fpage>–<lpage>R251</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Kristjansson1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Kristjansson T, Hershey J, Olsen P, Rennie S, Gopinath R (2006) Super-human multi-talker speech recognition: The IBM 2006 speech separation challenge system. In: in ICSLP. pp. 97–100.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Comon1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Comon P, Jutten C (2010) Handbook of Blind Source Separation: Independent Component Analysis and Applications. Academic Press.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Smaragdis1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Smaragdis P (2004) Non-negative matrix factor deconvolution; extraction of multiple sound sources from monophonic inputs. In: PuntonetCG, PrietoA, editors, Independent Component Analysis and Blind Signal Separation, Springer Berlin Heidelberg, number 3195 in Lecture Notes in Computer Science. pp. 494–499.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Ellis1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Ellis DPW (2006) Model-based scene analysis. In: Computational Auditory Scene Analysis: Principles, Algorithms, and Applications, Wiley/IEEE Press. pp. 115–146.</mixed-citation>
</ref>
<ref id="pcbi.1003985-King1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">King B, Atlas L (2010) Single-channel source separation using simplified-training complex matrix factorization. In: 2010 IEEE International Conference on Acoustics Speech and Signal Processing. pp. 4206–4209.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Cooke1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooke</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hershey</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Rennie</surname><given-names>SJ</given-names></name> (<year>2010</year>) <article-title>Monaural speech separation and recognition challenge</article-title>. <source>Computer Speech &amp; Language</source> <volume>24</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Brown1"><label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Brown GJ (2010) Physiological models of auditory scene analysis. In: Meddis R, opez-Poveda L E A, Fay R R, Popper A N, editors, Computational Models of the Auditory System, Springer US, number 35 in Springer Handbook of Auditory Research. pp. 203–236.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Shao1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>D</given-names></name> (<year>2009</year>) <article-title>Sequential organization of speech in computational auditory scene analysis</article-title>. <source>Speech Communication</source> <volume>51</volume>: <fpage>657</fpage>–<lpage>667</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Hartmann1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hartmann</surname><given-names>WM</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>D</given-names></name> (<year>1991</year>) <article-title>Stream segregation and peripheral channeling</article-title>. <source>Music Perception: An Interdisciplinary Journal</source> <volume>9</volume>: <fpage>155</fpage>–<lpage>183</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Beauvois1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauvois</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Meddis</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Computer simulation of auditory stream segregation in alternating-tone sequences</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>99</volume>: <fpage>2270</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-McCabe1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McCabe</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Denham</surname><given-names>MJ</given-names></name> (<year>1997</year>) <article-title>A model of auditory streaming</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>101</volume>: <fpage>1611</fpage>–<lpage>1621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Stark1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stark</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wohlmayr</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pernkopf</surname><given-names>F</given-names></name> (<year>2011</year>) <article-title>Source-filter-based single-channel speech separation using pitch information</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source> <volume>19</volume>: <fpage>242</fpage>–<lpage>255</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Hu1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hu</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>A tandem algorithm for pitch estimation and voiced speech segregation</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source> <volume>18</volume>: <fpage>2067</fpage>–<lpage>2079</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Clark1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clark</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Atlas</surname><given-names>L</given-names></name> (<year>2009</year>) <article-title>Time-frequency coherent modulation filtering of nonstationary signals</article-title>. <source>IEEE Transactions on Signal Processing</source> <volume>57</volume>: <fpage>4323</fpage>–<lpage>4332</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Mill1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Mill R, Bohm T, Bendixen A, Winkler I, Denham S (2011) CHAINS: competition and cooperation between fragmentary event predictors in a model of auditory scene analysis. In: 2011 45th Annual Conference on Information Sciences and Systems (CISS). pp. 1–6.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Hupe1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hupe</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>The initial phase of auditory and visual scene analysis</article-title>. <source>Philosophical transactions of the Royal Society of London Series B, Biological sciences</source> <volume>367</volume>: <fpage>942</fpage>–<lpage>953</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-VonDerMalsburg1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Von Der Malsburg</surname><given-names>C</given-names></name> (<year>1994</year>) <article-title>The correlation theory of brain function</article-title>. <source>Models of neural networks</source> <volume>2</volume>: <fpage>95119</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Wang1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Buhmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>von der Malsburg</surname><given-names>C</given-names></name> (<year>1990</year>) <article-title>Pattern segmentation in associative memory</article-title>. <source>Neural Computation</source> <volume>2</volume>: <fpage>94</fpage>–<lpage>106</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Shamma1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Temporal coherence and attention in auditory scene analysis</article-title>. <source>Trends in Neurosciences</source> <volume>34</volume>: <fpage>114</fpage>–<lpage>123</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Elhilali1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2009</year>) <article-title>Temporal coherence in the perceptual organization and cortical representation of auditory scenes</article-title>. <source>Neuron</source> <volume>61</volume>: <fpage>317</fpage>–<lpage>329</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Micheyl1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Hunter</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2010</year>) <article-title>Auditory stream segregation and the perception of across-frequency synchrony</article-title>. <source>Journal of experimental psychology Human perception and performance</source> <volume>36</volume>: <fpage>1029</fpage>–<lpage>1039</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Teki1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Teki S, Chait M, Kumar S, Shamma S, Griffiths TD (2013) Segregation of complex acoustic scenes based on temporal coherence. eLife 2.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Wang2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>D</given-names></name> (<year>1996</year>) <article-title>Primitive auditory segregation based on oscillatory correlation</article-title>. <source>Cognitive Science</source> <volume>20</volume>: <fpage>409</fpage>–<lpage>456</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Large1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Large</surname><given-names>EW</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>MR</given-names></name> (<year>1999</year>) <article-title>The dynamics of attending: How people track time-varying events</article-title>. <source>Psychological Review</source> <volume>106</volume>: <fpage>119</fpage>–<lpage>159</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Wang3"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>GJ</given-names></name> (<year>1999</year>) <article-title>Separation of speech from interfering sounds based on oscillatory correlation</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>10</volume>: <fpage>684</fpage>–<lpage>697</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Wrigley1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wrigley</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>GJ</given-names></name> (<year>2004</year>) <article-title>A computational model of auditory selective attention</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>15</volume>: <fpage>1151</fpage>–<lpage>1163</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Almonte1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Almonte</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Jirsa</surname><given-names>VK</given-names></name>, <name name-style="western"><surname>Large</surname><given-names>EW</given-names></name>, <name name-style="western"><surname>Tuller</surname><given-names>B</given-names></name> (<year>2005</year>) <article-title>Integration and segregation in auditory streaming</article-title>. <source>Physica D: Nonlinear Phenomena</source> <volume>212</volume>: <fpage>137</fpage>–<lpage>159</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Chi1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ru</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2005</year>) <article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>118</volume>: <fpage>887</fpage>–<lpage>906</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Kramer1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kramer</surname><given-names>MA</given-names></name> (<year>1991</year>) <article-title>Nonlinear principal component analysis using autoassociative neural networks</article-title>. <source>AIChE Journal</source> <volume>37</volume>: <fpage>233</fpage>–<lpage>243</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Nair1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Nair V, Hinton GE (2010) Rectified linear units improve restricted boltzmann machines. In: Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807–814.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Shamma2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Klein</surname><given-names>D</given-names></name> (<year>2000</year>) <article-title>The case of the missing pitch templates: how harmonic templates emerge in the early auditory system</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>107</volume>: <fpage>2631</fpage>–<lpage>2644</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Bendor1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bendor</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name> (<year>2005</year>) <article-title>The neuronal representation of pitch in primate auditory cortex</article-title>. <source>Nature</source> <volume>436</volume>: <fpage>1161</fpage>–<lpage>1165</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Langner1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name> (<year>1988</year>) <article-title>Periodicity coding in the inferior colliculus of the cat. i. neuronal mechanisms</article-title>. <source>J Neurophysiol</source> <volume>60</volume>: <fpage>1799</fpage>–<lpage>1822</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Viemeister1"><label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Viemeister NF, Stellmack MA, Byrne AJ (2005) The role of temporal structure in envelope processing. In: Pressnitzer D, Cheveign A d, McAdams S, Collet L, editors, Auditory Signal Processing, Springer New York. pp. 220–228.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Chandrasekaran1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Trubanova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Stillittano</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000436</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Lee1"><label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee DD, Seung HS (2000) Algorithms for non-negative matrix factorization. In: Advances in neural information processing systems. pp. 556–562.</mixed-citation>
</ref>
<ref id="pcbi.1003985-vonderMalsburg1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>von der Malsburg</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>W</given-names></name> (<year>1986</year>) <article-title>A neural cocktail-party processor</article-title>. <source>Biological cybernetics</source> <volume>54</volume>: <fpage>29</fpage>–<lpage>40</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Schimmel1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Schimmel S, Atlas L, Nie K (2007) Feasibility of single channel speaker separation based on modulation frequency analysis. In: IEEE International Conference on Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. volume 4, pp. 605–608.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Moore1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Moore BCJ (2003) An introduction to the psychology of hearing. Amsterdam; Boston: Academic Press.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Micheyl2"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name> (<year>2010</year>) <article-title>Pitch, harmonicity and concurrent sound segregation: psychoacoustical and neurophysiological findings</article-title>. <source>Hearing research</source> <volume>266</volume>: <fpage>36</fpage>–<lpage>51</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Christiansen1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Christiansen</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>Jepsen</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Dau</surname><given-names>T</given-names></name> (<year>2014</year>) <article-title>Effects of tonotopicity, adaptation, modulation tuning, and temporal coherence in primitive auditory stream segregationa)</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>135</volume>: <fpage>323</fpage>–<lpage>333</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Shamma3"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname><given-names>AJ</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Temporal coherence and the streaming of complex sounds</article-title>. <source>Advances in experimental medicine and biology</source> <volume>787</volume>: <fpage>535</fpage>–<lpage>543</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Sejnowski1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Sejnowski TJ, Tesauro G (1989) The hebb rule for synaptic plasticity: algorithms and implementations. In: Neural models of plasticity: Experimental and theoretical approaches, Academic Press, New York. pp. 94–103.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Abbott1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2000</year>) <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1178</fpage>–<lpage>1183</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Elhilali2"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2008</year>) <article-title>A cocktail party with a cortical twist: How cortical mechanisms contribute to sound segregation</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>124</volume>: <fpage>3751</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Duda1"><label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Duda RO, Hart PE (1973) Pattern classification and scene analysis. New York: Wiley.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Blake1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blake</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>SH</given-names></name> (<year>2005</year>) <article-title>The role of temporal structure in human vision</article-title>. <source>Behavioral and Cognitive Neuroscience Reviews</source> <volume>4</volume>: <fpage>21</fpage>–<lpage>42</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Alais1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Blake</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>SH</given-names></name> (<year>1998</year>) <article-title>Visual features that vary together over time group together over space</article-title>. <source>Nature neuroscience</source> <volume>1</volume>: <fpage>160</fpage>–<lpage>164</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Wang4"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Self-normalization and noise-robustness in early auditory representations</article-title>. <source>IEEE Transactions on Speech and Audio Processing</source> <volume>2</volume>: <fpage>421</fpage>–<lpage>435</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003985-Schmidt1"><label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Schmidt M (2012). minFunc - unconstrained differentiable multivariate optimization in matlab. URL <ext-link ext-link-type="uri" xlink:href="http://www.di.ens.fr/mschmidt/Software/minFunc.html" xlink:type="simple">http://www.di.ens.fr/mschmidt/Software/minFunc.html</ext-link>.</mixed-citation>
</ref>
</ref-list></back>
</article>