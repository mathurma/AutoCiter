<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01162</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003466</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Behavioral neuroscience</subject><subject>Computational neuroscience</subject><subject>Learning and memory</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Modelling Individual Differences in the Form of Pavlovian Conditioned Approach Responses: A Dual Learning Systems Approach with Factored Representations</article-title>
<alt-title alt-title-type="running-head">Modelling Individual Differences in Pavlovian CRs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lesaint</surname><given-names>Florian</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sigaud</surname><given-names>Olivier</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Flagel</surname><given-names>Shelly B.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Robinson</surname><given-names>Terry E.</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname><given-names>Mehdi</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Institut des Systèmes Intelligents et de Robotique, UMR 7222, UPMC Univ Paris 06, Paris, France</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Institut des Systèmes Intelligents et de Robotique, UMR 7222, CNRS, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department of Psychiatry, University of Michigan, Ann Arbor, Michigan, United States of America</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Molecular and Behavioral Neuroscience Institute, University of Michigan, Ann Arbor, Michigan, United States of America</addr-line></aff>
<aff id="aff5"><label>5</label><addr-line>Department of Psychology, University of Michigan, Ann Arbor, Michigan, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">lesaint@isir.upmc.fr</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: FL OS SBF TER MK. Performed the experiments: FL. Analyzed the data: FL OS SBF TER MK. Contributed reagents/materials/analysis tools: FL SBF TER. Wrote the paper: FL OS SBF TER MK.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>2</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>13</day><month>2</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>2</issue>
<elocation-id>e1003466</elocation-id>
<history>
<date date-type="received"><day>1</day><month>7</month><year>2013</year></date>
<date date-type="accepted"><day>19</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Lesaint et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Reinforcement Learning has greatly influenced models of conditioning, providing powerful explanations of acquired behaviour and underlying physiological observations. However, in recent autoshaping experiments in rats, variation in the form of Pavlovian conditioned responses (CRs) and associated dopamine activity, have questioned the classical hypothesis that phasic dopamine activity corresponds to a reward prediction error-like signal arising from a classical Model-Free system, necessary for Pavlovian conditioning. Over the course of Pavlovian conditioning using food as the unconditioned stimulus (US), some rats (sign-trackers) come to approach and engage the conditioned stimulus (CS) itself – a lever – more and more avidly, whereas other rats (goal-trackers) learn to approach the location of food delivery upon CS presentation. Importantly, although both sign-trackers and goal-trackers learn the CS-US association equally well, only in sign-trackers does phasic dopamine activity show classical reward prediction error-like bursts. Furthermore, neither the acquisition nor the expression of a goal-tracking CR is dopamine-dependent. Here we present a computational model that can account for such individual variations. We show that a combination of a Model-Based system and a revised Model-Free system can account for the development of distinct CRs in rats. Moreover, we show that revising a classical Model-Free system to individually process stimuli by using factored representations can explain why classical dopaminergic patterns may be observed for some rats and not for others depending on the CR they develop. In addition, the model can account for other behavioural and pharmacological results obtained using the same, or similar, autoshaping procedures. Finally, the model makes it possible to draw a set of experimental predictions that may be verified in a modified experimental protocol. We suggest that further investigation of factored representations in computational neuroscience studies may be useful.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Acquisition of responses towards full predictors of rewards, namely Pavlovian conditioning, has long been explained using the reinforcement learning theory. This theory formalizes learning processes that, by attributing values to situations and actions, makes it possible to direct behaviours towards rewarding objectives. Interestingly, the implied mechanisms rely on a reinforcement signal that parallels the activity of dopamine neurons in such experiments. However, recent studies challenged the classical view of explaining Pavlovian conditioning with a single process. When presented with a lever whose retraction preceded the delivery of food, some rats started to chew and bite the food magazine whereas others chew and bite the lever, even if no interactions were necessary to get the food. These differences were also visible in brain activity and when tested with drugs, suggesting the coexistence of multiple systems. We present a computational model that extends the classical theory to account for these data. Interestingly, we can draw predictions from this model that may be experimentally verified. Inspired by mechanisms used to model instrumental behaviours, where actions are required to get rewards, and advanced Pavlovian behaviours (such as overexpectation, negative patterning), it offers an entry point to start modelling the strong interactions observed between them.</p>
</abstract>
<funding-group><funding-statement>This work was supported by Grant ANR-11-BSV4-006 “LU2” (Learning Under Uncertainty) from L'Agence Nationale de la Recherche, France (FL, OS, MK), by Grant “HABOT” from the Ville de Paris Emergence(s) Program, France (MK), by Grant “GoHaL” from the Centre National de la Recherche Scientifique PEPS Program, France (MK), and by Grant P01 DA031656 from the National Institute on Drug Abuse, USA (SBF, TER). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="18"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Standard Reinforcement Learning (RL) <xref ref-type="bibr" rid="pcbi.1003466-Sutton1">[1]</xref> is a widely used normative framework for modelling conditioning experiments <xref ref-type="bibr" rid="pcbi.1003466-Sutton2">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Barto1">[3]</xref>. Different RL systems, mainly Model-Based and Model-Free systems, have often been combined to better account for a variety of observations suggesting that multiple valuation processes coexist in the brain <xref ref-type="bibr" rid="pcbi.1003466-Clark1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Cardinal1">[6]</xref>. Model-Based systems employ an explicit model of consequences of actions, making it possible to evaluate situations by forward inference. Such systems best explain goal-directed behaviours and rapid adaptation to novel or changing environments <xref ref-type="bibr" rid="pcbi.1003466-Yin1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Daw1">[9]</xref>. In contrast, Model-Free systems do not rely on internal models and directly associate values to actions or states by experience such that higher valued situations are favoured. Such systems best explain habits and persistent behaviours <xref ref-type="bibr" rid="pcbi.1003466-Daw1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Yin2">[11]</xref>. Of significant interest, learning in Model-Free systems relies on a computed reinforcement signal, the reward prediction error (RPE). This signal parallels the observed shift of dopamine neurons' response from the time of an initially unexpected reward – an outcome that is better or worse than expected – to the time of the conditioned stimulus that precedes it, which, in Pavlovian conditioning experiments, is fully predictive of the reward <xref ref-type="bibr" rid="pcbi.1003466-Schultz1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Fiorillo1">[13]</xref>.</p>
<p>However recent work by Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, raises questions about the exclusive use of classical RL Model-Free methods to account for data in Pavlovian conditioning experiments. Using an autoshaping procedure, a lever-CS was presented for 8 seconds, followed immediately by delivery of a food pellet into an adjacent food magazine. With training, some rats (sign-trackers; STs) learned to rapidly approach and engage the lever-CS. However, others (goal-trackers; GTs) learned to approach the food magazine upon CS presentation, and made anticipatory head entries into it. Furthermore, in STs, phasic dopamine release in the nucleus accumbens, measured with fast scan cyclic voltammetry, matched RPE signalling, and dopamine was necessary for the acquisition of a sign-tracking CR. In contrast, despite the fact that GTs acquired a Pavlovian conditioned approach response, this was not accompanied with the expected RPE-like dopamine signal, nor was the acquisition of a goal-tracking CR blocked by administration of a dopamine antagonist (see also <xref ref-type="bibr" rid="pcbi.1003466-Danna1">[15]</xref>).</p>
<p>Classical dual systems models <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Glscher1">[19]</xref> should be able to account for these behavioural and pharmacological data, but the physiological data are not consistent with the classical view of RPE-like dopamine bursts. Based on the observation that STs and GTs focus on different stimuli in the environment, we suggest that the differences observed in dopamine recordings may be due to an independent valuation of each stimulus. In classical RL, valuation is usually done at the <italic>state</italic> level. Stimuli, embedded into <italic>states</italic> – snapshots of specific configurations in time –, are therefore hidden to systems. In this case, it would prevent dealing separately with the lever and the magazine at the same time. However, such data may still be explained by a dual systems theory, when extended to support and benefit from factored representations; that is, learning the specific value of stimuli independently from the states in which they are presented.</p>
<p>In this paper, we present and test a model using a large set of behavioural, physiological and pharmacological data obtained from studies on individual variation in Pavlovian conditioned approach behaviour <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Flagel2">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>. It combines Model-Free and Model-Based systems that provide the specific components of the observed behaviours <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref>. It explains why inactivating dopamine in the core of the nucleus accumbens or in the entire brain results in blocking specific components and not others <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>. By weighting the contribution of each system, it also accounts for the full spectrum of observed behaviours ranging from one extreme – sign-tracking – to the other <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref> – goal-tracking. Above all, by extending classical Model-Free methods with factored representations, it potentially explains why the lever-CS and the food magazine might acquire different motivational values in different individuals, even when they are trained in the same task <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>. It may also account for why the RPE-like dopaminergic responses are observed in STs but not GTs, and also the differential dependence on dopamine <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We model the task as a simple Markov Decision Process (MDP) with different paths that parallel the diverse observed behaviours ranging from sign-tracking – engaging with the lever as soon as it appears – to goal-tracking – engaging with the magazine as soon as the lever-CS appears – (see <xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref>).</p>
<fig id="pcbi-1003466-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g001</object-id><label>Figure 1</label><caption>
<title>Computational representation of the autoshaping procedure.</title>
<p>(<bold>A</bold>) MDP accounting for the experiments described in <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref>. States are described by a set of variables: <italic>L</italic>/<italic>F</italic> - Lever/Food is available, <italic>cM</italic>/<italic>cL</italic> - close to the Magazine/Lever, <italic>La</italic> - Lever appearance. The initial state is double circled, the dashed state is terminal and ends the current episode. Actions are <italic>eng</italic>age with the proximal stimuli, <italic>exp</italic>lore, or <italic>go</italic> to the <italic>M</italic>agazine/<italic>L</italic>ever and <italic>eat</italic>. For each action, the feature that is being focused on is displayed within brackets. The path that STs should favour is in red. The path that GTs should favour is in dashed blue. (<bold>B</bold>) Time line corresponding to the unfolding of the MDP.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g001" position="float" xlink:type="simple"/></fig>
<p>The computational model (see <xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2</xref>) consists of two learning systems, employing distinct mechanisms to learn the same task: (1) a Model-Based system which learns the structure of the task from which it infers its values; (2) a Feature-Model-Free system where values for the relevant stimuli (lever-CS and the food magazine) are directly learned by trial and error using RPEs. The respective values of each system are then weighted by an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e001" xlink:type="simple"/></inline-formula> parameter before being used in a classical softmax action-selection mechanism (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<fig id="pcbi-1003466-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g002</object-id><label>Figure 2</label><caption>
<title>General architecture of the model and variants.</title>
<p>The model is composed of a Model-Based system (MB, in blue) and a Feature-Model-Free system (FMF, in red) which provide respectively an Advantage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e002" xlink:type="simple"/></inline-formula> and a value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e003" xlink:type="simple"/></inline-formula> values for actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e004" xlink:type="simple"/></inline-formula> given a state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e005" xlink:type="simple"/></inline-formula>. These values are integrated in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e006" xlink:type="simple"/></inline-formula>, prior to be used into an action selection mechanism. The various elements may rely on parameters (in purple). The impact of flupentixol on dopamine is represented by a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e007" xlink:type="simple"/></inline-formula> that influences the action selection mechanism and/or any reward prediction error that might be computed in the model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g002" position="float" xlink:type="simple"/></fig>
<p>An important feature of the model is that varying the systems weighting parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e008" xlink:type="simple"/></inline-formula> (while sharing the other parameter values of the model across subgroups) is sufficient to qualitatively reproduce the characteristics of the different subgroups of rats observed experimentally during these studies.</p>
<p>To improve the matching of the following results with the main experimental data, a different set of parameter values was used for each subgroup (ST, GT and IG). The values were retrieved after fitting autoshaping data only (see <xref ref-type="sec" rid="s4">Methods</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>). Simulated results on other behavioural, physiological and pharmacological data are generated with the same parameter values. While it might result in a weaker fitting of the other experimental data, this permits a straightforward comparison of results at different levels for the same simulation. Moreover, it confirms that the model can reproduce behavioural, physiological and pharmacological results with a single simulation per subgroup.</p>
<p>On each set of experimental data, we compare different variants of the computational model in order to highlight the key mechanisms that are required for their reproduction. Simulation results on each data subset are summarized in <xref ref-type="fig" rid="pcbi-1003466-g003">Figure 3</xref>. The role of each specific mechanism of the model in reproducing each experimental data is detailed in <xref ref-type="fig" rid="pcbi-1003466-g004">Figure 4</xref>.</p>
<fig id="pcbi-1003466-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g003</object-id><label>Figure 3</label><caption>
<title>Summary of simulations and results.</title>
<p>Each line represents a different model composed of a pair of Reinforcement Learning systems. Each column represents a simulated experiment. Experiments are grouped by the kind of data accounted for: behavioural (autoshaping <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref>, CRE <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>, Incentive salience <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref>), physiological <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref> and pharmacological (Flu post-NAcC <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>, Flu pre-systemic <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref>). Variant 4 (i.e. Model-based/Model-Free without features) is not included as it failed to even reproduce the autoshaping behavioural results and was not investigated further.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g003" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003466-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g004</object-id><label>Figure 4</label><caption>
<title>Summary of the key mechanisms required by the model to reproduce experimental results.</title>
<p>Each line represents a different mechanism of the model. Each column represents a simulated experiment. For each mechanism, it states in which experiment and for which behaviour – sign-tracking (red), goal-tracking (blue) or both (+) – it is required. Note however that all mechanisms and associated parameters have, to a certain extent, an impact on any presented results.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g004" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Behavioural data</title>
<sec id="s2a1">
<title>Autoshaping</title>
<p>The central phenomenon that the model is meant to account for is the existence of individual behavioural differences in the acquisition of conditioned approach responses in rats undergoing an autoshaping procedure; that is, the development of a sign-tracking CR, a goal-tracking CR, or an intermediate response.</p>
<p>Based on their engagement towards the lever, Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref> divided rats into three groups (see <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref> for a more recently defined criterion). At lever appearance, rats that significantly increased their engagement towards it (top 30%) were classified as STs, whereas rats that almost never engaged with the lever (bottom 30%) were classified as GTs (these latter animals engaged the food magazine upon CS presentation). The remaining rats, engaging in both lever and magazine approach behaviours were defined as the Intermediate Group (IGs) (see <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5 A, B</xref>). STs and GTs acquired their respective CRs at a similar rate over days of training <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>.</p>
<fig id="pcbi-1003466-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g005</object-id><label>Figure 5</label><caption>
<title>Reproduction of sign- versus goal-tracking tendencies in a population of rats undergoing an autoshaping experiment.</title>
<p>Mean probabilities to engage at least once with the lever (<bold>A,C</bold>) or the magazine (<bold>B,D</bold>) during trials. Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e009" xlink:type="simple"/></inline-formula> S.E.M. and illustrated in 50-trial (2-session) blocks. (<bold>A,B</bold>) Reproduction of Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref> experimental results (<xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2 A,B</xref>). Sign-trackers (ST) made the most lever presses (black), goal-trackers (GT) made the least lever presses (white), Intermediate group (IG) is in between (grey). (<bold>C,D</bold>) Simulation of the same procedure (squares) with the model. Simulated groups of rats are defined as STs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e010" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e011" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e012" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e013" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e014" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e015" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e016" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e017" xlink:type="simple"/></inline-formula>; n = 14) in red, GTs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e018" xlink:type="simple"/></inline-formula> ; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e019" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e020" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e021" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e022" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e023" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e024" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e025" xlink:type="simple"/></inline-formula>; n = 14) in blue and IGs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e026" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e027" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e028" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e029" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e030" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e031" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e032" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e033" xlink:type="simple"/></inline-formula>; n = 14) in white. The model reproduces the same behavioural tendencies. With training, STs tend to engage more and more with the lever and less with the magazine, while GTs neglect the lever to increasingly engage with the magazine. IGs are in between.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g005" position="float" xlink:type="simple"/></fig>
<p>The current model is able to reproduce such results (see <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5 C, D</xref>). By running a simulation for each group of rats, using different parameters (mainly varying the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e034" xlink:type="simple"/></inline-formula> parameter) the model reproduces the different tendencies to engage with the lever (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e035" xlink:type="simple"/></inline-formula>), with the magazine (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e036" xlink:type="simple"/></inline-formula>) or to fluctuate between the two (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e037" xlink:type="simple"/></inline-formula>). A high <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e038" xlink:type="simple"/></inline-formula> strengthens the influence of the Feature-Model-Free system, which learns to associate a high motivational value to the lever CS, and a sign-tracking CR dominates. A low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e039" xlink:type="simple"/></inline-formula> increases the influence of the Model-Based system, which infers the optimal behaviour to maximize reward, and goal-tracking is favoured. When both systems are mixed, i.e. with an intermediate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e040" xlink:type="simple"/></inline-formula>, the behaviour is more likely to oscillate between sign- and goal-tracking, representative of the intermediate group.</p>
<p>These results rely on the combination of two systems that would independently lead to ‘pure’ sign-tracking or goal-tracking CRs. Three tested variants of the model could reproduce these behavioural results as well (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s001">Figure S1</xref>): a combination of Feature-Model-Free systems and simple Model-Free system (Variant 1); a multi-step extension of Dayan 2006's model <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref> giving a Pavlovian impetus for the lever (Variant 2); and a symmetrical version of this last model with two impetuses, one for the lever, and one for the magazine (Variant 3) (see <xref ref-type="sec" rid="s4">Methods</xref>). Interestingly, a combination of Model-Based and classical Model-Free (not feature-based : Variant 4) fails in reproducing these results (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s008">Figure S8</xref>). This is because both systems are proven to converge to the same values and both would favour pure goal-tracking, such that varying their contribution has no impact on the produced behaviours.</p>
<p>Thus, at this stage, we can conclude that several computational models based on dual learning systems can reproduce these behavioural results, given that the systems favour different behaviours (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s001">Figure S1</xref>). However, Variants 1, 2 and 3 fail to reproduce other behavioural, pharmacological and physiological data characteristic of STs and GTs (see following sections).</p>
</sec><sec id="s2a2">
<title>Incentive salience</title>
<p>The results in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref> only represent the probability of approach to either the lever-CS or the food magazine. Thus, they do not account for the specific ways rats engage and interact with the respective stimuli. In fact, if food is used as the US, rats are known to chew and bite the stimuli on which they are focusing <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref> (see <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 A</xref>). Importantly, both STs and GTs express this consumption-like behaviour during the CS period, directed towards the lever or the food magazine, respectively. It has been argued that this behaviour may reflect the degree to which incentive salience is attributed to these stimuli, and thus the extent to which they become “wanted” <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Berridge1">[27]</xref>.</p>
<fig id="pcbi-1003466-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g006</object-id><label>Figure 6</label><caption>
<title>Possible explanation of incentive salience and Conditioned Reinforcement Effect by values learned during autoshaping procedure.</title>
<p>Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e041" xlink:type="simple"/></inline-formula> S.E.M. Simulated groups of rats are defined as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref>. (<bold>A</bold>) Number of nibbles and sniffs of preferred cue by STs and GTs as a measure for incentive salience. Data extracted from Mahler et al. <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref> from <xref ref-type="fig" rid="pcbi-1003466-g003">Figure 3</xref> (bottom-left). (<bold>B</bold>) Reproduction of Robinson et al. <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref> experimental results (<xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2 B</xref>). Lever contacts by STs and GTs during a conditioned reinforcer experiment. (<bold>C</bold>) Probability to engage with the respective favoured stimuli of STs and GTs at the end of the simulation (white, similar to the last session of <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref> C for STs and D for GTs) superimposed with the contribution in percentage of the values attributed by the Feature-Model-Free system in such engagement for STs (red) and GTs (blue). We hypothesize that such value is the source of incentive salience and explains why STs and GTs have a consumption-like behaviour towards their favoured stimulus. (<bold>D</bold>) Probability to engage with the lever versus exploring when presented with the lever and no magazine for STs (red), GTs (blue) and a random-policy group UN (white), simulating the unpaired group (UN) of the experimental data. Probabilities were computed by applying the softmax function after removing the values for the magazine interactions (see <xref ref-type="sec" rid="s4">Methods</xref>). STs would hence actively seek to engage with the lever relatively to GTs in a Conditioned Reinforcement Effect procedure.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g006" position="float" xlink:type="simple"/></fig>
<p>In an RL-like framework, incentive salience attribution can be represented as a bonus mechanism for interacting with stimuli. The Feature-Model-Free system in the model realizes such a function, providing a specific bonus for each stimulus in any simulated rat. Such bonus was inspired by the Pavlovian impetus mechanism of Dayan 2006's model <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>. <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 C</xref> shows the percentage of Feature-Model-Free value that contributed to the computation of the probability to engage with the respective favoured cues of STs and GTs at the end of the simulation.</p>
<p>The presence of the magazine in the inter-trial interval (ITI), and the necessary revision of the associated bonus at a lower value when exploring, makes the associated bonus smaller than that of the lever (see <xref ref-type="sec" rid="s4">Methods</xref>). This results in a even smaller contribution of this bonus in GTs behaviour (blue bar in <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 C</xref>) compared to STs (red bar in <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 C</xref>). Although it is not straightforward to interpret how the probability of engagement (white bars in <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 C</xref>) in the model might be translated into a consumption-like behaviour from a computational point of view, we propose that the different contributions of bonuses could explain the slightly smaller number of nibbles and sniffs of preferred cue observed experimentally in GTs compared to STs (<xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 A</xref>, adapted from <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>). This may also explain why other studies have observed a smaller proportion of nibbles on the magazine in GTs <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref> and less impulsiveness <xref ref-type="bibr" rid="pcbi.1003466-Lovic1">[28]</xref> in GTs compared to STs. We come back to this issue in the <xref ref-type="sec" rid="s3">discussion</xref>.</p>
<p>Variants 1 and 3 also realize such function by providing bonuses for actions leading to both stimuli (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s002">Figure S2</xref>). Only providing bonus for sign-tracking behaviour – as in Dayan's model (Variant 2) – does not fit well with the attribution of incentive salience to both stimuli. It would suggest that we should not observe incentive salience towards the magazine in any rats, which is in discrepancy with the experimental data. Thus, the important mechanism here is that stimuli are not processed differently. Any stimulus is attributed with its respective bonus, which is pertinent in regard to the attribution of incentive salience.</p>
</sec><sec id="s2a3">
<title>Conditioned Reinforcement Effect (CRE)</title>
<p>An important question about the difference in observed behaviours is about the properties acquired by the lever that makes it more attractive to STs than to GTs. To answer this question, Robinson and Flagel studied the dissociation of the predictive and motivational properties of the lever <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>. Part of their results involves asking whether the Pavlovian lever-CS would serve as a conditioned reinforcer, capable of reinforcing the learning of a new instrumental response <xref ref-type="bibr" rid="pcbi.1003466-Williams1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Skinner1">[30]</xref>. In a new context, rats were presented with an active and an inactive nose port. Nose poking into the active port resulted in presentation of the lever for 2 seconds without subsequent reward delivery, whereas poking into the inactive one had no consequence. The authors observed that while both STs and GTs preferred the active nose port to an inactive one, STs made significantly more active nose pokes than GTs (see <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 B</xref>, see also <xref ref-type="bibr" rid="pcbi.1003466-Lomanowska1">[31]</xref>). This suggests that the lever acquired greater motivational value in STs than in GTs.</p>
<p>Without requiring additional simulations, the model can explain these results by the value that has been incrementally learned and associated with approaching the lever in the prior autoshaping procedure for STs and GTs. In the model, STs attribute a higher value to interacting with the lever than GTs and should actively work for its appearance enabling further engagement. <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6 D</xref> shows the probabilities of engagement that would be computed at lever appearance after removing the magazine (and related actions) at the end of the experiment. Indeed, even though the lever is presented only very briefly, upon its presentation in the conditioned reinforcement test, STs actively engage and interact with it <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>. Any value associated to a state-action pair makes this action in the given state rewarding in itself, favouring actions (e.g. nosepokes) that would lead to such state. Repeatedly taking this action without receiving rewards should eventually lead to a decrease of this value and reduce the original engagement.</p>
</sec></sec><sec id="s2b">
<title>Physiological data</title>
<p>Not only have Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref> provided behavioural data but they also provide physiological and pharmacological data. This raises the opportunity to challenge the model at different levels, as developed in the current and next sections.</p>
<p>Using Fast Scan Cyclic Voltammetry (FSCV) in the core of the nucleus accumbens they recorded the mean of phasic dopamine (DA) signals upon CS (lever) and US (food) presentation. It was observed that depending on the subgroup of rats, distinct dopamine release patterns emerge (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 A,B</xref>) during Pavlovian training. STs display the classical propagation of a phasic dopamine burst from the US to the CS over days of training and the acquisition of conditioned responding (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 A</xref>). This pattern of dopamine activity is similar to that seen in the firing of presumed dopamine cells in monkeys reported by Schultz and colleagues <xref ref-type="bibr" rid="pcbi.1003466-Schultz1">[12]</xref> and interpreted as an RPE corresponding to the reinforcement signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e042" xlink:type="simple"/></inline-formula> of Model-Free RL systems <xref ref-type="bibr" rid="pcbi.1003466-Sutton1">[1]</xref>. In GTs, however, a different pattern was observed. Initially there were small responses to both the CS and US, of which the amplitudes seemed to follow a similar trend over training (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 B</xref>).</p>
<fig id="pcbi-1003466-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g007</object-id><label>Figure 7</label><caption>
<title>Reproduction of patterns of dopaminergic activity of sign- versus goal-trackers undergoing an autoshaping experiment.</title>
<p>Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e043" xlink:type="simple"/></inline-formula> S.E.M. (<bold>A,B</bold>) Reproduction of Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref> experimental results (<xref ref-type="fig" rid="pcbi-1003466-g003">Figure 3 d,f</xref>). Phasic dopamine release recorded in the core of the nucleus accumbens in STs (light grey) and GTs (grey) using Fast Scan Cyclic Voltammetry. Change in peak amplitude of the dopamine signal observed in response to CS and US presentation for each session of conditioning (<bold>C,D</bold>) Average RPE computed by the Feature-Model-Free system in response to CS and US presentation for each session of conditioning. Simulated groups of rats are defined as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref>. The model is able to qualitatively reproduce the physiological data. STs (blue) show a shift of activity from US to CS time over training, while GTs develop a second activity at CS time while maintaining the initial activity at US time.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g007" position="float" xlink:type="simple"/></fig>
<p>By recording the mean of the RPEs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e044" xlink:type="simple"/></inline-formula> computed in the Feature-Model-Free system during the autoshaping simulation (i.e. only fitted to behavioural data), the model can still qualitatively reproduce the different patterns observed in dopamine recordings for STs and GTs (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 C,D</xref>). For STs, the model reproduces the progressive propagation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e045" xlink:type="simple"/></inline-formula> from the US to the CS (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 C</xref>). For GTs, it reproduces the absence of such propagation. The RPE at the time of the US remains over training, while a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e046" xlink:type="simple"/></inline-formula> also appears at the time of the CS (see <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 D</xref>). In the model, such discrepancy is explained by the difference in the values that STs and GTs use for the computation of RPEs at the time of the CS and the US. STs, by repeatedly focusing on the lever, propagate the total value of food to the lever and end up having a unique <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e047" xlink:type="simple"/></inline-formula> at the unexpected lever appearance only. By contrast, by repeatedly focusing on the magazine during the lever appearance but, as all rats, also from time to time during ITI, GTs revise the magazine value multiple times, positively just after food delivery and negatively during ITI. Such revisions lead to a permanent discrepancy between the expected and observed value, i.e. a permanent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e048" xlink:type="simple"/></inline-formula>, at lever appearance and food delivery, when engaging with the magazine.</p>
<p>The key mechanism to reproduce these results resides in the generalization capacities of the Feature-Model-Free system. Based on features rather than states, feature-values are to be used, and therefore revised, at different times and states of the experiment, favouring the appearance of RPEs. Variants 2, 3 and 4 relying on classical Model-Free systems are unable to reproduce such results (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s003">Figure S3</xref>). By using values over abstract states rather than stimuli, it makes it impossible to only revise the value of the magazine during ITI. Therefore, given the deterministic nature of the MDP, we observe a classical propagation of RPEs in all pathways up to the appearance of the lever.</p>
</sec><sec id="s2c">
<title>Pharmacological data</title>
<sec id="s2c1">
<title>Effects of systemic flupentixol administration on the learning of sign- and goal-tracking behaviours</title>
<p>Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref> also studied the impact of systemic injections of the non specific dopamine antagonist, flupentixol, on the acquisition of sign-tracking and goal-tracking CRs. The authors injected flupentixol in rats prior to each of 7 sessions and observed the resulting behaviours. Behaviour during the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e049" xlink:type="simple"/></inline-formula> session was observed without flupentixol.</p>
<p>Systemic injections of flupentixol in STs and GTs (Flu groups, black curves in <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 A,B</xref>) blocked expression of their respective behaviours during training. Saline injections (white curves in <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 A,B</xref>) left their performances intact. The crucial test for learning took place on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e050" xlink:type="simple"/></inline-formula> day, when all rats were tested without flupentixol. STs failed to approach the lever, and performed as the saline-injected controls did on the first day of training.</p>
<fig id="pcbi-1003466-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g008</object-id><label>Figure 8</label><caption>
<title>Reproduction of the effect of systemic injections of flupentixol on sign-tracking and goal-tracking behaviours.</title>
<p>Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e051" xlink:type="simple"/></inline-formula> S.E.M. (<bold>A,B</bold>) Reproduction of Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref> experimental results (<xref ref-type="fig" rid="pcbi-1003466-g004">Figure 4 a,d</xref>). Effects of flupentixol on the probability to approach the lever for STs (<bold>A</bold>) and the magazine for GTs (<bold>B</bold>) during lever presentation. (<bold>C,D</bold>) Simulation of the same procedure (squares) with the model. Simulated groups of rats are defined as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref>. (<bold>C</bold>) By flattening the softmax temperature and reducing the RPEs of the Feature-Model-Free system, to mimic the possible effect of flupentixol, the model can reproduce the blocked acquisition of sign-tracking in STs (red), engaging less the lever relatively to a saline-injected control group (white). (<bold>D</bold>) Similarly, the model reproduces that goal-tracking was learned but its expression was blocked. Under flupentixol (first 7 sessions), GTs (blue) did not express goal-tracking, but on a flupentixol-free control test (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e052" xlink:type="simple"/></inline-formula> session) their engagement with the magazine was almost identical to the engagement of a saline-injected control group (white).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g008" position="float" xlink:type="simple"/></fig>
<p>Thus, in STs flupentixol blocked the acquisition of a sign-tracking CR (see <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 A</xref>). Interestingly, on the flupentixol-free test day GTs did not differ from the saline-injected control group, indicating that flupentixol did not block the acquisition of a goal-tracking CR (see <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 B</xref>). Thus, acquisition of a sign-tracking CR, but not a goal-tracking CR, is dependent on dopamine (see also <xref ref-type="bibr" rid="pcbi.1003466-Danna1">[15]</xref>).</p>
<p>The model reproduces these pharmacological results (see <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 C,D</xref>). As in the experimental data, simulated GTs and STs do not show a specific conditioned response during the first 7 sessions under flupentixol. On the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e053" xlink:type="simple"/></inline-formula> session, without flupentixol, we observe that STs still do not show a specific conditioned response while GTs perform at a level close to that of the saline-injected control group (see <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 C,D</xref>).</p>
<p>The absence of specific conditioned response in the whole population for the first 7 sessions is first due to the hypothesized <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref> impact of flupentixol on action selection (see <xref ref-type="sec" rid="s4">Methods</xref>). With enough flupentixol, the elevation of the selection temperature leads to a decrease of the influence of learned values in the expressed behaviour, masking any possibly acquired behaviour.</p>
<p>The absence of a specific conditioned response in STs is due to the blockade of learning in the second system by flupentixol, since it is RPE-dependent. Therefore almost no learning occurs in the system (see <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8</xref>).</p>
<p>In contrast, with the first system being RPE-independent, flupentixol has no effect on learning, because it is Model-Based rather than Model-Free <xref ref-type="bibr" rid="pcbi.1003466-Khamassi1">[33]</xref>. The expression of behaviour is blocked at the action selection level, which does not make use of values learned by the Model-Based system. Thus, GTs, relying mainly on the first system, learn their CR under flupentixol but are just not able to express it until flupentixol is removed. The lower level of goal-tracking in the Flu group relative to the saline-injected control group on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e054" xlink:type="simple"/></inline-formula> session is due to the lack of exploitation induced by flupentixol injection during the previous 7 sessions. By engaging less with the magazine, the Flu group ends up associating a lower value to the magazine (i.e. the value did not fully converge in 7 sessions) to guide its behaviour.</p>
<p>Interestingly, if the model had been constituted of Model-Free systems only – as in Variants 1, 2 and 3 – it would not have been able to reproduce these results, because both systems would have been RPE-dependent and thus sensitive to the effect of flupentixol (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s004">Figure S4</xref>).</p>
</sec><sec id="s2c2">
<title>Effects of local flupentixol administration on the expression of sign- and goal-tracking behaviours</title>
<p>In a related experiment, Saunders et al. <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref> studied the role of dopamine in the nucleus accumbens core in the expression of Pavlovian-conditioned responses that had already been acquired. After the same autoshaping procedure as in <xref ref-type="bibr" rid="pcbi.1003466-Flagel2">[20]</xref>, they injected different doses of flupentixol in the core of the nucleus accumbens of rats and quantified its impact on the expression of sign-tracking and goal-tracking CRs in an overall population (without distinguishing between STs and GTs).</p>
<p>They found that flupentixol dose dependently attenuated the expression of sign-tracking, while having essentially no effect on goal-tracking (see <xref ref-type="fig" rid="pcbi-1003466-g009">Figure 9 A, B</xref>). Along with the Flagel et al. <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref> study, these results suggest that both the acquisition and expression of a sign-tracking CR is dopamine-dependent (at least in the core) whereas the acquisition and expression of a goal-tracking CR is not.</p>
<fig id="pcbi-1003466-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g009</object-id><label>Figure 9</label><caption>
<title>Reproduction of the effect of post injections of flupentixol in the core of the nucleus accumbens.</title>
<p>Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e055" xlink:type="simple"/></inline-formula> S.E.M. (<bold>A,B</bold>) Reproduction of Saunders et al. <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref> experimental results (<xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2 A,D</xref>). Effects of different doses of flupentixol on the general tendency to sign-track (<bold>A</bold>) and goal-track (<bold>B</bold>) in a population of rats, without discriminating between sign- and goal-trackers. (<bold>C,D</bold>) Simulation of the same procedure with the model. The simulated population is composed of groups of rats defined as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref>. By simulating the effect of flupentixol as in <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8</xref>, the model is able to reproduce the decreasing tendency to sign-track in the overall population by increasing the dose of flupentixol.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g009" position="float" xlink:type="simple"/></fig>
<p>Given the assumption that the Feature-Model-Free system would take place in or rely on the core of the nucleus accumbens, this model reproduces the main experimental result: the decreased tendency to sign-track in the population (see <xref ref-type="fig" rid="pcbi-1003466-g009">Figure 9 C</xref>). Note that in the previous experiment, the injection of flupentixol was systemic, and assumed to affect any region of the brain relying on dopamine, whereas in the present experiment it was local to the core of the nucleus accumbens. Therefore, we modelled the impact of flupentixol differently between the current and previous simulations (see <xref ref-type="sec" rid="s4">Methods</xref>). In the model, the tendency to sign-track is directly correlated with a second operational system. Any dysfunction in the learning process (here by a distortion of RPEs) reduces this trend.</p>
<p>The model successfully reproduced the absence of reduction of goal-tracking, in contrast to the reduction of sign-tracking. However, it was unable to reproduce the invariance in goal-tracking (see <xref ref-type="fig" rid="pcbi-1003466-g009">Figure 9 D</xref>) and rather produced an increase in goal-tracking. This is due to the use of a softmax operator for action selection, as this is the case in the vast majority of computational neuroscience RL models <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Glscher1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Huys1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Redish1">[36]</xref>, which automatically favours goal-tracking when sign-tracking is blocked (see Limitations). We did not attempt to cope with this limitation because our focus here was the absence of reduction of goal-tracking.</p>
<p>Besides, the model could, after re-learning, reproduce the selective impact of intra-accumbal flupentixol injections observed in sign-tracking but not in goal-tracking, because such injections affected the learning process in the Feature-Model-Free system only.</p>
</sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We tested several mechanisms from the current literature on modelling individual variation in the form of Pavlovian conditioned responses (ST vs GT) that emerge using a classical autoshaping procedure, and the role of dopamine in both the acquisition and expression of these CRs. Benefiting from a rich set of data, we identified key mechanisms that are sufficient to account for specific properties of the observed behaviours. The resulting model relies on two major concepts: Dual learning systems and factored representations. <xref ref-type="fig" rid="pcbi-1003466-g004">Figure 4</xref> summarizes the role of each mechanism in the model.</p>
<sec id="s3a">
<title>Dual learning systems</title>
<p>Combining Model-Based and Model-Free systems has previously been successful in explaining the shift from goal-directed to habitual behaviours observed in instrumental conditioning <xref ref-type="bibr" rid="pcbi.1003466-Daw2">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Glscher1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Khamassi1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Huys1">[34]</xref>. However, few models based on the same concept have been developed to account for Pavlovian conditioning <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>. While the need for two systems is relevant in instrumental conditioning given the distinct temporal engagement of each system, such a distinction has not been applied to Pavlovian phenomena (but see recent studies on orbitofrontal cortex <xref ref-type="bibr" rid="pcbi.1003466-Takahashi1">[37]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-McDannald2">[39]</xref>). The variability of behaviours and the need for multiple systems have been masked by focusing on whole populations and, for the most part, ignoring individual differences in studies of Pavlovian conditioning. The nature of the CS is especially important, as many studies of Pavlovian conditioned approach behaviour have used an auditory stimulus as the CS, and in such cases only a goal-tracking CR emerges in rats <xref ref-type="bibr" rid="pcbi.1003466-Cleland1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Meyer2">[41]</xref>.</p>
<p>As expected from the behavioural data, combining two learning systems was successful in reproducing sign- and goal-tracking behaviours. The Model-Based system, learning the structure of the task, favours systematic approach towards the food magazine, and waiting for food to be delivered, and hence the development of a goal-tracking CR. The Feature-Model-Free system, directly evaluating features by trials and errors, favours systematic approach towards the lever, a full predictor of food delivery, and hence the development of a sign-tracking CR. Moreover, utilizing the Feature-Model-Free system to represent sign-tracking behaviour yields results consistent with the pharmacological data. Disrupting RPEs, which reflects the effects of flupentixol on dopamine, blocks the acquisition of a sign-tracking CR, but not a goal-tracking CR. The model does not make a distinction between simple approach behaviour versus consumption-like engagement, as reported for both STs and GTs <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref>. However given that such engagement results from the development of incentive salience <xref ref-type="bibr" rid="pcbi.1003466-Mahler1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-DiFeliceantonio1">[24]</xref>, the values learned by the Feature-Model-Free system to bias behaviour towards stimuli attributed with motivational value are well-suited to explain such observations. The higher motivational value attributed to the lever by STs relative to GTs can also explain why the lever-CS is a more effective conditioned reinforcer for STs than for GTs <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>.</p>
<p>Importantly, none of the systems are dedicated to a specific behaviour, nor rely on <italic>a priori</italic> information to guide their processes. The underlying mechanisms increasingly make one behaviour more pronounced than the other through learning. Each system contributes to a certain extent to sign- and goal-tracking behaviour. This property is emphasized by the weighted sum integration of the values computed by each system before applying the softmax action-selection mechanism. The variability of behaviours in the population can then be accounted for by adjusting the weighting parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e056" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e057" xlink:type="simple"/></inline-formula> (i.e. favouring sign-tracking) to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e058" xlink:type="simple"/></inline-formula> (i.e. favouring goal-tracking). This suggests that the rats' actions result from some combination of rational and impulsive processes, with individual variation contributing to the weight of each component.</p>
<p>The integration mechanism is directly inspired by the work of Dayan et al. <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref> and as the authors suggest, the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e059" xlink:type="simple"/></inline-formula> may fluctuate over time, making the contribution of the two systems vary with experience. In contrast to their model, however, the model presented here does not assign different goals to each system. Thus, the current model is more similar to their previous model <xref ref-type="bibr" rid="pcbi.1003466-Daw2">[17]</xref>, which uses another method for integration.</p>
<p>A common alternative to integration when using multiple systems <xref ref-type="bibr" rid="pcbi.1003466-Daw2">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Keramati1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Doya1">[35]</xref> is to select at each step, based on a given criterion (certainty, speed/accuracy trade-off, energy cost), a single system to pick the next action. Such switch mechanism does not fit well with the present model, given that it would be interpreted as if actions relied sometimes only on motivational values (i.e. Feature-Model-Free system) and sometimes only on a rational analysis of the situation (i.e. Model-Based system). It also does not fit well with pharmacological observation that STs do not express goal-tracking tendencies in the drug-free test session following systemic-injections of flupentixol <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, as Flagel et al. stated, “[sign-tracking] rats treated with flupentixol did not develop a goal-tracking CR”.</p>
</sec><sec id="s3b">
<title>Factored representations</title>
<p>Classical RL algorithms used in neuroscience <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Keramati1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Doya1">[35]</xref>, designed mainly to account for instrumental conditioning, work at the state level. Tasks are defined as graphs of states, and corresponding models are unaware of any similarity within states. Therefore, any subsequent valuation process cannot use any underlying structure to generalize updates to states that share stimuli. Revising the valuation process to handle features rather than states <italic>per se</italic>, makes it possible to attribute motivational values to stimuli independently of the states in which they are presented.</p>
<p>Recent models dedicated to Pavlovian conditioning <xref ref-type="bibr" rid="pcbi.1003466-Redish1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Schmajuk1">[42]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Gershman1">[46]</xref> usually represent and process stimuli independently and can be said to use factored representations, a useful property to account for phenomena such as blocking <xref ref-type="bibr" rid="pcbi.1003466-Kamin1">[47]</xref> or overexpectation <xref ref-type="bibr" rid="pcbi.1003466-Lattal1">[48]</xref>. In contrast to the present model, while taking inspiration from RL theory (e.g. using incremental updates), these models are usually far from the classical RL framework. Of significant difference with the present study, most of these models tend to describe the varying intensity of a unique conditioned response and do not account for variations in the actual form of the response, as we do here. In such models, the magazine would not be taken into account and/or taken as part of the context, making it unable to acquire a value for itself nor be the focus of a particular response.</p>
<p>In RL theory, factorization is mainly evoked when trying to overcome the curse of dimensionality <xref ref-type="bibr" rid="pcbi.1003466-Bellman1">[49]</xref> (i.e. standard algorithms do not scale well to high dimensional spaces and require too much physical space or computation time). Amongst methods that intend to overcome this problem are value function approximations and Factored Reinforcement Learning. Value function approximations <xref ref-type="bibr" rid="pcbi.1003466-Doya1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Khamassi2">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Elfwing1">[51]</xref> attempt to split problems into orthogonal subproblems making computations easier and providing valuations that can then be aggregated to estimate the value of states. Factored Reinforcement Learning <xref ref-type="bibr" rid="pcbi.1003466-Boutilier1">[52]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Vigorito1">[54]</xref> attempts to find similarities between states so that they can share values, reducing the physical space needed and relies on factored Markov Decision Processes. We also use factored Markov Decision processes, hence the “factored” terminology. However, our use of factored representations serves a different purpose. We do not intend to build a compact value-function nor infer the value of states from values of features but rather make these values compete in the choice for the next action.</p>
<p>Taking advantage of factored representations into classical RL algorithms is at the very heart of the present results. By individually processing stimuli within states (i.e. in the same context, at the same time and same location) and making them compete, the Feature-Model-Free system favours a different policy – oriented towards engaging with the most valued stimuli – (sign-tracking) than would have been favoured by classical algorithms such as Model-Based or Model-Free systems (goal-tracking). Hence, combining a classical RL algorithm with the Feature-Model-Free system enables the model to reproduce the difference in behaviours observed between STs and GTs during an autoshaping procedure. Moreover, by biasing expected optimal behaviours towards cues with motivational values (incentive salience), it is well suited to explain the observed commitment to unnecessary and possibly counter-productive actions (see also <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-GuitartMasip1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Huys2">[56]</xref>). Most of all, it enables the model to replicate the different patterns of dopamine activity recorded with FSCV in the core of the nucleus accumbens of STs and GTs. The independent processing of stimuli leads to patterns of RPE that match those of dopamine activity for STs – a shift of bursts from the US to the CS; and in GTs – a persistence of bursts at both the time of the US and the CS.</p>
</sec><sec id="s3c">
<title>A promising combination</title>
<p>By combining the two concepts of dual learning systems and factored representations in a single model, we are able to reproduce individual variation in behavioural, physiological and pharmacological effects in rats trained using an autoshaping procedure. Interestingly, our approach does not require a deep revision of mechanisms that are extensively used in our current field of research.</p>
<p>While Pavlovian and instrumental conditioning seem entangled in the brain <xref ref-type="bibr" rid="pcbi.1003466-Yin3">[57]</xref>, the two major concepts on which rely their respective models, dual learning systems and factored representations, have to our knowledge never been combined into a single model in this field of research.</p>
<p>This approach could contribute to the understanding of interactions between these two classes of learning, such as CRE or Pavlovian-Instrumental Transfer (PIT), where motivation for stimuli acquired via Pavlovian learning modulates the expression of instrumental responses. Interestingly, the Feature-Model-Free system nicely fits with what would be expected from a mechanism contributing to general PIT <xref ref-type="bibr" rid="pcbi.1003466-Corbit1">[58]</xref>. It is focused on values over stimuli without regard to their nature <xref ref-type="bibr" rid="pcbi.1003466-Corbit1">[58]</xref>, it biases and interferes with some more instrumental processes <xref ref-type="bibr" rid="pcbi.1003466-GuitartMasip1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Huys2">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Corbit1">[58]</xref> and it is hypothesized to be located in the core of the nucleus accumbens <xref ref-type="bibr" rid="pcbi.1003466-Corbit1">[58]</xref>. It would thus be interesting to study whether future simulations of the model could explain and help better formalize these aspects of PIT.</p>
<p>We do not necessarily imply that instrumental and Pavlovian conditioning might rely on a unique model. Rather, we propose that if they were the results of separated systems, they should somehow rely on similar representations and valuation mechanisms, given the strength of the observed interactions.</p>
</sec><sec id="s3d">
<title>Theoretical and practical implications</title>
<p>The proposed model explains the persistent dopamine response to the US in GTs over days of training as a permanent RPE due to the revision of the magazine value during each ITI. Therefore, a prediction of the model is that shortening the ITI should reduce the amplitude of this burst (i.e. there should be less time to revise the value and reduce the size of the RPE); whereas increasing the ITI should increase the amplitude of this burst. Removing the food dispenser during ITI, similar to theoretically suppressing the ITI, should make this same burst disappear. Studying physiological data by grouping them given the duration of the preceding ITI might be sufficient, relatively to noise, to confirm that its duration impacts the amplitude of dopamine bursts. In the current experimental procedure, the ITI is indeed randomly picked in a list of values with an average of 90 sec. Moreover, reducing ITI duration should lead to an increase of the tendency to goal-track in the overall population. Indeed, with a higher value of the food magazine, the Feature-Model-Free system would be less likely to favour sign-tracking over goal-tracking CR. The resulting decrease in sign-tracking in the overall population would be consistent with findings of previous works <xref ref-type="bibr" rid="pcbi.1003466-Balsam1">[59]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Tomie1">[62]</xref>, where a shorter ITI reduces the observed performance in the acquisition of sign-tracking CRs. Alternatively, it would also be interesting to examine the amplitude of dopamine bursts during the ITI (especially when exploring the food magazine), to determine whether or not physiological responses during this period affect the outcome of the conditioned response.</p>
<p>It would be interesting to split physiological data not only between STs and GTs but also between the stimuli on which the rats started and/or ended focusing on during CS presentation at each trial. This would help to confirm that the pattern of dopamine activity is indeed due to a separate valuation of each stimuli. We would predict that at the time of the US, dopamine bursts during engagement with the lever should be small relatively to dopamine bursts during engagement with the magazine. Moreover, comparing dopamine activity at the time of the CS when engaging with the lever versus the magazine could help elucidate which update mechanism is being used. If activity differs, this would suggest that the model should be revised to use SARSA-like updates, i.e. taking into account the next action in RPE computation. Such a question has already been the focus of some studies on dopamine activity <xref ref-type="bibr" rid="pcbi.1003466-Morris1">[63]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Bellot1">[65]</xref>.</p>
<p>There is no available experimental data for the phasic dopaminergic activity of the intermediate group. The model predicts that such a group would have a permanent phasic dopamine burst, i.e. RPE, at US and a progressively appearing burst at CS (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s006">Figure S6</xref>). Over training, the amplitude of the phasic dopamine burst at US should decrease until a point of convergence, while at the mean time the response at CS should increase until reaching a level higher than the one observed at US. However, one must note, that the fitting of the intermediate group is not as good as for STs or GTs, as it regroups behaviours that range from sign-tracking to goal-tracking, such that this is a weak prediction.</p>
<p>There is the possibility that regularly presenting the magazine or the lever could, without pairing with food, lead to responses that are indistinguishable from CRs. However, ample evidence suggests that the development of a sign-tracking or goal-tracking CR is not due to this pseudoconditioning phenomenon, but rather a result of learned CS-US associations. That is, experience with lever-CS presentations or with food US does not account for the acquisition of lever-CS induced directed responding <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Tomie2">[66]</xref>. Nonetheless, it should be noted that the current model cannot distinguish between pseudoconditioning CR-like responses and sign-tracking or goal-tracking behaviours. This would require us to introduce more complex MDPs that embed the ITI and can more clearly distinguish between approach and engagement.</p>
</sec><sec id="s3e">
<title>Limitations</title>
<p>The Feature-Model-Free system presented in this article was designed as a proof of concept for the use of factored representations in computational neuroscience. In its present form it updates the value of one feature (the focused one) at a time, and this is sufficient to account for much of the experimental data. It does not address whether multiple features could be processed in parallel, such that multiple synchronized, but independently computed, signals would update distinct values relative to the attention paid to the associated features. Further experiments should be performed to confirm this hypothesis. Subsequently, using factored representations in the Model-Based system was not necessary to account for the experimental data and the question remains whether explaining some phenomena would require it.</p>
<p>While using factored representations, our approach still relies on the discrete-time state paradigm of classical RL, where updates are made at regular intervals. Although such simplification can explain the set of data considered here, one would need to extend this to continuous time if one would like to also model experimental data where rats take more or less time to initiate actions that can vary in duration <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>. The present model, which does not take timing into consideration, cannot account for the fact that STs and GTs both come to approach their preferred stimuli faster and faster as a function of training nor does it make use of the variations of ITI duration. Our attempt to overcome this limitation using the MDP framework was unsuccessful. Focusing on features, it becomes more tempting to deal with the timing of their presence, a property that is known to be learned and to have some impact on behaviours <xref ref-type="bibr" rid="pcbi.1003466-Gallistel1">[61]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Kobayashi1">[67]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Fiorillo2">[69]</xref>.</p>
<p>Moreover, in the current model, we did not attempt to account for the conditioned orienting responses (i.e. orientation towards the CS) that both STs and GTs exhibit upon CS presentation <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>. However, we hypothesize that such learned orienting responses could be due to state discrimination mechanisms that are not included in the model, and would be better explained with partial observability and actions dedicated to collect information. This is beyond the scope of the current article, but is of interest for future studies.</p>
<p>As evident by the only partial reproduction of the flupentixol effects on the expression of sign- and goal-tracking behaviours, the model is limited by the use of the softmax action-selection mechanism, which is widely used in computational neuroscience <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Glscher1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Huys1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Redish1">[36]</xref>. In the model, all actions are equal – there is no action with a specific treatment – and the action-selection mechanism necessarily selects an action at each time step. Any reduction in the value of one action favours the selection of all other actions in proportion to their current associated values. In reality, however, blocking the expression of an action would certainly lead mainly to inactivity rather than necessarily picking the alternative and almost never expressed action. One way of improving the model in this direction could be to replace the classical softmax function by a more realistic model of action selection in the basal ganglia (e.g. <xref ref-type="bibr" rid="pcbi.1003466-Gurney1">[70]</xref>). In such a model, no action is performed when no output activity gets above a certain threshold. Humphries et al. <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref> have shown that changing the exploration level in a softmax function can be equivalent to changing the level of tonic dopamine in the basal ganglia model of Gurney et al. <xref ref-type="bibr" rid="pcbi.1003466-Gurney1">[70]</xref>. Interestingly, in the latter model, reducing the level of tonic dopamine results in difficulty in initiating actions and thus produces lower motor behaviour, as is seen in Parkinsonian patients and as can be seen in rats treated with higher doses of flupentixol <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>. Thus a natural sequel to the current model would be to combine it with a more realistic basal ganglia model for action selection.</p>
<p>We simulated the effect of flupentixol as a reduction of the RPE in the learning processes of Model-Free systems to parallel its blockade of the dopamine receptors. While this is sufficient to account for the pharmacological results previously reported <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, it fails to account for some specific aspects that have more recently emerged. Mainly, it is unable to reproduce the instant decreased engagement observed at the very first trial after post-training local injections of flupentixol <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>. Our current approach requires re-learning to see any impact of flupentixol. A better understanding of the mechanisms that enable instant shifts in motivational values, by shifts in the motivational state <xref ref-type="bibr" rid="pcbi.1003466-Robinson2">[71]</xref> or the use of drugs <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Saunders1">[25]</xref>, might be useful to extend the model on such aspects.</p>
<p>We also tried to model the effect of flupentixol on RPEs with a multiplicative effect, as it would have accounted for an instant impact on behaviour. However, it failed to account for the effects of flupentixol on learning of the sign-tracking CRs, as a multiplicative effect only slowed down learning but did not disrupt it. How to model the impact of flupentixol, and dopamine antagonists or drugs such as cocaine remains an open question (e.g. see <xref ref-type="bibr" rid="pcbi.1003466-Panlilio1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Redish2">[73]</xref>).</p>
<p>Finally, our work does not currently address the anatomical counterpart of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e060" xlink:type="simple"/></inline-formula> at the heart of the model, nor the regions of the brain that would match the current Model-Based system and the Feature-Model-Free system. Numerous studies have already discussed the potential substrates of Model-Based/Model-Free systems in the prefrontal cortex/dorsolateral striatum <xref ref-type="bibr" rid="pcbi.1003466-Daw4">[74]</xref>, or the dorsomedial and dorsolateral striatum <xref ref-type="bibr" rid="pcbi.1003466-Khamassi1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Yin4">[75]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-vanderMeer1">[78]</xref>. The weighted sum integration may suggest a crossed projection of brains regions favouring sign- and goal-tracking behaviours (Model-Based and Feature-Model-Free systems) into a third one. We postulate there is a difference in strength of “connectivity” between such regions in STs vs GTs <xref ref-type="bibr" rid="pcbi.1003466-Flagel4">[79]</xref>. Further, one might hypothesize that the core of the nucleus accumbens contributes to the Feature-Model-Free system. The integration and action selection mechanisms would naturally fit within the basal ganglia, stated to contribute to such functions <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Mink1">[80]</xref>–<xref ref-type="bibr" rid="pcbi.1003466-Gurney2">[82]</xref>.</p>
</sec><sec id="s3f">
<title>Conclusion</title>
<p>Here we have presented a model that accounts for variations in the form of Pavlovian conditioned approach behaviour seen during autoshaping in rats; that is, the development of a sign-tracking vs goal-tracking CR. This works adds to an emerging set of studies suggesting the presence and collaboration of multiple RL systems in the brain. It questions the classical paradigm of state representation and suggests that further investigation of factored representations in RL models of Pavlovian and instrumental conditioning experiments may be useful.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Modelling the autoshaping experiment</title>
<p>In the classical reinforcement learning theory <xref ref-type="bibr" rid="pcbi.1003466-Sutton1">[1]</xref>, tasks are usually described as Markov Decision Processes (MDPs). As the proposed model is based on RL algorithms, we use the MDP formalism to computationally describe the Pavlovian autoshaping procedure used in all simulations.</p>
<p>An MDP describes the interactions of an agent with its environment and the rewards it might receive. An agent being in a state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e061" xlink:type="simple"/></inline-formula> can execute an action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e062" xlink:type="simple"/></inline-formula> which results in a new state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e063" xlink:type="simple"/></inline-formula> and the possible retrieval of some reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e064" xlink:type="simple"/></inline-formula>. More precisely, an agent can be in a finite set of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e065" xlink:type="simple"/></inline-formula>, in which it can perform a finite set of discrete actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e066" xlink:type="simple"/></inline-formula>, the consequences of which are defined by a transition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e067" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e068" xlink:type="simple"/></inline-formula> is the probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e069" xlink:type="simple"/></inline-formula> of reaching state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e070" xlink:type="simple"/></inline-formula> doing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e071" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e072" xlink:type="simple"/></inline-formula>. Additionally, the reward function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e073" xlink:type="simple"/></inline-formula> is the reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e074" xlink:type="simple"/></inline-formula> for doing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e075" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e076" xlink:type="simple"/></inline-formula>. Importantly, MDPs should theoretically comply with the Markov property: the probability of reaching state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e077" xlink:type="simple"/></inline-formula> should only depend on the last state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e078" xlink:type="simple"/></inline-formula> and the last action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e079" xlink:type="simple"/></inline-formula>. An MDP is defined as episodic if it includes at least one state which terminates the current episode.</p>
<p><xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref> shows the deterministic MDP used to simulate the autoshaping procedure. Given the variable time schedule (30–150s) and the net difference observed in behaviours in inter-trial intervals, we can reasonably assume that each experimental trial can be simulated with a finite horizon episode.</p>
<p>The agent starts from an empty state (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e080" xlink:type="simple"/></inline-formula>) where there is nothing to do but explore. At some point the lever appears (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e081" xlink:type="simple"/></inline-formula>) and the agent must make a critical choice: It can either go to the lever (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e082" xlink:type="simple"/></inline-formula>) and engage with it (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e083" xlink:type="simple"/></inline-formula>), go to the magazine (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e084" xlink:type="simple"/></inline-formula>) and engage with it (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e085" xlink:type="simple"/></inline-formula>) or just keep exploring (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e086" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e087" xlink:type="simple"/></inline-formula>). At some point, the lever is retracted and food is delivered. If the agent is far from the magazine (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e088" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e089" xlink:type="simple"/></inline-formula>), it first needs to get closer. Once close (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e090" xlink:type="simple"/></inline-formula>), it consumes the food. It ends in an empty state (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e091" xlink:type="simple"/></inline-formula>) which symbolizes the start of the inter-trial interval (ITI): no food, no lever and <italic>an empty but still present magazine</italic>.</p>
<p>The MDP in <xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref> is common to all of the simulations and independent of the reinforcement learning systems we use. STs should favour the red path, while GTs should favour the <italic>shorter</italic> blue path. All of the results rely mainly on the action taken at the lever appearance (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e092" xlink:type="simple"/></inline-formula>), when choosing to go to either the lever, the magazine, or to explore. Exploring can be understood as not going to the lever nor to the magazine.</p>
<p>To fit with the requirements of the MDP framework, we introduce two limitations in our description, which also simplify our analyses. We assume that engagement is necessarily exclusive to one or no stimulus, and we make no use of the precise timing of the procedure – the ITI duration nor the CS duration – in our simulations.</p>
<sec id="s4a1">
<title>Inter-trial interval (ITI)</title>
<p>While the MDP does not model the ITI, the results regarding physiological data rely partially on its presence. Extending the MDP with a set of states to represent this interval would increase the complexity of the MDP and the time required for simulations. The behaviour that could have resulted from such an extension is easily replaced by applying the following formula at the beginning of each episode:<disp-formula id="pcbi.1003466.e093"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e093" xlink:type="simple"/><label>(1)</label></disp-formula>where the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e094" xlink:type="simple"/></inline-formula> reflects the interaction with the magazine that occurred during the ITI. A low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e095" xlink:type="simple"/></inline-formula> symbolizes a low interaction and therefore a low revision of the value associated to the magazine. A high <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e096" xlink:type="simple"/></inline-formula> symbolizes a strong exploration of the magazine during the inter-trial interval and therefore a strong decrease in the associated value due to unrewarded exploration.</p>
</sec></sec><sec id="s4b">
<title>Model</title>
<p>The model relies on the architecture shown in <xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2</xref>. The main idea is to combine the computations of two distinct reinforcement learning systems to define what behavioural response is chosen at each step.</p>
<sec id="s4b1">
<title>Model-Based system (MB)</title>
<p>The first system is Model-Based <xref ref-type="bibr" rid="pcbi.1003466-Sutton1">[1]</xref>, and classically relies on a transition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e097" xlink:type="simple"/></inline-formula> and a reward function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e098" xlink:type="simple"/></inline-formula> which are learned by experience given the following rules:<disp-formula id="pcbi.1003466.e099"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e099" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pcbi.1003466.e100"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e100" xlink:type="simple"/><label>(3)</label></disp-formula>where the learning rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e101" xlink:type="simple"/></inline-formula> classically represents the speed at which new experiences replace old ones. Using a learning rate rather than counting occurrences is a requirement for accordance with the incremental expression of the observed behaviours. This can account for some resistance or uncertainty in learning from new experiences.</p>
<p>Given this model, an action-value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e102" xlink:type="simple"/></inline-formula> can then be computed with the following classical formula:<disp-formula id="pcbi.1003466.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e103" xlink:type="simple"/><label>(4)</label></disp-formula>where the discount rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e104" xlink:type="simple"/></inline-formula> classically represents the preference for immediate versus distant rewards. The resulting Advantage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e105" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003466-BairdIII1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Dayan2">[84]</xref>, the output of the first system, is computed as follows:<disp-formula id="pcbi.1003466.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e106" xlink:type="simple"/><label>(5)</label></disp-formula>It defines the (negative) advantage of taking action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e107" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e108" xlink:type="simple"/></inline-formula> relatively to the optimal action known. The optimal action therefore has an advantage value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e109" xlink:type="simple"/></inline-formula>.</p>
<p>In terms of computation, the advantage function could be replaced by the action-value function without changing the simulation results (we only compare <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e110" xlink:type="simple"/></inline-formula> over the same state and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e111" xlink:type="simple"/></inline-formula> is constant whatever the action). It has been used in preceding works dealing with interactions between instrumental and Pavlovian conditioning <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Dayan2">[84]</xref> and we kept it for a better and more straightforward comparison with variants of the model that were directly inspired by these preceding works.</p>
</sec><sec id="s4b2">
<title>Feature-Model-Free system (FMF)</title>
<p>A state is generally described by multiple features. Animals, especially engaged in a repetitive task, might not pay attention to all of them at once. For example, when the lever appears and a rat decides to engage with the magazine, it focuses primarily on the magazine while ignoring the lever, such that it could update a value associated to the magazine but leave intact any value related to the lever (see <xref ref-type="fig" rid="pcbi-1003466-g010">Figure 10 A</xref>). Although this could be related to an attentional process that bias learning, we do not pretend to model attention with such a mechanism.</p>
<fig id="pcbi-1003466-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g010</object-id><label>Figure 10</label><caption>
<title>Characteristics of the Feature-Model-Free system.</title>
<p>(<bold>A</bold>) Focusing on a particular feature. The Feature-Model-Free system relies on a value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e112" xlink:type="simple"/></inline-formula> based on features. Choosing an action (e.g. <italic>goL</italic>, <italic>goM</italic> or <italic>exp</italic>), defines the feature it is focusing on (e.g. <italic>L</italic>ever, <italic>M</italic>agazine or nothing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e113" xlink:type="simple"/></inline-formula>). Once the action is chosen (e.g. <italic>goM</italic> in blue), only the value of the focused feature (e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e114" xlink:type="simple"/></inline-formula>) is updated by a standard reward prediction error, while leaving the values of the other features unchanged. (<bold>B</bold>) Feature-values permit generalization. At a different place and time in the episode, the agent can choose an action (e.g. <italic>goM</italic> in blue) focusing on a feature (e.g. <italic>M</italic>) that might have already been focused on. This leads to the revision of the same value (e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e115" xlink:type="simple"/></inline-formula>) for two different states (e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e116" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e117" xlink:type="simple"/></inline-formula>). Values of features are shared amongst multiple states.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g010" position="float" xlink:type="simple"/></fig>
<p>Relying on this idea, the second system is a revision of classical Model-Free systems which is based on features rather than states. It relies on a value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e118" xlink:type="simple"/></inline-formula> based on a set of features <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e119" xlink:type="simple"/></inline-formula>, which is updated with an RPE:<disp-formula id="pcbi.1003466.e120"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e120" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003466.e121"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e121" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e122" xlink:type="simple"/></inline-formula> is a feature-function that returns the feature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e123" xlink:type="simple"/></inline-formula> the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e124" xlink:type="simple"/></inline-formula> was focusing on in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e125" xlink:type="simple"/></inline-formula> (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s010">Table S2</xref>; <xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref> also embeds the features returned by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e126" xlink:type="simple"/></inline-formula> for each action and state). One could argue that this feature-function, defined <italic>a priori</italic>, introduces an additional requirement relative to classical Model-Free systems. This is a weak requirement since this function is straightforward when actions, instead of being abstractly defined, are described as interactions towards objects in the environment. This function simply states that, for example, when pressing a lever, the animal is focusing on the lever rather than on the magazine. Similar to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e127" xlink:type="simple"/></inline-formula>, we assume that the future action to be chosen is the most rewarding one. Therefore, the value chosen for the reached state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e128" xlink:type="simple"/></inline-formula>, in the computation of the RPE, is the highest value reachable by any possible future action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e129" xlink:type="simple"/></inline-formula>.</p>
<p>Classical Model-Free systems do not permit generalization in their standard form: even when two states share most of their features, updating the value of one state leaves the value of the other untouched. This new system overcomes such limitation (see <xref ref-type="fig" rid="pcbi-1003466-g010">Figure 10 B</xref>). In Feature-Model-Free Reinforcement Learning, multiple states in time and space can share features and their associated values. For example, while in ITI, rats tend from time to time to explore the magazine <xref ref-type="bibr" rid="pcbi.1003466-Robinson1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref>, which might lead them to revise any associated value, which can also be used when the lever appears. Therefore, actions in ITIs might impact the rest of the experiment.</p>
<p>In the simulated experiment (see <xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref>), this generalization phenomenon happens as follows: Assuming that the simulated rat was engaging the magazine (eng) before food delivery (from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e130" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e131" xlink:type="simple"/></inline-formula>), then the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e132" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e133" xlink:type="simple"/></inline-formula> is updated with the following <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e134" xlink:type="simple"/></inline-formula>. As the best subsequent action (and, for simplification, the only possible one) is to consume the food (in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e135" xlink:type="simple"/></inline-formula>), it results in a positive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e136" xlink:type="simple"/></inline-formula>. During ITI (which in the MDP is simulated by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e137" xlink:type="simple"/></inline-formula> parameter), if the simulated rat checks the magazine (goM) and finds no food, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e138" xlink:type="simple"/></inline-formula> is revised with a negative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e139" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003466-g010">Figure 10 B</xref>). The value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e140" xlink:type="simple"/></inline-formula> is therefore revised at multiple times in the experiment and, for example, a decrease of value during ITI has an impact on the choice of engaging with the magazine (goM) at lever appearance.</p>
<p>Processing features rather than states and the generalization that results from it is a key mechanism of the presented model. It makes the system favour a different path than the one favoured by classical reinforcement learning systems.</p>
<p>Contrary to what the system suggests, it is almost certain that rats might handle multiple features at once and could simultaneously update multiple values. We present here a version without such capacity since it is not required in the simulated experiments and simplifies its understanding.</p>
</sec><sec id="s4b3">
<title>Integration</title>
<p>The Feature-Model-Free system accounts for motivational bonuses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e141" xlink:type="simple"/></inline-formula> that impact values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e142" xlink:type="simple"/></inline-formula> computed by the Model-Based system. The integration of these values is made through a weighted sum:<disp-formula id="pcbi.1003466.e143"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e143" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e144" xlink:type="simple"/></inline-formula> is a combination parameter which defines the importance of each system in the overall model. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e145" xlink:type="simple"/></inline-formula> is equivalent to the responsibility signal in Mixture of Experts <xref ref-type="bibr" rid="pcbi.1003466-Doya1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Jacobs1">[85]</xref>. We want to emphasize that the two systems are not in simple competition, and it is not the case that there is a unique system acting at a time. Rather, they are both active and take part in the decision proportionally to the fixed parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e146" xlink:type="simple"/></inline-formula>. A simple switch between systems would not account for the full spectrum of observed behaviours ranging from STs to GTs <xref ref-type="bibr" rid="pcbi.1003466-Meyer1">[26]</xref>.</p>
</sec><sec id="s4b4">
<title>Action selection</title>
<p>We use a softmax rule on the integrated values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e147" xlink:type="simple"/></inline-formula> to compute the probability to select an action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e148" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e149" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003466.e150"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e150" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e151" xlink:type="simple"/></inline-formula> is the selection temperature that defines how probabilities are distributed. A high temperature (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e152" xlink:type="simple"/></inline-formula>) makes all actions equiprobable, a low one makes the most rewarding action almost exclusive.</p>
</sec><sec id="s4b5">
<title>Impact of flupentixol</title>
<p>When simulating the pharmacological experiments, namely the impact of flupentixol, a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e153" xlink:type="simple"/></inline-formula> is used to represent the impact of flupentixol on parts of the model.</p>
<p>As a dopamine receptor antagonist, we model the impact of flupentixol on phasic dopamine by revising any RPE <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e154" xlink:type="simple"/></inline-formula> used in the model given the following formula:<disp-formula id="pcbi.1003466.e155"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e155" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e156" xlink:type="simple"/></inline-formula> is the new RPE after flupentixol injection. The impact is filtered (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e157" xlink:type="simple"/></inline-formula>) such that flupentixol injection could not lead to negative learning when the RPE was positive, but at most block it (i.e. the sign of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e158" xlink:type="simple"/></inline-formula> cannot be different from the one of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e159" xlink:type="simple"/></inline-formula>). With a low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e160" xlink:type="simple"/></inline-formula>, the RPE is not affected (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e161" xlink:type="simple"/></inline-formula>). A high <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e162" xlink:type="simple"/></inline-formula> reduces the RPE, imitating a blockade of dopamine receptors.</p>
<p>Various studies (e.g. <xref ref-type="bibr" rid="pcbi.1003466-Humphries1">[32]</xref>) also suggest that tonic dopamine has an impact on action selection such that any decrease in dopamine level results in favouring exploration over exploitation. We therefore simulated the effect of flupentixol on action selection by revising the selection temperature given the following formula:<disp-formula id="pcbi.1003466.e163"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e163" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e164" xlink:type="simple"/></inline-formula> is the new selection temperature, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e165" xlink:type="simple"/></inline-formula> represents the strength of the flupentixol impact. A strong <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e166" xlink:type="simple"/></inline-formula>, which represents an effective dose of flupentixol, favours a high temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e167" xlink:type="simple"/></inline-formula> and therefore exploration. A low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e168" xlink:type="simple"/></inline-formula>, i.e. a low dose or an absence of flupentixol, leaves the temperature unaffected: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e169" xlink:type="simple"/></inline-formula>.</p>
<p>For the first pharmacological experiment (Effects of systemic flupentixol administration on the learning of sign- and goal-tracking behaviours) both the impact on the softmax and on the RPE were activated, as the flupentixol was injected systemically and assumed to diffuse in the whole brain. For the second experiment (Effects of local flupentixol administration on the expression of sign- and goal-tracking behaviours) only the impact on the RPE was activated, as the flupentixol was injected locally in the core of the nucleus accumbens. We hypothesize that the Feature-Model-Free system relies in the core of the nucleus accumbens whereas the selection process (softmax) does not.</p>
</sec><sec id="s4b6">
<title>Initialization</title>
<p>In the original experiments <xref ref-type="bibr" rid="pcbi.1003466-Flagel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Flagel2">[20]</xref>, prior to the autoshaping procedure, rats are familiarized with the Skinner box and the delivery of food into the magazine. While the MDP does not account for such pretraining, we can initialize the model with values (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e170" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e171" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e172" xlink:type="simple"/></inline-formula>) that reflect it (see the estimation of the model parameters). These initial values can be seen as extra parameters common to the model and its variants.</p>
</sec></sec><sec id="s4c">
<title>Variants</title>
<p>Given the modular architecture of the model, we were able to test different combinations of RL systems. Their analysis underlined the key mechanisms required for reproducing each result (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s001">Figures S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003466.s002">S2</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003466.s004">S4</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003466.s005">S5</xref>). <xref ref-type="fig" rid="pcbi-1003466-g011">Figure 11 (B, C and D)</xref> schematically represents the analysed variants.</p>
<fig id="pcbi-1003466-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003466.g011</object-id><label>Figure 11</label><caption>
<title>Systems combined in the model and the variants.</title>
<p>Variants of the model rely on the same architecture (described in <xref ref-type="fig" rid="pcbi-1003466-g002">Figure 2</xref>) and only differ in the combined systems. Colours are shared for similar systems. (<bold>A</bold>) The model combines a Model-Based system (MB, in blue) and a Feature-Model-Free (FMF, in red) system. (<bold>B</bold>) Variant 1 combines a Model-Free system (MF, in green) and a Feature-Model-Free system. (<bold>C</bold>) Variant 2 combines a Model-Free system and a Bias system (BS, in grey), that relies on values from the Model-Free system. (<bold>D</bold>) Variant 3 combines a Model-Free system and two Bias systems, that rely on values from the Model-Free system. Variant 4 is not included as it failed to even reproduce the autoshaping behavioural results.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003466.g011" position="float" xlink:type="simple"/></fig>
<p>Most of the results rely on the action taken by the agent at the lever appearance. The action taken results from the values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e173" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e174" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e175" xlink:type="simple"/></inline-formula>, the computation of which differs in each of the variants described below.</p>
<sec id="s4c1">
<title>Variant 1 : Model-Free/Feature-Model-Free</title>
<p>Variant 1 was tested to assert the necessity of the Model-Based system as part of the model to reproduce the results. Thus in Variant 1, the Model-Based system is replaced by a classical Model-Free system, Advantage learning <xref ref-type="bibr" rid="pcbi.1003466-BairdIII1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Dayan2">[84]</xref>, while the Feature-Model-Free system remains unchanged (see <xref ref-type="fig" rid="pcbi-1003466-g011">Figure 11 B</xref>).</p>
<p>In such a Model-Free system, the action-value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e176" xlink:type="simple"/></inline-formula> is updated online according to the transition just experienced. At each time step the function is updated given an RPE <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e177" xlink:type="simple"/></inline-formula> that computes the difference between the observed and the expected value, as follows:<disp-formula id="pcbi.1003466.e178"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e178" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pcbi.1003466.e179"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e179" xlink:type="simple"/></disp-formula></p>
<p>Computation of the associated Advantage function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e180" xlink:type="simple"/></inline-formula> follows <xref ref-type="disp-formula" rid="pcbi.1003466.e106">Equation (5)</xref>. This model computes integrated values as follows:<disp-formula id="pcbi.1003466.e181"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e181" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>It is important to note that while <xref ref-type="disp-formula" rid="pcbi.1003466.e181">Equation (12)</xref> looks similar to <xref ref-type="disp-formula" rid="pcbi.1003466.e143">Equation (7)</xref>, the Advantage function is computed by a Model-Based system in the model (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e182" xlink:type="simple"/></inline-formula>) and a Model-Free system in this variant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e183" xlink:type="simple"/></inline-formula>), leading to very different results on pharmacological experiments.</p>
</sec><sec id="s4c2">
<title>Variant 2 : Asymmetrical</title>
<p>Inspired by a work from Dayan et al. <xref ref-type="bibr" rid="pcbi.1003466-Dayan1">[16]</xref>, Variant 2 combines a classical Advantage learning system <xref ref-type="bibr" rid="pcbi.1003466-BairdIII1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1003466-Dayan2">[84]</xref> with some Bias system taking its values directly from the other system (see <xref ref-type="fig" rid="pcbi-1003466-g011">Figure 11 C</xref>). This system computes the integrated values as follows:<disp-formula id="pcbi.1003466.e184"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e184" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>It asymmetrically gives a bonus to the path that should be taken by STs. In slight discrepancy with the original model, it uses the maximum value over action-value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e185" xlink:type="simple"/></inline-formula> as the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e186" xlink:type="simple"/></inline-formula> used to compute the advantage function. Hence, there is a single RPE computed at each step.</p>
</sec><sec id="s4c3">
<title>Variant 3 : Symmetrical</title>
<p>In the same line as Variant 2, Variant 3 symmetrically gives a bonus to both paths using a classical Advantage learning system in combination with a Pavlovian system. This system computes the integrated values as follows:<disp-formula id="pcbi.1003466.e187"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e187" xlink:type="simple"/><label>(14)</label></disp-formula></p>
<p>This model does not exactly fit <xref ref-type="disp-formula" rid="pcbi.1003466.e143">Equation (7)</xref> of the general architecture. It is based on 3 systems, where the real competition is between the two bias systems, whereas the Model-Free system is mainly used to compute the values used by the two others (see <xref ref-type="fig" rid="pcbi-1003466-g011">Figure 11 D</xref>). The rest of the architecture is not impacted.</p>
</sec><sec id="s4c4">
<title>Variant 4 : Model-Based/Model-Free</title>
<p>Variant 4 was developed to confirm the necessity of a feature-based system. It combines two advantage functions computed from a Model-Based (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e188" xlink:type="simple"/></inline-formula>) and a Model-Free (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e189" xlink:type="simple"/></inline-formula>) system.<disp-formula id="pcbi.1003466.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003466.e190" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>While computed differently, both advantage functions will eventually converge to the same optimal values <xref ref-type="bibr" rid="pcbi.1003466-Sutton1">[1]</xref> making both systems favouring the same optimal policy. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e191" xlink:type="simple"/></inline-formula> cannot be used in this variant as there exists no value over the magazine itself. While varying the parameters might slow down learning or make the process more exploratory, this could never lead to sign-tracking as both systems, whatever the weighting, would favour goal-tracking. As such, Variant 4 is unable to even account for the main behavioural results in the autoshaping procedure (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s008">Figure S8</xref>).</p>
<p>Given that all the subsequent simulated results relies on a correct reproduction of the default behaviours, this variant was not investigated further and is not compared to the other variants in supplementary results figures.</p>
</sec></sec><sec id="s4d">
<title>Estimating the model parameters</title>
<p>The model relies on model-specific parameters (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e192" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e193" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e194" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e195" xlink:type="simple"/></inline-formula>) and experience-specific parameters (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e196" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e197" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e198" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e199" xlink:type="simple"/></inline-formula>). If the model were used to simulate a different experiment, the model-specific parameters would be the same while different experience-specific parameters might be required. For an easier analysis and a simpler comparison between the model and its variants, we reduce the number of parameters by sharing parameters with identical meanings amongst systems (i.e. both systems within the model share values for their learning rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e200" xlink:type="simple"/></inline-formula> and discount rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e201" xlink:type="simple"/></inline-formula>, rather than having independent parameter values).</p>
<p>Due to the number of parameters, finding the best values to qualitatively fit the experimental data cannot be done by hand. Using a genetic algorithm makes it possible to optimize the search of suitable values for the parameters.</p>
<p>Parameter values were retrieved by fitting the simulation of the probabilities to engage either the lever or the magazine with the experimental data of one of the previous studies <xref ref-type="bibr" rid="pcbi.1003466-Flagel3">[21]</xref>. No direct fitting was intended on other experimental data. Hence, a single set of values was used to simulate behavioural, physiological and pharmacological data.</p>
<p>If for a variant, the optimization algorithm fails to fit the experimental data, it suggests that whatever the values, the mechanisms involved cannot explain the behavioural data (Variant 4).</p>
<p>Probabilities to engage the lever or the magazine were taken as independent objectives of the algorithm, since fitting sign-tracking probabilities is easier than fitting goal-tracking probabilities. For each objective, the fitness function is computed as the least square errors between the experimental and simulated data. Parameter optimization is done with the multi-objective genetic algorithm NSGA-II <xref ref-type="bibr" rid="pcbi.1003466-Deb1">[86]</xref>. We used the implementation provided by the Sferes 2 framework <xref ref-type="bibr" rid="pcbi.1003466-Mouret1">[87]</xref>. All parameters required for reproducing the behavioural data were fitted at once.</p>
<p>For NSGA-II, we arbitrarily use a population of 200 individuals and run it over 1000 generations. We use a polynomial mutation with a rate of 0.1, and simulate binary cross-overs with a rate of 0.5. We select the representative individual, to be displayed in figures, from the resulting Pareto front by hand, such that it best visually fits the observed data.</p>
<p>To confirm that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e202" xlink:type="simple"/></inline-formula> is the key parameter of the model, we additionally tried to fit the whole population at once (i.e. sharing all parameter values in agents but <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e203" xlink:type="simple"/></inline-formula>) and we were still able to reproduce the observed tendencies of sign- and goal-tracking in the population (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s007">Figure S7 A,B</xref>) and the resulting different phasic dopaminergic patterns (see <xref ref-type="supplementary-material" rid="pcbi.1003466.s007">Figure S7 C,D</xref>).</p>
<p>It is however almost certain that each subgroup does not express the exact same values for the other parameters. Removing such constraint by fitting each subgroup separately, indeed provides better results. Results presented in this article are based on such separate fitting.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003466.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Comparison of variants of the model on simulations of autoshaping experiment.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5 (C,D)</xref>. Simulation parameters for STs (red), GTs (blue) and IGs (white) in the model (<bold>A</bold>), Variant 1 (<bold>B</bold>), Variant 2 (<bold>C</bold>) and Variant 3 (<bold>D</bold>) are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. All variants reproduce the spectrum of behaviours ranging from sign-tracking to goal-tracking.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Comparison of variants of the model on incentive salience and Conditioned Reinforcement Effect intuitions.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g006">Figure 6</xref>. Simulation parameters for STs (red), GTs (blue) and IGs (white) are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. Variant 2 (<bold>C</bold>) relying on asymmetrical bonuses given only to sign-tracking cannot reproduce the attribution of a motivational value by the second system to both the lever and the magazine. Others (<bold>A,B,D</bold>) attribute values to both stimuli and parallels the supposed acquisition of motivational values by stimuli, i.e. incentive salience. All variants are able to account for a Conditioned Reinforcement Effect more pronounced in STs than in GTs.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Comparison of variants of the model on simulations of patterns of dopaminergic activity.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 (C,D)</xref>. Simulation parameters for STs (left) and GTs (right) are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. The model (<bold>A</bold>) and Variant 1 (<bold>B</bold>) can reproduce the difference observed in dopaminergic patterns of activity in STs versus GTs. Other variants (<bold>C,D</bold>) fail to do so, given that the classical Model-Free system propagates the RPE from food delivery to lever appearance on all pathways of the MDP.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s004" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Comparison of variants on simulations of the effect of systemic injections of flupentixol.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g008">Figure 8 (C,D)</xref>. Simulation parameters for STs (left) and GTs (right) are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. Only the Model (<bold>A</bold>) can reproduce the difference in response to injections of flupentixol observed in STs versus GTs. All variants (<bold>B,C,D</bold>) fail to do so, given that they only rely on Model-Free, i.e. RPE-dependent, mechanisms that are blocked by flupentixol.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s005" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Comparison of variants on simulations of the effect of post injections of flupentixol.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g009">Figure 9 (C,D)</xref>. Simulation parameters for groups of rats composing the population are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. Variants 2 (<bold>C</bold>) and 3 (<bold>D</bold>), accounting for sign- and goal-tracking using a single set of values, have a similar impact of flupentixol on both behaviours, leaving relative probabilities to engage with lever and magazine unaffected. Variant 1 (<bold>B</bold>) uses different systems, thus flupentixol impacts sign-tracking in the model in the same way as it does in experimental data. However, given that both systems rely on RPE-dependent mechanisms, the impact is not as visible as in the model (<bold>A</bold>).</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s006" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p><bold>Prediction of the model about expected patterns of dopaminergic activity in intermediate groups.</bold> Data are expressed as mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e204" xlink:type="simple"/></inline-formula> S.E.M. Average RPE computed by the Feature-Model-Free system in response to CS and US presentation for each session of conditioning in the intermediate group. Simulated group is defined as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5</xref>.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s007" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s007" position="float" xlink:type="simple"><label>Figure S7</label><caption>
<p><bold>Behavioural and physiological simulations of autoshaping with shared parameter values across STs, GTs and IGs.</bold> (<bold>A,B</bold>) Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5 (C,D)</xref>. Reproduction of the respective tendencies to sign- and goal-track of STs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e205" xlink:type="simple"/></inline-formula>), IGs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e206" xlink:type="simple"/></inline-formula>) and GTs (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e207" xlink:type="simple"/></inline-formula>) using a single set of parameters (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e208" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e209" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e210" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e211" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e212" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e213" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e214" xlink:type="simple"/></inline-formula>). (<bold>C,D</bold>) Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g007">Figure 7 (C,D)</xref>. Reproduction of the different patterns of phasic dopaminergic activity in STs and GTs using the same single set of parameters. By simply varying the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e215" xlink:type="simple"/></inline-formula> parameter, the model can still qualitatively reproduce the observations in experimental data.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s008" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s008" position="float" xlink:type="simple"><label>Figure S8</label><caption>
<p><bold>Simulation of autoshaping experiment for Variant 4.</bold> Legend is as in <xref ref-type="fig" rid="pcbi-1003466-g005">Figure 5 (C,D)</xref>. Simulation for parameters STs (red), GTs (blue) and IGs (white) in the Variant 4 are summarized in <xref ref-type="supplementary-material" rid="pcbi.1003466.s009">Table S1</xref>. Variant 4 is not even able to reproduce the main behavioural data.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s009" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s009" position="float" xlink:type="simple"><label>Table S1</label><caption>
<p><bold>Summary of parameters used in simulations.</bold> Parameters retrieved by optimisation with NSGA-II and used to produce the results presented in this article for the model and its variants. Parameters for STs, GTs and IGs were optimized separately (A,B,C,D,E). To confirm that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e216" xlink:type="simple"/></inline-formula> is the key parameter of the model, we also optimized parameters for STs, GTs and IGs by sharing all but the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e217" xlink:type="simple"/></inline-formula> parameter (F) to produce <xref ref-type="supplementary-material" rid="pcbi.1003466.s007">Figure S7</xref>.</p>
<p>(TIFF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003466.s010" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003466.s010" position="float" xlink:type="simple"><label>Table S2</label><caption>
<p><bold>Definition of feature-function </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e218" xlink:type="simple"/></inline-formula><bold>.</bold> Stimuli (<italic>L</italic>ever, <italic>M</italic>agazine, <italic>F</italic>ood or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e219" xlink:type="simple"/></inline-formula>) returned by the feature-function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e220" xlink:type="simple"/></inline-formula> for each possible state-action pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003466.e221" xlink:type="simple"/></inline-formula> in the MDP described in <xref ref-type="fig" rid="pcbi-1003466-g001">Figure 1</xref>. The feature-function simply defines the stimulus that is the focus of an action in a particular state.</p>
<p>(TIFF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors would like to thank Angelo Arleo, Kent Berridge, Etienne Coutureau, Alain Marchand and Benjamin Saunders for helpful discussions. The authors would also like to thank the reviewers for their valuable comments and suggestions that helped to improve the contents of this paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003466-Sutton1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutton RS, Barto AG (1998) Reinforcement learning: An introduction. The MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Sutton2"><label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutton RS, Barto AG (1987) A temporal-difference model of classical conditioning. In: Proceedings of the ninth annual conference of the cognitive science society. Seattle, WA, pp. 355–378.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Barto1"><label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">Barto AG (1995) Adaptive critics and the basal ganglia. In: Houk JC, Davis JL, Beiser DG, editors, Models of information processing in the basal ganglia, The MIT Press. pp. 215–232.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Clark1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clark</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Hollon</surname><given-names>NG</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>PEM</given-names></name> (<year>2012</year>) <article-title>Pavlovian valuation systems in learning and decision making</article-title>. <source>Curr Opin Neurobiol</source> <volume>22</volume>: <fpage>1054</fpage>–<lpage>1061</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Simon1"><label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Simon DA, Daw ND (2012) Dual-system learning models and drugs of abuse. In: Computational Neuroscience of Drug Addiction, Springer. pp. 145–161.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Cardinal1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cardinal</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Parkinson</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Hall</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Everitt</surname><given-names>BJ</given-names></name> (<year>2002</year>) <article-title>Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex</article-title>. <source>Neurosci Biobehav Rev</source> <volume>26</volume>: <fpage>321</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Yin1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Ostlund</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2005</year>) <article-title>The role of the dorsomedial striatum in instrumental conditioning</article-title>. <source>Eur J neurosci</source> <volume>22</volume>: <fpage>513</fpage>–<lpage>523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Solway1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Solway</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Botvinick</surname><given-names>MM</given-names></name> (<year>2012</year>) <article-title>Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title>. <source>Psychol Rev</source> <volume>119</volume>: <fpage>120</fpage>–<lpage>154</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Daw1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2011</year>) <article-title>Model-based influences on humans' choices and striatal prediction errors</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>1204</fpage>–<lpage>1215</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Graybiel1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graybiel</surname><given-names>AM</given-names></name> (<year>2008</year>) <article-title>Habits, rituals, and the evaluative brain</article-title>. <source>Annu Rev Neurosci</source> <volume>31</volume>: <fpage>359</fpage>–<lpage>387</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Yin2"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2004</year>) <article-title>Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning</article-title>. <source>Eur J neurosci</source> <volume>19</volume>: <fpage>181</fpage>–<lpage>189</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Schultz1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>1998</year>) <article-title>Predictive reward signal of dopamine neurons</article-title>. <source>J Neurophysiol</source> <volume>80</volume>: <fpage>1</fpage>–<lpage>27</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Fiorillo1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>2003</year>) <article-title>Discrete coding of reward probability and uncertainty by dopamine neurons</article-title>. <source>Science</source> <volume>299</volume>: <fpage>1898</fpage>–<lpage>1902</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Flagel1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Mayo</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Czuj</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>A selective role for dopamine in stimulus-reward learning</article-title>. <source>Nature</source> <volume>469</volume>: <fpage>53</fpage>–<lpage>57</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Danna1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Danna</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Elmer</surname><given-names>GI</given-names></name> (<year>2010</year>) <article-title>Disruption of conditioned reward association by typical and atypical antipsychotics</article-title>. <source>Pharmacol Biochem Behav</source> <volume>96</volume>: <fpage>40</fpage>–<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Dayan1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2006</year>) <article-title>The misbehavior of value and the discipline of the will</article-title>. <source>Neural Netw</source> <volume>19</volume>: <fpage>1153</fpage>–<lpage>1160</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Daw2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>1704</fpage>–<lpage>1711</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Keramati1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keramati</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dezfouli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Piray</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Speed/Accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002055</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Glscher1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gläscher</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>O'Doherty</surname><given-names>JP</given-names></name> (<year>2010</year>) <article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>585</fpage>–<lpage>595</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Flagel2"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Watson</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Akil</surname><given-names>H</given-names></name> (<year>2007</year>) <article-title>Individual differences in the propensity to approach signals vs goals promote different adaptations in the dopamine system of rats</article-title>. <source>Psychopharmacology</source> <volume>191</volume>: <fpage>599</fpage>–<lpage>607</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Flagel3"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Akil</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name> (<year>2009</year>) <article-title>Individual differences in the attribution of incentive salience to reward-related cues: Implications for addiction</article-title>. <source>Neuropharmacology</source> <volume>56</volume>: <fpage>139</fpage>–<lpage>148</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Robinson1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name> (<year>2009</year>) <article-title>Dissociating the predictive and incentive motivational properties of reward-related cues through the study of individual differences</article-title>. <source>Biol psychiatry</source> <volume>65</volume>: <fpage>869</fpage>–<lpage>873</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Mahler1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mahler</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name> (<year>2009</year>) <article-title>Which cue to “want?” Central amygdala opioid activation enhances and focuses incentive salience on a prepotent reward cue</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>6500</fpage>–<lpage>13</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-DiFeliceantonio1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiFeliceantonio</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name> (<year>2012</year>) <article-title>Which cue to ‘want’? Opioid stimulation of central amygdala makes goal-trackers show stronger goal-tracking, just as sign-trackers show stronger sign-tracking</article-title>. <source>Behav Brain Res</source> <volume>230</volume>: <fpage>399</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Saunders1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saunders</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name> (<year>2012</year>) <article-title>The role of dopamine in the accumbens core in the expression of pavlovian-conditioned responses</article-title>. <source>Eur J neurosci</source> <volume>36</volume>: <fpage>2521</fpage>–<lpage>2532</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Meyer1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyer</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Lovic</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Saunders</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Yager</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Quantifying individual variation in the propensity to attribute incentive salience to reward cues</article-title>. <source>PLoS ONE</source> <volume>7</volume>: <fpage>e38987</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Berridge1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name> (<year>2007</year>) <article-title>The debate over dopamines role in reward: the case for incentive salience</article-title>. <source>Psychopharmacology</source> <volume>191</volume>: <fpage>391</fpage>–<lpage>431</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Lovic1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lovic</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Saunders</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Yager</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name> (<year>2011</year>) <article-title>Rats prone to attribute incentive salience to reward cues are also prone to impulsive action</article-title>. <source>Behav Brain Res</source> <volume>223</volume>: <fpage>255</fpage>–<lpage>261</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Williams1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>BA</given-names></name> (<year>1994</year>) <article-title>Conditioned reinforcement: Experimental and theoretical issues</article-title>. <source>Behav Anal</source> <volume>17</volume>: <fpage>261</fpage>–<lpage>285</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Skinner1"><label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Skinner BF (1938) The behavior of organisms: An experimental analysis. Appleton-Century-Crofts New York, 82–82 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Lomanowska1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lomanowska</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Lovic</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Rankine</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Mooney</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>TE</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Inadequate early social experience increases the incentive salience of reward-related cues in adulthood</article-title>. <source>Behav Brain Res</source> <volume>220</volume>: <fpage>91</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Humphries1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Humphries</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name> (<year>2012</year>) <article-title>Dopaminergic control of the exploration-exploitation trade-off via the basal ganglia</article-title>. <source>Front Neurosci</source> <volume>6</volume>: <fpage>9</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Khamassi1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Khamassi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Humphries</surname><given-names>MD</given-names></name> (<year>2012</year>) <article-title>Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies</article-title>. <source>Front Behav Neurosci</source> <volume>6</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Huys1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name>, <name name-style="western"><surname>Eshel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>O'Nions</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sheridan</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Bonsai trees in your head: How the pavlovian system sculpts goal-directed choices by pruning decision trees</article-title>. <source>PLoS Comput Biol</source> <volume>8</volume>: <fpage>e1002410</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Doya1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Samejima</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Katagiri</surname><given-names>Ki</given-names></name>, <name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name> (<year>2002</year>) <article-title>Multiple model-based reinforcement learning</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>1347</fpage>–<lpage>1369</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Redish1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname><given-names>Z</given-names></name> (<year>2007</year>) <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: Implications for addiction, relapse, and problem gambling</article-title>. <source>Psychol Rev</source> <volume>114</volume>: <fpage>784</fpage>–<lpage>805</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Takahashi1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname><given-names>YK</given-names></name>, <name name-style="western"><surname>Roesch</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Stalnaker</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Haney</surname><given-names>RZ</given-names></name>, <name name-style="western"><surname>Calu</surname><given-names>DJ</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>The orbitofrontal cortex and ventral tegmental area are necessary for learning from unexpected outcomes</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>269</fpage>–<lpage>280</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-McDannald1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDannald</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Lucantonio</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Burke</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname><given-names>G</given-names></name> (<year>2011</year>) <article-title>Ventral striatum and orbitofrontal cortex are both required for model-based, but not model-free, reinforcement learning</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>2700</fpage>–<lpage>2705</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-McDannald2"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDannald</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Takahashi</surname><given-names>YK</given-names></name>, <name name-style="western"><surname>Lopatina</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Pietras</surname><given-names>BW</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>JL</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Model-based learning and the contribution of the orbitofrontal cortex to the model-free world</article-title>. <source>Eur J neurosci</source> <volume>35</volume>: <fpage>991</fpage>–<lpage>996</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Cleland1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cleland</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Davey</surname><given-names>GCL</given-names></name> (<year>1983</year>) <article-title>Autoshaping in the rat: The effects of localizable visual and auditory signals for food</article-title>. <source>J Exp Anal Behav</source> <volume>40</volume>: <fpage>47</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Meyer2"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Meyer PJ, Aldridge JW, Robinson TE (2010) Auditory and visual cues are differentially attributed with incentive salience but similarly affected by amphetamine, 2010 neuroscience meeting planner. In: Society for Neuroscience Annual Meeting (SfN10).</mixed-citation>
</ref>
<ref id="pcbi.1003466-Schmajuk1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schmajuk</surname><given-names>NA</given-names></name>, <name name-style="western"><surname>Lam</surname><given-names>YW</given-names></name>, <name name-style="western"><surname>Gray</surname><given-names>JA</given-names></name> (<year>1996</year>) <article-title>Latent inhibition: A neural network approach</article-title>. <source>J Exp Psychol Anim Behav Process</source> <volume>22</volume>: <fpage>321</fpage>–<lpage>349</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Balkenius1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balkenius</surname><given-names>C</given-names></name> (<year>1999</year>) <article-title>Dynamics of a classical conditioning model</article-title>. <source>Auton Robots</source> <volume>7</volume>: <fpage>41</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Stout1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stout</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>RR</given-names></name> (<year>2007</year>) <article-title>Sometimes-competing retrieval (SOCR): A formalization of the comparator hypothesis</article-title>. <source>Psychol Rev</source> <volume>114</volume>: <fpage>759</fpage>–<lpage>783</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Courville1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Courville</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name> (<year>2006</year>) <article-title>Bayesian theories of conditioning in a changing world</article-title>. <source>Trends Cogn Sci</source> <volume>10</volume>: <fpage>294</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Gershman1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>Exploring a latent cause theory of classical conditioning</article-title>. <source>Anim Learn Behav</source> <volume>40</volume>: <fpage>255</fpage>–<lpage>268</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Kamin1"><label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">Kamin LJ (1967) Predictability, surprise, attention, and conditioning. In: Campbell BA, Church RMa, editors, Punishment and aversive behavior, New York: Appleton-Century-Crofts. pp. 279–296.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Lattal1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lattal</surname><given-names>KM</given-names></name>, <name name-style="western"><surname>Nakajima</surname><given-names>S</given-names></name> (<year>1998</year>) <article-title>Overexpectation in appetitive pavlovian and instrumental conditioning</article-title>. <source>Anim Learn Behav</source> <volume>26</volume>: <fpage>351</fpage>–<lpage>360</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Bellman1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Bellman R (1957) Dynamic programming. Princeton University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Khamassi2"><label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Khamassi M, Martinet LE, Guillot A (2006) Combining self-organizing maps with mixtures of experts: application to an actor-critic model of reinforcement learning in the basal ganglia. In: From Animals to Animats 9, Springer. pp. 394–405.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Elfwing1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elfwing</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Uchibe</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2013</year>) <article-title>Scaled free-energy based reinforcement learning for robust and efficient learning in high-dimensional state spaces</article-title>. <source>Front Neurorobot</source> <volume>7</volume>: <fpage>3</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Boutilier1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boutilier</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Dearden</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Goldszmidt</surname><given-names>M</given-names></name> (<year>2000</year>) <article-title>Stochastic dynamic programming with factored representations</article-title>. <source>Artif Intell</source> <volume>121</volume>: <fpage>49</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Degris1"><label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Degris T, Sigaud O, Wuillemin PH (2006) Learning the structure of factored markov decision processes in reinforcement learning problems. In: Proceedings of the 23rd international conference on Machine learning. ACM, pp. 257–264.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Vigorito1"><label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Vigorito CM, Barto AG (2008) Autonomous hierarchical skill acquisition in factored mdps. In: Yale Workshop on Adaptive and Learning Systems, New Haven, Connecticut. volume 63, p. 109.</mixed-citation>
</ref>
<ref id="pcbi.1003466-GuitartMasip1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guitart-Masip</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name>, <name name-style="western"><surname>Fuentemilla</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Duzel</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Go and no-go learning in reward and punishment: interactions between affect and effect</article-title>. <source>Neuroimage</source> <volume>62</volume>: <fpage>154</fpage>–<lpage>166</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Huys2"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huys</surname><given-names>QJM</given-names></name>, <name name-style="western"><surname>Cools</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gölzer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Friedel</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Heinz</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Disentangling the roles of approach, activation and valence in instrumental and pavlovian responding</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002028</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Yin3"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Ostlund</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2008</year>) <article-title>Reward-guided learning beyond dopamine in the nucleus accumbens: the integrative functions of cortico-basal ganglia networks</article-title>. <source>Eur J neurosci</source> <volume>28</volume>: <fpage>1437</fpage>–<lpage>1448</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Corbit1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corbit</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2005</year>) <article-title>Double dissociation of basolateral and central amygdala lesions on the general and outcome-specific forms of pavlovian-instrumental transfer</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>962</fpage>–<lpage>970</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Balsam1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balsam</surname><given-names>PD</given-names></name>, <name name-style="western"><surname>Payne</surname><given-names>D</given-names></name> (<year>1979</year>) <article-title>Intertrial interval and unconditioned stimulus durations in autoshaping</article-title>. <source>Anim Learn Behav</source> <volume>7</volume>: <fpage>477</fpage>–<lpage>482</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Gibbon1"><label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Gibbon J, Balsam P (1981) Spreading association in time, Academic Press. pp. 219–253.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Gallistel1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Gibbon</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>Time, rate, and conditioning</article-title>. <source>Psychol Rev</source> <volume>107</volume>: <fpage>289</fpage>–<lpage>344</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Tomie1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tomie</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Festa</surname><given-names>ED</given-names></name>, <name name-style="western"><surname>Sparta</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Pohorecky</surname><given-names>LA</given-names></name> (<year>2003</year>) <article-title>Lever conditioned stimulus–directed autoshaping induced by saccharin–ethanol unconditioned stimulus solution: effects of ethanol concentration and trial spacing</article-title>. <source>Alcohol</source> <volume>30</volume>: <fpage>35</fpage>–<lpage>44</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Morris1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morris</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Nevet</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arkadir</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Midbrain dopamine neurons encode decisions for future action</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>1057</fpage>–<lpage>1063</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Roesch1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roesch</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Calu</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Schoenbaum</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>1615</fpage>–<lpage>1624</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Bellot1"><label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Bellot J, Sigaud O, Khamassi M (2012) Which temporal difference learning algorithm best reproduces dopamine activity in a multi-choice task? In: From Animals to Animats 12, Springer. pp. 289–298.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Tomie2"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tomie</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lincks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nadarajah</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Pohorecky</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>L</given-names></name> (<year>2012</year>) <article-title>Pairings of lever and food induce pavlovian conditioned approach of sign-tracking and goal-tracking in c57bl/6 mice</article-title>. <source>Behav Brain Res</source> <volume>226</volume>: <fpage>571</fpage>–<lpage>578</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Kobayashi1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kobayashi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>Influence of reward delays on responses of dopamine neurons</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>7837</fpage>–<lpage>7846</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Daw3"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Courville</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name> (<year>2006</year>) <article-title>Representation and timing in theories of the dopamine system</article-title>. <source>Neural Comput</source> <volume>18</volume>: <fpage>1637</fpage>–<lpage>1677</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Fiorillo2"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>The temporal precision of reward prediction in dopamine neurons</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>966</fpage>–<lpage>973</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Gurney1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gurney</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Humphries</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Wood</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Prescott</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Testing computational hypotheses of brain systems function: a case study with the basal ganglia</article-title>. <source>Network</source> <volume>15</volume>: <fpage>263</fpage>–<lpage>290</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Robinson2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname><given-names>MJF</given-names></name>, <name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name> (<year>2013</year>) <article-title>Instant transformation of learned repulsion into motivational “wanting”</article-title>. <source>Current Biology</source> <volume>23</volume>: <fpage>282</fpage>–<lpage>289</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Panlilio1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Panlilio</surname><given-names>LV</given-names></name>, <name name-style="western"><surname>Thorndike</surname><given-names>EB</given-names></name>, <name name-style="western"><surname>Schindler</surname><given-names>CW</given-names></name> (<year>2007</year>) <article-title>Blocking of conditioning to a cocaine-paired stimulus: testing the hypothesis that cocaine perpetually produces a signal of larger-than-expected reward</article-title>. <source>Pharmacol Biochem Behav</source> <volume>86</volume>: <fpage>774</fpage>–<lpage>777</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Redish2"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2004</year>) <article-title>Addiction as a computational process gone awry</article-title>. <source>Science</source> <volume>306</volume>: <fpage>1944</fpage>–<lpage>1947</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Daw4"><label>74</label>
<mixed-citation publication-type="other" xlink:type="simple">Daw ND, Niv Y, Dayan P (2006) Actions, policies, values and the basal ganglia. In: Bezard E, editor, Recent Breakthroughs in Basal Ganglia Research, Nova Science Publishers, Inc Hauppauge, NY. pp. 91–106.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Yin4"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name> (<year>2006</year>) <article-title>The role of the basal ganglia in habit formation</article-title>. <source>Nat Rev Neurosci</source> <volume>7</volume>: <fpage>464</fpage>–<lpage>476</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Thorn1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorn</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Atallah</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Howe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Graybiel</surname><given-names>AM</given-names></name> (<year>2010</year>) <article-title>Differential dynamics of activity changes in dorsolateral and dorsomedial striatal loops during learning</article-title>. <source>Neuron</source> <volume>66</volume>: <fpage>781</fpage>–<lpage>795</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Bornstein1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornstein</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name> (<year>2011</year>) <article-title>Multiplicity of control in the basal ganglia: computational roles of striatal subregions</article-title>. <source>Curr Opin Neurobiol</source> <volume>21</volume>: <fpage>374</fpage>–<lpage>380</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-vanderMeer1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van der Meer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2012</year>) <article-title>Information processing in decision-making systems</article-title>. <source>Neuroscientist</source> <volume>18</volume>: <fpage>342</fpage>–<lpage>359</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Flagel4"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flagel</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Cameron</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Pickup</surname><given-names>KN</given-names></name>, <name name-style="western"><surname>Watson</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Akil</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>A food predictive cue must be attributed with incentive salience for it to induce c-fos mRNA expression in cortico-striatalthalamic brain regions</article-title>. <source>Neuroscience</source> <volume>196</volume>: <fpage>80</fpage>–<lpage>96</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Mink1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mink</surname><given-names>JW</given-names></name> (<year>1996</year>) <article-title>The basal ganglia: focused selection and inhibition of competing motor programs</article-title>. <source>Prog Neurobiol</source> <volume>50</volume>: <fpage>381</fpage>–<lpage>425</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Redgrave1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Prescott</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name> (<year>1999</year>) <article-title>The basal ganglia: a vertebrate solution to the selection problem?</article-title> <source>Neuroscience</source> <volume>89</volume>: <fpage>1009</fpage>–<lpage>1023</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Gurney2"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Prescott</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name> (<year>2001</year>) <article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title>. <source>Biol Cybern</source> <volume>84</volume>: <fpage>401</fpage>–<lpage>410</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-BairdIII1"><label>83</label>
<mixed-citation publication-type="other" xlink:type="simple">Baird III LC (1993) Advantage updating. Technical report, DTIC Document.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Dayan2"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Balleine</surname><given-names>BW</given-names></name> (<year>2002</year>) <article-title>Reward, motivation, and reinforcement learning</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>285</fpage>–<lpage>298</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Jacobs1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jacobs</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Nowlan</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name> (<year>1991</year>) <article-title>Adaptive mixtures of local experts</article-title>. <source>Neural Comput</source> <volume>3</volume>: <fpage>79</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Deb1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deb</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Pratap</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Agarwal</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Meyarivan</surname><given-names>T</given-names></name> (<year>2002</year>) <article-title>A fast and elitist multiobjective genetic algorithm: Nsga-ii</article-title>. <source>IEEE Trans Evol Comput</source> <volume>6</volume>: <fpage>182</fpage>–<lpage>197</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003466-Mouret1"><label>87</label>
<mixed-citation publication-type="other" xlink:type="simple">Mouret JB, Doncieux S (2010) SFERESv2: Evolvin' in the Multi-Core World. In: WCCI 2010 IEEE World Congress on Computational Intelligence, Congress on Evolutionary Computation (CEC). pp. 4079–4086.</mixed-citation>
</ref>
</ref-list></back>
</article>