<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-17-00984</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006106</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Epidemiology</subject><subj-group><subject>Ethnic epidemiology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Ethnicities</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject><subj-group><subject>Health informatics</subject><subj-group><subject>Electronic medical records</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Support vector machines</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Ethnicities</subject><subj-group><subject>Hispanic people</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Kernel methods</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>RIDDLE: Race and ethnicity Imputation from Disease history with Deep LEarning</article-title>
<alt-title alt-title-type="running-head">Race/Ethnicity Imputation from Disease history</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8966-529X</contrib-id>
<name name-style="western">
<surname>Kim</surname> <given-names>Ji-Sung</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7108-3574</contrib-id>
<name name-style="western">
<surname>Gao</surname> <given-names>Xin</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6959-7405</contrib-id>
<name name-style="western">
<surname>Rzhetsky</surname> <given-names>Andrey</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, Princeton University, Princeton, New Jersey, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>King Abdullah University of Science and Technology (KAUST), Computational Bioscience Research Center (CBRC), Computer, Electrical and Mathematical Sciences and Engineering (CEMSE) Division, Thuwal, Saudi Arabia</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Institute for Genomics and Systems Biology, Computation Institute, Departments of Medicine and Human Genetics, University of Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Vert</surname> <given-names>Jean-Philippe</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Mines ParisTech, FRANCE</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">andrey.rzhetsky@uchicago.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>4</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>26</day>
<month>4</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>4</issue>
<elocation-id>e1006106</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>3</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Kim et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006106"/>
<abstract>
<p>Anonymized electronic medical records are an increasingly popular source of research data. However, these datasets often lack race and ethnicity information. This creates problems for researchers modeling human disease, as race and ethnicity are powerful confounders for many health exposures and treatment outcomes; race and ethnicity are closely linked to population-specific genetic variation. We showed that deep neural networks generate more accurate estimates for missing racial and ethnic information than competing methods (e.g., logistic regression, random forest, support vector machines, and gradient-boosted decision trees). RIDDLE yielded significantly better classification performance across all metrics that were considered: accuracy, cross-entropy loss (error), precision, recall, and area under the curve for receiver operating characteristic plots (all <italic>p</italic> &lt; 10<sup>−9</sup>). We made specific efforts to interpret the trained neural network models to identify, quantify, and visualize medical features which are predictive of race and ethnicity. We used these characterizations of informative features to perform a systematic comparison of differential disease patterns by race and ethnicity. The fact that clinical histories are informative for imputing race and ethnicity could reflect (1) a skewed distribution of blue- and white-collar professions across racial and ethnic groups, (2) uneven accessibility and subjective importance of prophylactic health, (3) possible variation in lifestyle, such as dietary habits, and (4) differences in background genetic variation which predispose to diseases.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Race and ethnicity are typically unspecified in very large electronic medical claims datasets. Computationally estimating a patient’s missing race and ethnicity from their medical records is important on both an academic and practical basis. Academically, discriminative medical events tell us about racial and ethnic health disparities and divergent genetic predispositions. Practically, imputed race and ethnicity information can substantially improve genetic and epidemiological analyses with these large datasets.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000025</institution-id>
<institution>National Institute of Mental Health</institution>
</institution-wrap>
</funding-source>
<award-id>1P50MH094267</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6959-7405</contrib-id>
<name name-style="western">
<surname>Rzhetsky</surname> <given-names>Andrey</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000050</institution-id>
<institution>National Heart, Lung, and Blood Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01HL122712</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6959-7405</contrib-id>
<name name-style="western">
<surname>Rzhetsky</surname> <given-names>Andrey</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000185</institution-id>
<institution>Defense Advanced Research Projects Agency</institution>
</institution-wrap>
</funding-source>
<award-id>W911NF1410333</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6959-7405</contrib-id>
<name name-style="western">
<surname>Rzhetsky</surname> <given-names>Andrey</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>The study was supported by funds from the Defense Advanced Projects Agency, contract W911NF1410333 to AR (<ext-link ext-link-type="uri" xlink:href="https://www.darpa.mil/program/big-mechanism" xlink:type="simple">https://www.darpa.mil/program/big-mechanism</ext-link>), the National Heart Lung and Blood Institute, award R01HL122712 to AR (<ext-link ext-link-type="uri" xlink:href="https://www.nhlbi.nih.gov/" xlink:type="simple">https://www.nhlbi.nih.gov/</ext-link>), the National Institute of Mental Health, award P50 MH094267 to AR (<ext-link ext-link-type="uri" xlink:href="https://grants.nih.gov/grants/guide/pa-files/PAR-14-120.html" xlink:type="simple">https://grants.nih.gov/grants/guide/pa-files/PAR-14-120.html</ext-link>), by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR), awards FCC/1/1976-04, URF/1/3007-01, URF/1/ 3450-01 and URF/1/3454-01to XG, and a gift from Liz and Kent Dauten to AR. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="4"/>
<page-count count="15"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-05-08</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data comprise millions of de-identified patient clinical records that cannot be deposited publicly and cannot be shared without special agreement with the Columbia University and the University of Chicago. Data are available from third party: to access the University of Chicago data, please visit the Center for Research Informatics, <ext-link ext-link-type="uri" xlink:href="http://cri.uchicago.edu" xlink:type="simple">http://cri.uchicago.edu</ext-link>; at the Columbia University, data can be accessed through the Electronic Medical Records and Genomics (eMERGE) network, <ext-link ext-link-type="uri" xlink:href="http://emerge.cumc.columbia.edu" xlink:type="simple">http://emerge.cumc.columbia.edu</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Electronic medical records (EMRs) are an increasingly popular source of biomedical research data [<xref ref-type="bibr" rid="pcbi.1006106.ref001">1</xref>]. EMRs are digital records of patient medical histories, describing the occurrence of specific diseases and medical events such as the observation of heart disease or dietary counseling. EMRs can also contain demographic information such as gender or age.</p>
<p>However, these datasets are often anonymized and lack race and ethnicity information (e.g., insurance claims datasets). Race and ethnicity information may also be missing for specific individuals within datasets. This is problematic in research settings as race and ethnicity can be powerful confounders for a variety of effects. Race and ethnicity are strong correlates of socioeconomic status, a predictor of access to and quality of education and healthcare. These factors are differentially associated with disease incidence and trajectories. As a result of this correlation, race and ethnicity may be associated with variation in medical histories. As an example, it has been reported that referrals for cardiac catheterization are rarer among African American patients than in White patients [<xref ref-type="bibr" rid="pcbi.1006106.ref002">2</xref>]. Furthermore, researchers have reported differences in genetic variation which influence disease across racial and ethnic groups [<xref ref-type="bibr" rid="pcbi.1006106.ref003">3</xref>]. Due to the association between race, ethnicity and medical histories, we hypothesize that clinical features in EMRs can be used to impute missing race and ethnicity information.</p>
<p>In addition, race and ethnicity information can be useful for producing and investigating hypotheses in epidemiology. For example, variation in disease risk across racial and ethnic groups that cannot be fully explained by allele frequency information may provide insights into the possible environmental modifiers of genes [<xref ref-type="bibr" rid="pcbi.1006106.ref003">3</xref>].</p>
<sec id="sec002">
<title>Imputation</title>
<p>The task of race and ethnicity imputation can be serialized as a supervised learning problem. Typically, the goal of imputation is to estimate a posterior probability distribution over plausible values for a missing variable. This distribution of plausible values can be used to generate a single imputed dataset (e.g., by choosing plausible values with highest probability), or to generate multiple imputed datasets as in <italic>multiple imputation</italic> [<xref ref-type="bibr" rid="pcbi.1006106.ref004">4</xref>]. In our setting, the goal was to impute the distribution of mutually-exclusive race and ethnicity classes given a set of clinical features. Features comprised age, gender, and codes from the International Disease Classification, version 9 (ICD9, [<xref ref-type="bibr" rid="pcbi.1006106.ref005">5</xref>]); ICD9 codes describe medical conditions, medical procedures, family information, and some treatment outcomes.</p>
<p>Bayesian approaches to race and ethnicity imputation using census data have been proposed [<xref ref-type="bibr" rid="pcbi.1006106.ref006">6</xref>] and have been used for race and ethnicity imputation in EMR datasets [<xref ref-type="bibr" rid="pcbi.1006106.ref007">7</xref>]. However, these approaches require sensitive geolocation and surname data from patients. Geolocation and surname data can be missing in anonymized EMR datasets (as in the datasets used here), limiting the utility of approaches which use this information.</p>
</sec>
<sec id="sec003">
<title>Deep learning</title>
<p>Traditionally, logistic regression classifiers have been used to impute categorical variables such as race and ethnicity [<xref ref-type="bibr" rid="pcbi.1006106.ref008">8</xref>]. However, there has been recent interest in the use of deep learning for solving similar supervised learning tasks. Deep learning is particularly exciting as it offers the ability to automatically learn complex representations of high-dimensional data. These representations can be used to solve learning tasks such as regression or classification [<xref ref-type="bibr" rid="pcbi.1006106.ref009">9</xref>].</p>
<p>Deep learning involves the approximation of some utility function (e.g., classification of an image) as a neural network. A neural network is a directed graph of functions which are referred to as units, neurons or nodes. This network is organized into several layers; each layer corresponds to a different representation of the input data. As the input data is transformed and propagated through this network, the data at each layer corresponds to a new representation of the sample [<xref ref-type="bibr" rid="pcbi.1006106.ref009">9</xref>]. For our imputation task, the aim was to learn the representation of an individual as a mixture of race and ethnicity classes where each class is assigned a probability. This representation is encoded in the final output layer of the neural network. The output of a neural network functions as a prediction of the distribution of race and ethnicity classes given a set of input features.</p>
<p>We introduce a framework for using deep learning to estimate missing race and ethnicity information in EMR datasets: <bold>RIDDLE</bold> or <bold>R</bold>ace and ethnicity <bold>I</bold>mputation from <bold>D</bold>isease history with <bold>D</bold>eep <bold>LE</bold>arning. RIDDLE uses a relatively simple multilayer perceptron (MLP), a type of neural network architecture that is a directed acyclic graph (see <xref ref-type="fig" rid="pcbi.1006106.g001">Fig 1</xref>).</p>
<fig id="pcbi.1006106.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Neural network architecture.</title>
<p>RIDDLE uses a multi-layer perceptron (MLP) network containing two hidden layers of either Rectified Linear Units (ReLU) or Parametric Rectified Linear Unit (PReLU) nodes. The input to the MLP is the set of binary encoded features comprising age, gender, and International Disease Classification, version 9 (ICD9) codes. The output is the set of probability estimates for each of the four race and ethnicity classes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.g001" xlink:type="simple"/>
</fig>
<p>In addition to investigating the novel utility of deep learning for race and ethnicity imputation, we used recent methods in interpreting neural network models [<xref ref-type="bibr" rid="pcbi.1006106.ref010">10</xref>] to perform a systematic evaluation of racial and ethnic patterns for approximately 15,000 different medical events. We believe that this type of large-scale evaluation of disease patterns and maladies by race and ethnicity has not been done heretofore.</p>
</sec>
</sec>
<sec id="sec004" sec-type="results">
<title>Results</title>
<p>We aimed to assess RIDDLE’s imputation performance in a multiclass classification setting. We used EMR datasets from Chicago and New York City, collectively describing over 1.5 million unique patients. There were approximately 15,000 unique input features consisting of basic demographic information (gender, age) and observations of clinical events (codified as ICD9 codes). The target class was race and ethnicity; possible values were White, Black, Other or Hispanic (see <xref ref-type="table" rid="pcbi.1006106.t001">Table 1</xref>). Although race and ethnicity can be described as a mixture, our training datasets labeled race and ethnicity as one of four mutually exclusive classes. For the testing set, we treated the target race and ethnicity class as unknown, and compared the predicted class against the true class. The large dimensionality of features, high number of samples, and heterogeneity of the source populations present a unique and challenging classification problem.</p>
<table-wrap id="pcbi.1006106.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.t001</object-id>
<label>Table 1</label>
<caption>
<title>Race and ethnicity composition of the EMR dataset.</title>
<p>The dataset comprised individuals from four race and ethnicity classes: Other, White, Hispanic, Black.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006106.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-top:thick">Ethnicity</th>
<th align="center" style="border-top:thick">Number of samples</th>
<th align="center" style="border-top:thick">Percent in dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Other</td>
<td align="center">878,017</td>
<td align="char" char=".">53.2%</td>
</tr>
<tr>
<td align="left">White</td>
<td align="center">308,323</td>
<td align="char" char=".">18.7%</td>
</tr>
<tr>
<td align="left">Hispanic</td>
<td align="center">256,015</td>
<td align="char" char=".">15.5%</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">Black</td>
<td align="center" style="border-bottom:thick">207,645</td>
<td align="char" char="." style="border-bottom:thick">12.6%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In our experiments, RIDDLE yielded an average accuracy of 0.668, and cross-entropy loss of 0.857 on test data, significantly outperforming logistic regression, random forest classifiers, and gradient-boosted decision tree (GBDT) classifiers across all classification metrics (<italic>p</italic> &lt; 10<sup>−9</sup>; see <xref ref-type="table" rid="pcbi.1006106.t002">Table 2</xref>).</p>
<table-wrap id="pcbi.1006106.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.t002</object-id>
<label>Table 2</label>
<caption>
<title>Evaluation of RIDDLE and baseline classification methods.</title>
<p>All values are averaged over ten <italic>k</italic>-fold cross-validation experiments. In addition, the precision, recall and ROC scores are averaged across classes, weighted by the number of samples in each class. Support vector machines (SVMs) could not be evaluated on the full dataset as individual trials required more than 36 hours of computation. For runtime comparisons a standard computing configuration was used: 16 Intel Sandybridge cores at 2.6 GHz and 16GB RAM; graphics processing units were not utilized.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006106.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-top:thick">Method</th>
<th align="center" style="border-top:thick">Accuracy</th>
<th align="center" style="border-top:thick">Loss</th>
<th align="center" style="border-top:thick">Precision</th>
<th align="center" style="border-top:thick">Recall</th>
<th align="center" style="border-top:thick">F1</th>
<th align="center" style="border-top:thick">Macro-average ROC</th>
<th align="center" style="border-top:thick">Runtime (h)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">RIDDLE</td>
<td align="center"><bold>0.668</bold></td>
<td align="center"><bold>0.857</bold></td>
<td align="center"><bold>0.663</bold></td>
<td align="center"><bold>0.668</bold></td>
<td align="center"><bold>0.652</bold></td>
<td align="center"><bold>0.833</bold></td>
<td align="center">0.962</td>
</tr>
<tr>
<td align="left">logistic regression</td>
<td align="center">0.644</td>
<td align="center">0.928</td>
<td align="center">0.639</td>
<td align="center">0.644</td>
<td align="center">0.611</td>
<td align="center">0.807</td>
<td align="center"><bold>0.024</bold></td>
</tr>
<tr>
<td align="left">random forest</td>
<td align="center">0.629</td>
<td align="center">0.962</td>
<td align="center">0.641</td>
<td align="center">0.629</td>
<td align="center">0.578</td>
<td align="center">0.799</td>
<td align="center">2.395</td>
</tr>
<tr>
<td align="left">GBDT</td>
<td align="center">0.634</td>
<td align="center">0.948</td>
<td align="center">0.635</td>
<td align="center">0.634</td>
<td align="center">0.592</td>
<td align="center">0.793</td>
<td align="center">0.265</td>
</tr>
<tr>
<td align="left">SVM, linear kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">&gt;36</td>
</tr>
<tr>
<td align="left">SVM, polynomial kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">&gt;36</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">SVM, RBF kernel</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">&gt;36</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Support vector machines (SVMs) with various kernels were also evaluated. However, SVMs could not be feasibly used with the full dataset as individual trials took longer than 36 hours each (36 hours runtime was the allowed maximum on the system used in our analysis). Additional experiments involving a smaller subset of the full dataset (165K samples) were performed; in such experiments, SVMs could be practically utilized and RIDDLE significantly outperformed the baseline methods across all classification metrics (<italic>p</italic> &lt; 10<sup>−2</sup>; see Table E in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>).</p>
<p>While the multiclass learning problem appeared relatively hard, RIDDLE achieved class-specific receiver operating characteristic’s (ROC) area under the curve (AUC) values above 0.8 (see <xref ref-type="fig" rid="pcbi.1006106.g002">Fig 2</xref>), and a micro-average (all cases considered as binary) AUC of 0.874—significantly higher than that of logistic regression (mean = 0.854, <italic>p</italic> = 6.67 × 10<sup>−11</sup>), random forest (mean = 0.844, <italic>p</italic> = 2.05 × 10<sup>−10</sup>) and GBDT (mean = 0.846, <italic>p</italic> = 1.20 × 10<sup>−10</sup>) classifiers (see <xref ref-type="table" rid="pcbi.1006106.t002">Table 2</xref>).</p>
<fig id="pcbi.1006106.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Receiver operating characteristic (ROC) curves.</title>
<p>ROC curves and their corresponding area under the curve (AUC) values were calculated for each of the four race and ethnicity classes. Micro-average (all cases considered as binary, e.g., Hispanic vs. non-Hispanic) and macro-average (average across classes) curves were also computed. Data and metrics for a representative experiment is shown. Across experiments, the <italic>mean</italic> micro-average AUC was 0.874, and the macro-average AUC was 0.833.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.g002" xlink:type="simple"/>
</fig>
<p>RIDDLE exhibited runtime performance comparable to that of other machine learning methods on a standard computing configuration without the use of a graphics processing unit or GPU (see <xref ref-type="table" rid="pcbi.1006106.t002">Table 2</xref>).</p>
<p>As explained prior, SVMs were also evaluated but precise runtime measurements could not be obtained as the computational cost was too high. However, on a smaller subset (165K samples) of the full dataset where SVMs could be utilized, RIDDLE exhibited significantly faster runtime performance compared to all SVM methods (<italic>p</italic> &lt; 10<sup>−10</sup>; see Table E in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>).</p>
<sec id="sec005">
<title>Influence of missing data on classifier performance</title>
<p>In order to replicate real-world applications where data other than race and ethnicity (e.g., features for specific samples) may be missing, we conducted additional experiments to simulate random missing data. A random subset of feature observations (ranging from 10% to 30% of all feature observations) was artificially masked completely at random.</p>
<p>Feature observations at the sample level (e.g., a particular ICD9 code for a specific patient) were randomly deleted to simulate random missing data. The number of whole features was kept fixed—only individual observations were removed. Otherwise, the same classification training and evaluation scheme was used as before. Under simulation of random missing data, RIDDLE significantly outperformed logistic regression, random forest classifiers and GBDTs in classification metrics across all simulation experiments (<italic>p</italic> &lt; 10<sup>−9</sup> for 10% and 20% missing data simulation, <italic>p</italic> &lt; 10<sup>−4</sup> for 30% missing data simulation; see <xref ref-type="table" rid="pcbi.1006106.t003">Table 3</xref>).</p>
<table-wrap id="pcbi.1006106.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.t003</object-id>
<label>Table 3</label>
<caption>
<title>Evaluation of RIDDLE and other methods under simulation of random missing data.</title>
<p>All values are averaged over ten <italic>k</italic>-fold cross-validation experiments involving different proportions of random missing data (10%–30%). In addition, the precision, recall and ROC scores are averaged across classes, weighted by the number of samples in each class. SVMs could not be evaluated on the full dataset as individual trials required more than 36 hours of computation.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006106.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="left" style="border-top:thick">Method</td>
<td align="center" style="border-top:thick">Accuracy</td>
<td align="center" style="border-top:thick">Loss</td>
<td align="center" style="border-top:thick">Precision</td>
<td align="center" style="border-top:thick">Recall</td>
<td align="center" style="border-top:thick">F1</td>
<td align="center" style="border-top:thick">Macro-average ROC</td>
</tr>
<tr>
<td align="left">RIDDLE</td>
<td align="center"><bold>0.660</bold></td>
<td align="center"><bold>0.878</bold></td>
<td align="center"><bold>0.656</bold></td>
<td align="center"><bold>0.660</bold></td>
<td align="center"><bold>0.643</bold></td>
<td align="center"><bold>0.822</bold></td>
</tr>
<tr>
<td align="left">logistic regression</td>
<td align="center">0.639</td>
<td align="center">0.941</td>
<td align="center">0.634</td>
<td align="center">0.639</td>
<td align="center">0.604</td>
<td align="center">0.800</td>
</tr>
<tr>
<td align="left">random forest</td>
<td align="center">0.623</td>
<td align="center">0.978</td>
<td align="center">0.635</td>
<td align="center">0.623</td>
<td align="center">0.567</td>
<td align="center">0.789</td>
</tr>
<tr>
<td align="left">GBDT</td>
<td align="center">0.627</td>
<td align="center">0.967</td>
<td align="center">0.628</td>
<td align="center">0.627</td>
<td align="center">0.580</td>
<td align="center">0.782</td>
</tr>
<tr>
<td align="left">SVM, linear kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left">SVM, polynomial kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">SVM, RBF kernel</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
</tr>
<tr>
<td align="left" colspan="7">(a) 10% missing data</td>
</tr>
<tr>
<td align="left" style="border-top:thick">Method</td>
<td align="center" style="border-top:thick">Accuracy</td>
<td align="center" style="border-top:thick">Loss</td>
<td align="center" style="border-top:thick">Precision</td>
<td align="center" style="border-top:thick">Recall</td>
<td align="center" style="border-top:thick">F1</td>
<td align="center" style="border-top:thick">Macro-average ROC</td>
</tr>
<tr>
<td align="left">RIDDLE</td>
<td align="center"><bold>0.654</bold></td>
<td align="center"><bold>0.897</bold></td>
<td align="center"><bold>0.649</bold></td>
<td align="center"><bold>0.654</bold></td>
<td align="center"><bold>0.631</bold></td>
<td align="center"><bold>0.814</bold></td>
</tr>
<tr>
<td align="left">logistic regression</td>
<td align="center">0.634</td>
<td align="center">0.954</td>
<td align="center">0.629</td>
<td align="center">0.634</td>
<td align="center">0.596</td>
<td align="center">0.792</td>
</tr>
<tr>
<td align="left">random forest</td>
<td align="center">0.616</td>
<td align="center">0.994</td>
<td align="center">0.631</td>
<td align="center">0.616</td>
<td align="center">0.556</td>
<td align="center">0.779</td>
</tr>
<tr>
<td align="left">GBDT</td>
<td align="center">0.622</td>
<td align="center">0.979</td>
<td align="center">0.624</td>
<td align="center">0.622</td>
<td align="center">0.572</td>
<td align="center">0.774</td>
</tr>
<tr>
<td align="left">SVM, linear kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left">SVM, polynomial kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">SVM, RBF kernel</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
</tr>
<tr>
<td align="left" colspan="7">(b) 20% missing data</td>
</tr>
<tr>
<td align="left" style="border-top:thick">Method</td>
<td align="center" style="border-top:thick">Accuracy</td>
<td align="center" style="border-top:thick">Loss</td>
<td align="center" style="border-top:thick">Precision</td>
<td align="center" style="border-top:thick">Recall</td>
<td align="center" style="border-top:thick">F1</td>
<td align="center" style="border-top:thick">Macro-average ROC</td>
</tr>
<tr>
<td align="left">RIDDLE</td>
<td align="center"><bold>0.643</bold></td>
<td align="center"><bold>0.926</bold></td>
<td align="center"><bold>0.640</bold></td>
<td align="center"><bold>0.643</bold></td>
<td align="center"><bold>0.614</bold></td>
<td align="center"><bold>0.800</bold></td>
</tr>
<tr>
<td align="left">logistic regression</td>
<td align="center">0.629</td>
<td align="center">0.968</td>
<td align="center">0.623</td>
<td align="center">0.629</td>
<td align="center">0.587</td>
<td align="center">0.784</td>
</tr>
<tr>
<td align="left">random forest</td>
<td align="center">0.610</td>
<td align="center">1.009</td>
<td align="center">0.625</td>
<td align="center">0.610</td>
<td align="center">0.545</td>
<td align="center">0.769</td>
</tr>
<tr>
<td align="left">GBDT</td>
<td align="center">0.616</td>
<td align="center">0.995</td>
<td align="center">0.617</td>
<td align="center">0.616</td>
<td align="center">0.561</td>
<td align="center">0.764</td>
</tr>
<tr>
<td align="left">SVM, linear kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left">SVM, polynomial kernel</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">SVM, RBF kernel</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
<td align="center" style="border-bottom:thick">N/A</td>
</tr>
<tr>
<td align="left" colspan="7">(c) 30% missing data</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec006">
<title>Feature interpretation</title>
<p>A major criticism of deep learning is the opaqueness of trained neural network models for intuitive interpretation. While intricate functional architectures enable neural networks to learn complex tasks, they also create a barrier to understanding how learning decisions (e.g., classifications) are made. In addition to creating a precise race and ethnicity estimation framework, we sought to identify and describe the factors which contribute to these estimations. We computed DeepLIFT (Deep Learning Important FeaTures) scores to quantitatively describe how specific features contribute to the probability estimates of each class. The DeepLIFT algorithm compares the activation of each node to a reference activation; the difference between the reference and observed activation is used to compute the contribution score of a neuron to a class (see the <xref ref-type="sec" rid="sec008">Methods</xref>) [<xref ref-type="bibr" rid="pcbi.1006106.ref010">10</xref>].</p>
<p>If a feature contributes to selecting <italic>for</italic> a particular class, this feature-class pair is assigned a positive DeepLIFT score; conversely, if a feature contributes to selecting <italic>against</italic> a particular class, the pair is assigned a negative score. The magnitude of a DeepLIFT score represents the strength of the contribution.</p>
<p>Using DeepLIFT scores, we were able to construct natural orderings of race and ethnicity classes for each feature, sorting classes by positive to negative scores. The following example ordering shows how the example feature (heart disease) is a strong predictor for the African American class, and a weak (or negative) predictor for the Other class.</p>
<disp-formula id="pcbi.1006106.e001">
<alternatives>
<graphic id="pcbi.1006106.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006106.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mtext>heart</mml:mtext>
<mml:mspace width="4.pt"/>
<mml:mtext>disease</mml:mtext>
<mml:mo>→</mml:mo>
<mml:mtext>Other,</mml:mtext>
<mml:mspace width="4.pt"/>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mtext>score</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mo>-</mml:mo>
<mml:mn>500</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mtext>heart</mml:mtext>
<mml:mspace width="4.pt"/>
<mml:mtext>disease</mml:mtext>
<mml:mo>→</mml:mo>
<mml:mtext>White,</mml:mtext>
<mml:mspace width="4.pt"/>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mtext>score</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mo>-</mml:mo>
<mml:mn>100</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mtext>heart</mml:mtext>
<mml:mspace width="4.pt"/>
<mml:mtext>disease</mml:mtext>
<mml:mo>→</mml:mo>
<mml:mtext>Hispanic,</mml:mtext>
<mml:mspace width="4.pt"/>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mtext>score</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mo>+</mml:mo>
<mml:mn>200</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mtext>heart</mml:mtext>
<mml:mspace width="4.pt"/>
<mml:mtext>disease</mml:mtext>
<mml:mo>→</mml:mo>
<mml:mtext>Black,</mml:mtext>
<mml:mspace width="4.pt"/>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mtext>score</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mo>+</mml:mo>
<mml:mn>500</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mspace width="1.em"/>
<mml:mo>⇒</mml:mo>
<mml:mspace width="1.em"/>
<mml:mtext>Black</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mtext>Hispanic</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mtext>White</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mtext>Other</mml:mtext>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
<p>We computed the class orderings for all ∼15,000 features (see <xref ref-type="supplementary-material" rid="pcbi.1006106.s002">S1 Data</xref>). The orderings of the 10 most predictive features (by highest ranges of DeepLIFT scores) are described in <xref ref-type="table" rid="pcbi.1006106.t004">Table 4</xref>.</p>
<table-wrap id="pcbi.1006106.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.t004</object-id>
<label>Table 4</label>
<caption>
<title>DeepLIFT contribution score orderings for 10 most <italic>predictive</italic> ICD9 codes.</title>
<p>DeepLIFT scores were computed using separate test samples and models from ten k-fold cross validation experiments; scores were summed across experiments. DeepLIFT scores were produced for each pair of feature, and output (race and ethnicity) class; we list ten ICD9 codes with the highest ranges of scores—which correspond to discriminative ability. The feature-to-class contribution scores were used to construct orderings of race and ethnicity classes, for each feature. Scores were summed across all samples. Positive scores indicate favorable contribution to a class, zero scores indicate no contribution, and negative scores indicate discrimination against a class.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006106.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" style="border-top:thick">Rank</th>
<th align="left" style="border-top:thick">ICD9</th>
<th align="left" style="border-top:thick">Description</th>
<th align="center" style="border-top:thick">Ordering of race and ethnicity classes (DeepLIFT scores)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">1</td>
<td align="left">401.9</td>
<td align="left">Hypertension NOS</td>
<td align="center">H (961793) &gt; B (723387) &gt; W (330416) &gt; O (102487)</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">789.00</td>
<td align="left">Abdominal pain, unspecified site</td>
<td align="center">H (533874) &gt; B (374665) &gt; W (343653) &gt; O (-45645)</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">V72.6</td>
<td align="left">Laboratory examination</td>
<td align="center">W (385026) &gt; B (-1114) &gt; O (-23509) &gt; H (-86539)</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">V70.0</td>
<td align="left">Routine general medical examination at a health care facility</td>
<td align="center">H (139118) &gt; B (-34600) &gt; W (-35159) &gt; O (-259566)</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">V65.44</td>
<td align="left">Human immunodeficiency virus [HIV] counseling</td>
<td align="center">H (-162191) &gt; B (-248484) &gt; W (-355563) &gt; O (-474608)</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">V76.12</td>
<td align="left">Other screening mammogram</td>
<td align="center">H (535820) &gt; B (425450) &gt; W (414514) &gt; O (253839)</td>
</tr>
<tr>
<td align="left">7</td>
<td align="left">V72.9</td>
<td align="left">Unspecified examination</td>
<td align="center">W (313506) &gt; H (212228) &gt; B (193901) &gt; O (68319)</td>
</tr>
<tr>
<td align="left">8</td>
<td align="left">V20.2</td>
<td align="left">Routine infant or child health check</td>
<td align="center">H (28390) &gt; B (-68712) &gt; W (-136391) &gt; O (-211301)</td>
</tr>
<tr>
<td align="left">9</td>
<td align="left">724.2</td>
<td align="left">Lumbago</td>
<td align="center">H (252024) &gt; B (169995) &gt; W (97782) &gt; O (15679)</td>
</tr>
<tr>
<td align="left" style="border-bottom:thick">10</td>
<td align="left" style="border-bottom:thick">V72.3</td>
<td align="left" style="border-bottom:thick">Special investigations and examinations—Gynecological examination</td>
<td align="center" style="border-bottom:thick">H (-515409) &gt; B (-643566) &gt; W (-665030) &gt; O (-741763)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We visualized the orderings of the 25 most <italic>common</italic> features using both frequencies and DeepLIFT scores (see <xref ref-type="fig" rid="pcbi.1006106.g003">Fig 3</xref>; the full table of features is shown in <xref ref-type="supplementary-material" rid="pcbi.1006106.s002">S1 Data</xref>). Frequency-based orderings were obtained by sorting the four classes by the number of samples within a class exhibiting a particular feature. Race and ethnicity class orderings obtained from frequency scores were distinctly different than those obtained from DeepLIFT scores. This suggests that RIDDLE’s MLP network is able to learn non-linear and non-frequentist relationships between ICD9 codes and race and ethnicity categories.</p>
<fig id="pcbi.1006106.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006106.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Visualizing class orderings for the 25 most <italic>common</italic> features.</title>
<p>We constructed natural orderings of features for the 25 most common ICD9 code features, using (A) frequency information and (B) DeepLIFT scores. Frequency scores were mean-centered; higher scores indicate larger contribution by a feature to a class. These orderings rank the contribution of an ICD9 code to a particular class, and are visualized as a stacked bar. The strongest (positive) feature-to-class contributions are represented by the rightmost bar; the length of the bar corresponds to the magnitude of the contribution on a linear scale. Scores were summed across all samples.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.g003" xlink:type="simple"/>
</fig>
<p>According to orderings constructed using DeepLIFT scores, sex is an important feature for predicting race and ethnicity in our models: men who seek medical attention are least likely to be Other followed by African American men. Men who seek medical attention are most likely to be White or Hispanic.</p>
<p>In addition, specific medical diagnoses convey grains of racial and ethnic information: hypertension and human immunodeficiency virus (HIV) are more predictive for African American and Hispanic individuals than White individuals. This finding is also reflected in medical literature, where it has been reported that African American and Hispanic populations are at significantly higher risk for heart disease [<xref ref-type="bibr" rid="pcbi.1006106.ref011">11</xref>–<xref ref-type="bibr" rid="pcbi.1006106.ref013">13</xref>] and HIV [<xref ref-type="bibr" rid="pcbi.1006106.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006106.ref016">16</xref>] than their White peers.</p>
<p>The fact that these features are important for imputing race and ethnicity could reflect (1) a skewed distribution of blue- and white-collar professions across racial and ethnic groups, (2) uneven accessibility and subjective importance of prophylactic health care across racial and ethnic groups, and (3) possible variation in lifestyle, such as dietary habits. Further work would involve investigating epidemiological hypotheses on how these environmental factors may affect differential clinical patterns across race and ethnicity.</p>
<p>Some of the genetic diseases are famously discriminative for races and ethnicities. For example, sickle cell disease occurs more frequently in African Americans and Hispanic populations than in the rest of the US population [<xref ref-type="bibr" rid="pcbi.1006106.ref017">17</xref>]. In our model, sickle cell anemia most strongly predicts for the African American and Hispanic classes over the White or Other classes. It has been reported Lyme disease predominately occurs in Whites, and largely unreported for Hispanics or African Americans [<xref ref-type="bibr" rid="pcbi.1006106.ref018">18</xref>]. This finding is also reflected in our model, where Lyme disease serves as a strong predictor of the White race. Additional strongly White-predictive diseases and medical procedures include atrial fibrillation, hypothyroidism, prostate neoplasm, dressing and sutures, lump in breast, coronary atherosclerosis. These are primarily diseases of older age, suggesting that lifespan varies across race and ethnicity due to socioeconomic and lifestyle reasons, as reported in literature [<xref ref-type="bibr" rid="pcbi.1006106.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1006106.ref020">20</xref>].</p>
<p>These orderings provide a high-level description of community structure, and may reflect socioeconomic, cultural, habitual, and genetic variation linked to race and ethnicity across the population of two large cities, New York City and Chicago.</p>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>In our experiments, RIDDLE yielded favorable classification performance with class-specific AUC values of above 0.8. Although, RIDDLE uses a fairly simple deep neural network architecture, RIDDLE displayed significantly better classification performance across all tested metrics compared to the popular classification methods logistic regression, random forest and GBDTs. RIDDLE maintained a robust (and significant) classification performance advantage over competitors in experiments simulating missing data. In other experiments, the use of pre-trained bagged embeddings were not helpful to RIDDLE (see Table H in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>).</p>
<p>RIDDLE’s superior accuracy and loss results suggest that RIDDLE produces more accurate probability estimates for race and ethnicity classes compared to currently used techniques. Although results could not be obtained for SVMs due to unacceptably high computational costs, RIDDLE significantly outperformed SVMs in runtime efficiency and classification performance on smaller subsets of the full dataset (see Table E in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>).</p>
<p>Furthermore, RIDDLE, without the use of a GPU, displayed runtimes comparable to those of traditional classification techniques. With these findings, we argue that deep-learning-driven imputation offers notable utility for race and ethnicity imputation in anonymized EMR datasets. Our current work simulated conditions where ethnicity was missing completely at random. Future work will involve simulating conditions where race and ethnicity are missing at random or missing not at random, and formalizing a multiple imputation framework involving deep-learning estimators.</p>
<p>However, these results also highlight a growing privacy concern. It has been shown that the application of machine learning poses non-trivial privacy risks, as sensitive information can be recovered from non-sensitive features [<xref ref-type="bibr" rid="pcbi.1006106.ref021">21</xref>]. Our results underscore the need for further anonymization in clinical datasets where race and ethnicity are private information; simple exclusion is not sufficient.</p>
<p>In addition to assessing the predictive and computational performance of our imputation framework, we made efforts to analyze how specific features contribute to race and ethnicity imputations in our neural network model. Each individual feature may represent only a weak trend, but together numerous indicators can synergize to provide a compelling evidence of how a person’s lifestyle, her social circles, and even genetic background can vary by race and ethnicity.</p>
<p>The aforementioned highlights of race- and ethnicity-influenced patterns of health diversity and disparity (see the <xref ref-type="sec" rid="sec004">Results</xref>) can be extended to thousands of codes (please see <xref ref-type="supplementary-material" rid="pcbi.1006106.s002">S1 Data</xref> for the complete table of features and corresponding annotations). To the best of our knowledge, this systematic comparison across all classes of maladies with respect to race and ethnicity is done for the first time in our study.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec009">
<title>Ethics statement</title>
<p>Our study used de-identified, independently collected patient data, and was determined by the Internal Review Board (IRB) of the University of Chicago to be exempt from further IRB review, under the Federal Regulations category 45 CFR 46.101(b).</p>
</sec>
<sec id="sec010">
<title>Data</title>
<p>We used an anonymized EMR datasets jointly comprising 1,650,000 individual medical histories from the New York City (Columbia University) and Chicago metropolitan populations (University of Chicago). Medical histories are encoded as variable length lists of ICD9 codes (approximately 15,000 unique codes) coupled with onset ages in years. Each individual belongs to one of four mutually exclusive classes of race (Other, White, Black) or ethnicity (Hispanic). Features included quinary gender (male, female, trans, other, unknown), and reported age in years. Age was quantized into discrete categories by integer values.</p>
<p>Onset age information of each ICD9 code was removed and continuous age information was coerced into discrete integer categories. Features were vectorized in a binary encoding scheme, where each individual is represented by a binary vector of zeros (feature absent) and ones (feature present). Each element in the binary encoded vector corresponds to an input node in the trained neural network (see <xref ref-type="fig" rid="pcbi.1006106.g001">Fig 1</xref>).</p>
<p><italic>k</italic>-fold cross-validation (<italic>k</italic> = 10) and random shuffling were used to produce ten complementary subsets of training and testing data, corresponding to ten classification experiments; this allowed for test coverage of the entire dataset. From the training set, approximately 10% of samples were used as holdout validation data for parameter tuning and performance monitoring. Testing data was held out separately and was only used during the evaluation process.</p>
</sec>
<sec id="sec011">
<title>A deep learning approach</title>
<p>We used Keras [<xref ref-type="bibr" rid="pcbi.1006106.ref022">22</xref>] with a TensorFlow backend [<xref ref-type="bibr" rid="pcbi.1006106.ref023">23</xref>] to train a deep multilayer perceptron (MLP). Neural network architectures and hyperparameters were selected using randomized grid search on 10,000 samples from the validation data. It has been reported that randomized grid search requires far less computational effort than exhaustive grid search with only slightly worse performance [<xref ref-type="bibr" rid="pcbi.1006106.ref024">24</xref>]. The final neural network hyperparameters are detailed in Table A in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>.</p>
<p>The structural architecture of the neural network was fixed across different k-fold partitions prior to training. The neural network was composed of an input layer of 15,122 nodes, two hidden layers of 512 nodes each, and a softmax output layer of four nodes (see <xref ref-type="fig" rid="pcbi.1006106.g001">Fig 1</xref>).</p>
<p>Dropout regularization was applied to each hidden layer with a dropout rate ranging from 0.2–0.8. Dropout regularizes the neural network by randomly dropping neurons and their connections during training; this limits complex co-adaptations between neurons which may not generalize well outside of the training data [<xref ref-type="bibr" rid="pcbi.1006106.ref025">25</xref>].</p>
<p>For its nodes, our neural network architecture utilizes either Parametric Rectifier Linear Units (PReLUs) [<xref ref-type="bibr" rid="pcbi.1006106.ref026">26</xref>] or Rectified Linear Units (ReLUs); the choice of which activation to use was determined during hyperparameter tuning.</p>
<p>PReLUs are variants of rectifier functions:
<disp-formula id="pcbi.1006106.e002"><alternatives><graphic id="pcbi.1006106.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006106.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>x</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>x</mml:mi> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>x</mml:mi> <mml:mo>≤</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/> <mml:mspace width="1.em"/><mml:mspace width="1.em"/><mml:mtext>where</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>α</mml:mi> <mml:mspace width="4.pt"/><mml:mtext>is</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>a</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>learned</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>parameter.</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>x</italic> is the input, and <italic>f</italic>(<italic>x</italic>) is the output of the PReLU node. ReLUs are simply PReLUs with the coeficient parameter fixed at <italic>α</italic> = 0.</p>
<p>The MLP was trained iteratively using the <italic>Adam</italic> optimizer [<xref ref-type="bibr" rid="pcbi.1006106.ref027">27</xref>]. The learning rate, which controls the magnitude of updates during gradient descent, was tuned via randomized grid search. Training was performed in a batch-wise fashion; data vectorization (via binary encoding) was also done batch-wise in coordination with training. The large number of samples (1.65M) and attention to scalability necessitated “on the fly” vectorization. The number of training epochs (passes over the data) was determined by early stopping and model caching [<xref ref-type="bibr" rid="pcbi.1006106.ref024">24</xref>], where the model from the epoch with minimal validation loss was selected. In order to encourage exploration beyond local minima, a number of epochs with poorer validation loss was permitted in accordance to a fixed patience parameter.</p>
<p>Categorical cross-entropy was chosen as the loss function; categorical cross-entropy penalizes the assignment of lower probability on the correct class and the assignment of non-zero probability to incorrect classes.</p>
</sec>
<sec id="sec012">
<title>Other machine learning approaches</title>
<p>We evaluated several other machine learning approaches: logistic regression, random forest classifier, gradient-boosted decision trees (GBDTs), and support vector machines (SVMs) with various kernels (linear, polynomial, radial basis function). Traditionally, logistic regression has been used for categorical imputation tasks [<xref ref-type="bibr" rid="pcbi.1006106.ref008">8</xref>]. We used fast Cython (C compiled from Python) or array implementations of these methods (with the exception of GBDTs) offered in the popular ‘scikit-learn’ library. For the GBDT methods, we used a Python wrapper of the popular XGBoost C library [<xref ref-type="bibr" rid="pcbi.1006106.ref028">28</xref>].</p>
<p>To handle the multiclass ethnicity imputation problem, we used a one-vs-one implementation of SVMs and a one-vs-all implementation of GBDTs. The implementations of logistic regression and random forest are inherently multiclass. Model hyperparameters were tuned in the same fashion (randomized grid search) as for the deep neural networks. The final hyperparameters are detailed in Tables B-D in <xref ref-type="supplementary-material" rid="pcbi.1006106.s001">S1 Supplement</xref>.</p>
</sec>
<sec id="sec013">
<title>Missing data simulation</title>
<p>In order to replicate real-world scenarios where additional information (other than race and ethnicity) may be absent, we conducted simulation experiments where we randomly removed some proportion of feature data (10%, 20%, or 30%). The number of input features was kept the same as feature observations at the sample level were removed; entire features were not removed.</p>
<p>For example, if 500 patient samples exhibited ICD9 code 401.9 (hypertension NOS) in the training data, we removed, with some fixed probability, the observation of ICD9 code 401.9 for <italic>each</italic> of the 500 individuals. The entire ICD9 code 401.9 feature was not removed—only sample observations of this feature.</p>
<p>We conducted training and testing pipelines with these new “deficient” datasets in the same fashion as before, using ten train/test partitions of the data given by k-fold cross-validation.</p>
<p>The code used to conduct all experiments is available on GitHub (see <xref ref-type="supplementary-material" rid="pcbi.1006106.s003">S1 Code</xref>).</p>
</sec>
<sec id="sec014">
<title>Evaluation</title>
<p>We computed standard accuracy, cross-entropy loss, precision, and recall scores for testing data across all ten experiments. We also computed class-specific ROC AUC scores as well as micro-average and macro-average ROC AUC metrics. Class-specific ROC AUC scores refer to the ROC AUC scores computed by binarizing the classification problem to a specific class. The micro-average ROC AUC score was computed by reducing all multiclass classification problems to binary prediction problems (true class vs. other classes). The macro-average ROC AUC score was calculated by averaging all class-specific ROC scores, weighted by the number of cases in each class.</p>
<p>In addition to evaluating classification performance, we also monitored runtime performance across methods. Models were trained on a standard computing configuration on the Midway compute cluster at the University of Chicago: 16 Intel Sandybridge cores at 2.6 GHz, and 32GB RAM.</p>
<p>Significant differences in performance scores were detected using paired t-tests with Bonferroni adjustment.</p>
</sec>
<sec id="sec015">
<title>Neural network interpretation</title>
<p>We computed DeepLIFT scores to interpret how certain features contribute to probability estimates for each class [<xref ref-type="bibr" rid="pcbi.1006106.ref010">10</xref>]. The DeepLIFT algorithm takes a trained neural network and produces feature-to-class contribution scores for each passed sample.</p>
<p>DeepLIFT scores describe how differences in values for some input neuron (compared to a reference value) result in differences in output neuron values (compared to a reference value). The DeepLIFT interpretation method relies on a central summation-to-delta property:
<disp-formula id="pcbi.1006106.e003"><alternatives><graphic id="pcbi.1006106.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006106.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where Δ<italic>t</italic> is the difference-from-reference value for an output neuron. <italic>C</italic><sub>Δ<italic>x</italic><sub><italic>i</italic></sub>, Δ<italic>t</italic></sub> is the difference-from-reference value for the output neuron which can be attributed to differences-from-reference value for a neuron <italic>x</italic><sub><italic>i</italic></sub> which is necessary to compute the output neuron; this also serves as the DeepLIFT score. Although DeepLIFT does not use gradient information, DeepLIFT scores are computed using a backpropagation-like algorithm which uses a chaining principle analogous to the chain rule. Unlike gradient-based approaches, DeepLIFT scores can be meaningful and non-zero even when the gradient is zero [<xref ref-type="bibr" rid="pcbi.1006106.ref010">10</xref>].</p>
<p>To compute DeepLIFT scores for the RIDDLE neural networks, we assumed reference values of zeros for all input neurons because our training features were binary and sparse; furthermore, a value of zero for an input feature naturally indicates the absence of a disease state. Alternatively, population statistics for disease incidences could have been used as reference values. Reference values for the hidden layers were obtained by performing a forward pass using input values of zero (the input reference values).</p>
<p>We computed DeepLIFT scores using separate test samples and models from each of our k-fold cross validation experiments to achieve full coverage of the dataset. Scores were summed across experiments for aggregation purposes. To describe high-level relationships between features and classes, we summed scores across all samples to produce an aggregate score. The aggregate DeepLIFT scores for the ten most predictive features are summarized in <xref ref-type="table" rid="pcbi.1006106.t004">Table 4</xref>.</p>
<p>As described prior, we computed orderings of race and ethnicity classes with each feature’s DeepLIFT scores. These orderings describe how certain features (e.g., medical conditions) can predict for or against a particular race and ethnicity class. We visualize the orderings defined by DeepLIFT scores for the twenty-five most common features in <xref ref-type="fig" rid="pcbi.1006106.g003">Fig 3</xref>, and compare them to the orderings produced from sorting classes by the total number of feature observations within the class. We visualized the orderings of the 25 most frequently observed features in the dataset in <xref ref-type="fig" rid="pcbi.1006106.g003">Fig 3</xref>. For the visualizations, frequency counts were mean-centered to facilitate comparison to DeepLIFT scores.</p>
</sec>
</sec>
<sec id="sec016">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006106.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.s001" xlink:type="simple">
<label>S1 Supplement</label>
<caption>
<title>Supporting tables are provided in the attached document <monospace>supplement.pdf</monospace>.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006106.s002" mimetype="text/tab-separated-values" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.s002" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>The full list of orderings constructed from DeepLIFT scores is available in the file provided.</title>
<p>(TSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006106.s003" mimetype="text/plain" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006106.s003" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>Our implementation of RIDDLE is available as an open-source Python library, <monospace>riddle</monospace>.</title>
<p>The code is hosted on <ext-link ext-link-type="uri" xlink:href="https://www.github.com/jisungk/riddle" xlink:type="simple">GitHub</ext-link>, and documentation is available at <ext-link ext-link-type="uri" xlink:href="https://riddle.ai/" xlink:type="simple">riddle.ai</ext-link>.</p>
<p>(TXT)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We are grateful to Rita Rzhetsky for comments on earlier version of the manuscript, and to Drs. Raul Rabadan and Rachel Melamed for preparing the Columbia University dataset.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006106.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jensen</surname> <given-names>PB</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Brunak</surname> <given-names>S</given-names></name>. <article-title>Mining electronic health records: towards better research applications and clinical care</article-title>. <source>Nature Reviews Genetics</source>. <year>2012</year>;<volume>13</volume>(<issue>6</issue>):<fpage>395</fpage>–<lpage>405</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrg3208" xlink:type="simple">10.1038/nrg3208</ext-link></comment> <object-id pub-id-type="pmid">22549152</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schulman</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Berlin</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Harless</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kerner</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Sistrunk</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gersh</surname> <given-names>BJ</given-names></name>, <etal>et al</etal>. <article-title>The effect of race and sex on physicians’ recommendations for cardiac catheterization</article-title>. <source>New England Journal of Medicine</source>. <year>1999</year>;<volume>340</volume>(<issue>8</issue>):<fpage>618</fpage>–<lpage>626</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1056/NEJM199902253400806" xlink:type="simple">10.1056/NEJM199902253400806</ext-link></comment> <object-id pub-id-type="pmid">10029647</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Burchard</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Ziv</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Pérez-Stable</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Sheppard</surname> <given-names>D</given-names></name>. <article-title>The importance of race and ethnic background in biomedical research and clinical practice</article-title>. <source>The New England Journal of Medicine</source>. <year>2003</year>;<volume>348</volume>(<issue>12</issue>):<fpage>1170</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1056/NEJMsb025007" xlink:type="simple">10.1056/NEJMsb025007</ext-link></comment> <object-id pub-id-type="pmid">12646676</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sterne</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>White</surname> <given-names>IR</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Spratt</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Royston</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kenward</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls</article-title>. <source>BMJ</source>. <year>2009</year>;<volume>338</volume>:<fpage>b2393</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1136/bmj.b2393" xlink:type="simple">10.1136/bmj.b2393</ext-link></comment> <object-id pub-id-type="pmid">19564179</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref005">
<label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">WHO; 2010. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.who.int/classifications/icd/en/" xlink:type="simple">http://www.who.int/classifications/icd/en/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elliott</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Fremont</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Pantoja</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lurie</surname> <given-names>N</given-names></name>. <article-title>A New Method for Estimating Race/Ethnicity and Associated Disparities Where Administrative Records Lack Self-Reported Race/Ethnicity</article-title>. <source>Health Services Research</source>. <year>2008</year>;<volume>43</volume>(<issue>5p1</issue>):<fpage>1722</fpage>–<lpage>1736</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1475-6773.2008.00854.x" xlink:type="simple">10.1111/j.1475-6773.2008.00854.x</ext-link></comment> <object-id pub-id-type="pmid">18479410</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grundmeier</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Song</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ramos</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Fiks</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Elliott</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Fremont</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Imputing missing race/ethnicity in pediatric electronic health records: reducing bias with use of US census location and surname data</article-title>. <source>Health Services Research</source>. <year>2015</year>;<volume>50</volume>(<issue>4</issue>):<fpage>946</fpage>–<lpage>960</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1475-6773.12295" xlink:type="simple">10.1111/1475-6773.12295</ext-link></comment> <object-id pub-id-type="pmid">25759144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sentas</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Angelis</surname> <given-names>L</given-names></name>. <article-title>Categorical missing data imputation for software cost estimation by multinomial logistic regression</article-title>. <source>Journal of Systems and Software</source>. <year>2006</year>;<volume>79</volume>(<issue>3</issue>):<fpage>404</fpage>–<lpage>414</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jss.2005.02.026" xlink:type="simple">10.1016/j.jss.2005.02.026</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>444</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14539" xlink:type="simple">10.1038/nature14539</ext-link></comment> <object-id pub-id-type="pmid">26017442</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref010">
<label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. arXiv preprint arXiv:170402685. 2017;.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barber</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hickson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Sims</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Diez-Roux</surname> <given-names>AV</given-names></name>. <article-title>Neighborhood Disadvantage, Poor Social Conditions, and Cardiovascular Disease Incidence Among African American Adults in the Jackson Heart Study</article-title>. <source>Am J Public Health</source>. <year>2016</year>;<volume>106</volume>(<issue>12</issue>):<fpage>2219</fpage>–<lpage>2226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2105/AJPH.2016.303471" xlink:type="simple">10.2105/AJPH.2016.303471</ext-link></comment> <object-id pub-id-type="pmid">27736207</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gilbert</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Elder</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Lyons</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kaphingst</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Blanchard</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>M</given-names></name>. <article-title>Racial Composition Over the Life Course: Examining Separate and Unequal Environments and the Risk for Heart Disease for African American Men</article-title>. <source>Ethn Dis</source>. <year>2015</year>;<volume>25</volume>(<issue>3</issue>):<fpage>295</fpage>–<lpage>304</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18865/ed.25.3.295" xlink:type="simple">10.18865/ed.25.3.295</ext-link></comment> <object-id pub-id-type="pmid">26673460</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guzman</surname> <given-names>NJ</given-names></name>. <article-title>Epidemiology and management of hypertension in the hispanic population</article-title>. <source>American Journal of Cardiovascular Drugs</source>. <year>2012</year>;<volume>12</volume>(<issue>3</issue>):<fpage>165</fpage>–<lpage>178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2165/11631520-000000000-00000" xlink:type="simple">10.2165/11631520-000000000-00000</ext-link></comment> <object-id pub-id-type="pmid">22583147</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crepaz</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Horn</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Rama</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Griffin</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Deluca</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Mullins</surname> <given-names>MM</given-names></name>, <etal>et al</etal>. <article-title>The efficacy of behavioral interventions in reducing HIV risk sex behaviors and incident sexually transmitted disease in black and Hispanic sexually transmitted disease clinic patients in the United States: a meta-analytic review</article-title>. <source>Sexually Transmitted Diseases</source>. <year>2007</year>;<volume>34</volume>(<issue>6</issue>):<fpage>319</fpage>–<lpage>332</lpage>. <object-id pub-id-type="pmid">17038965</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gillum</surname> <given-names>RF</given-names></name>, <name name-style="western"><surname>Mussolino</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Madans</surname> <given-names>JH</given-names></name>. <article-title>Diabetes mellitus, coronary heart disease incidence, and death from all causes in African American and European American women: The NHANES I epidemiologic follow-up study</article-title>. <source>J Clin Epidemiol</source>. <year>2000</year>;<volume>53</volume>(<issue>5</issue>):<fpage>511</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0895-4356(99)00208-5" xlink:type="simple">10.1016/S0895-4356(99)00208-5</ext-link></comment> <object-id pub-id-type="pmid">10812324</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kinsler</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Sayles</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Newman</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Diamant</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>W</given-names></name>. <article-title>The impact of acculturation on utilization of HIV prevention services and access to care among an at-risk Hispanic population</article-title>. <source>Journal of Health Care for the Poor and Underserved</source>. <year>2009</year>;<volume>20</volume>(<issue>4</issue>):<fpage>996</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1353/hpu.0.0204" xlink:type="simple">10.1353/hpu.0.0204</ext-link></comment> <object-id pub-id-type="pmid">20168013</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ojodu</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hulihan</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Pope</surname> <given-names>SN</given-names></name>, <name name-style="western"><surname>Grant</surname> <given-names>AM</given-names></name>, <collab>Centers for Disease C, Prevention</collab>. <article-title>Incidence of sickle cell trait–United States, 2010</article-title>. <source>MMWR Morb Mortal Wkly Rep</source>. <year>2014</year>;<volume>63</volume>(<issue>49</issue>):<fpage>1155</fpage>–<lpage>8</lpage>. <object-id pub-id-type="pmid">25503918</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fix</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Peña</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Strickland</surname> <given-names>GT</given-names></name>. <article-title>Racial differences in reported Lyme disease incidence</article-title>. <source>American Journal of Epidemiology</source>. <year>2000</year>;<volume>152</volume>(<issue>8</issue>):<fpage>756</fpage>–<lpage>759</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/aje/152.8.756" xlink:type="simple">10.1093/aje/152.8.756</ext-link></comment> <object-id pub-id-type="pmid">11052554</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Olshansky</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Antonucci</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Berkman</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Binstock</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Boersch-Supan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cacioppo</surname> <given-names>JT</given-names></name>, <etal>et al</etal>. <article-title>Differences in life expectancy due to race and educational differences are widening, and many may not catch up</article-title>. <source>Health Affairs</source>. <year>2012</year>;<volume>31</volume>(<issue>8</issue>):<fpage>1803</fpage>–<lpage>1813</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1377/hlthaff.2011.0746" xlink:type="simple">10.1377/hlthaff.2011.0746</ext-link></comment> <object-id pub-id-type="pmid">22869659</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Crimmins</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Saito</surname> <given-names>Y</given-names></name>. <article-title>Trends in healthy life expectancy in the United States, 1970–1990: gender, racial, and educational differences</article-title>. <source>Social science &amp; medicine</source>. <year>2001</year>;<volume>52</volume>(<issue>11</issue>):<fpage>1629</fpage>–<lpage>1641</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0277-9536(00)00273-2" xlink:type="simple">10.1016/S0277-9536(00)00273-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006106.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Calandrino JA, Kilzer A, Narayanan A, Felten EW, Shmatikov V. “You Might Also Like:” Privacy Risks of Collaborative Filtering. In: Security and Privacy (SP), 2011 IEEE Symposium on. IEEE; 2011. p. 231–246.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Chollet F. Keras; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="https://keras.io/" xlink:type="simple">https://keras.io/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:160304467. 2016;.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref024">
<label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <chapter-title>Practical recommendations for gradient-based training of deep architectures</chapter-title>. In: <source>Neural Networks: Tricks of the Trade</source>. <publisher-name>Springer</publisher-name>; <year>2012</year>. p. <fpage>437</fpage>–<lpage>478</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Srivastava</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Krizhevsky</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proceedings of the IEEE international conference on computer vision; 2015. p. 1026–1034.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Kingma D, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980. 2014;.</mixed-citation>
</ref>
<ref id="pcbi.1006106.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen T, Guestrin C. XGBoost: A Scalable Tree Boosting System. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD’16. New York, NY, USA: ACM; 2016. p. 785–794. Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/2939672.2939785" xlink:type="simple">http://doi.acm.org/10.1145/2939672.2939785</ext-link>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>