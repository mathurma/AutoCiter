<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="other" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosbiol</journal-id>
<journal-title-group>
<journal-title>PLOS Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1544-9173</issn>
<issn pub-type="epub">1545-7885</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pbio.2005343</article-id>
<article-id pub-id-type="publisher-id">pbio.2005343</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Community Page</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject><subj-group><subject>Information retrieval</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Geographical locations</subject><subj-group><subject>North America</subject><subj-group><subject>United States</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Ranking algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Ranking algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computer networks</subject><subj-group><subject>Internet</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Microbiology</subject><subj-group><subject>Medical microbiology</subject><subj-group><subject>Microbial pathogens</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Immunodeficiency viruses</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pathology and laboratory medicine</subject><subj-group><subject>Pathogens</subject><subj-group><subject>Microbial pathogens</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Immunodeficiency viruses</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Viruses</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Immunodeficiency viruses</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Viruses</subject><subj-group><subject>Immunodeficiency viruses</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Viruses</subject><subj-group><subject>RNA viruses</subject><subj-group><subject>Retroviruses</subject><subj-group><subject>Lentivirus</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Microbiology</subject><subj-group><subject>Medical microbiology</subject><subj-group><subject>Microbial pathogens</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Retroviruses</subject><subj-group><subject>Lentivirus</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pathology and laboratory medicine</subject><subj-group><subject>Pathogens</subject><subj-group><subject>Microbial pathogens</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Retroviruses</subject><subj-group><subject>Lentivirus</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Viruses</subject><subj-group><subject>Viral pathogens</subject><subj-group><subject>Retroviruses</subject><subj-group><subject>Lentivirus</subject><subj-group><subject>HIV</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Vascular medicine</subject><subj-group><subject>Thromboembolism</subject><subj-group><subject>Venous thromboembolism</subject><subj-group><subject>Deep vein thrombosis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Best Match: New relevance search for PubMed</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9260-1326</contrib-id>
<name name-style="western">
<surname>Fiorini</surname>
<given-names>Nicolas</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Canese</surname>
<given-names>Kathi</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Starchenko</surname>
<given-names>Grisha</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kireev</surname>
<given-names>Evgeny</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kim</surname>
<given-names>Won</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Miller</surname>
<given-names>Vadim</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Osipov</surname>
<given-names>Maxim</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kholodov</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ismagilov</surname>
<given-names>Rafis</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mohan</surname>
<given-names>Sunil</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ostell</surname>
<given-names>James</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Lu</surname>
<given-names>Zhiyong</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH), Bethesda, Maryland, United States of America</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">zhiyong.lu@nih.gov</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>28</day>
<month>8</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<month>8</month>
<year>2018</year>
</pub-date>
<volume>16</volume>
<issue>8</issue>
<elocation-id>e2005343</elocation-id>
<permissions>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">
<license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">Creative Commons CC0</ext-link> public domain dedication.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pbio.2005343"/>
<abstract>
<p>PubMed is a free search engine for biomedical literature accessed by millions of users from around the world each day. With the rapid growth of biomedical literature—about two articles are added every minute on average—finding and retrieving the most relevant papers for a given query is increasingly challenging. We present Best Match, a new relevance search algorithm for PubMed that leverages the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date sort order. The Best Match algorithm is trained with past user searches with dozens of relevance-ranking signals (factors), the most important being the past usage of an article, publication date, relevance score, and type of article. This new algorithm demonstrates state-of-the-art retrieval performance in benchmarking experiments as well as an improved user experience in real-world testing (over 20% increase in user click-through rate). Since its deployment in June 2017, we have observed a significant increase (60%) in PubMed searches with relevance sort order: it now assists millions of PubMed searches each week. In this work, we hope to increase the awareness and transparency of this new relevance sort option for PubMed users, enabling them to retrieve information more effectively.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="1"/>
<page-count count="12"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>PubMed (<ext-link ext-link-type="uri" xlink:href="http://www.pubmed.gov/" xlink:type="simple">www.pubmed.gov</ext-link>) is a widely used search engine, built and maintained by the United States National Center for Biotechnology Information (NCBI) at the US National Library of Medicine (NLM), that provides access to more than 28 million scholarly publications in biomedicine. On an average working day, there are about 2.5 million PubMed users conducting 3 million searches and 9 million page views. Every article and its associated data elements (also known as Fields, such as title, abstract, author names, and author affiliations; see <xref ref-type="supplementary-material" rid="pbio.2005343.s001">S1 Glossary</xref> for definitions and abbreviations) must be first built into the search index of PubMed before users can search. Then, at query time, PubMed employs all the terms specified in the search to find matches in all possible fields. Next, by default, all matching articles will be returned in reverse chronological order. That is, newly published articles are always returned first. While this sort order is desirable for seeking the latest information on a given topic or for an individual author, it may not be ideal for other types of searches (e.g., new topics) or deliver the most relevant articles to our users most efficiently, as irrelevant results can be returned at the top due to query ambiguity and complexity. E.g., if a search intent was to find articles studying a given disease in a certain geographic area or ethnic group (e.g., "melioidosis Taiwan"), then top results matching the location term in the author affiliation field (instead of treating it as a content keyword) would be unsatisfactory. Inability to locate semantic concepts in relative proximity can also result in suboptimal results [<xref ref-type="bibr" rid="pbio.2005343.ref001">1</xref>]. The query "cancer related fatigue," for instance, returns many seemingly irrelevant articles on the first page when sorted by publication date.</p>
<p>We have previously observed that over 80% of the user clicks of search results happened on the first page. This user behavior [<xref ref-type="bibr" rid="pbio.2005343.ref002">2</xref>] is highly similar to that of general web searches despite the very different date sort order used in PubMed. Thus, for the majority of the PubMed queries for which there are over 20 results, more useful and often still recent papers on page two and beyond could be easily missed by users.</p>
<p>In response, in 2013, a relevance sort option was made available in PubMed that implemented term frequency–inverse document frequency (TF–IDF) weighting, a classic information retrieval (IR) strategy for computing query-document relevancy [<xref ref-type="bibr" rid="pbio.2005343.ref003">3</xref>] based on how many search terms are found, in which fields they are found, and the frequency of the term across all documents. Additionally, recently published articles are given an artificial boost for sorting. For databases other than PubMed, alternative IR methods such as BM25 [<xref ref-type="bibr" rid="pbio.2005343.ref004">4</xref>] and variations of the classic TF–IDF algorithm have been studied and applied elsewhere [<xref ref-type="bibr" rid="pbio.2005343.ref005">5</xref>–<xref ref-type="bibr" rid="pbio.2005343.ref009">9</xref>].</p>
<p>While the classic TF–IDF method shows good performance for relevance ranking, all of its parameters (e.g., recency boost factor) are based on manual experiments or analyses. Often, with this approach, parameters are tuned empirically and/or based on domain knowledge. Recent studies have shown that one can build more robust ranking models trained on large-scale datasets by using machine-learning algorithms [<xref ref-type="bibr" rid="pbio.2005343.ref010">10</xref>]. Particularly, learning to rank (L2R), a class of machine-learning algorithms for ranking problems, have emerged since the late 2000s and shown significant improvements in retrieval quality over traditional relevance models by taking advantage of big datasets [<xref ref-type="bibr" rid="pbio.2005343.ref011">11</xref>]. With a pretrained L2R model, a relevance score is assigned for each matching document given a query, with more relevant documents receiving a higher score. Because of their superior performance, these L2R algorithms have also been recently applied to many other tasks in biomedical research [<xref ref-type="bibr" rid="pbio.2005343.ref012">12</xref>–<xref ref-type="bibr" rid="pbio.2005343.ref016">16</xref>].</p>
<p>While there are a number of research studies on L2R, few have explored its applicability and feasibility as an end-to-end system for real-world use in biomedicine [<xref ref-type="bibr" rid="pbio.2005343.ref017">17</xref>]. Furthermore, although machine-learning or L2R methods have been implemented in large-scale commercial search systems [<xref ref-type="bibr" rid="pbio.2005343.ref018">18</xref>], because of proprietary information, little has been published regarding its scalability and overall performance with real users.</p>
<p>To this end, we describe the use of L2R to create a new relevance search algorithm for PubMed search, the first of its kind in (biomedical) scientific literature retrieval to the best of our knowledge. For validating our method, we present both the offline evaluation results (computer-ranked results against a gold standard) as well as the online results when tested with real PubMed users (measured by user click-through rate—CTR). Finally, to demonstrate its utility, we report its usage rate since its full deployment in PubMed in June 2017 with a focus on when and how to use it in practice. In doing so, we hope to increase the transparency of this new relevance sort option (labeled as Best Match in PubMed) for our users such that they can better understand and ultimately search more effectively in PubMed. The technical details (in Supporting Information) may also be beneficial to those who are interested in implementing such a method in production systems. The research source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ncbi-nlp/PubMed-Best-Match" xlink:type="simple">https://github.com/ncbi-nlp/PubMed-Best-Match</ext-link>.</p>
</sec>
<sec id="sec002">
<title>Tool description</title>
<sec id="sec003">
<title>Two-stage ranking architecture with improved performance</title>
<p>For PubMed's Best Match, we adopted a two-stage ranking architecture—in which the two separate steps, retrieval and reordering, can be optimized independently—for using L2R [<xref ref-type="bibr" rid="pbio.2005343.ref019">19</xref>,<xref ref-type="bibr" rid="pbio.2005343.ref020">20</xref>], as it provides both efficiency and flexibility. As shown in <xref ref-type="fig" rid="pbio.2005343.g001">Fig 1A</xref>, (1) given a user query translated and mapped to fields automatically, PubMed first retrieves documents that match it and orders them with a classical term weighting function, BM25 (see <xref ref-type="supplementary-material" rid="pbio.2005343.s002">S1 Text</xref>). (2) The top-ranked documents are further sorted by a second ranker called LambdaMART [<xref ref-type="bibr" rid="pbio.2005343.ref021">21</xref>] (see <xref ref-type="supplementary-material" rid="pbio.2005343.s003">S2 Text</xref>), which stands out as a robust and fast approach with superlative performance in various ranking tasks (e.g., the 2011 L2R challenge [<xref ref-type="bibr" rid="pbio.2005343.ref022">22</xref>] or various TREC tasks [<xref ref-type="bibr" rid="pbio.2005343.ref023">23</xref>]). Note that the first layer is very similar to the previous relevance system used in PubMed starting in 2013. The main novelty is thus the addition of the second, machine-learning–based layer.</p>
<fig id="pbio.2005343.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2005343.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The overall architecture of the new relevance search algorithm in PubMed.</title>
<p>(a) It consists of two stages: processing first by BM25, a classic term-weighting algorithm; the top 500 results are then re-ranked by LambdaMART, a high-performance L2R algorithm. The machine-learning–based ranking model is learned offline using relevance-ranked training data together with a set of features extracted from queries, documents, or both. (b) Features designed and experimented in this study with their brief descriptions and identifiers. D, document; IDF, inverse document frequency; L2R, learning to rank; Q, query; QD, query–document relationship; TIAB, title and abstract</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.g001" xlink:type="simple"/>
</fig>
<p>In order to train LambdaMART and test its effectiveness, a set of gold-standard query-document pairs is required. Given the lack of real-world datasets for biomedical information retrieval, we used the user-click information from PubMed search logs (see <xref ref-type="supplementary-material" rid="pbio.2005343.s004">S3 Text</xref>) as the (pseudo-)gold standard for document relevance and created a benchmark dataset, which contains 46,000 unique queries in total (see <xref ref-type="supplementary-material" rid="pbio.2005343.s005">S4 Text</xref>). A random split of 70% was used for training the LambdaMART algorithm. When evaluated on the held-out test data (the remaining 30%) using the Normalized Discounted Cumulative Gain (NDCG), a standard measure for ranking quality (see <xref ref-type="supplementary-material" rid="pbio.2005343.s006">S5 Text</xref>), our results show that the second ranker is able to learn from the "ground truth" and obtain more than 3-fold increases in ranking quality when compared with the previous TF–IDF method (0.48 versus 0.15 in NDCG scores) (see <xref ref-type="supplementary-material" rid="pbio.2005343.s007">S6 Text</xref>).</p>
</sec>
<sec id="sec004">
<title>Document ranking features and their impact on performance</title>
<p>Besides labeled data, another prerequisite for training machine-learning algorithms is transforming each data instance into feature representations. Hence, for distinguishing relevant versus irrelevant articles, we designed a set of distinctive features ("ranking factors/signals") that aim to capture the various characteristics of a document D (e.g., publication year or type), the relationship between a query and document QD (e.g., number of query term matches in title), or the specifications of the query Q (e.g., query length). See <xref ref-type="fig" rid="pbio.2005343.g001">Fig 1B</xref> for a complete list and how they are encoded in <xref ref-type="supplementary-material" rid="pbio.2005343.s008">S7 Text</xref>. Document features are used to represent the inherent nature of documents irrespective of the query. Specifically, we characterize a document in multiple dimensions such as its publication time, publication type, past usage, etc. We use publication year, as we know recency is a critical factor in finding and reading scholarly articles. Similarly, the type of publication can also be important (e.g., review articles are generally desired in a literature survey process). The past usage of an article can be seen as an approximation for assessing its popularity among users. Finally, we also include features such as document length and language for a fuller description of a document.</p>
<p>Query–document features intend to capture to what degree a document is related to the query. For instance, the BM25 score is used as a feature to capture this relationship. We also take into count the number of term matches in specific fields (e.g., title), as well as text proximity—how close the matches are to each other in the document. The latter is used to favor documents in which matched term positions are grouped together rather than scattered over the document. Specifically, we followed the lead of [<xref ref-type="bibr" rid="pbio.2005343.ref024">24</xref>] and used 19 features to represent this (e.g., count of words between query terms).</p>
<p>The third group regards queries only, ranging from its length (the number of search terms) to the count of special characters (e.g., those in chemical names) to the number of returned results (as a measure for whether it relates to a broad versus narrow topic).</p>
<p>To assess the importance of each feature (group) towards the overall performance, we conducted feature-ablation studies in which we recorded performance loss when individual (or groups of) features were removed. We find that the D features (especially publication year and past usage) and QD features (especially BM25 relevance score) are the most critical and complementary to each other. Although Q features have a relatively minor effect, they can also contribute to improve the overall ranking quality (see <xref ref-type="supplementary-material" rid="pbio.2005343.s009">S8 Text</xref>).</p>
</sec>
<sec id="sec005">
<title>Improved search experience in online evaluation with real users</title>
<p>Given the benchmarking results and feature analysis, we proceeded with a widely used web analytics method called A/B testing [<xref ref-type="bibr" rid="pbio.2005343.ref025">25</xref>], which compares two or more variations of a feature with real users in a controlled experiment. In our case, for all queries for which users selected relevance sort order, we routed 25% of them to the newly proposed Best Match algorithm while keeping the rest of the queries (75% of total) with the original TF–IDF algorithm. We then compared the CTRs, the fraction of queries with at least one user click on the top-ranked results (see <xref ref-type="supplementary-material" rid="pbio.2005343.s006">S5 Text</xref>). Note that queries for which PubMed returned zero or a single article were excluded from this experiment, as they were not applicable (no click was needed). In addition to focusing on the rank of 20 (the default number of returned results in the first page), we compared CTR@10, CTR@5, and CTR@3 to get a sense of the improvement at top-ranked results. Also, for comparison, we included the results using the default date sort option. This experiment ran from March 1st, 2017, to June 8th, 2017, consisting of 133,822,362 searches by date, 7,527,507 searches routed to TF–IDF, and 2,509,169 searches routed to Best Match.</p>
<p>As shown in <xref ref-type="table" rid="pbio.2005343.t001">Table 1</xref>, the new Best Match algorithm performs significantly better than both the default date sort as well as the previous relevance search algorithm at every rank position. Furthermore, relative improvements in CTRs increase steadily as the rank threshold decreases (e.g., 40% improvement for CTR@3 versus 22% for CTR@20 in comparison with date sort results), demonstrating that Best Match is especially better at optimizing the top-ranked results. We also observed that the increase in CTR is applicable to a wide variety of different queries. That is, both popular and infrequent queries benefit from the new Best Match algorithm (see details in <xref ref-type="supplementary-material" rid="pbio.2005343.s011">S1 Fig</xref>). For instance, over 87% of PubMed queries are unique, and they have an average CTR@20 of 0.408—see the GitHub repository for more details.</p>
<table-wrap id="pbio.2005343.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2005343.t001</object-id>
<label>Table 1</label> <caption><title>Comparison of the user click-through rate of best match versus the previous TF–IDF method and the default date sort order.</title></caption>
<alternatives>
<graphic id="pbio.2005343.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Ranking Method</th>
<th align="center">CTR@20</th>
<th align="center">CTR@10</th>
<th align="center">CTR@5</th>
<th align="center">CTR@3</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify">Sort by date</td>
<td align="center">0.32</td>
<td align="left">0.29</td>
<td align="left">0.24</td>
<td align="left">0.20</td>
</tr>
<tr>
<td align="justify">Sort by TF–IDF</td>
<td align="center">0.36</td>
<td align="left">0.33</td>
<td align="left">0.29</td>
<td align="left">0.25</td>
</tr>
<tr>
<td align="justify">Sort by Best Match</td>
<td align="center"><bold>0.39</bold></td>
<td align="left"><bold>0.36</bold></td>
<td align="left"><bold>0.32</bold></td>
<td align="left"><bold>0.28</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>All improvements in CTRs by Best Match are statistically significant with 99% confidence (paired <italic>t</italic> test). <bold>Abbreviations:</bold> CTR, click-through rate; TF–IDF, term frequency–inverse document frequency.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Note that while the absolute increase in CTRs may seem modest, a relative improvement of 1–2% in CTRs in real-world settings (e.g., online ads seen in web search results) is typically considered successful [<xref ref-type="bibr" rid="pbio.2005343.ref026">26</xref>,<xref ref-type="bibr" rid="pbio.2005343.ref027">27</xref>]. We also noticed that algorithmic improvements in NDCG scores can translate into more modest real-world improvements in CTR scores. We believe this is due to the fact that search quality is just one of the factors affecting CTRs. E.g., a system that highlights matching terms or returns with snippets (highlights from the article that are related to the user query) would usually have a higher CTR compared to the same results without such visual cues.</p>
</sec>
<sec id="sec006">
<title>Increased usage of relevance search in PubMed</title>
<p>Given the significant increase in performance of the new Best Match algorithm over the previous method, we deployed the new algorithm to production in June 2017. To further promote the update, a Best Match banner was developed as shown in <xref ref-type="fig" rid="pbio.2005343.g002">Fig 2</xref>. Through log analysis during December 2017, we find that the Best Match banners are clicked 1 out of 10 times when displayed, with a much higher chance of follow-up document clicks: CTR@20 of 52% for over 100,000 queries re-run under Best Match after switching the sort order. This is markedly higher than the usual CTR of 39% shown in <xref ref-type="table" rid="pbio.2005343.t001">Table 1</xref>. In addition, only a very small percentage (2.5%) of users chose to switch back to the date sort order.</p>
<fig id="pbio.2005343.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2005343.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The Best Match search option in action.</title>
<p>When our system detects that search results by Best Match could be helpful to our users, a Best Match banner is displayed on top of the regular search results (a). A user can click title(s) to view the article abstract (as shown in (b)) or click on the Switch button see complete results returned by Best Match (as shown in (c)).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.g002" xlink:type="simple"/>
</fig>
<p>We have observed that the CTRs of relevance search using the new Best Match algorithm have continued to increase since June. Moreover, there is a rapid growth in the overall usage of the relevance sort option. As shown in <xref ref-type="fig" rid="pbio.2005343.g003">Fig 3</xref>, usage of the relevance sort is steadily increasing with a faster increase since Best Match has been deployed. From June 2017 to April 2018, the overall usage of relevance search has increased from 7.5% to 12% (a 60% increase) of all PubMed queries.</p>
<fig id="pbio.2005343.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pbio.2005343.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Usage rate of relevance sort order over 6 months (May 2017 to October 2017).</title>
<p>The blue line represents the trend, and the blue area represents the variance. The vertical line denotes the switch to the new relevance algorithm, Best Match, which is followed by a significant and steady increase in usage. Note that the 1% usage rate on the <italic>y</italic>-axis represents about 30,000 queries on an average work day.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007">
<title>The new ranking system is highly scalable</title>
<p>The proposed system has been optimized for throughput (see <xref ref-type="supplementary-material" rid="pbio.2005343.s010">S9 Text</xref>) so that it is able to scale up and exceed the real-world throughput requirement of PubMed searches, approximately 200 queries per second. At maximum, our system is able to process approximately 700 queries per second at an average of approximately 70 ms per query as we run 100 threads in parallel.</p>
</sec>
<sec id="sec008">
<title>Best practices for using Best Match</title>
<p>Generally speaking, PubMed queries can be categorized in two broad classes: navigational versus informational. Navigational searches, also referred to as known-item searches, are ones in which the search intent is to find a specific article or set of articles (e.g., a search with an article title or author name). On the other hand, informational searches seek to find and/or explore articles satisfying information needs on a given topic (e.g., using a query like "HIV DVT" to gather evidence of deep vein thrombosis related to HIV). In this regard, Best Match is more appropriate for the latter use cases, for which the most relevant set of results are desired, and is therefore complementary to the traditional Most Recent sort order in PubMed.</p>
<p>As mentioned earlier, to familiarize our users with the newly developed Best Match search, a banner is displayed as shown in <xref ref-type="fig" rid="pbio.2005343.g002">Fig 2</xref> when appropriate. That is, each time a search is run under the default "Most Recent" sort order and the query is found to be informational by the Field Sensor [<xref ref-type="bibr" rid="pbio.2005343.ref028">28</xref>], the Best Match banner will be triggered. However, in order to minimize any potential disruption of usual PubMed searches, it is not triggered if the query returns less than 20 results or if other results are displayed, such as those from our spell checker. As a result of these rules, currently Best Match banners are only triggered for about 35% of the total PubMed queries, though topical searches generally account for half of total searches in PubMed.</p>
<p>Finally, as we know different information needs may be better fulfilled by different sort orders [<xref ref-type="bibr" rid="pbio.2005343.ref029">29</xref>], we have improved PubMed’s usability by making it simple for our users to choose and switch between the two sort orders. In particular, we have implemented and added a two-part toggle at the top right in the search results page, which allows users to conveniently change between the two most used search modes, "Most Recent" and "Best Match." When users switch the sort order, using this new toggle function or the traditional "Sort By" drop-down menu, it is saved automatically so that all further searches will run using the new order. Because of the recent success of "Best Match" in PubMed, this mode is now being tested as the default sort order in the newly developed PubMed Labs (<ext-link ext-link-type="uri" xlink:href="http://www.pubmed.gov/labs" xlink:type="simple">www.pubmed.gov/labs</ext-link>) system, in which search results are further accompanied with rich snippets.</p>
</sec>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Discussion</title>
<p>As mentioned, there is unfortunately no existing dataset that meets the need for a machine-learning–based retrieval system for PubMed, and it is not possible to manually curate a large-scale relevance data set. Hence, we adopted a common industry practice for assembling a gold-standard training dataset through the extraction of click-through data in search logs as pseudorelevance [<xref ref-type="bibr" rid="pbio.2005343.ref030">30</xref>–<xref ref-type="bibr" rid="pbio.2005343.ref034">34</xref>].</p>
<p>There are several known issues with this method. First, in our logs, the number of searches using relevance sort is still modest at present. Over the last year, we were able to collect some data (about 46,000 queries) to train a ranking model. To this end, we need queries that are frequent and with explicit user actions so that we have relevance estimation of articles with regards to these queries. In 2016, with about 150,000 queries run under Best Match per month, only hundreds of them met the threshold to build a gold standard (see <xref ref-type="supplementary-material" rid="pbio.2005343.s004">S3 Text</xref> for details on the filters and threshold used and <xref ref-type="supplementary-material" rid="pbio.2005343.s005">S4 Text</xref> for details on the gold standard creation). But, as relevance search gains popularity in PubMed, we will soon be able to collect several thousands of recurrent queries every few months to better train the ranker over time.</p>
<p>Second, when users click a result or request the full text of an article, they often do not explore the entire set of search results. Hence, potentially relevant documents may be missed in the gold standard or considered as irrelevant. Conversely, when an article is clicked, it could still be irrelevant to the user information need.</p>
<p>Third, there is a potential bias in the fact that we do not account for the position in which clicked documents were ranked. In other words, if a document is clicked at the 10th position, it should, in theory, have more weight in training than the one at the first position because the top document is naturally more likely to be clicked. We are currently experimenting with ways to account for this particular factor during the creation of training data.</p>
<p>In summary, this paper presents the latest major improvement in PubMed for relevance search. We used a state-of-the-art information retrieval technique, adapted it to the biological domain (e.g., by creating training data and ranking features specific to the scientific literature), and scaled it to meet the throughput requirement of PubMed with millions of searches each day. Specifically, we developed an end-to-end pipeline based on an open source search platform (Solr) and an advanced machine-learning algorithm (LambdaMART) for optimizing the quality of the top-ranked results. We described in detail what features ("signals") we selected for the machine-learning algorithm, how they were evaluated, and in what way they contribute to the final ranking results. This paper also demonstrates the whole process and steps in adopting state-of-the-art research findings into a real-world application such as offline versus online evaluation, scalability test, usage analysis, etc.</p>
<p>Overall, the new Best Match algorithm shows a significant improvement in finding relevant information over the default time order in PubMed. It has also resulted in an increased usage of relevance search over time, which allows us to accumulate more relevance data for iteratively improving our machine-learning–based ranker.</p>
<p>We have also noticed that in the last few years, the IR community has started developing and experimenting with new retrieval methods for document ranking using the latest deep-learning techniques. While early results (including our own) are promising [<xref ref-type="bibr" rid="pbio.2005343.ref035">35</xref>–<xref ref-type="bibr" rid="pbio.2005343.ref041">41</xref>], more work is warranted with regards to retrieval quality, robustness, and scalability for adoption into real-world applications such as PubMed.</p>
<p>Finally, it is important to note that we design and build our methods based on our users and their search behaviors. Therefore, we encourage them to try this new relevance search and provide input so that they can help us continue to improve the ranking method.</p>
</sec>
<sec id="sec010">
<title>Supporting information</title>
<supplementary-material id="pbio.2005343.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s001" xlink:type="simple">
<label>S1 Glossary</label>
<caption>
<title>List of abbreviations and definitions.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s002" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>First stage ranking by BM25.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s003" xlink:type="simple">
<label>S2 Text</label>
<caption>
<title>Second stage ranking by L2R.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s004" xlink:type="simple">
<label>S3 Text</label>
<caption>
<title>Search log data.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s005" xlink:type="simple">
<label>S4 Text</label>
<caption>
<title>Generating gold-standard relevance data.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s006" xlink:type="simple">
<label>S5 Text</label>
<caption>
<title>Evaluation metrics.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s007" xlink:type="simple">
<label>S6 Text</label>
<caption>
<title>Improved ranking quality in offline benchmarking evaluation.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s008" xlink:type="simple">
<label>S7 Text</label>
<caption>
<title>Feature representation.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s009" xlink:type="simple">
<label>S8 Text</label>
<caption>
<title>Feature contribution.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s010" xlink:type="simple">
<label>S9 Text</label>
<caption>
<title>System setup and optimization.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s011" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Average click through rate at rank 20 for queries occurring less than 1,000 times.</title>
<p>The observed overall average CTR@20 of near 0.4 appears to be strongly influenced by unique queries. The chart is cut at 1,000, but only a minimal number of queries occur more than a thousand times over a year.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s012" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Offline evaluation of the new relevance algorithm against the silver standard extracted from the search logs.</title>
<p>Precision-recall curves are plotted after the first step (green) and the second (blue) accordingly. A much higher precision is achieved after the second re-ranking step, especially for the top ranked results.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pbio.2005343.s013" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.2005343.s013" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Impact of feature ablation on overall ranking quality (measured by NDCG@20 scores).</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Dr. John Wilbur for his help with the early version of this manuscript and Drs. David Lipman and Udi Manber for their helpful discussion of the project. We are grateful to Dr. Kelley Skeff for his valuable comments and examples in which Best Match helped clinicians. We thank Kate Majewski, Dr. Robert Leaman, Susan Chacko, Narmada Thanki, and Shazia Dharssi for their help with proofreading and improving the clarity of the paper.</p>
</ack>
<fn-group>
<fn fn-type="other" id="fn001">
<p><bold>Provenance:</bold> Not commissioned; externally peer reviewed</p>
</fn>
</fn-group>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>BM25</term>
<def><p>Okapi Best-Matching algorithm</p></def>
</def-item>
<def-item><term>CTR</term>
<def><p>click-through rate</p></def>
</def-item>
<def-item><term>D</term>
<def><p>document</p></def>
</def-item>
<def-item><term>DVT</term>
<def><p>deep vein thrombosis</p></def>
</def-item>
<def-item><term>IR</term>
<def><p>information retrieval</p></def>
</def-item>
<def-item><term>L2R</term>
<def><p>learning to rank</p></def>
</def-item>
<def-item><term>NCBI</term>
<def><p>National Center for Biotechnology Information</p></def>
</def-item>
<def-item><term>NDCG</term>
<def><p>Normalized Discounted Cumulative Gain</p></def>
</def-item>
<def-item><term>NIH</term>
<def><p>National Institutes of Health</p></def>
</def-item>
<def-item><term>NLM</term>
<def><p>National Library of Medicine</p></def>
</def-item>
<def-item><term>Q</term>
<def><p>query</p></def>
</def-item>
<def-item><term>QD</term>
<def><p>query–document relationship</p></def>
</def-item>
<def-item><term>TF–IDF</term>
<def><p>term frequency–inverse document frequency</p></def>
</def-item>
<def-item><term>TREC</term>
<def><p>Text REtrieval Conference</p></def>
</def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="pbio.2005343.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jensen</surname> <given-names>L. J.</given-names></name>, <name name-style="western"><surname>Saric</surname> <given-names>J.</given-names></name>, and <name name-style="western"><surname>Bork</surname> <given-names>P.</given-names></name> <article-title>Literature mining for the biologist: from information retrieval to biological discovery</article-title>. <source>Nature reviews genetics</source>, <volume>7</volume>(<issue>2</issue>):<fpage>119</fpage>–<lpage>129</lpage>, <year>2006</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrg1768" xlink:type="simple">10.1038/nrg1768</ext-link></comment> <object-id pub-id-type="pmid">16418747</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Islamaj Dogan</surname> <given-names>R.</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>G. C.</given-names></name>, <name name-style="western"><surname>Neveol</surname> <given-names>A.</given-names></name>, and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Understanding pubmed user search behavior through log analysis</article-title>. <source>Database (Oxford)</source>, <year>2009</year>:<fpage>bap018</fpage>, 2009.</mixed-citation></ref>
<ref id="pbio.2005343.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>W.</given-names></name>, and <name name-style="western"><surname>Wilbur</surname> <given-names>W. J.</given-names></name> <article-title>Evaluating relevance ranking strategies for medline retrieval</article-title>. <source>Journal of the American Medical Informatics Association: JAMIA</source>, <volume>16</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>36</lpage>, <year>2009</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1197/jamia.M2935" xlink:type="simple">10.1197/jamia.M2935</ext-link></comment> <object-id pub-id-type="pmid">18952932</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Robertson</surname> <given-names>S. E.</given-names></name>, <name name-style="western"><surname>Walker</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Hancock-Beaulieu</surname> <given-names>M.</given-names></name>, and <name name-style="western"><surname>Gatford</surname> <given-names>M.</given-names></name> <chapter-title>Okapi at TREC-3</chapter-title>, page <fpage>109</fpage>. <publisher-name>Nist Special Publication</publisher-name>, <year>1994</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hersh</surname> <given-names>W. R.</given-names></name> <chapter-title>Information retrieval: a health and biomedical perspective</chapter-title>. <publisher-name>Springer Science &amp; Business Media</publisher-name>, <year>2008</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hersh</surname> <given-names>W. R.</given-names></name> and <article-title>Ellen M. Voorhees. Trec genomics special issue overview</article-title>. <source>Information Retrieval</source>, <volume>12</volume>:<fpage>1</fpage>–<lpage>15</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname> <given-names>J.</given-names></name> and <name name-style="western"><surname>Zhai</surname> <given-names>C.</given-names></name> <article-title>An empirical study of tokenization strategies for biomedical information retrieval</article-title>. <source>Information Retrieval</source>, <volume>10</volume>(<issue>4–5</issue>):<fpage>341</fpage>–<lpage>363</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref008"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">S. Greuter, P. Junker, L. Kuhn, F. Mance, V. Mermet, A. Rellstab, and C. Eickhoff. Eth zurich at trec clinical decision support 2016. In TREC, 2016.</mixed-citation></ref>
<ref id="pbio.2005343.ref009"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">A. Ghenai, E. Khalilov, P. Valov, and C. L. Clarke. Waterlooclarke: Trec 2015 clinical decision support track. Report, University of Waterloo Waterloo, ON Canada, 2015.</mixed-citation></ref>
<ref id="pbio.2005343.ref010"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">K. M. Svore and C. J.C. Burges. A machine learning approach for improved BM25 retrieval. Proceeding of the 18th ACM conference on Information and knowledge management—CIKM '09, page 1811, 2009.</mixed-citation></ref>
<ref id="pbio.2005343.ref011"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">A. Phophalia. A survey on learning to rank (letor) approaches in information retrieval. In 2011 Nirma University International Conference on Engineering, pages 1–6, 2011.</mixed-citation></ref>
<ref id="pbio.2005343.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mao</surname> <given-names>Y.</given-names></name> and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Mesh now: automatic mesh indexing at PubMed scale via learning to rank</article-title>. <source>Journal of biomedical semantics</source>, <volume>8</volume>(<issue>1</issue>):<fpage>15</fpage>–15, <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13326-017-0123-3" xlink:type="simple">10.1186/s13326-017-0123-3</ext-link></comment> <object-id pub-id-type="pmid">28412964</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>J.</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>M.</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>S.</given-names></name>, and <name name-style="western"><surname>Liu</surname> <given-names>B.</given-names></name> <article-title>Protdec-ltr2.0: an improved method for protein remote homology detection by combining pseudo protein and supervised learning to rank</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>33</volume>(<issue>21</issue>):<fpage>3473</fpage>–<lpage>3476</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shang</surname> <given-names>Y.</given-names></name>, <name name-style="western"><surname>Hao</surname> <given-names>H.</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>J.</given-names></name>, and <name name-style="western"><surname>Lin</surname> <given-names>H.</given-names></name> <article-title>Learning to rank-based gene summary extraction</article-title>. <source>BMC bioinformatics</source>, <volume>15</volume> <issue>Suppl 12</issue>(Suppl 12):<fpage>S10</fpage>–<lpage>S10</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leaman</surname> <given-names>R.</given-names></name>, <name name-style="western"><surname>Islamaj Dogan</surname> <given-names>R.</given-names></name>, and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Dnorm: disease name normalization with pairwise learning to rank</article-title>. <source>Bioinformatics (Oxford, England)</source>, <volume>29</volume>(<issue>22</issue>):<fpage>2909</fpage>–<lpage>2917</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kavuluru</surname> <given-names>R.</given-names></name>, <name name-style="western"><surname>Rios</surname> <given-names>A.</given-names></name>, and <name name-style="western"><surname>Lu</surname> <given-names>Y.</given-names></name> <article-title>An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records</article-title>. <source>Artificial intelligence in medicine</source>, <volume>65</volume>(<issue>2</issue>):<fpage>155</fpage>–<lpage>166</lpage>, <year>2015</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.artmed.2015.04.007" xlink:type="simple">10.1016/j.artmed.2015.04.007</ext-link></comment> <object-id pub-id-type="pmid">26054428</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Svore</surname> <given-names>K. M.</given-names></name> and <name name-style="western"><surname>Burges</surname> <given-names>C.J.</given-names></name> <article-title>Large-scale learning to rank using boosted decision trees</article-title>. <source>Scaling Up Machine Learning: Parallel and Distributed Approaches</source>, <volume>2</volume>, <fpage>2011</fpage>.</mixed-citation></ref>
<ref id="pbio.2005343.ref018"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">D. Cossock. Method and apparatus for machine learning a document relevance function, March 27 2007. US Patent 7,197,497.</mixed-citation></ref>
<ref id="pbio.2005343.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dang</surname> <given-names>V.</given-names></name>, <name name-style="western"><surname>Bendersky</surname> <given-names>M.</given-names></name>, and <name name-style="western"><surname>Croft</surname> <given-names>W. B.</given-names></name> <article-title>Two-stage learning to rank for information retrieval</article-title>. In <source>ECIR</source>, pages <fpage>423</fpage>–<lpage>434</lpage>. Springer, <year>2013</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Y Liu</surname> <given-names>T.</given-names></name> <article-title>Learning to rank for information retrieval</article-title>. <source>Found. Trends Inf. Retr</source>., <volume>3</volume>:<fpage>225</fpage>–<lpage>331</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>JC Burges</surname> <given-names>C.</given-names></name> <article-title>From ranknet to lambdarank to lambdamart: An overview</article-title>. <source>Learning</source>, <volume>11</volume>(<issue>23–581</issue>):<fpage>81</fpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chapelle</surname> <given-names>O.</given-names></name> and <name name-style="western"><surname>Chang</surname> <given-names>Y.</given-names></name> <article-title>Yahoo! learning to rank challenge overview</article-title>. In <source>Yahoo! Learning to Rank Challenge</source>, pages <fpage>1</fpage>–<lpage>24</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref023"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">B. Xu, H. Lin, Y. Lin, Y. Ma, L. Yang, J. Wang, and Z. Yang. Learning to rank for biomedical information retrieval. In Proceeding of the 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 464–469. IEEE, 2015.</mixed-citation></ref>
<ref id="pbio.2005343.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">K. M. Svore, P. H Kanani, and N. Khan. How good is a span of terms?: exploiting proximity to improve web retrieval. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 154–161. ACM, 2010.</mixed-citation></ref>
<ref id="pbio.2005343.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kohavi</surname> <given-names>R.</given-names></name> and <name name-style="western"><surname>Longbotham</surname> <given-names>R.</given-names></name> <chapter-title>Online controlled experiments and a/b testing</chapter-title>. In <source>Encyclopedia of Machine Learning and Data Mining</source>, pages <fpage>922</fpage>–<lpage>929</lpage>. <publisher-name>Springer US</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sherman</surname> <given-names>L.</given-names></name> and <name name-style="western"><surname>Deighton</surname> <given-names>J.</given-names></name> <article-title>Banner advertising: Measuring effectiveness and optimizing placement</article-title>. <source>Journal of Interactive Marketing</source>, <volume>15</volume>(<issue>2</issue>):<fpage>60</fpage>–<lpage>64</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname> <given-names>H.</given-names></name> and <name name-style="western"><surname>Leckenby</surname> <given-names>J. D.</given-names></name> <article-title>Internet advertising formats and effectiveness</article-title>. <source>Center for Interactive Advertising</source>, pages <fpage>1</fpage>–<lpage>31</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yeganova</surname> <given-names>L.</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>W.</given-names></name>, <name name-style="western"><surname>Comeau</surname> <given-names>D. C.</given-names></name>, <name name-style="western"><surname>Wilbur</surname> <given-names>W. J.</given-names></name> and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>A Field Sensor: computing the composition and intent of PubMed queries</article-title>. <source>Database</source>, Volume <volume>2018</volume>, <day>1</day> <month>January</month> <year>2018</year>, <fpage>bay052</fpage>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/database/bay052" xlink:type="simple">https://doi.org/10.1093/database/bay052</ext-link></mixed-citation></ref>
<ref id="pbio.2005343.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiorini</surname> <given-names>N.</given-names></name>, <name name-style="western"><surname>Lipman</surname> <given-names>D. J.</given-names></name> and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Towards PubMed 2.0</article-title>. <source>eLife</source>, <year>2017</year>;<volume>6</volume>:<fpage>e28801</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.28801" xlink:type="simple">10.7554/eLife.28801</ext-link></comment> <object-id pub-id-type="pmid">29083299</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133–142. ACM, 2002.</mixed-citation></ref>
<ref id="pbio.2005343.ref031"><label>31</label><mixed-citation publication-type="other" xlink:type="simple">T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 154–161. Acm, 2005.</mixed-citation></ref>
<ref id="pbio.2005343.ref032"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">G. Dupret and C. Liao. A model to estimate intrinsic document relevance from the clickthrough logs of a web search engine. In Proceedings of the third ACM international conference on Web search and data mining, pages 181–190. ACM, 2010.</mixed-citation></ref>
<ref id="pbio.2005343.ref033"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">X. Ye, J. Li, Z. Qi, B. Peng, and D. Massey. A generative model for generating relevance labels from human judgments and click-logs. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1907–1910. ACM, 2014.</mixed-citation></ref>
<ref id="pbio.2005343.ref034"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra, and P. Tsaparas. Generating labels from clicks. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 172–181. ACM, 2009.</mixed-citation></ref>
<ref id="pbio.2005343.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Onal</surname> <given-names>K. D.</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Y.</given-names></name>, <name name-style="western"><surname>Sengor Altingovde</surname> <given-names>I.</given-names></name>, <name name-style="western"><surname>Mustafizur Rahman</surname> <given-names>M.</given-names></name>, <name name-style="western"><surname>Karagoz</surname> <given-names>P.</given-names></name>, <name name-style="western"><surname>Braylan</surname> <given-names>A.</given-names></name>, <name name-style="western"><surname>Dang</surname> <given-names>B.</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>H.-L.</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>H.</given-names></name>, and <name name-style="western"><surname>McNamara</surname> <given-names>Q.</given-names></name> <article-title>Neural information retrieval: At the end of the early years</article-title>. <source>Information Retrieval Journal</source>, pages <fpage>1</fpage>–<lpage>72</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohan</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Fiorini</surname> <given-names>N.</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>S.</given-names></name>, and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Deep learning for biomedical information retrieval: Learning textual relevance from click logs</article-title>. <source>BioNLP</source> <italic>2017</italic>, pages <fpage>222</fpage>–<lpage>231</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="pbio.2005343.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname> <given-names>S.</given-names></name>, <name name-style="western"><surname>Fiorini</surname> <given-names>N.</given-names></name>, <name name-style="western"><surname>Wilbur</surname> <given-names>W. J.</given-names></name>, and <name name-style="western"><surname>Lu</surname> <given-names>Z.</given-names></name> <article-title>Bridging the gap: Incorporating a semantic similarity measure for effectively mapping pubmed queries to documents</article-title>. <source>Journal of Biomedical Informatics</source>, <volume>75</volume>:<fpage>122</fpage>–<lpage>127</lpage>, <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jbi.2017.09.014" xlink:type="simple">10.1016/j.jbi.2017.09.014</ext-link></comment> <object-id pub-id-type="pmid">28986328</object-id></mixed-citation></ref>
<ref id="pbio.2005343.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">J. Guo, Y. Fan, Q. Ai, and W. B. Croft. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, pages 55–64. ACM, 2016.</mixed-citation></ref>
<ref id="pbio.2005343.ref039"><label>39</label><mixed-citation publication-type="other" xlink:type="simple">A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks categories and subject descriptors. Sigir, pages 373–382, 2015.</mixed-citation></ref>
<ref id="pbio.2005343.ref040"><label>40</label><mixed-citation publication-type="other" xlink:type="simple">H. Li and Z. Lu. Deep learning for information retrieval. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 1203–1206. ACM, 2016.</mixed-citation></ref>
<ref id="pbio.2005343.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">K. Hui, A. Yates, K. Berberich, and G. de Melo. Position-aware representations for relevance matching in neural information retrieval. In Proceedings of the 26th International Conference on World Wide Web Companion, pages 799–800. International World Wide Web Conferences Steering Committee, 2017.</mixed-citation></ref>
</ref-list>
</back>
</article>