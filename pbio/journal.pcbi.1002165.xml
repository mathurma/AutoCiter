<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00481</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002165</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Sensory perception</subject>
              <subj-group>
                <subject>Psychophysics</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Auditory system</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Animal cognition</subject>
              <subject>Behavioral neuroscience</subject>
              <subject>Neuroethology</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Psychophysics</subject>
              <subject>Sensory perception</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Monkeys and Humans Share a Common Computation for Face/Voice Integration</article-title><alt-title alt-title-type="running-head">Face/Voice Integration in Humans and Monkeys</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Chandrasekaran</surname>
            <given-names>Chandramouli</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Lemus</surname>
            <given-names>Luis</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Trubanova</surname>
            <given-names>Andrea</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Gondan</surname>
            <given-names>Matthias</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
          <xref ref-type="aff" rid="aff5">
            <sup>5</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Ghazanfar</surname>
            <given-names>Asif A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff6">
            <sup>6</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Psychology, Princeton University, Princeton, New Jersey, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Marcus Autism Center, Emory University School of Medicine, Atlanta, Georgia, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Department of Psychology, University of Regensburg, Regensburg, Germany</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Medical Biometry and Informatics, University of Heidelberg, Heidelberg, Germany</addr-line>       </aff><aff id="aff6"><label>6</label><addr-line>Department of Ecology and Evolutionary Biology, Princeton University, Princeton, New Jersey, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">asifg@princeton.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: CC AT AAG. Performed the experiments: CC LL AT. Analyzed the data: CC MG. Contributed reagents/materials/analysis tools: CC MG AAG. Wrote the paper: CC AAG.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>9</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>29</day>
        <month>9</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>9</issue><elocation-id>e1002165</elocation-id><history>
        <date date-type="received">
          <day>8</day>
          <month>4</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>3</day>
          <month>7</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Chandrasekaran et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Speech production involves the movement of the mouth and other regions of the face resulting in visual motion cues. These visual cues enhance intelligibility and detection of auditory speech. As such, face-to-face speech is fundamentally a multisensory phenomenon. If speech is fundamentally multisensory, it should be reflected in the evolution of vocal communication: similar behavioral effects should be observed in other primates. Old World monkeys share with humans vocal production biomechanics and communicate face-to-face with vocalizations. It is unknown, however, if they, too, combine faces and voices to enhance their perception of vocalizations. We show that they do: monkeys combine faces and voices in noisy environments to enhance their detection of vocalizations. Their behavior parallels that of humans performing an identical task. We explored what common computational mechanism(s) could explain the pattern of results we observed across species. Standard explanations or models such as the principle of inverse effectiveness and a “race” model failed to account for their behavior patterns. Conversely, a “superposition model”, positing the linear summation of activity patterns in response to visual and auditory components of vocalizations, served as a straightforward but powerful explanatory mechanism for the observed behaviors in both species. As such, it represents a putative homologous mechanism for integrating faces and voices across primates.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The evolution of speech is one of our most fascinating and enduring mysteries—enduring partly because all the critical features of speech (brains, vocal tracts, ancestral speech-like sounds) do not fossilize. Furthermore, it is becoming increasingly clear that speech is, by default, a multimodal phenomenon: we use both faces and voices together to communicate. Thus, understanding the evolution of speech requires a comparative approach using closely-related extant primate species and recognition that vocal communication is audiovisual. Using computer-generated avatar faces, we compared the integration of faces and voices in monkeys and humans performing an identical detection task. Both species responded faster when faces and voices were presented together relative to the face or voice alone. While the details sometimes appeared to differ, the behavior of both species could be well explained by a “superposition” model positing the linear summation of activity patterns in response to visual and auditory components of vocalizations. Other, more popular computational models of multisensory integration failed to explain our data. Thus, the superposition model represents a putative homologous mechanism for integrating faces and voices across primate species.</p>
      </abstract><funding-group><funding-statement>AAG is supported by the National Institute of Neurological Disorders and Stroke (NINDS, R01NS054898), the National Science Foundation CAREER award (BCS-0547760) and the James S McDonnell Scholar Award. CC was supported by the Charlotte Elizabeth Procter and Centennial Fellowships from Princeton University. The funders had no role in study design, data collection and analysis, decision to publish, or preparation.</funding-statement></funding-group><counts>
        <page-count count="20"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>When we speak, our face moves and deforms the mouth and other regions <xref ref-type="bibr" rid="pcbi.1002165-Ohala1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Summerfield1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Summerfield2">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Yehia1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran1">[5]</xref>. These dynamics and deformations lead to a variety of visual motion cues (“visual speech”) related to the auditory components of speech and are integral to face-to-face communication. In noisy, real world environments, visual speech can provide considerable intelligibility benefits to the perception of auditory speech <xref ref-type="bibr" rid="pcbi.1002165-Sumby1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ross1">[7]</xref>, faster reaction times <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Besle1">[9]</xref>, and is hard to ignore—integrating readily and automatically with auditory speech <xref ref-type="bibr" rid="pcbi.1002165-McGurk1">[10]</xref>. For these and other reasons, it's been argued that audiovisual (or “multisensory”) speech is the primary mode of speech perception and is not a capacity that is simply piggy-backed onto auditory speech perception <xref ref-type="bibr" rid="pcbi.1002165-Rosenblum1">[11]</xref>.</p>
      <p>If the processing of multisensory signals forms the default mode of speech perception, then this should be reflected in the evolution of vocal communication. Naturally, any vertebrate organism (from fishes and frogs, to birds and dogs) that produces vocalizations will have a simple, concomitant visual motion in the area of the mouth. However, in the primate lineage, both the number and diversity of muscles innervating the face <xref ref-type="bibr" rid="pcbi.1002165-Burrows1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Huber1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Huber2">[14]</xref> and the amount of neural control related to facial movement <xref ref-type="bibr" rid="pcbi.1002165-Sherwood1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sherwood2">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sherwood3">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sherwood4">[18]</xref> increased over time relative to other taxa. This ultimately allowed the production of a greater diversity of facial and vocal expressions in primates <xref ref-type="bibr" rid="pcbi.1002165-Andrew1">[19]</xref>, with different patterns of facial motion uniquely linked to different vocal expressions <xref ref-type="bibr" rid="pcbi.1002165-Hauser1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Partan1">[21]</xref>. This is similar to what is observed in humans. In macaque monkeys, for example, coo calls, like the /u/ in speech, are produced with the lips protruded, while screams, like the /i/ in speech, are produced with the lips retracted <xref ref-type="bibr" rid="pcbi.1002165-Hauser1">[20]</xref>.</p>
      <p>These and other homologies between human and nonhuman primate vocal production <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar1">[22]</xref> imply that the mechanisms underlying multisensory vocal <italic>perception</italic> should also be homologous across primate species. Three lines of evidence suggest that perceptual mechanisms may be shared as well. First, nonhuman primates, like human infants <xref ref-type="bibr" rid="pcbi.1002165-Kuhl1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Patterson1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Patterson2">[25]</xref>, can match facial expressions to their appropriate vocal expressions <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Evans1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Izumi1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Parr1">[29]</xref>. Second, monkeys also use eye movement strategies similar to human strategies when viewing dynamic, vocalizing faces <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar3">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-VatikiotisBateson1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Lansing1">[32]</xref>. The third, indirect line of evidence comes from neurophysiological work. Regions of the neocortex that are modulated by audiovisual speech in humans [e.g., 8,33,34,35,36,37], such as the superior temporal sulcus, prefrontal cortex and auditory cortex, are similarly modulated by species-specific audiovisual communication signals in the macaque monkey <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar4">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar5">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran2">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar6">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Barraclough1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sugihara1">[43]</xref>. However, <underline>none</underline> of these behavioral and neurophysiological results from nonhuman primates provide evidence for the critical feature of human audiovisual speech: a <italic>behavioral advantage</italic> via integration of the two signal components of speech (faces and voices) over either component alone. Henceforth, we define “integration” as a statistically significant difference between the responses to audiovisual versus auditory-only and visual-only conditions<xref ref-type="bibr" rid="pcbi.1002165-Stein1">[44]</xref>.</p>
      <p>For a homologous perceptual mechanism to evolve in monkeys, apes and humans from a common ancestor, there must be some behavioral advantage to justify devoting the neural resources mediating such a mechanism. One behavioral advantage conferred by audiovisual speech in humans is faster detection of speech sounds in noisy environments—faster than if only the auditory or visual component is available <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Besle1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Klucharev1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Murase1">[46]</xref>. Here, in a task operationalizing the perception of natural audiovisual communication signals in noisy environments, we tested macaque monkeys on an audiovisual ‘coo call’ detection task using computer-generated monkey avatars. We then compared their performance with that of humans performing an identical task, where the only difference was that humans detected /u/ sounds made by human avatars. Behavioral patterns in response to audiovisual, visual and auditory vocalizations were used to test if any of the classical principles or mechanisms of multisensory integration [e.g. 47,48,49,50,51,52,53] could serve as homologous computational mechanism(s) mediating the perception of audiovisual communication signals.</p>
      <p>We report two main findings. First, monkeys integrate faces and voices. They exhibit faster reaction times to faces and voices presented together relative to faces or voices presented alone —and this behavior closely parallels the behavior of humans in the same task. Second, after testing multiple computational mechanisms for multisensory integration, we found that a simple superposition model, which posits the linear summation of activity from visual and auditory channels, is a likely homologous mechanism. This model explains both the monkey and human behavioral patterns.</p>
    </sec>
    <sec id="s2" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s2a">
        <title>Ethics statement</title>
        <p>All experiments and surgical procedures were performed in compliance with the guidelines of the Princeton University Institutional Animal Care and Use Committee. For human participants, all procedures were approved by the Institutional Review Board at Princeton University. Informed consent was obtained from all human participants.</p>
      </sec>
      <sec id="s2b">
        <title>Subjects</title>
        <p>Nonhuman primate subjects were two adult male macaques <italic>(Macaca fascicularis</italic>). These monkeys were born in captivity and provided various sources of enrichment, including cartoons displayed on a large screen TV as well as olfactory, auditory and visual contact with conspecifics. The monkeys underwent sterile surgery for the implantation of a head-post.</p>
        <p>The human participants consisted of staff or graduate students (n = 6, 4 males, mean age  = 27) at Princeton University. Two of the subjects were authors on the paper (CC, LL). The other four human subjects were naïve to the purposes and goals of the experiment.</p>
      </sec>
      <sec id="s2c">
        <title>Avatars</title>
        <p>We would like to briefly explain here why we chose to use avatars. First, it is quite difficult to record monkey vocalizations which only contain mouth motion without other dynamic motion components such as arbitrary head motion and rotation— which themselves may lead to audiovisual integration <xref ref-type="bibr" rid="pcbi.1002165-Munhall1">[54]</xref>. Second, start and end positions of the head from such videos of vocalizations, at least for monkeys, tend to be very variable which would add additional visual motion cues. Third, we wanted constant lighting and background and the ability to modulate the size of the mouth opening and thereby parameterize visual stimuli. Fourth, the goal of this experiment was to understand how mouth motion integrated with the auditory components of vocalizations and we wanted to avoid transient visual stimuli. Real videos would not have allowed us to control for these factors; avatars provide us with considerable control.</p>
      </sec>
      <sec id="s2d">
        <title>Monkey behavior</title>
        <p>Experiments were conducted in a sound attenuating radio frequency (RF) enclosure. The monkey sat in a primate chair fixed 74 cm opposite a 19 inch CRT color monitor with a 1280×1024 screen resolution and 75 Hz refresh rate. The 1280×1024 screen subtended a visual angle of ∼25° horizontally and 20° vertically. All stimuli were centrally located on the screen and occupied a total area (including blank regions) of 640×653 pixels. For every session, the monkeys were placed in a restraint chair and head-posted. A depressible lever (ENV-610M, Med Associates) was located at the center-front of the chair. Both monkeys spontaneously used their left hand for responses. Stimulus presentation and data collection were performed using Presentation (Neurobehavioral Systems).</p>
        <sec id="s2d1">
          <title>Stimuli: Monkeys</title>
          <p>We used coo calls from two macaques as the auditory components of vocalizations; these were from individuals that were unknown to the monkey subjects. The auditory vocalizations were resized to a constant duration of 400 milliseconds using a Matlab implementation of a phase vocoder <xref ref-type="bibr" rid="pcbi.1002165-Flanagan1">[55]</xref> and normalized in amplitude. The visual components of the vocalizations were 400 ms long videos of synthetic monkey agents making a coo vocalization. The animated stimuli were generated using 3D Studio Max 8 (Autodesk) and Poser Pro (Smith Micro), and were extensively modified from a stock model made available by DAZ Productions (Silver key 3D monkey). As a direct stare or eye contact in monkeys means a challenge or a threat, the direction of the gaze of monkey avatars was averted slightly to a target approximately 20 degrees to the left of straight ahead. To increase the realism of the monkey avatars, we used the base skin texture extracted from photographs of real macaques. When presented on the screen, the monkey avatar was 6.5” wide (12.25°) at the shoulder and 4.5” (8.55°) tall from the top of the head to the bottom of the screen. The face itself was 2.75” wide (5.25°) between the eyes and 3” (5.72°) tall from the top of the head to the bottom of the chin, with the width of the face tapering as it neared the mouth. The audiovisual stimuli were generated by presenting both visual and auditory-only components with an 85-millisecond lag between the onset of mouth opening and the sound of the vocalization. Such a lag is within the natural range for macaque monkeys <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran2">[40]</xref>.</p>
        </sec>
        <sec id="s2d2">
          <title>Task structure for monkeys</title>
          <p>Monkeys were trained to detect two coo vocalizations according to a redundant target free-response paradigm <xref ref-type="bibr" rid="pcbi.1002165-Egan1">[56]</xref>; detection was indicated by a lever-press. Redundant target paradigms refer to experimental designs where two or more targets appear simultaneously and responses to any target are considered as hits (see for example, <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller3">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Raab1">[59]</xref>). They could be from different modalities (visual and auditory) or from the same modality (color and shape). In our case, the redundant targets were motion of the mouth and the sound of the coo vocalization. Free response paradigms refer to the absence of explicit trial markers <xref ref-type="bibr" rid="pcbi.1002165-Egan1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Shub1">[60]</xref>. We chose a free response paradigm because it mimics natural audiovisual communication—faces are usually continuously visible and move during vocal production. Coo vocalizations were presented at different loudness levels (50–85 dB) and at random intervals in ∼63 dB spectrally pink background noise; each vocalization was paired with a synthetic monkey face whose mouth opened in a manner concordant with the loudness of the vocalizations. During every block of the experimental session, a face was always visible, but, only moved for its corresponding identity matched vocalization. The identity of the avatar was counter-balanced across blocks of 60 trials with an inter-block interval ranging from 10–12 seconds in duration. The stimuli-- auditory-only, visual-only and audiovisual conditions--were presented at an inter-sound-interval drawn from a uniform distribution between 1 and 3 seconds. Monkeys were trained to respond to the coo vocalization events in visual, auditory or audiovisual conditions while withholding responses when no stimuli were presented. A press of the lever within a window of 135 to 2000 milliseconds after onset of the vocalization event led to a juice reward. Lever presses outside this window were defined as false alarms and a timeout ranging anywhere from 3 to 5.5 seconds was imposed. Any press during this timeout period led to a renewal of the timeout with the duration again randomly drawn from a uniform distribution from 3 to 5.5 seconds. The monkeys had to wait the entire duration of this timeout period before a new stimulus was presented. A session usually lasted from 300 to 600 trials spanning durations of 25 to 50 minutes.</p>
        </sec>
        <sec id="s2d3">
          <title>Reaction times and accuracy</title>
          <p>Reaction times (RT) were measured as the first depression of the lever after onset of the stimulus. In a free response paradigm, one can define hits, misses and false alarms. A response within the 135 – 2000-millisecond window after the onset of the stimulus was rewarded with a drop of juice and defined as a hit. An omitted response in this window was classified as a miss <xref ref-type="bibr" rid="pcbi.1002165-Egan1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Shub1">[60]</xref>. A response outside this 2 second period where no vocalizations was defined as a false alarm. Hit rate was defined as the ratio of hits to hits plus misses. For each SNR and condition, the accuracy was defined as the ratio of hits to hits plus misses expressed as a percentage. The false alarm rate was defined as number of false alarms divided by the sum of hits and false alarms. We only took sessions where false alarm percentages were low (in the 10 – 20 %) range keeping with prior standards in the literature <xref ref-type="bibr" rid="pcbi.1002165-Gourevitch1">[61]</xref>. Monkeys were trained until false alarms were around 10–20 %. Hit rate and False alarm rate from two example sessions from monkey 1 are shown in <xref ref-type="supplementary-material" rid="pcbi.1002165.s001">Figures S1A</xref>, B along with the quantification of the average false alarm rate across sessions (<xref ref-type="supplementary-material" rid="pcbi.1002165.s001">Figure S1C</xref>).</p>
        </sec>
        <sec id="s2d4">
          <title>Training</title>
          <p>Monkeys were shaped through standard operant conditioning techniques over several months (8 months for monkey 1 and 5 months for monkey 2) to respond to vocalization events but withhold responses during the inter-stimulus interval. Training was performed by first shaping the monkey to press a lever for juice reward, then to a sound with a large signal to noise and a large window for gaining a reward. Once the monkey had learned to respond reliably to vocalizations (&gt;80 % hit rate) but withheld responses during the inter-stimulus interval (&lt;20 % false alarms), different SNRs were introduced along with a concomitant restriction of the response window to two seconds. Static and moving faces were then introduced and auditory and audiovisual stimuli were randomly presented. Finally, the control condition of visual-only, that is, facial motion without any accompanying vocalization was introduced, leading monkeys to respond reliably to visual-only, auditory-only and audiovisual coo vocalizations.</p>
        </sec>
      </sec>
      <sec id="s2e">
        <title>Human behavior</title>
        <p>Experiments were conducted in a psychophysics booth. The human sat in a comfortable chair approximately 65 cm opposite a 17 inch LCD color monitor with a 1280×1024 screen resolution and 75 Hz refresh rate. The 1280×1024 screen subtended a visual angle of 28 degrees horizontally and 24 degrees vertically. All stimuli were centrally located on the screen and occupied an area of 640×653 pixels. All stimulus presentation and data collection were performed using Presentation (Neurobehavioral Systems).</p>
        <sec id="s2e1">
          <title>Stimuli: humans</title>
          <p>For humans, vocalization stimuli were /u/sounds made by two female undergraduates at Princeton University recorded using a Canon Vixia HD100 digital camcorder. These vocalizations were resized to a constant length of 400 milliseconds and normalized in amplitude. For visual stimuli, we again used 400 ms synthetic avatars created using the Poser Pro program. In particular, we modified the stock poser model (Sydney, Smith micro pro) and used the animation available for making an /u/ sound. Two avatars were created by altering the shape of the face and the hair to generate two different avatars. The human avatar was 5” wide (11.95°) and 6” tall from the top of the head (14 degrees). The face itself was 5” wide (11.95°) between the eyes and 5.5” (13°) tall from the top of the head to the bottom of the chin, with the width of the face decreasing in width near the mouth. Each avatar was paired with a vocalization and this identity correspondence was always maintained.</p>
        </sec>
        <sec id="s2e2">
          <title>Task structure for humans</title>
          <p>For humans, we used an almost identical task structure as the monkeys with minor modifications. In particular, we reduced the timeout periods for false alarms as humans very rarely made them. Second, blocks were longer with number of trials ranging from 90 to 105 trials per block. Each session contained 4 blocks. Again, the order of avatars was randomized and counterbalanced across avatars. Each human subject completed at least 2 sessions, leading to approximately 60 trials per modality and SNR level. Humans pressed the spacebar button on the keyboard to denote successful detection. We measured accuracy and RTs as described above for monkeys.</p>
        </sec>
      </sec>
      <sec id="s2f">
        <title>Statistical analysis of behavioral performance</title>
        <sec id="s2f1">
          <title>Comparison between audiovisual, auditory-only and visual-only RTs</title>
          <p>We used non-parametric tests for comparing the RT distributions for visual, auditory and audiovisual vocalizations. For single subjects (both monkeys and humans), RTs were compared first by using a Kruskal-Wallis non-parametric ANOVA comparing whether RT distributions were significantly different between visual-only, auditory-only and audiovisual conditions. Post-hoc tests were conducted using Mann-Whitney tests comparing whether these distributions were different on a single subject and SNR basis. For the monkeys, we chose the Mann-Whitney test over paired t-tests because the RT distributions were not normal and also had different number of trials for different conditions (because of misses, etc). In addition, for humans, we performed, a paired samples t-test comparing normalized RTs for the audiovisual to the auditory-only and visual-only conditions.</p>
        </sec>
        <sec id="s2f2">
          <title>Standard error for means and medians</title>
          <p>We computed bootstrap error bars by resampling the raw RT distributions with replacement 1000 times and estimating the standard deviation of the mean of the resampled data.</p>
        </sec>
        <sec id="s2f3">
          <title>Bootstrap test for benefits in monkeys</title>
          <p>To test if values of benefits for monkeys were significantly different from 0, we calculated the difference between mean RT of the audiovisual and the minimum of the mean RTs of the auditory-only and visual-only conditions. We then computed a sampling distribution for these benefits by resampling from the audiovisual and the condition of interest. The difference of the means was considered statistically significantly if the 95% bootstrap confidence interval did not include zero.</p>
        </sec>
        <sec id="s2f4">
          <title>Deriving a window of integration using pooled data from monkeys and humans</title>
          <p>We used the session-by-session variability of monkeys and variance across humans to derive a window of integration. For example, Monkey 1's RTs on three successive days for the loudest SNR of 22 dB were as follows. Day1: V – 598 ms, A – 526 ms, AV – 470. Day 2: V – 779 ms, A – 498 ms, AV – 492 – ms. Day 3: V – 611 ms, A – 492 ms, AV – 438 ms. The benefits, in each of these cases are, 55 ms, 5 ms and 53 ms. The corresponding differences between the visual-only and auditory-only RTs are 71 ms, 281 ms and 118 ms. Thus, when the discrepancy between visual and auditory-only RT was too large (&gt;250 ms) then the benefit was at most 5 ms. On the other hand, when the visual-only and auditory-only RTs were closer (∼80 to 120 ms) then the benefit was robust and was 55 ms in magnitude.</p>
        </sec>
      </sec>
      <sec id="s2g">
        <title>Race model</title>
        <p>Our audiovisual detection experiment is an extension of the classical redundant signals paradigm. In such experiments, it is common to observe that RTs to multisensory targets presented simultaneously are faster than unisensory RTs. This effect is usually termed the “redundant signals effect”. One important class of explanations for the redundant signals effect is the “race model”. According to the race model (or a “parallel first terminating” model), redundancy benefits are not due to an actual integration of visual and auditory cues. To illustrate, assume that the time to detect and respond to a single target—e.g., the facial motion--varies according to a statistical distribution. Similarly, the time to detect and respond to the auditory-only vocalization also varies according to a statistical distribution. Whenever, both facial motion and the vocalization are presented together, the stimulus that is processed faster in a given trial determines the response time. As the RT distributions typically overlap with one another, slow processing times are removed. Thus, RTs to redundant stimuli are always faster than those for the single stimuli. A standard way to test whether this principle can explain RT data is to use the race model inequality <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, which posits that the cumulative RT distribution for the redundant stimuli never exceeds the sum of the RT distributions for the unisensory stimuli. That is, if <italic>F<sub>AV</sub></italic> (t), <italic>F<sub>V</sub></italic> (t) and <italic>F<sub>A</sub></italic> (t) are the estimated cumulative distributions (CDF) of the RTs for the three different modalities<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e001" xlink:type="simple"/></disp-formula>then one cannot rule out race models as an explanation for the facilitation of RT. On the other hand, if this inequality is violated in a given data set, then parallel processing cannot completely account for the benefits observed for multisensory stimuli and an explanation based on co-activation is necessary. We computed the CDFs of our conditions and then computed the difference between the actual CDF of the audiovisual condition and the CDF predicted by the race model. The maximum positive point of this difference was taken as the index of violation, <italic>R</italic>. Positive values of <italic>R</italic> means that the race model is rejected. If this value is 0, then the race model is upheld.</p>
        <sec id="s2g1">
          <title>Tests of race model violations for single subjects</title>
          <p>We needed to test whether these race model violations were statistically significant. To test the violations of the race model on a single subject basis, we compared the true value of R to one computed by a bootstrap method that performs artificial iterations of our multisensory experiments. A variant of this conservative bootstrap method using the area instead of the maximum was originally outlined in seminal studies of behavioral multisensory integration in humans <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[<italic>51</italic>]</xref>. The entire experiment (i.e. AV, V, A) was simulated 10,000 times for each monkey and each SNR with the following steps carried out for each simulation. Simulated RTs for the auditory-only and visual-only conditions were obtained by randomly sampling (with replacement) from the observed distributions of auditory and visual-only RTs for that subject. For the audiovisual condition, in accordance with the race model, the simulated RT was obtained by sampling two RTs (one from the visual RT distribution, one from the auditory RT distribution), and then selecting the minimum of the two values. In addition, as the maximum value for the race model inequality is obtained when the times for the two racers are strongly negatively correlated with one another, we adopted a criterion that introduces a strong negative correlation in the simulation. This was accomplished by randomly selecting an RT from one distribution (e.g., visual-only) that is at a percentile P in the distribution and then sampling the auditory distribution with percentile 100 – P. Thus fast responses to visual motion are paired with slow responses to auditory vocalizations and vice versa, providing a strong and conservative test of the race model. After sampling the appropriate number of trials for each condition, the CDFs and the index of violation (R<sub>b</sub>) for each experiment was obtained. The distribution of R<sub>b</sub> was then compared to the real value of R. A p-value as a test of significance was obtained by computing the number of simulated values (R<sub>b</sub>) that exceeded the true value R estimated from the data.</p>
        </sec>
        <sec id="s2g2">
          <title>Tests of race model violations for humans</title>
          <p>When data from multiple subjects are available (as in the case of our human data), instead of using bootstrap models, one can test the race model by obtaining the cumulative distribution functions for all subjects and evaluating the consistency of violations of the race model across subjects. To achieve this, we again adapted a permutation test recently developed for testing the violations of the race model. Here we briefly describe the procedure; further details are provided in <xref ref-type="bibr" rid="pcbi.1002165-Gondan1">[<italic>62</italic>]</xref>. We first estimated for each participant, the cumulative distribution function for each SNR for auditory, visual and audiovisual conditions, we then computed the difference d for each participant i as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e002" xlink:type="simple"/></disp-formula>where <italic>F</italic> denotes the estimate of the true cumulative distribution function. When <italic>d<sup>i</sup></italic> is positive it means that the race model is violated and when d<sup>i</sup> is negative it means that the pattern of RTs are not different from those predicted by a race model. If <italic>d<sup>i</sup></italic> follows an approximate normal distribution and <italic>s<sub>d</sub></italic> is the standard deviation estimated from the sample data, one can then use a one sample t-test to test if this is significant at a single <italic>t</italic>. However, a more robust way that controls for type I error for multiple time points is to use a permutation test. The assumption for this permutation test is as follows. If the race model holds, then the sign of d will be random across participants with some participants showing violations of the race model (<italic>d</italic> is positive) and some showing no violation of the race model (<italic>d</italic> is negative). The average across participants would then be equal to 0. We therefore used a permutation test where we randomly multiplied the participant-specific value of <italic>d</italic> by +1 or -1 (with probability 0.5) and then calculated the t values for multiple time points (280 – 350 ms, into 8 bins). The test statistic for multiple time points then corresponds to the maximum of the t values (so-called t<sub>max</sub> statistic). We repeated this permutation 10001 times to generate a distribution of the test statistic and computed a p value by identifying the proportion of permuted test statistics that exceeded the true value of the test statistic.</p>
        </sec>
      </sec>
      <sec id="s2h">
        <title>Superposition models of audiovisual integration</title>
        <p>Several models of audiovisual integration have been proposed over the years, but superposition models are simple and possess considerable explanatory power. Here we briefly describe the model, and detailed explanations are available elsewhere <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich1">[64]</xref>. We first consider the case of single modality trials (visual or auditory). We assume that the onset of the stimulus (i.e. visual mouth motion or the auditory vocalization) induces a neural renewal counting process (for examples, action potentials or spikes, but it could be any event which is counted) that counts up to a critical number of events, <italic>c</italic>. The assumption is that, as soon as a critical number of events, <italic>c</italic>, have been registered at some decision mechanism, a response execution process, <italic>M</italic>, which consumes an amount of time with a mean <italic>µ<sub>M</sub></italic><sub>,</sub> is started. The main postulate of the superposition model is that in the audiovisual condition the renewal processes generated by either the visual and the auditory signals are superposed, thereby reducing the waiting time for the critical count. Specifically, if <italic>N<sub>V</sub></italic> (t) and <italic>N<sub>A</sub></italic> (t) are themselves counting processes for the visual-only and auditory-only conditions, and the two stimuli are presented simultaneously, that is with 0 lag, then the new process for the audiovisual stimulus is given as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e003" xlink:type="simple"/></disp-formula></p>
        <p>It is immediately apparent that this audiovisual process will reach the critical level of <italic>c</italic> counts faster than the individual auditory and visual processes. If the auditory-only and visual-only stimuli are presented with a lag of say τ, as in our case with visual mouth motion preceding the auditory vocalization by τ milliseconds, then the process becomes,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e004" xlink:type="simple"/></disp-formula></p>
        <p>To specify this model fully and test and fit to data, one must specify an inter-arrival distribution. Usually this is assumed to be exponential in nature that leads to a homogenous Poisson counting process. For τ  =  0, the waiting time for the <italic>c<sup>th</sup></italic> event is well defined and follows a gamma distribution with mean <italic>c</italic>/λ and variance <italic>c</italic>/λ<sup>2</sup>, where λ (λ&gt;0) is the intensity parameter of the Poisson process. For example, the auditory and visual-only RT would then be<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e005" xlink:type="simple"/></disp-formula></p>
        <p>The mean audiovisual RT would be given by the following simple expression. It is the waiting time for the <italic>c</italic><sup>th</sup> with the visual and auditory rates summed and is given as follows.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e006" xlink:type="simple"/></disp-formula></p>
        <p>When this model is to be applied when there are differences in the SOAs, that is, τ&gt;0, the waiting time for the <italic>c</italic><sup>th</sup> event is no longer gamma distributed and instead follows a more complicated distribution. Fortunately, this model has been completely explicated and published expressions are already available <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich1">[64]</xref>. The audiovisual RT in this case is the expected value of the waiting time to the <italic>c</italic><sup>th</sup> count.</p>
        <sec id="s2h1">
          <title>Prediction of audiovisual RTs</title>
          <p>We assumed that across all conditions and intensities, the values of c and µ<sub>M</sub> are constant across conditions. These assumptions are reasonable for the following reasons. A constant c means the criterion is constant across conditions and intensities. A constant value of µ<sub>M</sub> means that the average motor time is constant across all these conditions. For the five auditory-only and visual-only conditions, we estimated for each of the different SNRs, using the real values of RTs, RT<sub>A</sub>, RT<sub>V</sub>, the values of the rate parameters λ<sub>A</sub> and λ<sub>V</sub> for each of the different intensities then are given by the following expressions.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e007" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.e008" xlink:type="simple"/></disp-formula><italic>i</italic> denotes the <italic>i<sup>t</sup></italic><sup>h</sup> SNR or mouth opening size. We then estimated the audiovisual RTs according to the equation for RT<sub>AV</sub>, we allowed the value of <italic>c</italic> to vary from (2 to 50) and µ<sub>M</sub> from (150 to 450) and found the best values of <italic>c</italic> and µ<sub>M</sub> which minimized the least square error between the true audiovisual R and the predicted value of the RT according to the equation. We chose these values of <italic>c</italic> and µ<sub>M</sub> and then created simulated audiovisual RTs.</p>
        </sec>
        <sec id="s2h2">
          <title>Simulation of auditory, visual and audiovisual reaction times</title>
          <p>To derive simulated auditory, visual and audiovisual RTs and benefit curves, we first found the relationship between λ<sub>Ai</sub> and SNR at each of the different measured SNRs and then interpolated to get smooth estimates of λ<sub>A</sub> and SNR. We performed a similar analysis to get an estimate of λ<sub>V</sub> and values of <italic>c</italic> and <italic>M</italic> were derived from the true data. We then again estimated the value of the audiovisual RT and derived the benefit values in a similar way as the real data.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <p>Monkeys were trained, and humans were asked, to detect auditory, visual or audiovisual vocalizations embedded in noise as fast and as accurately as possible. This task was similar to a redundant signals paradigm <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, and was designed to approximate a natural face-to-face vocal communication event in close encounters. In such settings, the vocal components of the communication signals are degraded by environmental noise. The face and its motion, on the other hand, are usually perceived clearly. In the task, monkeys responded to ‘coo’ calls that are affiliative vocalizations commonly produced by macaque monkeys in a variety of contexts [65,66, <xref ref-type="fig" rid="pcbi-1002165-g001">Figure 1A</xref>]; humans were asked to detect the acoustically similar vowel sound /u/ (<xref ref-type="fig" rid="pcbi-1002165-g001">Figure 1B</xref>). All vocalizations had five different levels of sound intensity and were embedded in a constant background noise. The signal-to-noise ratio (SNR) ranged from −10 dB to +22 dB relative to a background noise of 63 dB. For dynamic faces, we used computer-generated monkey and human avatars (<xref ref-type="fig" rid="pcbi-1002165-g001">Figures 1C, D</xref>). The use of avatars allowed us to restrict facial motion to the mouth region, ensure constant lighting and background, and to parameterize the size of the mouth opening while keeping eye and head positions constant. The degree of mouth-opening was in accordance with the intensity of the associated vocalization: greater sound intensity was coupled to larger mouth openings by the dynamic face (<xref ref-type="fig" rid="pcbi-1002165-g001">Figure 1E</xref>). Two coos and two /u/ sounds were paired with two monkey and human avatars, respectively, and this pairing was kept constant. Furthermore, species-stimuli pairings were kept constant: monkeys only saw and heard monkey vocalizations, and humans only saw and heard human vocalizations. Previous psychophysical and fMRI experiments have successfully used computer-generated human avatars to probe the processing of audiovisual speech <xref ref-type="bibr" rid="pcbi.1002165-Munhall1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Wright1">[67]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ouni1">[68]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Munhall2">[69]</xref>.</p>
      <fig id="pcbi-1002165-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Stimuli and Task structure for monkeys and humans.</title>
          <p><bold>A:</bold> Waveform and spectrogram of coo vocalizations detected by the monkeys. <bold>B:</bold> Waveform and spectrogram of the /u/ sound detected by human observers. <bold>C:</bold> Frames of the two monkey avatars at the point of maximal mouth opening for the largest SNR. <bold>D:</bold> Frames of the two human avatars at the point of maximal mouth opening for the largest SNR. <bold>E</bold>: Frames with maximal mouth opening from one of the monkey avatars for three different SNRs of + 22 dB, +5 dB and – 10 dB. <bold>F:</bold> Task structure for monkeys. An avatar face was always on the screen. Visual, auditory and audiovisual stimuli were randomly presented with an inter stimulus interval of 1–3 seconds drawn from a uniform distribution. Responses within a 2 second window after stimulus onset were considered to be hits. Responses in the inter-stimulus interval are considered to be false alarms and led to timeouts.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g001" xlink:type="simple"/>
      </fig>
      <p>During the task, one avatar face would be continuously on the screen for a block of trials (n = 60); the background noise was also continuous (<xref ref-type="fig" rid="pcbi-1002165-g001">Figure 1F</xref>). In the “visual only (V)” condition, this avatar would move its mouth without any corresponding auditory component; that is, it silently produced a coo for monkey avatars or an /u/for human avatars. In the “auditory-only (A)” condition, the vocalization normally paired with the <italic>other</italic> avatar (which is not on the screen) is presented with the <italic>static</italic> face of the avatar. Finally, in the “audiovisual (AV)” condition, the avatar moves its mouth accompanied by the corresponding vocalization and in accordance (degree of mouth opening) with its intensity. Each condition (V, A, or AV) was presented after a variable interval (drawn from a uniform distribution) between 1 and 3 seconds. Subjects indicated the detection of an event (visible mouth motion, auditory signal or both) by pressing a lever (monkeys) or a key (humans) within 2 seconds following the onset of the stimulus. Every 60 trials, a brief pause was imposed, followed by a new block in which the avatar face was switched on the screen, and the identity of the coo or /u/ sound used for the auditory-only condition was switched as well.</p>
      <sec id="s3a">
        <title>Accuracy and reaction time</title>
        <p>We measured the accuracy of the monkeys and humans detecting vocalizations in the audiovisual, auditory-only and visual-only conditions. <xref ref-type="fig" rid="pcbi-1002165-g002">Figure 2A</xref> shows the detection performance of Monkey 1 averaged over all sessions (both coo calls) as a function of SNR for the three conditions of interest. In the case of the visual-only condition, the size of mouth opening has a constant relationship with the auditory SNR and it is thus shown on the same x-axis. In the auditory-only condition, as the coo call intensity increased relative to the background noise, the detection accuracy of the monkey improved. In contrast, modulating the size of the mouth opening in the visual-only condition had only a weak effect on the detection accuracy. Finally, the detection accuracy for audiovisual vocalizations was mildly enhanced relative to the visual-only condition and with very little modulation as a function of the SNR. The same pattern was seen for Monkey 2 (<xref ref-type="fig" rid="pcbi-1002165-g002">Figure 2B</xref>). When the data was pooled over all the SNRs, accuracy was significantly better for both monkeys in the audiovisual condition compared to either unisensory condition (paired t-tests, <bold>Monkey 1</bold>: AV vs V, t (47)  = 3.77, p&lt;.001, AV vs A, t (47)  = 19.94, p&lt;.001; <bold>Monkey 2</bold>: AV vs V, t (47)  = 15.85, p&lt;.001, AV vs A, t (47)  = 8.1, p&lt;.001).</p>
        <fig id="pcbi-1002165-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Detection accuracy for monkeys and humans.</title>
            <p><bold>A</bold>: Average accuracy across all sessions (n = 48) for Monkey 1 as a function of the SNR for the unisensory and multisensory conditions. Error bars denote standard error of mean across sessions. X-axes denote SNR in dB. Y-axes denote accuracy in %. <bold>B</bold>: Average accuracy across all sessions (n = 48) for Monkey 2 as a function of the SNR for the unisensory and multisensory conditions. Conventions as in A. <bold>C</bold>: Accuracy as a function of the SNR for the unisensory and multisensory conditions from a single human subject. Conventions as in A. <bold>D</bold>: Average accuracy across all human subjects (n = 6) as a function of the SNR for the unisensory and multisensory conditions. Conventions as in A.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g002" xlink:type="simple"/>
        </fig>
        <p>This general pattern was replicated in humans (n = 6). <xref ref-type="fig" rid="pcbi-1002165-g002">Figure 2C</xref> shows the performance of a single human observer on this same task detecting the /u/ sound. Excluding the lowest SNR value in the auditory-only condition, accuracy is almost at ceiling for all three stimulus conditions. The average accuracy over the 6 human subjects as a function of SNR is shown in <xref ref-type="fig" rid="pcbi-1002165-g002">Figure 2D</xref>. Performance pooled across all SNRs was maximal for the audiovisual condition and was enhanced relative to the auditory-only condition (t (5)  = 2.71, p = 0.04). It was not significantly enhanced relative to the visual-only condition (t (5) = 0.97, p = 0.37). The lack of enhancement relative to the visual-only condition is likely because the visual-only performance itself was close to ceiling for humans.</p>
        <p>In both species, the similarities in detection accuracy for visual-only and audiovisual conditions (<xref ref-type="fig" rid="pcbi-1002165-g002">Figures 2A–D</xref>) suggest that they were perhaps not integrating auditory and visual signals but instead may have adopted a unisensory (visual) strategy. According to this strategy, they used visible mouth motion only for both the visual and audiovisual conditions, and used the sound only when forced to do so in the auditory-only condition. We therefore examined the reaction times (RTs) to distinguish between a unisensory versus an integration strategy. <xref ref-type="fig" rid="pcbi-1002165-g003">Figures 3A, B</xref> show the mean RT as a function of the SNR and modality computed by pooling RT data from all the sessions for Monkeys 1 and 2. RTs for the auditory-only vocalization increased as the SNR decreased (i.e. the sound was harder to hear relative to the background). In contrast, RTs to the visual-only condition only increased weakly with an increase in the mouth opening size — a result consistent with the accuracy data. Although the audiovisual accuracy was only modestly better than the visual-only accuracy (<xref ref-type="fig" rid="pcbi-1002165-g002">Figure 2A,B</xref>), audiovisual RTs decreased relative to both auditory-only and visual-only RTs for several SNR levels. To illustrate, a non-parametric ANOVA (Kruskal-Wallis) computed for Monkey 1, which compared the ranks of the RTs for the auditory-only, visual-only and audiovisual conditions for the highest SNR (+22dB), was significant (χ<sup>2</sup> = 490.91, p&lt;.001). Post-hoc Mann-Whitney U tests revealed that the RT distribution for the audiovisual condition was significantly different from the auditory-only distribution and the visual-only distribution for all SNRs; that is, RTs in the audiovisual condition were faster than visual and auditory RTs. In Monkey 2, the audiovisual RT distribution was different from the auditory-only distribution for all SNRs (p&lt;0.001), and was significantly different from the visual-only distribution for all but the lowest SNR (−10 dB, p = 0.68). It is notable that at the highest SNR (+22 dB; largest mouth opening), the RTs of both monkeys seem more like the auditory-only RTs, while at the lowest SNR (−10 dB; smallest mouth opening), the RTs seem to be more similar to the visual-only RTs.</p>
        <fig id="pcbi-1002165-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>RTs to Auditory, visual and audiovisual vocalizations.</title>
            <p><bold>A</bold>: Mean RTs obtained by pooling across all sessions as a function of SNR for the unisensory and multisensory conditions for Monkey 1. Error bars denote standard error of the mean estimated using bootstrapping. X-axes denote SNR in dB. Y-axes depict RT in milliseconds. <bold>B</bold>: Mean RTs obtained by pooling across all sessions all sessions as a function of SNR for the unisensory and multisensory conditions for Monkey 2.Conventions as in A. <bold>C</bold>: Mean RTs obtained by pooling across all sessions as a function of SNR for the unisensory and multisensory conditions for a single human subject. Conventions as in A. <bold>D</bold>: Average RT across all human subjects as a function of SNR for the unisensory and multisensory conditions. Error bars denote SEM across subjects. Conventions as in A.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g003" xlink:type="simple"/>
        </fig>
        <p>Humans also show a RT benefit in the audiovisual versus unisensory conditions, with a similar, but not identical, pattern to that observed in monkeys. <xref ref-type="fig" rid="pcbi-1002165-g003">Figure 3C</xref> shows the average RTs of a single human subject as a function of the SNR. Similar to monkeys, decreasing the SNR of the auditory-only condition leads to an increase in the RTs, and RTs for the visual-only condition were only weakly modulated by the size of the mouth opening. For a range of SNRs, the audiovisual RTs were faster than auditory- and visual-only RTs. <xref ref-type="fig" rid="pcbi-1002165-g003">Figure 3D</xref> shows the average RTs over all 6 subjects. Paired t-tests comparing audiovisual RTs to auditory-only and visual-only RTs reveal that they were significantly different in all but the lowest SNR condition (p = 0.81 for the −10 dB condition, p&lt;0.05 for all other conditions, df  = 5). Though the RT patterns from human participants seem dissimilar to the monkey RT patterns (e.g., in monkeys the auditory-RT curve crossed the visual-only RT curve but for humans there was no cross over), we can show that the two species are adopting a similar strategy by exploring putative mechanisms. We do so in the next sections.</p>
      </sec>
      <sec id="s3b">
        <title>A race model cannot explains benefits for audiovisual vocalizations</title>
        <p>Our analysis of RTs rules out the simple hypothesis that monkeys and humans are defaulting to a unisensory strategy (using visual in all conditions except when forced to use auditory information). Another hypothesis is that a “race” mechanism is at play <xref ref-type="bibr" rid="pcbi.1002165-Raab1">[59]</xref>. A race mechanism postulates parallel channels for visual and auditory signals that compete with one another to terminate in a motor or decision structure and thereby trigger the behavioral response. We chose to test this model to ensure that the observers were actually integrating the faces and vocalizations of the avatar. A simple physiological correlate of such a model would be the existence of independent processing pathways for the visual mouth motion and an independent processing pathway for the auditory vocalization. In the race scenario, there would be no cross-talk between these signals. Race models are extremely powerful and are often used to show independent processing in discrimination tasks <xref ref-type="bibr" rid="pcbi.1002165-Churchland1">[70]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Yang1">[71]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Roitman1">[72]</xref>. In our task, independent processing would mean that in the decision structure, two populations of neurons received either auditory or visual input. These two independent populations count spikes until a threshold is reached; the population that reaches threshold first triggers a response. Such a model can lead to a decrease in the RTs for the multisensory condition, not through integration, but through a statistical mechanism: the mean of the minimum of two distributions is always less than or equal to the minimum of the mean of two distributions.</p>
        <p><xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4A</xref> shows a simulation of this race model. The audiovisual distribution, if it is due to a race mechanism, is obtained by taking the minimum of the two distributions and will have a lower mean and variance compared to the individual auditory and visual distributions. Typically, to test if a race model can explain the data, cumulative distributions of the RTs (<xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4B</xref>) are used to reject the so-called race model inequality <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>. The inequality is a strong, conservative test and provides an upper bound for the benefits provided by any class of race models. Reaction times faster than this upper bound mean that the race model cannot explain the pattern of RTs for the audiovisual condition; the RT data would therefore necessitate an explanation based on integration.</p>
        <fig id="pcbi-1002165-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Race models cannot explain audiovisual RTs.</title>
            <p><bold>A:</bold> Schematic of a race mechanism for audiovisual integration. The minimum of two reaction time distributions is always faster and narrower than the individual distributions. <bold>B</bold>: Race models can be tested using the race model inequality for cumulative distributions. The graph shows the cumulative distributions for the density functions shown in A along with the race model inequality. <bold>C</bold>: Cumulative distributions of the auditory, visual and audiovisual RTs from monkey 1 for one SNR (+5dB) and one inter stimulus interval (ISI) window (1000 – 1400 ms) along with the prediction provided by the race model. X-axes depict RT in milliseconds. Y-axes depict the cumulative probability. <bold>D</bold>: Violation of race model predictions for real and simulated experiments as a function of RT for the same SNR and ISI shown in C. X-axes depict RT in milliseconds. Y-axes depict difference in probability units. <bold>E</bold>: Average race model violation as a function of SNR for the ISI of 1000 to 1400 ms for Monkey 1. Error bars denote the standard error estimated by bootstrapping. * denotes significant race model violation using the bootstrap test shown in D. <bold>F</bold>: Average race model violation across human subjects as a function of SNR. X-axes depict SNR; y-axes depict the amount of violation of the race model in probability units. * denotes significant race model violation according to the permutation test.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g004" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4C</xref> plots the cumulative distributions for RTs collected in the intermediate SNR level and for ISIs between 1000 and 1400 ms for Monkey 1; the prediction from the race model is shown in grey. We used this ISI interval because, in monkeys only, the ISI influenced the pattern of audiovisual benefits (<bold>see <xref ref-type="supplementary-material" rid="pcbi.1002165.s007">Text S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002165.s002">Figure S2</xref></bold>). Maximal audiovisual benefits were for ISIs in the 1000–1400 ms range. The cumulative distribution of audiovisual RTs is faster than can be predicted by the race model for multiple regions of RT distribution, suggesting that the RTs cannot be fully explained by this model. To test whether this violation was statistically significant, we compared the violation from the true data to one using conservative bootstrap estimates. Several points for the true violation were much larger than the violation values estimated by bootstrapping (<xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4D</xref>). Audiovisual RTs are therefore not explained by a race model. For the entire range of SNRs and this ISI for the monkeys, maximal race model violations were seen for the intermediate to high SNRs (+5, +13 and + 22 dB; <xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4E</xref>). For the softer SNRs (−10,−4 dB), a race model could not be rejected as an explanation. The amount of race model violation for the entire range of ISIs and SNRs is provided in <bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s003">Figure S3</xref></bold>. For both monkeys, longer ISIs resulted in weaker violations of the race model and rarely did the p-values from the bootstrap test reach significance.</p>
        <p>For humans, we observed similar robust violations of the race model. <xref ref-type="fig" rid="pcbi-1002165-g004">Figure 4F</xref> shows the average amount of race model violation across subjects as a function of SNR. Since humans showed much less dependence on the ISI, we did not bin the data as we did for monkeys. Similar, to monkeys, maximal violation of the race model was seen for loud and intermediate SNRs. For 3 out of the 5 SNRs (+22, +13, +5 dB), a permutation test comparing maximal race model violation to a null distribution was significant (p&lt;0.05). In conclusion, for both monkeys and humans, a race model cannot explain the pattern of RTs at least for the loud and intermediate SNRs.</p>
        <p>These results strongly suggest that monkeys do <italic>integrate</italic> visual and auditory components of vocalizations and that they are similar to humans in their computational strategy. In the next sections, we therefore leveraged these behavioral data and attempt to identify a homologous mechanism(s) that could explain this pattern of results. Our search was based on the assumption that classical principles and mechanisms of multisensory integration <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove2">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hershenson1">[73]</xref>, originally developed for simpler stimuli, could potentially serve as starting hypotheses for a mechanism mediating the behavioral integration of the complex visual and auditory components of vocalizations.</p>
      </sec>
      <sec id="s3c">
        <title>Mechanism/Principle 1: Principle of inverse effectiveness</title>
        <p>The first mechanism we tested was whether the integration of faces and voices demonstrated in our data followed the “principle of inverse effectiveness” <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>. This idea, originally developed to explain neurophysiological data, suggests that maximal benefits from multisensory integration should occur when the stimuli are themselves maximally impoverished <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein4">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein5">[75]</xref>. That is, the weaker the magnitude of the unisensory response, the greater would be the gain in the response due to integration. In our case with behavior, this principle makes the following prediction. As the RTs and accuracy were the poorest for the lowest auditory SNR, the benefit of multisensory integration should be maximal when the lowest auditory SNR is combined with the corresponding mouth opening. Our metric for multisensory benefit was defined as the speedup for the audiovisual RT relative to the fastest mean RT in response to the unisensory signal (regardless of whether it was the auditory- or visual-only condition). The principle of inverse effectiveness would thus predict greater reaction time benefits with decreasing SNR for both monkeys and humans. <xref ref-type="fig" rid="pcbi-1002165-g005">Figures 5A and B</xref> plot this benefit as a function of SNR for Monkeys 1 and 2. For monkeys, the maximal audiovisual benefit occurs for intermediate SNRs. The corresponding pattern of benefits for humans is shown in <xref ref-type="fig" rid="pcbi-1002165-g005">Figure 5C</xref>. For humans, this benefit increases as the SNR increases and starts to flatten for the largest SNRs. This pattern of benefits reveals that the maximal audiovisual RT benefits <underline>do not</underline> occur at the lowest SNRs. This is at odds with the principle of inverse effectiveness <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>. If our results had followed this principle, then the maximal benefit relative to both unisensory conditions should have occurred at the lowest SNR (lowest sound intensity coupled with smallest mouth opening). Neither monkey nor human RTs followed this principle and therefore it cannot be a homologous mechanism mediating the integration of faces and voices in primates.</p>
        <fig id="pcbi-1002165-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Benefit in RT for the audiovisual condition compared to unisensory conditions.</title>
            <p><bold>A</bold>: Mean benefit in RT for the audiovisual condition relative to the minimum of mean visual-only and auditory-only RTs for monkey 1. X-axes depict SNR. Y-axes depict the benefit in milliseconds. Error bars denote standard errors estimated through bootstrap. <bold>B</bold>: Mean benefit in RT for the audiovisual condition relative to the minimum of mean visual-only and auditory-only RTs for monkey 2. Conventions as in A. <bold>C:</bold> Mean benefit in RT for the audiovisual condition relative to the minimum of the mean visual-only and auditory-only conditions averaged across subjects. Axis onventions as in A. Error bars denote standard errors of the mean.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g005" xlink:type="simple"/>
        </fig>
        <p>One potential caveat is that we are testing the principle of inverse effectiveness using absolute reaction time benefits whereas the original idea was developed using proportional referents. Thus, we re-expressed the benefits as a percent gain relative to the minimum of the auditory and visual reaction times for each SNR. We observed that, even when converted to a percent benefit relative to the minimum reaction time for each SNR, the inverted U-shape pattern of gains for monkeys (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s004">Figures S4A</xref>, B</bold>), as well as increasing gain with SNR for humans (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s004">Figure S4C</xref></bold>), was replicated. Thus, whether one uses raw benefits or a proportional measure, RT benefits from combining visual and auditory signals could not be explained by invoking the principle of inverse effectiveness.</p>
      </sec>
      <sec id="s3d">
        <title>Mechanism/Principle 2: Physiological synchrony</title>
        <p>If inverse effectiveness could not explain our results, then what other mechanism(s) could explain the patterns of reaction time benefits? Monkey performance at intermediate SNRs (where the maximal benefits were observed; <xref ref-type="fig" rid="pcbi-1002165-g003">Figures 3A, B</xref>), the visual-only and auditory-only reaction time values were similar to each other. Similarly, for humans at intermediate to large SNRs (where maximal benefits were observed for humans), the visual-only and auditory-only reaction time values were similar to one another. This suggests a simple timing principle: the closer the visual-only and auditory-only RTs are to one another, the greater is the multisensory benefit. A similar behavioral result has been previously observed in the literature, albeit with simpler stimuli, and a mechanism explaining this behavior was (somewhat confusingly) dubbed “physiological synchrony” <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hershenson1">[73]</xref>. According to this mechanism, developed in a psychophysical framework, performance benefits for the multisensory condition are modulated by the degree of overlap between the theoretical neural activity patterns (response magnitude and latency) elicited by the two unisensory stimuli <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hershenson1">[73]</xref>. Maximal benefits occur during “synchrony” of these activity patterns; that is, when the latencies overlap. To put it another way, maximal RT benefits will occur when the visual and auditory inputs arrive almost at the same time.</p>
        <p>To test this idea, we transformed the benefit curves shown in <xref ref-type="fig" rid="pcbi-1002165-g005">Figures 5A-C</xref> by plotting the benefits as a function of the absolute value of the difference between visual-only and auditory-only RTs. That is, instead of plotting the benefits as a function of SNR (as in <xref ref-type="fig" rid="pcbi-1002165-g005">Figures 5A–C</xref>), we plotted them as a function of the difference between the visual-only and auditory-only RTs for each SNR. If our intuition is correct, then the closer the auditory- and visual-only RTs are (i.e., the smaller the difference between them), then the greater would be the benefit. <xref ref-type="fig" rid="pcbi-1002165-g006">Figure 6A</xref> plots the benefit in reaction time as a function of the absolute difference between visual- and auditory-only RT for monkeys 1 &amp; 2. The corresponding plot for humans is shown in <xref ref-type="fig" rid="pcbi-1002165-g006">Figure 6B</xref>. By and large, as the difference between RTs increase, the benefit for the audiovisual condition decreases with the minimum benefit occurring when visual- and auditory-only RTs differ by more than 100 to 200 milliseconds. Thus, physiological synchrony can serve as a homologous mechanism for the integration of faces and voices in both monkeys and humans.</p>
        <fig id="pcbi-1002165-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Time window of integration.</title>
            <p><bold>A:</bold> Reaction time benefits for the audiovisual condition in monkeys decrease as the absolute difference between visual-only and auditory-only RTs decrease. X-axes depict difference in ms. Y-axes the benefit in milliseconds. <bold>B:</bold> Reaction time benefits for the audiovisual condition in humans also decrease as the absolute difference between visual-only and auditory-only RTs decrease. Conventions as in A. <bold>C</bold>: Mean benefit in the RT for the audiovisual condition relative to minimum of the auditory-only and visual-only RTs as a function of the difference between mean visual-only and auditory-only RTs for monkey 1. X-axes depict reaction time difference in ms. Y-axes depict benefit in ms. <bold>D</bold>: Mean benefit in the RT for the audiovisual condition relative to minimum of the auditory-only and visual-only RTs as a function of the difference between mean visual-only and auditory-only RTs for humans. Conventions as in C.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g006" xlink:type="simple"/>
        </fig>
        <p>Although the original formulation of the principle suggested “synchrony”, it seemed too restrictive. The reaction time data—at least for integrating faces and voices—suggest that there is a range of reaction time differences over which multisensory benefits can be achieved. That is, there is a “window of integration” within which multisensory benefits emerge. We use the term “window of integration” as typically defined in studies of multisensory integration: It is the time span within which auditory and visual response latencies must fall so that their combination leads to behavioral or physiological changes significantly different from responses to unimodal stimuli. Such windows have been demonstrated in physiological <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Meredith1">[76]</xref> as well as in psychophysical studies of multisensory integration<xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove2">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Colonius1">[77]</xref>. To explore the extent of this “window of integration”, we elaborated upon the analysis shown in <xref ref-type="fig" rid="pcbi-1002165-g006">Figures 6A and B</xref> to the whole dataset of sessions and SNRs. For all the sessions and SNRs (48 sessions and 5 SNRs for 2 monkeys), we computed a metric that was the difference between the mean visual-only and auditory-only RTs. This gave us 480 values where there was a difference between visual and auditory RTs and, corresponding to this value, the benefit for the audiovisual condition. After sorting and binning these values, we then plotted the audiovisual benefit as a function of the difference between the mean visual-only and auditory-only RTs. <xref ref-type="fig" rid="pcbi-1002165-g006">Figure 6C</xref> shows this analysis for monkeys. Only in an intermediate range, where differences between unisensory RTs are around 100 – 200 ms, is the audiovisual benefit non-zero—with a maximal benefit occurring at approximately 0 ms. In addition, this window is not symmetrical around zero. It is 200 ms long when visual RTs are faster than auditory RTs and around 100 ms long when auditory-only RTs are faster than visual-only RTs. We repeated the same analysis for humans and the results are plotted in <xref ref-type="fig" rid="pcbi-1002165-g006">Figure 6D</xref>. For humans, a similar window exists: when visual reaction times are faster than auditory reaction times then the window is approximately 160 ms long. We could not determine the extent of the window because, in humans, auditory RTs were never faster than visual RTs.</p>
        <p>To summarize, combining visual and auditory cues leads to a speedup in the detection of audiovisual vocalizations relative to the auditory-only and visual-only vocalizations. Our analysis of the patterns of benefit for the audiovisual condition reveals that maximal benefits do not follow a principle of inverse effectiveness. However, the principle of physiological synchrony that incorporates a time window of integration provided a better explanation of these results.</p>
      </sec>
      <sec id="s3e">
        <title>Mechanism/Principle 3: A linear superposition model</title>
        <p>The principle of physiological synchrony with a time window of integration provides an insight into the processes that lead to the integration of auditory and visual components of communication signals. The issue however is that although this insight can be used to predict behavior, it does not have any immediate mechanistic basis. We therefore sought a computational model that could plausible represent the neural basis for these behavioral patterns. We specified two criteria for the model based on our results. First, audiovisual RTs should be faster than auditory- and visual-only RTs. Second, it should be consistent with, and perhaps subsume, the principle of physiological synchrony with a time window of integration—benefits accrued by combining visual and auditory cues should occur when the visual- and auditory-only RTs are almost equal to one another. If these two criteria are validated, then the model would be a straightforward homologous mechanism.</p>
        <p>Superposition models are one class of integration models that could incorporate our criteria <xref ref-type="bibr" rid="pcbi.1002165-Schwarz1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich1">[64]</xref>. According to these models, activation from different sensory channels is linearly combined until it reaches a criterion/threshold and thereby triggers a response. We will use a model formulation based on counters for simplicity <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>. According to this counter model, the onset of a stimulus would lead to a sequence of events occurring randomly over time. Let N (<italic>t</italic>) denote the number of events that have occurred by time <italic>t</italic> after stimulus presentation. After the number of counts reaches a criterion, <italic>c</italic>, it triggers a response. Let us assume that there are separate counters for visual and auditory conditions, N<sub>V</sub> (<italic>t</italic>) and N<sub>A</sub> (<italic>t</italic>). During the audiovisual condition, a composite counter, N<sub>AV</sub> (<italic>t</italic>)  =  N<sub>A</sub> (<italic>t</italic>) + N<sub>V</sub> (<italic>t</italic>), comprised of both the visual and auditory signals, counts to the criterion, <italic>c</italic> (<xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7A</xref>). This composite, multisensory counter would reach the criterion faster than either of the unisensory counters alone. <xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7B</xref> shows that a computer simulation of a counter composed of superposed activity from both visual and auditory cues would reach criterion faster than the unisensory ones alone.</p>
        <fig id="pcbi-1002165-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002165.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Superposition models can explain audiovisual RTs.</title>
            <p><bold>A:</bold> Illustration of the superposition model of audiovisual integration. Ticks denote events which are registered by the individual counters. <bold>B:</bold> Simulated individual trials from the audiovisual, auditory-only and visual-only counters. X-axes denotes RT in milliseconds, y-axes the number of counts. <bold>C:</bold> Simulated and raw mean RTs using parameters estimated from the visual-only and auditory-only conditions for monkey 1. X-axes denote simulated SNR in dB. Y-axes denote RTs in ms estimated using a superposition model. The raw data are shown as circles along with error bars. The estimated data for the audiovisual condition is shown in a red line. <bold>D:</bold> Simulated benefits for audiovisual RTs relative to the auditory-only and visual only conditions as a function of SNR. Note how the peak appears at intermediate SNRs. <bold>E:</bold> Simulated and raw mean RTs using parameters estimated from the real visual- and auditory-only conditions for humans. X-axes denote simulated SNR in dB. Y-axes denote RTs in ms estimated using a superposition model. The raw data are shown as circles along with errorbars. The estimated data for the audiovisual condition is shown in red. Conventions as in C. <bold>F:</bold> Simulated benefits for human audiovisual RTs relative to the auditory-only and visual only conditions as a function of SNR, note how as in real data, benefit increases with increasing SNR and plateaus for large SNRs. Conventions as in D.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.g007" xlink:type="simple"/>
        </fig>
        <p>Using the RT data from Monkey 1, we set the parameters of the superposition model for the auditory- and visual-only RTs and then used the model to estimate the audiovisual RTs (<xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7C</xref>). From this, the model produced audiovisual RTs that were faster than both the auditory-only and visual-only RTs—like the pattern of results we observed for monkeys (<xref ref-type="fig" rid="pcbi-1002165-g003">Figures 3A, B</xref>). As <xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7C</xref> shows, except for the lowest SNR, there is a good one to one correspondence between the model's prediction of audiovisual RTs and the actual raw data. Thus, this model can at least generate the patterns of reaction times observed in response to audiovisual vocalization.</p>
        <p>We next estimated the benefits in RT for the audiovisual condition relative to the visual-only and auditory-only condition from the simulated model (<xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7D</xref>). The benefit curve has the same inverted U-shaped profile as the real patterns of benefit shown in <xref ref-type="fig" rid="pcbi-1002165-g005">Figure 5A</xref>. We repeated this analysis for the human RTs and the pattern of results is shown in <xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7E–F</xref>. <xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7E</xref> shows the predicted reaction time of the average participant as a function of SNR along with actual data. The predicted reaction times are very similar to the actual RTs observed for humans in <xref ref-type="fig" rid="pcbi-1002165-g003">Figure 3D</xref>. As with the monkey behavioral data, the fits performed worst for the softest SNR. Like the benefit patterns shown in <xref ref-type="fig" rid="pcbi-1002165-g005">Figure 5C</xref>, the benefit for the AV condition increases as SNR increases (<xref ref-type="fig" rid="pcbi-1002165-g007">Figure 7F</xref>). This replication by the model of the pattern of monkey and human data—faster audiovisual RTs and maximal benefit when visual and auditory RTs are well matched—suggests that a superposition model is a viable homologous mechanism.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>The goal of our study was threefold. First, do monkeys integrate the visual and auditory components of vocalizations? Second, is monkey behavior similar to that of humans perfoming an identical task? Third, is there a homologous mechanism for the processing of audiovisual communication signals? We trained monkeys and asked humans to detect vocalizations by monkey and human avatars, respectively, in a noisy background. We measured their accuracy and reaction times. We found that monkeys do integrate the visual and auditory components of vocalizations (as measured by faster reaction times for the audiovisual relative to the unisensory conditions). Similar speedups in reaction times were observed also for human subjects. Rejection of the race model demonstrated that the behavioral patterns must be explained by an integrative process (one requiring the use of both unisensory channels together to drive behavioral change), and not one based on competing independent unisensory channels. We then tested whether classical principles of multisensory integration could serve as homologous mechanisms for the integration of faces and voices. The “principle of inverse effectiveness” failed to explain the data for either primate species. Both monkey and human RTs were better explained by the principle of “physiological synchrony” that incorporated a time window of integration. We found that a simple computational model positing the linear superposition of activity induced by visual and auditory cues could explain the pattern of results in monkeys as well as humans. Critically, its explanatory power was such that it could explain the small differences in behavior observed for monkeys and humans. Furthermore, the superposition model is completely consistent with the principles of physiological synchrony with a time window of integration. The superposition model, therefore, is an excellent candidate for a homologous mechanism used by monkeys and humans to integrate faces and voices.</p>
      <sec id="s4a">
        <title>Monkeys like humans can integrate visual and auditory components of vocalizations</title>
        <p>Monkeys and humans share many homologous mechanisms for the production of vocalizations <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar1">[22]</xref>. In humans, these production mechanisms deform the face in such a manner that facial motion enhances the detection and discrimination of vocal sounds by receivers <xref ref-type="bibr" rid="pcbi.1002165-Sumby1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ross1">[7]</xref>, . Often this enhanced behavior takes the form of decreased reaction times to audiovisual versus unisensory presentations of speech <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Besle1">[9]</xref>. While nonhuman primates could theoretically use the same or very similar facial motion to enhance their auditory perception, there has been no evidence of this to date. Several studies demonstrated that, like human infants, monkeys and apes can match facial expressions to vocal expressions <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Evans1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Izumi1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Parr1">[29]</xref>, and that eye movement patterns generated by viewing vocalizing conspecifics is similar between monkeys and humans <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar3">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-VatikiotisBateson1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Lansing1">[32]</xref>. None of these nonhuman primate studies, however, demonstrated a <italic>behavioral advantage</italic> for perceiving audiovisual vocalizations over unisensory expressions. Demonstration of such an advantage is necessary to invoke the hypothesis that a multisensory integration mechanism for communication signals is homologous across species. In the current study, we provide the first demonstration that monkeys exhibit a behavioral advantage for audiovisual versus unisensory presentations of vocalizations. The patterns of both accuracy and reaction time benefits were similar to humans performing an identical task.</p>
        <p>Although we have emphasized throughout the similarities in the patterns of behavior for monkeys and humans, it is important to note that there were also differences. The most important difference was that humans were consistently faster for the visual-only vocalization compared to the auditory-only vocalization across the range of auditory intensities. Monkeys, on the other hand, responded faster to some auditory-only conditions versus visual-only conditions across the range of intensities. These differences ultimately led to differences in the amount of integration. Such differences could potentially arise due to the differences in auditory stimuli (/u/ sounds in humans vs coo calls in monkeys) or the amount of attentional engagement. We have suggested acoustic equivalence of “coos” and /u/ vocalizations, but they are not communicatively equivalent. Coos are common vocalizations in monkeys with behavioral significance including a positive emotional valence. In contrast, the /u/ sound we used with humans does not have any behavioral significance. With regard to engagement, we trained our monkeys using standard operant conditioning techniques. This meant the use of timeouts as negative reinforcement whenever the monkeys made false alarms. As a result, when compared to human performance, monkeys may adopt a more conservative criterion for the detection of these sounds to avoid false alarms. Despite these caveats, it is worth emphasizing that positing a linear superposition of visual and auditory signals reconciled these dissimilar results from the two species.</p>
        <p>Two other design features of our study are worth pointing out before we discuss the broader implications of our results. First, we used a fixed delay between the mouth motion and the onset of the vocalization. Under natural conditions, delays between onset of mouth opening and sound onset, which we term time-to-voice (TTV), are wide ranging and can vary from utterance to utterance and speaker to speaker <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran1">[5]</xref>. At the neural level, different TTVs modulate the degree of integration in local field potential signals recorded from the upper bank of the superior temporal sulcus of monkeys <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran2">[40]</xref>. Thus, how this variable would affect behavioral integration of faces and voices in monkeys and humans is not tested in our experiments or in any other study.</p>
        <p>A second design feature that we used consisted of the presence of a static face on the screen during the auditory-only vocalization. This face was also identity-incongruent with the auditory vocalization. Thus, both of these features could potentially <italic>slow down</italic> auditory-only RTs by creating confusion: the face doesn't move when it should during a vocalization and/or the face doesn't match the identity of the voice. However, we believe this concern is mitigated by the more naturalistic conditions that our design mimics and more pressing problems that it avoids. Our paradigm is naturalistic in the following sense: faces in noisy, cocktail-party like scenarios do not appear and disappear. Furthermore, monkeys like humans can recognize indexical cues in vocalizations (cues that indicate body size, age, identity, etc) and match them to faces <xref ref-type="bibr" rid="pcbi.1002165-Ghazanfar7">[82]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sliwa1">[83]</xref>. Thus, in our paradigm, it is not odd to hear one individual's voice while seeing another individual's face, a typical occurrence under natural conditions. The key to the face-voice integration is combining motion of the face to the correct, corresponding voice. If we did not present a static face during the auditory-only condition and observed an audiovisual benefit, then the benefits could be attributed to differences in overall attention or arousal (a frequent criticism of physiological studies of AV integration). Moreover, if we adopted a design where audiovisual vocalizations involved the sudden onset of a face followed by its mouth motion, then any RT benefits for audiovisual compared to auditory-only vocalizations would be uninterpretable: we could not be sure if it was due to the integration of facial motion with the sound or from the integration of the sound with the sudden onset of the face.</p>
        <p>Whatever influences our design may actually have on our participants' RTs; we can model the outcome of hypothetically faster RTs that may arise with a study design that did not use a static, incongruent face in the auditory-only conditions. Since our data demonstrate that the principle of physiological synchrony with a time window of integration, we can actually perform a thought experiment to see what would happen if our auditory RTs are sped up. Simply put, the result would be that the point at which visual and auditory RT curves cross will be at a different SNR and this point of crossing would be the new point of maximal integration. <bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s005">Figure S5</xref></bold> shows that if we sped up all auditory RTs by 40, 80 and 120 ms in the model relative to the original data, the point of maximal integration shifts to lower SNRs.</p>
      </sec>
      <sec id="s4b">
        <title>Integration of complex versus simple multisensory signals</title>
        <p>We demonstrated that combining visual mouth motion with auditory vocalizations speeds up reaction times in monkeys and humans. Faster reaction times to multisensory signals compared to unisensory signals are a frequent outcome in human psychophysical studies <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller3">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Raab1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Corneil1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich2">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller4">[86]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Todd1">[87]</xref>. The first such demonstration, nearly a hundred years ago, showed that there was a speedup in responses for bi- and tri-modal stimuli compared to unimodal stimuli <xref ref-type="bibr" rid="pcbi.1002165-Todd1">[87]</xref>. Since then, this seminal result has been replicated in a variety of settings almost always with the use of simple stimuli <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich2">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Gondan2">[88]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Colonius2">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Giray1">[90]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Plat1">[91]</xref>. In particular, shortened reaction times are observed in response to multimodal stimuli using both saccades and lever presses as dependent measures <xref ref-type="bibr" rid="pcbi.1002165-Diederich2">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hughes1">[92]</xref>. Physiologically, there are similar results. Neurons in the superior colliculus of anaesthetized cats respond faster to audiovisual compared to auditory and visual stimuli <xref ref-type="bibr" rid="pcbi.1002165-Rowland1">[93]</xref>. Our results confirm that similar behavioral advantages exist when combining the visual and auditory components of complex social signals encountered in everyday settings.</p>
        <p>While there are certainly similarities in the integration processes for simple and complex signals like speech, there are also differences. An important issue which has been repeatedly demonstrated is that there are differences in the window of integration for simple versus complex stimuli<xref ref-type="bibr" rid="pcbi.1002165-Hirsh1">[94]</xref>. For the integration of simple stimuli, tolerance of asynchrony between visual and auditory cues is very small leading to a narrow window of integration <xref ref-type="bibr" rid="pcbi.1002165-Hirsh1">[94]</xref>. In contrast, for speech stimuli, observers are able to tolerate very large asynchronies and still bind them into a common percept<xref ref-type="bibr" rid="pcbi.1002165-Dixon1">[47]</xref>. We return to this issue later in the Discussion.</p>
      </sec>
      <sec id="s4c">
        <title>Behavioral detection of audiovisual communication signals cannot be explained by the principle of inverse effectiveness</title>
        <p>For both monkeys and humans, we found that the maximal benefit obtained by combining visual and auditory cues was for <italic>intermediate</italic> values of SNR. This is at odds with the principle of inverse effectiveness <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>. This idea was originally formulated in the context of electrophysiological experiments and suggests that the maximal benefit (greater proportional response magnitude) from multisensory stimulus inputs would be achieved by combining visual and auditory cues that, individually, elicit weak responses. Support for the inverse effectiveness rule is also evident at the behavioral level in both monkeys and humans in detection tasks involving simple stimuli <xref ref-type="bibr" rid="pcbi.1002165-Diederich2">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hughes1">[92]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Frens1">[95]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Bell1">[96]</xref>. If this principle held true for detecting vocalizations, then we would have observed maximal reaction time savings for the lowest SNR, with the benefit decreasing with increasing SNR. On the contrary, monkeys' detection of vocalizations generated a non-monotonic curve with peak multisensory benefits occurring at intermediate SNRs. For humans, the multisensory benefit increased with increasing SNRs. Thus, for the multisensory integration of vocalizations (with reaction times as a behavioral measure), neither in monkeys nor in humans does the principle of inverse effectiveness explain the behavior. Other results from the speech processing literature support our assertion. For example, in studies of speech intelligibility, maximal benefits gained by integration of auditory speech with visual speech are found when the auditory speech is presented in an intermediate, versus high, level of noise <xref ref-type="bibr" rid="pcbi.1002165-Ross1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ma1">[81]</xref>. Similarly, the McGurk effect occurs even under clear listening conditions (i.e., noisy signals aren't required to generate the illusory percept) <xref ref-type="bibr" rid="pcbi.1002165-McGurk1">[10]</xref>, and vision can boost the comprehension of extended auditory passages even under excellent listening conditions <xref ref-type="bibr" rid="pcbi.1002165-Reisberg1">[97]</xref>.</p>
        <p>As mentioned before, there are several studies which claim to support this principle in behavior <xref ref-type="bibr" rid="pcbi.1002165-Stein5">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich2">[85]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hughes1">[92]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Frens1">[95]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Bell1">[96]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Jiang1">[98]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Senkowski1">[99]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Cappe1">[100]</xref>, so why do we not see support for the principle of inverse effectiveness in our data or in other studies <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Gondan2">[88]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hughes1">[92]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Giard1">[101]</xref>? We think that this principle is sensitive to the way multisensory stimuli are parameterized and tested in different experiments. In particular, the choice of stimuli, levels of intensity and the pairing of stimuli could all affect whether this principle will be apparent in the resultant data. To illustrate what we mean, we tested two hypothetical scenarios, where inverse effectiveness can be observed using RTs and compare it to a scenario resembling our experimental data. For each scenario, we constructed auditory and visual RTs to have a certain profile with respect to different intensity levels. Then, given that the superposition model is an excellent explanation of our RT data, as well as RTs to simple stimuli<xref ref-type="bibr" rid="pcbi.1002165-Schwarz1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Gondan1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller4">[86]</xref>, we used it to simulate the expected audiovisual reaction times for these same intensity levels. We then examined if the multisensory benefits were consistent with the principle of inverse effectiveness or not. The first scenario is a case wherein RTs to both senses increase with decreases in intensity level, but at every intensity level, they are still roughly equal to one another (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6A</xref></bold>). In this scenario, RTs to visual and auditory stimuli increase with decreasing intensity and visual and auditory RTs are largely similar at every intensity level. Keeping with multisensory integration, audiovisual RTs are faster than both auditory-only and visual-only RTs. Critically, in line with our intuition, the multisensory benefit increases with the decrease in SNR — and is thus consistent with the principle of inverse effectiveness (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6B</xref></bold>).</p>
        <p>We can also outline a second scenario where this principle would be observed to be in action. This is the case when the stimuli are such that the RT of one modality approaches the RT of the other modality only for the lowest intensity levels. <bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6C</xref></bold> shows a simulation of this scenario. The auditory-only RTs are much faster than the visual-only RTs for the highest intensity levels. However, as the stimulus intensity decreases, the auditory- and visual-only RTs approach each other. Again, audiovisual RTs are faster than auditory- and visual–only RTs. Like the previous scenario, as intensity decreases, the benefit increases and is thus consistent with the principle of inverse effectiveness (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6D</xref></bold>). A recent study showing support for inverse effectiveness had visual and auditory RTs closely following this scenario <xref ref-type="bibr" rid="pcbi.1002165-Senkowski1">[99]</xref>. The third scenario is one that is a simulation of our data (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6E</xref></bold>). In this case, visual RTs do not change much with intensity level, but auditory RTs increase with a decrease in intensity. Audiovisual RTs are again faster than auditory and visual-only RTs. Critically, these data result in a pattern of benefits that is non-monotonic and takes the form of an inverted U; it is <underline>not</underline> consistent with the principle of inverse effectiveness (<bold><xref ref-type="supplementary-material" rid="pcbi.1002165.s006">Figure S6F</xref></bold>).</p>
        <p>In summary, given that the superposition model is an excellent fit to data, simulations of this model using the scenarios above suggest that observing the principle of inverse effectiveness in behavior is to some extent dependent upon the way the parameters of the stimuli that are used in an experiment. Different multisensory stimuli (speech versus non-speech) as well as the choice of intensity levels are bound to have different effects on multisensory benefit. Thus, the principle of inverse effectiveness may be operational only under some situations. We would however note that, this framework of superposition only explains the inconsistencies about inverse effectiveness in RT output. A similar careful analysis is needed to explain accuracy of subjects as well as performance in tasks such as localization <xref ref-type="bibr" rid="pcbi.1002165-Stein5">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Jiang1">[98]</xref>.</p>
      </sec>
      <sec id="s4d">
        <title>Audiovisual integration of communication signals adheres to the principle of “physiological synchrony” with a time window of integration</title>
        <p>We showed that maximal benefits from integration of visual and auditory components of communication signals occurred when the reaction times to visual and auditory cues are themselves very similar to one another. This is consistent with the idea of “physiological synchrony”, a principle proposed to explain behavioral data. The principle of physiological synchrony was first formulated based on psychophysical experiments using punctate, simple stimuli <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hershenson1">[73]</xref>. In these experiments, it was noted that maximal multisensory benefits occurred when the stimulus-onset asynchrony between visual and auditory stimuli was adjusted to be equal to the difference between visual-only and auditory-only RTs. That is, “synchrony” was defined by theoretical neurophysiological activity (with reaction times as a proxy) rather than physical synchrony defined by the stimulus-onset asynchrony. According to this idea, performance benefits for the multisensory condition are modulated by the degree of temporal overlap between the theoretical neurophysiological activity patterns elicited by the two unisensory stimuli <xref ref-type="bibr" rid="pcbi.1002165-Miller1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Hershenson1">[73]</xref>. Maximal benefits occur during synchrony of these neural activity patterns; that is, when their latencies over-lap.</p>
        <p>It is worth repeating that this notion of physiological synchrony is a behavioral construct derived by considering RTs. RTs are a simple but powerful metric for indexing this behavior. However, they are the output of a complex mixture of sensory processing, motor preparation, temporal expectation, attention and other cognitive processes. Thus, the physiological synchrony mechanism, although it explains patterns of behavior using RTs to sensory stimuli does not necessarily predict that the integration is occurring in a purely sensory circuit. The neural locus where integration is taking place is not known. Sensory, premotor and/or motor circuits involved in multisensory processing are very likely all involved in generating behavioral responses during this task.</p>
        <p>We found that there was a time window within which differences in reaction times between visual and auditory signals could lead to integration. This notion of a “temporal window of integration” is a recurring concept in behavioral and neurophysiological experiments of multisensory integration <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove2">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Meredith1">[76]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Colonius1">[77]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Corneil1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Colonius2">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Musacchia1">[102]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Navarra1">[103]</xref>. For example, participants perceive the McGurk effect when the stimulus-onset asynchrony between visual and auditory cues is in a window approximately 400 milliseconds wide, beyond which the illusion disappears <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove2">[48]</xref>. Similarly, studies of orienting responses to audiovisual stimuli using saccades show that speedup of saccadic RTs occur in a variety of experimental settings within a time window of 150–250 ms <xref ref-type="bibr" rid="pcbi.1002165-Colonius1">[77]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Corneil1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Colonius2">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich3">[104]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich4">[105]</xref>. Finally, neurophysiologically, maximal integration in multisensory neural responses in the superior colliculus is observed when the stimulus onset asynchrony is adjusted such that the discharge patterns to visual and auditory signals themselves overlap with each other <xref ref-type="bibr" rid="pcbi.1002165-Meredith1">[76]</xref>.</p>
      </sec>
      <sec id="s4e">
        <title>A linear superposition model of integration is a putative homologous mechanism</title>
        <p>We showed that a simple computational model of integration—a linear superposition model—explained the behavioral patterns observed for the integration of audiovisual vocalizations by monkeys and humans. The main tenet of this model is that the information from the two unisensory channels is integrated at a specific processing stage by the linear summation of channel-specific activity patterns. Superposition models have been successfully used to predict the reaction times of observers in other multisensory detection tasks, albeit with much simpler stimuli <xref ref-type="bibr" rid="pcbi.1002165-Schwarz1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Gondan1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Schwarz2">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Diederich1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Miller4">[86]</xref>. Physiologically, support for this principle was suggested in studies of the sensitivity of multisensory neurons in superior colliculus <xref ref-type="bibr" rid="pcbi.1002165-Meredith1">[76]</xref>. Our results suggest that this model can be readily extended to the integration of visual and auditory components of vocalizations, at least during behaviors involving speeded detection. Indeed, invoking this mechanism reconciled the observed dissimilarity in RTs from monkeys and humans. In addition, it automatically subsumes the principle of physiological synchrony and generates appropriately asymmetric time windows of integration. Whether this model works well for other tasks such as multisensory spatial orientation <xref ref-type="bibr" rid="pcbi.1002165-Stein5">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Jiang1">[98]</xref>, is an open question. Nevertheless, for the task presented in this study, i.e. the detection of vocalizations in noise, it is a parsimonious homologous mechanism.</p>
        <p>That a linear, additive model could provide a good explanation for the detection of audiovisual vocalizations might seem irreconcilable with typical notions of multisensory integration that emphasize “super-additivity” or non-linear responses <xref ref-type="bibr" rid="pcbi.1002165-Stein2">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Stein3">[50]</xref>. Recent studies, however, report that multisensory neurons can integrate their inputs in an additive manner both in terms of spiking activity [See for e.g. 50,52,106], as well at the level of synaptic input <xref ref-type="bibr" rid="pcbi.1002165-Skaliora1">[107]</xref>. Our emphasis on the superposition model as a homologous mechanism has another important implication. First, there are a remarkable number of nodes on which visual and auditory inputs that are sensitive to faces and voices, respectively, could converge. Any or all of these sites could be responsible for the behavioral advantage we report here. For example, neurons in the amygdala and association areas such as the upper bank of STS and prefrontal cortex respond to both the visual and auditory components of vocalizations. In some cases, we know that they integrate these vocalization-related cues <xref ref-type="bibr" rid="pcbi.1002165-Chandrasekaran2">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Barraclough1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Sugihara1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Avillac1">[108]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Kohler1">[109]</xref>—at least during the passive reception of these signals. For example, in keeping with the linear superposition model we posited here, approximately 7% of ventrolateral prefrontal cortical neurons integrate visual and auditory components of vocalizations linearly <xref ref-type="bibr" rid="pcbi.1002165-Sugihara1">[43]</xref>.</p>
        <p>The superposition model subsumes the time window of integration. The basis of superposition models is that they require activity patterns to overlap with one another and add together to generate benefits. Thus, activity patterns that overlap with one another have a higher probability of leading to integration, whereas activity patterns that do not overlap will not lead to integration. This implies that the measured window of integration is going to depend on the inherent statistics of the visual and auditory signals and the response profiles to the two signals in some neural structure on which they converge. The narrowness and the latency of these response profiles will thus determine the window of integration. Thus, in any given experiment, choices of the strength and duration of these visual and auditory signals would automatically result in corresponding changes in latencies and response profiles. A flash is highly likely to be processed in primary visual cortex and a moving face through a combination of face- and motion-sensitive neural structures. A similar argument can be made for auditory stimuli. Thus, unless the response profile(s) in some integrative structure(s) mediating detection of these various stimuli are identical, the windows of integration are bound to be different for simple stimuli such as flashes and tone pips versus more complex audiovisual vocalizations and speech signals. This might be a partial explanation for one of the best known findings in the multisensory literature — asymmetric broad windows for speech <xref ref-type="bibr" rid="pcbi.1002165-Dixon1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-vanWassenhove2">[48]</xref>, versus the small windows for simple stimuli <xref ref-type="bibr" rid="pcbi.1002165-Hirsh1">[94]</xref> .</p>
        <p>Finally, the superposition model is similar in many respects to a Bayesian model of bimodal integration. For example, in models developed by Ernst and colleagues <xref ref-type="bibr" rid="pcbi.1002165-Ernst1">[110]</xref>, <xref ref-type="bibr" rid="pcbi.1002165-Ernst2">[111]</xref>, maximal benefit due to bimodal discrimination occurs when the difficulty of each modality is roughly equated <xref ref-type="bibr" rid="pcbi.1002165-Alais1">[112]</xref>. This is remarkably similar to the notion of physiological synchrony. Thus, Bayesian models could, presumably, be adapted to explain the reaction times and would also subsume the time window of integration concept. However, the advantage the superposition model has is that its neurophysiological implementation is immediately apparent. Bayesian models, in contrast, are usually more abstract, and it is unclear what their neural implementation would look like.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002165.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><bold>Hit rate and False Alarm rate of one monkey.</bold> <bold>A:</bold> Hit rate and false alarm rate from a single session. X-axes denotes bin number. Y-axes denotes percentage. <bold>B:</bold> Hit rate and false alarm rate from another session. Conventions as in A. <bold>C:</bold> Average hit rate and false alarm rate across all sessions for monkey 1. X-axes depict different types of metrics (Hit rate, False Alarm rate). Y-axes depict percentage. Error bars denote twice the standard error.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p><bold>Reaction time as a function of the inter stimulus interval for monkeys and humans.</bold> <bold>A</bold>: Mean reaction times of monkey 1 as a function of the inter-stimulus interval for the three conditions of interest, auditory-only, visual-only and audiovisual for the +5 dB SNR condition. X-axes depict ISI in milliseconds. Y-axes depict reaction times in milliseconds. Error bars denote standard errors estimated using a bootstrap method. <bold>B</bold>: Mean gain in reaction times for monkey 1 for the audiovisual condition relative to the auditory-only condition as a function of the inter-stimulus interval for three SNRs (+22 dB, +5 dB, −10 dB). X-axes depict ISI in milliseconds. Y-axes depict the gain in reaction times in milliseconds. Error bars denote standard error of the mean estimated using a bootstrap method. <bold>C</bold>: Same analysis as A but for Monkey 2. <bold>D</bold>: Same analysis as B but for Monkey 2. <bold>E</bold>: Same analysis as A for human subjects. <bold>F</bold>: Same analysis as B for human subjects.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p><bold>Race models cannot explain audiovisual reaction times for monkeys.</bold> <bold>A</bold>: Contour plot of the violation of race model as a function of both ISI and SNR for the reaction time data from Monkey 1. X-axes depict ISI in milliseconds. Y-axes depict SNR. Color bar denotes the amount of violation of the race model. <bold>B</bold>: Same analysis as A, but for monkey 2. Conventions are as in A.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p><bold>Proportional benefit in RT for the audiovisual condition compared to unisensory conditions.</bold> <bold>A</bold>: Mean benefit in RT for the audiovisual condition expressed as a percentage of speedup relative to the minimum of mean visual-only and auditory-only RTs for monkey 1. X-axes depict SNR. Y-axes depict the benefit in percent. Error bars denote standard errors estimated through bootstrap. <bold>B</bold>: Same analysis as in A except for Monkey 2. Conventions as in A. <bold>C:</bold> Same analysis as in A except averaged across human subjects. Conventions as in A.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <p><bold>Speeding up auditory RTs shifts the point of maximal integration.</bold> <bold>Left panels –</bold> Simulated reaction times to visual, auditory and audiovisual conditions. X-axes depict SNR in dB. Y-axes the RT in milliseconds. From top to bottom, auditory-only RTs are sped up by 0, 40, 80 and 120 ms. <bold>Right panels –</bold> Benefit in simulated RT for the audiovisual compared to the auditory and visual-only conditions as a function of SNR for the scenarios shown in the left panel. X-axes depict SNR in dB. Y-axes the benefit in RT in milliseconds. One can see that the point of maximal integration and the shape of the benefit curve changes.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s006" xlink:type="simple">
        <label>Figure S6</label>
        <caption>
          <p><bold>Scenarios demonstrating the sensitivity of the principle of inverse effectiveness to stimulus characteristics.</bold> <bold>A, C, E –</bold> Simulated reaction times to visual, auditory and audiovisual conditions. X-axes depict SNR in dB. Y-axes the RT in milliseconds. <bold>B,D,F –</bold> Benefit in simulated RT for the audiovisual compared to the auditory and visual-only conditions as a function of SNR for the scenarios shown in A,C,E. X-axes depict SNR in dB. Y-axes the benefit in RT in milliseconds. Note how in the first two scenarios (A,C and B, D) the simulated benefits follow the principle of inverse effectiveness. However for the last scenario (E,F), the simulated benefits do not follow it.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002165.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002165.s007" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p><bold>Effect of ISI on auditory, visual and audiovisual RTs.</bold> A section describing how audiovisual integration in RTs are modulated by the inter-stimulus interval.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Shawn Steckenfinger for creating avatars of vocalizing macaque monkeys, Lauren Kelly for the expert care of our monkey subjects, and Daniel Takahashi, Hjalmar Turesson, Stephen Shepherd and Chris Davis for helpful comments and discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002165-Ohala1">
        <label>1</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ohala</surname><given-names>J</given-names></name></person-group>             <year>1975</year>             <article-title>Temporal Regulation of Speech.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Fant</surname><given-names>G</given-names></name><name name-style="western"><surname>Tatham</surname><given-names>MAA</given-names></name></person-group>             <source>Auditory Analysis and Perception of Speech</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Academic Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Summerfield1">
        <label>2</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Summerfield</surname><given-names>Q</given-names></name></person-group>             <year>1987</year>             <article-title>Some preliminaries to a comprehensive account of audio-visual speech perception.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Dodd</surname><given-names>B</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>R</given-names></name></person-group>             <source>Hearing by Eye: The Psychology of Lipreading</source>             <publisher-loc>Hillsdale, New Jersey</publisher-loc>             <publisher-name>Lawrence Earlbaum</publisher-name>             <fpage>3</fpage>             <lpage>51</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Summerfield2">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Summerfield</surname><given-names>Q</given-names></name></person-group>             <year>1992</year>             <article-title>Lipreading and Audio-Visual Speech Perception.</article-title>             <source>Philos Trans Roy Soc B</source>             <volume>335</volume>             <fpage>71</fpage>             <lpage>78</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Yehia1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yehia</surname><given-names>H</given-names></name><name name-style="western"><surname>Rubin</surname><given-names>P</given-names></name><name name-style="western"><surname>Vatikiotis-Bateson</surname><given-names>E</given-names></name></person-group>             <year>1998</year>             <article-title>Quantitative association of vocal-tract and facial behavior.</article-title>             <source>Speech Comm</source>             <volume>26</volume>             <fpage>23</fpage>             <lpage>43</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Chandrasekaran1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name><name name-style="western"><surname>Trubanova</surname><given-names>A</given-names></name><name name-style="western"><surname>Stillittano</surname><given-names>S</given-names></name><name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group>             <year>2009</year>             <article-title>The natural statistics of audiovisual speech.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000436</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sumby1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sumby</surname><given-names>WH</given-names></name><name name-style="western"><surname>Pollack</surname><given-names>I</given-names></name></person-group>             <year>1954</year>             <article-title>Visual Contribution to Speech Intelligibility in Noise.</article-title>             <source>J. Acoust Soc Am</source>             <volume>26</volume>             <fpage>212</fpage>             <lpage>215</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ross1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ross</surname><given-names>LA</given-names></name><name name-style="western"><surname>Saint-Amour</surname><given-names>D</given-names></name><name name-style="western"><surname>Leavitt</surname><given-names>VM</given-names></name><name name-style="western"><surname>Javitt</surname><given-names>DC</given-names></name><name name-style="western"><surname>Foxe</surname><given-names>JJ</given-names></name></person-group>             <year>2007</year>             <article-title>Do You See What I Am Saying? Exploring Visual Enhancement of Speech Comprehension in Noisy Environments.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>1147</fpage>             <lpage>1153</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-vanWassenhove1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Wassenhove</surname><given-names>V</given-names></name><name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name></person-group>             <year>2005</year>             <article-title>Visual speech speeds up the neural processing of auditory speech.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>102</volume>             <fpage>1181</fpage>             <lpage>1186</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Besle1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Besle</surname><given-names>J</given-names></name><name name-style="western"><surname>Fort</surname><given-names>A</given-names></name><name name-style="western"><surname>Delpuech</surname><given-names>C</given-names></name><name name-style="western"><surname>Giard</surname><given-names>M-H</given-names></name></person-group>             <year>2004</year>             <article-title>Bimodal speech: early suppressive visual effects in human auditory cortex.</article-title>             <source>Eur. J. Neurosci</source>             <volume>20</volume>             <fpage>2225</fpage>             <lpage>2234</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-McGurk1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McGurk</surname><given-names>H</given-names></name><name name-style="western"><surname>MacDonald</surname><given-names>J</given-names></name></person-group>             <year>1976</year>             <article-title>Hearing lips and seeing voices.</article-title>             <source>Nature</source>             <volume>264</volume>             <fpage>746</fpage>             <lpage>748</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Rosenblum1">
        <label>11</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rosenblum</surname><given-names>LD</given-names></name></person-group>             <year>2005</year>             <article-title>Primacy of Multimodal Speech Perception.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Pisoni</surname><given-names>DB</given-names></name><name name-style="western"><surname>Remez</surname><given-names>RE</given-names></name></person-group>             <source>The Handbook of Speech Perception: Blackwell publishing</source>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Burrows1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burrows</surname><given-names>AM</given-names></name><name name-style="western"><surname>Waller</surname><given-names>BM</given-names></name><name name-style="western"><surname>Parr</surname><given-names>LA</given-names></name></person-group>             <year>2009</year>             <article-title>Facial musculature in the rhesus macaque (Macaca mulatta): evolutionary and functional contexts with comparisons to chimpanzees and humans.</article-title>             <source>J. Anat</source>             <volume>215</volume>             <fpage>320</fpage>             <lpage>334</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Huber1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Huber</surname><given-names>E</given-names></name></person-group>             <year>1930</year>             <article-title>Evolution of facial musculature and cutaneous field of trigeminus.</article-title>             <source>Part I. Q. Rev Biol</source>             <volume>5</volume>             <fpage>133</fpage>             <lpage>188</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Huber2">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Huber</surname><given-names>E</given-names></name></person-group>             <year>1930</year>             <article-title>Evolution of facial musculature and cutaneous field of trigeminus. Part II.</article-title>             <source>Q. Rev Biol</source>             <volume>5</volume>             <fpage>389</fpage>             <lpage>437</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sherwood1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sherwood</surname><given-names>CC</given-names></name></person-group>             <year>2005</year>             <article-title>Comparative anatomy of the facial motor nucleus in mammals, with an analysis of neuron numbers in primates.</article-title>             <source>Anat Rec. A. Discov Mol Cell Evol Biol</source>             <volume>287A</volume>             <fpage>1067</fpage>             <lpage>1079</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sherwood2">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sherwood</surname><given-names>CC</given-names></name><name name-style="western"><surname>Hof</surname><given-names>PR</given-names></name><name name-style="western"><surname>Holloway</surname><given-names>RL</given-names></name><name name-style="western"><surname>Semendeferi</surname><given-names>K</given-names></name><name name-style="western"><surname>Gannon</surname><given-names>PJ</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>Evolution of the brainstem orofacial motor system in primates: a comparative study of trigeminal, facial, and hypoglossal nuclei.</article-title>             <source>J. Hum Evol</source>             <volume>48</volume>             <fpage>45</fpage>             <lpage>84</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sherwood3">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sherwood</surname><given-names>CC</given-names></name><name name-style="western"><surname>Holloway</surname><given-names>RL</given-names></name><name name-style="western"><surname>Erwin</surname><given-names>JM</given-names></name><name name-style="western"><surname>Hof</surname><given-names>PR</given-names></name></person-group>             <year>2004</year>             <article-title>Cortical orofacial motor representation in old world monkeys, great apes, and humans - II. Stereologic analysis of chemoarchitecture.</article-title>             <source>Brain Behav Evolut</source>             <volume>63</volume>             <fpage>82</fpage>             <lpage>106</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sherwood4">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sherwood</surname><given-names>CC</given-names></name><name name-style="western"><surname>Holloway</surname><given-names>RL</given-names></name><name name-style="western"><surname>Erwin</surname><given-names>JM</given-names></name><name name-style="western"><surname>Schleicher</surname><given-names>A</given-names></name><name name-style="western"><surname>Zilles</surname><given-names>K</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Cortical orofacial motor representation in old world monkeys, great apes, and humans - I. Quantitative analysis of cytoarchitecture.</article-title>             <source>Brain Behav Evolut</source>             <volume>63</volume>             <fpage>61</fpage>             <lpage>81</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Andrew1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Andrew</surname><given-names>RJ</given-names></name></person-group>             <year>1962</year>             <article-title>The origin and evolution of the calls and facial expressions of the primates.</article-title>             <source>Behaviour</source>             <volume>20</volume>             <fpage>1</fpage>             <lpage>109</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Hauser1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hauser</surname><given-names>MD</given-names></name><name name-style="western"><surname>Evans</surname><given-names>CS</given-names></name><name name-style="western"><surname>Marler</surname><given-names>P</given-names></name></person-group>             <year>1993</year>             <article-title>The Role of Articulation in the Production of Rhesus-Monkey, Macaca-Mulatta, Vocalizations.</article-title>             <source>Anim Behav</source>             <volume>45</volume>             <fpage>423</fpage>             <lpage>433</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Partan1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Partan</surname><given-names>SR</given-names></name></person-group>             <year>2002</year>             <article-title>Single and Multichannel facial composition: Facial Expressions and Vocalizations Of Rhesus Macaques(Macaca Mulata).</article-title>             <source>Behaviour</source>             <volume>139</volume>             <fpage>993</fpage>             <lpage>1027</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Rendall</surname><given-names>D</given-names></name></person-group>             <year>2008</year>             <article-title>Evolution of human vocal production.</article-title>             <source>Curr Biol</source>             <volume>18</volume>             <fpage>R457</fpage>             <lpage>R460</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Kuhl1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name><name name-style="western"><surname>Meltzoff</surname><given-names>AN</given-names></name></person-group>             <year>1982</year>             <article-title>The bimodal perception of speech in infancy.</article-title>             <source>Science</source>             <volume>218</volume>             <fpage>1138</fpage>             <lpage>1141</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Patterson1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Patterson</surname><given-names>ML</given-names></name><name name-style="western"><surname>Werker</surname><given-names>JF</given-names></name></person-group>             <year>2002</year>             <article-title>Infants' ability to match dynamic phonetic and gender, information in the face and voice.</article-title>             <source>J. Exp Child Psychol</source>             <volume>81</volume>             <fpage>93</fpage>             <lpage>115</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Patterson2">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Patterson</surname><given-names>ML</given-names></name><name name-style="western"><surname>Werker</surname><given-names>JF</given-names></name></person-group>             <year>2003</year>             <article-title>Two-month-old infants match phonetic information in lips and voice.</article-title>             <source>Dev Sci</source>             <volume>6</volume>             <fpage>191</fpage>             <lpage>196</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar2">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2003</year>             <article-title>Facial expressions linked to monkey calls.</article-title>             <source>Nature</source>             <volume>423</volume>             <fpage>937</fpage>             <lpage>938</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Evans1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Evans</surname><given-names>TA</given-names></name><name name-style="western"><surname>Howell</surname><given-names>S</given-names></name><name name-style="western"><surname>Westergaard</surname><given-names>GC</given-names></name></person-group>             <year>2005</year>             <article-title>Auditory-visual cross-modal perception of communicative stimuli in tufted capuchin monkeys (Cebus apella).</article-title>             <source>J. Exp. Psychol. Anim. B</source>             <volume>31</volume>             <fpage>399</fpage>             <lpage>406</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Izumi1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Izumi</surname><given-names>A</given-names></name><name name-style="western"><surname>Kojima</surname><given-names>S</given-names></name></person-group>             <year>2004</year>             <article-title>Matching vocalizations to vocalizing faces in a chimpanzee (Pan troglodytes).</article-title>             <source>Anim Cogn</source>             <volume>7</volume>             <fpage>179</fpage>             <lpage>184</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Parr1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Parr</surname><given-names>LA</given-names></name></person-group>             <year>2004</year>             <article-title>Perceptual biases for multimodal cues in chimpanzee (Pan troglodytes) affect recognition.</article-title>             <source>Anim Cogn</source>             <volume>7</volume>             <fpage>171</fpage>             <lpage>178</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar3">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Nielsen</surname><given-names>K</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2006</year>             <article-title>Eye movements of monkey observers viewing vocalizing conspecifics.</article-title>             <source>Cognition</source>             <volume>101</volume>             <fpage>515</fpage>             <lpage>529</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-VatikiotisBateson1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vatikiotis-Bateson</surname><given-names>E</given-names></name><name name-style="western"><surname>Eigsti</surname><given-names>IM</given-names></name><name name-style="western"><surname>Yano</surname><given-names>S</given-names></name><name name-style="western"><surname>Munhall</surname><given-names>KG</given-names></name></person-group>             <year>1998</year>             <article-title>Eye movement of perceivers during audiovisual speech perception.</article-title>             <source>Percept Psychophys</source>             <volume>60</volume>             <fpage>926</fpage>             <lpage>940</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Lansing1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lansing</surname><given-names>IR</given-names></name><name name-style="western"><surname>McConkie</surname><given-names>GW</given-names></name></person-group>             <year>2003</year>             <article-title>Word identification and eye fixation locations in visual and visual-plus-auditory presentations of spoken sentences.</article-title>             <source>Percept Psychophys</source>             <volume>65</volume>             <fpage>536</fpage>             <lpage>552</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stevenson1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stevenson</surname><given-names>RA</given-names></name><name name-style="western"><surname>Altieri</surname><given-names>NA</given-names></name><name name-style="western"><surname>Kim</surname><given-names>S</given-names></name><name name-style="western"><surname>Pisoni</surname><given-names>DB</given-names></name><name name-style="western"><surname>James</surname><given-names>TW</given-names></name></person-group>             <year>2010</year>             <article-title>Neural processing of asynchronous audiovisual speech perception.</article-title>             <source>Neuroimage</source>             <volume>49</volume>             <fpage>3308</fpage>             <lpage>3318</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Arnal1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arnal</surname><given-names>LH</given-names></name><name name-style="western"><surname>Morillon</surname><given-names>B</given-names></name><name name-style="western"><surname>Kell</surname><given-names>CA</given-names></name><name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name></person-group>             <year>2009</year>             <article-title>Dual Neural Routing of Visual Facilitation in Speech Processing.</article-title>             <source>J. Neurosci</source>             <volume>29</volume>             <fpage>13445</fpage>             <lpage>13453</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Callan1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Callan</surname><given-names>DE</given-names></name><name name-style="western"><surname>Jones</surname><given-names>JA</given-names></name><name name-style="western"><surname>Munhall</surname><given-names>K</given-names></name><name name-style="western"><surname>Callan</surname><given-names>AM</given-names></name><name name-style="western"><surname>Kroos</surname><given-names>C</given-names></name><etal/></person-group>             <year>2003</year>             <article-title>Neural processes underlying perceptual enhancement by visual speech gestures.</article-title>             <source>Neuroreport</source>             <volume>14</volume>             <fpage>2213</fpage>             <lpage>2218</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Calvert1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Calvert</surname><given-names>GA</given-names></name><name name-style="western"><surname>Brammer</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Bullmore</surname><given-names>ET</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>R</given-names></name><name name-style="western"><surname>Iversen</surname><given-names>SD</given-names></name><etal/></person-group>             <year>1999</year>             <article-title>Response amplification in sensory-specific cortices during crossmodal binding.</article-title>             <source>Neuroreport</source>             <volume>10</volume>             <fpage>2619</fpage>             <lpage>2623</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-vonKriegstein1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>von Kriegstein</surname><given-names>K</given-names></name><name name-style="western"><surname>Dogan</surname><given-names>O</given-names></name><name name-style="western"><surname>Gruter</surname><given-names>M</given-names></name><name name-style="western"><surname>Giraud</surname><given-names>AL</given-names></name><name name-style="western"><surname>Kell</surname><given-names>CA</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Simulation of talking faces in the human brain improves auditory speech recognition.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>105</volume>             <fpage>6747</fpage>             <lpage>6752</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar4">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Maier</surname><given-names>JX</given-names></name><name name-style="western"><surname>Hoffman</surname><given-names>KL</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2005</year>             <article-title>Multisensory Integration of Dynamic Faces and Voices in Rhesus Monkey Auditory Cortex.</article-title>             <source>J. Neurosci</source>             <volume>25</volume>             <fpage>5004</fpage>             <lpage>5012</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar5">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name></person-group>             <year>2008</year>             <article-title>Interactions between the Superior Temporal Sulcus and Auditory Cortex Mediate Dynamic Face/Voice Integration in Rhesus Monkeys.</article-title>             <source>J. Neurosci</source>             <volume>28</volume>             <fpage>4457</fpage>             <lpage>4469</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Chandrasekaran2">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group>             <year>2009</year>             <article-title>Different Neural Frequency Bands Integrate Faces and Voices Differently in the Superior Temporal Sulcus.</article-title>             <source>J. Neurophysiol</source>             <volume>101</volume>             <fpage>773</fpage>             <lpage>788</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar6">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>A</given-names></name><name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name><name name-style="western"><surname>Morrill</surname><given-names>RJ</given-names></name></person-group>             <year>2010</year>             <article-title>Dynamic, rhythmic facial expressions and the superior temporal sulcus of macaque monkeys: implications for the evolution of audiovisual speech.</article-title>             <source>Eur. J. Neurosci</source>             <volume>31</volume>             <fpage>1807</fpage>             <lpage>1817</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Barraclough1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barraclough</surname><given-names>NE</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>D</given-names></name><name name-style="western"><surname>Baker</surname><given-names>CI</given-names></name><name name-style="western"><surname>Oram</surname><given-names>MW</given-names></name><name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name></person-group>             <year>2005</year>             <article-title>Integration of Visual and Auditory Information by Superior Temporal Sulcus Neurons Responsive to the Sight of Actions.</article-title>             <source>J. Cogn Neurosci</source>             <volume>17</volume>             <fpage>377</fpage>             <lpage>391</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sugihara1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sugihara</surname><given-names>T</given-names></name><name name-style="western"><surname>Diltz</surname><given-names>MD</given-names></name><name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name><name name-style="western"><surname>Romanski</surname><given-names>LM</given-names></name></person-group>             <year>2006</year>             <article-title>Integration of Auditory and Visual Communication Information in the Primate Ventrolateral Prefrontal Cortex.</article-title>             <source>J. Neurosci</source>             <volume>26</volume>             <fpage>11138</fpage>             <lpage>11147</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stein1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Burr</surname><given-names>D</given-names></name><name name-style="western"><surname>Constantinidis</surname><given-names>C</given-names></name><name name-style="western"><surname>Laurienti</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Alex Meredith</surname><given-names>M</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Semantic confusion regarding the development of multisensory integration: a practical solution.</article-title>             <source>Eur. J. Neurosci</source>             <volume>31</volume>             <fpage>1713</fpage>             <lpage>1720</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Klucharev1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Klucharev</surname><given-names>V</given-names></name><name name-style="western"><surname>Mottonen</surname><given-names>R</given-names></name><name name-style="western"><surname>Sams</surname><given-names>M</given-names></name></person-group>             <year>2003</year>             <article-title>Electrophysiological indicators of phonetic and non-phonetic multisensory interactions during audiovisual speech perception. .</article-title>             <source>Cognitive Brain Res</source>             <volume>18</volume>             <fpage>65</fpage>             <lpage>75</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Murase1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Murase</surname><given-names>M</given-names></name><name name-style="western"><surname>Saito</surname><given-names>DN</given-names></name><name name-style="western"><surname>Kochiyama</surname><given-names>T</given-names></name><name name-style="western"><surname>Tanabe</surname><given-names>HC</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>S</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Cross-modal integration during vowel identification in audiovisual speech: A functional magnetic resonance imaging study. .</article-title>             <source>Neurosci Lett</source>             <volume>434</volume>             <fpage>71</fpage>             <lpage>76</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Dixon1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dixon</surname><given-names>NF</given-names></name><name name-style="western"><surname>Spitz</surname><given-names>LT</given-names></name></person-group>             <year>1980</year>             <article-title>The detection of auditory visual desynchrony.</article-title>             <source>Perception</source>             <volume>9</volume>             <fpage>719</fpage>             <lpage>721</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-vanWassenhove2">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Wassenhove</surname><given-names>V</given-names></name><name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name></person-group>             <year>2007</year>             <article-title>Temporal window of integration in auditory-visual speech perception.</article-title>             <source>Neuropsychologia</source>             <volume>45</volume>             <fpage>598</fpage>             <lpage>607</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stein2">
        <label>49</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Meredith</surname><given-names>MA</given-names></name></person-group>             <year>1993</year>             <article-title>Merging of the Senses.</article-title>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stein3">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Stanford</surname><given-names>TR</given-names></name></person-group>             <year>2008</year>             <article-title>Multisensory integration: current issues from the perspective of the single neuron.</article-title>             <source>Nat Rev Neurosci</source>             <volume>9</volume>             <fpage>255</fpage>             <lpage>266</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Miller1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>J</given-names></name></person-group>             <year>1986</year>             <article-title>Timecourse of coactivation in bimodal divided attention.</article-title>             <source>Percept Psychophys</source>             <volume>40</volume>             <fpage>331</fpage>             <lpage>343</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stanford1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stanford</surname><given-names>TR</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>2007</year>             <article-title>Superadditivity in multisensory integration: putting the computation in context.</article-title>             <source>Neuroreport</source>             <volume>18</volume>             <fpage>787</fpage>             <lpage>792</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Schwarz1">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwarz</surname><given-names>W</given-names></name></person-group>             <year>1994</year>             <article-title>Diffusion, Superposition and the Redundant-Targets Effect.</article-title>             <source>J. Math Psychol</source>             <volume>38</volume>             <fpage>504</fpage>             <lpage>520</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Munhall1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Munhall</surname><given-names>KG</given-names></name><name name-style="western"><surname>Jones</surname><given-names>JA</given-names></name><name name-style="western"><surname>Callan</surname><given-names>DE</given-names></name><name name-style="western"><surname>Kuratate</surname><given-names>T</given-names></name><name name-style="western"><surname>Vatikiotis-Bateson</surname><given-names>EB</given-names></name></person-group>             <year>2003</year>             <article-title>Visual Prosody and Speech Intelligibility:Head movement improves auditory speech perception.</article-title>             <source>Psychol Sci</source>             <volume>15</volume>             <fpage>133</fpage>             <lpage>137</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Flanagan1">
        <label>55</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Flanagan</surname><given-names>JL</given-names></name><name name-style="western"><surname>Golden</surname><given-names>RM</given-names></name></person-group>             <year>1966</year>             <article-title>Phase Vocoder.</article-title>             <publisher-name>Bell System Technical Journal</publisher-name>             <fpage>1493</fpage>             <lpage>1509</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Egan1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Egan</surname><given-names>JP</given-names></name><name name-style="western"><surname>Greenberg</surname><given-names>GZ</given-names></name><name name-style="western"><surname>Schulman</surname><given-names>AI</given-names></name></person-group>             <year>1961</year>             <article-title>Operating Characteristics, Signal Detectability, and the Method of Free Response.</article-title>             <source>J. Acoust Soc Am</source>             <volume>33</volume>             <fpage>993</fpage>             <lpage>1007</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Miller2">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>J</given-names></name></person-group>             <year>1982</year>             <article-title>Divided attention: Evidence for coactivation with redundant signals.</article-title>             <source>Cognitive Psychol</source>             <volume>14</volume>             <fpage>247</fpage>             <lpage>279</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Miller3">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>J</given-names></name><name name-style="western"><surname>Ulrich</surname><given-names>R</given-names></name><name name-style="western"><surname>Lamarre</surname><given-names>Y</given-names></name></person-group>             <year>2001</year>             <article-title>Locus of the redundant-signals effect in bimodal divided attention: a neurophysiological analysis.</article-title>             <source>Percept Psychophys</source>             <volume>63</volume>             <fpage>555</fpage>             <lpage>562</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Raab1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Raab</surname><given-names>DH</given-names></name></person-group>             <year>1962</year>             <article-title>Statistical facilitation of simple reaction times.</article-title>             <source>Trans N Y Acad Sci</source>             <volume>24</volume>             <fpage>574</fpage>             <lpage>590</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Shub1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shub</surname><given-names>DE</given-names></name><name name-style="western"><surname>Richards</surname><given-names>VM</given-names></name></person-group>             <year>2009</year>             <article-title>Psychophysical spectro-temporal receptive fields in an auditory task.</article-title>             <source>Hear Res</source>             <volume>251</volume>             <fpage>1</fpage>             <lpage>9</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Gourevitch1">
        <label>61</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gourevitch</surname><given-names>G</given-names></name></person-group>             <year>1970</year>             <article-title>Detectability of Tones in Quiet and Noise by Rats and Monkeys.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Stebbins</surname><given-names>WC</given-names></name></person-group>             <source>Animal Psychophysics: the design and conduct of sensory experiments</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Appleton Century Crofts</publisher-name>             <fpage>67</fpage>             <lpage>97</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Gondan1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gondan</surname><given-names>M</given-names></name></person-group>             <year>2010</year>             <article-title>A permutation test for the race model inequality.</article-title>             <source>Behav Res Meth</source>             <volume>42</volume>             <fpage>23</fpage>             <lpage>28</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Schwarz2">
        <label>63</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwarz</surname><given-names>W</given-names></name></person-group>             <year>1989</year>             <article-title>A new model to explain the redundant-signals effect.</article-title>             <source>Percept Psychophys</source>             <volume>46</volume>             <fpage>498</fpage>             <lpage>500</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Diederich1">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name></person-group>             <year>1991</year>             <article-title>A further test of the superposition model for the redundant-signals effect in bimodal detection.</article-title>             <source>Percept Psychophys</source>             <volume>50</volume>             <fpage>83</fpage>             <lpage>86</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Hauser2">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hauser</surname><given-names>MD</given-names></name><name name-style="western"><surname>Marler</surname><given-names>P</given-names></name></person-group>             <year>1993</year>             <article-title>Food-associated calls in rhesus macaques (Macaca mulatta): I. Socioecological factors.</article-title>             <source>Behav Ecol</source>             <volume>4</volume>             <fpage>194</fpage>             <lpage>205</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Rowell1">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rowell</surname><given-names>TE</given-names></name><name name-style="western"><surname>Hinde</surname><given-names>RA</given-names></name></person-group>             <year>1962</year>             <article-title>Vocal communication by the rhesus monkey (Macaca mulatta).</article-title>             <source>Proceedings of the Zoological Society London</source>             <volume>138</volume>             <fpage>279</fpage>             <lpage>294</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Wright1">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wright</surname><given-names>TM</given-names></name><name name-style="western"><surname>Pelphrey</surname><given-names>KA</given-names></name><name name-style="western"><surname>Allison</surname><given-names>T</given-names></name><name name-style="western"><surname>McKeown</surname><given-names>MJ</given-names></name><name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name></person-group>             <year>2003</year>             <article-title>Polysensory Interactions along Lateral Temporal Regions Evoked by Audiovisual Speech.</article-title>             <source>Cereb Cortex</source>             <volume>13</volume>             <fpage>1034</fpage>             <lpage>1043</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ouni1">
        <label>68</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ouni</surname><given-names>S</given-names></name><name name-style="western"><surname>Cohen</surname><given-names>M</given-names></name><name name-style="western"><surname>Hope</surname><given-names>I</given-names></name><name name-style="western"><surname>Massaro</surname><given-names>D</given-names></name></person-group>             <year>2007</year>             <article-title>Visual Contribution to Speech Perception: Measuring the Intelligibility of Animated Talking Heads.</article-title>             <publisher-name>EURASIP Journal on Audio, Speech, and Music Processing</publisher-name>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1155/2007/47891" xlink:type="simple">10.1155/2007/47891</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Munhall2">
        <label>69</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Munhall</surname><given-names>KG</given-names></name><name name-style="western"><surname>Kroos</surname><given-names>C</given-names></name><name name-style="western"><surname>Kuratate</surname><given-names>T</given-names></name><name name-style="western"><surname>Lucero</surname><given-names>J</given-names></name><name name-style="western"><surname>Pitermann</surname><given-names>M</given-names></name><etal/></person-group>             <year>2000</year>             <article-title>Studies of audiovisual speech perception using production-based animation.</article-title>             <source>Sixth International Conference on Spoken Language Processing (ICSLP).</source>             <publisher-loc>Beijing, China</publisher-loc>             <publisher-name>3</publisher-name>             <fpage>7</fpage>             <lpage>10</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Churchland1">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Churchland</surname><given-names>AK</given-names></name><name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2008</year>             <article-title>Decision-making with multiple alternatives.</article-title>             <source>Nat Neurosci</source>             <volume>11</volume>             <fpage>693</fpage>             <lpage>702</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Yang1">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>T</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2007</year>             <article-title>Probabilistic reasoning by neurons.</article-title>             <source>Nature</source>             <volume>447</volume>             <fpage>1075</fpage>             <lpage>1080</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Roitman1">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roitman</surname><given-names>JD</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>             <year>2002</year>             <article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task.</article-title>             <source>J. Neurosci</source>             <volume>22</volume>             <fpage>9475</fpage>             <lpage>9489</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Hershenson1">
        <label>73</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hershenson</surname><given-names>M</given-names></name></person-group>             <year>1962</year>             <article-title>Reaction time as a measure of intersensory facilitation.</article-title>             <source>J. Exp Psychol</source>             <volume>63</volume>             <fpage>289</fpage>             <lpage>293</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stein4">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>1998</year>             <article-title>Neural mechanisms for synthesizing sensory information and producing adaptive behaviors.</article-title>             <source>Exp Brain Res</source>             <volume>123</volume>             <fpage>124</fpage>             <lpage>135</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Stein5">
        <label>75</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Meredith</surname><given-names>MA</given-names></name><name name-style="western"><surname>Huneycutt</surname><given-names>WS</given-names></name><name name-style="western"><surname>McDade</surname><given-names>L</given-names></name></person-group>             <year>1989</year>             <article-title>Behavioral Indices of Multisensory Integration: Orientation to Visual Cues is Affected by Auditory Stimuli.</article-title>             <source>J. Cogn Neurosci</source>             <volume>1</volume>             <fpage>12</fpage>             <lpage>24</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Meredith1">
        <label>76</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Meredith</surname><given-names>MA</given-names></name><name name-style="western"><surname>Nemitz</surname><given-names>JW</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>1987</year>             <article-title>Determinants of multisensory integration in superior colliculus neurons. I. Temporal factors.</article-title>             <source>J. Neurosci</source>             <volume>7</volume>             <fpage>3215</fpage>             <lpage>3229</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Colonius1">
        <label>77</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name></person-group>             <year>2010</year>             <article-title>The optimal time window of visual-auditory integration: a reaction time analysis.</article-title>             <source>Front Integr Neurosci</source>             <volume>4</volume>             <fpage>11</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Grant1">
        <label>78</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Grant</surname><given-names>KW</given-names></name><name name-style="western"><surname>Seitz</surname><given-names>P-F</given-names></name></person-group>             <year>2000</year>             <article-title>The use of visible speech cues for improving auditory detection of spoken sentences.</article-title>             <source>J. Acoust Soc Am</source>             <volume>108</volume>             <fpage>1197</fpage>             <lpage>1208</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Bernstein1">
        <label>79</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bernstein</surname><given-names>LE</given-names></name><name name-style="western"><surname>Auer</surname><given-names>JET</given-names></name><name name-style="western"><surname>Takayanagi</surname><given-names>S</given-names></name></person-group>             <year>2004</year>             <article-title>Auditory speech detection in noise enhanced by lipreading.</article-title>             <source>Speech Commun</source>             <volume>44</volume>             <fpage>5</fpage>             <lpage>18</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Schwartz1">
        <label>80</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schwartz</surname><given-names>J-L</given-names></name><name name-style="western"><surname>Berthommier</surname><given-names>F</given-names></name><name name-style="western"><surname>Savariaux</surname><given-names>C</given-names></name></person-group>             <year>2004</year>             <article-title>Seeing to hear better: evidence for early audio-visual interactions in speech identification.</article-title>             <source>Cognition</source>             <volume>93</volume>             <fpage>B69</fpage>             <lpage>B78</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ma1">
        <label>81</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X</given-names></name><name name-style="western"><surname>Ross</surname><given-names>LA</given-names></name><name name-style="western"><surname>Foxe</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Parra</surname><given-names>LC</given-names></name></person-group>             <year>2009</year>             <article-title>Lip-reading aids word recognition most in moderate noise: a Bayesian explanation using high-dimensional feature space.</article-title>             <source>PLoS ONE</source>             <volume>4</volume>             <fpage>e4638</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ghazanfar7">
        <label>82</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name><name name-style="western"><surname>Turesson</surname><given-names>HK</given-names></name><name name-style="western"><surname>Maier</surname><given-names>JX</given-names></name><name name-style="western"><surname>van Dinther</surname><given-names>R</given-names></name><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Vocal-Tract Resonances as Indexical Cues in Rhesus Monkeys.</article-title>             <source>Curr Biol</source>             <volume>17</volume>             <fpage>425</fpage>             <lpage>430</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Sliwa1">
        <label>83</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sliwa</surname><given-names>J</given-names></name><name name-style="western"><surname>Duhamel</surname><given-names>JR</given-names></name><name name-style="western"><surname>Pascalis</surname><given-names>O</given-names></name><name name-style="western"><surname>Wirth</surname><given-names>S</given-names></name></person-group>             <year>2011</year>             <article-title>Spontaneous voice-face identity matching by rhesus monkeys for familiar conspecifics and humans.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>108</volume>             <fpage>1735</fpage>             <lpage>1740</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Corneil1">
        <label>84</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Corneil</surname><given-names>BD</given-names></name><name name-style="western"><surname>Van Wanrooij</surname><given-names>M</given-names></name><name name-style="western"><surname>Munoz</surname><given-names>DP</given-names></name><name name-style="western"><surname>Van Opstal</surname><given-names>AJ</given-names></name></person-group>             <year>2002</year>             <article-title>Auditory-visual interactions subserving goal-directed saccades in a complex scene.</article-title>             <source>J. Neurophysiol</source>             <volume>88</volume>             <fpage>438</fpage>             <lpage>454</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Diederich2">
        <label>85</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name></person-group>             <year>2004</year>             <article-title>Bimodal and trimodal multisensory enhancement: effects of stimulus onset and intensity on reaction time.</article-title>             <source>Percept Psychophys</source>             <volume>66</volume>             <fpage>1388</fpage>             <lpage>1404</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Miller4">
        <label>86</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>J</given-names></name><name name-style="western"><surname>Ulrich</surname><given-names>R</given-names></name></person-group>             <year>2003</year>             <article-title>Simple reaction time and statistical facilitation: a parallel grains model.</article-title>             <source>Cognitive Psychol</source>             <volume>46</volume>             <fpage>101</fpage>             <lpage>151</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Todd1">
        <label>87</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Todd</surname><given-names>J</given-names></name></person-group>             <year>1912</year>             <article-title>Reaction to multiple stimuli.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Woodworth</surname><given-names>R</given-names></name></person-group>             <source>Archives of Psychology, No 25 Columbia Contributions to Philosophy and Psychology</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>The Science Press</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Gondan2">
        <label>88</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gondan</surname><given-names>M</given-names></name><name name-style="western"><surname>Gotze</surname><given-names>C</given-names></name><name name-style="western"><surname>Greenlee</surname><given-names>MW</given-names></name></person-group>             <year>2010</year>             <article-title>Redundancy gains in simple responses and go/no-go tasks.</article-title>             <source>Atten Percept Psychophys</source>             <volume>72</volume>             <fpage>1692</fpage>             <lpage>1709</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Colonius2">
        <label>89</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name></person-group>             <year>2004</year>             <article-title>Multisensory interaction in saccadic reaction time: a time-window-of-integration model.</article-title>             <source>J. Cogn Neurosci</source>             <volume>16</volume>             <fpage>1000</fpage>             <lpage>1009</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Giray1">
        <label>90</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Giray</surname><given-names>M</given-names></name><name name-style="western"><surname>Ulrich</surname><given-names>R</given-names></name></person-group>             <year>1993</year>             <article-title>Motor coactivation revealed by response force in divided and focused attention.</article-title>             <source>J. Exp Psychol Hum Percept Perform</source>             <volume>19</volume>             <fpage>1278</fpage>             <lpage>1291</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Plat1">
        <label>91</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Plat</surname><given-names>FM</given-names></name><name name-style="western"><surname>Praamstra</surname><given-names>P</given-names></name><name name-style="western"><surname>Horstink</surname><given-names>MW</given-names></name></person-group>             <year>2000</year>             <article-title>Redundant-signals effects on reaction time, response force, and movement-related potentials in Parkinson's disease.</article-title>             <source>Exp Brain Res</source>             <volume>130</volume>             <fpage>533</fpage>             <lpage>539</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Hughes1">
        <label>92</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hughes</surname><given-names>HC</given-names></name><name name-style="western"><surname>Reuter-Lorenz</surname><given-names>PA</given-names></name><name name-style="western"><surname>Nozawa</surname><given-names>G</given-names></name><name name-style="western"><surname>Fendrich</surname><given-names>R</given-names></name></person-group>             <year>1994</year>             <article-title>Visual-auditory interactions in sensorimotor processing: saccades versus manual responses.</article-title>             <source>J. Exp Psychol Hum Percept Perform</source>             <volume>20</volume>             <fpage>131</fpage>             <lpage>153</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Rowland1">
        <label>93</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rowland</surname><given-names>BA</given-names></name><name name-style="western"><surname>Quessy</surname><given-names>S</given-names></name><name name-style="western"><surname>Stanford</surname><given-names>TR</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>2007</year>             <article-title>Multisensory integration shortens physiological response latencies.</article-title>             <source>J. Neurosci</source>             <volume>27</volume>             <fpage>5879</fpage>             <lpage>5884</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Hirsh1">
        <label>94</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hirsh</surname><given-names>IJ</given-names></name><name name-style="western"><surname>Sherrick</surname><given-names>CE</given-names></name></person-group>             <year>1961</year>             <article-title>Perceived order in different sense modalities.</article-title>             <source>J. Exp Psychol</source>             <volume>62</volume>             <fpage>423</fpage>             <lpage>432</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Frens1">
        <label>95</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Frens</surname><given-names>MA</given-names></name><name name-style="western"><surname>Van Opstal</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Van der Willigen</surname><given-names>RF</given-names></name></person-group>             <year>1995</year>             <article-title>Spatial and temporal factors determine auditory-visual interactions in human saccadic eye movements.</article-title>             <source>Percept Psychophys</source>             <volume>57</volume>             <fpage>802</fpage>             <lpage>816</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Bell1">
        <label>96</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>AH</given-names></name><name name-style="western"><surname>Meredith</surname><given-names>MA</given-names></name><name name-style="western"><surname>Van Opstal</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Munoz</surname><given-names>DP</given-names></name></person-group>             <year>2005</year>             <article-title>Crossmodal integration in the primate superior colliculus underlying the preparation and initiation of saccadic eye movements.</article-title>             <source>J. Neurophysiol</source>             <volume>93</volume>             <fpage>3659</fpage>             <lpage>3673</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Reisberg1">
        <label>97</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reisberg</surname><given-names>D</given-names></name><name name-style="western"><surname>McLean</surname><given-names>J</given-names></name><name name-style="western"><surname>A</surname><given-names>G</given-names></name></person-group>             <year>1987</year>             <article-title>Easy to hear but hard to understand: a lip-reading advantage with intact auditory stimuli.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Dodd</surname><given-names>B</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>R</given-names></name></person-group>             <source>Hearing by eye: the psychology of lip-reading</source>             <publisher-loc>Hillsdale, NJ</publisher-loc>             <publisher-name>Lawrence Erlbaum Associates</publisher-name>             <fpage>97</fpage>             <lpage>113</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Jiang1">
        <label>98</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>W</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>H</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>2002</year>             <article-title>Two corticotectal areas facilitate multisensory orientation behavior.</article-title>             <source>J. Cogn Neurosci</source>             <volume>14</volume>             <fpage>1240</fpage>             <lpage>1255</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Senkowski1">
        <label>99</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Senkowski</surname><given-names>D</given-names></name><name name-style="western"><surname>Saint-Amour</surname><given-names>D</given-names></name><name name-style="western"><surname>Höfle</surname><given-names>M</given-names></name><name name-style="western"><surname>Foxe</surname><given-names>JJ</given-names></name></person-group>             <year>2011</year>             <article-title>Multisensory interactions in early evoked brain activity follow the principle of inverse effectiveness.</article-title>             <source>Neuroimage</source>             <volume>56</volume>             <fpage>2200</fpage>             <lpage>2208</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Cappe1">
        <label>100</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Cappe</surname><given-names>C</given-names></name><name name-style="western"><surname>Murray</surname><given-names>MM</given-names></name><name name-style="western"><surname>Barone</surname><given-names>P</given-names></name><name name-style="western"><surname>Rouiller</surname><given-names>EM</given-names></name></person-group>             <year>2010</year>             <article-title>Multisensory Facilitation of Behavior in Monkeys: Effects of Stimulus Intensity.</article-title>             <publisher-name>J Cogn Neurosci</publisher-name>             <fpage>1</fpage>             <lpage>14</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Giard1">
        <label>101</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Giard</surname><given-names>MH</given-names></name><name name-style="western"><surname>Peronnet</surname><given-names>F</given-names></name></person-group>             <year>1999</year>             <article-title>Auditory-Visual Integration during Multimodal Object Recognition in Humans: A Behavioral and Electrophysiological Study.</article-title>             <source>J. Cogn Neurosci</source>             <volume>11</volume>             <fpage>473</fpage>             <lpage>490</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Musacchia1">
        <label>102</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Musacchia</surname><given-names>G</given-names></name><name name-style="western"><surname>Schroeder</surname><given-names>CE</given-names></name></person-group>             <year>2009</year>             <article-title>Neuronal mechanisms, response dynamics and perceptual functions of multisensory interactions in auditory cortex.</article-title>             <source>Hear Res</source>             <volume>258</volume>             <fpage>72</fpage>             <lpage>79</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Navarra1">
        <label>103</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Navarra</surname><given-names>J</given-names></name><name name-style="western"><surname>Vatakis</surname><given-names>A</given-names></name><name name-style="western"><surname>Zampini</surname><given-names>M</given-names></name><name name-style="western"><surname>Soto-Faraco</surname><given-names>S</given-names></name><name name-style="western"><surname>Humphreys</surname><given-names>W</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration.</article-title>             <source>Brain Res Cogn Brain Res</source>             <volume>25</volume>             <fpage>499</fpage>             <lpage>507</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Diederich3">
        <label>104</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name></person-group>             <year>2008</year>             <article-title>Crossmodal interaction in saccadic reaction time: separating multisensory from warning effects in the time window of integration model.</article-title>             <source>Exp Brain Res</source>             <volume>186</volume>             <fpage>1</fpage>             <lpage>22</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Diederich4">
        <label>105</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Diederich</surname><given-names>A</given-names></name><name name-style="western"><surname>Colonius</surname><given-names>H</given-names></name></person-group>             <year>2009</year>             <article-title>Crossmodal interaction in speeded responses: time window of integration model.</article-title>             <source>Prog Brain Res</source>             <volume>174</volume>             <fpage>119</fpage>             <lpage>135</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Populin1">
        <label>106</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Populin</surname><given-names>LC</given-names></name><name name-style="western"><surname>Yin</surname><given-names>TCT</given-names></name></person-group>             <year>2002</year>             <article-title>Bimodal Interactions in the Superior Colliculus of the Behaving Cat.</article-title>             <source>J. Neurosci</source>             <volume>22</volume>             <fpage>2826</fpage>             <lpage>2834</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Skaliora1">
        <label>107</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Skaliora</surname><given-names>I</given-names></name><name name-style="western"><surname>Doubell</surname><given-names>TP</given-names></name><name name-style="western"><surname>Holmes</surname><given-names>NP</given-names></name><name name-style="western"><surname>Nodal</surname><given-names>FR</given-names></name><name name-style="western"><surname>King</surname><given-names>AJ</given-names></name></person-group>             <year>2004</year>             <article-title>Functional topography of converging visual and auditory inputs to neurons in the rat superior colliculus.</article-title>             <source>J. Neurophysiol</source>             <volume>92</volume>             <fpage>2933</fpage>             <lpage>2946</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Avillac1">
        <label>108</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Avillac</surname><given-names>M</given-names></name><name name-style="western"><surname>Ben Hamed</surname><given-names>S</given-names></name><name name-style="western"><surname>Duhamel</surname><given-names>JR</given-names></name></person-group>             <year>2007</year>             <article-title>Multisensory integration in the ventral intraparietal area of the macaque monkey.</article-title>             <source>J. Neurosci</source>             <volume>27</volume>             <fpage>1922</fpage>             <lpage>1932</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Kohler1">
        <label>109</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kohler</surname><given-names>E</given-names></name><name name-style="western"><surname>Keysers</surname><given-names>C</given-names></name><name name-style="western"><surname>Umiltà</surname><given-names>MA</given-names></name><name name-style="western"><surname>Fogassi</surname><given-names>L</given-names></name><name name-style="western"><surname>Gallese</surname><given-names>V</given-names></name><etal/></person-group>             <year>2002</year>             <article-title>Hearing Sounds, Understanding Actions: Action Representation in Mirror Neurons.</article-title>             <source>Science</source>             <volume>297</volume>             <fpage>846</fpage>             <lpage>848</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ernst1">
        <label>110</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name><name name-style="western"><surname>Banks</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>Humans integrate visual and haptic information in a statistically optimal fashion.</article-title>             <source>Nature</source>             <volume>415</volume>             <fpage>429</fpage>             <lpage>433</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Ernst2">
        <label>111</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name><name name-style="western"><surname>Bulthoff</surname><given-names>HH</given-names></name></person-group>             <year>2004</year>             <article-title>Merging the senses into a robust percept.</article-title>             <source>Trends Cogn Sci</source>             <volume>8</volume>             <fpage>162</fpage>             <lpage>169</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002165-Alais1">
        <label>112</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alais</surname><given-names>D</given-names></name><name name-style="western"><surname>Burr</surname><given-names>D</given-names></name></person-group>             <year>2004</year>             <article-title>The Ventriloquist Effect Results from Near-Optimal Bimodal Integration.</article-title>             <source>Curr Biol</source>             <volume>14</volume>             <fpage>257</fpage>             <lpage>262</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>