<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006633</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-01165</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Electromagnetic radiation</subject><subj-group><subject>Light</subject><subj-group><subject>Visible light</subject><subj-group><subject>Luminance</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep image reconstruction from human brain activity</article-title>
<alt-title alt-title-type="running-head">Deep image reconstruction from human brain activity</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Shen</surname>
<given-names>Guohua</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6524-9398</contrib-id>
<name name-style="western">
<surname>Horikawa</surname>
<given-names>Tomoyasu</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2405-4113</contrib-id>
<name name-style="western">
<surname>Majima</surname>
<given-names>Kei</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name name-style="western">
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Computational Neuroscience Laboratories, Advanced Telecommunications Research Institute International, Kyoto, Japan</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Graduate school of Informatics, Kyoto University, Kyoto, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>O'Reilly</surname>
<given-names>Jill</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Oxford University, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors declare no conflict of interest.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">kamitani@i.kyoto-u.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>14</day>
<month>1</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<month>1</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>1</issue>
<elocation-id>e1006633</elocation-id>
<history>
<date date-type="received">
<day>5</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>8</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Shen et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006633"/>
<abstract>
<p>The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>Machine learning-based analysis of human functional magnetic resonance imaging (fMRI) patterns has enabled the visualization of perceptual content. However, prior work visualizing perceptual contents from brain activity has failed to combine visual information of multiple hierarchical levels. Here, we present a method for visual image reconstruction from the brain that can reveal both seen and imagined contents by capitalizing on multiple levels of visual cortical representations. We decoded brain activity into hierarchical visual features of a deep neural network (DNN), and optimized an image to make its DNN features similar to the decoded features. Our method successfully produced perceptually similar images to viewed natural images and artificial images (colored shapes and letters), whereas the decoder was trained only on an independent set of natural images. It also generalized to the reconstruction of mental imagery of remembered images. Our approach allows for studying subjective contents represented in hierarchical neural representations by objectifying them into images.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>New Energy and Industrial Technology Development Organization (JP)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name name-style="western">
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>JSPS KAKENHI</institution>
</funding-source>
<award-id>JP15H05920</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name name-style="western">
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>ImPACT Program of Council for Science, Technology 295 and Innovation (Cabinet Office, Government of Japan)</institution>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name name-style="western">
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by grants from the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI Grant number JP26119536, JP15H05920, JP15H05710, JP17K12771 and ImPACT Program of Council for Science, Technology and Innovation (Cabinet Office, Government of Japan). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="0"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-01-25</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The experimental data and codes used in the present study are available from our repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/KamitaniLab/DeepImageReconstruction" xlink:type="simple">https://github.com/KamitaniLab/DeepImageReconstruction</ext-link>) and from the OpenfMRI (<ext-link ext-link-type="uri" xlink:href="https://openneuro.org/datasets/ds001506" xlink:type="simple">https://openneuro.org/datasets/ds001506</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>While the externalization of states of the mind is a long-standing theme in science fiction, it is only recently that the advent of machine learning-based analysis of functional magnetic resonance imaging (fMRI) data has expanded its potential in the real world. Although sophisticated decoding and encoding models have been developed to render human brain activity into images or movies, the methods are essentially limited to image reconstructions with low-level image bases [<xref ref-type="bibr" rid="pcbi.1006633.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref002">2</xref>], or to matching to exemplar images or movies [<xref ref-type="bibr" rid="pcbi.1006633.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref004">4</xref>], failing to combine the visual features of multiple hierarchical levels. While several recent approaches have introduced deep neural networks (DNNs) for the image reconstruction task, they have failed to fully utilize hierarchical information to reconstruct visual images [<xref ref-type="bibr" rid="pcbi.1006633.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref006">6</xref>]. Furthermore, whereas categorical decoding of imagery contents has been demonstrated [<xref ref-type="bibr" rid="pcbi.1006633.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref008">8</xref>], the reconstruction of internally generated images has been challenging.</p>
<p>The recent success of DNNs provides technical innovations to study the hierarchical visual processing in computational neuroscience [<xref ref-type="bibr" rid="pcbi.1006633.ref009">9</xref>]. Our recent study used DNN visual features as a proxy for the hierarchical neural representations of the human visual system and found that a brain activity pattern measured by fMRI could be decoded (translated) into the response patterns of DNN units in multiple layers representing the hierarchical visual features given the same input [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>]. This finding revealed a homology between the hierarchical representations of the brain and the DNN, providing a new opportunity to utilize the information from hierarchical visual features.</p>
<p>Here, we present a novel approach, named deep image reconstruction, to visualize perceptual content from human brain activity. This technique combines the DNN feature decoding from fMRI signals with recently developed methods for image generation from the machine learning field (<xref ref-type="fig" rid="pcbi.1006633.g001">Fig 1</xref>) [<xref ref-type="bibr" rid="pcbi.1006633.ref011">11</xref>]. The reconstruction algorithm starts with a given initial image and iteratively optimizes the pixel values so that the DNN features of the current image become similar to those decoded from brain activity across multiple DNN layers. The resulting optimized image is considered as a reconstruction from the brain activity. We optionally introduced a deep generator network (DGN) [<xref ref-type="bibr" rid="pcbi.1006633.ref012">12</xref>] to constrain the reconstructed images to look similar to natural images by performing optimization in the input space of the DGN.</p>
<fig id="pcbi.1006633.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Deep image reconstruction.</title>
<p>An overview of a deep image reconstruction is shown. The pixel values of the input image are optimized so that the DNN features of the image are similar to those decoded from fMRI activity. A deep generator network (DGN) is optionally combined with the DNN to produce natural-looking images, in which optimization is performed at the input space of the DGN.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>We trained the decoders that predicted the DNN features of viewed images from fMRI activity patterns following the procedures of Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>]. In the present study, we used the <italic>VGG19</italic> DNN model [<xref ref-type="bibr" rid="pcbi.1006633.ref013">13</xref>], which consisted of sixteen convolutional layers and three fully connected layers and was pre-trained with images in <italic>ImageNet</italic> [<xref ref-type="bibr" rid="pcbi.1006633.ref014">14</xref>] to classify images into 1,000 object categories (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>: “Deep neural network features” for details). We constructed one decoder for a single DNN unit to predict outputs of the unit. We trained decoders corresponding to all the units in all the layers (see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>: “DNN feature decoding analysis” for details).</p>
<p>The feature decoding analysis was performed with fMRI activity patterns in visual cortex (VC) measured while subjects viewed or imagined visual images. Our experiments consisted of the training sessions in which only natural images were presented and the test sessions in which independent sets of natural images, artificial shapes, and alphabetical letters were presented. In another test session, a mental imagery task was performed. The decoders were trained using the fMRI data from the training sessions, and the trained decoders were then used to predict DNN feature values from the fMRI data of the test sessions (the accuracies are shown in <xref ref-type="supplementary-material" rid="pcbi.1006633.s002">S1 Fig</xref>).</p>
<p>Decoded features were then forwarded to the reconstruction algorithm to generate an image using variants of gradient descent optimization (see <xref ref-type="sec" rid="sec004">Material and Methods</xref>: “Reconstruction from a single DNN layer” and “Reconstruction from multiple DNN layers” for details). The optimization was performed to minimize the error between multi-layer DNN features decoded from brain activity patterns and those calculated from the input image by iteratively modifying the input image. For natural image reconstructions, to improve the “naturalness” of reconstructed images, we further introduced the constraint using a deep generator network (DGN) derived from the generative adversarial network algorithm (GAN) [<xref ref-type="bibr" rid="pcbi.1006633.ref015">15</xref>], which is known to capture a latent space explaining natural images [<xref ref-type="bibr" rid="pcbi.1006633.ref016">16</xref>] (see <xref ref-type="sec" rid="sec004">Material and Methods</xref>: “Natural image prior” for details).</p>
<p>Examples of reconstructions for natural images are shown in <xref ref-type="fig" rid="pcbi.1006633.g002">Fig 2</xref> (see <xref ref-type="supplementary-material" rid="pcbi.1006633.s003">S2 Fig</xref> for more examples, and see <xref ref-type="supplementary-material" rid="pcbi.1006633.s024">S1 Movie</xref> for reconstructions through the optimization processes). The reconstructions obtained with the DGN capture the dominant structures of the objects within the images. Furthermore, fine structures reflecting semantic aspects like faces, eyes, and texture patterns were also generated in several images. Our extensive analysis on each of the individual subjects demonstrated replicable results across the subjects. Moreover, the same analysis on a previously published dataset [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>] also replicated qualitatively similar reconstructions to those in the present study (<xref ref-type="supplementary-material" rid="pcbi.1006633.s004">S3 Fig</xref>).</p>
<fig id="pcbi.1006633.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Seen natural image reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (reconstructed from VC activity using DNN1–8). Reconstructed images obtained through the optimization processes are shown for seen natural images. Reconstructions were constrained by the DGN.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g002" xlink:type="simple"/>
</fig>
<p>To investigate the effect of the DGN, we evaluated the quality of reconstructions generated both with and without using it (<xref ref-type="fig" rid="pcbi.1006633.g003">Fig 3A and 3B</xref>; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s005">S4 Fig</xref> for individual subjects; see <xref ref-type="sec" rid="sec004">Material and Methods</xref>: “Evaluation of reconstruction quality”). While the reconstructions obtained without the DGN also successfully reproduced rough silhouettes of dominant objects, they did not show semantically meaningful appearances (see <xref ref-type="supplementary-material" rid="pcbi.1006633.s006">S5 Fig</xref> for more examples; also see <xref ref-type="supplementary-material" rid="pcbi.1006633.s007">S6 Fig</xref> for reconstructions from different initial states for both with and without the DGN). Evaluations using pixel-wise spatial correlation and human judgment both showed almost comparable accuracy for reconstructions with and without the DGN (accuracy of pixel-wise spatial correlation, with and without the DGN, 76.1% and 79.7%; accuracy of human judgment, with and without the DGN, 97.0% and 96.0%). However, reconstruction accuracy evaluated using pixel-wise spatial correlation showed slightly higher accuracy with reconstructions performed without the DGN than with the DGN (two-sided signed-rank test, <italic>P</italic> &lt; 0.01), whereas the opposite was observed for evaluations by human judgment (two-sided signed-rank test, <italic>P</italic> &lt; 0.01). These results suggest the utility of the DGN that enhances the perceptual similarity of reconstructed images to target images by rendering semantically meaningful details in the reconstructions.</p>
<fig id="pcbi.1006633.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Effect of the deep generator network (DGN).</title>
<p>(<bold>A</bold>) Reconstructions with and without the DGN. The first, second, and third rows show presented images, and reconstructions with and without the DGN respectively (reconstructed from VC activity, DNN1–8). (<bold>B</bold>) Reconstruction quality of seen natural images (three subjects pooled, <italic>N</italic> = 150; chance level, 50%).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g003" xlink:type="simple"/>
</fig>
<p>To characterize the ‘deep’ nature of our method, the effectiveness of combining multiple DNN layers was tested using both objective and subjective assessments [<xref ref-type="bibr" rid="pcbi.1006633.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref018">18</xref>]. For each of the 50 test natural images, reconstructed images were generated with a variable number of multiple layers (<xref ref-type="fig" rid="pcbi.1006633.g004">Fig 4A</xref>; DNN1 only, DNN1–2, DNN1–3, …, DNN1–8; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s008">S7 Fig</xref> for more examples). In the objective assessment, the pixel-wise spatial correlations to the original image were compared between two combinations of DNN layers. In the subjective assessment, an independent rater was presented with an original image and a pair of reconstructed images, both from the same original image but generated with different combinations of multiple layers, and was required to indicate which of the reconstructed images looked more similar to the original image. While the objective assessment showed higher winning percentages for the earliest layer (DNN1) alone, the subjective assessment showed increasing winning percentages for a larger number of DNN layers (<xref ref-type="fig" rid="pcbi.1006633.g004">Fig 4B</xref>). Our additional analysis showed poor reconstruction quality from individual layers especially from higher layers (see <xref ref-type="supplementary-material" rid="pcbi.1006633.s009">S8 Fig</xref> for reconstructions from individual layers). These results suggest that combining multiple levels of visual features enhanced the perceptual reconstruction quality even though the pixel-wise accuracy is lost.</p>
<fig id="pcbi.1006633.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Effect of multi-level visual features.</title>
<p>(<bold>A</bold>) Reconstructions using different combinations of DNN layers (without the DGN). The black and gray surrounding frames indicate presented and reconstructed images respectively (reconstructed from VC activity). (<bold>B</bold>) Objective and subjective assessments of reconstructions from different combinations of DNN layers (error bars, 95% confidence interval [C.I.] across samples, <italic>N</italic> = 50; see <xref ref-type="sec" rid="sec004">Material and Methods</xref>: “Evaluation of reconstruction quality” for the procedure to calculate winning percentage).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g004" xlink:type="simple"/>
</fig>
<p>Given the true DNN features, instead of decoded features, as the input, the reconstruction algorithm produces almost complete reconstructions of original images (<xref ref-type="supplementary-material" rid="pcbi.1006633.s009">S8 Fig</xref>), indicating that the DNN feature decoding accuracy would determine the quality of reconstructed images. To further confirm this, we calculated the correlation between the feature decoding accuracy and the reconstruction quality for individual images (<xref ref-type="supplementary-material" rid="pcbi.1006633.s010">S9 Fig</xref>). The analyses showed positive correlations for both the objective and subjective assessments, suggesting that improving feature decoding accuracy could improve reconstruction quality.</p>
<p>We found that the luminance contrast of a reconstruction was often reversed (e.g., the stained-glass images in <xref ref-type="fig" rid="pcbi.1006633.g002">Fig 2</xref>), presumably because of the lack of (absolute) luminance information in the fMRI signals, even in the early visual areas [<xref ref-type="bibr" rid="pcbi.1006633.ref019">19</xref>]. Additional analyses revealed that the feature values of filters with high luminance contrast in the earliest DNN layers (conv1_1 in VGG19) were better decoded when they were converted to absolute values (<xref ref-type="fig" rid="pcbi.1006633.g005">Fig 5A and 5B</xref>), demonstrating a clear discrepancy between the fMRI and raw DNN signals. The large improvement levels demonstrate the insensitivity of fMRI signals to pixel luminance, suggesting the linear-nonlinear discrepancy of DNN and fMRI responses to pixel luminance. This discrepancy may explain the reversal of luminance observed in several reconstructed images. While this may limit the potential for reconstructions from fMRI signals, the ambiguity might be resolved by modelling DNNs to fill the gaps between signals of DNNs and fMRI. Alternatively, further emphasis of the high-level visual information in hierarchical visual features may help to resolve the ambiguity of luminance by incorporating information on semantic context.</p>
<fig id="pcbi.1006633.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g005</object-id>
<label>Fig 5</label>
<caption>
<title>DNN feature decoding accuracy of raw and absolute features.</title>
<p>The analysis was performed with features from the conv1_1 layer of the VGG19 model using the test natural image dataset (error bar, 95% C.I. across subjects). (<bold>A</bold>) Mean feature decoding accuracy of all units. (<bold>B</bold>) Mean feature decoding accuracy for individual filters. The feature decoding accuracies of units within the same filters were individually averaged. The filters were sorted according to the ascending order of the raw feature decoding accuracy averaged for individual filters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g005" xlink:type="simple"/>
</fig>
<p>To confirm that our method was not restricted to the specific image domain used for the model training, we tested whether it was possible to generalize the reconstruction to artificial images. This was challenging, because both the DNN and our decoding models were solely trained on natural images. The reconstructions of artificial shapes and alphabetical letters are shown in <xref ref-type="fig" rid="pcbi.1006633.g006">Fig 6A and 6B</xref> (also see <xref ref-type="supplementary-material" rid="pcbi.1006633.s011">S10 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006633.s025">S2 Movie</xref> for more examples of artificial shapes, and see <xref ref-type="supplementary-material" rid="pcbi.1006633.s012">S11 Fig</xref> for more examples of alphabetical letters). The results show that artificial shapes were successfully reconstructed with moderate accuracy (<xref ref-type="fig" rid="pcbi.1006633.g006">Fig 6C</xref> left; 70.5% by pixel-wise spatial correlation, 91.0% by human judgment; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s013">S12 Fig</xref> for individual subjects) and alphabetical letters were also reconstructed with high accuracy (<xref ref-type="fig" rid="pcbi.1006633.g006">Fig 6C</xref> right; 95.6% by pixel-wise spatial correlation, 99.6% by human judgment; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s014">S13 Fig</xref> for individual subjects). These results indicate that our model did indeed ‘reconstruct’ or ‘generate’ images from brain activity, and that it was not simply making matches to exemplars. Furthermore, the successful reconstructions of alphabetical letters demonstrate that our method can expand the possible states of visualizations, with advance in resolution over reconstructions performed in previous studies [<xref ref-type="bibr" rid="pcbi.1006633.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref020">20</xref>].</p>
<fig id="pcbi.1006633.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Seen artificial image reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). (<bold>A</bold>) Reconstructions for seen artificial shapes. (<bold>B</bold>) Reconstructions for seen alphabetical letters. The reconstructed letters were arranged in the word: “NEURON”. (<bold>C</bold>) Reconstruction quality of artificial shapes and alphabetical letters (three subjects pooled, <italic>N</italic> = 120 and 30 for artificial shapes and alphabetical letters, respectively; chance level, 50%).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g006" xlink:type="simple"/>
</fig>
<p>To assess how the shapes and colors of the stimulus images were reconstructed, we separately evaluated the reconstruction quality of each of shape and color by comparing reconstructed images of the same colors and shapes. Analyses with different visual areas showed different trends in reconstruction quality for shapes and colors (<xref ref-type="fig" rid="pcbi.1006633.g007">Fig 7A</xref> and see <xref ref-type="supplementary-material" rid="pcbi.1006633.s015">S14 Fig</xref> for more examples). Human judgment evaluations suggested that shapes were reconstructed better from early visual areas, whereas colors were reconstructed better from the mid-level visual area V4 (<xref ref-type="fig" rid="pcbi.1006633.g007">Fig 7B</xref> and see <xref ref-type="supplementary-material" rid="pcbi.1006633.s016">S15 Fig</xref> for individual subjects; ANOVA, interaction between task type [shape vs. color] and brain areas [V1 vs. V4], <italic>P</italic> &lt; 0.01), although the interaction effect was marginal when considering evaluations by pixel-wise spatial correlation (<italic>P</italic> = 0.06). These contrasting patterns further support the success of shape and color reconstructions and indicate that our method can be a useful tool to characterize the information content encoded in the activity patterns of individual brain areas by visualization.</p>
<fig id="pcbi.1006633.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Reconstructions of shape and color from multiple visual areas.</title>
<p>(<bold>A</bold>) Reconstructions of artificial shapes from multiple visual areas (DNN 1–8, without the DGN). The black and gray surrounding frames indicate presented and reconstructed images respectively. (<bold>B</bold>) Reconstruction quality of shape and color for different visual areas (three subjects pooled, <italic>N</italic> = 120; chance level, 50%).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g007" xlink:type="simple"/>
</fig>
<p>Finally, to explore the possibility of visually reconstructing subjective content, we performed an experiment in which participants were asked to produce mental imagery of natural and artificial images shown prior to the task session. The reconstructions generated from brain activity due to mental imagery are shown in <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8</xref> (see <xref ref-type="supplementary-material" rid="pcbi.1006633.s017">S16 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006633.s026">S3 Movie</xref> for more examples). While the reconstruction quality varied across subjects and images, rudimentary reconstructions were obtained for some of the artificial shapes (<xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8A and 8B</xref> for high and low accuracy images, respectively). In contrast, imagined natural images were not well reconstructed, possibly because of the difficulty of imagining complex natural images (<xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8C</xref>; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s018">S17 Fig</xref> for vividness scores of imagery). While the pixel-wise spatial correlation evaluations of reconstructed artificial images did not show high accuracy (<xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8D</xref>; 51.9%; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s019">S18 Fig</xref> for individual subjects), this may have been due to the possible disagreements in positions, colors and luminance between target and reconstructed images. Meanwhile, the human judgment evaluations showed accuracy higher than the chance level, suggesting that imagined artificial images were recognizable from the reconstructed images (<xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8D</xref>; 83.2%; one-sided signed-rank test, <italic>P</italic> &lt; 0.01; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s019">S18 Fig</xref> for individual subjects). Furthermore, separate evaluations of color and shape reconstructions of artificial images suggested that shape rather than color had a major contribution to the high proportion of correct answers by human raters (<xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8E</xref>; color, 64.8%; shape, 87.0%; two-sided signed-rank test, <italic>P</italic> &lt; 0.01; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s020">S19 Fig</xref> for individual subjects). Additionally, poor but sufficiently recognizable reconstructions were obtained even from brain activity patterns in the primary visual area (V1; 63.8%; three subjects pooled; one-sided signed-rank test, <italic>P</italic> &lt; 0.01; see <xref ref-type="supplementary-material" rid="pcbi.1006633.s021">S20 Fig</xref> for reconstructed images and <xref ref-type="supplementary-material" rid="pcbi.1006633.s022">S21 Fig</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006633.s023">S22 Fig</xref> for quantitative evaluations), possibly supporting the notion that low-level visual features are encoded in early visual cortical activity during mental imagery [<xref ref-type="bibr" rid="pcbi.1006633.ref021">21</xref>]. Taken together, these results provide evidence for the feasibility of visualizing imagined content from brain activity patterns.</p>
<fig id="pcbi.1006633.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006633.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Imagery reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). (<bold>A</bold>) Reconstructions for imagined artificial shapes through optimization processes. Reconstructed images obtained through the optimization processes are shown for images with high human judgment accuracy. (<bold>B</bold>) Reconstructions of imagined artificial shapes with low human judgment accuracy. (<bold>C</bold>) Reconstructions for imagined natural images. (<bold>D</bold>) Reconstruction quality of imagined artificial shapes (three subjects pooled, <italic>N</italic> = 45; chance level, 50%). (<bold>E</bold>) Reconstruction quality of imagined artificial shapes separately evaluated for color and shape by human judgment (three subjects pooled, <italic>N</italic> = 45; chance level, 50%).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec003" sec-type="conclusions">
<title>Discussion</title>
<p>We have presented a novel approach to reconstruct perceptual and mental content from human brain activity combining visual features from the multiple layers of a DNN. We successfully reconstructed viewed natural images, especially when combined with a DGN. The results from the extensive analysis on each subject were replicated across different subjects. Reconstruction of artificial shapes was also successful, even though the reconstruction models used were trained only on natural images. The same method was also applied to mental imagery, and revealed rudimentary reconstructions of mental content.</p>
<p>Our method is capable of reconstructing various types of images, including natural images, colored artificial shapes, and alphabetical letters, even though each component of our reconstruction model, the DNN models and the DNN feature decoders, was solely trained with natural images. The results strongly demonstrated that our method was certainly able to ‘reconstruct’ or ‘generate’ images from brain activity, differentiating our method from the previous attempts to visualize perceptual contents using the exemplar matching approach, which suffers from restrictions derived from pre-selected image/movie sets [<xref ref-type="bibr" rid="pcbi.1006633.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref004">4</xref>].</p>
<p>We introduced the GAN-based constraint using the DGN for natural image reconstructions to enhance the naturalness of reconstructed images, rendering semantically meaningful details to the reconstructions. A variant of the GAN-based approach has demonstrated the utility in a previous face image reconstruction study, too [<xref ref-type="bibr" rid="pcbi.1006633.ref022">22</xref>]. GAN-derived feature space appears to provide efficient constraints on resultant images to enhance the perceptual resemblance to the image set on which a GAN is trained.</p>
<p>While one of the strengths of the present method is its generalizability across image types, there remains room for substantial improvements in reconstruction performance. Because we used the models (DNNs and decoders) trained with natural ‘object’ images from the ImageNet database [<xref ref-type="bibr" rid="pcbi.1006633.ref014">14</xref>], whose images contain objects around the center, it would not be optimal for the reconstruction of other types of images. Furthermore, because we used the DNN model trained to classify images into 1,000 object categories, the representations acquired in the DNN would be specifically suited to the particular objects. One could train the models with diverse types of images, such as scenes, textures, and artificial shapes, as well as object images, to improve general reconstruction performance. If the target image type is known in prior, one can use a specific set of images and a DNN model training task that are matched to it.</p>
<p>Other DNN models with different architectures could also be used to improve general reconstruction performance. As the reconstruction quality is positively correlated with the feature decoding accuracy (<xref ref-type="supplementary-material" rid="pcbi.1006633.s010">S9 Fig</xref>), DNNs with highly decodable units are likely to improve reconstructions. Recent studies evaluated different types of DNNs in term of the prediction accuracy of brain activity given their feature values (or the encoding accuracy) [<xref ref-type="bibr" rid="pcbi.1006633.ref023">23</xref>–<xref ref-type="bibr" rid="pcbi.1006633.ref025">25</xref>]. Although it remains to be seen how closely the encoding and decoding accuracies are linked, it is expected that more ‘brain-like’ DNN models would yield high-quality reconstructions.</p>
<p>Our approach provides a unique window into our internal world by translating brain activity into images via hierarchical visual features. Our method can also be extended to decode mental contents other than visual perception and imagery. By choosing an appropriate DNN architecture with substantial homology with neural representations, brain-decoded DNN features could be rendered into movies, sounds, text, or other forms of sensory/mental representations. The externalization of mental contents by this approach might prove useful in communicating our internal world via brain–machine/computer interfaces.</p>
</sec>
<sec id="sec004" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec005">
<title>Ethics statement</title>
<p>All subjects provided written informed consent for participation in our experiments, in accordance with the Declaration of Helsinki, and the study protocol was approved by the Ethics Committee of ATR.</p>
</sec>
<sec id="sec006">
<title>Subjects</title>
<p>Three healthy subjects with normal or corrected-to-normal vision participated in our experiments: Subject 1 (male, age 33), Subject 2 (male, age 23) and Subject 3 (female, age 23). This sample size was chosen on the basis of previous fMRI studies with similar experimental designs [<xref ref-type="bibr" rid="pcbi.1006633.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>].</p>
</sec>
<sec id="sec007">
<title>Visual stimuli</title>
<p>Visual stimuli consisted of natural images, artificial shapes, and alphabetical letters. The natural images were identical to those used in Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>], which were originally collected from the online image database <italic>ImageNet</italic> (2011, fall release) [<xref ref-type="bibr" rid="pcbi.1006633.ref014">14</xref>]. The images were cropped to the center and resized to 500 × 500 pixels. The artificial shapes consisted of a total of 40 combinations of 5 shapes and 8 colors (red, green, blue, cyan, magenta, yellow, white, and black), in which the shapes were identical to those used in Miyawaki et al. (2008) [<xref ref-type="bibr" rid="pcbi.1006633.ref001">1</xref>] and the luminance was matched across colors except for white and black. The alphabetical letter images consisted of the 10 black letters, A, C, E, I, N, O, R, S, T, and U.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Experimental design</title>
<p>We conducted two types of experiments: image presentation experiments and a mental imagery experiment. The image presentation experiments consisted of four distinct session types, in which different variants of visual images were presented (training natural images, test natural images, artificial shapes, and alphabetical letters). All visual stimuli were rear-projected onto a screen in the fMRI scanner bore using a luminance-calibrated liquid crystal display projector. To minimize head movements during fMRI scanning, subjects were required to fix their heads using a custom-molded bite-bar individually made for each subject. Data from each subject were collected over multiple scanning sessions spanning approximately 10 months. On each experimental day, one consecutive session was conducted for a maximum of 2 hours. Subjects were given adequate time for rest between runs (every 5–8 min) and were allowed to take a break or stop the experiment at any time.</p>
</sec>
<sec id="sec009">
<title>Image presentation experiment</title>
<p>The image presentation experiments consisted of four distinct types of sessions: training natural-image sessions, test natural-image sessions, artificial-shape sessions, and alphabetical-letter sessions. Each session consisted of 24, 24, 20, and 12 separate runs, respectively. For these four sessions, each run comprised 55, 55, 44, and 11 stimulus blocks, respectively, with these consisting of 50, 50, 40, and 10 blocks with different images, and 5, 5, 4, and 1 randomly interspersed repetition blocks where the same image as in the previous block was presented (7 min 58 s for the training and test natural-image sessions, 6 min 30 s for the artificial-shape sessions, and 5 min 2 s for the alphabetical-letter sessions, for each run). Each stimulus block was 8 s (training natural-images, test natural-images, and artificial-shapes) or 12 s (alphabetical-letters) long, and was followed by a 12-s rest period for the alphabetical-letters, while no rest period was used for the training natural-images, test natural-images, and artificial-shapes. Images were presented at the center of the display with a central fixation spot and were flashed at 2 Hz (12 × 12 and 0.3 × 0.3 degrees of visual angle for the visual images and fixation spot respectively). The color of the fixation spot changed from white to red for 0.5 s before each stimulus block began, to indicate the onset of the block. Additional 32- and 6-s rest periods were added to the beginning and end of each run respectively. Subjects were requested to maintain steady fixation throughout each run and performed a one-back repetition detection task on the images, responding with a button press for each repeated image, to ensure they maintained their attention on the presented images (mean task performance across three subjects: sensitivity 0.9820; specificity 0.9995; pooled across sessions). In one set of training natural-image session, a total of 1,200 images were presented only once. This set of training natural-image session was repeated five times (1,200 × 5 = 6,000 samples for training). In the test natural-image, artificial-shape, and alphabetical-letter sessions, 50, 40, and 10 images were presented 24, 20, and 12 times each respectively. The presentation order of the images was randomized across runs.</p>
</sec>
<sec id="sec010">
<title>Mental imagery experiment</title>
<p>In the mental imagery experiment, subjects were required to visually imagine (recall) one of 25 images selected from those presented in the test natural image and artificial shape sessions of the image presentation experiment (10 natural images and 15 artificial images). Prior to the experiment, subjects were asked to relate words to visual images, so that they could recall the visual images from word cues. The imagery experiment consisted of 20 separate runs, with each run containing 26 blocks (7 min 34 s for each run). The 26 blocks consisted of 25 imagery trials and a fixation trial, in which subjects were required to maintained a steady fixation without any imagery. Each imagery block consisted of a 4-s cue period, an 8-s mental imagery period, a 3-s evaluation period, and a 1-s rest period. Additional 32- and 6-s rest periods were added to the beginning and end of each run respectively. During the rest periods, a white fixation spot was presented at the center of the display. At 0.8 s before each cue period, the color of the fixation spot changed from white to red for 0.5 s, to indicate the onset of the blocks. During the cue period, words specifying the visual images to be imagined were visually presented around the center of the display (1 target and 25 distractors). The position of each word was randomly changed across blocks to avoid cue-specific effects contaminating the fMRI response during mental imagery periods. The word corresponding to the image to be imagined was presented in red (target) and the other words were presented in black (distractors). Subjects were required to start imagining a target image immediately after the cue words disappeared. The imagery period was followed by a 3-s evaluation period, in which the word corresponding to the target image and a scale bar was presented, to allow the subjects to evaluate the correctness and vividness of their mental imagery on a five-point scale (very vivid, fairly vivid, rather vivid, not vivid, cannot correctly recognize the target). This was performed by pressing the left and right buttons of a button box placed in their right hand, to change the score from its random initial setting. As an aid for remembering the associations between words and images, the subjects were able to use control buttons to view the word and visual image pairs during every inter-run-rest period.</p>
</sec>
<sec id="sec011">
<title>MRI acquisition</title>
<p>fMRI data were collected using a 3.0-Tesla Siemens MAGNETOM Verio scanner located at the Kokoro Research Center, Kyoto University. An interleaved T2*-weighted gradient-echo echo planar imaging (EPI) scan was performed to acquire functional images covering the entire brain (TR, 2000 ms; TE, 43 ms; flip angle, 80 deg; FOV, 192 × 192 mm; voxel size, 2 × 2 × 2 mm; slice gap, 0 mm; number of slices, 76; multiband factor, 4). High-resolution anatomical images of the same slices obtained for the EPI were acquired using a T2-weighted turbo spin echo sequence (TR, 11000 ms; TE, 59 ms; flip angle, 160 deg; FOV, 192 × 192 mm; voxel size, 0.75 × 0.75 × 2.0 mm). T1-weighted magnetization-prepared rapid acquisition gradient-echo (MP-RAGE) fine-structural images of the entire head were also acquired (TR, 2250 ms; TE, 3.06 ms; TI, 900 ms; flip angle, 9 deg, FOV, 256 × 256 mm; voxel size, 1.0 × 1.0 × 1.0 mm).</p>
</sec>
<sec id="sec012">
<title>MRI data preprocessing</title>
<p>The first 8 s of scans from each run were discarded to avoid MRI scanner instability effects. We then used SPM (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) to perform three-dimensional motion correction on the fMRI data. The motion-corrected data were then coregistered to the within-session high-resolution anatomical images with the same slices as the EPI, and then subsequently to the whole-head high-resolution anatomical images. The coregistered data were then re-interpolated to 2 × 2 × 2 mm voxels.</p>
<p>Data samples were created by first regressing out nuisance parameters from each voxel amplitude for each run, including any linear trend and the temporal components proportional to the six motion parameters calculated during the motion correction procedure. After that, voxel amplitudes were normalized relative to the mean amplitude of the initial 24-s rest period of each run and were despiked to reduce extreme values (beyond ± 3 SD for each run). The voxel amplitudes were then averaged within each 8-s (training natural image-sessions) or 12-s (test natural-image, artificial-shape, and alphabetical-letter sessions) stimulus block (four or six volumes), and within the 16-s mental imagery block (eight volumes, mental imagery experiment), after shifting the data by 4 s (two volumes) to compensate for hemodynamic delays.</p>
</sec>
<sec id="sec013">
<title>Regions of interest (ROI)</title>
<p>V1, V2, V3, and V4 were delineated following the standard retinotopy experiment [<xref ref-type="bibr" rid="pcbi.1006633.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref027">27</xref>]. The lateral occipital complex (LOC), fusiform face area (FFA), and parahippocampal place area (PPA) were identified using conventional functional localizers [<xref ref-type="bibr" rid="pcbi.1006633.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1006633.ref030">30</xref>] (See <xref ref-type="supplementary-material" rid="pcbi.1006633.s001">S1 Supporting Information</xref> for details). A contiguous region covering the LOC, FFA, and PPA was manually delineated on the flattened cortical surfaces, and the region was defined as the higher visual cortex (HVC). Voxels overlapping with V1–V3 were excluded from the HVC. Voxels from V1–V4 and the HVC were combined to define the visual cortex (VC). In the regression analysis, voxels showing the highest correlation coefficient with the target variable in the training image session were selected to decode each feature (with a maximum of 500 voxels).</p>
</sec>
<sec id="sec014">
<title>Deep neural network features</title>
<p>We used the <italic>Caffe</italic> implementation of the <italic>VGG19</italic> deep neural network (DNN) model [<xref ref-type="bibr" rid="pcbi.1006633.ref013">13</xref>], which was pre-trained with images in <italic>ImageNet</italic> [<xref ref-type="bibr" rid="pcbi.1006633.ref014">14</xref>] to classify 1,000 object categories (the pre-trained model is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/BVLC/caffe/wiki/Model-Zoo" xlink:type="simple">https://github.com/BVLC/caffe/wiki/Model-Zoo</ext-link>). The VGG19 model consisted of a total of sixteen convolutional layers and three fully connected layers. To compute outputs by the VGG19 model, all visual images were resized to 224 × 224 pixels and provided to the model. The outputs from the units in each of the 19 layers (immediately after convolutional or fully connected layers, before rectification) were treated as a vector in the following decoding and reconstruction analysis. The number of units in each of the19 layers is the following: conv1_1 and conv1_2, 3211264; conv2_1 and conv2_2, 1605632; conv3_1, conv3_2, conv3_3, and conv3_4, 802816; conv4_1, conv4_2, conv4_3, and conv4_4, 401408; conv5_1, conv5_2, conv5_3, and conv5_4, 100352; fc6 and fc7, 4096; and fc8, 1000. In this study, we named five groups of convolutional layers as DNN1–5 (DNN1: conv1_1, and conv1_2; DNN2: conv2_1, and conv2_2; DNN3: conv3_1, conv3_2, conv3_3, and conv3_4; DNN4: conv4_1, conv4_2, conv4_3, and conv4_4; and DNN5: conv5_1, conv5_2, conv5_3, and conv5_4), and three fully-connected layers as DNN6–8 (DNN6: fc6; DNN7: fc7; and DNN8: fc8). We used the original pre-trained VGG19 model to compute the feature unit activities, but for analyses with fMRI data from the mental imagery experiment, we changed the DNN model so that the max pooling layers were replaced by average pooling layers, and the ReLU activation function was replaced by a leaky ReLU activation function with a negative slope of 0.2 (see Simonyan &amp; Zisserman (2015) [<xref ref-type="bibr" rid="pcbi.1006633.ref013">13</xref>] for the details of the original DNN architecture).</p>
</sec>
<sec id="sec015">
<title>DNN feature decoding analysis</title>
<p>We used a set of linear regression models to construct multivoxel decoders to decode the DNN feature vector of a seen image from the fMRI activity patterns obtained in the training natural-image sessions (training dataset). In this study, we used the sparse linear regression algorithm (SLR) [<xref ref-type="bibr" rid="pcbi.1006633.ref031">31</xref>], which can automatically select important voxels for decoding by introducing sparsity into a weight estimation through Bayesian estimation of parameters with the automatic relevance determination (ARD) prior (see Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>] for a detailed description). The training dataset was used to train the decoders to decode the values of individual units in the feature vectors of all DNN layers (one decoder for one DNN feature unit), and the trained decoders were then applied to the test datasets. For details of the general procedure of feature decoding, see Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>].</p>
<p>For the test datasets, fMRI samples corresponding to the same stimulus or mental imagery were averaged across trials to increase the signal-to-noise ratio of the fMRI signals. To compensate for possible differences in the signal-to-noise ratio between training and test samples, the decoded features of individual DNN layers were normalized by multiplying them by a single scalar, so that the norm of the decoded vectors of individual DNN layers matched with the mean norm of the true DNN feature vectors computed from independent 10,000 natural images. This norm-corrected vector was then subsequently provided to the reconstruction algorithm (See <xref ref-type="sec" rid="sec021">Supporting Information</xref> for details of the norm-correction procedure).</p>
</sec>
<sec id="sec016">
<title>Reconstruction from a single DNN layer</title>
<p>Given a DNN feature vector decoded from brain activity, an image was generated by solving the following optimization problem [<xref ref-type="bibr" rid="pcbi.1006633.ref011">11</xref>].
<disp-formula id="pcbi.1006633.e001">
<alternatives>
<graphic id="pcbi.1006633.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1006633.e002">
<alternatives>
<graphic id="pcbi.1006633.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where <inline-formula id="pcbi.1006633.e003"><alternatives><graphic id="pcbi.1006633.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="bold">v</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is a vector whose elements are pixel values of an image (224 × 224 × 3 corresponds to height × width × RGB color channel), and <bold>v</bold>* is the reconstructed image. <inline-formula id="pcbi.1006633.e004"><alternatives><graphic id="pcbi.1006633.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></alternatives></inline-formula> is the feature extraction function of the <italic>i</italic>-th DNN feature in the <italic>l</italic>-th layer, with <inline-formula id="pcbi.1006633.e005"><alternatives><graphic id="pcbi.1006633.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> being the output value from the <italic>i</italic>-th DNN unit in the <italic>l</italic>-th layer for the image <bold>v</bold>. <italic>I</italic><sub><italic>l</italic></sub> is the number of units in the <italic>l</italic>-th layer, and <inline-formula id="pcbi.1006633.e006"><alternatives><graphic id="pcbi.1006633.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the value decoded from brain activity for the <italic>i</italic>-th feature in the <italic>l</italic>-th layer. For simplicity, the same cost function was rewritten with a vector function in the second line. <inline-formula id="pcbi.1006633.e007"><alternatives><graphic id="pcbi.1006633.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msup><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>224</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is the function whose <italic>i</italic>-th element is <inline-formula id="pcbi.1006633.e008"><alternatives><graphic id="pcbi.1006633.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006633.e009"><alternatives><graphic id="pcbi.1006633.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is the vector whose <italic>i</italic>-th element is <inline-formula id="pcbi.1006633.e010"><alternatives><graphic id="pcbi.1006633.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
<p>The above cost function was minimized by either a limited-memory BFGS algorithm (L-BFGS) [<xref ref-type="bibr" rid="pcbi.1006633.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006633.ref034">34</xref>] or by a gradient descent with momentum algorithm [<xref ref-type="bibr" rid="pcbi.1006633.ref035">35</xref>], with L-BFGS being used unless otherwise stated. The obtained solution was taken to be the image reconstructed from the brain activity (see <xref ref-type="sec" rid="sec021">Supporting Information</xref> for details of optimization methods).</p>
</sec>
<sec id="sec017">
<title>Reconstruction from multiple DNN layers</title>
<p>To combine the DNN features from multiple layers, we took a weighted sum of the cost functions for individual DNN layers, given by
<disp-formula id="pcbi.1006633.e011">
<alternatives>
<graphic id="pcbi.1006633.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where <italic>L</italic> is a set of DNN layers and <italic>β</italic><sub><italic>l</italic></sub> is a parameter that determines the contribution of the <italic>l</italic>-th layer. We set <italic>β</italic><sub><italic>l</italic></sub> to <inline-formula id="pcbi.1006633.e012"><alternatives><graphic id="pcbi.1006633.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to balance the contributions of individual DNN layers. This cost function was minimized by the L-BFGS algorithm. The DNN layers included in <italic>L</italic> were combined. In the main analyses, we combined all convolutional (DNN1–5) and fully connected layers (DNN6–8), unless otherwise stated.</p>
</sec>
<sec id="sec018">
<title>Natural image prior</title>
<p>To improve the ‘naturalness’ of reconstructed images, we modified the reconstruction algorithm by introducing a constraint. To constrain the resulting images from all possible pixel contrast patterns, we reduced the degrees of freedom by introducing a generator network derived using the generative adversarial network algorithm (GAN) [<xref ref-type="bibr" rid="pcbi.1006633.ref015">15</xref>], which has recently been shown to have good performance in capturing a latent space explaining natural images [<xref ref-type="bibr" rid="pcbi.1006633.ref016">16</xref>]. In the GAN framework, a set of two neural networks, which are called a generator and a discriminator, are trained. The generator is a function to map from a latent space to the data space (i.e. pixel space), and the discriminator is a classifier that predicts whether a given image is a sample from real natural images or an output from the generator. The discriminator is trained to increase its predictive power, while the generator is trained to decrease it. We considered constraining our reconstructed images to be in the subspace consisting of the images that could be produced by a generator trained to produce natural images [<xref ref-type="bibr" rid="pcbi.1006633.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref036">36</xref>]. This is expressed by
<disp-formula id="pcbi.1006633.e013">
<alternatives>
<graphic id="pcbi.1006633.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">argmin</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
and
<disp-formula id="pcbi.1006633.e014">
<alternatives>
<graphic id="pcbi.1006633.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006633.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
<italic>G</italic> is the generator, as the mapping function from the latent space to the image space, which we have called a deep generator network (DGN). In our reconstruction analysis, we used a pre-trained DGN which was provided by Dosovitskiy &amp; Brox (2016; available from <ext-link ext-link-type="uri" xlink:href="https://github.com/dosovits/caffe-fr-chairs" xlink:type="simple">https://github.com/dosovits/caffe-fr-chairs</ext-link>; trained model for fc7) [<xref ref-type="bibr" rid="pcbi.1006633.ref036">36</xref>].</p>
<p>The above cost function for the reconstruction with respect to <bold>z</bold> was minimized by gradient descent with momentum. We used the zero vector as the initial value. To keep <bold>z</bold> within a moderate range, we restricted the range of each element of <bold>z</bold> following the method of a previous study [<xref ref-type="bibr" rid="pcbi.1006633.ref036">36</xref>].</p>
</sec>
<sec id="sec019">
<title>Evaluation of reconstruction quality</title>
<p>Reconstruction quality was evaluated by either objective or subjective assessment [<xref ref-type="bibr" rid="pcbi.1006633.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1006633.ref018">18</xref>]. For the objective assessment, we performed a pairwise similarity comparison analysis, in which a reconstructed image was compared with two candidate images (its original image and a randomly selected image), to test whether its pixel-wise spatial correlation coefficient (Pearson correlation between vectorized pixel values) with the original image was higher than that for a randomly selected image. For the subjective assessment, we conducted a behavioral experiment with a group of 13 raters (5 females and 8 males, aged between 19 and 37 years). On each trial of the experiment, the raters viewed a display presenting a reconstructed image (at the bottom) and two candidate images (displayed at the top; the original image and a randomly selected image), and were asked to select the candidate image most similar to the reconstructed one presented at the bottom. Each trial continued until the raters made a response. For both types of assessments, the proportion of trials, in which the original image was selected as more similar was calculated as a quality measure. In both objective and subjective assessments, each reconstructed image was tested with all pairs of the images from the same types of images (natural-images, artificial-shapes, and alphabetical-letters for images from the image presentation sessions, and natural-images and artificial-shapes for images from the mental imagery session; e.g., for the test natural-images, one of the 50 reconstructions was tested with 49 pairs, with each one consisting of one original image and another image from the rest of 49, resulting in 50 × 49 = 2,450 comparisons). The quality of an individual reconstructed image was evaluated by the percentage of correct answers that was calculated as the proportion of correct trials among all trials where the reconstructed image was tested (i.e., a total of 49 trials for each one of the test natural-images). The resultant percentages of correct answers were then used for the following statistical tests.</p>
<p>To compare the reconstruction quality across different combinations of DNN layers, we also used objective and subjective assessments. For the subjective assessment, we conducted another behavioral experiment with another group of 7 raters (2 females and 5 males, aged between 20 and 37 years). On each trial of the experiment, the raters viewed a display presenting one original image (at the top) and two reconstructed images of the same original image (at the bottom) obtained from different combinations of the DNN layers, and were asked to judge which of the two reconstructed images was better. This pairwise comparison was conducted for all pairs of the combinations of DNN layers (28 pairs), and for all stimulus images presented in the test natural-image session (50 samples). Each trial continued until the raters made a response. We calculated the proportion of trials, in which the reconstructed image obtained from a specific combination of DNN layers was judged as the better one, and then this value was treated as the winning percentage of this combination of DNN layers. For the objective assessment, the same pairwise comparison was conducted using pixel-wise spatial correlations, in which pixel-wise spatial correlations to the original image were compared between two combinations of DNN layers to judge the better combination of DNN layers. The results obtained from all test samples (50 samples from the test natural-image dataset) were used to calculate the winning percentage of each combination of DNN layers in the same manner with the subjective assessment.</p>
<p>These assessments were performed individually for each set of reconstructions from the different subjects and datasets (e.g., test natural-images from Subject 1). For the subjective assessments, one set of reconstructed images was tested with at least three raters. The evaluation results from different raters were averaged within the same set of reconstructions and were treated in the same manner as the evaluation results from the objective assessment.</p>
</sec>
<sec id="sec020">
<title>Statistics</title>
<p>We used two-sided signed-rank tests to examine differences in assessed reconstruction quality according to the different conditions (<italic>N</italic> = 150, 120, and 45 for the test-natural images, artificial shapes, and imagery images, respectively) and used ANOVA to examine interaction effects between task types and brain areas for artificial shapes (<italic>F</italic> (1,1) = 28.40 by human judgment; <italic>F</italic> (1,1) = 3.53 by pixel-wise spatial correlation). We used one-sided signed-rank tests to examine the significance of correct classification accuracy by the human judgment for evaluations of the imagery image reconstructions (<italic>N</italic> = 45).</p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006633.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s001" xlink:type="simple">
<label>S1 Supporting Information</label>
<caption>
<title>Supplementary materials and methods.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>DNN feature decoding accuracy.</title>
<p>DNN feature decoding accuracy obtained from VC activity was evaluated by the correlation coefficient between the true and decoded feature values of each feature unit following the procedure in Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>]. The evaluation was individually performed for each of the three types of seen images (natural images, artificial shapes, and alphabetical letters) and each of the two types imagery images (natural images and artificial shapes). Correlation coefficients were averaged across units in each DNN layer. The mean correlation coefficients are shown for each types of layers (error bars, 95% confidence interval [C.I.] across units; <italic>N</italic> of each layer equals to the number of units in each layer; see <xref ref-type="sec" rid="sec004">Material and Methods</xref>: “Deep neural network features” for details).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Examples of natural image reconstructions obtained with the DGN.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, with the DGN). The three columns of reconstructed images correspond to reconstructions from three subjects. For copyright reasons, we present only a subset of the 50 test natural images; those for which the copyright holders gave us permission to use.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Reconstructions from the generic object decoding dataset.</title>
<p>The same reconstruction analysis was performed with a previously published dataset [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>] (VC activity, DNN 1–8, with the DGN). See Horikawa &amp; Kamitani (2017) [<xref ref-type="bibr" rid="pcbi.1006633.ref010">10</xref>] for details of the data. The black and gray surrounding frames indicate presented and reconstructed images respectively. The five columns of reconstructed images correspond to reconstructions from five subjects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s005" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Reconstruction quality of seen natural images for individual subjects.</title>
<p>Evaluations on individual subjects’ results are separately shown (VC activity; DNN1–8; <italic>N</italic> = 50; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g003">Fig 3B</xref>), indicating that overall tendency was almost consistent across different subjects, except that the human judgment accuracy of reconstructions from Subject 3 showed slightly higher accuracy without the DGN than that with the DGN. Evaluations of reconstructions using pixel-wise spatial correlation for Subject 1–3 showed 78.4%, 74.2%, and 75.7% with the DGN, and 80.4%, 77.2%, and 81.3% without the DGN, respectively. Evaluations of reconstructions using human judgment for Subject 1–3 showed 98.5%, 97.3%, and 95.3% with the DGN, and 96.6%, 94.7%, and 96.7% without the DGN, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s006" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Other examples of natural image reconstructions obtained without the DGN.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). The three columns of reconstructed images correspond to reconstructions from three subjects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s007" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Reconstructions from different initial states.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8). We used different initial states for reconstructions with and without the DGN. For reconstructions with the DGN, we additionally performed the reconstruction analysis using a Gaussian-random-value vector (mean = 0, standard deviation = 1) as the initial state as well as the zero-value vector (main analysis; e.g., <xref ref-type="fig" rid="pcbi.1006633.g002">Fig 2</xref>). For reconstructions without the DGN, we also performed reconstructions from a uniform-random-value image (ranged between 0 and 255) and the zero-value image in addition to the spatially uniform image with the mean RGB values of natural images (main analysis; e.g., <xref ref-type="fig" rid="pcbi.1006633.g003">Fig 3</xref>). For comparison, reconstructed images from different initial states are compared within the same subjects. The results showed slightly different but almost equivalent images from different initial states, demonstrating the stability of our reconstructions.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s008" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Other examples of reconstructions with a variable number of multiple DNN layers.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, without the DGN).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s009" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s009" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Examples of reconstructions from individual DNN layers.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (without the DGN). We used DNN features from individual layers (DNN1, DNN2, …, or DNN8) as well as the combination of all DNN layers (DNN1–8) for the reconstruction analysis, in which either of true or decoded features (VC activity) were provided to the reconstruction algorithm. While reconstructions from individual layers, especially from higher layers, showed poorer reconstruction quality even from true features, combining multiple layers produced almost complete reconstructions of original images from true features and good reconstructions from decoded features (cf., <xref ref-type="fig" rid="pcbi.1006633.g004">Fig 4</xref> and <xref ref-type="supplementary-material" rid="pcbi.1006633.s008">S7 Fig</xref>).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s010" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s010" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Correlations between feature decoding accuracy and reconstruction quality.</title>
<p>To investigate the relations between feature decoding accuracy and reconstruction quality, we first evaluated feature decoding accuracies for individual samples instead of those for individual DNN units (cf., <xref ref-type="supplementary-material" rid="pcbi.1006633.s002">S1 Fig</xref>; see <xref ref-type="sec" rid="sec004">Materials and Methods</xref>: “Evaluation of reconstruction quality” for how to evaluate the reconstruction quality for individual samples). To evaluate the feature decoding accuracy for each sample, Pearson’s correlation coefficients were caluculated between the decoded and true feature values for a single stimulus image using all units within each layer. To avoid estimating spuriously high correlations due to baseline and scale differences across units, feature values of each unit of the test data (test natural-image) were z-normalized using means and standard deviations estimated from the training data (training natural-image) before calculating correlations. Using the estimated feature decoding accuracy and reconstruction quality for individual samples (<italic>N</italic> = 50), Pearson’s correlation coefficients were further calculated between the reconstruction quality (VC activity; with or without the DGN, DNN1–8) and the feature decoding accuracy from individual layers or mean accuracy averaged across 19 layers. While the correlations varied across layers and subjects, the results on average showed positive correlations between the feature decoding accuracy and the reconstruction quality for all combinations of the assessments and the reconstruction algorithms, suggesting that higher decoding accuracy would lead to better reconstruction quality. Interestingly, the analysis showed distinct correlation patterns across layers between the two assessment types, showing that high correlaions were specifically observed from early layers with the pixel-wise spatial correlations although moderately high correlations were observed rather evenly from most layers with the human judgment. These results may reflect the different characteristics of the two assessments, indicating that the pixel-wise correlation is suited to evaluate accuracy in low-level features whereas the human judgment is capable of evaluating accuracy in multiple-levels of visual features.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s011" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s011" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>All examples of artificial shape reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). The three rows of reconstructed images correspond to reconstructions from three subjects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s012" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s012" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>All examples of alphabetical letter reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). The three rows of reconstructed images correspond to reconstructions from three subjects.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s013" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s013" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Reconstruction quality of artificial shapes for individual subjects.</title>
<p>Evaluations on individual subjects’ results are separately shown (VC activity; DNN1–8; without the DGN; <italic>N</italic> = 40; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g006">Fig 6C</xref> left). Evaluations of reconstructions using pixel-wise spatial correlation showed 69.6%, 72.1%, and 69.8% for Subject 1–3, respectively. Evaluations of reconstructions using human judgment showed 91.7%, 91.3%, and 90.1% for Subject 1–3, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s014" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s014" xlink:type="simple">
<label>S13 Fig</label>
<caption>
<title>Reconstruction quality of alphabetical letters for individual subjects.</title>
<p>Evaluations on individual subjects’ results are separately shown (VC activity; DNN1–8; without the DGN; <italic>N</italic> = 10; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g006">Fig 6C</xref> right). Evaluations of reconstructions using pixel-wise spatial correlation showed 98.9%, 87.8%, and 100.0% for Subject 1–3, respectively. Evaluations of reconstructions using human judgment showed 100.0%, 98.9%, and 100.0% for Subject 1–3, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s015" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s015" xlink:type="simple">
<label>S14 Fig</label>
<caption>
<title>All examples of artificial shape reconstructions obtained from different visual areas (Subject 1).</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (DNN 1–8, without the DGN).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s016" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s016" xlink:type="simple">
<label>S15 Fig</label>
<caption>
<title>Reconstruction quality of shape and color for different visual areas for individual subjects.</title>
<p>Evaluations on individual subjects’ results are separately shown (DNN1–8; without the DGN; <italic>N</italic> = 40; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g007">Fig 7B</xref>). Evaluations by pixel-wise correlations and human judgment both showed almost consistent tendency across different subjects, showing that shapes were reconstructed better from early visual areas, whereas colors were reconstructed better from relatively higher visual areas.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s017" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s017" xlink:type="simple">
<label>S16 Fig</label>
<caption>
<title>Other examples of imagery image reconstructions.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (VC activity, DNN 1–8, without the DGN). The three rows of reconstructed images correspond to reconstructions from three subjects. The rightmost images in the bottom row show reconstructions during maintenance of fixation without imagery.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s018" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s018" xlink:type="simple">
<label>S17 Fig</label>
<caption>
<title>Vividness scores for imagery images reported by subjects.</title>
<p>Vividness scores reported during the imagery experiment are shown in descending order of mean vividness scores across trials for individual images. For each subject, the vividness scores were averaged across trials for the same imagery images (<italic>N</italic> = 20). For the pooled results, to eliminate baseline and variability differences across subjects, the vividness scores obtained from individual subjects were first converted to z-scores within each subject, and then averaged across all trials from three subjects (<italic>N</italic> = 60). The rightmost two bars indicated as “artificial” and “natural” show mean vividness scores separately pooled for artificial shapes (15 artificial shapes) and natural images (10 natural images). Error bars indicate 95% confidence intervals across trials.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s019" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s019" xlink:type="simple">
<label>S18 Fig</label>
<caption>
<title>Reconstruction quality of imagined artificial shapes for individual subjects.</title>
<p>Evaluations on individual subjects’ results are separately shown (VC activity; DNN 1–8; without the DGN; <italic>N</italic> = 15; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8D</xref>). Evaluations of reconstructions using pixel-wise spatial correlation showed 49.5%, 52.4%, and 53.8% for Subject 1–3, respectively. Evaluations of reconstructions using human judgment showed 85.6%, 84.4%, and 79.5% for Subject 1–3, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s020" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s020" xlink:type="simple">
<label>S19 Fig</label>
<caption>
<title>Reconstruction quality of imagined artificial shapes for individual subjects separately evaluated for color and shape by human judgment.</title>
<p>Evaluations on individual subjects’ results are separately shown (VC activity; DNN 1–8; without the DGN; <italic>N</italic> = 15; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8E</xref>). Evaluations of reconstructions with respect to color showed 71.1%, 56.7%, and 66.7% for Subject 1–3, respectively. Evaluations of reconstructions with respect to shape showed 91.1%, 88.9%, and 81.1% for Subject 1–3, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s021" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s021" xlink:type="simple">
<label>S20 Fig</label>
<caption>
<title>Imagery image reconstructions from V1.</title>
<p>The black and gray surrounding frames indicate presented and reconstructed images respectively (V1 activity, DNN 1–8, without the DGN). The three rows of reconstructed images correspond to reconstructions from three subjects. The rightmost images in the bottom row show reconstructions during maintenance of fixation without imagery.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s022" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s022" xlink:type="simple">
<label>S21 Fig</label>
<caption>
<title>Reconstruction quality of imagined artificial shapes (reconstructed from V1).</title>
<p>Evaluations on individual subjects’ results and their pooled result are separately shown (V1 activity; DNN 1–8; without the DGN; <italic>N</italic> = 15 for individual subjects and <italic>N</italic> = 45 for the pooled result; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8D</xref>). Evaluations of reconstructions using pixel-wise spatial correlation showed 48.2%, 51.3%, 48.4%, and 48.8% for Subject 1–3 and the pooled result, respectively. Evaluations of reconstructions using human judgment showed 57.7%, 73.5%, 60.1%, and 63.8% for Subject 1–3 and the pooled result, respectively.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s023" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s023" xlink:type="simple">
<label>S22 Fig</label>
<caption>
<title>Reconstruction quality of imagined artificial shapes separately evaluated for color and shape by human judgment (reconstructed from V1).</title>
<p>Evaluations on individual subjects’ results and their pooled result are separately shown (V1 activity; DNN 1–8; without the DGN; <italic>N</italic> = 15 for individual subjects and <italic>N</italic> = 45 for the pooled result; chance level, 50%; cf., <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8E</xref>). Evaluations of reconstructions with respect to color showed 60.0%, 56.7%, 55.6%, and 57.4% for Subject 1–3 and the pooled result, respectively. Evaluations of reconstructions with respect to shape showed 63.9%, 77.8%, 63.3%, and 68.3% for Subject 1–3 and the pooled result, respectively. As shown with the reconstructed images from VC (cf., <xref ref-type="fig" rid="pcbi.1006633.g008">Fig 8E</xref>), separate evaluations of color and shape reconstructions of artificial images from V1 also showed that shape rather than color had a major contribution to the high proportion of correct answers by human raters (three subjects pooled; two-sided signed-rank test, <italic>P</italic> &lt; 0.05).</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s024" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s024" xlink:type="simple">
<label>S1 Movie</label>
<caption>
<title>Deep image reconstruction: Natural images.</title>
<p>The iterative optimization process is shown (left, presented images; right, reconstructed images).</p>
<p>(MOV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s025" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s025" xlink:type="simple">
<label>S2 Movie</label>
<caption>
<title>Deep image reconstruction: Artificial shapes.</title>
<p>The iterative optimization process is shown (left, presented images; right, reconstructed images).</p>
<p>(MOV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006633.s026" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006633.s026" xlink:type="simple">
<label>S3 Movie</label>
<caption>
<title>Deep image reconstruction: Imagery images.</title>
<p>The iterative optimization process is shown (left, imagined images; right, reconstructed images).</p>
<p>(MOV)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors thank Mitsuaki Tsukamoto, and Hiroaki Yamane for help with data collection, and Mohamed Abdelhack and Karl Embleton from Edanz Group (<ext-link ext-link-type="uri" xlink:href="http://www.edanzediting.com/ac" xlink:type="simple">www.edanzediting.com/ac</ext-link>) for the comments on the manuscript. This study was conducted using the MRI scanner and related facilities of Kokoro Research Center, Kyoto University.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006633.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyawaki</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yamashita</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Morito</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tanabe</surname> <given-names>HC</given-names></name>, <etal>et al</etal>. <article-title>Visual image reconstruction from human brain activity using a combination of multiscale local image decoders</article-title>. <source><italic>Neuron</italic></source>. <year>2008</year>; <volume>60</volume>: <fpage>915</fpage>–<lpage>929</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.11.004" xlink:type="simple">10.1016/j.neuron.2008.11.004</ext-link></comment> <object-id pub-id-type="pmid">19081384</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cao</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>. <article-title>Neural encoding and decoding with deep learning for dynamic natural vision</article-title>. <source><italic>Cereb Cortex</italic></source>. <year>2017</year>; <fpage>1</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw362" xlink:type="simple">10.1093/cercor/bhw362</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006633.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Prenger</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Oliver</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Bayesian reconstruction of natural images from human brain activity</article-title>. <source><italic>Neuron</italic></source>. <year>2009</year>; <volume>63</volume>: <fpage>902</fpage>–<lpage>915</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2009.09.006" xlink:type="simple">10.1016/j.neuron.2009.09.006</ext-link></comment> <object-id pub-id-type="pmid">19778517</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nishimoto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vu</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>Reconstructing visual experiences from brain activity evoked by natural movies</article-title>. <source><italic>Curr Biol</italic></source>. <year>2011</year>; <volume>21</volume>: <fpage>1641</fpage>–<lpage>1646</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2011.08.031" xlink:type="simple">10.1016/j.cub.2011.08.031</ext-link></comment> <object-id pub-id-type="pmid">21945275</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seeliger</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Ambrogioni</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Güçlütürk</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MAJ</given-names></name>. <article-title>Generative adversarial networks for reconstructing natural images from brain activity</article-title>; <year>2017</year>. <source>Preprint</source>. Available from: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/226688" xlink:type="simple">10.1101/226688</ext-link></comment> Cited 30 December 2017.</mixed-citation></ref>
<ref id="pcbi.1006633.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Han</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Wen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>. <article-title>Variational Autoencoder: An Unsupervised Model for Modeling and Decoding fMRI Activity in Visual Cortex</article-title>; <year>2017</year>. <source>Preprint</source>. Available from: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/214247" xlink:type="simple">10.1101/214247</ext-link></comment> Cited 30 December 2017.</mixed-citation></ref>
<ref id="pcbi.1006633.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thirion</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Duchesnay</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hubbard</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dubois</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Poline</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lebihan</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Inverse retinotopy: inferring the visual content of images from brain activation patterns</article-title>. <source><italic>Neuroimage</italic></source>. <year>2006</year>; <volume>33</volume>: <fpage>1104</fpage>–<lpage>1116</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2006.06.062" xlink:type="simple">10.1016/j.neuroimage.2006.06.062</ext-link></comment> <object-id pub-id-type="pmid">17029988</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horikawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tamaki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Miyawaki</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kamitani</surname> <given-names>Y</given-names></name>. <article-title>Neural decoding of visual imagery during sleep</article-title>. <source><italic>Science</italic></source>. <year>2013</year>; <volume>340</volume>: <fpage>639</fpage>–<lpage>642</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1234330" xlink:type="simple">10.1126/science.1234330</ext-link></comment> <object-id pub-id-type="pmid">23558170</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source><italic>Nat Neurosci</italic></source>. <year>2016</year>; <volume>19</volume>: <fpage>356</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4244" xlink:type="simple">10.1038/nn.4244</ext-link></comment> <object-id pub-id-type="pmid">26906502</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horikawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kamitani</surname> <given-names>Y</given-names></name>. <article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title>. <source><italic>Nat Commun</italic></source>. <year>2017</year>; <volume>8</volume>: <fpage>15037</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms15037" xlink:type="simple">10.1038/ncomms15037</ext-link></comment> <object-id pub-id-type="pmid">28530228</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mahendran</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vedaldi</surname> <given-names>A</given-names></name>. <article-title>Understanding deep image representations by inverting them</article-title>. <source><italic>Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit</italic></source>. <year>2015</year>; <fpage>5188</fpage>–<lpage>5196</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2015.7299155" xlink:type="simple">10.1109/CVPR.2015.7299155</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006633.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nguyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dosovitskiy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yosinski</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brox</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Clune</surname> <given-names>J</given-names></name>. <article-title>Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</article-title>. <source><italic>Adv Neural Inf Process Syst</italic></source>. <year>2016</year>; <volume>29</volume>: <fpage>3387</fpage>–<lpage>3395</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.09304" xlink:type="simple">https://arxiv.org/abs/1605.09304</ext-link></mixed-citation></ref>
<ref id="pcbi.1006633.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simonyan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Zisserman</surname> <given-names>A</given-names></name>. <article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title>; <year>2014</year>. <source>Preprint</source>. Available from: arXiv:1409.1556v1. Cited 30 December 2017.</mixed-citation></ref>
<ref id="pcbi.1006633.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dong</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Socher</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname> <given-names>L</given-names></name>. <article-title>Imagenet: A large-scale hierarchical image database</article-title>. <source><italic>Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit</italic></source>. <year>2009</year>; <fpage>248</fpage>–<lpage>255</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2009.5206848" xlink:type="simple">10.1109/CVPR.2009.5206848</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006633.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>IJ</given-names></name>, <name name-style="western"><surname>Pouget-Abadie</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mirza</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Warde-Farley</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ozair</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Generative adversarial nets</article-title>. <source><italic>Adv Neural Inf Process Syst</italic></source>. <year>2014</year>; <volume>2</volume>: <fpage>2672</fpage>–<lpage>2680</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.2661" xlink:type="simple">https://arxiv.org/abs/1406.2661</ext-link></mixed-citation></ref>
<ref id="pcbi.1006633.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Radford</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Metz</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Chintala</surname> <given-names>S</given-names></name>. <article-title>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</article-title>; <year>2015</year>. <source>Preprint</source>. Available from: arXiv:1511.06434v1. Cited 30 December 2017.</mixed-citation></ref>
<ref id="pcbi.1006633.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowen</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Kuhl</surname> <given-names>BA</given-names></name>. <article-title>Neural portraits of perception: Reconstructing face images from evoked brain activity</article-title>. <source><italic>NeuroImage</italic></source>. <year>2014</year>; <fpage>1</fpage>–<lpage>11</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2014.03.018" xlink:type="simple">10.1016/j.neuroimage.2014.03.018</ext-link></comment> <object-id pub-id-type="pmid">24650597</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kuhl</surname> <given-names>BA</given-names></name>. <article-title>Reconstructing Perceived and Retrieved Faces from Activity Patterns in Lateral Parietal Cortex</article-title>. <source><italic>Journal of Neuroscience</italic></source>. <year>2016</year>; <volume>36</volume>: <fpage>6069</fpage>–<lpage>6082</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4286-15.2016" xlink:type="simple">10.1523/JNEUROSCI.4286-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27251627</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haynes</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Lotto</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Rees</surname> <given-names>G</given-names></name>. <article-title>Responses of human visual cortex to uniform surfaces</article-title>. <source><italic>Proc Natl Acad Sci USA</italic></source>. <year>2004</year>; <volume>101</volume>: <fpage>4286</fpage>–<lpage>4291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0307948101" xlink:type="simple">10.1073/pnas.0307948101</ext-link></comment> <object-id pub-id-type="pmid">15010538</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schoenmakers</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Barth</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Heskes</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MAJ</given-names></name>. <article-title>Linear reconstruction of perceived images from human brain activity</article-title>. <source><italic>Neuroimage</italic></source>. <year>2013</year>; <volume>83</volume>: <fpage>951</fpage>–<lpage>961</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2013.07.043" xlink:type="simple">10.1016/j.neuroimage.2013.07.043</ext-link></comment> <object-id pub-id-type="pmid">23886984</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naselaris</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Olman</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Stansbury</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Ugurbil</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gallant</surname> <given-names>JL</given-names></name>. <article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title>. <source><italic>Neuroimage</italic></source>. <year>2015</year>; <volume>105</volume>: <fpage>215</fpage>–<lpage>228</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2014.10.018" xlink:type="simple">10.1016/j.neuroimage.2014.10.018</ext-link></comment> <object-id pub-id-type="pmid">25451480</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Güçlütürk</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Güçlü</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Seeliger</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Bosch</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Van Lier</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>van Gerven</surname> <given-names>MAJ</given-names></name>. <article-title>Deep adversarial neural decoding</article-title>. <source><italic>31st Conference on Neural Information Processing Systems</italic></source>. <year>2017</year>. <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006633.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cadieu</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Solomon</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Seibert</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>DiCarlo</surname> <given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source><italic>Proc</italic>. <italic>Natl Acad</italic>. <italic>Sci</italic>. <italic>USA</italic></source>. <year>2014</year>; <volume>111</volume>: <fpage>8619</fpage>–<lpage>8624</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1403112111" xlink:type="simple">10.1073/pnas.1403112111</ext-link></comment> <object-id pub-id-type="pmid">24812127</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kell</surname> <given-names>AJE</given-names></name>, <name name-style="western"><surname>Yamins</surname> <given-names>DLK</given-names></name>, <name name-style="western"><surname>Shook</surname> <given-names>EN</given-names></name>, <name name-style="western"><surname>Norman-Haignere</surname> <given-names>SV</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>. <article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title>. <source><italic>Neuron</italic></source>. <year>2018</year>; <volume>98</volume>: <fpage>630</fpage>–<lpage>644</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.03.044" xlink:type="simple">10.1016/j.neuron.2018.03.044</ext-link></comment> <object-id pub-id-type="pmid">29681533</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schrimpf</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kubilius</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hong</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Majaj</surname> <given-names>NJ</given-names></name>, <name name-style="western"><surname>Rajalingham</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Issa</surname> <given-names>EB</given-names></name>, <etal>et al</etal>. <article-title>Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like?</article-title> <year>2018</year>. <source>Preprint</source>. Available from: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/407007" xlink:type="simple">10.1101/407007</ext-link></comment> Cited 7 September 2018.</mixed-citation></ref>
<ref id="pcbi.1006633.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Wandell</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AT</given-names></name>, <name name-style="western"><surname>Glover</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>fMRI of human visual cortex</article-title>. <source><italic>Nature</italic></source>. <year>1994</year>; <volume>369</volume>: <fpage>525</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/369525a0" xlink:type="simple">10.1038/369525a0</ext-link></comment> <object-id pub-id-type="pmid">8031403</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sereno</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Reppas</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Kwong</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Belliveau</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Brady</surname> <given-names>TJ</given-names></name>, <etal>et al</etal>. <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source><italic>Science</italic></source>. <year>1995</year>; <volume>268</volume>: <fpage>889</fpage>–<lpage>893</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.7754376" xlink:type="simple">10.1126/science.7754376</ext-link></comment> <object-id pub-id-type="pmid">7754376</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kourtzi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>Cortical regions involved in perceiving object shape</article-title>. <source><italic>J Neurosci</italic></source>. <year>2000</year>; <volume>20</volume>: <fpage>3310</fpage>–<lpage>3318</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.20-09-03310.2000" xlink:type="simple">10.1523/JNEUROSCI.20-09-03310.2000</ext-link></comment> <object-id pub-id-type="pmid">10777794</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name>. <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source><italic>J Neurosci</italic></source>. <year>1997</year>; <volume>17</volume>: <fpage>4302</fpage>–<lpage>4311</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.17-11-04302.1997" xlink:type="simple">10.1523/JNEUROSCI.17-11-04302.1997</ext-link></comment> <object-id pub-id-type="pmid">9151747</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Epstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>A cortical representation of the local visual environment</article-title>. <source><italic>Nature</italic></source>. <year>1998</year>; <volume>392</volume>: <fpage>598</fpage>–<lpage>601</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/33402" xlink:type="simple">10.1038/33402</ext-link></comment> <object-id pub-id-type="pmid">9560155</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref031"><label>31</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source><italic>Pattern Recognition and Machine Learning</italic></source>. <edition>1st ed.</edition> <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pcbi.1006633.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Le</surname> <given-names>QV</given-names></name>, <name name-style="western"><surname>Ngiam</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Coates</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lahiri</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Prochnow</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ng</surname> <given-names>AY</given-names></name>. <article-title>On optimization methods for deep learning</article-title>. <source><italic>Proc Int Conf Mach Learn</italic> (Bellevue, Washington, USA)</source>. <year>2011</year>; <fpage>265</fpage>–<lpage>272</lpage>.</mixed-citation></ref>
<ref id="pcbi.1006633.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Nocedal</surname> <given-names>J</given-names></name>. <article-title>On the limited memory BFGS method for large scale optimization</article-title>. <source><italic>Math Program</italic></source>. <year>1989</year>; <volume>45</volume>: <fpage>503</fpage>–<lpage>528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF01589116" xlink:type="simple">10.1007/BF01589116</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006633.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gatys</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Ecker</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Bethge</surname> <given-names>M</given-names></name>. <article-title>Image style transfer using convolutional neural networks</article-title>. <source><italic>Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit</italic></source>. <year>2016</year>; <fpage>2414</fpage>–<lpage>2423</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2016.265" xlink:type="simple">10.1109/CVPR.2016.265</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1006633.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Qian</surname> <given-names>N.</given-names></name> <article-title>On the momentum term in gradient descent learning algorithms</article-title>. <source><italic>Neural Netw</italic></source>. <year>1999</year>; <volume>12</volume>: <fpage>145</fpage>–<lpage>151</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(98)00116-6" xlink:type="simple">10.1016/S0893-6080(98)00116-6</ext-link></comment> <object-id pub-id-type="pmid">12662723</object-id></mixed-citation></ref>
<ref id="pcbi.1006633.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dosovitskiy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Brox</surname> <given-names>T</given-names></name>. <article-title>Generating images with perceptual similarity metrics based on deep networks</article-title>. <source><italic>Adv Neural Inf Process Syst</italic></source>. <year>2016</year>; <volume>29</volume>: <fpage>658</fpage>–<lpage>666</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1602.02644" xlink:type="simple">https://arxiv.org/abs/1602.02644</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>