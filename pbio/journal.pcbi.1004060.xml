<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004060</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-02017</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>How Attention Can Create Synaptic Tags for the Learning of Working Memories in Sequential Tasks</article-title>
<alt-title alt-title-type="running-head">Feedback Gates Learning of Memory Representations</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rombouts</surname>
<given-names>Jaldert O.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bohte</surname>
<given-names>Sander M.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Roelfsema</surname>
<given-names>Pieter R.</given-names>
</name>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
<xref rid="aff004" ref-type="aff"><sup>4</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Life Sciences, Centrum Wiskunde &amp; Informatica, Amsterdam, The Netherlands</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Vision &amp; Cognition, Netherlands Institute for Neurosciences, an institute of the Royal Netherlands Academy of Arts and Sciences (KNAW), Amsterdam, The Netherlands</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Integrative Neurophysiology, Centre for Neurogenomics and Cognitive Research, VU University, Amsterdam, The Netherlands</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Psychiatry Department, Academic Medical Center, Amsterdam, The Netherlands</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gutkin</surname>
<given-names>Boris S.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>École Normale Supérieure, College de France, CNRS, FRANCE</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: PRR JOR. Performed the experiments: JOR. Analyzed the data: JOR PRR SB. Contributed reagents/materials/analysis tools: JOR. Wrote the paper: JOR PRR SB.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">p.roelfsema@nin.knaw.nl</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>5</day>
<month>3</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>3</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>3</issue>
<elocation-id>e1004060</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>11</month>
<year>2013</year>
</date>
<date date-type="accepted">
<day>24</day>
<month>11</month>
<year>2014</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Rombouts et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004060" xlink:type="simple"/>
<abstract>
<p>Intelligence is our ability to learn appropriate responses to new stimuli and situations. Neurons in association cortex are thought to be essential for this ability. During learning these neurons become tuned to relevant features and start to represent them with persistent activity during memory delays. This learning process is not well understood. Here we develop a biologically plausible learning scheme that explains how trial-and-error learning induces neuronal selectivity and working memory representations for task-relevant information. We propose that the response selection stage sends attentional feedback signals to earlier processing levels, forming synaptic tags at those connections responsible for the stimulus-response mapping. Globally released neuromodulators then interact with tagged synapses to determine their plasticity. The resulting learning rule endows neural networks with the capacity to create new working memory representations of task relevant information as persistent activity. It is remarkably generic: it explains how association neurons learn to store task-relevant information for linear as well as non-linear stimulus-response mappings, how they become tuned to category boundaries or analog variables, depending on the task demands, and how they learn to integrate probabilistic evidence for perceptual decisions.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Working memory is a cornerstone of intelligence. Most, if not all, tasks that one can imagine require some form of working memory. The optimal solution of a working memory task depends on information that was presented in the past, for example choosing the right direction at an intersection based on a road-sign some hundreds of meters before. Interestingly, animals like monkeys readily learn difficult working memory tasks, just by receiving rewards such as fruit juice when they perform the desired behavior. Neurons in association areas in the brain play an important role in this process; these areas integrate perceptual and memory information to support decision-making. Some of these association neurons become tuned to relevant features and memorize the information that is required later as a persistent elevation of their activity. It is, however, not well understood how these neurons acquire their task-relevant tuning. Here we formulate a simple biologically plausible learning mechanism that can explain how a network of neurons can learn a wide variety of working memory tasks by trial-and-error learning. We also show that the solutions learned by the model are comparable to those found in animals when they are trained on similar tasks.</p>
</abstract>
<funding-group>
<funding-statement>The work was supported by grants of the European Union (project 269921 ‘‘BrainScaleS”; PITN-GA-2011-290011 “ABC”; ERC Grant Agreement n. 339490) and a NWO grants (VICI and Brain, Cognition grant n. 433-09-208 and EW grant n. 612.066.826). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="3"/>
<page-count count="34"/>
</counts>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Animals like monkeys can be trained to perform complex cognitive tasks, simply by giving rewards at the right times. They can learn to map sensory stimuli onto responses, to store task-relevant information and to integrate and combine unreliable sensory evidence. Training induces new stimulus and memory representations in ‘multiple-demand’ regions of the cortex [<xref rid="pcbi.1004060.ref001" ref-type="bibr">1</xref>]. For example, if monkeys are trained to memorize the location of a visual stimulus, neurons in lateral intra-parietal cortex (LIP) represent this location as a persistent increase of their firing rate [<xref rid="pcbi.1004060.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004060.ref003" ref-type="bibr">3</xref>]. However, if the animals learn a visual categorization task, persistent activity of LIP cells becomes tuned to the boundary between categories [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>] whereas the neurons integrate probabilistic evidence if the task is sensory decision making [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>]. Similar effects of training on persistent activity have been observed in the somatosensory system. If monkeys are trained to compare frequencies of successive vibrotactile stimuli, working memory representations of analog variables are formed in somatosensory, prefrontal and motor cortex [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>].</p>
<p>Which learning mechanism induces appropriate working memories in these tasks? We here outline AuGMEnT (Attention-Gated MEmory Tagging), a new reinforcement learning [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>] scheme that explains the formation of working memories during trial-and-error learning and that is inspired by the role of attention and neuromodulatory systems in the gating of neuronal plasticity. AuGMEnT addresses two well-known problems in learning theory: temporal and structural credit-assignment [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref008" ref-type="bibr">8</xref>]. The temporal credit-assignment problem arises if an agent has to learn actions that are only rewarded after a sequence of intervening actions, so that it is difficult to assign credit to the appropriate ones. AuGMEnT solves this problem like previous temporal-difference reinforcement learning (RL) theories [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]. It learns action-values (known as <italic>Q</italic>-values [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]), i.e. the amount of reward that is predicted for a particular action when executed in a particular state of the world. If the outcome deviates from the reward-prediction, a neuromodulatory signal that codes the global reward-prediction error (RPE) gates synaptic plasticity in order to change the <italic>Q</italic>-value, in accordance with experimental findings [<xref rid="pcbi.1004060.ref009" ref-type="bibr">9</xref>–<xref rid="pcbi.1004060.ref012" ref-type="bibr">12</xref>]. The key new property of AuGMEnT is that it can also learn tasks that require working memory, thus going beyond standard RL models [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref013" ref-type="bibr">13</xref>].</p>
<p>AuGMEnT also solves the structural credit-assignment problem of networks with multiple layers. Which synapses should change to improve performance? AuGMEnT solves this problem with an ‘attentional’ feedback mechanism. The output layer has feedback connections to units at earlier levels that provide feedback to those units that were responsible for the action that was selected [<xref rid="pcbi.1004060.ref014" ref-type="bibr">14</xref>]. We propose that this feedback signal tags [<xref rid="pcbi.1004060.ref015" ref-type="bibr">15</xref>] relevant synapses and that the persistence of tags (known as eligibility traces [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>]) permits learning if time passes between the action and the RPE [see <xref rid="pcbi.1004060.ref017" ref-type="bibr">17</xref>]. We will here demonstrate the neuroscientific plausibility of AuGMEnT. A preliminary and more technical version of these results has been presented at a conference [<xref rid="pcbi.1004060.ref018" ref-type="bibr">18</xref>].</p>
</sec>
<sec id="sec002">
<title>Model</title>
<sec id="sec003">
<title>Model architecture</title>
<p>We used AuGMEnT to train networks composed of three layers of units connected by two layers of modifiable synapses (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). Time was modeled in discrete steps.</p>
<fig id="pcbi.1004060.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Model Architecture.</title>
<p><bold><italic>A</italic></bold>, The model consists of a sensory input layer with units that code the input (instantaneous units) and transient units that only respond when a stimulus appears (on-units) or if it disappears (off-units). The association layer contains regular units (circles) with activities that depend on instantaneous input units, and integrating memory units (diamonds) that receive input from transient sensory units. The connections from the input layer to the memory cells maintain a synaptic trace (sTrace; blue circle) if the synapse was active. Units in the third layer code the value of actions (Q-values). After computing feed-forward activations, a Winner-Take-All competition determines the winning action (see middle panel). Action selection causes a feedback signal to earlier levels (through feedback connections <inline-formula id="pcbi.1004060.e001"><alternatives><graphic id="pcbi.1004060.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e001" xlink:type="simple"/><mml:math display="inline" id="M1" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>Sj</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, see middle panel) that lays down synaptic tags (orange pentagons) at synapses that are responsible for the selected action. If the predicted <italic>Q</italic>-value of the next action <italic>S′</italic> (<italic>Q<sub>S′</sub></italic>) plus the obtained reward <italic>r</italic>(<italic>t</italic>) is higher than <italic>Q<sub>S</sub></italic>, a globally released neuromodulator <italic>δ</italic> (see <xref rid="pcbi.1004060.e038" ref-type="disp-formula">eq. (17)</xref>) interacts with the tagged synapses to increase the strength of tagged synapses (green connections). If the predicted value is lower than expected, the strength of tagged synapses is decreased. <bold><italic>B</italic></bold>, Schematic illustration of the tagging process for regular units. FF is a feed-forward connection and FB is a feedback connection. The combination of feed-forward and feedback activation gives rise to a synaptic tag in step ii. Tags interact with the globally released neuromodulator <italic>δ</italic> to change the synaptic strength (step iv,v). <bold><italic>C</italic></bold>, Tagging process for memory units. Any presynaptic feed-forward activation gives rise to a synaptic trace (step ii; sTrace—purple circle). A feedback signal from the <italic>Q</italic>-value unit selected for action creates synaptic tags on synapses that carry a synaptic trace (step iv). The neuromodulator can interact with the tags to modify synaptic strength (v,vi).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g001" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Input layer</title>
<p>At the start of every time step, feedforward connections propagate information from the sensory layer to the association layer through modifiable connections <italic>v<sub>ij</sub></italic>. The sensory layer represents stimuli with instantaneous and transient units (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). Instantaneous units represent the current sensory stimulus <italic>x</italic>(<italic>t</italic>) and are active as long as the stimulus is present. Transient units represent changes in the stimulus and behave like ‘on (+)’ and ‘off (-)’ cells in sensory cortices [<xref rid="pcbi.1004060.ref019" ref-type="bibr">19</xref>]. They encode positive and negative changes in sensory inputs w.r.t. the previous time-step <italic>t</italic> - 1:
<disp-formula id="pcbi.1004060.e002">
<alternatives>
<graphic id="pcbi.1004060.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pcbi.1004060.e003">
<alternatives>
<graphic id="pcbi.1004060.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mtext> </mml:mtext><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
where [⋅]<sub>+</sub> is a threshold operation that returns 0 for all negative values, but leaves positive values unchanged. Every input is therefore represented by three sensory units. We assume that all units have zero activity at the start of the trial (<italic>t</italic> = 0), and that <italic>t</italic> = 1 at the first time-step of the trial.</p>
</sec>
<sec id="sec005">
<title>Association layer</title>
<p>The second (hidden) layer of the network models the association cortex, and contains regular units (circles in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>) and memory units (diamonds). We use the term ‘regular unit’ to reflect the fact that these are regular sigmoidal units that do not exhibit persistent activity in the absence of input. Regular units <italic>j</italic> are fully connected to instantaneous units <italic>i</italic> in the sensory layer by connections <inline-formula id="pcbi.1004060.e004"><alternatives><graphic id="pcbi.1004060.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e004" xlink:type="simple"/><mml:math display="inline" id="M4" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (the superscript <italic>R</italic> indexes synapses onto regular units, and <inline-formula id="pcbi.1004060.e005"><alternatives><graphic id="pcbi.1004060.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e005" xlink:type="simple"/><mml:math display="inline" id="M5" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is a bias weight). Their activity <inline-formula id="pcbi.1004060.e006"><alternatives><graphic id="pcbi.1004060.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e006" xlink:type="simple"/><mml:math display="inline" id="M6" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is determined by:
<disp-formula id="pcbi.1004060.e007">
<alternatives>
<graphic id="pcbi.1004060.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e007" xlink:type="simple"/>
<mml:math display="block" id="M7" overflow="scroll">
<mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
<disp-formula id="pcbi.1004060.e008">
<alternatives>
<graphic id="pcbi.1004060.e008g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e008" xlink:type="simple"/>
<mml:math display="block" id="M8" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
here <inline-formula id="pcbi.1004060.e009"><alternatives><graphic id="pcbi.1004060.e009g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e009" xlink:type="simple"/><mml:math display="inline" id="M9" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the synaptic input and <italic>σ</italic> a sigmoidal activation function;
<disp-formula id="pcbi.1004060.e010">
<alternatives>
<graphic id="pcbi.1004060.e010g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e010" xlink:type="simple"/>
<mml:math display="block" id="M10" overflow="scroll">
<mml:mrow><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mtext>​</mml:mtext><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula>
although our results do not depend on this particular choice of <italic>σ</italic>. The derivative of <inline-formula id="pcbi.1004060.e011"><alternatives><graphic id="pcbi.1004060.e011g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e011" xlink:type="simple"/><mml:math display="inline" id="M11" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be conveniently expressed as:
<disp-formula id="pcbi.1004060.e012">
<alternatives>
<graphic id="pcbi.1004060.e012g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e012" xlink:type="simple"/>
<mml:math display="block" id="M12" overflow="scroll">
<mml:mrow><mml:msubsup><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>Memory units <italic>m</italic> (diamonds in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>) are fully connected to the transient (+/-) units in the sensory layer by connections <inline-formula id="pcbi.1004060.e013"><alternatives><graphic id="pcbi.1004060.e013g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e013" xlink:type="simple"/><mml:math display="inline" id="M13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (superscript <italic>M</italic> indexes synapses onto memory units) and they integrate their input over the duration of the trial:
<disp-formula id="pcbi.1004060.e014">
<alternatives>
<graphic id="pcbi.1004060.e014g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e014" xlink:type="simple"/>
<mml:math display="block" id="M14" overflow="scroll">
<mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mtext>l</mml:mtext><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
<disp-formula id="pcbi.1004060.e015">
<alternatives>
<graphic id="pcbi.1004060.e015g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e015" xlink:type="simple"/>
<mml:math display="block" id="M15" overflow="scroll">
<mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula>
where we use the shorthand <inline-formula id="pcbi.1004060.e016"><alternatives><graphic id="pcbi.1004060.e016g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e016" xlink:type="simple"/><mml:math display="inline" id="M16" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> that stands for both + and - cells, so <inline-formula id="pcbi.1004060.e017"><alternatives><graphic id="pcbi.1004060.e017g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e017" xlink:type="simple"/><mml:math display="inline" id="M17" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mtext>l</mml:mtext><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> should be read as <inline-formula id="pcbi.1004060.e018"><alternatives><graphic id="pcbi.1004060.e018g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e018" xlink:type="simple"/><mml:math display="inline" id="M18" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mtext>l</mml:mtext><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mtext>l</mml:mtext><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> The selective connectivity between the transient input units and memory cells is advantageous. We found that the learning scheme is less stable when memory units also receive input from the instantaneous input units because in that case even weak constant input becomes integrated across time as an activity ramp. We note, however, that there are other neuronal mechanisms which can prevent the integration of constant inputs. For example, the synapses between instantaneous input units and memory units could be rapidly adapting, so that the memory units only integrate variations in their input.</p>
<p>The simulated integration process causes persistent changes in the activity of memory units. It is easy to see that the activity of a memory unit equals the activity of a hypothetical regular unit that would receive input from all previous time-steps of the trial at the same time. To keep the model simple, we do not simulate the mechanisms responsible for persistent activity, which have been addressed in previous work [<xref rid="pcbi.1004060.ref020" ref-type="bibr">20</xref>–<xref rid="pcbi.1004060.ref022" ref-type="bibr">22</xref>]. Although the perfect integration assumed in <xref rid="pcbi.1004060.e014" ref-type="disp-formula">Eqn. (7)</xref> does not exist in reality, we suggest that it is an acceptable approximation for trials with a relatively short duration as in the tasks that will be described below. Indeed, there are reports of single neuron integrators in entorhinal cortex with stable firing rates that persist for ten minutes or more [<xref rid="pcbi.1004060.ref023" ref-type="bibr">23</xref>], which is orders of magnitude longer than the trials modeled here. In neurophysiological studies in behaving animals, the neurons that behave like regular and memory units in e.g. LIP [<xref rid="pcbi.1004060.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004060.ref003" ref-type="bibr">3</xref>] and frontal cortex [<xref rid="pcbi.1004060.ref024" ref-type="bibr">24</xref>] would be classified as visual cells and memory cells, respectively.</p>
</sec>
<sec id="sec006">
<title>Q-value layer</title>
<p>The third layer receives input from the association layer through plastic connections <italic>w</italic><sub>jk</sub> (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). Its task is to compute action-values (i.e. <italic>Q</italic>-values [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]) for every possible action. Specifically, a <italic>Q</italic>-value unit aims to represent the (discounted) expected reward for the remainder of a trial if the network selects an action <italic>a</italic> in the current state <italic>s</italic> [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]:
<disp-formula id="pcbi.1004060.e019">
<alternatives>
<graphic id="pcbi.1004060.e019g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e019" xlink:type="simple"/>
<mml:math display="block" id="M19" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>π</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>with</mml:mtext><mml:mspace width="0.5em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup></mml:mstyle></mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
where the <inline-formula id="pcbi.1004060.e020"><alternatives><graphic id="pcbi.1004060.e020g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e020" xlink:type="simple"/><mml:math display="inline" id="M20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mo>⋅</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> term is the expected discounted future reward <italic>R<sub>t</sub></italic> given <italic>a</italic> and <italic>s</italic>, under action-selection policy <italic>π</italic> and <inline-formula id="pcbi.1004060.e021"><alternatives><graphic id="pcbi.1004060.e021g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e021" xlink:type="simple"/><mml:math display="inline" id="M21" overflow="scroll"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> determines the discounting of future rewards <italic>r</italic>. It is informative to explicitly write out the above expectation to see that <italic>Q</italic>-values are recursively defined as:
<disp-formula id="pcbi.1004060.e022">
<alternatives>
<graphic id="pcbi.1004060.e022g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e022" xlink:type="simple"/>
<mml:math display="block" id="M22" overflow="scroll">
<mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>π</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mi>π</mml:mi></mml:mstyle><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>π</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
where <inline-formula id="pcbi.1004060.e023"><alternatives><graphic id="pcbi.1004060.e023g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e023" xlink:type="simple"/><mml:math display="inline" id="M23" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is a transition matrix, containing the probabilities that executing action <italic>a</italic> in state <italic>s</italic> will move the agent to state <italic>s</italic>′, <inline-formula id="pcbi.1004060.e024"><alternatives><graphic id="pcbi.1004060.e024g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e024" xlink:type="simple"/><mml:math display="inline" id="M24" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the expected reward for this transition, and <italic>S</italic> and <italic>A</italic> are the sets of states and actions, respectively. Note that the action selection policy <italic>π</italic> is assumed to be stochastic in general. By executing the policy <italic>π</italic>, an agent samples trajectories according to the probability distributions <italic>π</italic>, <inline-formula id="pcbi.1004060.e025"><alternatives><graphic id="pcbi.1004060.e025g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e025" xlink:type="simple"/><mml:math display="inline" id="M25" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004060.e026"><alternatives><graphic id="pcbi.1004060.e026g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e026" xlink:type="simple"/><mml:math display="inline" id="M26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> where every observed transition can be used to update the original prediction <italic>Q</italic>(<italic>s<sub>t</sub></italic>, <italic>a<sub>t</sub></italic>). Importantly, temporal difference learning schemes such as AuGMEnT are <italic>model-free</italic>, which means that they do not need explicit access to these probability distributions while improving their <italic>Q</italic>-values.</p>
<p><italic>Q</italic>-value units <italic>k</italic> are fully connected to the association layer by connections <inline-formula id="pcbi.1004060.e027"><alternatives><graphic id="pcbi.1004060.e027g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e027" xlink:type="simple"/><mml:math display="inline" id="M27" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (from regular units, with <inline-formula id="pcbi.1004060.e028"><alternatives><graphic id="pcbi.1004060.e028g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e028" xlink:type="simple"/><mml:math display="inline" id="M28" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> as bias weight) and <inline-formula id="pcbi.1004060.e029"><alternatives><graphic id="pcbi.1004060.e029g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e029" xlink:type="simple"/><mml:math display="inline" id="M29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> (from memory units). The action value <italic>q<sub>k</sub></italic>(<italic>t</italic>) is estimated as:
<disp-formula id="pcbi.1004060.e030">
<alternatives>
<graphic id="pcbi.1004060.e030g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e030" xlink:type="simple"/>
<mml:math display="block" id="M30" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula>
where <italic>q<sub>k</sub></italic>(<italic>t</italic>) aims to represent the value of action <italic>k</italic> at time step <italic>t</italic>, i.e. if <italic>a<sub>t</sub></italic> = <italic>k</italic>. In AuGMEnT, the state <italic>s</italic> in <xref rid="pcbi.1004060.e019" ref-type="disp-formula">Eq. (9)</xref> is represented by the vector of activations in the association layer. Association layer units must therefore learn to represent and memorize information about the environment to compute the value of all possible actions <italic>a</italic>. They transform a so-called partially observable Markov decision process (POMDP) where the optimal decision depends on information presented in the past into a simpler Markov decision process (MDP) by storing relevant information as persistent activity, making it available for the next decision.</p>
</sec>
<sec id="sec007">
<title>Action selection</title>
<p>The action-selection policy <italic>π</italic> is implemented by a stochastic winner-takes-all (WTA) competition biased by the <italic>Q</italic>-values. The network usually chooses the action <italic>a</italic> with the highest value, but occasionally explores other actions to improve its value estimates. We used a Max-Boltzmann controller [<xref rid="pcbi.1004060.ref025" ref-type="bibr">25</xref>] to implement the action selection policy <italic>π</italic>. It selects the greedy action (highest <italic>q<sub>k</sub></italic>(<italic>t</italic>), ties are broken randomly) with probability 1 - <italic>ε</italic>, and a random action <italic>k</italic> sampled from the Boltzmann distribution <italic>P<sub>B</sub></italic> with small probability <italic>ε</italic>:</p>
<disp-formula id="pcbi.1004060.e031">
<alternatives>
<graphic id="pcbi.1004060.e031g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e031" xlink:type="simple"/>
<mml:math display="block" id="M31" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:munder></mml:mstyle><mml:mtext>exp</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
<p>This controller ensures that the model explores all actions, but usually selects the one with the highest expected value. We assume that the controller is implemented downstream, e.g. in the motor cortex or basal ganglia, but do not simulate the details of action selection, which have been addressed previously [<xref rid="pcbi.1004060.ref026" ref-type="bibr">26</xref>–<xref rid="pcbi.1004060.ref030" ref-type="bibr">30</xref>]. After selecting an action <italic>a</italic>, the activity in the third layer becomes <italic>z<sub>k</sub></italic> = <italic>δ<sub>ka</sub></italic>, where <italic>δ<sub>ka</sub></italic> is the Kronecker delta function (1 if <italic>k</italic> = <italic>a</italic> and 0 otherwise). In other words, the selected action is the only one active after the selection process, and it then provides an “attentional” feedback signal to the association cortex (orange feedback connections in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1A</xref>).</p>
</sec>
<sec id="sec008">
<title>Learning</title>
<p>Learning in the network is controlled by two factors that gate plasticity: a global neuromodulatory signal (described below) and the attentional feedback signal. Once an action is selected, the unit that codes the winning action <italic>a</italic> feeds back to earlier processing levels to create synaptic tags [<xref rid="pcbi.1004060.ref031" ref-type="bibr">31</xref>,<xref rid="pcbi.1004060.ref032" ref-type="bibr">32</xref>], also known as eligibility traces [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>] on the responsible synapses (orange pentagons in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). Tagging of connections from the association layer to the motor layer follows a form of Hebbian plasticity: the tag strength depends on presynaptic activity (<italic>y<sub>j</sub></italic>) and postsynaptic activity <italic>after</italic> action selection (<italic>z<sub>k</sub></italic>) and tags thus only form at synapses <italic>w<sub>ja</sub></italic> onto the winning (i.e. selected) motor unit <italic>a</italic>:
<disp-formula id="pcbi.1004060.e032">
<alternatives>
<graphic id="pcbi.1004060.e032g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e032" xlink:type="simple"/>
<mml:math display="block" id="M32" overflow="scroll">
<mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo> <mml:mtext>which is equivalent to:</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo> <mml:mtext>for the winning action</mml:mtext><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mtext>because</mml:mtext><mml:mspace width="0.25em"/><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mtext>and</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo> <mml:mtext>for</mml:mtext><mml:mspace width="0.25em"/><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mtext>because</mml:mtext><mml:mspace width="0.25em"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
where <italic>α</italic> controls the decay of tags. Here, Δ denotes the change in one time-step, i.e <italic>Tag</italic>(<italic>t</italic>+1) = <italic>Tag</italic>(<italic>t</italic>)+Δ<italic>Tag</italic>(<italic>t</italic>).</p>
<p>The formation of tags on the feedback connections <inline-formula id="pcbi.1004060.e033"><alternatives><graphic id="pcbi.1004060.e033g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e033" xlink:type="simple"/><mml:math display="inline" id="M33" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> follows the same rule so that the strength of feedforward and feedback connections becomes similar during learning, in accordance with neurophysiological findings [<xref rid="pcbi.1004060.ref033" ref-type="bibr">33</xref>]. Thus, the association units that provided strong input to the winning action <italic>a</italic> also receive strongest feedback (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>, middle panel): they will be held responsible for the outcome of <italic>a</italic>. Importantly, the attentional feedback signal also guides the formation of tags on connections <italic>v<sub>ij</sub></italic> so that synapses from the input layer onto responsible association units <italic>j</italic> (strong <inline-formula id="pcbi.1004060.e034"><alternatives><graphic id="pcbi.1004060.e034g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e034" xlink:type="simple"/><mml:math display="inline" id="M34" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>) are most strongly tagged (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref>).</p>
<p>For regular units we propose:
<disp-formula id="pcbi.1004060.e035">
<alternatives>
<graphic id="pcbi.1004060.e035g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e035" xlink:type="simple"/>
<mml:math display="block" id="M35" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
where <italic>σ</italic>′ is the derivative of the association unit’s activation function <italic>σ</italic> (<xref rid="pcbi.1004060.e010" ref-type="disp-formula">Eq. (5)</xref>), which determines the influence that a change in the input <italic>inp<sub>j</sub></italic> has on the activity of unit <italic>j</italic>. The idea has been illustrated in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref>. Feedback from the winning action (lower synapse in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref>) enables the formation of tags on the feedforward connections onto the regular unit. These tags can interact with globally released neuromodulators that inform all synapses about the RPE (green cloud ‘<italic>δ</italic>’ in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). Note that feedback connections only influence the plasticity of representations in the association layer but do not influence activity in the present version of the model. We will come back to this point in the discussion.</p>
<p>In addition to synaptic tags, AuGMEnT uses synaptic traces (sTrace, blue circle in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1A,C</xref>) for the learning of new working memories. These traces are located on the synapses from the sensory units onto memory cells. Any pre-synaptic activity in these synapses leaves a trace that persists for the duration of a trial. If one of the selected actions provides a feedback signal (panel iv in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>) to the post-synaptic memory unit, the trace gives rise to a tag making the synapse plastic as it can now interact with globally released neuromodulators:
<disp-formula id="pcbi.1004060.e036">
<alternatives>
<graphic id="pcbi.1004060.e036g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e036" xlink:type="simple"/>
<mml:math display="block" id="M36" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<disp-formula id="pcbi.1004060.e037">
<alternatives>
<graphic id="pcbi.1004060.e037g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e037" xlink:type="simple"/>
<mml:math display="block" id="M37" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
We assume that the time scale of trace updates is fast compared to the tag updates, so that tags are updated with the latest traces. The traces persist for the duration of the trial, but all tags decay exponentially (0&lt;<italic>α</italic>&lt;1).</p>
<p>After executing an action, the network may receive a reward <italic>r</italic>(<italic>t</italic>). Moreover, an action <italic>a</italic> at time step (<italic>t</italic>-1) may have caused a change in the sensory stimulus. For example, in most studies of monkey vision, a visual stimulus appears if the animal directs gaze to a fixation point. In the model, the new stimulus causes feedforward processing on the next time step <italic>t</italic>, which results in another set of <italic>Q</italic>-values. To evaluate whether <italic>a</italic> was better or worse than expected, the model compares the predicted outcome <italic>Q<sub>a</sub></italic>(<italic>t</italic>-1), which has to be temporarily stored in the system, to the sum of the reward <italic>r</italic>(<italic>t</italic>) and the discounted action-value <italic>Q<sub>a′</sub></italic>(<italic>t</italic>) of unit <italic>a′</italic> that wins the subsequent stochastic WTA-competition. This temporal difference learning rule is known as SARSA [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref034" ref-type="bibr">34</xref>]:
<disp-formula id="pcbi.1004060.e038">
<alternatives>
<graphic id="pcbi.1004060.e038g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e038" xlink:type="simple"/>
<mml:math display="block" id="M38" overflow="scroll">
<mml:mrow><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.5em"/><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(17)</label>
</disp-formula></p>
<p>The RPE <italic>δ</italic>(<italic>t</italic>) is positive if the outcome of <italic>a</italic> is better than expected and negative if it is worse. Neurons representing action values have been found in the frontal cortex, basal ganglia and midbrain [<xref rid="pcbi.1004060.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004060.ref035" ref-type="bibr">35</xref>,<xref rid="pcbi.1004060.ref036" ref-type="bibr">36</xref>] and some orbitofrontal neurons specifically code the chosen value, <italic>q<sub>a</sub></italic> [<xref rid="pcbi.1004060.ref037" ref-type="bibr">37</xref>]. Moreover, dopamine neurons in the ventral tegmental area and substantia nigra represent <italic>δ</italic> [<xref rid="pcbi.1004060.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1004060.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004060.ref038" ref-type="bibr">38</xref>]. In the model, the release of neuromodulators makes <italic>δ</italic> available throughout the brain (green cloud in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>).</p>
<p>Plasticity of all synapses depends on the product of <italic>δ</italic> and tag strength:
<disp-formula id="pcbi.1004060.e039">
<alternatives>
<graphic id="pcbi.1004060.e039g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e039" xlink:type="simple"/>
<mml:math display="block" id="M39" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(18)</label>
</disp-formula>
where <italic>β</italic> is the learning rate, and where the latter equation also holds for the feedback weights <inline-formula id="pcbi.1004060.e040"><alternatives><graphic id="pcbi.1004060.e040g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e040" xlink:type="simple"/><mml:math display="inline" id="M40" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. These equations capture the key idea of AuGMEnT: tagged synapses are held accountable for the RPE and change their strength accordingly. Note that AuGMEnT uses a four-factor learning rule for synapses <italic>v<sub>ij</sub></italic>. The first two factors are the pre- and postsynaptic activity that determine the formation of tags (Eqns. (<xref rid="pcbi.1004060.e035" ref-type="disp-formula">14</xref>)–(<xref rid="pcbi.1004060.e037" ref-type="disp-formula">16</xref>)). The third factor is the “attentional” feedback from the motor selection stage, which ensures that tags are only formed in the circuit that is responsible for the selected action. The fourth factor is the RPE <italic>δ</italic>, which reflects whether the outcome of an action was better or worse than expected and determines if the tagged synapses increase or decrease in strength. The computation of the RPE demands the comparison of <italic>Q</italic>-values in different time-steps. The RPE at time <italic>t</italic> depends on the action that the network selected at <italic>t</italic>-1 (see <xref rid="pcbi.1004060.e038" ref-type="disp-formula">Eqn. (17)</xref> and the next section), but the activity of the units that gave rise to this selection have typically changed at time <italic>t</italic>. The synaptic tags solve this problem because they labeled those synapses that were responsible for the selection of the previous action.</p>
<p>AuGMEnT is biologically plausible because the equations that govern the formation of synaptic tags (Eqns. (<xref rid="pcbi.1004060.e032" ref-type="disp-formula">13</xref>), (<xref rid="pcbi.1004060.e035" ref-type="disp-formula">14</xref>), <xref rid="pcbi.1004060.e037" ref-type="disp-formula">(16)</xref>) and traces (<xref rid="pcbi.1004060.e036" ref-type="disp-formula">Eq. (15)</xref>) and the equations that govern plasticity (<xref rid="pcbi.1004060.e039" ref-type="disp-formula">Eq. (18)</xref>) rely only on information that is available locally, at the synapse. Furthermore, the hypothesis that a neuromodulatory signal, like dopamine, broadcasts the RPE to all synapses in the network is supported by neurobiological findings [<xref rid="pcbi.1004060.ref009" ref-type="bibr">9</xref>,<xref rid="pcbi.1004060.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004060.ref038" ref-type="bibr">38</xref>].</p>
</sec>
</sec>
<sec id="sec009" sec-type="results">
<title>Results</title>
<p>We will now present the main theoretical result, which is that the AuGMEnT learning rules minimize the temporal difference errors (<xref rid="pcbi.1004060.e038" ref-type="disp-formula">Eqn. (17)</xref>) of the transitions that are experienced by the network by on-line gradient descent. Although AuGMEnT is not guaranteed to find optimal solutions (we cannot provide a proof of convergence), we found that it reliably learns difficult non-linear working memory problems, as will be illustrated below.</p>
<sec id="sec010">
<title>AuGMEnT minimizes the reward-prediction error (RPE)</title>
<p>The aim of AuGMEnT is to reduce the RPE <italic>δ</italic>(<italic>t</italic>) because low RPEs for all network states imply reliable <italic>Q</italic>-values so that the network can choose the action that maximizes reward at every time-step. The RPE <italic>δ</italic>(<italic>t</italic>) implies a comparison between two quantities: the <italic>predicted Q</italic>-value before the transition, <italic>q<sub>a</sub></italic>(<italic>t</italic>-1), and a <italic>target Q</italic>-value <italic>r</italic>(<italic>t</italic>)+<italic>γq<sub>a′</sub></italic>(<italic>t</italic>), which consists of the actually observed reward and the next predicted <italic>Q</italic>-value [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]. If the two terms cancel, the prediction was correct. SARSA aims to minimize the prediction error by adjusting the network weights <italic>w</italic> to improve the prediction <italic>q<sub>a</sub></italic>(<italic>t</italic>-1) to bring it closer to the observed value <italic>r</italic>(<italic>t</italic>)+<italic>γq<sub>a′</sub></italic>(<italic>t</italic>). It is convenient to do this through on-line gradient descent on the squared prediction error <inline-formula id="pcbi.1004060.e041"><alternatives><graphic id="pcbi.1004060.e041g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e041" xlink:type="simple"/><mml:math display="inline" id="M41" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> with respect to the parameters <italic>w</italic> [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>,<xref rid="pcbi.1004060.ref034" ref-type="bibr">34</xref>]:
<disp-formula id="pcbi.1004060.e042">
<alternatives>
<graphic id="pcbi.1004060.e042g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e042" xlink:type="simple"/>
<mml:math display="block" id="M42" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:mi>w</mml:mi><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(19)</label>
</disp-formula>
where <inline-formula id="pcbi.1004060.e043"><alternatives><graphic id="pcbi.1004060.e043g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e043" xlink:type="simple"/><mml:math display="inline" id="M43" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> is the gradient of the predicted <italic>Q</italic>-value <italic>Q<sub>a</sub></italic>(<italic>t</italic>-1) with respect to parameters <italic>w</italic>. In <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref> we have used <inline-formula id="pcbi.1004060.e044"><alternatives><graphic id="pcbi.1004060.e044g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e044" xlink:type="simple"/><mml:math display="inline" id="M44" overflow="scroll"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, which follows from the definition of <italic>E</italic>(<italic>q<sub>a</sub></italic>(<italic>t</italic>-1)). Note that <italic>E</italic> is defined with regard to the sampled transition only so that the definition typically differs between successive transitions experienced by the network. For notational convenience we will abbreviate <italic>E</italic>(<italic>q<sub>a</sub></italic>(<italic>t</italic>-1)) to <italic>E<sub>q<sub>a</sub></sub></italic> in the remainder of this paper.</p>
<p>We will refer to the negative of <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref> as “error gradient” in the remainder of this paper. The RPE is high if the sum of the reward <italic>r</italic>(<italic>t</italic>) and discounted <italic>q<sub>a′</sub></italic>(<italic>t</italic>) deviates strongly from the prediction <italic>q<sub>a</sub></italic>(<italic>t</italic>-1) on the previous time step. As in other SARSA methods, the updating of synaptic weights is only performed for the transitions that the network actually experiences. In other words, AuGMEnT is a so-called “on policy” learning method [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>].</p>
<p>We will first establish the equivalence of on-line gradient descent defined in <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref> and the AuGMEnT learning rule for the synaptic weights <inline-formula id="pcbi.1004060.e045"><alternatives><graphic id="pcbi.1004060.e045g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e045" xlink:type="simple"/><mml:math display="inline" id="M45" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> from the regular units onto the <italic>Q</italic>-value units (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). According to <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref>, weights <inline-formula id="pcbi.1004060.e046"><alternatives><graphic id="pcbi.1004060.e046g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e046" xlink:type="simple"/><mml:math display="inline" id="M46" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> for the chosen action <italic>k</italic> = <italic>a</italic> on time step <italic>t</italic>-1 should change as:
<disp-formula id="pcbi.1004060.e047">
<alternatives>
<graphic id="pcbi.1004060.e047g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e047" xlink:type="simple"/>
<mml:math display="block" id="M47" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mspace width="0.5em"/><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(20)</label>
</disp-formula>
leaving the other weights <italic>k</italic>≠<italic>a</italic> unchanged.</p>
<p>We will now show that AuGMEnT causes equivalent changes in synaptic strength. It follows from <xref rid="pcbi.1004060.e030" ref-type="disp-formula">Eq. (11)</xref> that the influence of <inline-formula id="pcbi.1004060.e048"><alternatives><graphic id="pcbi.1004060.e048g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e048" xlink:type="simple"/><mml:math display="inline" id="M48" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> on <italic>q<sub>a</sub></italic>(<italic>t</italic>-1) (i.e. <inline-formula id="pcbi.1004060.e049"><alternatives><graphic id="pcbi.1004060.e049g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e049" xlink:type="simple"/><mml:math display="inline" id="M49" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> in <xref rid="pcbi.1004060.e047" ref-type="disp-formula">Eq. (20)</xref>) equals <inline-formula id="pcbi.1004060.e050"><alternatives><graphic id="pcbi.1004060.e050g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e050" xlink:type="simple"/><mml:math display="inline" id="M50" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the activity of association unit <italic>j</italic> on the previous time step. This result allows us to rewrite <xref rid="pcbi.1004060.e047" ref-type="disp-formula">(20)</xref> as:
<disp-formula id="pcbi.1004060.e051">
<alternatives>
<graphic id="pcbi.1004060.e051g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e051" xlink:type="simple"/>
<mml:math display="block" id="M51" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(21)</label>
</disp-formula></p>
<p>Recall from <xref rid="pcbi.1004060.e032" ref-type="disp-formula">Eq. (13)</xref> that the tags on synapses onto the winning output unit <italic>a</italic> are updated according to Δ<italic>Tag<sub>ja</sub></italic> = - <italic>αTag<sub>ja</sub></italic>+<italic>y<sub>j</sub></italic> (orange pentagons in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>). In the special case <italic>α</italic> = 1, it follows that on time step <italic>t</italic>, <inline-formula id="pcbi.1004060.e052"><alternatives><graphic id="pcbi.1004060.e052g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e052" xlink:type="simple"/><mml:math display="inline" id="M52" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and that tags on synapses onto output units <italic>k</italic>≠<italic>a</italic> are 0. As a result,
<disp-formula id="pcbi.1004060.e053">
<alternatives>
<graphic id="pcbi.1004060.e053g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e053" xlink:type="simple"/>
<mml:math display="block" id="M53" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(22)</label>
</disp-formula>
<disp-formula id="pcbi.1004060.e054">
<alternatives>
<graphic id="pcbi.1004060.e054g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e054" xlink:type="simple"/>
<mml:math display="block" id="M54" overflow="scroll">
<mml:mrow><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(23)</label>
</disp-formula>
for the synapses onto the selected action <italic>a</italic>, and the second, generalized, equation follows from the fact that <inline-formula id="pcbi.1004060.e055"><alternatives><graphic id="pcbi.1004060.e055g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e055" xlink:type="simple"/><mml:math display="inline" id="M55" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for output units <italic>k</italic>≠<italic>a</italic> that were not selected and therefore do not contribute to the RPE. Inspection of Eqns. (<xref rid="pcbi.1004060.e039" ref-type="disp-formula">18</xref>) and (<xref rid="pcbi.1004060.e054" ref-type="disp-formula">23</xref>) reveals that AuGMEnT indeed takes a step of size <italic>β</italic> in the direction opposite to the error gradient of <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref> (provided <italic>α</italic> = 1; we discuss the case <italic>α</italic>≠1 below).</p>
<p>The updates for synapses between memory units <italic>m</italic> and <italic>Q</italic>-value units <italic>k</italic> are equivalent to those between regular units and the <italic>Q</italic>-value units. Thus,
<disp-formula id="pcbi.1004060.e056">
<alternatives>
<graphic id="pcbi.1004060.e056g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e056" xlink:type="simple"/>
<mml:math display="block" id="M56" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mspace width="0.5em"/><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(24)</label>
</disp-formula></p>
<p>The plasticity of the feedback connections <inline-formula id="pcbi.1004060.e057"><alternatives><graphic id="pcbi.1004060.e057g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e057" xlink:type="simple"/><mml:math display="inline" id="M57" overflow="scroll"><mml:mrow><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004060.e058"><alternatives><graphic id="pcbi.1004060.e058g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e058" xlink:type="simple"/><mml:math display="inline" id="M58" overflow="scroll"><mml:mrow><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> from the <italic>Q</italic>-value layer to the association layer follows the same rule as the updates of connections <inline-formula id="pcbi.1004060.e059"><alternatives><graphic id="pcbi.1004060.e059g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e059" xlink:type="simple"/><mml:math display="inline" id="M59" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004060.e060"><alternatives><graphic id="pcbi.1004060.e060g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e060" xlink:type="simple"/><mml:math display="inline" id="M60" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and the feedforward and feedback connections between two units therefore become proportional during learning [<xref rid="pcbi.1004060.ref014" ref-type="bibr">14</xref>].</p>
<p>We will now show that synapses <inline-formula id="pcbi.1004060.e061"><alternatives><graphic id="pcbi.1004060.e061g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e061" xlink:type="simple"/><mml:math display="inline" id="M61" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> between the input layer and the regular association units (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref>) also change according to the negative gradient of the error function defined above. Applying the chain rule to compute the influence of <inline-formula id="pcbi.1004060.e062"><alternatives><graphic id="pcbi.1004060.e062g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e062" xlink:type="simple"/><mml:math display="inline" id="M62" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> on <italic>q<sub>a</sub></italic>(<italic>t</italic>-1) results in the following equation:
<disp-formula id="pcbi.1004060.e063">
<alternatives>
<graphic id="pcbi.1004060.e063g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e063" xlink:type="simple"/>
<mml:math display="block" id="M63" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(25)</label>
</disp-formula></p>
<p>The amount of attentional feedback that was received by unit <italic>j</italic> from the selected <italic>Q</italic>-value unit <italic>a</italic> at time <italic>t</italic>-1 is equal to <inline-formula id="pcbi.1004060.e064"><alternatives><graphic id="pcbi.1004060.e064g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e064" xlink:type="simple"/><mml:math display="inline" id="M64" overflow="scroll"><mml:mrow><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> because the activity of unit <italic>a</italic> equals 1 once it has been selected. As indicated above, learning makes the strength of feedforward and feedback connections similar so that <inline-formula id="pcbi.1004060.e065"><alternatives><graphic id="pcbi.1004060.e065g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e065" xlink:type="simple"/><mml:math display="inline" id="M65" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> can be estimated as the amount of feedback <inline-formula id="pcbi.1004060.e066"><alternatives><graphic id="pcbi.1004060.e066g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e066" xlink:type="simple"/><mml:math display="inline" id="M66" overflow="scroll"><mml:mrow><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> that unit <italic>j</italic> receives from the selected action <italic>a</italic>,
<disp-formula id="pcbi.1004060.e067">
<alternatives>
<graphic id="pcbi.1004060.e067g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e067" xlink:type="simple"/>
<mml:math display="block" id="M67" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(26)</label>
</disp-formula></p>
<p>Recall from <xref rid="pcbi.1004060.e035" ref-type="disp-formula">Eq. (14)</xref> that the tags on synapses <inline-formula id="pcbi.1004060.e068"><alternatives><graphic id="pcbi.1004060.e068g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e068" xlink:type="simple"/><mml:math display="inline" id="M68" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are updated according to <inline-formula id="pcbi.1004060.e069"><alternatives><graphic id="pcbi.1004060.e069g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e069" xlink:type="simple"/><mml:math display="inline" id="M69" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref> illustrates how feedback from action <italic>a</italic> controls the tag formation process. If <italic>α</italic> = 1, then on time step <italic>t</italic>, <inline-formula id="pcbi.1004060.e070"><alternatives><graphic id="pcbi.1004060.e070g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e070" xlink:type="simple"/><mml:math display="inline" id="M70" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> so that <xref rid="pcbi.1004060.e067" ref-type="disp-formula">Eq. (26)</xref> can be written as:
<disp-formula id="pcbi.1004060.e071">
<alternatives>
<graphic id="pcbi.1004060.e071g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e071" xlink:type="simple"/>
<mml:math display="block" id="M71" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(27)</label>
</disp-formula></p>
<p>A comparison to <xref rid="pcbi.1004060.e039" ref-type="disp-formula">Eq. (18)</xref> demonstrates that AuGMEnT also takes a step of size <italic>β</italic> in the direction opposite to the error gradient for these synapses.</p>
<p>The final set of synapses that needs to be considered are between the transient sensory units and the memory units. We approximate the total input <inline-formula id="pcbi.1004060.e072"><alternatives><graphic id="pcbi.1004060.e072g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e072" xlink:type="simple"/><mml:math display="inline" id="M72" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of memory unit <italic>m</italic> as (see <xref rid="pcbi.1004060.e014" ref-type="disp-formula">Eq. (7)</xref>):
<disp-formula id="pcbi.1004060.e073">
<alternatives>
<graphic id="pcbi.1004060.e073g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e073" xlink:type="simple"/>
<mml:math display="block" id="M73" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>l</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(28)</label>
</disp-formula>
The approximation is good if synapses <inline-formula id="pcbi.1004060.e074"><alternatives><graphic id="pcbi.1004060.e074g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e074" xlink:type="simple"/><mml:math display="inline" id="M74" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> change slowly during a trial. According to <xref rid="pcbi.1004060.e042" ref-type="disp-formula">Equation (19)</xref>, the update for these synapses is:
<disp-formula id="pcbi.1004060.e075">
<alternatives>
<graphic id="pcbi.1004060.e075g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e075" xlink:type="simple"/>
<mml:math display="block" id="M75" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
<label>(29)</label>
</disp-formula></p>
<p><xref rid="pcbi.1004060.e036" ref-type="disp-formula">Eq. (15)</xref> specifies that Δ<italic>sTrace<sub>lm</sub></italic> = <italic>x<sub>l</sub></italic> so that <inline-formula id="pcbi.1004060.e076"><alternatives><graphic id="pcbi.1004060.e076g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e076" xlink:type="simple"/><mml:math display="inline" id="M76" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>l</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>, the total presynaptic activity of the input unit up to time <italic>t</italic>-1 (blue circle in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>). Thus, <xref rid="pcbi.1004060.e075" ref-type="disp-formula">Eq. (29)</xref> can also be written as:
<disp-formula id="pcbi.1004060.e077">
<alternatives>
<graphic id="pcbi.1004060.e077g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e077" xlink:type="simple"/>
<mml:math display="block" id="M77" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(30)</label>
</disp-formula></p>
<p><xref rid="pcbi.1004060.e037" ref-type="disp-formula">Eq. (16)</xref> states that <inline-formula id="pcbi.1004060.e078"><alternatives><graphic id="pcbi.1004060.e078g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e078" xlink:type="simple"/><mml:math display="inline" id="M78" overflow="scroll"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>)</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, because the feedback from the winning action <italic>a</italic> converts the trace into a tag (panel iv in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>). Thus, if <italic>α</italic> = 1 then <inline-formula id="pcbi.1004060.e079"><alternatives><graphic id="pcbi.1004060.e079g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e079" xlink:type="simple"/><mml:math display="inline" id="M79" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:msup><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msup><mml:mi>σ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> so that:
<disp-formula id="pcbi.1004060.e080">
<alternatives>
<graphic id="pcbi.1004060.e080g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e080" xlink:type="simple"/>
<mml:math display="block" id="M80" overflow="scroll">
<mml:mrow><mml:mi>Δ</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>∝</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow>
</mml:math>
</alternatives>
<label>(31)</label>
</disp-formula>
Again, a comparison of Eqns. (<xref rid="pcbi.1004060.e080" ref-type="disp-formula">31</xref>) and (<xref rid="pcbi.1004060.e039" ref-type="disp-formula">18</xref>) shows that AuGMEnT takes a step of size <italic>β</italic> in the direction opposite to the error gradient, just as is the case for all other categories of synapses. We conclude that AuGMEnT causes an on-line gradient descent on all synaptic weights to minimize the temporal difference error if <italic>α</italic> = 1.</p>
<p>AuGMEnT provides a biological implementation of the well known RL method called SARSA, although it also goes beyond traditional SARSA [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>] by (i) including memory units (ii) representing the current state of the external world as a vector of activity at the input layer (iii) providing an association layer that aids in computing <italic>Q</italic>-values that depend non-linearly on the input, thus providing a biologically plausible equivalent of the error-backpropagation learning rule [<xref rid="pcbi.1004060.ref008" ref-type="bibr">8</xref>], and (iv) using synaptic tags and traces (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B,C</xref>) so that all the information necessary for plasticity is available locally at every synapse.</p>
<p>The tags and traces determine the plasticity of memory units and aid in decreasing the RPE by improving the <italic>Q</italic>-value estimates. If a memory unit <italic>j</italic> receives input from input unit <italic>i</italic> then a trace of this input is maintained at synapse <italic>v<sub>ij</sub></italic> for the remainder of the trial (blue circle in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>). Suppose that <italic>j</italic>, in turn, is connected to action <italic>a</italic> which is selected at a later time point. Now unit <italic>j</italic> receives feedback from <italic>a</italic> so that the trace on synapse <italic>v<sub>ij</sub></italic> becomes a tag making it sensitive to the globally released neuromodulator that codes the RPE <italic>δ</italic> (panel iv in <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>). If the outcome of <italic>a</italic> was better than expected (<italic>δ</italic>&gt;0) (green cloud in panel v), <italic>v<sub>ij</sub></italic> strengthens (thicker synapse in panel vi). When the stimulus that activated unit <italic>i</italic> reappears on a later trial, the larger <italic>v<sub>ij</sub></italic> increases unit <italic>j</italic> ’s persistent activity which, in turn, enhances the activity of the <italic>Q</italic>-value unit representing <italic>a</italic>, thereby decreasing the RPE.</p>
<p>The synaptic tags of AuGMEnT correspond to the eligibility traces used in RL schemes. In SARSA learning speeds up if the eligibility traces do not fully decay on every time step, but exponentially with parameter λ∈[0,1] [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]; the resulting rule is called SARSA(λ). In AuGMEnT, the parameter <italic>α</italic> plays an equivalent role and precise equivalence can be obtained by setting <italic>α</italic> = 1-λ<italic>γ</italic> as can be verified by making this substitution in Eqn. (<xref rid="pcbi.1004060.e032" ref-type="disp-formula">13</xref>) (<xref rid="pcbi.1004060.e035" ref-type="disp-formula">14</xref>) and (<xref rid="pcbi.1004060.e037" ref-type="disp-formula">16</xref>) (noting that <italic>Tag</italic>(<italic>t</italic>+1) = <italic>Tag</italic>(<italic>t</italic>)+Δ<italic>Tag</italic>(<italic>t</italic>)). It follows that tags decay exponentially as <italic>Tag</italic>(<italic>t</italic>+1) = λ<italic>γTag</italic>(t), equivalent to the decay of eligibility traces in SARSA(λ). These results establish the correspondence between the biologically inspired AuGMEnT learning scheme and the RL method SARSA(λ). A special condition occurs at the end of a trial. The activity of memory units, traces, tags, and <italic>Q</italic>-values are set to zero (see [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]), <italic>after</italic> updating of the weights with a <italic>δ</italic> that reflects the transition to the terminal state.</p>
<p>In the remainder of the results section we will illustrate how AuGMEnT can train multi-layered networks with the form of <xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1</xref> to perform a large variety of tasks that have been used to study neuronal representations in the association cortex of monkeys.</p>
</sec>
<sec id="sec011">
<title>Using AuGMEnT to simulate animal learning experiments</title>
<p>We tested AuGMEnT on four different tasks that have been used to investigate the learning of working memory representations in monkeys. The first three tasks have been used to study the influence of learning on neuronal activity in area LIP and the fourth task to study vibrotactile working memory in multiple cortical regions. All tasks have a similar overall structure: the monkey starts a trial by directing gaze to a fixation point or by touching a response key. Then stimuli are presented to the monkey and it has to respond with the correct action after a memory delay. At the end of a trial, the model could choose between two possible actions. The full task reward (<italic>r<sub>f</sub></italic>, 1.5 units) was given if this choice was correct, while we aborted trials and gave no reward if the model made the wrong choice or broke fixation (released the key) before a go signal.</p>
<p>Researchers usually train monkeys on these tasks with a shaping strategy. The monkey starts with simple tasks and then the complexity is gradually increased. It is also common to give small rewards for reaching intermediate goals in the task, such as attaining fixation. We encouraged fixation (or touching the key in the vibrotactile task below) by giving a small shaping reward (<italic>r<sub>i</sub></italic>, 0.2 units) if the model directed gaze to the fixation point (touched the key). In the next section we will demonstrate that the training of networks with AuGMEnT is facilitated by shaping. Shaping was not necessary for learning in any of the tasks, however, but it enhanced learning speed and increased the proportion of networks that learned the task within the alloted number of training trials.</p>
<p>Across all the simulations, we used a single, fixed configuration of the association layer (three regular units, four memory units) and <italic>Q</italic>-layer (three units) and a single set of learning parameters (Tables <xref rid="pcbi.1004060.t001" ref-type="table">1</xref>,<xref rid="pcbi.1004060.t002" ref-type="table">2</xref>). The number of input units varied across tasks as the complexity of the sensory stimuli differed. We note, however, that the results described below would have been identical had we simulated a fixed, large input layer with silent input units in some of the tasks, because silent input units have no influence on activity in the rest of the network.</p>
<table-wrap id="pcbi.1004060.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.t001</object-id>
<label>Table 1</label> <caption><title>Model parameters.</title></caption>
<alternatives>
<graphic id="pcbi.1004060.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Parameter</th>
<th align="left" rowspan="1" colspan="1">Description</th>
<th align="left" rowspan="1" colspan="1">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>β</italic></td>
<td align="left" rowspan="1" colspan="1">Learning rate</td>
<td align="left" rowspan="1" colspan="1">0.15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">λ</td>
<td align="left" rowspan="1" colspan="1">Tag/Trace decay rate</td>
<td align="left" rowspan="1" colspan="1">0.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>γ</italic></td>
<td align="left" rowspan="1" colspan="1">Discount factor</td>
<td align="left" rowspan="1" colspan="1">0.90</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>α</italic></td>
<td align="left" rowspan="1" colspan="1">Tag persistence</td>
<td align="left" rowspan="1" colspan="1">1-λ<italic>γ</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>ε</italic></td>
<td align="left" rowspan="1" colspan="1">Exploration rate</td>
<td align="left" rowspan="1" colspan="1">0.025</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1004060.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.t002</object-id>
<label>Table 2</label> <caption><title>Network architecture parameters.</title></caption>
<alternatives>
<graphic id="pcbi.1004060.t002g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.t002" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Architecture</th>
<th align="left" rowspan="1" colspan="1">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Input units</bold></td>
<td align="left" rowspan="1" colspan="1">Task dependent</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Memory units</bold></td>
<td align="left" rowspan="1" colspan="1">N = 4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Regular units</bold></td>
<td align="left" rowspan="1" colspan="1">N = 3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Q-value units</bold></td>
<td align="left" rowspan="1" colspan="1">N = 3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Initial weights</bold></td>
<td align="left" rowspan="1" colspan="1">Uniform over [-0.25,0.25]</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec012">
<title>Saccade/antisaccade task</title>
<p>The first task (<xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2A</xref>) is a memory saccade/anti-saccade task modeled after Gottlieb and Goldberg [<xref rid="pcbi.1004060.ref003" ref-type="bibr">3</xref>]. Every trial started with an empty screen, shown for one time step. Then a fixation mark was shown that was either black or white, indicating that a pro- or anti-saccade would be required. The model had to fixate within 10 time-steps, otherwise the trial was terminated without reward. If the model fixated for two time-steps, we presented a cue on the left or the right side of the screen for one time-step and gave the fixation reward <italic>r</italic><sub>i</sub>. This was followed by a memory delay of two time steps during which only the fixation point was visible. At the end of the memory delay the fixation mark turned off. To collect the final reward <italic>r</italic><sub>f</sub> in the pro-saccade condition, the model had to make an eye-movement to the remembered location of the cue and to the opposite location on anti-saccade trials. The trial was aborted if the model failed to respond within eight time steps.</p>
<fig id="pcbi.1004060.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Saccade/antisaccade task.</title>
<p><bold><italic>A</italic></bold>, Structure of the task, all possible trials have been illustrated. Fixation mark color indicates whether a saccade (P) or anti-saccade (A) is required after a memory delay. Colored arrows show the required action for the indicated trial types. L: cue left; R: cue right. <bold><italic>B</italic></bold>, The sensory layer represents the visual information (fixation point, cue left/right) with sustained and transient (on/off) units. Units in the <italic>Q</italic>-value layer code three possible eye positions: left (green), center (blue) and right (red). <bold><italic>C</italic></bold>, Time course of learning: 10,000 networks were trained, of which 9,945 learned the task within 25,000 trials. Histograms show the distribution of trials when the model learned to fixate (‘fix’), maintain fixation until the ‘go’-signal (‘go’) and learned the complete task (‘task’). <bold><italic>D</italic></bold>, Activity of example units in the association and Q-layer. The grey trace illustrates a regular unit and the green and orange traces memory units. The bottom graphs show activity of the Q-value layer cells. Colored letters denote the action with highest <italic>Q</italic>-value. Like the memory cells, <italic>Q</italic>-value units also have delay activity that is sensitive to cue location (* in the lower panel) and their activity increases after the go-signal. <bold><italic>E</italic></bold>, 2D-PCA projection of sequence of association layer activations for the four different trial types for an example network. S marks the start of the trials (empty screen). Pro saccade trials are shown with solid lines and anti-saccade trials with dashed lines. Color indicates cue location (green – left; red – right) and labels indicate trial type (P/A = type pro/anti; L/R = cue left/right). Percentages on the axes show variance explained by the PCs. <bold><italic>F</italic></bold>, Mean variance explained as a function of the number of PCs over all 100 trained networks, error bars s.d. <bold><italic>G</italic></bold>, Pairwise analysis of activation vectors of different unit types in the network (see main text for explanation). MEM: memory; REG: regular. This panel is aligned with the events in panel (A). Each square within a matrix indicates the proportion of networks where the activity vectors of different trial types were most similar. Color scale is shown below. For example, the right top square for the memory unit matrix in the ‘go’ phase of the task indicates that around 25% of the networks had memory activation vectors that were most similar for Pro-Left and Anti-Right trials. <bold><italic>H</italic></bold>, Pairwise analysis of activation-vectors for networks trained on a version of the task where only pro-saccades were required. Conventions as in (G).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g002" position="float" xlink:type="simple"/>
</fig>
<p>The input units of the model (<xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2B</xref>) represented the color of the fixation point and the presence of the peripheral cues. The three <italic>Q</italic>-value units had to represent the value of directing gaze to the centre, left and right side of the screen. This task can only be solved by storing cue location in working memory and, in addition, requires a non-linear transformation and can therefore not be solved by a linear mapping from the sensory units to the <italic>Q</italic>-value units. We trained the models for maximally 25,000 trials, or until they learned the task. We kept track of accuracy for all four trial types as the proportion correct responses in the last 50 trials. When all accuracies reached 0.9 or higher, learning and exploration were disabled (i.e. <italic>β</italic> and <italic>ε</italic> were set to zero) and we considered learning successful if the model performed all trial-types accurately.</p>
<p>We found that learning of this task with AuGMEnT was efficient. We distinguished three points along the task learning trajectory: learning to obtain the fixation reward (‘Fix’), learning to fixate until fixation-mark offset (‘Go’) and finally to correctly solve the task (‘Task’). To determine the ‘Fix’-learn trial, we determined the time point when the model attained fixation in 90 out of 100 consecutive trials. The model learned to fixate after 224 trials (median) (<xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2C</xref>). The model learned to maintain gaze until the go signal after ∼1,300 trials and it successfully learned the complete task after ∼4,100 trials. Thus, the learning process was at least an order of magnitude faster than in monkeys that typically learn such a task after months of training with more than 1,000 trials per day.</p>
<p>To investigate the effect of the shaping strategy, we also trained 10,000 networks without the extra fixation reward (<italic>r<sub>i</sub></italic> was zero). Networks that received fixation rewards were more likely to learn than networks that did not (99.45% versus 76.41%; <italic>χ</italic><sup>2</sup> = 2,498, <italic>p</italic>&lt;10<sup>-6</sup>). Thus, shaping strategies facilitate training with AuGMEnT, similar to their beneficial effect in animal learning [<xref rid="pcbi.1004060.ref039" ref-type="bibr">39</xref>].</p>
<p>The activity of a fully trained network is illustrated in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref>. One of the association units (grey in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref>) and the <italic>Q</italic>-unit for fixating at the centre of the display (blue in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2B,D</xref>) had strongest activity at fixation onset and throughout the fixation and memory delays. If recorded in a macaque monkey, these neurons would be classified as fixation cells. After the go-signal the <italic>Q</italic>-unit for the appropriate eye movement became more active. The activity of the <italic>Q</italic>-units also depended on cue-location during the memory delay as is observed, for example, in the frontal eye fields (* in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref>) [<xref rid="pcbi.1004060.ref040" ref-type="bibr">40</xref>]. This activity is caused by the input from memory units in the association layer that memorized cue location as a persistent increase in their activity (green and orange in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref>). Memory units were also tuned to the color of the fixation mark which differentiated pro-saccade trials from anti-saccade trials, a conjoined selectivity necessary to solve this non-linear task [<xref rid="pcbi.1004060.ref041" ref-type="bibr">41</xref>]. There was an interesting division of labor between regular and memory units in the association layer. Memory units learned to remember the cue location. In contrast, regular units learned to encode the presence of task-relevant sensory information on the screen. Specifically, the fixation unit in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref> (upper row) was active as long as the fixation point was present and switched off when it disappeared, thus cueing the model to make an eye movement. Interestingly, these two classes of memory neurons and regular (“light sensitive”) neurons are also found in areas of the parietal and frontal cortex of monkeys [<xref rid="pcbi.1004060.ref002" ref-type="bibr">2</xref>,<xref rid="pcbi.1004060.ref040" ref-type="bibr">40</xref>] where they appear to have equivalent roles.</p>
<p><xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2D</xref> provides a first, casual impression of the representations that the network learns. To gain a deeper understanding of the representation in the association layer that supports the non-linear mapping from the sensory units to the <italic>Q</italic>-value units, we performed a principal component analysis (PCA) on the activations of the association units. We constructed a single (32x7) observation matrix from the association layer activations for each time-step (there were seven association units and eight time-points in each of the four trial-types), with the learning rate <italic>β</italic> and exploration rate <italic>ε</italic> of the network set to zero. <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2E</xref> shows the projection of the activation vectors onto the first two principal components for an example network. It can be seen activity in the association layer reflects the important events in the task. The color of the fixation point and the cue location provide information about the correct action and lead to a ‘split’ in the 2D principal component (PC) space. In the ‘Go’ phase, there are only two possible correct actions: ‘left’ for the Pro-Left and Anti-Right trials and ‘right’ otherwise. The 2D PC plot shows that the network splits the space into three parts based on the optimal action: here the ‘left’ action is clustered in the middle, and the two trial types with target action ‘right’ are adjacent to this cluster. This pattern (or its inversion with the ‘right’ action in the middle) was typical for the trained networks. <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2F</xref> shows how the explained variance in the activity of association units increases with the number of PCs, averaged over 100 simulated networks; most variance was captured by the first two PCs.</p>
<p>To investigate the representation that formed during learning across all simulated networks, we next evaluated the similarity of activation patterns (Euclidean distance) across the four trial types for the regular and memory association units and also for the units in the <italic>Q</italic>-value layer (<xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2G</xref>). For every network we entered a ‘1’ in the matrix for trial types with the smallest distance and a ‘0’ for all other pairs of trials and then aggregated results over all networks by averaging the resulting matrices. Initially the patterns of activity in the association layer are similar for all trial types, but they diverge after the presentation of the fixation point and the cue. The regular units convey a strong representation of the color of the fixation point (e.g. activity in pro-saccade trials with a left cue is similar to activity in pro-saccade trials with a right cue; PL and PR in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2G</xref>), which is visible at all times. Memory units have a clear representation of the previous cue location during the delay (e.g. AL trials similar to PL trials and AR to PR trials in <xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2G</xref>). At the go-cue their activity became similar for trials requiring the same action (e.g. AL trials became similar to PR trials), and the same was true for the units in the <italic>Q</italic>-value layer.</p>
<p>In our final experiment with this task, we investigated if working memories are formed specifically for task-relevant features. We used the same stimuli, but we now only required pro-saccades so that the color of the fixation point became irrelevant. We trained 100 networks, of which 96 learned the task and we investigated the similarities of the activation patterns. In these networks, the memory units became tuned to cue-location but not to color of the fixation point (<xref rid="pcbi.1004060.g002" ref-type="fig">Fig. 2H</xref>; note the similar activity patterns for trials with a differently colored fixation point, e.g. AL and PL trials). Thus, AuGMEnt specifically induces selectivity for task-relevant features in the association layer.</p>
</sec>
<sec id="sec013">
<title>Delayed match-to-category task</title>
<p>The selectivity of neurons in the association cortex of monkeys changes if the animals are trained to distinguish between categories of stimuli. After training, neurons in frontal [<xref rid="pcbi.1004060.ref042" ref-type="bibr">42</xref>] and parietal cortex [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>] respond similarly to stimuli from the same category and discriminate between stimuli from different categories. In one study [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>], monkeys had to group motion stimuli in two categories in a delayed-match-to-category task (<xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3A</xref>). They first had to look at a fixation point, then a motion stimulus appeared and after a delay a second motion stimulus was presented. The monkeys’ response depended on whether the two stimuli came from the same category or from different categories. We investigated if AuGMEnT could train a network with an identical architecture (with 3 regular and 4 memory units in the association layer) as the network of the delayed saccade/antisaccade task to perform this categorization task. We used an input layer with a unit for the fixation point and 20 units with circular Gaussian tuning curves of the form <inline-formula id="pcbi.1004060.e081"><alternatives><graphic id="pcbi.1004060.e081g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e081" xlink:type="simple"/><mml:math display="inline" id="M81" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with preferred directions <italic>θ<sub>c</sub></italic> evenly distributed over the unit circle and a standard deviation <italic>σ</italic> of 12 deg (<xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3B</xref>). The two categories were defined by a boundary that separated the twelve motion directions (adjacent motion directions were separated by 30 deg.) into two sets of six directions each.</p>
<fig id="pcbi.1004060.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Match-to-category task.</title>
<p><bold><italic>A</italic></bold>, When the network directed gaze to the fixation point, we presented a motion stimulus (cue-1), and after a delay a second motion stimulus (cue-2). The network had to make a saccade to the left when the two stimuli belonged to the same category (match) and to the right otherwise. There were twelve motion directions, which were divided into two categories (right). <bold><italic>B</italic></bold>, The sensory layer had a unit representing the fixation point and 20 units with circular Gaussian tuning curves (s.d. 12 deg.) with preferred directions evenly distributed over the unit circle. <bold><italic>C</italic></bold>, Activity of two example memory units in a trained network evoked by the twelve cue-1 directions. Each line represents one trial, and color represents cue category. Responses to cues closest to the categorization boundary are drawn with a dashed line of lighter color. F, fixation mark onset; C, cue-1 presentation. D, delay; G, cue-2 presentation (go signal); S, saccade. <bold><italic>D</italic></bold>, Activity of the same two example memory units as in (C) in the ‘go’ phase of the task for all 12x12 combinations of cues. Colors of labels and axes indicate cue category. <bold><italic>E</italic></bold>, Left, Motion tuning of the memory units (in C) at the end of the memory delay. Error bars show s.d. across trials and the dotted vertical line indicates the category boundary. Right, Tuning of a typical LIP neuron (from [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>]), error bars show s.e.m. <bold><italic>F</italic></bold>, Left, Distribution of the direction change that evoked the largest difference in response across memory units from 100 networks. Right, Distribution of direction changes that evoked largest response differences in LIP neurons (from [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>]).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g003" position="float" xlink:type="simple"/>
</fig>
<p>We first waited until the model directed gaze to the fixation point. Two time-steps after fixation we presented one of twelve motion-cues (cue-1) for one time step and gave the fixation reward <italic>r<sub>i</sub></italic> (<xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3A</xref>). We added Gaussian noise to the motion direction (s.d. 5 deg.) to simulate noise in the sensory system. The model had to maintain fixation during the ensuing memory delay that lasted two time steps. We then presented a second motion stimulus (cue-2) and the model had to make an eye-movement (either left or right; the fixation mark did not turn off in this task) that depended on the match between the categories of the cues. We required an eye movement to the left if both stimuli belonged to the same category and to the right otherwise, within eight time-steps after cue-2. We trained 100 models and measured accuracy for the preceding 50 trials with the same cue-1. We determined the duration of the learning phase as the trial where accuracy had reached 80% for all cue-1 types.</p>
<p>In spite of their simple feedforward structure with only seven units in the association layer, AuGMEnT trained the networks to criterion in all simulations within a median of 11,550 trials. <xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3C</xref> illustrates motion tuning of two example memory neurons in a trained network. Both units had become category selective, from cue onset onwards and throughout the delay period. <xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3D</xref> shows the activity of these units at ‘Go’ time (i.e. after presentation of cue-2) for all 144 combinations of the two cues. <xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3E</xref> shows the tuning of the memory units during the delay period. For every memory unit of the simulations (<italic>N</italic> = 400), we determined the direction change eliciting the largest difference in activity (<xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3F</xref>) and found that the units exhibited the largest changes in activity for differences in the motion direction that crossed a category boundary, as do neurons in LIP [<xref rid="pcbi.1004060.ref004" ref-type="bibr">4</xref>] (<xref rid="pcbi.1004060.g003" ref-type="fig">Fig. 3E,F</xref>, right). Thus, AuGMEnT can train networks to perform a delayed match-to-category task and it induces memory tuning for those feature variations that matter.</p>
</sec>
<sec id="sec014">
<title>Probabilistic decision making task</title>
<p>We have shown that AuGMEnT can train a single network to perform a delayed saccade/anti-saccade task or a match-to-category task and to maintain task-relevant information as persisitent activity. Persistent activity in area LIP has also been related to perceptual decision making, because LIP neurons integrate sensory information over time in decision making tasks [<xref rid="pcbi.1004060.ref043" ref-type="bibr">43</xref>]. Can AuGMEnT train the very same network to integrate evidence for a perceptual decision?</p>
<p>We focused on a recent study [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>] in which monkeys saw a red and a green saccade target and then four symbols that were presented successively. The four symbols provided probabilistic evidence about whether a red or green eye-movement target was baited with reward (<xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4A</xref>). Some of the symbols provided strong evidence in favor of the red target (e.g. the triangle in the inset of <xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4A</xref>), others strong evidence for the green target (heptagon) and other symbols provided weaker evidence. The pattern of choices revealed that the monkeys assigned high weights to symbols carrying strong evidence and lower weights to less informative ones. A previous model with only one layer of modifiable synapses could learn a simplified, linear version of this task where the symbols provided direct evidence for one of two actions [<xref rid="pcbi.1004060.ref044" ref-type="bibr">44</xref>]. This model used a pre-wired memory and it did not simulate the full task where symbols only carry evidence about red and green choices while the position of the red and green targets varied across trials. Here we tested if AuGMEnT could train our network with three regular and four memory units to perform the full non-linear task.</p>
<fig id="pcbi.1004060.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Probabilistic classification task.</title>
<p><bold><italic>A</italic></bold>, After the network attained fixation, we presented four shapes in a random order at four locations. The shapes <italic>s</italic><sub>1</sub>,⋯,<italic>s</italic><sub>4</sub> cued a saccade to the red or green target: their location varied randomly across trials. Reward was assigned to the red target with probability <inline-formula id="pcbi.1004060.e082"><alternatives><graphic id="pcbi.1004060.e082g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e082" xlink:type="simple"/><mml:math display="inline" id="M82" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, with <inline-formula id="pcbi.1004060.e083"><alternatives><graphic id="pcbi.1004060.e083g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e083" xlink:type="simple"/><mml:math display="inline" id="M83" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>, and to the green target otherwise. Inset shows weights <italic>w<sub>i</sub></italic> associated with cues <italic>s<sub>i</sub></italic>. <bold><italic>B</italic></bold>, The sensory layer had units for the fixation point, for the colors of the targets on each side of the screen and there was a set of units for the symbols at each of the four retinotopic locations. <bold><italic>C</italic></bold>, Activity of two context sensitive memory units and Q-value units (bottom) in a trial where four shield-shaped symbols were presented to a trained network. The green target is the optimal choice. F: fixation mark onset; D: memory delay; G: fixation mark offset (‘Go’-signal).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g004" position="float" xlink:type="simple"/>
</fig>
<p>We trained the model with a shaping strategy using a sequence of tasks of increasing complexity, just as in the monkey experiment [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>]. We will first decribe the most complex version of the task. In this version, the model (<xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4B</xref>) had to first direct gaze to the fixation point. After fixating for two time-steps, we gave the fixation reward <italic>r<sub>i</sub></italic> and presented the colored targets and also one of the 10 symbols at one of four locations around the fixation mark, In the subsequent three time-steps we presented the additional symbols. We randomized location of the red and green targets, the position of the successively presented symbols as well as the symbol sequence over trials. There was a memory delay of two time steps after all symbols (<italic>s</italic><sub>1</sub>,⋯,<italic>s</italic><sub>4</sub>) had been presented and we then removed the fixation point, as a cue to make a saccade to one of the colored targets. Reward <italic>r<sub>f</sub></italic> was assigned to the red target with probability <inline-formula id="pcbi.1004060.e084"><alternatives><graphic id="pcbi.1004060.e084g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e084" xlink:type="simple"/><mml:math display="inline" id="M84" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, with <inline-formula id="pcbi.1004060.e085"><alternatives><graphic id="pcbi.1004060.e085g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e085" xlink:type="simple"/><mml:math display="inline" id="M85" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> (<italic>w<sub>i</sub></italic> is specified in <xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4A</xref>, inset) and to the green target otherwise. The model’s choice was considered correct if it selected the target with highest reward probability, or either target if reward probabilities were equal. However, <italic>r<sub>f</sub></italic> was only given if the model selected the baited target, irrespective of whether it had the highest reward probability.</p>
<p>The shaping strategy used for training gradually increased the set of input symbols (2,4,⋯,10) and sequence length (1,⋯,4) in eight steps (<xref rid="pcbi.1004060.t003" ref-type="table">Table 3</xref>). Training started with the two `trump' shapes which guarantee reward for the correct decision (triangle and heptagon, see <xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4A</xref>, inset). We judged that the task had been learned when the success rate in the last <italic>n</italic> trials was 85%. As the number of possible input patterns grew we increased <italic>n</italic> to ensure that a significant fraction of possible input-patterns had been presented before we determined convergence (see <xref rid="pcbi.1004060.t003" ref-type="table">Table 3</xref>). Difficulty was first increased by adding the pair of symbols with the next smaller absolute weight, until all shapes had been introduced (level 1–5) and then by increasing sequence length (level 6–8).</p>
<table-wrap id="pcbi.1004060.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.t003</object-id>
<label>Table 3</label> <caption><title>Probabilistic Classification convergence windows.</title></caption>
<alternatives>
<graphic id="pcbi.1004060.t003g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.t003" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Task difficulty</th>
<th align="left" rowspan="1" colspan="1"># Input Symbols</th>
<th align="left" rowspan="1" colspan="1">Sequence Length</th>
<th align="left" rowspan="1" colspan="1"><italic>n</italic> trials to determine success</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>1</bold></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>2</bold></td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1,500</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>3</bold></td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">2,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>4</bold></td>
<td align="left" rowspan="1" colspan="1">8</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">2,500</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>5</bold></td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">3,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>6</bold></td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">10,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>7</bold></td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">10,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>8</bold></td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">20,000</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>With this shaping strategy AuGMEnT successfully trained 99 of 100 networks within a total of 500,000 trials. Training of the model to criterion (85% correct in the final task) took a median total of 55,234 trials across the eight difficulty levels, which is faster than the monkeys learned. After the training procedure, the memory units had learned to integrate information for either the red or green choice over the symbol sequence and maintained information about the value of this choice as persistent activity during the memory delay. <xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4C</xref> shows the activity of two memory units and the <italic>Q</italic>-value units of an example network during a trial where the shield symbol was presented four times, providing strong evidence that the green target was baited with reward. The memory units became sensitive to the context determined by the position of the red and green saccade targets. The unit in the first row of <xref rid="pcbi.1004060.g004" ref-type="fig">Fig. 4C</xref> integrated evidence for the green target if it appeared on the right side and the unit in the second row if the green target appeared on the left. Furthermore, the activity of these memory units ramped up gradually as more evidence accumulated.</p>
<p>The activity of neurons in LIP was correlated to the log likelihood that the targets are baited [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>]. To investigate the influence of log likelihood on the activity of the memory units, we computed log likelihood ratio (logLR) quintiles as follows. We enumerated all 10,000 length 4 symbol combinations <italic>s</italic>∈<italic>S</italic> and computed the probability of reward for a saccade to the red target, <italic>P</italic>(<italic>R</italic>|<italic>S</italic>) for every combination. We next computed the conditional probabilities of reward <italic>P</italic>(<italic>R</italic>|<italic>s<sub>l</sub></italic>) and <italic>P</italic>(<italic>G</italic>|<italic>s<sub>l</sub></italic>) = 1-<italic>P</italic>(<italic>R</italic>|<italic>s<sub>l</sub></italic>) for sequences <italic>s<sub>l</sub></italic> of length <italic>l</italic>∈{1,⋯,4} (marginalizing over the unobserved symbols). We then computed <italic>LogLR</italic>(<italic>s<sub>l</sub></italic>) as log<sub>10</sub>(<italic>P</italic>(<italic>R</italic>|<italic>s<sub>l</sub></italic>)/<italic>P</italic>(<italic>G</italic>|<italic>s<sub>l</sub></italic>)) for each specific sequence of length <italic>l</italic> and divided those into quintiles.</p>
<p>To determine how the activity of memory units depended on the log likelihood that the targets were baited we first compared their average activity after observing a complete sequence of the lower and upper quintile, and reordered the quintiles so they were increasing for each unit. We then computed the average within-quintile activities over the aligned population. The upper panel of <xref rid="pcbi.1004060.g005" ref-type="fig">Fig. 5A</xref> shows how the average activity of the four memory units of an example network depended on the log likelihood that the targets were baited and the lower panel shows LIP data [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>] for comparison. It can be seen that the memory units’ activity became correlated to the log likelihood, just like LIP neurons. Importantly, the synaptic weights from input neurons to memory cells depended on the true weights of the symbols after learning (<xref rid="pcbi.1004060.g005" ref-type="fig">Fig. 5B</xref>). This correlation was also strong at the population level as can be seen in <xref rid="pcbi.1004060.g005" ref-type="fig">Fig. 5C</xref> which shows the distribution of all the correlation coefficients (N = 396). Thus, plasticity of synapses onto the memory neurons can explain how the monkeys valuate the symbols and AuGMEnT explains how these neurons learn to integrate the most relevant information. Furthermore, our results illustrate that AuGMEnT not only trains the association units to integrate stochastic sensory evidence but that it also endows them with the required mixed selectivity for target color and symbol sequence that is required to solve this non-linear task [<xref rid="pcbi.1004060.ref041" ref-type="bibr">41</xref>].</p>
<fig id="pcbi.1004060.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Tuning in the association layer in the probabilistic classification task.</title>
<p><bold><italic>A</italic></bold>, Trials were subdivided in quintiles based on the log-likelihood ratio of the evidence favoring one target. Average activations of the four memory units of a trained model network (top; 100,000 trials) and LIP neurons (bottom, from [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>]) depend on the log-likelihood ratio. <bold><italic>B</italic></bold>, Left, Average synaptic weights between input units representing symbols and an example memory unit are strongly correlated (<italic>ρ</italic>≈1, <italic>p</italic>&lt;10<sup>-6</sup>) with true symbol weights. Right, Subjective weights assigned by a monkey as estimated from the performance data (from [<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>]). <bold><italic>C</italic></bold>, Histogram of Spearman correlations between average synaptic weights for symbols and true symbol weights for 396 memory units (AuGMEnT trained 99 of 100 simulated networks to criterion). Note that there are also units with zero correlation that do not contribute to the mapping of the symbols onto <italic>Q</italic>-values. These units were accompanied by other association units with stronger correlations.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g005" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec015">
<title>Vibrotactile discrimination task</title>
<p>The previous simulations addressed tasks that have been employed for the study of neurons in area LIP of monkeys. Our last simulation investigated a task that has been used to study vibrotactile working memory [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1004060.ref045" ref-type="bibr">45</xref>]. In this task, the monkey touches a key with one hand and then two vibration stimuli are applied sequentially to a fingertip of the other hand (<xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6A</xref>). The monkey has to indicate whether the frequency of the first vibration stimulus (F1) is higher or lower than the frequency of the second one (F2). At the end of the trial the animal indicates its choice by releasing the key and pressing one of two buttons. The overall structure of the task is similar to that of the visual tasks described above, but the feature of interest here is that it requires a comparison between two scalar values; F2 that is sensed on the finger and F1 that has to be maintained in working memory.</p>
<fig id="pcbi.1004060.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Vibrotactile discrimination task.</title>
<p><bold><italic>A</italic></bold>, Top line shows vibrotactile stimuli, bottom colored lines show target actions for the example trial (F1 &lt; F2). H, hold key; L, press left button to indicate F2 &lt; F1; R, press right button to indicate F2 &gt; F1. <bold><italic>B</italic></bold>, Network model. The units in the sensory layer are tuned for the tactile frequency with monotonically increasing or decreasing sigmoidal tuning curves. The binary ‘S’ neuron codes for skin contact of the vibrotactile probe and becomes active at ‘Contact’ in A. <bold><italic>C</italic></bold>, Average psychometric curves for 100 networks trained on the variable F1 task. Each set of data points (grouped by color) shows responses for the F1 stimulus that is indicated with a vertical line for flanking F2 stimuli; blue: F1 = 20Hz, yellow: F1 = 30Hz and pink: F1 = 40Hz. Y-axis shows the mean proportion of trials where networks indicated that F2 &gt; F1 (each comparison was evaluated 100 times for every network). Error bars show s.d. over networks. Curves are logistic fits to the model responses. <bold><italic>D</italic></bold>, Tuning of two example memory units to F1 frequency during the delay phase. <bold><italic>E</italic></bold>, Histogram of linear correlations between F1 frequency and memory unit activations during the delay phase for 100 networks (N = 400). <bold><italic>F</italic></bold>, Example activity traces for two memory units and the three Q-value units. Left panel shows the response for F1 = 20Hz and F2 = F1±5Hz (solid +5Hz, dashed -5Hz). The response of the <italic>Q</italic>-value units is coded following the color scheme in panels A and B. Right panel shows activity of these units when F1 was 30 Hz. F2 indicates onset of second vibration stimulus. D: Memory delay phase. Note that F2 is 25Hz for the continuous lines in the left panel and also for the dashed lines in the right panel, but that these trials require different responses (right button if F1 = 20Hz and left button if F1 = 30Hz). <bold><italic>G</italic></bold>, Scatter plot of linear regression parameters of various unit types when F2 was presented (as explained in the main text). A positive A1 (A2) parameter indicates that a unit becomes more active for higher F1 (F2). Green line shows y = x and the activity of units on this line is related to the sum of F1 and F2. The red line represents y = -x, and the activity of units on this line represents the difference between F1 and F2. The color scheme for the Q-value units is the same as in (A) and (B). <bold><italic>H</italic></bold>, Scatter plot of linear regression parameters at the time of F2 presentation for networks trained on the version of the task with fixed F1. <bold><italic>I</italic></bold>, Psychometric curves for block-trained fixed F1 networks (see main text). Same conventions as for (C). Only the logistic fit (black line) for F1 = 30 Hz is drawn.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g006" position="float" xlink:type="simple"/>
</fig>
<p>Recent computational work has addressed various aspects of the vibrotactile discrimination task. Several models addressed how neural network models can store F1 and compare it to F2 [<xref rid="pcbi.1004060.ref046" ref-type="bibr">46</xref>–<xref rid="pcbi.1004060.ref048" ref-type="bibr">48</xref>]. More recently, Barak et al. [<xref rid="pcbi.1004060.ref049" ref-type="bibr">49</xref>] investigated the dynamics of the memory states in networks trained with three different supervised learning methods and compared them to the neuronal data. However, these previous studies did not yet address trial-and-error learning of the vibrotactile discrimination task with a biologically plausible learning rule. We therefore investigated if AuGMEnT could train the same network that had been used for LIP, with three regular units and four memory units, to solve this task.</p>
<p>The input layer was modeled after sensory area S2 of the monkey. Neurons in this cortical area have broad tuning curves and either monotonically increase or decrease their firing rate as function of the frequency of the vibrotactile stimulus [<xref rid="pcbi.1004060.ref050" ref-type="bibr">50</xref>]. The input units of the model had sigmoidal tuning curves <italic>r</italic>(<italic>x</italic>) = 1/(1+exp(<italic>w</italic>(<italic>θ</italic><sub>c</sub>-<italic>x</italic>))), with 10 center points <italic>θ</italic><sub>c</sub> evenly distributed over the interval between 5.5Hz and 49.5Hz. We used a pair of units at every <italic>θ</italic><sub>c</sub> with one unit increasing its activity with stimulus frequency and the other one decreasing, so that there were a total of 20 input units. Parameter <italic>w</italic> determines the steepness of the tuning curve and was +/- 5. We modeled sensory noise by adding independent zero mean Gaussian noise (s.d. 7.5%) to the firing rates of the input units. We also included a binary input unit that signaled skin contact with the stimulation device (unit S in <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6B</xref>). The association and <italic>Q</italic>-value layers were identical to those of the other simulations (<xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6B</xref>).</p>
<p>Our first simulation addressed a version of the task where F1 varied from trial to trial [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>]. A trial started when the input unit indicating skin contact with the vibrating probe became active and the model had to select the hold-key within ten time-steps, or else the trial was terminated. When the model had held the key for two time-steps, a vibration stimulus (F1, uniformly random between 5 and 50 Hz) was presented to the network for one time-step and the small shaping reward (<italic>r<sub>i</sub></italic>) was given. This was followed by a memory delay after which we presented the second vibration stimulus (F2), drawn from a uniform distribution between 5 and 50 Hz, but with a minimal separation of 2 Hz from F1. If F2 was lower than F1 the model had to select the left button (green <italic>Q</italic>-value unit in <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6B</xref>)—and the right button (red) otherwise—within eight time steps after the presentation of F2 to obtain the reward <italic>r<sub>f</sub></italic>.</p>
<p>To determine model performance, we divided the range of F1 stimuli into 9 bins of 5 Hz and kept track of the running average of performance in 50 trials for each bin. When the model reached a performance of 80% for every F1 we disabled learning and exploration (setting learning parameters <italic>β</italic> and <italic>ε</italic> to zero) and checked the performance of the model for F1 stimuli of 20, 30 and 40 Hz and F2 stimuli with offsets of [-10, -8, …, -2,2, …, 8, 10] Hz, repeating each test 20 times. We considered learning to be successful if the model classified the nearest F2 frequencies (2 Hz distance) with a minimal accuracy of 50% and all other F2 frequencies with an accuracy better than 75%, for every F1 bin.</p>
<p>AuGMEnT trained all 100 simulated networks to criterion within a median of 3,036 trials. <xref rid="pcbi.1004060.g007" ref-type="fig">Fig. 7C</xref> illustrates the average (±s.d.) choices of these 100 trained models as a function of F2, for three values of F1 as well as a logistic function fitted to the data [as in <xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>]. It can be seen that the model correctly indicates whether F1 is higher or lower than F2 and that the criterion depends on the value F1, implying that the model has learned to store this analog scalar value in its working memory. What are the memory representations that emerged during learning? <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6D</xref> shows the F1 tuning of two memory units in an example network; typically the tunings are broad and can be increasing or decreasing as a function of F1, similar to what was found in experiments in the frontal cortex of monkeys [<xref rid="pcbi.1004060.ref051" ref-type="bibr">51</xref>]. <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6E</xref> shows the distribution of linear correlations between 400 memory units in 100 trained networks and F1 frequency; most units exhibit a strong positive or negative correlation, indicating that the networks learned to code the memory of F1 as the level of persistent firing of the memory units.</p>
<fig id="pcbi.1004060.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Robustness to variations in the parameters that control learning rate.</title>
<p>The upper row shows how the proportion of networks that converged varies as function of <italic>β</italic> (learning rate) and <italic>λ</italic> (decay of tags); white regions had a proportion of convergence lower than 0.8. The lower row shows the effect of <italic>β</italic> and <italic>λ</italic> on the median trial when the learning criterion was reached; white regions reached convergence later than the yellow regions (see insets).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g007" position="float" xlink:type="simple"/>
</fig>
<p>We next investigated how the model carried out the comparison process that has to take place after the presentation of F2. This comparison process depends critically on the order of presentation of the two stimuli, yet it involves information that comes in via the same sensory inputs and association units [<xref rid="pcbi.1004060.ref048" ref-type="bibr">48</xref>]. We found that the memory units were indeed sensitive to both F1 and F2 in the comparison period. <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6F</xref> shows the response of two example memory units and the three <italic>Q</italic>-value units for a trials with an F1 of 20 or 30 Hz, followed by an F2 with a frequency that was either 5Hz higher (solid line) or lower than F1 (dashed line). The activity of the memory units encodes F1 during the memory delay, but these units also respond to F2 so that the activity after the presentation of F2 depends on both frequencies. The lower panel illustrates the activity of the <italic>Q</italic>-value units. The activity of the Hold <italic>Q</italic>-value unit (H, blue) is highest until the presentation of F2, causing the model to hold the key until the go-signal. This unit did not distinguish between trials that required a right or left button press. The activities of <italic>Q</italic>-value units for the left and right button press (red and green traces) explain how the network made correct decisions at the go-signal because the <italic>Q</italic>-value of the appropriate action became highest (the solid lines in <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6F</xref> show activity if F2&gt;F1 and dashed lines F2&lt;F1). It can be seen, for example, how the response elicited in the <italic>Q</italic>-value layer by an F2 of 25Hz depended on whether the preceding F1 was 20Hz (continuous curves in the left panel of <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6F</xref>) or 30Hz (dashed curves in the right panel).</p>
<p>We next quantified how the activity of the memory, regular and <italic>Q</italic>-value units from 100 networks (<italic>N</italic> = 400, 300 and 300 units, respectively) depended on F1 and F2 during the comparison phase with a regression [see <xref rid="pcbi.1004060.ref052" ref-type="bibr">52</xref>] using all trials where the F2 stimulus was presented and for all combinations of the two frequencies between 5 and 50 Hz (step size 1Hz),
<disp-formula id="pcbi.1004060.e086">
<alternatives>
<graphic id="pcbi.1004060.e086g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e086" xlink:type="simple"/>
<mml:math display="block" id="M86" overflow="scroll">
<mml:mrow><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow>
</mml:math>
</alternatives>
<label>(32)</label>
</disp-formula>
Here <italic>a<sub>1</sub></italic> and <italic>a<sub>2</sub></italic> estimate the dependence of the unit’s activity on F1 and F2, respectively. The activity of many memory units depended on F1 and also on F2 (<xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6G</xref>, left) and the overall negative correlation between the coefficients (<italic>r</italic> = -0.81, <italic>p</italic>&lt;10<sup>-6</sup>) indicates that units that tended to respond more strongly for increasing F1 tended to decrease their response for increasing F2 and vice versa, just as is observed in area S2, the prefrontal cortex and the medial premotor cortex of monkeys [<xref rid="pcbi.1004060.ref045" ref-type="bibr">45</xref>,<xref rid="pcbi.1004060.ref051" ref-type="bibr">51</xref>,<xref rid="pcbi.1004060.ref052" ref-type="bibr">52</xref>]. In other words, many memory units became tuned to the difference between F1 and F2 in the comparison phase, as is required by this task. In spite of the fact that F1 and F2 activate memory units with the same synapses, the inverse tuning is possible because the F1 stimulus has turned off and activated the off-cells in the sensory layer in the comparison phase. In contrast, the F2 stimulus is still ‘on’ in this phase of the task so that the off-units coding F2 did not yet provide their input to the memory cells. As a result, the memory units’ final activity can reflect the difference between F1 and F2, as is required by the task. Regular units only have access to the current stimulus, and were therefore they are only tuned to F2 in the comparison phase (<xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6G</xref>, middle). <italic>Q</italic>-value units reflect the outcome of the comparison process (<xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6G</xref>, right): their regression coefficients with F1 and F2 fall into three clusters as predicted by the required action.</p>
<p>The version of the task described above demanded the comparison between two flutter frequencies because F1 varied from trial to trial. Hernández et al. [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>] also studied a version of the task where F1 was fixed for a block of trials. In this version, the monkeys based their response on F2 only and did not memorize F1. As a result their performance deteriorated at the start of a new block of trials with a different F1. Networks trained with AuGMEnT also only memorize task-relevant information. Do networks trained with AuGMEnT also fail to memorize F1 if it is fixed during training? To investigate this question, we trained models with a fixed F1 of 30 Hz [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>] and presented F2 stimuli in the range between 5–50 Hz (2.5 Hz spacing) with a minimal distance from F1 of 10 Hz. We estimated convergence as the trial when accuracy reached 90% (running average of 50 trials).</p>
<p>AuGMEnT trained all 100 networks to criterion in this simpler task within a median of 1,390 trials. After learning the fixed F1 task, we subjected the networks to block training with F1 stimuli of 20, 30 and 40 Hz as in [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>] while we presented F2 stimuli with frequencies of ([-10,-8, …,-2,2,…, 8,10] Hz relative to F1 (10 total, each shown 150 times). These blocks of trials had a pseudorandom ordering but we always presented a 30Hz F1 in the last block. When we tested immediately after every block, we found that the models were well able to adapt to a specific F1. However, the models were not able to solve the variable F1 task after this extensive block training, even though they had significant exposure to different F1 stimuli. <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6I</xref> shows the average psychometric curves for 100 networks after the last block with F1 = 30Hz. Colors represent trials with different F1 stimuli (as in <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6C</xref>). It can be seen that the models disregarded F1 and only determined whether F2 was higher or lower than 30 Hz, just as monkeys that are trained with a blocked procedure [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>]. Thus, the model can explain why the monkeys do not learn to compare the two stimuli if the F1 is fixed for longer blocks of trials. The memory units and the <italic>Q</italic>-value units now had similar rather than opposite tuning for F1 and F2 (positive correlations in the left and right panel of <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6H</xref>; compare to <xref rid="pcbi.1004060.g006" ref-type="fig">Fig. 6G</xref>), which indicates that blocked training causes a failure to learn to subtract the memory trace of F1 from the representation of F2.</p>
<p>We conclude that AuGMEnT is able to train networks on a task that requires a comparison between two analog stimuli and where the correct decision depends on stimulus order. Memory units learn to represent the analog value that needs to be memorized as a graded level of persistent activity. However, if F1 is fixed for blocks of trials, the network does not memorize F1 but learns to base its decision on F2 only, in accordance with experimental findings.</p>
</sec>
<sec id="sec016">
<title>Varying the learning parameters and the size of the network</title>
<p>It is remarkable that AuGMEnT can train the same simple network to perform a wide range of tasks, simply by delivering rewards at the appropriate times. In the simulations described above we fixed the number of units in the association layer and <italic>Q</italic>-value layer and used a single set of learning parameters. To examine the stability of the learning scheme, we also evaluated learning speed and convergence rate for various values of the learning rate <italic>β</italic> and the SARSA learning parameter <italic>λ</italic> (which determines the tag-decay parameter <italic>α</italic> because <italic>α</italic> = 1-<italic>λγ</italic> as was explained above, <italic>γ</italic> was kept at the default value). For the saccade/antisaccade, match-to-category and vibrotactile discrimination tasks we tested <italic>β</italic>∈{0.05,0.10,⋯,1.0} and λ∈{0.0,0.1,⋯,0.9} while the other parameters remained the same (Table <xref rid="pcbi.1004060.t001" ref-type="table">1</xref>,<xref rid="pcbi.1004060.t002" ref-type="table">2</xref>) and ran 100 simulations for every combination. <xref rid="pcbi.1004060.g007" ref-type="fig">Fig. 7</xref> shows the proportion of networks that converged and the median convergence trial. Training in the probabilistic classification task required a number of different training stages and a longer overall training time and we evaluated this task with a smaller set of parameters (<xref rid="pcbi.1004060.g007" ref-type="fig">Fig. 7</xref>, right). There was a wide range for the learning parameters where most of the networks converged and these ranges overlapped for the four tasks, implying that the AuGMEnT learning scheme is relatively robust and stable.</p>
<p>So far our simulations used a fixed network with only 7 units in the association layers. Can AuGMEnT also train networks with a larger association layer? To further investigate the generality of the learning scheme, we ran a series of simulations with increasing numbers of association units, multiplying the number of association units in the network described above by 2, 4, …, 128 and training 100 networks of each size in the saccade/antisaccade task. We first evaluated these larger networks without changing the learning parameters and found that the learning was largely unaffected within a limited range of network sizes, whereas performance deteriorated for networks that were 32–128 fold larger (<xref rid="pcbi.1004060.g008" ref-type="fig">Fig. 8A</xref>). The decrease in performance is likely caused by the larger number of synapses, causing larger adjustments of the <italic>Q</italic>-values after each time step than in the smaller networks. It is possible to compensate for this effect by choosing a smaller <italic>β</italic> (learning rate) and <italic>λ</italic>. We jointly scaled these parameters by <inline-formula id="pcbi.1004060.e087"><alternatives><graphic id="pcbi.1004060.e087g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e087" xlink:type="simple"/><mml:math display="inline" id="M87" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004060.e088"><alternatives><graphic id="pcbi.1004060.e088g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.e088" xlink:type="simple"/><mml:math display="inline" id="M88" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and selected the parameter combination which resulted in the highest convergence rate and the fastest median convergence speed for every network size (<xref rid="pcbi.1004060.g008" ref-type="fig">Fig. 8B</xref>). The performance of the larger networks was at least as good as that of the network with 7 units if learning parameters were scaled. Thus, AuGMEnT can also successfully train networks with a much larger association layer.</p>
<fig id="pcbi.1004060.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004060.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Varying the size of the association layer.</title>
<p><bold><italic>A</italic></bold>, Scaling with unchanged learning parameters <italic>β</italic> and <italic>λ</italic>. Left, convergence rate (proportion of 100 networks that learned the saccade/antisaccade task). Error bars denote 95% confidence intervals. Right, median convergence speed (number of trials to criterion). <bold><italic>B</italic></bold>, Left, convergence rates with adjusted learning parameters. Bar shading indicates parameter setting (see legend in right panel). Right, median convergence speed with optimized parameters.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004060.g008" position="float" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>AuGMEnT provides a new theoretical framework that can explain how neurons become tuned to relevant sensory stimuli in sequential decision tasks during trial-and-error learning. The scheme uses units inspired by transient and sustained neurons in sensory cortices [<xref rid="pcbi.1004060.ref019" ref-type="bibr">19</xref>], action-value coding neurons in frontal cortex, basal ganglia and midbrain [<xref rid="pcbi.1004060.ref012" ref-type="bibr">12</xref>,<xref rid="pcbi.1004060.ref035" ref-type="bibr">35</xref>,<xref rid="pcbi.1004060.ref036" ref-type="bibr">36</xref>] and neurons with mnemonic activity that integrate input in association cortex. To the best of our knowledge, AuGMEnT is the first biologically plausible learning scheme that implements SARSA in a multi-layer neural network equipped with working memory. The model is simple, yet is able to learn a wide range of difficult tasks requiring non-linear sensory-motor transformations, decision making, categorization, and working memory. AuGMEnT can train the very same network to perform either of these tasks by presenting the appropriate sensory inputs and reward contingency, and the representations it learns are similar to those found in animals trained on these tasks. AuGMEnT is a so-called on-policy method because it only relies on the <italic>Q</italic>-values that the network experiences during learning. These on-policy methods appear to be more stable than off-policy algorithms (such as <italic>Q</italic>-learning which considers transitions not experienced by the network), if combined with neural networks (see e.g. [<xref rid="pcbi.1004060.ref053" ref-type="bibr">53</xref>,<xref rid="pcbi.1004060.ref054" ref-type="bibr">54</xref>]).</p>
<p>AuGMEnT forms memory representations for features that need to be remembered. In the delayed saccade/anti-saccade task, training induced persistent neuronal activity tuned to the cue location and to the color of the fixation point, but only if it was relevant. In the categorization task, units became sensitive to category boundaries and in the decision making task, units integrated sensory evidence with stronger weights for the more reliable inputs. These properties resemble those of neurons in LIP [<xref rid="pcbi.1004060.ref002" ref-type="bibr">2</xref>–<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>] and the frontal cortex [<xref rid="pcbi.1004060.ref024" ref-type="bibr">24</xref>] of monkeys. Finally, the memory units learned to memorize and compare analog values in the vibrotactile task, just as has been observed in the frontal cortex of monkeys [<xref rid="pcbi.1004060.ref006" ref-type="bibr">6</xref>,<xref rid="pcbi.1004060.ref045" ref-type="bibr">45</xref>].</p>
<p>AuGMEnT makes a number of predictions that could be tested in future neuroscientific experiments. The first and foremost prediction is that feedback connections gate plasticity of the connections by inducing synaptic tags. Specifically, the learning scheme predicts that feedback connections are important for the induction of tags on feedforward connections from sensory cortices to the association cortex (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref>). A second prediction is the existence of traces in synapses onto neurons with persistent activity (i.e. memory units) that are transformed into tags upon the arrival of feedback from the response selection stage, which may occur at a later point in time. The third prediction is that these tags interact with globally released neuromodulators (e.g. dopamine, acetylcholine or serotonin), which determine the strength and sign of the synaptic changes (potentiation or depression). Neurobiological evidence for the existence of these tags and their interaction with neuromodulatory substances will be discussed below. A final prediction is that stationary stimuli provide transient input to neurons with persistent activity. As a result, stimuli that are visible for a longer time do not necessarily cause a ramping of activity. In our network ramping was prevented because memory units received input from “on” and “off” input units only. We note, however, that other mechanisms such as, for example, rapidly adapting synapses onto memory cells, could achieve the same effect. In contrast, neurons in association cortex without persistent activity are predicted to receive continuous input, for as long as a stimulus is present. These specific predictions could all be tested in future neuroscientific work.</p>
<sec id="sec018">
<title>Role of attentional feedback and neuromodulators in learning</title>
<p>AuGMEnT implements a four-factor learning rule. The first two factors are pre- and post-synaptic activity of the units and there are two additional “gating factors” that enable synaptic plasticity. The first gating factor is the feedback from units in the motor layer that code the selected action. These units send an attentional signal back to earlier processing levels to tag synapses responsible for selecting this action. The importance of selective attention for learning is supported by experiments in cognitive psychology. If observers select a stimulus for an action, attention invariably shifts to this stimulus [<xref rid="pcbi.1004060.ref055" ref-type="bibr">55</xref>] and this selective attention signal gates perceptual learning so that attended objects have larger impact on future behavior [<xref rid="pcbi.1004060.ref056" ref-type="bibr">56</xref>–<xref rid="pcbi.1004060.ref058" ref-type="bibr">58</xref>]. Moreover, neurophysiological studies demonstrated that such a feedback signal exists, because neurons in the motor cortex that code an action enhance the activity of upstream neurons providing input for this action [<xref rid="pcbi.1004060.ref059" ref-type="bibr">59</xref>,<xref rid="pcbi.1004060.ref060" ref-type="bibr">60</xref>].</p>
<p>The second gating-factor that enables plasticity is a global neuromodulatory signal that broadcasts the RPE to many brain regions and determines the sign and strength of the changes in synapses that have been tagged. Dopamine is often implicated because it is released if reward expectancy increases and it influences synaptic plasticity [<xref rid="pcbi.1004060.ref010" ref-type="bibr">10</xref>,<xref rid="pcbi.1004060.ref038" ref-type="bibr">38</xref>]. There is also a potential role for acetylcholine because cholinergic cells project diffusely to cortex, respond to rewards [<xref rid="pcbi.1004060.ref061" ref-type="bibr">61</xref>–<xref rid="pcbi.1004060.ref063" ref-type="bibr">63</xref>] and influence synaptic plasticity [<xref rid="pcbi.1004060.ref061" ref-type="bibr">61</xref>,<xref rid="pcbi.1004060.ref064" ref-type="bibr">64</xref>]. Furthermore, a recent study demonstrated that serotonergic neurons also carry a reward-predicting signal and that the optogenetic activation of serotonergic neurons acts as a positive reinforcer [<xref rid="pcbi.1004060.ref065" ref-type="bibr">65</xref>]. Guidance of synaptic plasticity by the combination of neuromodulatory signals and cortico-cortical feedback connections is biologically plausible because all information for the synaptic update is available at the synapse.</p>
</sec>
<sec id="sec019">
<title>Synaptic tags and synaptic traces</title>
<p>Learning in AuGMEnT depends on synaptic tags and traces. The first step in the plasticity of a synapse onto a memory cell is the formation of a synaptic trace that persists until the end of the trial (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1C</xref>). The second step is the conversion of the trace into a tag, when a selected motor unit feeds back to the memory cell. The final step is the release of the neuromodulator that modifies tagged synapses. The learning rule for the synapses onto the regular (i.e. non-memory) association units is similar (<xref rid="pcbi.1004060.g001" ref-type="fig">Fig. 1B</xref>), but tags form directly onto active synapses, skipping the first step. We note, however, that the same learning rule is obtained if these synapses also have traces that decay within one time-step. The hypothesis that synaptic plasticity requires a sequence of events [<xref rid="pcbi.1004060.ref066" ref-type="bibr">66</xref>,<xref rid="pcbi.1004060.ref067" ref-type="bibr">67</xref>] is supported by the synapses’ complex biochemical machinery. There is evidence for synaptic tags [<xref rid="pcbi.1004060.ref015" ref-type="bibr">15</xref>,<xref rid="pcbi.1004060.ref031" ref-type="bibr">31</xref>,<xref rid="pcbi.1004060.ref032" ref-type="bibr">32</xref>] and recent studies have started to elucidate their identity [<xref rid="pcbi.1004060.ref032" ref-type="bibr">32</xref>]. Neuromodulatory signals influence synaptic plasticity even if released seconds or minutes later than the plasticity-inducing event [<xref rid="pcbi.1004060.ref015" ref-type="bibr">15</xref>,<xref rid="pcbi.1004060.ref017" ref-type="bibr">17</xref>,<xref rid="pcbi.1004060.ref032" ref-type="bibr">32</xref>], which supports the hypothesis that they interact with some form of tag.</p>
</sec>
<sec id="sec020">
<title>Comparison to previous modeling approaches</title>
<p>There has been substantial progress in biologically inspired reinforcement learning models with spiking neurons [<xref rid="pcbi.1004060.ref068" ref-type="bibr">68</xref>–<xref rid="pcbi.1004060.ref071" ref-type="bibr">71</xref>] and with models that approximate population activity with continuous variables [<xref rid="pcbi.1004060.ref014" ref-type="bibr">14</xref>,<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004060.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004060.ref044" ref-type="bibr">44</xref>,<xref rid="pcbi.1004060.ref067" ref-type="bibr">67</xref>,<xref rid="pcbi.1004060.ref072" ref-type="bibr">72</xref>–<xref rid="pcbi.1004060.ref074" ref-type="bibr">74</xref>]. Many of the models rely either on Actor-Critic learning [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>] or on policy gradient learning [<xref rid="pcbi.1004060.ref075" ref-type="bibr">75</xref>]. An advantage of Actor-Critic models is that model components relate to brain regions [<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004060.ref071" ref-type="bibr">71</xref>,<xref rid="pcbi.1004060.ref073" ref-type="bibr">73</xref>]. AuGMEnT has features in common with these models. For example, it uses the change in <italic>Q</italic>-value to compute the RPE (<xref rid="pcbi.1004060.e038" ref-type="disp-formula">Eqn. (17)</xref>). Another widely used class of models is formed by policy gradient learning methods [<xref rid="pcbi.1004060.ref068" ref-type="bibr">68</xref>,<xref rid="pcbi.1004060.ref075" ref-type="bibr">75</xref>] where units (or synapses [<xref rid="pcbi.1004060.ref068" ref-type="bibr">68</xref>]) act as local agents that try to increase the global reward. An advantage of these models is that learning does not require knowledge about the influence of units on other units in the network, but a disadvantage is that the learning process does not scale well to larger networks where the correlation between local activity and the global reward is weak [<xref rid="pcbi.1004060.ref070" ref-type="bibr">70</xref>]. AuGMEnT uses ‘attentional’ feedback from the selected action to improve leaning [<xref rid="pcbi.1004060.ref014" ref-type="bibr">14</xref>] and it also generalizes to multi-layer networks. It thereby alleviates a limitation of many previous biologically plausible RL models, which can only train a single layer of modifiable synaptic weights and solve linear tasks [<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>,<xref rid="pcbi.1004060.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004060.ref044" ref-type="bibr">44</xref>,<xref rid="pcbi.1004060.ref067" ref-type="bibr">67</xref>,<xref rid="pcbi.1004060.ref070" ref-type="bibr">70</xref>,<xref rid="pcbi.1004060.ref071" ref-type="bibr">71</xref>,<xref rid="pcbi.1004060.ref073" ref-type="bibr">73</xref>,<xref rid="pcbi.1004060.ref076" ref-type="bibr">76</xref>] and binary decisions [<xref rid="pcbi.1004060.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004060.ref044" ref-type="bibr">44</xref>,<xref rid="pcbi.1004060.ref067" ref-type="bibr">67</xref>,<xref rid="pcbi.1004060.ref070" ref-type="bibr">70</xref>].</p>
<p>Unlike these previous models, AuGMEnT is a model of action-value learning (SARSA(<italic>λ</italic>) [<xref rid="pcbi.1004060.ref007" ref-type="bibr">7</xref>]). It differs from many previous models in its ability to train task-relevant working memory representations, without pre-wiring. We modeled memory units as integrators, because neurons that act as integrators and maintain their activity during memory delays have been found in many cortical regions [<xref rid="pcbi.1004060.ref002" ref-type="bibr">2</xref>–<xref rid="pcbi.1004060.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1004060.ref023" ref-type="bibr">23</xref>,<xref rid="pcbi.1004060.ref024" ref-type="bibr">24</xref>]. To keep the model simple, we did not specify the mechanisms causing persistent activity, which could derive from intracellular processes, local circuit reverberations or recurrent activity in larger networks spanning cortex, thalamus and basal ganglia [<xref rid="pcbi.1004060.ref020" ref-type="bibr">20</xref>–<xref rid="pcbi.1004060.ref022" ref-type="bibr">22</xref>].</p>
<p>A few studies included a pre-wired working memory in RL [<xref rid="pcbi.1004060.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1004060.ref044" ref-type="bibr">44</xref>] but there has been comparatively little work on biologically plausible learning of new memories. Earlier neural networks models used “backpropagation-through-time”, but its mechanisms are biologically implausible [<xref rid="pcbi.1004060.ref077" ref-type="bibr">77</xref>]. The long short-term memory model (LSTM) [<xref rid="pcbi.1004060.ref078" ref-type="bibr">78</xref>] is a more recent and popular approach. Working memories in LSTM rely on the persistent activity of memory units, which resemble the ones used by AuGMEnT. However, LSTM relies on the biologically implausible error-backpropagation rule. To our knowledge, only one previous model addressed the creation of working memories with a neurobiologically inspired learning scheme, the prefrontal basal-ganglia working memory model (PBWM) [<xref rid="pcbi.1004060.ref072" ref-type="bibr">72</xref>], which is part of the Leabra cognitive architecture [<xref rid="pcbi.1004060.ref079" ref-type="bibr">79</xref>,<xref rid="pcbi.1004060.ref080" ref-type="bibr">80</xref>]. Although a detailed comparison of AuGMEnT and Leabra is beyond the scope of this article, it is useful to mention a few key differences. First, the complexity and level of detail of the Leabra/PBWM framework is greater than that of AuGMEnT. The PBWM framework uses more than ten modules, each with its own dynamics and learning rules, making formal analysis difficult. We chose to keep the models trained with AuGMEnT as simple as possible, so that learning is easier to understand. AuGMEnT’s simplicity comes at a cost because many functions remained abstract (see next section). Second, the PBWM model uses a teacher that informs the model about the correct decision, i.e. it uses more information than just reward feedback. Third, PBWM is an actor-critic architecture that learns the value of states, whereas AuGMEnT learns the value of actions. Fourth and finally, there are important differences in the mechanisms for working memory. In PBMW, memory units are bi-stable and the model is equipped with a system to gate information in prefrontal cortex via the basal ganglia. In AuGMEnT, memory units are directly activated by on- and off-units in the input layer and they have continuous activity levels. The activity profile of memory units is task-dependent in AuGMEnT. It can train memory units to integrate evidence for probabilistic decision making, to memorize analog values as graded levels of persistent activity but also to store categories with almost binary responses in a delayed match-to-category task.</p>
</sec>
<sec id="sec021">
<title>Biological plausibility, biological detail and future work</title>
<p>We suggested that AuGMEnT is biologically plausible, but what do we mean with this statement? Our aim was to propose a learning rule based on Hebbian plasticity that is gated by two factors known to gate plasticity: a neuromodulatory signal that is released globally and codes the reward-prediction error and an attentional feedback signal that highlights the part of the network that is accountable for the outcome of an action. We showed that the combination of these two factors, which are indeed available at the level of the individual synapses, can cause changes in synaptic strength that follow gradient descent on the reward-prediction error for the transitions that the network experiences. At the same time, the present model provides only a limited degree of detail. The advantage of such a more abstract model is that it remains mathematically tractable. The downside is that more work will be needed to map the proposed mechanisms onto specific brain structures. We pointed out the correspondence between the tuning that developed in the association layer and tuning in the association cortex of monkeys. We now list a number of simplifying assumptions that we made and that will need to be alleviated by future models that incorporate more biological detail.</p>
<p>First, we assumed that the brain can compute the SARSA temporal difference error, which implies a comparison between the <italic>Q</italic>-value of one state-action combination to the <italic>Q</italic>-value of the next combination. Future modeling studies could include brain structures for storing the <italic>Q</italic>-value of the previously selected action while new action-values are computed. Although we do not know the set of brain structures that store action values, previous studies implicated the medial and lateral prefrontal cortex in storing the outcome that is associated with an action [<xref rid="pcbi.1004060.ref081" ref-type="bibr">81</xref>,<xref rid="pcbi.1004060.ref082" ref-type="bibr">82</xref>]. Prefrontal neurons even update the predicted outcome as new information comes in during the trial [<xref rid="pcbi.1004060.ref083" ref-type="bibr">83</xref>]. An alternative to storing <italic>Q</italic>-values is provided by Actor-Critic architectures that assign values to the various states instead of state-action combinations. They use one network to estimate state-values and another network to select actions [<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>]. Interestingly, [<xref rid="pcbi.1004060.ref016" ref-type="bibr">16</xref>] proposed that the basal ganglia could compute temporal difference errors by comparing activity in the indirect pathway, which might store the predicted value of the previous time-step, and the direct pathway, which could code the predicted value of the next state. We hypothesize that a similar circuit could be used to compute SARSA temporal difference errors. In addition, we also did not model the action-selection process itself, which has been suggested to take place in the basal ganglia (see [<xref rid="pcbi.1004060.ref030" ref-type="bibr">30</xref>]).</p>
<p>A second simplification is that we did not constrain model units to be either inhibitory or excitatory—outgoing weights could have either sign and they could even change sign during learning. Future studies could specify more detailed network architectures with constrained weights ([e.g. as in <xref rid="pcbi.1004060.ref072" ref-type="bibr">72</xref>]). Indeed, it is possible to change networks into functionally equivalent ones with excitatory and inhibitory units that have only positive weights [<xref rid="pcbi.1004060.ref084" ref-type="bibr">84</xref>], but the necessary generalization of AuGMEnT-like learning rules would require additional work.</p>
<p>The third major simplification is that feedback connections in AuGMEnT influence the formation of synaptic tags, but do not influence the activity of units at earlier processing levels. Future studies could include feedback connections that also influence activity of units in the lower layers and develop learning rules for the plasticity of activity propagating feedback connections. These connections might further expand the set of tasks that neural networks can master if trained by trial-and-error. In this context it is of interest that previous studies demonstrated that feedforward propagation of activity to higher cortical areas mainly utilizes the AMPA receptor, whereas feedback effects rely more on the NMDA receptor [<xref rid="pcbi.1004060.ref085" ref-type="bibr">85</xref>], which plays an important role in synaptic plasticity. NMDA receptors also modify neuronal activity in lower areas, and another candidate receptor that could have a specific role in the influence of feedback connections on plasticity are metabotropic glutamate receptors, which are prominent in feedback pathways [<xref rid="pcbi.1004060.ref086" ref-type="bibr">86</xref>,<xref rid="pcbi.1004060.ref087" ref-type="bibr">87</xref>] and known to influence synaptic plasticity [<xref rid="pcbi.1004060.ref088" ref-type="bibr">88</xref>].</p>
<p>A fourth simplification is that we modeled time in discrete steps and used units with scalar activity levels and differentiable activation functions. Therefore, implementations of AuGMEnT using populations of spiking neurons in continuous time deserve to be studied. We leave the integration of the necessary biological detail in AuGMEnT-like networks that would alleviate all these simplifications for future work.</p>
</sec>
<sec id="sec022">
<title>Conclusions</title>
<p>Here we have shown that interactions between synaptic tags and neuromodulatory signals can explain how neurons in ‘multiple-demand’ association areas acquire mnemonic signals for apparently disparate tasks that require working memory, categorization or decision making. The finding that a single network can be trained by trial and error to perform these diverse tasks implies that these learning problems now fit into a unified reinforcement learning framework.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank John Assad and Ariel Zylberberg for helpful comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004060.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duncan</surname> <given-names>J</given-names></name> (<year>2010</year>) <article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>172</fpage>–<lpage>179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2010.01.004" xlink:type="simple">10.1016/j.tics.2010.01.004</ext-link></comment> <object-id pub-id-type="pmid">20171926</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gnadt</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Andersen</surname> <given-names>RA</given-names></name> (<year>1988</year>) <article-title>Memory related motor planning activity in posterior parietal cortex of macaque</article-title>. <source>Exp Brain Res</source> <volume>70</volume>: <fpage>216</fpage>–<lpage>220</lpage>. <object-id pub-id-type="pmid">3402565</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gottlieb</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>ME</given-names></name> (<year>1999</year>) <article-title>Activity of neurons in the lateral intraparietal area of the monkey during an antisaccade task</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>906</fpage>–<lpage>912</lpage>. <object-id pub-id-type="pmid">10491612</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freedman</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>JA</given-names></name> (<year>2006</year>) <article-title>Experience-dependent representation of visual categories in parietal cortex</article-title>. <source>Nature</source> <volume>443</volume>: <fpage>85</fpage>–<lpage>88</lpage>. <object-id pub-id-type="pmid">16936716</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name> (<year>2007</year>) <article-title>Probabilistic reasoning by neurons</article-title>. <source>Nature</source> <volume>447</volume>: <fpage>1075</fpage>–<lpage>1080</lpage>. <object-id pub-id-type="pmid">17546027</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>García</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name> (<year>1997</year>) <article-title>Discrimination in the sense of flutter: new psychophysical measurements in monkeys</article-title>. <source>J Neurosci</source> <volume>17</volume>: <fpage>6391</fpage>–<lpage>6400</lpage>. <object-id pub-id-type="pmid">9236247</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name> (<year>1998</year>) <chapter-title>Reinforcement Learning: an introduction</chapter-title>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name> (<year>1986</year>) <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source> <volume>323</volume>: <fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>2007</year>) <article-title>Multiple Dopamine Functions at Different Time Courses</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>259</fpage>–<lpage>288</lpage>. <object-id pub-id-type="pmid">17600522</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Hyman</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name> (<year>2004</year>) <article-title>Computational roles for dopamine in behavioural control</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>760</fpage>–<lpage>767</lpage>. <object-id pub-id-type="pmid">15483596</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Balleine</surname> <given-names>BW</given-names></name> (<year>2002</year>) <article-title>Reward, Motivation, and Reinforcement Learning</article-title>. <source>Neuron</source> <volume>38</volume>: <fpage>285</fpage>–<lpage>298</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morris</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Nevet</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Arkadir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Vaadia</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname> <given-names>H</given-names></name> (<year>2006</year>) <article-title>Midbrain dopamine neurons encode decisions for future action</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>1057</fpage>–<lpage>1063</lpage>. <object-id pub-id-type="pmid">16862149</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todd</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>JD</given-names></name> (<year>2009</year>) <article-title>Learning to use working memory in partially observable environments through dopaminergic reinforcement</article-title>. <source>NIPS</source> <volume>21</volume>: <fpage>1689</fpage>–<lpage>1696</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name> (<year>2005</year>) <article-title>Attention-gated reinforcement learning of internal representations for classification</article-title>. <source>Neural Comp</source> <volume>17</volume>: <fpage>2176</fpage>–<lpage>2214</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cassenaer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name> (<year>2012</year>) <article-title>Conditional modulation of spike-timing-dependent plasticity for olfactory learning</article-title>. <source>Nature</source> <volume>482</volume>: <fpage>47</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10776" xlink:type="simple">10.1038/nature10776</ext-link></comment> <object-id pub-id-type="pmid">22278062</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Adams</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name> (<year>1995</year>) <chapter-title>A model of how the basal ganglia generate and use neural signals that predict reinforcement</chapter-title>. In: <name name-style="western"><surname>Houk</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Beiser</surname> <given-names>DG</given-names></name>, editors. <source>Models of Information Processing in the Basal Ganglia</source>. <publisher-name>MIT Press</publisher-name>. pp. <fpage>1</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-011-0439-5" xlink:type="simple">10.1007/s00422-011-0439-5</ext-link></comment> <object-id pub-id-type="pmid">21701878</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yagishita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname> <given-names>GCR</given-names></name>, <name name-style="western"><surname>Urakubo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>S</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title>. <source>Science</source> <volume>345</volume>: <fpage>1616</fpage>–<lpage>1620</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1255514" xlink:type="simple">10.1126/science.1255514</ext-link></comment> <object-id pub-id-type="pmid">25258080</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rombouts</surname> <given-names>JO</given-names></name>, <name name-style="western"><surname>Bohte</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2012</year>) <article-title>Neurally Plausible Reinforcement Learning of Working Memory Tasks</article-title>. <source>NIPS</source> <volume>25</volume>. pp. <fpage>1880</fpage>–<lpage>1888</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassi</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Callaway</surname> <given-names>EM</given-names></name> (<year>2009</year>) <article-title>Parallel processing strategies of the primate visual system</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>360</fpage>–<lpage>372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2619" xlink:type="simple">10.1038/nrn2619</ext-link></comment> <object-id pub-id-type="pmid">19352403</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koulakov</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Raghavachari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kepecs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lisman</surname> <given-names>JE</given-names></name> (<year>2002</year>) <article-title>Model for a robust neural integrator</article-title>. <source>Nat Neurosci</source> <volume>5</volume>: <fpage>775</fpage>–<lpage>782</lpage>. <object-id pub-id-type="pmid">12134153</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2011</year>) <article-title>Same or Different? A Neural Circuit Mechanism of Similarity-Based Pattern Match Decision Making</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>6982</fpage>–<lpage>6996</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.6150-10.2011" xlink:type="simple">10.1523/JNEUROSCI.6150-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21562260</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fransén</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tahvildari</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Egorov</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Alonso</surname> <given-names>AA</given-names></name> (<year>2006</year>) <article-title>Mechanism of Graded Persistent Cellular Activity of Entorhinal Cortex Layer V Neurons</article-title>. <source>Neuron</source> <volume>49</volume>: <fpage>735</fpage>–<lpage>746</lpage>. <object-id pub-id-type="pmid">16504948</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egorov</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Hamam</surname> <given-names>BN</given-names></name>, <name name-style="western"><surname>Fransén</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hasselmo</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Alonso</surname> <given-names>AA</given-names></name> (<year>2002</year>) <article-title>Graded persistent activity in entorhinal cortex neurons</article-title>. <source>Nature</source> <volume>420</volume>: <fpage>173</fpage>–<lpage>178</lpage>. <object-id pub-id-type="pmid">12432392</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Funahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name> (<year>1989</year>) <article-title>Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex</article-title>. <source>J Neurophys</source> <volume>61</volume>: <fpage>331</fpage>–<lpage>349</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiering</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>HQ-learning</article-title>. <source>Adaptive Behavior</source> <volume>6</volume>: <fpage>219</fpage>–<lpage>246</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>KN</given-names></name> (<year>2006</year>) <article-title>A Physiologically Plausible Model of Action Selection and Oscillatory Activity in the Basal Ganglia</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>12921</fpage>–<lpage>12942</lpage>. <object-id pub-id-type="pmid">17167083</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Usher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name> (<year>2001</year>) <article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title>. <source>Psychol Rev</source> <volume>108</volume>: <fpage>550</fpage>–<lpage>592</lpage>. <object-id pub-id-type="pmid">11488378</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gurney</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name> (<year>2001</year>) <article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title>. <source>Biol Cybern</source> <volume>84</volume>: <fpage>401</fpage>–<lpage>410</lpage>. <object-id pub-id-type="pmid">11417052</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stewart</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Bekolay</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name> (<year>2012</year>) <article-title>Learning to select actions with spiking neurons in the Basal Ganglia</article-title>. <source>Front Neurosci</source> <volume>6</volume>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lo</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2006</year>) <article-title>Cortico–basal ganglia circuit mechanism for a decision threshold in reaction time tasks</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>956</fpage>–<lpage>963</lpage>. <object-id pub-id-type="pmid">16767089</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frey</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>RGM</given-names></name> (<year>1997</year>) <article-title>Synaptic tagging and long-term potentiation</article-title>. <source>Nature</source> <volume>385</volume>: <fpage>533</fpage>–<lpage>536</lpage>. <object-id pub-id-type="pmid">9020359</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moncada</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ballarini</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>JU</given-names></name>, <name name-style="western"><surname>Viola</surname> <given-names>H</given-names></name> (<year>2011</year>) <article-title>Identification of transmitter systems and learning tag molecules involved in behavioral tagging during memory formation</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>: <fpage>12931</fpage>–<lpage>12936</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1104495108" xlink:type="simple">10.1073/pnas.1104495108</ext-link></comment> <object-id pub-id-type="pmid">21768371</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mao</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kusefoglu</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hooks</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Petreanu</surname> <given-names>L</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Long-Range Neuronal Circuits Underlying the Interaction between Sensory and Motor Cortex</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>111</fpage>–<lpage>123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.07.029" xlink:type="simple">10.1016/j.neuron.2011.07.029</ext-link></comment> <object-id pub-id-type="pmid">21982373</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref034"><label>34</label><mixed-citation publication-type="other" xlink:type="simple">Rummery GA, Niranjan M (1994) On-line Q-learning using connectionist systems. Cambridge.</mixed-citation></ref>
<ref id="pcbi.1004060.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name> (<year>2005</year>) <article-title>Basal Ganglia Orient Eyes to Reward</article-title>. <source>J Neurophys</source> <volume>95</volume>: <fpage>567</fpage>–<lpage>584</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Representation of Action-Specific Reward Values in the Striatum</article-title>. <source>Science</source> <volume>310</volume>: <fpage>1337</fpage>–<lpage>1340</lpage>. <object-id pub-id-type="pmid">16311337</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Assad</surname> <given-names>JA</given-names></name> (<year>2006</year>) <article-title>Neurons in the orbitofrontal cortex encode economic value</article-title>. <source>Nature</source> <volume>441</volume>: <fpage>223</fpage>–<lpage>226</lpage>. <object-id pub-id-type="pmid">16633341</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>2002</year>) <article-title>Getting formal with dopamine and reward</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>241</fpage>–<lpage>263</lpage>. <object-id pub-id-type="pmid">12383780</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krueger</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>2009</year>) <article-title>Flexible shaping: How learning in small steps helps</article-title>. <source>Cognition</source> <volume>110</volume>: <fpage>380</fpage>–<lpage>394</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cognition.2008.11.014" xlink:type="simple">10.1016/j.cognition.2008.11.014</ext-link></comment> <object-id pub-id-type="pmid">19121518</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sommer</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Wurtz</surname> <given-names>RH</given-names></name> (<year>2001</year>) <article-title>Frontal eye field sends delay activity related to movement, memory, and vision to the superior colliculus</article-title>. <source>J Neurophys</source> <volume>85</volume>: <fpage>1673</fpage>–<lpage>1685</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rigotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Warden</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>The importance of mixed selectivity in complex cognitive tasks</article-title>. <source>Nature</source> <volume>497</volume>: <fpage>585</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12160" xlink:type="simple">10.1038/nature12160</ext-link></comment> <object-id pub-id-type="pmid">23685452</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freedman</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>EK</given-names></name> (<year>2001</year>) <article-title>Categorical representation of visual stimuli in the primate prefrontal cortex</article-title>. <source>Science</source> <volume>291</volume>: <fpage>312</fpage>–<lpage>316</lpage>. <object-id pub-id-type="pmid">11209083</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gold</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Shadlen</surname> <given-names>MN</given-names></name> (<year>2007</year>) <article-title>The Neural Basis of Decision Making</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>535</fpage>–<lpage>574</lpage>. <object-id pub-id-type="pmid">17600525</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soltani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2009</year>) <article-title>Synaptic computation underlying probabilistic inference</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>112</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2450" xlink:type="simple">10.1038/nn.2450</ext-link></comment> <object-id pub-id-type="pmid">20010823</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lemus</surname> <given-names>L</given-names></name> (<year>1999</year>) <article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title>. <source>Nature</source> <volume>399</volume>: <fpage>470</fpage>–<lpage>473</lpage>. <object-id pub-id-type="pmid">10365959</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name> (<year>2005</year>) <article-title>Flexible Control of Mutual Inhibition: A Neural Model of Two-Interval Discrimination</article-title>. <source>Science</source> <volume>307</volume>: <fpage>1121</fpage>–<lpage>1124</lpage>. <object-id pub-id-type="pmid">15718474</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X-J</given-names></name> (<year>2006</year>) <article-title>Inhibitory control by an integral feedback signal in prefrontal cortex: A model of discrimination between sequential stimuli</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>103</volume>: <fpage>201</fpage>–<lpage>206</lpage>. <object-id pub-id-type="pmid">16371469</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>ROlls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name> (<year>2010</year>) <article-title>Synaptic dynamics and decision making</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>107</volume>: <fpage>7545</fpage>–<lpage>7549</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1002333107" xlink:type="simple">10.1073/pnas.1002333107</ext-link></comment> <object-id pub-id-type="pmid">20360555</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2013</year>) <article-title>From fixed points to chaos: three models of delayed discrimination</article-title>. <source>Progress in Neurobiology</source> <volume>103</volume>: <fpage>214</fpage>–<lpage>222</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.pneurobio.2013.02.002" xlink:type="simple">10.1016/j.pneurobio.2013.02.002</ext-link></comment> <object-id pub-id-type="pmid">23438479</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zainos</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name> (<year>2003</year>) <article-title>Correlated neuronal discharges that increase coding efficiency during perceptual discrimination</article-title>. <source>Neuron</source> <volume>38</volume>: <fpage>649</fpage>–<lpage>657</lpage>. <object-id pub-id-type="pmid">12765615</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Salinas</surname> <given-names>E</given-names></name> (<year>2003</year>) <article-title>Flutter Discrimination: neural codes, perception, memory and decision making</article-title>. <source>Nat Rev Neurosci</source> <volume>4</volume>: <fpage>203</fpage>–<lpage>218</lpage>. <object-id pub-id-type="pmid">12612633</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Zainos</surname> <given-names>A</given-names></name> (<year>2004</year>) <article-title>Neuronal correlates of a perceptual decision in ventral premotor cortex</article-title>. <source>Neuron</source> <volume>41</volume>: <fpage>165</fpage>–<lpage>173</lpage>. <object-id pub-id-type="pmid">14715143</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Boyan J, Moore AW (1995) Generalization in reinforcement learning: Safely approximating the value function. NIPS: 369–376.</mixed-citation></ref>
<ref id="pcbi.1004060.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Baird L (1995) Residual algorithms: Reinforcement learning with function approximation. ICML-95: 30–37.</mixed-citation></ref>
<ref id="pcbi.1004060.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deubel</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>WX</given-names></name> (<year>1996</year>) <article-title>Saccade target selection and object recognition: Evidence for a common attentional mechanism</article-title>. <source>Vision Res</source> <volume>36</volume>: <fpage>1827</fpage>–<lpage>1837</lpage>. <object-id pub-id-type="pmid">8759451</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schoups</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Qian</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Orban</surname> <given-names>G</given-names></name> (<year>2001</year>) <article-title>Practising orientation identification improves orientation coding in V1 neurons</article-title>. <source>Nature</source> <volume>412</volume>: <fpage>549</fpage>–<lpage>553</lpage>. <object-id pub-id-type="pmid">11484056</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahissar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hochstein</surname> <given-names>S</given-names></name> (<year>1993</year>) <article-title>Attentional control of early perceptual learning</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>90</volume>: <fpage>5718</fpage>–<lpage>5722</lpage>. <object-id pub-id-type="pmid">8516322</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chun</surname> <given-names>MM</given-names></name> (<year>2001</year>) <article-title>Selective attention modulates implicit learning</article-title>. <source>Q J Exp Psychol</source> <volume>54</volume>: <fpage>1105</fpage>–<lpage>1124</lpage>. <object-id pub-id-type="pmid">11765735</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moore</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Armstrong</surname> <given-names>KM</given-names></name> (<year>2003</year>) <article-title>Selective gating of visual signals by microstimulation of frontal cortex</article-title>. <source>Nature</source> <volume>421</volume>: <fpage>370</fpage>–<lpage>373</lpage>. <object-id pub-id-type="pmid">12540901</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>van Ooyen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>T</given-names></name> (<year>2010</year>) <article-title>Perceptual learning rules based on reinforcers and attention</article-title>. <source>Trends Cogn Sci</source> <volume>14</volume>: <fpage>64</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2009.11.005" xlink:type="simple">10.1016/j.tics.2009.11.005</ext-link></comment> <object-id pub-id-type="pmid">20060771</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kilgard</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Merzenich</surname> <given-names>MM</given-names></name> (<year>1998</year>) <article-title>Cortical Map Reorganization Enabled by Nucleus Basalis Activity</article-title>. <source>Science</source> <volume>279</volume>: <fpage>1714</fpage>–<lpage>1718</lpage>. <object-id pub-id-type="pmid">9497289</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Richardson</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>DeLong</surname> <given-names>MR</given-names></name> (<year>1986</year>) <article-title>Nucleus basalis of Meynert neuronal activity during a delayed response task in monkey</article-title>. <source>Brain Res</source> <volume>399</volume>: <fpage>364</fpage>–<lpage>368</lpage>. <object-id pub-id-type="pmid">3828770</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peck</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Salzman</surname> <given-names>CD</given-names></name> (<year>2014</year>) <article-title>The Amygdala and Basal Forebrain as a Pathway for Motivationally Guided Attention</article-title>. <source>J Neurosci</source> <volume>34</volume>: <fpage>13757</fpage>–<lpage>13767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.2106-14.2014" xlink:type="simple">10.1523/JNEUROSCI.2106-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25297102</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Easton</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ridley</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Baker</surname> <given-names>HF</given-names></name>, <name name-style="western"><surname>Gaffan</surname> <given-names>D</given-names></name> (<year>2002</year>) <article-title>Unilateral lesions of the cholinergic basal forebrain and fornix in one hemisphere and inferior temporal cortex in the opposite hemisphere produce severe learning impairments in rhesus monkeys</article-title>. <source>Cereb Cortex</source> <volume>12</volume>: <fpage>729</fpage>–<lpage>736</lpage>. <object-id pub-id-type="pmid">12050084</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>Dorsal Raphe Neurons Signal Reward through 5-HT and Glutamate</article-title>. <source>Neuron</source> <volume>81</volume>: <fpage>1360</fpage>–<lpage>1374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.02.010" xlink:type="simple">10.1016/j.neuron.2014.02.010</ext-link></comment> <object-id pub-id-type="pmid">24656254</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2005</year>) <article-title>Cascade Models of Synaptically Stored Memories</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>599</fpage>–<lpage>611</lpage>. <object-id pub-id-type="pmid">15721245</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedrich</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2011</year>) <article-title>Spatio-Temporal Credit Assignment in Neuronal Population Learning</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002092</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002092" xlink:type="simple">10.1371/journal.pcbi.1002092</ext-link></comment> <object-id pub-id-type="pmid">21738460</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name> (<year>2003</year>) <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title>. <source>Neuron</source> <volume>40</volume>: <fpage>1063</fpage>–<lpage>1073</lpage>. <object-id pub-id-type="pmid">14687542</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname> <given-names>EM</given-names></name> (<year>2006</year>) <article-title>Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling</article-title>. <source>Cereb Cortex</source> <volume>17</volume>: <fpage>2443</fpage>–<lpage>2452</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Urbanczik</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name> (<year>2009</year>) <article-title>Reinforcement learning in populations of spiking neurons</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>250</fpage>–<lpage>252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.2264" xlink:type="simple">10.1038/nn.2264</ext-link></comment> <object-id pub-id-type="pmid">19219040</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potjans</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Morrison</surname> <given-names>A</given-names></name> (<year>2011</year>) <article-title>An Imperfect Dopaminergic Error Signal Can Drive Temporal-Difference Learning</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001133</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001133" xlink:type="simple">10.1371/journal.pcbi.1001133</ext-link></comment> <object-id pub-id-type="pmid">21589888</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name> (<year>2006</year>) <article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title>. <source>Neural Comp</source> <volume>18</volume>: <fpage>283</fpage>–<lpage>328</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Suri</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name> (<year>1998</year>) <article-title>Learning of sequential movements by neural network model with dopamine-like reinforcement signal</article-title>. <source>Exp Brain Res</source> <volume>121</volume>: <fpage>350</fpage>–<lpage>354</lpage>. <object-id pub-id-type="pmid">9746140</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerzer</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name> (<year>2014</year>) <article-title>Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning</article-title>. <source>Cereb Cortex</source> <volume>24</volume>: <fpage>677</fpage>–<lpage>690</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/bhs348" xlink:type="simple">10.1093/cercor/bhs348</ext-link></comment> <object-id pub-id-type="pmid">23146969</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name> (<year>1992</year>) <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title>. <source>Mach Learn</source> <volume>8</volume>: <fpage>229</fpage>–<lpage>256</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref076"><label>76</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fremaux</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sprekeler</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2013</year>) <article-title>Reinforcement Learning Using a Continuous Time Actor-Critic Framework with Spiking Neurons</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1003024</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003024" xlink:type="simple">10.1371/journal.pcbi.1003024</ext-link></comment> <object-id pub-id-type="pmid">23592970</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zipser</surname> <given-names>D</given-names></name> (<year>1991</year>) <article-title>Recurrent network model of the neural mechanism of short-term active memory</article-title>. <source>Neural Comp</source> <volume>3</volume>: <fpage>179</fpage>–<lpage>193</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochreiter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>Long short-term memory</article-title>. <source>Neural Comp</source> <volume>9</volume>: <fpage>1735</fpage>–<lpage>1780</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref079"><label>79</label><mixed-citation publication-type="other" xlink:type="simple">O’Reilly RC, Hazy TE, Herd SA (2012) The leabra cognitive architecture: how to play 20 principles with nature and win! The Oxford Handbook of Cognitive Science.</mixed-citation></ref>
<ref id="pcbi.1004060.ref080"><label>80</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Munakata</surname> <given-names>Y</given-names></name> (<year>2000</year>) <chapter-title>Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain</chapter-title>. <publisher-name>the MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matsumoto</surname> <given-names>K</given-names></name> (<year>2003</year>) <article-title>Neuronal Correlates of Goal-Based Motor Selection in the Prefrontal Cortex</article-title>. <source>Science</source> <volume>301</volume>: <fpage>229</fpage>–<lpage>232</lpage>. <object-id pub-id-type="pmid">12855813</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name> (<year>2007</year>) <article-title>Orbitofrontal Cortex and Its Contribution to Decision-Making</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>31</fpage>–<lpage>56</lpage>. <object-id pub-id-type="pmid">17417936</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luk</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Wallis</surname> <given-names>JD</given-names></name> (<year>2009</year>) <article-title>Dynamic Encoding of Responses and Outcomes by Neurons in Medial Prefrontal Cortex</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>7526</fpage>–<lpage>7539</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.0386-09.2009" xlink:type="simple">10.1523/JNEUROSCI.0386-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19515921</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parisien</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name> (<year>2008</year>) <article-title>Solving the problem of negative synaptic weights in cortical models</article-title>. <source>Neural Comp</source> <volume>20</volume>: <fpage>1473</fpage>–<lpage>1494</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004060.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Self</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Kooijmans</surname> <given-names>RN</given-names></name>, <name name-style="western"><surname>Supèr</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lamme</surname> <given-names>VAF</given-names></name>, <name name-style="western"><surname>Roelfsema</surname> <given-names>PR</given-names></name> (<year>2012</year>) <article-title>Different glutamate receptors convey feedforward and recurrent processing in macaque V1</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>109</volume>: <fpage>11031</fpage>–<lpage>11036</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1119527109" xlink:type="simple">10.1073/pnas.1119527109</ext-link></comment> <object-id pub-id-type="pmid">22615394</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Guillery</surname> <given-names>RW</given-names></name> (<year>1998</year>) <article-title>On the actions that one nerve cell can have on another: distinguishing “drivers” from ‘modulators’</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>95</volume>: <fpage>7121</fpage>–<lpage>7126</lpage>. <object-id pub-id-type="pmid">9618549</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Pasquale</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sherman</surname> <given-names>SM</given-names></name> (<year>2011</year>) <article-title>Synaptic Properties of Corticocortical Connections between the Primary and Secondary Visual Cortical Areas in the Mouse</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>16494</fpage>–<lpage>16506</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.3664-11.2011" xlink:type="simple">10.1523/JNEUROSCI.3664-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22090476</object-id></mixed-citation></ref>
<ref id="pcbi.1004060.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sajikumar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Korte</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Metaplasticity governs compartmentalization of synaptic tagging and capture through brain-derived neurotrophic factor (BDNF) and protein kinase Mζ (PKMζ)</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>: <fpage>2551</fpage>–<lpage>2556</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1016849108" xlink:type="simple">10.1073/pnas.1016849108</ext-link></comment> <object-id pub-id-type="pmid">21248226</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>