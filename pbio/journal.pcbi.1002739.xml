<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00730</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002739</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Systems biology</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Systems biology</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Genetics and Genomics</subject>
          <subject>Computational Biology</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Evolution of Associative Learning in Chemical Networks</article-title>
        <alt-title alt-title-type="running-head">Evolving Bayesian Learning</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>McGregor</surname>
            <given-names>Simon</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Vasas</surname>
            <given-names>Vera</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Husbands</surname>
            <given-names>Phil</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Fernando</surname>
            <given-names>Chrisantha</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Department of Informatics, University of Sussex, Falmer, Brighton, United Kingdom</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Departament de Genètica i de Microbiologia, Grup de Biologia Evolutiva (GBE), Universitat Autònoma de Barcelona, Bellaterra, Barcelona, Spain</addr-line>
      </aff>
      <aff id="aff3">
        <label>3</label>
        <addr-line>School of Electronic Engineering and Computer Science (EECS), Queen Mary, University of London, London, United Kingdom</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Morrison</surname>
            <given-names>Abigail</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>Research Center Jülich, Germany</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">ctf20@eecs.qmul.ac.uk</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: SM VV PH CF. Performed the experiments: SM VV PH CF. Analyzed the data: SM VV PH CF. Contributed reagents/materials/analysis tools: SM VV PH CF. Wrote the paper: SM VV PH CF.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>11</month>
        <year>2012</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>1</day>
        <month>11</month>
        <year>2012</year>
      </pub-date>
      <volume>8</volume>
      <issue>11</issue>
      <elocation-id>e1002739</elocation-id>
      <history>
        <date date-type="received">
          <day>4</day>
          <month>5</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>23</day>
          <month>8</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2012</copyright-year>
        <copyright-holder>McGregor et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Organisms that can learn about their environment and modify their behaviour appropriately during their lifetime are more likely to survive and reproduce than organisms that do not. While associative learning – the ability to detect correlated features of the environment – has been studied extensively in nervous systems, where the underlying mechanisms are reasonably well understood, mechanisms within single cells that could allow associative learning have received little attention. Here, using <italic>in silico</italic> evolution of chemical networks, we show that there exists a diversity of remarkably simple and plausible chemical solutions to the associative learning problem, the simplest of which uses only one core chemical reaction. We then asked to what extent a linear combination of chemical concentrations in the network could approximate the ideal Bayesian posterior of an environment given the stimulus history so far? This Bayesian analysis revealed the ‘memory traces’ of the chemical network. The implication of this paper is that there is little reason to believe that a lack of suitable phenotypic variation would prevent associative learning from evolving in cell signalling, metabolic, gene regulatory, or a mixture of these networks in cells.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Whilst one may have believed that associative learning requires a nervous system, this paper shows that chemical networks can be evolved <italic>in silico</italic> to undertake a range of associative learning tasks with only a small number of reactions. The mechanisms are surprisingly simple. The networks can be analysed using Bayesian methods to identify the components of the network responsible for learning. The networks evolved were simpler in some ways to hand-designed synthetic biology networks for associative learning. The motifs may be looked for in biochemical networks and the hypothesis that they undertake associative learning, e.g. in single cells or during development may be legitimately entertained.</p>
      </abstract>
      <funding-group>
        <funding-statement>The work is funded by an FP-7 FET OPEN Grant, E-FLUX, the Hungarian NAP project, and the Templeton Grant FQEB entitled “Bayes, Darwin and Hebb”. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="19"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Here we evolve chemical networks in simulation to undertake associative learning. We define learning as the process by which information about the world is encoded into internal state (a memory-trace) in order to behave more adaptively in the future. Associative learning is learning of a relation between two types of event. Remarkably, the most frequently found circuits consisted of only one or two core chemical reactions responsible for learning, the other reactions being involved in subsidiary functions such as signal transduction. This is functionally simpler than the previously hand-designed biochemical circuits for classical conditioning that require several chemical reactions to implement Hebbian learning (a term which we use to refer to a mechanism that ensures that event A co-occurring with event B results in a greater probability that event B will occur given future presentations of A alone <xref ref-type="bibr" rid="pcbi.1002739-Fernando1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Gandhi1">[2]</xref>). Thus, this is a beautiful example of how evolution can find elegant solutions.</p>
      <p>Chemical kinetics is Turing complete and therefore any computable mechanism for associative learning is theoretically possible <xref ref-type="bibr" rid="pcbi.1002739-Magnasco1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Hjelmfelt1">[4]</xref>, however, this says nothing about which kinds of chemical mechanisms for learning are likely to evolve. Here we use <italic>in silico</italic> natural selection <xref ref-type="bibr" rid="pcbi.1002739-Goldstein1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Parter1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Bray1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Bray2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Paladugu1">[9]</xref> to evolve chemical networks that are selected on the basis of their ability to carry out various associative learning tasks. Also known as genetic algorithms <xref ref-type="bibr" rid="pcbi.1002739-Holland1">[10]</xref> or evolutionary computation <xref ref-type="bibr" rid="pcbi.1002739-Fogel1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Baeck1">[12]</xref>, the principle follows that of selective breeding. An initial random population of chemical networks is constructed. Each network is assessed for its quality as defined by a ‘fitness function’ that maps quality to fitness. The next generation is produced by allowing networks to replicate with mutation (and crossover) in proportion to their fitness. This process iterates for many generations, eventually producing higher quality networks that are capable of solving the desired task. The closest work to ours is the evolution of associative learning in continuous recurrent neural networks <xref ref-type="bibr" rid="pcbi.1002739-Phattanasri1">[13]</xref>.</p>
      <p>Our simulation evolves an abstract chemistry; however unlike many experiments with purely artificial chemistries <xref ref-type="bibr" rid="pcbi.1002739-Bagley1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Fontana1">[15]</xref> it was designed to respect conservation of mass and energy, an essential consideration for transferring the insights from <italic>in silico</italic> models to chemical reality <xref ref-type="bibr" rid="pcbi.1002739-Fernando2">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Fernando3">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Vasas1">[18]</xref>, which is our ultimate goal. Each ‘molecule’ consists of ‘0’ and ‘1’ atoms, and only the number of digits (and not their sequence) determines the species' identity. Any interchange of building blocks between molecules was allowed to happen in reactions. With the exception of the implicit decay reactions, all the simulated chemical reactions are reversible., However, some reactions may be effectively irreversible because the reaction rate in the backward direction is very low compared to the reaction rate in the forward direction. For details of the artificial chemistry model refer to the Methods section. Results from a pilot study with simpler chemistry are described in Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref>, part 1.</p>
      <p>Traditionally there are two types of associative learning, classical and instrumental conditioning, the former involves passive observation of events, e.g. associating the sound of a bell with the smell of food, and the later involves relating self-generated actions and their consequences, e.g. learning that pressing a lever produces food <xref ref-type="bibr" rid="pcbi.1002739-Dayan1">[19]</xref>. We developed tasks that evoke the classical conditioning paradigm in psychology <xref ref-type="bibr" rid="pcbi.1002739-Mackintosh1">[20]</xref>. The network receives input from the environment (in the form of chemical boluses externally introduced into the system) and produces output (defined as the concentration of a particular chemical species measured over a particular test-period). The chemical dynamics of the system (the changes in concentration of chemical species) describe the behaviour of the network according to its sensory input.</p>
      <p>In all the learning tasks, the chemical network had to learn to anticipate the injection of a control chemical C, known as the unconditioned stimulus UCS in the classical conditioning literature. Anticipation of C means to act in a manner that shows knowledge that certain events can predict C. Anticipation can be learned or innate. In our tasks it is necessary to learn to anticipate, not just to evolve innate temporal expectations. All tasks involve two possible conditions. In one condition the network should be able to use another chemical S (stimulus pulse), i.e. the conditioned stimulus CS, that reliably precedes C to predict the occurrence of C. Prediction results in the production of an output chemical O - the conditioned response CR - immediately after S is presented but prior to C. If this condition has been properly inferred, output chemical O should then be reliably elicited by the stimulus pulse S alone, after pairing S with C. This describes the “associated” condition. In the other, “non-associated” condition, S cannot theoretically be used to predict C. We therefore no not wish to see a CR (i.e. no output O production) following S. Thus, in all cases the network's fitness depends on whether it has learned the association between S and C by requiring it to produce an output chemical after S only when it is reliably followed by C, but not otherwise. There is no explicit training and testing phase in our experiments. The network's task is to respond appropriately as quickly as possible.</p>
      <p>Consider a possible real-world example of how such functionality may be adaptive. Imagine that C (UCS) is a toxin and that S (CS) is a chemical that in some environments (but not others) can predict that toxin. Imagine that a metabolically expensive anti-toxin O (CR) can be synthesised to neutralise the toxin C. Then it would be advantageous to use S to initiate the synthesis of anti-toxin O in lieu of C in the environments in which S was predictive, but not in those environments in which S was not predictive, where instead the no O should occur, i.e. no production of anti-toxin in response to S. All tasks pose variants of this fundamental problem. The fact the network may find itself in either environment within a lifetime means that it could not evolve the simple strategy of sensitization where it always produces output chemical O in response to S. We used five different tasks, designed to provide a systematically more challenging associative learning problem. A summary of the tasks, and the information required for achieving maximal fitness on them (i.e. the simplest discrimination that is sufficient for optimal performance), is given in <xref ref-type="table" rid="pcbi-1002739-t001">Table 1</xref>. The first two tasks do not require detection of a temporal correlation between S and C, i.e. they can be solved without associative learning, i.e. by sensitization/habituation alone. They demonstrate that in restricted environments, information about associations between things can be equivalent to information about simpler (lower-order) environmental features, such as the frequency of individual event types. However, the later three tasks are designed such that they necessitate discriminations based on observation of associations, e.g. discriminating environments in which S and C are temporally correlated compared to environments in which they occur independently. Thus, the final three tasks are true associative learning tasks that cannot be solved without the capacity to observe associations and modify ones behaviour accordingly.</p>
      <table-wrap id="pcbi-1002739-t001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Five learning tasks of differing complexity on which chemical networks were evolved.</title>
        </caption>
        <alternatives>
          <graphic id="pcbi-1002739-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.t001" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1">Task</td>
                <td colspan="2" align="left" rowspan="1">Typical Inputs by Environment Type</td>
                <td align="left" rowspan="1" colspan="1">Network required to determine:</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Unassociated</td>
                <td align="left" rowspan="1" colspan="1">Associated</td>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Clocked [Non-associative]</td>
                <td align="left" rowspan="1" colspan="1">S pulses alone</td>
                <td align="left" rowspan="1" colspan="1">S→C pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">Do C pulses occur?</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Noisy Clocked [Non-associative]</td>
                <td align="left" rowspan="1" colspan="1">S pulses alone</td>
                <td align="left" rowspan="1" colspan="1">S→C pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">Do C pulses occur more often than S pulses alone?</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Non-Clocked [True Associative]</td>
                <td align="left" rowspan="1" colspan="1">S and C pulses with independent timing</td>
                <td align="left" rowspan="1" colspan="1">S→C pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">Do C pulses tend to occur shortly after S pulses?</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">AB-BA [True Associative]</td>
                <td align="left" rowspan="1" colspan="1">C→S pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">S→C pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">Do S→C pulse pairs tend to occur more often than C→S pulses, over an extended period?</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">2-bit Environment [True Associative]</td>
                <td align="left" rowspan="1" colspan="1">C→C, C→S and S→S pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">S→C pulse pairs</td>
                <td align="left" rowspan="1" colspan="1">Do more S→C pulse pairs occur than any other type of input event?</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>Classical conditioning involves a wide range of different training and testing regimes, e.g. Pavlovian conditioning <xref ref-type="bibr" rid="pcbi.1002739-Rescorla1">[21]</xref>, Blocking <xref ref-type="bibr" rid="pcbi.1002739-Kamin1">[22]</xref>, Backwards Blocking <xref ref-type="bibr" rid="pcbi.1002739-Gopnik1">[23]</xref>, overshadowing <xref ref-type="bibr" rid="pcbi.1002739-Dayan1">[19]</xref>, etc. Typically these paradigms show an unconditioned response to the control (UCS). Above we have used a set of training and testing regimes that do not explicitly require an unconditioned response (UCR) to the UCS (control) molecule alone. In other words, we have assumed that a straightforward chemical reaction exists, independent of the network modelled, that is capable of producing an UCR to the control molecule. An important aspect of classical conditioning is extinction, a reduction in the conditioned response CR when the conditioned stimulus CS (stimulus) is repeatedly presented in the absence of the unconditioned stimulus UCS (control). All the networks presented here show extinction, even though they were not explicitly evolved on an extinction paradigm, see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref> Part 2.</p>
      <sec id="s1a">
        <title>Clocked task</title>
        <p>The times when the network must respond by producing an output O when stimulus S is associated with chemical C were constrained to regular “clock ticks” to make the task as easy as possible for the networks. Because there is no noise, this is a simple task as the very first input event (which is either S on its own, or S followed by C) provides all the necessary information for maximising fitness (<xref ref-type="fig" rid="pcbi-1002739-g001">Figure 1</xref>). The blue blobs show the time at which the target output is required, i.e. when the target output contributes to fitness. In the associated condition the target output is high (1) and in the unassociated condition the target output is low (0). At all other times it does not matter what the target output is. This was intended to give evolution more leeway by imposing fewer constraints. Even so, many evolved solutions maintained the output concentration at low levels when the target output was not evaluated.</p>
        <fig id="pcbi-1002739-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Clocked task illustration.</title>
            <p>Above: “unassociated” condition. Below: “associated” condition. Stimulus (“S”) (CS) and control (“C”) (UCS) pulses are shown as black and grey spikes respectively. Circles show (desired) target concentrations of the output chemical O (CR = high O). In the unassociatied condition, input S is given and the output chemical must remain low during the period when the output is assessed (blue circles). In the lower, associated condition, two inputs (S and C) are provided and the network must now produce a high output chemical concentration during the period the output is assessed (blue circles). Note that input S signals the onset of the period that the output chemical must have a high concentration, and input C signals the end of that period. The chemical network must use its knowledge of input C to determine its response to input S.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g001" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s1b">
        <title>Noisy clocked task</title>
        <p>This task is identical to task 1., except that stimulus-control pulse pairs occurred with a low (non-zero) frequency in the unassociated environment and stimulus pulses without control pulses occurred with a low (non-zero) frequency in the associated environment. This produced ambiguity about the hidden state (which environment the network is in) on the basis of observed state variables (S and C pulses). Here, high fitness networks must consider more of the past, since isolated input events are unreliable indicators of the correct output chemical response (<xref ref-type="fig" rid="pcbi-1002739-g002">Figure 2</xref>.). A successful chemical network should update its ‘belief’ in which environment it is in on the basis of several observed associations, not just one; in other words, it must integrate information over time. For example, if we examine <xref ref-type="fig" rid="pcbi-1002739-g002">Figure 2</xref>., we see that the second stimulus pulse is followed by a control pulse even in the unassociated condition, and that the second stimulus pulse is not followed by a control pulse in the associated condition. However, the reader will notice that ‘cheating’ is possible in these two tasks because in the associated condition C occurs more often <italic>in total</italic> than in the unassociated condition, thus simply learning to respond to S when C is on average higher in concentration is a sufficient strategy. The temporal relation between S and C does not need to be learned here. This simple solution is excluded in the design of the next task.</p>
        <fig id="pcbi-1002739-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Noisy clocked task illustration.</title>
            <p>Above: “unassociated” condition. Below: “associated” condition. Stimulus (“S”) pulses and control (“C”) pulses are shown as black and grey spikes respectively. Circles show target output chemical concentration values. Note that the second input event, at time t = 150, (in both conditions) is a noisy event, either a false positive or a false negative control chemical pulse occurs. The environments can be distinguished on the fact that in the associated condition below the pulses co-occur with greater frequency than in the unassociated condition.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g002" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s1c">
        <title>Non-clocked associative task</title>
        <p>In this task the timing of stimulus pulse and control pulse input events was unconstrained, and, most importantly, the unassociated and the associated environments received the same number of control pulses, except that in the unassociatied environment they were randomly distributed while in the associated environment they reliably followed stimulus pulses. Therefore this task was harder still, since it involved detecting relational aspects of inputs rather than merely first-order statistics of control pulses like the first two tasks (<xref ref-type="fig" rid="pcbi-1002739-g003">Figure 3</xref>).</p>
        <fig id="pcbi-1002739-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Non-clocked task illustration.</title>
            <p>Above: “Unassociated” condition. Below: “associated” condition. Stimulus (“S”) and control (“C”) pulses are shown as black and grey spikes respectively. Circles show target (desired) output chemical concentration values. Here, in the unassociated condition both C and S pulses occur, but C pulses do not reliably follow S pulses unlike the associated condition. Higher fitness could be achieved by detecting relational aspects of inputs, rather than simply observing the occurrence of control events as in the previous tasks.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g003" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s1d">
        <title>AB-BA task</title>
        <p>Like task 3., this task used unconstrained input timing with noise and required relations between inputs to be detected. The difference is that in the first environment, where the network was required to keep the output chemical concentration low, control pulses reliably preceded stimulus pulses (<xref ref-type="fig" rid="pcbi-1002739-g004">Figure 4</xref>) rather than the other way around. In both cases S and C are associated, but occur in a different temporal order. The network must distinguish between these two kinds of temporal relationship.</p>
        <fig id="pcbi-1002739-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>AB-BA task illustration.</title>
            <p>Above: “C→S” condition. Below: “S→C” condition. Stimulus (“S”) and control (“C”) pulses are shown as grey and black spikes respectively. Circles show target output values. This task spans a longer time period than the others, because it is noisier. In the C→S condition C pulses typically precede S pulses, whereas in the S→C condition, S pulses typically precede C pulses. The chemical network must produce a high output chemical concentration following the S pulse in the S→C condition but not in the C→S condition. The noise involves flipping of the order of S and C pulses so that S→C pulses sometimes occur in the “C→S” condition and vice versa. The noisiness can be controlled.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g004" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s1e">
        <title>“2-bit environment” task</title>
        <p>The previous tasks described classes of stochastically-generated environment. Hence, any one network could be evaluated only on a sample of the environments typical of the task. By contrast, this task was designed by hand to provide a significant challenge while allowing exhaustive evaluation. The networks performance was measured in four environments (all possible combinations of stimulus-control pulse pairs). Maximal fitness required accumulating relational data over multiple input events; the task was specifically designed to exclude strategies that rely on the first or most recent input event (<xref ref-type="fig" rid="pcbi-1002739-g005">Figure 5</xref>). Unlike the previous experiments, the network must learn 2 bits of information because it must distinguish one of 2<sup>2</sup> states (not just 2<sup>1</sup> states).</p>
        <fig id="pcbi-1002739-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>2-bit environment task illustration.</title>
            <p>Conditions from top to bottom: “C→C”, “C→S”, “S→C”, “S→S”. Stimulus (“S”) and control (“C”) pulses are shown as grey and black spikes respectively. Circles show target output values. In this task the only condition in which the output chemical should be high is where S pulses precede C pulses. Notice that we only assess the output during the second part of each condition, giving the network some time to make a judgement about which condition it is in. This task was designed by hand to provide a significant challenge while allowing exhaustive evaluation.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g005" position="float" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>We were able to evolve highly fit networks for each of the tasks above. Dynamics of the best performing networks on the five different tasks are shown in <xref ref-type="fig" rid="pcbi-1002739-g006">Figures 6</xref>–<xref ref-type="fig" rid="pcbi-1002739-g010">10</xref> (for details of the chemical networks see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref> Part 3). The networks display learning – changes in state which reflect the statistics of their past inputs, and determine their response to input boluses adaptively. Note that the network's performance typically increases over the evaluation period, suggesting that a long-term memory-trace builds up over consecutive stimulus-control pairs.</p>
      <fig id="pcbi-1002739-g006" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g006</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Sample dynamics of an evolved network for the clocked task.</title>
          <p>Upper: “unassociated” condition. Lower: “associated” condition. Black solid line shows output concentration; blue solid line shows stimulus concentration; green solid line shows control concentration. Dotted lines show intermediate chemical concentrations. Circles indicate target output values for the network. Triangles show input boluses.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g006" position="float" xlink:type="simple"/>
      </fig>
      <fig id="pcbi-1002739-g007" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g007</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Sample dynamics of an evolved network for the noisy clocked task.</title>
          <p>Upper: “unassociated” condition. Lower: “associated” condition. Black solid line shows output concentration; red solid line shows stimulus concentration; blue solid line shows control concentration. Dotted lines show intermediate chemical concentrations. Circles indicate target output values for the network. Triangles show input boluses.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g007" position="float" xlink:type="simple"/>
      </fig>
      <fig id="pcbi-1002739-g008" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g008</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Sample dynamics of an evolved network for the non-clocked task.</title>
          <p>Upper: “unassociated” condition. Lower: “associated” condition. Black solid line shows output concentration; yellow solid line shows stimulus concentration; blue solid line shows control concentration. Dotted lines show intermediate chemical concentrations. Circles indicate target output values for the network. Triangles show input boluses.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g008" position="float" xlink:type="simple"/>
      </fig>
      <fig id="pcbi-1002739-g009" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g009</object-id>
        <label>Figure 9</label>
        <caption>
          <title>Sample dynamics of an evolved network for the AB-BA task.</title>
          <p>Upper: “unassociated” condition. Lower: “associated” condition. Black solid line shows output concentration; blue solid line shows stimulus concentration; purple solid line shows control concentration. Dotted lines show intermediate chemical concentrations. Circles indicate target output values for the network. Triangles show input boluses.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g009" position="float" xlink:type="simple"/>
      </fig>
      <fig id="pcbi-1002739-g010" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g010</object-id>
        <label>Figure 10</label>
        <caption>
          <title>Sample dynamics of an evolved network for the 2-bit environment task.</title>
          <p>From top to bottom: C→C, C→S, S→C and S→S environments. Black solid line shows output concentration; yellow solid line shows stimulus concentration; blue solid line shows control concentration. Dotted lines show intermediate chemical concentrations. Circles indicate target output values for the network. Triangles show input boluses.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g010" position="float" xlink:type="simple"/>
      </fig>
      <p>The differences in task difficulty can also be observed on the graphs. For the simplest, clocked, task one input event was enough for the network to decide about the environment; but for the AB-BA or the 2-bit task a much longer training period was required. <xref ref-type="fig" rid="pcbi-1002739-g006">Figure 6</xref> shows the performance in the Clocked task. The output chemical O (molecule ‘01’) is shown in black. In the top (unassociated) condition after the first presentation of stimulus S and the absence of a control bolus, its concentration drops and never returns. In the associated environment below, the output chemical shows the opposite dynamics after the first paired input of S and C. <xref ref-type="fig" rid="pcbi-1002739-g007">Figure 7</xref> shows the evolved performance of a network on the noisy clocked task. The output chemical is again shown in black, and again in the unassociated task its concentration gradually declines (except after a misleading S-C pair shown during the second input event). In the associated environment the black output chemical continues to be produced when the network is stimulated with S. <xref ref-type="fig" rid="pcbi-1002739-g008">Figure 8</xref> shows performance on the non-clocked task where it is necessary to learn explicitly the temporal correlation between S and C because in both tasks the overall amount of S and C is the same. Again an evolved network is successful in this because the black output chemical is only produced in the associated condition below and not in the unassociated condition above. <xref ref-type="fig" rid="pcbi-1002739-g009">Figure 9</xref> shows the performance of a network that successfully evolved to solve the AB-BA task. The concentration of the output chemical in the lower condition is higher on average than the output concentration in the upper condition. The performance was only assessed during the second half of the task and this is where the greatest difference in black chemical output is seen. <xref ref-type="fig" rid="pcbi-1002739-g010">Figure 10</xref> shows successful performance on the 2-bit environment task with the black output chemical only showing high concentration in the third condition.</p>
      <sec id="s2a">
        <title>Network structure</title>
        <p>Having evolved approximately 10 networks capable of solving each task, we ask, how do they work? The evolutionary algorithm permitted increases or decreases in the number of chemical species and the number of chemical reactions, see Methods. The smallest evolved network required only two reactions, but the typical number of reactions in an evolved network was 12 (mean 11.9, median 12). A greedy pruning algorithm applied to the networks revealed that most of these reactions were superfluous; typically only 5 reactions (mean 4.7, median 5) were necessary to achieve a fitness score within 10% of the entire network's fitness. The numbers given are for all tasks in aggregate; statistics for individual tasks are not very different. Although we did not select explicitly for simplicity, smaller networks emerged in the simulations. <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref> below shows the core network motifs that were evolved for associative learning, identified after pruning.</p>
        <fig id="pcbi-1002739-g011" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g011</object-id>
          <label>Figure 11</label>
          <caption>
            <title>Chemical motifs for associative learning.</title>
            <p>(A) A long-term memory chemical could be identified in most networks: this reacted with the stimulus to produce output, and was generated only in the “associated” environment. <italic>Top.</italic> A simple reversible reaction in which stimulus+slow decaying memory-trace molecule produce output, and the control molecule regenerates the memory molecule for reuse. <italic>Bottom.</italic> Two <italic>almost</italic> irreversible reactions allow an improvement on the previous motif because here the decay rate of the output chemical is made independent of the decay rate of a short-term memory molecule, allowing decoupling of the control from the output molecule. (B) In several networks the overlap between the signal and control initialized long-term memory production. Again, a single reversible reaction but with S and C reacting together. It works because the control chemical decays quickly but the stimulus molecule decays slowly. Therefore stimulus and control molecules only co-occur when control follows stimulus, and not when stimulus follows control.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g011" position="float" xlink:type="simple"/>
        </fig>
        <p>The second motif (<xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11A</xref>, bottom) is the most commonly evolved solution. It appeared as a solution to all the above tasks. We analyse that in detail below in a case where it evolved in the best network capable of solving the AB-BA task (the network is described in detail in Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref> part 3). The task in this case is to produce output (species 11) when control pulses follow stimulus pulses (S→C), but not to produce output chemical O when control pulses precede stimulus pulses (C→S), see <xref ref-type="fig" rid="pcbi-1002739-g012">Figure 12</xref>.</p>
        <fig id="pcbi-1002739-g012" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g012</object-id>
          <label>Figure 12</label>
          <caption>
            <title>Reactions in the best performing chemical network in the AB-BA task.</title>
            <p>All reactions are reversible, arrowheads only indicate the thermodynamically favoured direction. S- stimulus, C- control, O- output, STM- short-time memory-trace, LTM- long-term memory-trace. All species decay and there is a low-rate inflow of molecule ‘001’. Blue and red lines correspond to the motifs on <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref>. In the environment where stimulus pulses are followed by control pulses, output and a short-term memory chemical are produced in response to the stimulus from the long-term memory chemical; then, when the control pulse arrives, the memory chemical is regenerated from the short-term memory chemical.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g012" position="float" xlink:type="simple"/>
        </fig>
        <p>In the S→C environment, a slowly decaying long-term memory chemical LTM (chemical species ‘001’) reacts with the stimulus S to produce output O and a fairly rapidly decaying short term memory chemical STM (0001). Thus, output is produced in response to the stimulus when the memory chemical is present:<disp-formula id="pcbi.1002739.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e001" xlink:type="simple"/><label>(1)</label></disp-formula>When the control pulse C occurs, it converts the short-term memory chemical back into the long-term memory molecule, allowing the LTM molecule to be reused again in the next pulse pair.<disp-formula id="pcbi.1002739.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e002" xlink:type="simple"/><label>(2)</label></disp-formula>Now consider how the same network must behave quite differently in the C→S condition. Here C occurs before S so there is no STM molecule for C to react with to produce LTM. This means there is no LTM molecule for S to react with to produce output. Instead C readily disintegrates:<disp-formula id="pcbi.1002739.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e003" xlink:type="simple"/><label>(3)</label></disp-formula>and the disintegration product reacts with the output molecule thus removing any output that might be produced in response to the stimulus that follows:<disp-formula id="pcbi.1002739.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e004" xlink:type="simple"/><label>(4)</label></disp-formula>Whilst reactions (3) and (4) are not shown in <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref>, we note that such ‘extra’ reactions are typical additions to the core motifs that evolved. Each evolved network contains multiple such extra adaptive reactions that help in various ways to control the dynamics of the system.</p>
        <p>This hypothesis for the mechanism of learning was tested by modifying the concentration of the long-term and short-term memory chemicals by manipulating their inflow and decay rates and observing the response to stimulus pulses. We found that, as expected, the LTM and STM molecules determined the magnitude of output produced (<xref ref-type="fig" rid="pcbi-1002739-g013">Figure 13</xref>). Remarkably, this explanation can be re-interpreted in the light of Bayesian posteriors, i.e. ‘beliefs’ that the network has about which environment it is likely to be in, according to the information provided so far by the environment. To do this, we interpreted the internal state of the network as encoding a Bayesian posterior, by fitting a regression model from the chemical concentrations of the network at each point in time to the ideal Bayesian posterior of being in the associated environment given the sensory history encountered so far. If it is possible to fit such a regression model it means that a linear combination of chemical species concentrations encodes in a sense a near-optimal ‘belief’ about which environment the network is in. We found it was indeed possible to fit such a linear model for the above network, see <xref ref-type="table" rid="pcbi-1002739-t002">Table 2</xref>. Furthermore, the parameters of this model must correspond to each species' role in learning. Positive numbers signify chemical species that are typical for the S→C environment, while negative numbers indicate that these chemicals are more abundant in the C→S environment. As expected, the largest positive posteriors belong to the memory chemicals, and, of course, to the output chemical (reactions 1–2); while large negative numbers indicate the disintegration product and the waste chemical (reactions 3–4).</p>
        <fig id="pcbi-1002739-g013" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g013</object-id>
          <label>Figure 13</label>
          <caption>
            <title>Manipulating the chemical network.</title>
            <p>Black solid line shows output concentration; blue solid line shows stimulus and purple solid line shows control concentrations. Dotted lines show intermediate chemical concentrations. Triangles show input boluses. In the S→C environment (A) high decay of any of the memory chemicals diminish the response; in the C→S environment (B), high inflow of any of the memory chemicals is enough to produce output.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g013" position="float" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002739-t002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.t002</object-id>
          <label>Table 2</label>
          <caption>
            <title>Coefficients of the regression model that multiply each chemical species concentration to obtain the Bayesian posterior prediction for the best performing chemical network in the AB-BA task.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002739-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.t002" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td colspan="2" align="left" rowspan="1">S→C environment</td>
                  <td colspan="2" align="left" rowspan="1">C→S environment</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Chemical</td>
                  <td align="left" rowspan="1" colspan="1">Weight</td>
                  <td align="left" rowspan="1" colspan="1">Chemical</td>
                  <td align="left" rowspan="1" colspan="1">Weight</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">011</td>
                  <td align="left" rowspan="1" colspan="1">0.03</td>
                  <td align="left" rowspan="1" colspan="1">111</td>
                  <td align="left" rowspan="1" colspan="1">−1.32</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">001</td>
                  <td align="left" rowspan="1" colspan="1">1.8</td>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="left" rowspan="1" colspan="1">−2.38</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">11</td>
                  <td align="left" rowspan="1" colspan="1">2.57</td>
                  <td align="left" rowspan="1" colspan="1">01</td>
                  <td align="left" rowspan="1" colspan="1">−0.14</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0001</td>
                  <td align="left" rowspan="1" colspan="1">1.5</td>
                  <td align="left" rowspan="1" colspan="1"/>
                  <td align="left" rowspan="1" colspan="1"/>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0.81</td>
                  <td align="left" rowspan="1" colspan="1"/>
                  <td align="left" rowspan="1" colspan="1"/>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>Positive numbers indicate species that are more likely to have high concentration in the S→C environments, while negative numbers belong to species that are more prevalent in the C→S environment. The magnitude of the weight relate to the significance of the chemical. The Bayesian interpretation is consistent with our explanation for the learning mechanism (see text).</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>Many of the evolved networks used the motif described above. There were a few more general features that repeatedly appeared for all tasks. For example, the input (stimulus, control) and output chemicals' concentration typically decreased quickly, either by spontaneous decay or by reactions that converted them to waste products/memory chemicals. A long-term memory chemical could be identified in most networks: this reacted with the stimulus to produce output, and was generated only in the S→C environment.</p>
        <p>Apart from these features, the chemical background of learning was diverse and highly specific to the task in question. In the clocked and noisy clocked tasks only the S→C environment contained control pulses, and this was habitually exploited by converting the control directly to the long-term memory chemical (network not shown in <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref>). In the non-clocked task, many of the networks used the fact that the output needs to be low after the control arrives. The signal in itself was converted to output, while control removed output. This resulted in a dynamics where in the S→C environment, control removed the output of the previous signal; in the case of randomly distributed control pulses, there was no output available when control was added, so, it inhibited the output of the following signal. The AB-BA task was a very special problem and the networks evolved to solve it were even more diverse than usual. In several cases the control was used to inhibit output production, as in the C→S environment it reliably preceded the signal. As the 2-bit task included more environments, it was more difficult for the networks to use “tricks”, and they mostly used the mechanisms depicted on <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref>. We have evolved a few networks to be able to solve all tasks and the tendency towards simplicity was even clearer in them: they invariably used the most typical mechanism (<xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11A</xref>, bottom) that we have analysed above.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <sec id="s3a">
        <title>Bayesian analysis</title>
        <p>Bayesian statistics provides a valuable framework, not just for statistical analysis of data, but for conceptualising how physical systems can encode models of their environment and update those models. The central concept in Bayesian statistics is that a “belief” can be modelled as a probability distribution; the rational way to modify the belief in response to evidence can then be formally codified. In order to incorporate cumulative evidence rationally into a model of the environment, it is sufficient to apply Bayes' rule repeatedly over time, with the posterior probability after each observation becoming the prior probability for the next observation, see <xref ref-type="bibr" rid="pcbi.1002739-Barber1">[24]</xref> for an overview. This process is known as iterated (or recursive) Bayesian inference.</p>
        <p>The typical application of Bayesian statistics would (in effect) be for the experimenter to apply Bayesian inference to their own beliefs, beginning with some probabilistic belief about the system and refining it by the observation of evidence. We turn this on its head by considering, if the system <italic>itself</italic> were a rational observer, what “beliefs” it should have regarding its environment and how it should update them in response to evidence. A similar approach to ours can be seen in <xref ref-type="bibr" rid="pcbi.1002739-Libby1">[25]</xref>. We found that a Bayesian analysis provides insight into understanding network function. Note that there was no explicit pressure on the networks to perform Bayesian reasoning. However, achieving a high fitness during evolution required the networks to incorporate and integrate information over time. Iterated Bayesian inference is the formal ideal of the process of integrating cumulative evidence; hence, we have a theoretical motivation for interpreting the network dynamics in Bayesian terms.</p>
        <p>We attributed “beliefs” to the networks by analytically deriving the Bayesian beliefs (posteriors) of an ideal observer in a given task (over a variety of time steps and environments), and fitting a regression model from the network's state to this ideal belief. (We use a logistic regression model as the natural analogue of a linear model for a range bounded between 0 and 1.) Hence, we determined the maximum extent to which the network's state can be said to encode the correct posterior in a simple form. For comparison purposes, we also performed this procedure on networks that were not evolved on the task in question. This means that the “belief” attributed to a network depended on the task it was being observed on: “belief” in this context really means “most generous attribution of belief given the task”.</p>
        <p>The mean correlation between the fitted logistic regression model and the analytic posteriors is extremely high (0.97–0.98) for the highest-fitness evolved networks on both the noisy clocked association task and the AB-BA task (<xref ref-type="fig" rid="pcbi-1002739-g014">Figure 14</xref>). The information required to perform the noisy clocked task is relatively easy to accumulate in a detectable form: for random networks, the mean model/posterior correlation is fairly high (0.82). For the AB-BA task, which requires accumulating more subtle information, the quality of fit of the regression model for random networks is very low (0.06) (<xref ref-type="fig" rid="pcbi-1002739-g014">Figure 14</xref>). <xref ref-type="fig" rid="pcbi-1002739-g015">Figure 15</xref> shows the dynamics of an evolved network's best “belief” (the output of the regression model) over time for a particular lifetime, compared to the ideal rational belief (the posterior probability). Interestingly, the network evolved on the “2-bit environment” task demonstrated information capture on both the noisy clocked task (rho = 0.83) and the AB-BA task (rho = 0.73). See Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref>, part 4 for other example networks, including networks that were evolved on a different task to the one they are being tested on.</p>
        <fig id="pcbi-1002739-g014" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g014</object-id>
          <label>Figure 14</label>
          <caption>
            <title>Boxplot showing goodness of fit of a logistic regression model to the ideal Bayesian posteriors in 30 test environments for the noisy clocked and AB-BA tasks.</title>
            <p>The degree to which a network's state encodes the Bayesian posterior via a logistic model is shown for a single evolved network and 30 random networks.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g014" position="float" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002739-g015" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g015</object-id>
          <label>Figure 15</label>
          <caption>
            <title>Best “belief” change over time in an evolved network for two paired lifetime runs of the AB-BA task.</title>
            <p>Upper: network output. Lower: ideal Bayesian posterior (dotted line) and attributed network “belief” based on regression model from concentration values (solid line). Vertical bars illustrate input event timing: dark grey for C→S events and light grey for S→C.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g015" position="float" xlink:type="simple"/>
        </fig>
        <p>The process of Bayesian inference is characterised by the incorporation of relevant information into a system's internal state. This does not constrain the way in which a Bayesian posterior is encoded into the state of a system; the encoding in principle could be arbitrarily complex. However, our empirical results for the evolved networks indicate that the existence of an encoding can be demonstrated by a simple regression model.</p>
        <p>It is worth observing that just because a system's state contains the relevant information to perform a task, this does not necessarily mean that the system uses that information appropriately. For our noisy clocked task, the dynamics of a randomly constituted network usually encode the relevant information for task performance in a nearly linear way, whereas random networks have a poor fitness performance on the task. This is because in the artificial environment for that task, the overall rate of control pulses differs in the two different experimental conditions. To a first approximation, we can regard the two experimental conditions as providing constant driving inputs to the system, but at different rates. Hence, if a system's gross dynamics depend on the rates of control pulse inputs (which will be true for the majority of systems), then observing the system's state after interacting with one or other of our task environments will readily reveal which environment the system was exposed to. We will see below that this issue does not apply to the more complex AB-BA task that requires genuine sensitivity to stimulus pairing (see <xref ref-type="table" rid="pcbi-1002739-t001">Table 1</xref> for a comparison of the informational requirements in the noisy clocked task and the AB-BA tasks).</p>
        <p>There are important parallels here to <italic>liquid state machines</italic> <xref ref-type="bibr" rid="pcbi.1002739-Maass1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Maass2">[27]</xref> and other <italic>reservoir machines</italic> <xref ref-type="bibr" rid="pcbi.1002739-Jaeger1">[28]</xref> and to <italic>random projections</italic> <xref ref-type="bibr" rid="pcbi.1002739-Bingham1">[29]</xref> in machine learning: information capture is not necessarily the hardest part of information processing, and randomly constituted systems can often accumulate information in a usable fashion. So, the random networks store information about the rate of control pulses in the environment (although not as much information as a network evolved for the task). That information can be extracted by an observer using a simple regression model, similar to reservoir machine and random projection learning. However, the random networks do not incorporate the machinery to translate the stored information into an appropriate response: a high output following a stimulus pulse when control pulses have occurred at a high rate in the past, and a low output otherwise.</p>
        <p>By contrast, we determine empirically that the AB-BA task produces very different information dynamics to the noisy clocked task. In the AB-BA task, the overall rate of control (and stimulus) pulses is identical in the two different task environments. While random networks can be assigned a logistic-model Bayesian interpretation for the first task (i.e. a regression model can be fitted to map from the network state to the current optimal Bayesian posterior), the same is not true for the AB-BA task (see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref>, part 4), where only the evolved networks have a good logistic-model Bayesian interpretation. Note that to distinguish the AB-BA environments, the network must respond differently to a C pulse followed shortly by a S pulse than a S pulse followed by a C pulse. The information necessary to distinguish the environments optimally is the relative number of CS versus SC pulses.</p>
        <p>A nervous system is not necessary for learning. We have shown that associative learning mechanisms implemented by well-mixed chemical reactions can be discovered by simulated evolution. What differences in principle, then, are there between neurons and chemicals? The key difference between learning in neuronal network and learning in our chemical networks is that in neuronal systems generic learning mechanisms exist that are present at each synapse, irrespective of the particular identity of the pre- and post-synaptic neurons. For example, spike-time-dependent plasticity (STDP) can be found between many neurons. This is possible because neurons share the same genome, and this permits each neuron to express the molecular machinery required for plasticity. On top of this, specificity can be achieved through line labelling, i.e. it is the physical pathway from stimulus to neuron A to neuron B etc. that has meaning, and conveys reference. The capacity to associate arbitrary events X and Y arises when a plastic synapse exists between neurons that represent X and neurons that represent Y.</p>
        <p>In our chemical networks, however, there is no modular distinction between chemical species that represent events and the chemical reactions that implement learning. The chemical network for associating X and Y by forming memory-trace M cannot work separately to associate P and Q because of two reasons: (i) the reactor is well mixed and the memory-trace M for X and Y will interfere with the memory-trace M for P and Q (ii) the molecule M will react with X and Y but it cannot without modification react with arbitrary P and Q. In the neural system neither of these constraints exists.</p>
        <p>This has important consequences on the scaling properties of neural or chemical systems for associative learning. Suppose that the system needs to be able to learn three independent possible associations (say, A→C, B→C and A→D). The weight (strength) of each association needs to be represented independently in the network, and an associative mechanism implemented to update each weight.</p>
        <p>In the neural system this is easy; the associative mechanism is a set of molecules that are expressed in each synapse that implements Hebb's rule or some variant of that rule, which states that events that co-occur have a higher probability of co-occurring in the future. In neuronal systems the weights of the associations are the synaptic strengths. Each neural connection contains the molecular capacity to implement Hebb's rule specifically between distinct neurons. In the chemical system, however, each associative mechanism will be a different chemical pathway, and the pathways will need to be functionally similar while involving species whose chemical properties are distinct (since if the species are too similar, there will be crosstalk between the pathways). In essence, it seems plausible that the chemical system will have to re-implement associative learning independently for every possible association.</p>
        <p>We have described chemical networks in this paper that can learn to associate one stimulus with another stimulus. An important qualifier here is that they do not display generic associative learning: the two stimuli that can be associated are genetically specified. Of course, more sophisticated cellular systems such as genetic regulatory networks may be able to overcome the problems we have described. Also, the learning is not independent of timing, but instead the ability of an evolved network to undertake associative learning is greatest for environments where the period between successive stimulus-control pairs resembles that period encountered during evolution, see Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref>, part 5).</p>
        <p>We used <italic>in silico</italic> evolution to find small chemical networks capable of carrying out various associative learning tasks. It is often the case that evolution finds solutions that are much more concise, elegant, and parsimonious than would be produced by deliberative cognition. In fact, the simplest chemical network still capable of associative learning consisted of only two chemical reactions. This confirms that there is no reason in principle that associative learning within a lifetime should be confined to multicellular organisms.</p>
        <p>So why is the experimental evidence of associative learning in single cells to date equivocal? We are only aware of one experiment that addressed this question <xref ref-type="bibr" rid="pcbi.1002739-Hennessey1">[30]</xref>. Todd Hennessey showed that aversive classical conditioning occurs in Paramecia. He trained a single paramecium to avoid an electric shock by learning that vibration precedes it. The mechanisms underlying such learning are not known, although it seems possible that voltage gated Calcium channels <xref ref-type="bibr" rid="pcbi.1002739-Eckert1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Dunlap1">[32]</xref> are involved, perhaps with adenylate cyclase acting as a coincidence detector with cAMP dependent state changes mediating memory as in Aplysia <xref ref-type="bibr" rid="pcbi.1002739-Gustin1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Walters1">[34]</xref>. Similar studies have indicated that other single-celled organisms may have the capacity to learn to associate light and electric shocks <xref ref-type="bibr" rid="pcbi.1002739-Bergstrm1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002739-Bergstrm2">[36]</xref> although a recent study on individual human immune cells showed habituation but no conditioning <xref ref-type="bibr" rid="pcbi.1002739-Nilsonne1">[37]</xref>. Notice that the task of learning a contingency within a lifetime is entirely different from evolving to respond under an evolutionary regularity that B will regularly follow A in all environments. Whilst there was a recent report that such behaviour is observed in bacteria which anticipate the decrease in oxygen following increase in temperature, these bacteria did not learn to anticipate but rather they evolved to anticipate <xref ref-type="bibr" rid="pcbi.1002739-Tagkopoulos1">[38]</xref>. Often this critical distinction is not made, resulting in confusion between evolution and learning <xref ref-type="bibr" rid="pcbi.1002739-Dale1">[39]</xref>. To see the difference, note that no bacterium in the above experiment could learn within a lifetime that in some environments increased temperature predicts increased oxygen, whereas in other environments decreased temperature predicts increased oxygen. This association was not learned by a single bacterium, instead, it is an association that was discovered by evolutionary search by populations of bacteria. The very ease with which populations of bacteria and yeast can evolve to anticipate environmental changes in laboratory evolution experiments suggests that it may simply not have been necessary for individual single celled organisms to learn to anticipate within a lifetime <xref ref-type="bibr" rid="pcbi.1002739-Mitchell1">[40]</xref>.</p>
        <p>An important implication of our work is that the associative mechanisms we have described may be active during development in cells within a multicellular organism. It will be of interest to use bioinformatics to examine whether the motifs in <xref ref-type="fig" rid="pcbi-1002739-g011">Figure 11</xref> can be found in regulatory networks involved in development. This paper allows us to re-examine the possible function of simple chemical motifs within an associative learning framework.</p>
      </sec>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Artificial chemistry</title>
        <p>In order to enforce conservation of atomic mass in the networks' reactions, we used a combinatorial abstract chemistry for the networks. Each simulated chemical species had a “formula” consisting of a string of digits representing chemical “building blocks”, and reactions were constrained to conserve building blocks. These constraints were modelled using three different abstract combinatorial chemistries: An “aggregate” chemistry, where only the number of digits (and not their sequence) determined the species' identity, somewhat resembling inorganic chemistry with atoms as building blocks. Any interchange of building blocks was allowed to happen in reactions. A “rearrangement” chemistry, where the sequence of digits characterized species, somewhat resembling organic chemistry with atomic groups as building blocks. Any interchange of building blocks was allowed to happen in reactions. A “polymer” chemistry, where only ligation and cleavage reactions could happen among chemical species, resembling polymer reactions with monomers as building blocks.</p>
        <p>Simulations of a simple aggregation chemistry provided chemical networks with the highest fitness (Supporting Information <xref ref-type="supplementary-material" rid="pcbi.1002739.s001">TextS1</xref>, part 6), thus, results in the main text refer to this particular chemistry. Reactions were modelled reversibly. We incorporated further thermodynamic constraints by assigning “free energy” values to chemical species; these constrained the ratios between forward and reverse reaction rates. Each network received an inflow of one particular chemical type (“food”), and every chemical species exhibited first-order decay, as expected in a flow reactor scenario. Note that although all the parameters of the reaction networks – chemical species, chemical reactions, free energy values, inflow rate of food and species decay rates – were allowed to change during the evolutionary runs, each individual network had its own fixed chemistry that stayed the same during the learning trials. Therefore the difference between chemical networks in the unassociated and associated environments could only be induced by the different history of input boluses; these must have modified the state of the network (the concentration of different chemicals) so that it showed different behaviour when presented with the stimulus chemical.</p>
      </sec>
      <sec id="s4b">
        <title>Encoding</title>
        <p>Networks consisted of a number of chemicals and reactions, the relevant characteristics of which were encoded genetically. See <xref ref-type="fig" rid="pcbi-1002739-g016">Figure 16</xref> for illustration.</p>
        <fig id="pcbi-1002739-g016" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.g016</object-id>
          <label>Figure 16</label>
          <caption>
            <title>Network genotype and its meaning.</title>
            <p>For explanation see text.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.g016" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4c">
        <title>Chemicals</title>
        <p>Each abstract chemical species was associated with a number of real-valued parameters: A chemical “potential”, which affected the thermodynamics of the system, an initial concentration, a spontaneous decay rate (conceptualised as decay to inert waste products), an inflow rate if this species was chosen as the network “food” (see below). In addition, chemical species were assigned a binary “formula” string, which constrained how different species could combine (see “chemistry” section).</p>
      </sec>
      <sec id="s4d">
        <title>Reactions</title>
        <p>Reactions were represented as a list of one or two “Left Hand Side” (LHS) species, a list of one or two “Right Hand Side” (RHS) species, and a real- valued “favoured rate constant” (see below). The variation operators used in evolution guaranteed that reactions conserved mass and compositional elements (see below). Note that the intrinsically favoured direction for the reaction was not determined by the reaction's encoding but by the chemical potential values of the species involved. The “favoured rate constant” parameter of the reaction determined the rate constant in the favoured direction; the rate constant in the non-favoured direction was determined by the chemical potential values of the species involved.</p>
      </sec>
      <sec id="s4e">
        <title>Input and output</title>
        <p>The choice of which chemical species the network used as input, output and “food” were under evolutionary control. Part of the network encoding was an ordered list of species: the first species in the list functioned as inputs; the next species as output; the next species as “food”; and the remainder had no special environmental significance, see <xref ref-type="table" rid="pcbi-1002739-t003">Table 3</xref>.</p>
        <table-wrap id="pcbi-1002739-t003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.t003</object-id>
          <label>Table 3</label>
          <caption>
            <title>Allowable range, initialisation range, and description for real-valued network parameters.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002739-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.t003" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Chemical Species Parameter</td>
                  <td align="left" rowspan="1" colspan="1">Range</td>
                  <td align="left" rowspan="1" colspan="1">Description</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Chemical potential</td>
                  <td align="left" rowspan="1" colspan="1">0–7.5 units</td>
                  <td align="left" rowspan="1" colspan="1">Parameter affecting reaction rate constants</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Initial concentration</td>
                  <td align="left" rowspan="1" colspan="1">0–5 units (initialised 0–2)</td>
                  <td align="left" rowspan="1" colspan="1">Concentration of species at the start of a protocol simulation</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Inflow (if food)</td>
                  <td align="left" rowspan="1" colspan="1">0–5 units (initialised 0–1)</td>
                  <td align="left" rowspan="1" colspan="1">Inflow rate of the species if selected as the “food” species</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Spontaneous decay</td>
                  <td align="left" rowspan="1" colspan="1">0–10 units (initialised 0–1)</td>
                  <td align="left" rowspan="1" colspan="1">Decay rate of the species</td>
                </tr>
              </tbody>
            </table>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Reaction Parameter</td>
                  <td align="left" rowspan="1" colspan="1">Range</td>
                  <td align="left" rowspan="1" colspan="1">Description</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Favoured rate constant</td>
                  <td align="left" rowspan="1" colspan="1">0–60 units (initialised 0–0.1)</td>
                  <td align="left" rowspan="1" colspan="1">Rate constant of the reaction in the thermodynamically favoured direction (determined by potentials of reactants).</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
      </sec>
      <sec id="s4f">
        <title>Variation</title>
        <p>Network mutations were implemented as follows, based on a mutation rate sigma: All real-valued parameters were mutated by Gaussian noise, with reflection at the upper and lower parameter limits. The standard deviation of the noise was scaled by the product of sigma with the absolute size of the allowable range for that parameter. With probability sigma * 5, the program attempted to add a random new reaction to the network (see “adding new reactions”). With probability sigma * 5, a uniformly chosen reaction was deleted from the network. With probability sigma, two elements of the input-output list for the network were randomly swapped (most of the time, this involved swapping “non-special” elements and had no functional effect).</p>
      </sec>
      <sec id="s4g">
        <title>Adding new reactions</title>
        <p>When a mutation called for adding a new reaction to the network, one of the following three possibilities was chosen uniformly:</p>
        <list list-type="order">
          <list-item>
            <p>A reaction decomposing an existing chemical species into two molecules. If this was impossible (i.e. the chosen species had a “1” or “0” formula), no reaction was added</p>
          </list-item>
          <list-item>
            <p>A reaction composing two existing chemical species into a single molecule. If this would produce “too long” a molecule, a reaction of the third type was generated instead.</p>
          </list-item>
          <list-item>
            <p>A reaction rearranging two existing chemical species into two different species. This was modelled as composition followed by decomposition.</p>
          </list-item>
        </list>
        <p>In each case, the existing species were chosen uniformly and formulas for the reaction products were generated according to the current chemistry (see “chemistries”). If a formula was generated in this way that did not match a species already in the network, a new species was generated with that formula and added to the network. When a new reaction was added to the network, its “favoured rate constant” parameter was initialised to a low value (uniformly in the range [0, 0.1]) to allow for relatively neutral structural mutations.</p>
      </sec>
      <sec id="s4h">
        <title>Chemistries</title>
        <p>Each chemical species in a reaction network was given a binary string “formula” which constrained what products it could form with other species. Reactions were always constrained so that the total number of 0s on the reaction LHS was the same as the total number of 0s on the RHS, and similarly for 1s. In addition, we modelled three different string “chemistries”, each with different compositional rules, see <xref ref-type="table" rid="pcbi-1002739-t004">Table 4</xref>.</p>
        <list list-type="order">
          <list-item>
            <p>A “polymer” chemistry, where composite formulas involved only concatenation, e.g. 01+00↔0100.</p>
          </list-item>
          <list-item>
            <p>A “rearrangement” chemistry, where composite formulas could have their binary elements in any order, e.g. 01+00↔0001 or 0010 or 0100 or 1000. Composition here was implemented as concatenation followed by fair shuffling of string characters.</p>
          </list-item>
          <list-item>
            <p>An “agglomeration” chemistry, where only the total number of 0 s and 1 s in a formula (and not the order of them) distinguished different species, e.g. 01+011↔00111. Composition here was implemented as concatenation followed by lexicographic sorting of string characters.</p>
          </list-item>
        </list>
        <table-wrap id="pcbi-1002739-t004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002739.t004</object-id>
          <label>Table 4</label>
          <caption>
            <title>Composition and decomposition operators for three different types of network chemistry.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002739-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002739.t004" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Chemistry</td>
                  <td align="left" rowspan="1" colspan="1">Composition</td>
                  <td align="left" rowspan="1" colspan="1">Decomposition</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Polymer</td>
                  <td align="left" rowspan="1" colspan="1">“Gluing” one string to the end of the other (<italic>concatenation</italic>), e.g. 011+01→01101</td>
                  <td align="left" rowspan="1" colspan="1">String division at a uniformly chosen location guaranteed to respect maximum string length of products (<italic>splitting</italic>), e.g. 0110101→0110+101</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Rearrangement</td>
                  <td align="left" rowspan="1" colspan="1">Concatenation, followed by order randomisation of characters (<italic>shuffling</italic>), e.g. 011+01 (via 01101)→10011</td>
                  <td align="left" rowspan="1" colspan="1">Splitting of shuffled string, e.g. 0110101 (via 1011001)→101+1001</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Aggregation</td>
                  <td align="left" rowspan="1" colspan="1">Concatenation, followed by lexicographic reordering of characters in product string (<italic>sorting</italic>), e.g. 011+01 (via 01101)→00111</td>
                  <td align="left" rowspan="1" colspan="1">Splitting of shuffled string, followed by sorting of each product string. e.g. 0001111 (via 1011001) (via 101+1001)→011+0011</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
      </sec>
      <sec id="s4i">
        <title>Initialising networks</title>
        <p>Networks were initialised as follows. A small number of “seed” chemicals (by default, 4) with distinct formulas of length 3 were added to the network. New chemical species, whether generated at initialisation or due to adding a new reaction to the network during initialisation or mutation, were initialised with uniformly random parameters in the following ranges: potential [0–7.5], initial concentration [0–2], food inflow [0–1], decay [0–1]. The function to add a new reaction was called 20 times, thereby adding an unpredictable number of new chemicals to the network. New reactions, whether generated during initialisation or mutation, were initialised with a uniformly random “favoured reaction constant” in the range [0–0.1]. The input-output list for the network was shuffled fairly.</p>
      </sec>
      <sec id="s4j">
        <title>Evolution</title>
        <p>The networks were evolved using a non-generational genetic algorithm (GA) similar to the Microbial GA <xref ref-type="bibr" rid="pcbi.1002739-Harvey1">[41]</xref>. A genetic algorithm is the natural selection algorithm run in a computer <xref ref-type="bibr" rid="pcbi.1002739-Holland1">[10]</xref>, specifically it is artificial selection in which an explicit fitness function (phenotypic target) is defined, rather than allowing fitness to emerge as the result of ecological interactions. In our case the fitness function rewards chemical networks capable of the kind of associative learning we require. The basic algorithm was as follows:</p>
        <list list-type="simple">
          <list-item>
            <p>Initialise a population with a given number of networks</p>
          </list-item>
          <list-item>
            <p>For a fixed number of iterations,</p>
          </list-item>
          <list-item>
            <p>Pick two different networks from the population (for spatial evolution, choose two neighbours)</p>
          </list-item>
          <list-item>
            <p>Evaluate both networks</p>
          </list-item>
          <list-item>
            <p>Replace the worse-performing network with a mutated copy of the better-performing network</p>
          </list-item>
        </list>
      </sec>
      <sec id="s4k">
        <title>Simulation</title>
        <p>All reactions were modelled using reversible deterministic mass action kinetics (apart from the implicit decay reactions which are irreversible). It is clearest to explain this scheme by example.</p>
        <p>A single reversible reaction can be conceptually split into two parts, so that</p>
        <list list-type="simple">
          <list-item>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e005" xlink:type="simple"/></inline-formula> (with forward rate constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e006" xlink:type="simple"/></inline-formula> and reverse rate constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e007" xlink:type="simple"/></inline-formula>)</p>
          </list-item>
        </list>
        <p>is conceptually equivalent to the composition of two reactions</p>
        <list list-type="simple">
          <list-item>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e008" xlink:type="simple"/></inline-formula> (with rate constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e009" xlink:type="simple"/></inline-formula>)</p>
          </list-item>
        </list>
        <p>and</p>
        <list list-type="simple">
          <list-item>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e010" xlink:type="simple"/></inline-formula> (with rate constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e011" xlink:type="simple"/></inline-formula>)</p>
          </list-item>
        </list>
        <p>The rate at which a reaction takes place, in our simulation, is set equal to the product of the concentrations of those species on its left-hand side, multiplied by its rate constant. The reaction consumes its reactants at this rate and generates its products at this rate. The overall rate of change of a species' concentration due to explicitly-modelled reactions is equal to the sum of the rates at which it is generated (over all reactions) minus the sum of the rates at which it is consumed (over all reactions). Spontaneous decay (at a rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e012" xlink:type="simple"/></inline-formula> for chemical X) contributes an additional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e013" xlink:type="simple"/></inline-formula> term to this sum, and inflow (at rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e014" xlink:type="simple"/></inline-formula> for the food chemical F) contributes an additional <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e015" xlink:type="simple"/></inline-formula> term to F's rate of change. Hence, a system consisting only of the reaction<disp-formula id="pcbi.1002739.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e016" xlink:type="simple"/></disp-formula>(with A as the food chemical) has the following differential equations:<disp-formula id="pcbi.1002739.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e017" xlink:type="simple"/></disp-formula>For computational efficiency, simulations during evolution used Euler integration with a step size of 0.01 time units. Input boluses were modelled as discontinuous jumps in concentration at the given time steps. These simulations were qualitatively validated after evolution using a variable step-size Runge-Kutta ODE solver.</p>
      </sec>
      <sec id="s4l">
        <title>Task protocols</title>
        <p>Networks were simulated on chemical protocols, with each protocol consisting of a time series of input boluses, and a time series of target values for the network output. Note that for most time steps, the input bolus values were zero and the target output values were “don't care”. The exact details of the protocol inputs and targets varied from task to task.</p>
      </sec>
      <sec id="s4m">
        <title>Fitness evaluation</title>
        <p>For every task, networks were simulated on a number of protocols, and the (instantaneous) concentration of the designated network output chemical compared to the protocol target for every time step. The fitness of a network was set equal to the negative mean square difference between these two quantities averaged over all protocols and all time steps (ignoring time steps where a “don't care” target was specified). In order to provide a reliable fitness comparison, when two networks were chosen for competition during evolution, they were evaluated on the same set of protocols. Additionally, the protocols for different experimental conditions within the same task were deliberately matched to be similar, so that network response to the experimental condition could be measured as directly as possible.</p>
      </sec>
      <sec id="s4n">
        <title>Task descriptions</title>
        <p>Initial experiments indicated that randomly generating protocols during evolution results in very noisy fitness comparisons, with little fitness gradient for evolution to climb. To avoid this problem, for each task we generated fixed “training data” and saved it to file. Networks were evaluated during evolution on their performance on the training data set. For most tasks, the training data set was a file consisting of 10 randomly generated protocols. A number of tasks were devised requiring the detection of different environmental features by the networks. Some of these tasks were “clocked”, i.e. pulses were constrained to only occur at predetermined regular “clock tick” times, and some were not.</p>
      </sec>
      <sec id="s4o">
        <title>Clocked task</title>
        <p>This task constrained B boluses to a regular “clock tick” schedule every 100 time steps and had two experimental conditions. There was only a 0.5 probability of a chemical B bolus on a given clock tick. In the “associated” condition, a chemical B bolus was always followed 20 time steps later by a chemical A bolus. In the “unassociated” condition, chemical A boluses never occurred. A single protocol featured both experimental conditions, with identical B boluses in each condition. The desired behaviour for the network was: upon receiving a pulse of chemical B, output either zero (in the “unassociated” condition) or one (in the “associated” condition) for 20 time steps afterwards.</p>
      </sec>
      <sec id="s4p">
        <title>Clocked task with noise</title>
        <p>This was identical to the previously described task except that there was a small (p = 0.1) probability of “noise” occurring at each time step with a chemical B bolus. Noise consisted of a B bolus being followed by an A bolus in the “unassociated” condition or a B bolus followed by no A bolus in the “associated” condition. Within a single protocol, the occurrence of noise was matched between experimental conditions.</p>
      </sec>
      <sec id="s4q">
        <title>Non-clocked task</title>
        <p>This task had two experimental conditions and involved boluses at random intervals. In both conditions, pulses of chemical B occurred at random intervals uniformly in the range [100, 300]. In the first (“associated”) condition, a pulse of chemical B was followed shortly afterwards (20 time steps) by a pulse of chemical A. In the second (“unassociated”) condition, pulses of chemical A occurred independently of B, at random intervals uniformly in the range [100, 300]. Within a single protocol, pulses of chemical B were identical.</p>
      </sec>
      <sec id="s4r">
        <title>AB-BA task</title>
        <p>This task, featuring two experimental conditions, was specifically designed to involve a non-trivial accumulation of information. Within this task, input “events” occurred randomly at a low rate (0.025 per time step) with a refractory period of 50 time steps between events, over a total period of 2000 time steps. Each event consisted of either a pulse of chemical A followed closely (20 time steps later) by a pulse of chemical B, or vice versa. In the first experimental condition (“A→B”), events were 75% likely to be “A→B” pulses and 25% likely to be “B→A” pulses, and vice versa for the second (“B→A”) experimental condition. The desired output behaviour was to respond to a “B” pulse with a low output in “A→B” environments and a high output in “B→A” environments. Note that this task was both noisier than the other tasks and involved a longer evaluation period (to allow the noise some time to average out).</p>
      </sec>
      <sec id="s4s">
        <title>2-Bit environment task</title>
        <p>Unlike the other tasks, every environment in this task was designed by hand. The intention was to construct a range of radically different environments such that both short- and medium- term network memory-traces would be required to attain maximum fitness. The inspiration was loosely drawn from the concept of the “radical envelope of noise” [Jakobi, 1998]. Input pulses (boluses) in this task always occurred in closely-separated pairs, although the second bolus in a pair did not have to contain the same chemical as the first bolus. The pulse pairs occurred at regular intervals of 100 time units each. Each experimental condition was characterised by a “typical” pulse pair (A→A, A→B, B→A or B→B). In addition to the “typical” pulse pair corresponding to the experimental condition, every protocol for this task also had a “noise” pulse pair. There were in total 4 protocols (one for each pulse pair type), each containing 4 experimental conditions, for a total of 16 different input series. A single input series had the following structure: First, a pulse pair corresponding to the protocol's “noise” pair. Next, three “signal” pulse pairs all of the “typical” type for that experimental condition. Next, a “probe” pulse pair (see below). Next, another “noise” pulse pair of the protocol's “noise” type. Last, a final “probe” pulse pair. “Probe” pulse pairs consisted of a pulse of “B” chemical followed by either a pulse of “A” chemical (in the B→A environment) or a pulse of “B” chemical (in other environments). The desired network behaviour was to produce a low output for 10 time steps prior to each “probe” pulse pair, followed by either a high output (in the B→A environment) or a low output (in other environments) for 20 time steps. Errors in the B→A environment were weighted three times as heavily as errors in the three other environments.</p>
      </sec>
      <sec id="s4t">
        <title>Network connection density</title>
        <p>We calculate the number of reactions per effective chemical species in a network by first excluding any species which do not take part in reactions (this is possible if all reactions featuring a particular species are lost from a network by structural mutation). We then simply calculate the mean number of distinct reactions each remaining species is involved in.</p>
        <p>To investigate the effects of different genetic encoding factors on network connection density, we conducted 10 evolutionary runs on the 2-bit environment problem in each of 4 encoding variations. These were:</p>
        <list list-type="order">
          <list-item>
            <p>A benchmark case with maximum formula length 4, 2 symbols in the chemical alphabet, and the aggregation chemistry.</p>
          </list-item>
          <list-item>
            <p>A variation of the benchmark case with maximum formula length 6.</p>
          </list-item>
          <list-item>
            <p>A variation of the benchmark case with 4 symbols in the chemical alphabet.</p>
          </list-item>
          <list-item>
            <p>A variation of the benchmark case using the rearrangement chemistry.</p>
          </list-item>
        </list>
        <p>For all these runs, we recorded the effect of every mutation on both fitness and also the number of reactions per chemical species.</p>
      </sec>
      <sec id="s4u">
        <title>Bayesian interpretation of evolved networks</title>
        <p>Our method is as follows. We imagine an ideal Bayesian reasoner, equipped with knowledge of the statistics of the different network task environments. For each input train, at each point in time, we calculate what subjective probability the reasoner should assign to the possibility that the input train up to that point came from an “associated” environment. This establishes what the ideal Bayesian posterior would be at each point in time for each input train. If a network's chemical concentrations somehow encode this time-varying Bayesian posterior in all environments, then it would seem reasonable to attribute a Bayesian interpretation to the network. For the purposes of this paper, we will skirt over the complexities introduced by the non-dissipation of information in smooth continuous dynamical systems. In principle, the state of our simulated networks will usually contain all information about their historical inputs, because information can be stored in arbitrarily small differences in concentrations. However, in practice this information will be destroyed by noise.</p>
      </sec>
      <sec id="s4v">
        <title>Calculating ideal posteriors</title>
        <p>Calculation of the ideal posteriors for our environments is straightforward. A random variable X will represent the type of environment: either 1 (“associated”) or 0 (“unassociated”). Another random variable <italic>Y(t)</italic> will represent the train of input boluses up to time <italic>t</italic>. The ideal posterior probability of being in the “associated” environment after observing an input train <italic>y</italic> is<disp-formula id="pcbi.1002739.e018"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e018" xlink:type="simple"/></disp-formula>where<disp-formula id="pcbi.1002739.e019"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e019" xlink:type="simple"/></disp-formula>The prior <italic>P(X = 1)</italic> was set equal to the proportion of “associated” environments in the network training sets, i.e. 0.5. The probabilities <italic>P(Y(t) = y|X)</italic> were calculated as follows. In the environments we analysed in this way, Input trains were organised into “events”: a bolus of one or other input chemical, followed possibly at a set short interval by another bolus. The time between input events always exceeded the inter-spike interval within an event. The timing of events in an input train provided no information about the type of environment. Hence, events can be extracted from an input train and treated as a discrete process. “Associated” and “unassociated” environments correspond to Bernoulli processes with “associated” (B→A) and “unassociated” (B alone, for the noisy clocked task, or A→B for the AB-BA task) events. Thus, the posterior <italic>P(X|Y(t))</italic> can be calculated by counting the total number of associated and unassociated events in <italic>Y(t)</italic>.<disp-formula id="pcbi.1002739.e020"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e020" xlink:type="simple"/></disp-formula>where <italic>n</italic> and <italic>m</italic> are the number of “associated” and “unassociated” events in <italic>Y(t)</italic>, and <italic>p<sub>1</sub></italic> and <italic>p<sub>0</sub></italic> are the probabilities of an “associated” event in the “associated” and “unassociated” environments respectively. This gives<disp-formula id="pcbi.1002739.e021"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e021" xlink:type="simple"/></disp-formula>which can be attached to a time series at the appropriate points after events have occurred.</p>
      </sec>
      <sec id="s4w">
        <title>Matching posteriors to network state</title>
        <p>We use a straightforward logistic regression model to match network concentrations to Bayesian posteriors. Given a concentration vector <italic>x</italic>, a weight vector <italic>w</italic> and a bias value <italic>b</italic>, the model is<disp-formula id="pcbi.1002739.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e022" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002739.e023" xlink:type="simple"/></inline-formula> is the scalar product of <italic>x</italic> and <italic>w</italic>. Note that the output of this function is bounded between 0 and 1 like a probability value (this would not be the case for a linear regression model). The idea is that we can investigate the degree to which a rational Bayesian belief is encoded transparently in the network's state. We expect that at time <italic>t</italic>, having observed an input history <italic>Y(t)</italic>,<disp-formula id="pcbi.1002739.e024"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002739.e024" xlink:type="simple"/></disp-formula>To determine appropriate model parameters, we randomly generate 200 environments (100 “associated” and 100 “unassociated”), and run the evolved network in those environments. Weights w and bias b are set to minimise mean square error over all environments and time steps, using Levenburg-Marquadt optimisation. No attempt was made to regularise the parameters or otherwise avoid overfitting, since the model has relatively few parameters. For comparison, 200 random networks (produced by random initialisation followed by 200 mutations) were tested in the same way.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002739.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pcbi.1002739.s001" position="float" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Supporting information file.</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002739-Fernando1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fernando</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Liekens</surname><given-names>AML</given-names></name>, <name name-style="western"><surname>Bingle</surname><given-names>LEH</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Lenser</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Molecular circuits for associative learning in single-celled organisms</article-title>. <source>J Roy Soc Interface</source> <volume>6</volume>: <fpage>463</fpage>–<lpage>9</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Gandhi1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gandhi</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ashkenasy</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tannenbaum</surname><given-names>E</given-names></name> (<year>2007</year>) <article-title>Associative Learning in biochemical networks</article-title>. <source>J Theor Biol</source> <volume>249</volume>: <fpage>58</fpage>–<lpage>66</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Magnasco1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Magnasco</surname><given-names>MO</given-names></name> (<year>1997</year>) <article-title>Chemical kinetics is Turing Universal</article-title>. <source>Phys Rev Lett</source> <volume>78</volume>: <fpage>1190</fpage>–<lpage>1193</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Hjelmfelt1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hjelmfelt</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Weinberger</surname><given-names>ED</given-names></name>, <name name-style="western"><surname>Ross</surname><given-names>J</given-names></name> (<year>1991</year>) <article-title>Chemical implementation of neural networks and Turing machines</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>88</volume>: <fpage>10983</fpage>–<lpage>10987</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Goldstein1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Soyer</surname><given-names>OS</given-names></name> (<year>2008</year>) <article-title>Evolution of the Taxis Responses in Virtual Bacteria: Non-Adaptive Dynamics</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e10000084</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Parter1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parter</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kashtan</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Alon</surname><given-names>U</given-names></name> (<year>2008</year>) <article-title>Facilitated Variation: How Evolution Learns from Past Environments to Generalize to New Environments</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000206</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bray1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bray</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Molecular Networks: The Top-Down View</article-title>. <source>Science</source> <volume>26</volume>: <fpage>1864</fpage>–<lpage>1865</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bray2">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bray</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lay</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Computer simulated evolution of a network of cell-signaling molecules</article-title>. <source>Biophys J</source> <volume>66</volume>: <fpage>972</fpage>–<lpage>977</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Paladugu1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paladugu</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Chickarmane</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Deckard</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Frumkin</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>McCormack</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>In silico evolution of functional modules in biochemical networks</article-title>. <source>Syst Biol</source> <volume>153</volume>: <fpage>223</fpage>–<lpage>235</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Holland1">
        <label>10</label>
        <mixed-citation publication-type="other" xlink:type="simple">Holland JH (1975) Adaptation in Natural and Artificial Systems. Ann Arbor: University of Michigan Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Fogel1">
        <label>11</label>
        <mixed-citation publication-type="other" xlink:type="simple">Fogel DB (2006) Evolutionary Computation: Toward a New Pholosophy of Machine Intelligence. Piscataway, NJ: Wiley-Interscience.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Baeck1">
        <label>12</label>
        <mixed-citation publication-type="other" xlink:type="simple">Baeck T, Fogel DB, Michalewicz ZM (1997) Handbook of Evolutionary Computation New York: Taylor and Francis Group.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Phattanasri1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Phattanasri</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Chiel</surname><given-names>HJ</given-names></name>, <name name-style="western"><surname>Beer</surname><given-names>RD</given-names></name> (<year>2007</year>) <article-title>The dynamics of associative learning in evolved model circuits</article-title>. <source>Adapt Behav</source> <volume>15</volume>: <fpage>377</fpage>–<lpage>396</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bagley1">
        <label>14</label>
        <mixed-citation publication-type="other" xlink:type="simple">Bagley RJ, Farmer JD, Fontana W. (1992) Evolution of a Metabolism. In: Langton CG, Taylor C, Farmer JD, Rasmussen S, editors. Artificial Life II, Proceedings. Santa Fe: Addison-Wesley.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Fontana1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fontana</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Buss</surname><given-names>LW</given-names></name> (<year>1994</year>) <article-title>What would be conserved if ‘the tape were played twice’?</article-title> <source>Proc Natl Acad Sci U S A</source> <volume>91</volume>: <fpage>757</fpage>–<lpage>761</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Fernando2">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fernando</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Rowe</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Natural Selection in Chemical Evolution</article-title>. <source>J Theor Biol</source> <volume>247</volume>: <fpage>152</fpage>–<lpage>167</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Fernando3">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fernando</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Rowe</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>The origin of autonomous agents by natural selection</article-title>. <source>Biosystems</source> <volume>91</volume>: <fpage>355</fpage>–<lpage>373</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Vasas1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasas</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Fernando</surname><given-names>CT</given-names></name>, <name name-style="western"><surname>Santos</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kauffman</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Szathmáry</surname><given-names>E</given-names></name> (<year>2012</year>) <article-title>Evolution before genes</article-title>. <source>Biology Direct</source> <volume>7</volume> In press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Dayan1">
        <label>19</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dayan P, Abbott L (2001) Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Mackintosh1">
        <label>20</label>
        <mixed-citation publication-type="other" xlink:type="simple">Mackintosh NJ (1974) The psychology of animal learning. Oxford, England: Academic Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Rescorla1">
        <label>21</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rescorla RA, Wagner AR (1972) A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In: Black AH, Prokasy WF, editors. Classical Conditioning II: Current Research and Theory. Appleton-Century-Crofts. pp. 64–99.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Kamin1">
        <label>22</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kamin LJ (1969) Predictability, surprise, attention and conditioning. In: Campbell BA, Church RM, editors. Punishment and aversive behavior. New York: Appleton-Century-Crofts. pp. 279–296.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Gopnik1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schulz</surname><given-names>L</given-names></name> (<year>2004</year>) <article-title>Mechanisms of theory formation in young children</article-title>. <source>Trends in Cogn Sci</source> <volume>8</volume>: <fpage>371</fpage>–<lpage>377</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Barber1">
        <label>24</label>
        <mixed-citation publication-type="other" xlink:type="simple">Barber D (2012) Bayesian Reasoning and Machine Learning. Cambridge University Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Libby1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Libby</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Perkins</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Swain</surname><given-names>PS</given-names></name> (<year>2007</year>) <article-title>Noisy information processing through transcriptional regulation</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>104</volume>: <fpage>7151</fpage>–<lpage>7156</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Maass1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>On the computational power of recurrent circuits of spiking neurons</article-title>. <source>J Comput Syst Sci</source> <volume>69</volume>: <fpage>593</fpage>–<lpage>616</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Maass2">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Jaeger1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaeger</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Principe</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Introduction to the special issue on echo state networks and liquid state machines</article-title>. <source>Neural Netw</source> <volume>20</volume>: <fpage>287</fpage>–<lpage>289</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bingham1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bingham</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Mannila</surname><given-names>H</given-names></name> (<year>2001</year>) <article-title>Random projection in dimensionality reduction: Applications to image and text data</article-title>. <source>Data Min Knowl Discov</source> <fpage>245</fpage>–<lpage>250</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Hennessey1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hennessey</surname><given-names>T</given-names></name> (<year>1979</year>) <article-title>Classical Conditioning in Paramecia</article-title>. <source>Anim Learn Behav</source> <volume>7</volume>: <fpage>419</fpage>–<lpage>423</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Eckert1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eckert</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Naitoh</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>K</given-names></name> (<year>1972</year>) <article-title>Sensory mechanisms in Paramecium. I. Two cmoponents of the electric response to mechanical stimulation of the anterior surface</article-title>. <source>J Exp Biol</source> <volume>56</volume>: <fpage>683</fpage>–<lpage>694</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Dunlap1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunlap</surname><given-names>K</given-names></name> (<year>1977</year>) <article-title>Localization of Calcium Channels in Paramecium Caudatum</article-title>. <source>J Physiol</source> <volume>271</volume>: <fpage>119</fpage>–<lpage>133</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Gustin1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gustin</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>DL</given-names></name> (<year>1987</year>) <article-title>Regulation of ciliary adenylate cyclase by Ca2+ in Paramecium</article-title>. <source>Biochem J</source> <volume>246</volume>: <fpage>337</fpage>–<lpage>345</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Walters1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walters</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Carew</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Kandel</surname><given-names>ER</given-names></name> (<year>1979</year>) <article-title>Classical conditioning in <italic>Aplysia californica</italic></article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>76</volume>: <fpage>6675</fpage>–<lpage>6679</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bergstrm1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bergström</surname><given-names>SR</given-names></name> (<year>1968</year>) <article-title>Induced Avoidance Behaviour in the Protozoa Tetrahymena</article-title>. <source>Scand J Psychol</source> <volume>9</volume>: <fpage>215</fpage>–<lpage>219</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Bergstrm2">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bergström</surname><given-names>SR</given-names></name> (<year>1968</year>) <article-title>Acquisition of an avoidance reaction to light in the protozoa tetrahymena</article-title>. <source>Scand J Psychol</source> <volume>9</volume>: <fpage>220</fpage>–<lpage>224</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Nilsonne1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nilsonne</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Appelgren</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Axelsson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Fredrikson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lekander</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Learning in a simple biological system: a pilot study of classical conditioning of human macrophages in vitro</article-title>. <source>Behav Brain Funct</source> <volume>7</volume>: <fpage>47</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Tagkopoulos1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tagkopoulos</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Y-C</given-names></name>, <name name-style="western"><surname>Tavazoie</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Predictive Behavior Within Microbial Genetic Networks</article-title>. <source>Science</source> <volume>320</volume>: <fpage>1313</fpage>–<lpage>1317</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Dale1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dale</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Collett</surname><given-names>TS</given-names></name> (<year>2001</year>) <article-title>Using artificial evolution and selection to model insect navigation</article-title>. <source>Curr Biol</source> <volume>11</volume>: <fpage>1305</fpage>–<lpage>1316</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Mitchell1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Romano</surname><given-names>GH</given-names></name>, <name name-style="western"><surname>Groisman</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Dekel</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Kupiec</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Adaptive prediction of environmental changes by microorganisms</article-title>. <source>Nature</source> <volume>460</volume>: <fpage>220</fpage>–<lpage>225</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002739-Harvey1">
        <label>41</label>
        <mixed-citation publication-type="other" xlink:type="simple">Harvey I (2011) The Microbial Genetic Algorithm. In: Kampis G, editor. ECAL 2009. Budapest, Hungary: Springer, Heidelberg. pp. 126–133.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>