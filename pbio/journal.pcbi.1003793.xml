<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00169</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003793</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject><subject>Sensory systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Computing methods</subject><subj-group><subject>Mathematical computing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Sparsity and Compressed Coding in Sensory Systems</article-title>
<alt-title alt-title-type="running-head">Sparsity and Compressed Coding in Sensory Systems</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Barranca</surname><given-names>Victor J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kovačič</surname><given-names>Gregor</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zhou</surname><given-names>Douglas</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Courant Institute of Mathematical Sciences and Center for Neural Science, New York University, New York, New York, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>NYUAD Institute, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Mathematical Sciences Department, Rensselaer Polytechnic Institute, Troy, New York, United States of America</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Department of Mathematics, MOE-LSC, and Institute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">zdz@sjtu.edu.cn</email> (DZ); <email xlink:type="simple">cai@cims.nyu.edu</email> (DC)</corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: VJB GK DZ DC. Performed the experiments: VJB GK DZ DC. Analyzed the data: VJB GK DZ DC. Contributed reagents/materials/analysis tools: VJB GK DZ DC. Wrote the paper: VJB GK DZ DC.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>8</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>21</day><month>8</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>8</issue>
<elocation-id>e1003793</elocation-id>
<history>
<date date-type="received"><day>28</day><month>1</month><year>2014</year></date>
<date date-type="accepted"><day>4</day><month>7</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Barranca et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Considering that many natural stimuli are sparse, can a sensory system evolve to take advantage of this sparsity? We explore this question and show that significant downstream reductions in the numbers of neurons transmitting stimuli observed in early sensory pathways might be a consequence of this sparsity. First, we model an early sensory pathway using an idealized neuronal network comprised of receptors and downstream sensory neurons. Then, by revealing a linear structure intrinsic to neuronal network dynamics, our work points to a potential mechanism for transmitting sparse stimuli, related to compressed-sensing (CS) type data acquisition. Through simulation, we examine the characteristics of networks that are optimal in sparsity encoding, and the impact of localized receptive fields beyond conventional CS theory. The results of this work suggest a new network framework of signal sparsity, freeing the notion from any dependence on specific component-space representations. We expect our CS network mechanism to provide guidance for studying sparse stimulus transmission along realistic sensory pathways as well as engineering network designs that utilize sparsity encoding.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>In forming a mental percept of the surrounding world, sensory information is processed and transmitted through a wide array of neuronal networks of various sizes and functionalities. Despite, and perhaps because of, this, sensory systems are able to render highly accurate representations of stimuli. In the retina, for example, photoreceptors transform light into electric signals, which are later processed by a significantly smaller network of ganglion cells before entering the optic nerve. How then is sensory information preserved along such a pathway? In this work, we put forth a possible answer to this question using compressed sensing, a recent advance in the field of signal processing that demonstrates how sparse signals can be reconstructed using very few samples. Through model simulation, we discover that stimuli can be recovered from ganglion-cell dynamics, and demonstrate how localized receptive fields improve stimulus encoding. We hypothesize that organisms have evolved to utilize the sparsity of stimuli, demonstrating that compressed sensing may be a universal information-processing framework underlying both information acquisition and retention in sensory systems.</p>
</abstract>
<funding-group><funding-statement>The work was supported by grants NSF DMS-0636358 (for VJB), 10PJ1406300, NSFC-11101275, and NSFC-91230202 (for DZ), NSF DMS-1009575 (DC), SRF for ROCS, SEM (for DZ), and the NYU Abu Dhabi Institute G1301. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="11"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>It is well known that natural stimuli, such as visual images, are sparse in the sense that they can be well represented by a small number of dominant components, typically in an appropriate frequency space <xref ref-type="bibr" rid="pcbi.1003793-Field1">[1]</xref>. We may thus naturally expect that organisms' sensing has evolved to be adapted to such sparsity. One sign of this adaptation may be the great reduction in numbers between the receptor cells and the sensory neurons in the immediate downstream layers along the early stages of sensory pathways <xref ref-type="bibr" rid="pcbi.1003793-Barlow1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Buck1">[3]</xref>. For example, in the retina, the stimuli received by ∼150 million rods and cones are transmitted through only ∼1.5 million retinal ganglion cells <xref ref-type="bibr" rid="pcbi.1003793-Barlow1">[2]</xref>. More generally, it is important to know how the network topology of early sensory pathways reflects this type of adaptation. How have the networks along these pathways evolved so that they can best transmit sparse stimuli and the least amount of information is lost through network dynamics <xref ref-type="bibr" rid="pcbi.1003793-Barlow2">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Barlow3">[5]</xref>?</p>
<p>Theoretically, the above question translates into the search for a class of neuronal networks that takes advantage of stimulus sparsity and thus best encodes such stimuli. Naturally, such networks should need relatively few downstream neurons to sample the input from the receptors. An instructive technological analog is provided by the <italic>compressed sensing</italic> (CS) theory <xref ref-type="bibr" rid="pcbi.1003793-Candes1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Candes2">[7]</xref>. When using sufficiently random sampling of sparse images, this theory allows us to dramatically reduce the sampling rate as compared to that expected for the uniform sampling of finite-bandwidth stimuli <xref ref-type="bibr" rid="pcbi.1003793-Shannon1">[8]</xref>, without degrading the image reconstruction. Greatly improving the fidelity of high dimensional data reconstructions and developing efficient sampling algorithms, applications of CS have emerged in numerous fields, including physics, biology, and imaging <xref ref-type="bibr" rid="pcbi.1003793-Gross1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003793-Herman1">[13]</xref>.</p>
<p>In the context of neuroscience, it has been conjectured that the information processing in the brain may be related to the existence of an efficient coding scheme, such as compressed sensing <xref ref-type="bibr" rid="pcbi.1003793-Isley1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Ganguli1">[15]</xref>. Using adaptive CS, for example, sparse representations of sets of sub-sampled inputs can be developed through unsupervised learning without knowledge of either the sampling protocol or the sparse basis of the measured signal, revealing that CS may, in theory, help to explain signal interpretation and transmission in the brain <xref ref-type="bibr" rid="pcbi.1003793-Coulter1">[16]</xref>. Following the CS mathematical structure, it has also been suggested that linear, discrete-time network dynamics can be used to encode sparse temporal sequences of information in their current activity and therefore neuronal networks may possess a greater theoretical memory capacity than previously hypothesized <xref ref-type="bibr" rid="pcbi.1003793-Ganguli2">[17]</xref>. In this work, we take a new direction by constructing a spiking-neuron network model of an early sensory pathway and demonstrating how the firing rates of a relatively small set of sensory neurons with nonlinear dynamics can successfully encode network inputs. Deriving a linear mapping embedded in the network dynamics, we use CS theory and the dynamics of our model network over a biologically realistic time-scale to reconstruct visual stimuli, which are known to be sparse in frequency space <xref ref-type="bibr" rid="pcbi.1003793-Field1">[1]</xref>. We also find that the performance of this model can be greatly improved by incorporating the biologically realistic property of localized receptive fields <xref ref-type="bibr" rid="pcbi.1003793-Wiesel1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Hubel1">[19]</xref>. Unlike previous work <xref ref-type="bibr" rid="pcbi.1003793-Isley1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Ganguli1">[15]</xref>, the derived input-output relationship is not constructed through learning, and is instead intrinsic to the network dynamics, suggesting a possible way sensory information is transmitted downstream via sparse coding of stimuli through network dynamics.</p>
<p>Even before the discovery of CS, sparse coding was hypothesized as a feature fundamental to optimally representing sensory information, thus possibly leading to the emergence of spatial receptive-field properties of simple cells in the primary visual cortex <xref ref-type="bibr" rid="pcbi.1003793-Olshausen1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Olshausen2">[21]</xref>. Instead of using the framework of optimization <xref ref-type="bibr" rid="pcbi.1003793-Olshausen1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Olshausen2">[21]</xref>, we consider how the time-evolving output of populations of firing neurons encodes stimulus information and examine the key characteristics of a CS neuronal network best evolved to transmit sparse stimuli. Underlining a novel notion of sparsity in terms of network dynamics, our results suggest a stimulus may be considered sparse if it can be accurately encoded by networks in which the number of downstream neurons is much lower than the number of input components, separating the notion of sparsity from any dependence on a particular component-space transform choice.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Minimal Compressed Sensing Network</title>
<p>To study sparse stimulus transmission along early stages of sensory pathways, we have constructed our conceptual network model to consist of two layers, an input layer and a processing layer, representing the receptors and sensory neurons (sensory cells), e.g., retinal ganglion cells in the retina. Invoking the fact that the receptor neurons in the retina exhibit graded-potential rather than the usual action-potential responses <xref ref-type="bibr" rid="pcbi.1003793-Wu1">[22]</xref>, we represent the input layer by currents injected into the sensory neurons in the processing layer. Each current represents the stimulus intensity in the receptive field of a given receptor, which relays this intensity downstream to a number of sensory neurons. We describe these neurons using the pulse-coupled, integrate-and-fire (I&amp;F) model <xref ref-type="bibr" rid="pcbi.1003793-Burkitt1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003793-Zhou1">[30]</xref>. Our model is intentionally idealized, so that only the most general features of early sensory pathways are incorporated. In this way, we aim to emphasize the possible role of the CS mechanism in a broad class of sensory systems.</p>
<p>In our model, the membrane-potential dynamics of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e001" xlink:type="simple"/></inline-formula> sensory neuron is governed by the differential equation <disp-formula id="pcbi.1003793.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e002" xlink:type="simple"/></disp-formula> <disp-formula id="pcbi.1003793.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e003" xlink:type="simple"/><label>(1)</label></disp-formula>and evolves from the reset potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e004" xlink:type="simple"/></inline-formula> until it reaches the threshold potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e005" xlink:type="simple"/></inline-formula>. At the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e006" xlink:type="simple"/></inline-formula> time this occurs, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e007" xlink:type="simple"/></inline-formula>, we say that this neuron has fired (or spiked), reset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e008" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e009" xlink:type="simple"/></inline-formula>, and inject the currents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e010" xlink:type="simple"/></inline-formula> into all the other sensory neurons post-connected to it, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e011" xlink:type="simple"/></inline-formula> being the Dirac delta function. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e012" xlink:type="simple"/></inline-formula> is the membrane-potential time-scale, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e013" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e014" xlink:type="simple"/></inline-formula> are the numbers of the receptors and sensory neurons, respectively, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e015" xlink:type="simple"/></inline-formula> is the number of sensory-neuron connections, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e016" xlink:type="simple"/></inline-formula> are the stimulus strengths transmitted by the receptors, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e017" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e018" xlink:type="simple"/></inline-formula> are connection matrices between the receptors and sensory neurons and between sensory neuron pairs, respectively, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e019" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e020" xlink:type="simple"/></inline-formula> are the respective overall strengths of those connections. Stimulus components, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e021" xlink:type="simple"/></inline-formula>, take on integer values between 0 and 255, indicating the light intensity of the stimulus. These components will typically be fixed over time, since we primarily consider stationary stimuli. We simulate this model for a run-time of 200 ms using an event-driven algorithm in which we analytically solve for sensory neuron voltages and spike times, choosing parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e022" xlink:type="simple"/></inline-formula> ms, the dimensionless potential values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e024" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e025" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e026" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e027" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e028" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Brette1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Newhall1">[32]</xref>.</p>
<p>We first assume that every sensory neuron samples the stimulus randomly from the entire receptor pool, and choose the numbers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e029" xlink:type="simple"/></inline-formula> of the receptors and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e030" xlink:type="simple"/></inline-formula> of the sensory neurons to be such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e031" xlink:type="simple"/></inline-formula> (In most of our computations, due to the limitations imposed by our computational power, we take the ratio <italic>n</italic>∶<italic>m</italic> to be 10∶1 instead of the 100∶1 observed in early sensory pathways <xref ref-type="bibr" rid="pcbi.1003793-Barlow1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Buck1">[3]</xref>.) While this assumption of random sampling is fundamental to conventional CS theory, we later consider the more realistic case in which photoreceptors are sampled locally by sensory neurons, which yields a significant improvement in stimulus encoding <xref ref-type="bibr" rid="pcbi.1003793-Wiesel1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Hubel1">[19]</xref>. Moreover, the sensory neurons are also initially assumed to be connected to each other randomly, but, as we will subsequently demonstrate, we can also assume that the sensory neurons are uncoupled without affecting the results of this work. While retinal ganglion cells, for example, are in some cases not thought to be connected to each other, there are also other cases in which connectivity is observed, and we therefore address this by considering both scenarios <xref ref-type="bibr" rid="pcbi.1003793-DeBoer1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003793-Trenholm1">[35]</xref>. Although ganglion-cell connections are typically gap junctions <xref ref-type="bibr" rid="pcbi.1003793-Trong1">[34]</xref>, we model these connections with pulse-coupling to maintain model idealization and simplicity. Therefore, we first take the elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e033" xlink:type="simple"/></inline-formula> of both connectivity matrices to be Bernoulli-distributed, and later consider several realistic alternative assumptions, such as the localized properties of receptive fields <xref ref-type="bibr" rid="pcbi.1003793-Feller1">[36]</xref>. The inputs into the sensory neurons are assumed noiseless in our preliminary discussion, but we will address the impact of more biological noisy processing, due to fluctuations in photon absorption for example, in the upcoming section <xref ref-type="bibr" rid="pcbi.1003793-Dunn1">[37]</xref>.</p>
<p>We emphasize that we are modeling a general early sensory pathway, rather than incorporating details specific to the retina, and therefore omit several detailed properties in order to accentuate the underlying CS mechanism. For example, compared to the actual retinal network, we only consider “on,” rod-like receptors, neglecting any time-course details of the graded potentials the receptors produce <xref ref-type="bibr" rid="pcbi.1003793-Thoreson1">[38]</xref> and any crosstalk among the receptors <xref ref-type="bibr" rid="pcbi.1003793-Li1">[39]</xref>. In addition, we also neglect the rich variety of the neuron types in the retina <xref ref-type="bibr" rid="pcbi.1003793-Field2">[40]</xref> and their complex connectivity <xref ref-type="bibr" rid="pcbi.1003793-Anderson1">[41]</xref>, the center-surround structure of the ganglion neurons' receptive fields <xref ref-type="bibr" rid="pcbi.1003793-Wiesel1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Hubel1">[19]</xref>, any spatial differences in the density of the receptor distribution <xref ref-type="bibr" rid="pcbi.1003793-Curcio1">[42]</xref>, as well as any inhibition <xref ref-type="bibr" rid="pcbi.1003793-Wu1">[22]</xref>.</p>
<p>To determine the degree of connectivity between our networks, we introduce the notion of convergence, which is defined as the average number of neurons presynaptic to any neuron in a given network. In particular, we use a convergence of 50 for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e034" xlink:type="simple"/></inline-formula>, the sensory-neuron connection matrix, and a convergence of 10 for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e035" xlink:type="simple"/></inline-formula>, the sensory-neuron to receptor connection matrix. The architecture of the network is represented graphically in <xref ref-type="fig" rid="pcbi-1003793-g001">Fig. 1</xref>. For this neuronal network, conceptually, the question is whether its dynamics take advantage of the sparse stimulus structure, and whether its topology can effectively and efficiently transduce the input information to the sensory neurons.</p>
<fig id="pcbi-1003793-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g001</object-id><label>Figure 1</label><caption>
<title>Mathematical model.</title>
<p>Graphical depiction of mathematical model and network connectivity.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g001" position="float" xlink:type="simple"/></fig>
<p>The above question translates to how to design the network parameters so that the information from the original stimulus is best retained by the firing rates embedded in sensory-neuron spike trains when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e036" xlink:type="simple"/></inline-formula>, i.e., how closely we can reconstruct the original stimulus from the sensory neurons' firing rates given the model network's connectivity. A stimulus presented to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e037" xlink:type="simple"/></inline-formula> receptors is considered <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e038" xlink:type="simple"/></inline-formula>-sparse, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e039" xlink:type="simple"/></inline-formula>, when at least one of its transforms into an appropriate frequency or wavenumber space, such as Fourier or wavelet, has at most <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e040" xlink:type="simple"/></inline-formula> components whose magnitude exceeds a small threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e041" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Candes1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Donoho1">[43]</xref>. Given such a stimulus, our model sensory-neuron network generates a set of spike trains, which presumably encodes sparse stimulus information.</p>
<p>If we want to use the CS theory as a guiding principle in our model network construction, we immediately encounter a conceptual difficulty because a prerequisite for CS is linear signal measurement, whereas neuronal dynamics are nonlinear. However, it turns out that there is a linear structure corresponding to the input-output relationship embedded within this network. Using coarse-graining methods similar to kinetic theory in nonequilibrium statistical physics, we derive the linearized firing-rate system <disp-formula id="pcbi.1003793.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e042" xlink:type="simple"/><label>(2)</label></disp-formula>valid when the neuronal firing rates, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e043" xlink:type="simple"/></inline-formula>, satisfy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e044" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e045" xlink:type="simple"/></inline-formula> and the membrane potential jump induced by each spike is small, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e046" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Rangan2">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Cai2">[45]</xref>. The firing-rate model (2) was previously derived in the population sense for ensembles of neuronal networks with stochastic inputs of homogeneous strength. However, our work here reveals that through coarse-graining over an ensemble of network realizations differing in initial voltage conditions, in which each network realization is forced by the same set of heterogeneous deterministic inputs, this firing-rate model can be extended to each individual neuron coupled in the network. For weak sensory-neuron coupling-strength and high sensory-neuron firing rates, we therefore obtain a linear network mapping of the stimulus intensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e047" xlink:type="simple"/></inline-formula> arriving at each receptor onto the firing rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e048" xlink:type="simple"/></inline-formula> generated by each sensory neuron. In this case, the network is mean-driven, with each sensory neuron receiving a large number of small inputs from its neighbors, which can be approximated by a Poisson spike train of inputs. Under this assumption, we derive a non-linear input-output mapping, which we then linearize in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e049" xlink:type="simple"/></inline-formula> limit. The linear network mapping (2) is between the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e050" xlink:type="simple"/></inline-formula>-dimensional input vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e051" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e052" xlink:type="simple"/></inline-formula>-dimensional output vector of neuronal firing rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e053" xlink:type="simple"/></inline-formula>; it is not a map between population-averaged input (the network input) and the population-averaged output (the network firing rate) as in traditional coarse-graining applications <xref ref-type="bibr" rid="pcbi.1003793-Rangan2">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Cai2">[45]</xref>.</p>
<p>The proximity of the firing rates we have obtained from the I&amp;F model (1) and the linear network mapping (2) is depicted in <xref ref-type="fig" rid="pcbi-1003793-g002">Fig. 2</xref>. The red line displays the dependence of the relative firing rate difference on the overall stimulus intensity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e054" xlink:type="simple"/></inline-formula>. Since the firing rate of each neuron is determined by its unique external input and network connectivity, the error given in <xref ref-type="fig" rid="pcbi-1003793-g002">Fig. 2</xref> is the sum of the errors for all individual neurons. It is clear that, <italic>neuron-by-neuron</italic>, the two sets of firing rates agree well with one another over a wide stimulus-intensity range, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e055" xlink:type="simple"/></inline-formula>, i.e., as long as the input to the sensory neurons is not too weak. For much of this regime, especially near <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e056" xlink:type="simple"/></inline-formula>, the model sensory neuron firing rates are typically between 20 Hz and 100 Hz, closely resembling experimentally observed firing rates of biologically realistic neurons, such as retinal ganglion cells <xref ref-type="bibr" rid="pcbi.1003793-Trong1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Zaghloul1">[46]</xref>.</p>
<fig id="pcbi-1003793-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g002</object-id><label>Figure 2</label><caption>
<title>Validity of network firing-rate mapping.</title>
<p>Closeness of the firing rates produced by the I&amp;F model and firing-rate mapping for each neuron in the network. The stimulus used is the image in <xref ref-type="fig" rid="pcbi-1003793-g003">Fig. 3c</xref>; Left ordinate axis: Gain curves depicting the dependence of the <italic>network-averaged</italic> firing rate on the overall external drive strength. Right ordinate axis: The same dependence of the cumulative neuron-to-neuron relative firing rate difference between the I&amp;F model and the linear network mapping. This difference is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e057" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e058" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e059" xlink:type="simple"/></inline-formula> represents the vector of the sensory-neuron firing rates.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g002" position="float" xlink:type="simple"/></fig>
<p>With the embedded linear network mapping (2), we arrive at our hypothesis that CS can potentially be a governing principle in transmitting sparse stimuli from the receptors to the sensory neurons, while simultaneously achieving a great reduction in the number of sensory neurons. In signal processing, the well-known Shannon-Nyquist theorem asserts that we must sample a signal with a given bandwidth uniformly at the rate of twice that bandwidth in order to be able to faithfully reproduce it <xref ref-type="bibr" rid="pcbi.1003793-Shannon1">[8]</xref>. However, according to CS theory, images that are (approximately) sparse in a wavenumber-space can be reconstructed from random samplings whose number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e060" xlink:type="simple"/></inline-formula> is much smaller than the number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e061" xlink:type="simple"/></inline-formula> of pixels composing the image by finding the reconstruction with the smallest number of nonzero wavenumber-space components. Ref. <xref ref-type="bibr" rid="pcbi.1003793-Donoho1">[43]</xref> shows that this difficult optimization problem becomes equivalent to the much simpler question of finding the reconstruction whose wavenumber-space-component magnitudes add up to the smallest sum. Mathematically, one thus replaces a computationally expensive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e062" xlink:type="simple"/></inline-formula> optimization problem in wavenumber space by a much simpler <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e063" xlink:type="simple"/></inline-formula> optimization problem, which can be efficiently solved via several optimization algorithms <xref ref-type="bibr" rid="pcbi.1003793-Chen1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Tropp1">[48]</xref>.</p>
<p>By applying the CS approach of Candès and Tao to the linear mapping (2), we can reconstruct the stimulus from our model spike trains <xref ref-type="bibr" rid="pcbi.1003793-Candes2">[7]</xref>. Thus, to estimate the sensing accuracy of our model early-sensory-pathway network, we measure the firing rates of each neuron in this network, and use the linear network mapping embedded in this model to carry out the relevant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e064" xlink:type="simple"/></inline-formula> optimization procedure for finding the sparsest stimulus reconstruction. In particular, we reconstruct the stimulus given the rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e065" xlink:type="simple"/></inline-formula>, which we measure from the full simulation of the I&amp;F network (1), by minimizing the sum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e066" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e067" xlink:type="simple"/></inline-formula> is the vectorization of the two-dimensional discrete cosine transform of the pixel matrix corresponding to stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e068" xlink:type="simple"/></inline-formula>, subject to the condition that the stimulus components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e069" xlink:type="simple"/></inline-formula> satisfy the linear system (2). (See the Methods Section for details.) It is important to remark that this reconstruction procedure requires only a brief simulation time, generally no more than 100 ms, since any initial transients in the network dynamics are very brief and typically last no more than 25 ms. In the next section, we further analyze the dependence of the CS reconstruction on the simulation time, and demonstrate that successful signal recovery takes place over a biologically realistic time-scale.</p>
<p>We display three sets of results of our CS reconstruction procedure in <xref ref-type="fig" rid="pcbi-1003793-g003">Fig. 3</xref>, for which the stimuli are images of increasing complexity: stripes, dots, and flowers. Visually, the CS algorithm renders recognizable reconstructions of all the objects, and performs best with large shapes, flat surfaces, and gradual transitions, while leaving some graininess, which appears especially pronounced near sharp edges.</p>
<fig id="pcbi-1003793-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g003</object-id><label>Figure 3</label><caption>
<title>Stimulus reconstructions with CS.</title>
<p><bold>a</bold>, <bold>b</bold>, <bold>c</bold> two-dimensional images with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e070" xlink:type="simple"/></inline-formula> pixels, <bold>d</bold>, <bold>e</bold>, <bold>f</bold> reconstructions by CS; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e071" xlink:type="simple"/></inline-formula>, percent of Fourier components greater than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e072" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e073" xlink:type="simple"/></inline-formula>. We choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e074" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e075" xlink:type="simple"/></inline-formula>, simulating network dynamics and recording neuronal spikes for a run-time of 200 ms.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Network Characteristics for Optimal Reconstruction</title>
<p>In determining the type of networks that can best take advantage of stimulus sparsity and optimally encode information, we study how the relative error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e076" xlink:type="simple"/></inline-formula>, of the CS stimulus reconstruction depends on the various model network characteristics. We measure this error using the formula <disp-formula id="pcbi.1003793.e077"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e077" xlink:type="simple"/></disp-formula>where the Euclidean norm, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e078" xlink:type="simple"/></inline-formula>, is defined analogously to the definition in the caption of <xref ref-type="fig" rid="pcbi-1003793-g002">Fig. 2</xref>. To isolate the effect of each characteristic, we vary only one parameter at a time while holding the remaining parameters constant.</p>
<p>First, we address how these CS networks depend on the convergence of the connections between the receptors and the sensory neurons, as represented by the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e079" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Ganmor1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Ganmor2">[50]</xref>. As shown in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4a</xref>, the error decreases until the optimal convergence of about 10 is reached, and then increases. We remark that the high error for low convergence levels is due to the model sensory network not being able to sample all the receptors, while for high convergence levels all the sensory neurons receive nearly identical input. It should be clear why the convergence 10 is optimal; it is the ratio <italic>n</italic>∶<italic>m</italic> of the receptors versus the sensory neurons for our network model. At this ratio, with very high probability, each receptor feeds into precisely one sensory neuron. Due to the random sampling by the sensory neurons, on the other hand, again with high probability, the number of receptors relaying stimuli to a given sensory neuron will be approximately <italic>n</italic>∶<italic>m</italic>. Thus, all or most of the stimulus is used by the model sensory network, and there is little or no over- or undersampling.</p>
<fig id="pcbi-1003793-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g004</object-id><label>Figure 4</label><caption>
<title>Optimal CS network characteristics.</title>
<p>Characteristics of optimal CS networks examined using the dependence of relative reconstruction error on network features: <bold>a</bold> convergence of the sensory-cell (sensory-neuron) to receptor connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e080" xlink:type="simple"/></inline-formula>, <bold>b</bold> stimulus strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e081" xlink:type="simple"/></inline-formula>, <bold>c</bold> convergence of the sensory-cell connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e082" xlink:type="simple"/></inline-formula>, <bold>d</bold> connection strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e083" xlink:type="simple"/></inline-formula>, <bold>e</bold> simulation time over which spikes are recorded, <bold>f</bold> variance of Gaussian noise added to stimulus components, <bold>g</bold> number of sensory-cells, and <bold>h</bold> level of randomness in the sensory-cell to receptor connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e084" xlink:type="simple"/></inline-formula>. In panel <bold>a</bold>, we normalize the stimulus strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e085" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e086" xlink:type="simple"/></inline-formula>, the number of nonzero entries in the receptor-to-sensory-neuron connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e087" xlink:type="simple"/></inline-formula>, to keep the amount of drive to a sensory neuron approximately constant when changing convergence. The minimal relative error at the optimal convergence in panel <bold>a</bold> is approximately equal to 0.35.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g004" position="float" xlink:type="simple"/></fig>
<p>Likewise, we address the question of how stimulus strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e088" xlink:type="simple"/></inline-formula> controls the optimality of CS networks. In <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4b</xref>, we fix the convergence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e089" xlink:type="simple"/></inline-formula> and the relative component sizes of the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e090" xlink:type="simple"/></inline-formula>, and scale linearly the overall stimulus strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e091" xlink:type="simple"/></inline-formula>. We observe that the reconstruction is best for moderate strength values, with particularly high reconstruction error for low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e092" xlink:type="simple"/></inline-formula>, and slowly degrading reconstruction quality as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e093" xlink:type="simple"/></inline-formula> becomes too large. For the optimal stimulus strength, the sensory neurons are then neither underdriven, such that there are not enough firing events to properly encode network input, nor driven so hard that their interaction becomes too strong, overwhelming the information in the input signal.</p>
<p>In contrast, as displayed in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4c</xref>, the reconstruction error appears to depend little on the convergence of the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e094" xlink:type="simple"/></inline-formula> encoding the connections among the sensory neurons. In particular, for the error size, it makes little difference whether a sensory neuron receives many weak pulses or a few strong pulses from its neighbors, indicating that the amount of fluctuations received from within the sensory neuron network plays a rather small role. The error also appears to be relatively independent of the overall connection strength, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e095" xlink:type="simple"/></inline-formula>, at sufficiently low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e096" xlink:type="simple"/></inline-formula>-values, and then grows linearly with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e097" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4d</xref>. This reflects the fact that cross-talk among the sensory neurons that is too strong is likely to drown out the signal received from the receptors. Altogether, it thus appears that the connections among sensory neurons neither improve nor degrade the performance of the model network as long as their strengths are moderate. In the case of the retina, we note that while it was previously thought that there is no recurrent connectivity among retinal ganglion cells, recent experimental work shows that there is indeed gap-junction-type coupling among specific types of ganglion cells. <xref ref-type="bibr" rid="pcbi.1003793-Dai1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-DeBoer1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003793-Trenholm1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Cocco1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Bloomfield1">[52]</xref>. In either case, as long as the recurrent coupling is not too strong, the model sensory pathway will still accurately encode sparse stimuli. Therefore, the results of this work may more broadly apply to various types of ganglion cells, exhibiting diverse types of connectivity.</p>
<p>Clearly, for a CS network to be dynamically responsive in capturing transient stimuli, the system should be able to rapidly sample the stimulus within a sufficiently short time interval from the stimulus onset for the CS reconstruction. As shown in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4e</xref>, the reconstruction error drops precipitously until the sampling time increases to about 25 ms, and then remains approximately steady. The 25 ms time scale agrees with typical sensory time scales <xref ref-type="bibr" rid="pcbi.1003793-Amano1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Ando1">[54]</xref>. To address the possibility of minor distortions of information along sensory pathways, we further address how the performance of a CS network is degraded in the presence of noise. As shown in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4f</xref>, we find that the relative reconstruction error grows approximately linearly with the variance of the Gaussian noise added to each stimulus component, demonstrating that a recognizable reconstruction is still achievable even in the presence of relatively high-variance noise.</p>
<p>Since thus far we have used a fixed number of sensory neurons, a natural question to ask is how the performance of a CS network improves as the number of sensory neurons increases. <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4g</xref> shows that the performance will in fact improve with additional sensory neurons given a fixed number of receptors and corresponding optimal convergence. Since the reconstruction quality improves significantly less with the addition of sufficiently many sensory neurons, we observe that adding too many sensory neurons may be wasteful from a computational point of view, further justifying the optimality of sensory pathway architecture in processing sparse stimuli.</p>
<p>Hypothesizing that randomness is a key aspect in CS network sampling, we examine a central question of just how randomly sensory neurons need to sample the stimulus in order to achieve optimal sparsity encoding. To answer this question, we first design the connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e098" xlink:type="simple"/></inline-formula> so that all sensory neurons sample receptors from a regular grid. Then, we sequentially remove an original connection in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e099" xlink:type="simple"/></inline-formula>, and replace it by a connection between a randomly chosen receptor and the same sensory neuron. (See Methods Section for details.) From <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4h</xref>, we see that the error decreases rapidly until <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e100" xlink:type="simple"/></inline-formula> of the initial regular connections have been rewired, and then slowly levels off. Therefore, some degree of randomness is in fact necessary for a viable reconstruction, however the sampling need not be completely random for successful sparsity encoding. In fact, the success of intermediate levels of randomness may help to explain how the localized sampling in receptive fields further improves the performance of the CS network, which we will address later in this section.</p>
<p>Next, we investigate the characteristics of sensory-neuron spike dynamics that are significant in these sparsity-encoding CS networks. We find that the parameter regimes yielding the least error in the stimulus reconstructions are those in which the largest degree of variability or disorder exists among the dynamics of the model sensory neurons. We compute the average sensory network membrane potential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e101" xlink:type="simple"/></inline-formula>, which roughly models the network local field potential (“LFP”) signal measured experimentally <xref ref-type="bibr" rid="pcbi.1003793-Mitzdorf1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Henrie1">[56]</xref>, to give an indication of the variability in network dynamics. In <xref ref-type="fig" rid="pcbi-1003793-g005">Fig. 5a</xref>, we plot the “LFP” correlation time as a function of the convergence of the receptor-sensory-neuron connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e102" xlink:type="simple"/></inline-formula>. It is clear that the “LFP” decorrelates the fastest at the optimal convergence value, indicating relatively aperiodic network dynamics. To quantify the corresponding network information content, we compute the entropy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e103" xlink:type="simple"/></inline-formula>, of the spike train produced by the network of neurons <disp-formula id="pcbi.1003793.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e104" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e105" xlink:type="simple"/></inline-formula> denotes the probability distribution of the interspike-interval (ISI) lengths, computed from a binned histogram of ISI's collected from each sensory neuron in the network. In our case, the entropy of the ISIs measures the spike-train information capacity, and therefore gives an indication as to how much possible information can be encoded by the sensory-neuron network over the time-scale of network activity <xref ref-type="bibr" rid="pcbi.1003793-Rieke1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Nemenman1">[58]</xref>. This entropy reaches its maximum at the optimal convergence, as displayed in <xref ref-type="fig" rid="pcbi-1003793-g005">Fig. 5b</xref>, thereby transmitting the maximum amount of information. It is important to remark that while we specifically use firing rates to reconstruct stimuli, information about the actual sensory-neuron spike trains is embedded in the firing rate statistics. Since the firing rate gives the lowest order of information regarding the ISI distribution, the ISI distribution is of particular interest in quantifying the information encoded by sensory-neuron activity.</p>
<fig id="pcbi-1003793-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g005</object-id><label>Figure 5</label><caption>
<title>CS network dynamics for optimal signal encoding.</title>
<p>Impact of the convergence of the sensory-cell to receptor connection matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e106" xlink:type="simple"/></inline-formula>, on local-field-potential (“LFP”) and spike-train statistics: <bold>a</bold> correlation time of the sensory-cell network “LFP”, <bold>b</bold> entropy of the sensory-cell network interspike-interval (ISI) histogram, <bold>c</bold> variance of the sensory-cell network ISI distribution, and <bold>d</bold> skewness and kurtosis of the sensory-cell network ISI distribution.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g005" position="float" xlink:type="simple"/></fig>
<p>In examining the distribution of the ISI's, we observe a rich firing structure among the sensory neurons at the optimal convergence of connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e107" xlink:type="simple"/></inline-formula>. We demonstrate in <xref ref-type="fig" rid="pcbi-1003793-g005">Fig. 5c</xref> how the variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e108" xlink:type="simple"/></inline-formula> of the ISI distribution depends on the convergence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e109" xlink:type="simple"/></inline-formula>. (Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e110" xlink:type="simple"/></inline-formula> denotes the mean over the distribution.) This variance is clearly maximal at the optimal convergence. Moreover, the ISI structure is further characterized by its near-Gaussian distribution at optimal convergence value, as shown in <xref ref-type="fig" rid="pcbi-1003793-g005">Fig. 5d</xref>, reaching its minimal skewness, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e111" xlink:type="simple"/></inline-formula>, and kurtosis, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e112" xlink:type="simple"/></inline-formula>, which vanish for the Gaussian distribution. From these observations, it is clear that the connectivity between the receptors and sensory neurons plays a large role in determining the information content of the sensory neuron spike dynamics, and by maximizing the information content of these spikes, stimuli may be optimally encoded.</p>
</sec><sec id="s2c">
<title>Biological Extensions</title>
<p>We further corroborate the hypothesis that evolution may have driven early sensory pathways to become CS networks by incorporating a biologically realistic feature, i.e., localized receptive fields, into our model CS network. We discover that this feature indeed improves the performance of the highly idealized CS network we have investigated so far. We model such a receptive field by using a variant of the model in which each sensory neuron samples receptors primarily within a small area, which is closer to biological realism than random sampling <xref ref-type="bibr" rid="pcbi.1003793-Wiesel1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Hubel1">[19]</xref>. In particular, if the coordinates of the pixel representing a receptor are given by the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e113" xlink:type="simple"/></inline-formula> and the coordinates of the receptive-field center corresponding to a chosen sensory neuron are given by the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e114" xlink:type="simple"/></inline-formula>, we take the probability that a connection will exist between the two as <disp-formula id="pcbi.1003793.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e115" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e116" xlink:type="simple"/></inline-formula> represents the probability of a connection if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e117" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e118" xlink:type="simple"/></inline-formula> is the size of the sensory neuron's receptive field in the units of pixel size. A schematic illustration of this type of sampling is depicted in <xref ref-type="fig" rid="pcbi-1003793-g006">Fig. 6a</xref>, with the parameter values, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e119" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e120" xlink:type="simple"/></inline-formula>, resulting in a convergence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e121" xlink:type="simple"/></inline-formula> of 25. A reconstruction of the original image from the firing rates produced by this model is shown in <xref ref-type="fig" rid="pcbi-1003793-g006">Fig. 6b</xref>. Note that we found the error of this reconstruction to be 0.19, which is much less than the error of 0.35 we obtained for completely random stimulus-sampling over the entire receptor pool, shown in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4a</xref>. This result, reaching beyond the conventional CS theory, underscores the importance of the local-receptive-field architecture in the evolution of the CS properties of sensory pathways.</p>
<fig id="pcbi-1003793-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g006</object-id><label>Figure 6</label><caption>
<title>Localized receptive fields.</title>
<p><bold>a</bold> Stimulus sampling via localized receptive fields. <bold>b</bold> The corresponding CS stimulus reconstruction.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g006" position="float" xlink:type="simple"/></fig>
<p>We remark that we can also reconstruct moving stimuli using our CS approach. The reconstruction of a ten-snapshot image sequence spaced 25 ms apart is displayed in <xref ref-type="fig" rid="pcbi-1003793-g007">Fig. 7</xref>. In reconstructing each image frame, we only use spikes counted during the time-course over which each respective image is presented. In this way, we consider ten CS recovery problems, with each corresponding to a separate set of observed firing rates. From the highly accurate reconstructions even in the case of moving stimuli, it is clear that the CS architecture is therefore feasible for natural environments in which stimuli are constantly in motion. Moreover, if the image frames instead change every 200 ms, the average reconstruction quality is improved further. As in the case of realistic retinal video processing, correlations between frames and close corresponding dynamical regimes therefore allow for rapid encoding of changing stimuli <xref ref-type="bibr" rid="pcbi.1003793-FabreThorpe1">[59]</xref>–<xref ref-type="bibr" rid="pcbi.1003793-Thorpe1">[61]</xref>.</p>
<fig id="pcbi-1003793-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003793.g007</object-id><label>Figure 7</label><caption>
<title>Moving stimulus reconstruction.</title>
<p><bold>a</bold> A moving stimulus and <bold>b</bold> its reconstruction. Localized receptive fields were used; the average relative error over the frames is 0.24. Image frames were presented for 25 ms each.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003793.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We hypothesize that the CS principle for sparse-stimulus transmission in neuronal networks, as demonstrated in our computational model, should also hold in real neuronal systems in the brain. In more general settings, the underlying linear structure could be recovered using the first-order Wiener kernel from non-linear systems analysis for the entire network <xref ref-type="bibr" rid="pcbi.1003793-Wiener1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Victor1">[63]</xref>. Similarly, in deriving an input-output relationship outside of the mean-driven regime, a linear-nonlinear (LN) model can also be developed through application of a linear spatiotemporal filter and a static nonlinear transformation (e.g. sigmoidal function), which often can be cast in a linear form if the inverse of the nonlinear transformation exists <xref ref-type="bibr" rid="pcbi.1003793-Ostojic1">[64]</xref>. In either case, once the underlying linear structure is discovered, the presented methodology could in principle be used to attempt to reconstruct sparse stimuli using very few neuronal output measurements.</p>
<p>Mathematically, this work suggests two important extensions to conventional CS theory. First, while compressed sensing is traditionally applied to static linear systems, we demonstrate one possible way of generalizing this theory to dynamical systems that model a large number of interacting agents. Second, the improvement in stimulus encoding yielded by localized random sampling akin to receptive fields suggests that alternative sampling schemes, aside from purely random sampling, may in fact yield better reconstructions so long as there is a sufficient degree of incoherence in the samples such that CS is applicable. From this standpoint, measurement devices engineered with localized random sampling may be able to more successfully encode signals than by applying the completely random sampling conventionally used in compressive sensing data acquisition <xref ref-type="bibr" rid="pcbi.1003793-Duarte1">[65]</xref>. Likewise, engineered devices sampling the output of a time-evolving network may also have the capacity to reconstruct network input using compressive sensing combined with an underlying linear input-output network structure similar to the neuronal network studied in this work.</p>
<p>Finally, we point out a new way of looking at the mathematical framework of sparsity. Our findings give rise to a network definition of stimulus sparsity, freeing this concept from any dependence on the particular choice of wavenumber-space or other component-space transform as in conventional definitions of sparsity. In particular, we can define a stimulus as sparse if it can be accurately and efficiently transmitted through a sensory-pathway-type network, such as one that allows for a significant reduction in the numbers of downstream sensory neurons versus the numbers of upstream receptors. This alternative definition of sparsity therefore directly relates the structure of a stimulus to the type of network dynamics it evokes. Rather than indicating sparsity by the number of non-zero signal components, sparsity can alternatively be determined in the network framework by the amount of stimulus information embedded in the evoked network dynamics. Thus, visual images are clearly sparse according to both the networks that sample them completely randomly and those with localized receptive fields.</p>
<p>In the long way towards understanding how the brain forms a specific percept from a given stimulus, one must first understand how the brain samples this stimulus. Our aim here was to examine the hypothesis that the CS principle has evolved to govern the information transduction and retention of sparse stimuli in a sensory pathway while achieving a great reduction in the number of sensory neurons. Our work shows that this hypothesis indeed successfully captures information propagation in our model sensory network. In particular, our results on these network characteristics may provide insight into the CS properties of corresponding networks in the brain.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>1 Correlation Time Definition</title>
<p>The correlation time gives the expected amount of time necessary for signal responses to become decorrelated and is defined mathematically as <disp-formula id="pcbi.1003793.e122"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e122" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e123" xlink:type="simple"/></inline-formula> is the correlation function of the “LFP” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e124" xlink:type="simple"/></inline-formula>, with <disp-formula id="pcbi.1003793.e125"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e125" xlink:type="simple"/></disp-formula>and time average of the “LFP” is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e126" xlink:type="simple"/></inline-formula>. A short correlation time implies less periodicity and therefore greater variability in the “LFP”.</p>
</sec><sec id="s4b">
<title>2 Compressed Sensing Reconstruction</title>
<p>To reconstruct a stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e127" xlink:type="simple"/></inline-formula>, from the sensory-neuron firing rates, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e128" xlink:type="simple"/></inline-formula>, we first cast the linearized firing-rate model (2) into a form to which compressed sensing may be applied. To apply compressed sensing in recovering a sparse representation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e129" xlink:type="simple"/></inline-formula>, we consider the vectorization of the two-dimensional discrete cosine transform of the stimulus pixel matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e130" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e131" xlink:type="simple"/></inline-formula> denotes the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e132" xlink:type="simple"/></inline-formula> Kronecker product <disp-formula id="pcbi.1003793.e133"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e133" xlink:type="simple"/></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e134" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e135" xlink:type="simple"/></inline-formula>, one-dimensional discrete cosine transform matrix with entries <disp-formula id="pcbi.1003793.e136"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e136" xlink:type="simple"/></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e137" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e138" xlink:type="simple"/></inline-formula>. In solving the related problem of recovering <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e139" xlink:type="simple"/></inline-formula>, the linear model we consider is <disp-formula id="pcbi.1003793.e140"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e140" xlink:type="simple"/></disp-formula> <disp-formula id="pcbi.1003793.e141"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e141" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>Since the cosine transform of the stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e142" xlink:type="simple"/></inline-formula>, is sparse and the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e143" xlink:type="simple"/></inline-formula> is random, recovering <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e144" xlink:type="simple"/></inline-formula> is reduced to minimizing the sum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e145" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Candes1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Candes2">[7]</xref> under the constraint (3). Solving this minimization problem is equivalent to solving the well-known <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e146" xlink:type="simple"/></inline-formula> optimization problem <disp-formula id="pcbi.1003793.e147"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e147" xlink:type="simple"/></disp-formula> <disp-formula id="pcbi.1003793.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003793.e148" xlink:type="simple"/></disp-formula>under the constraint (3). We solve this optimization problem with a greedy algorithm, known as the Orthogonal Matching Pursuit <xref ref-type="bibr" rid="pcbi.1003793-Tropp1">[48]</xref>. Once <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e149" xlink:type="simple"/></inline-formula> is recovered, we recover the stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e150" xlink:type="simple"/></inline-formula> using the inverted two-dimensional discrete cosine transform of the matrix representation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e151" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c">
<title>3 Regular versus Random Sampling</title>
<p>As mentioned in the main body of this paper, CS theory posits that random sampling of sparse images significantly reduces the sampling rate as compared to uniform sampling of finite-bandwidth stimuli, while yielding the same quality of the reproduction. In particular, for uniform sampling, the Shannon-Nyquist theorem requires that finite-bandwidth stimuli must be sampled at the rate of twice their bandwidth in order to achieve a faithful reconstruction <xref ref-type="bibr" rid="pcbi.1003793-Shannon1">[8]</xref>. In our case, this would mean sampling by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e152" xlink:type="simple"/></inline-formula> sensory neurons when all the frequencies are represented in the image used as the stimulus. This is because we need to capture each Fourier mode represented in the stimulus in at least two points. On the other hand, the compressed-sensing theory implies that much less frequent sampling should suffice for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e153" xlink:type="simple"/></inline-formula>-sparse stimuli to perfectly reconstruct the stimulus with probability one, in particular, on the order of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e154" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003793-Candes1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003793-Candes2">[7]</xref>, provided the sampling is sufficiently random. Again, this is because, with probability one, we will thus capture each represented Fourier mode in two points. This is certainly not true if we sample the stimulus on a regular, coarse grid with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e155" xlink:type="simple"/></inline-formula> points in the spirit of the Shannon-Nyquist theorem, unless the stimulus contains nothing but the lowest <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e156" xlink:type="simple"/></inline-formula> modes. In fact, such sampling diminishes the resolution. We here elaborate on the illustration of this fact, as depicted in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4h</xref>.</p>
<p>In regularly sampling the stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e157" xlink:type="simple"/></inline-formula>, sensory neurons sample only receptors that lay on a coarse grid contained within the finer grid of receptors, modeled by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e158" xlink:type="simple"/></inline-formula> pixel matrix representation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e159" xlink:type="simple"/></inline-formula>. The coarse regular grid, say of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003793.e160" xlink:type="simple"/></inline-formula>, is composed of all even-numbered row and column entries of the finer pixel matrix. Fixing convergence at its optimal level, the regular sampling scheme randomly connects sensory neurons to receptors on the coarse grid. To add more randomness and a larger variety of frequency modes to the sampling scheme, we randomly select sensory neurons connected to receptors on the regular grid, and then randomly rewire them with any of the receptors on the pixel matrix.</p>
<p>As displayed in <xref ref-type="fig" rid="pcbi-1003793-g004">Fig. 4h</xref>, even if the sampling is random on a coarse grid, not enough frequency modes may be captured to yield a faithful signal reproduction. Upon random rewirings to the finer grid of receptors, more frequency components may be detected, thereby improving the quality of the reconstruction. However, once the sampling scheme is sufficiently random and enough variety in frequency modes is captured, an accurate reconstruction can be achieved with little improvement following additional rewirings.</p>
</sec></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003793-Field1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1994</year>) <article-title>What is the goal of sensory coding</article-title>? <source>Neural Computation</source> <volume>6</volume>: <fpage>559</fpage>–<lpage>601</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Barlow1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name> (<year>1981</year>) <article-title>The ferrier lecture, 1980. critical limiting factors in the design of the eye and visual cortex</article-title>. <source>Proc R Soc Lond B Biol Sci</source> <volume>212</volume>: <fpage>1</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Buck1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buck</surname><given-names>LB</given-names></name> (<year>1996</year>) <article-title>Information coding in the vertebrate olfactory system</article-title>. <source>Annu Rev Neurosci</source> <volume>19</volume>: <fpage>517</fpage>–<lpage>544</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Barlow2"><label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">Barlow HB (1961) The coding of sensory messages. In: Thorpe WH, Zangwill OL, Current Problems in Animal Behaviour, Cambridge University Press. pp. 331–360.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Barlow3"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name> (<year>2001</year>) <article-title>Redundancy reduction revisited</article-title>. <source>Network</source> <volume>12</volume>: <fpage>241</fpage>–<lpage>253</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Candes1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Candes</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Romberg</surname><given-names>JK</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>T</given-names></name> (<year>2006</year>) <article-title>Stable signal recovery from incomplete and inaccurate measurements</article-title>. <source>Communications on Pure and Applied Mathematics</source> <volume>59</volume>: <fpage>1207</fpage>–<lpage>1223</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Candes2"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Candes</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Wakin</surname><given-names>MB</given-names></name> (<year>2008</year>) <article-title>An Introduction To Compressive Sampling</article-title>. <source>Signal Processing Magazine, IEEE</source> <volume>25</volume>: <fpage>21</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Shannon1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname><given-names>CE</given-names></name> (<year>1949</year>) <article-title>Communication in the Presence of Noise</article-title>. <source>Proceedings of the IRE</source> <volume>37</volume>: <fpage>10</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Gross1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gross</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>YK</given-names></name>, <name name-style="western"><surname>Flammia</surname><given-names>ST</given-names></name>, <name name-style="western"><surname>Becker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Eisert</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Quantum state tomography via compressed sensing</article-title>. <source>Phys Rev Lett</source> <volume>105</volume>: <fpage>150401</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Lustig1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lustig</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Pauly</surname><given-names>JM</given-names></name> (<year>2007</year>) <article-title>Sparse MRI: The application of compressed sensing for rapid MR imaging</article-title>. <source>Magn Reson Med</source> <volume>58</volume>: <fpage>1182</fpage>–<lpage>1195</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Dai1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dai</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sheikh</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Milenkovic</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Baraniuk</surname><given-names>RG</given-names></name> (<year>2009</year>) <article-title>Compressive sensing DNA microarrays</article-title>. <source>J Bioinform Syst Biol</source> <volume>2009</volume>: <fpage>162824</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Berger1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berger</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>S</given-names></name> (<year>2010</year>) <article-title>Application of compressive sensing to sparse channel estimation</article-title>. <source>Comm Mag</source> <volume>48</volume>: <fpage>164</fpage>–<lpage>174</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Herman1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herman</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Strohmer</surname><given-names>T</given-names></name> (<year>2009</year>) <article-title>High-resolution radar via compressed sensing</article-title>. <source>Trans Sig Proc</source> <volume>57</volume>: <fpage>2275</fpage>–<lpage>2284</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Isley1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Isley</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Hillar</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name> (<year>2010</year>) <article-title>Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</article-title>. In: <source>NIPS. Curran Associates, Inc</source>. pp. <fpage>910</fpage>–<lpage>918</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ganguli1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2012</year>) <article-title>Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis</article-title>. <source>Annu Rev Neurosci</source> <volume>35</volume>: <fpage>485</fpage>–<lpage>508</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Coulter1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coulter</surname><given-names>WK</given-names></name>, <name name-style="western"><surname>Hillar</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Isley</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name> (<year>2010</year>) <article-title>Adaptive compressed sensing - a new class of self-organizing coding models for neuroscience</article-title>. In: <source>ICASSP. IEEE</source> pp. <fpage>5494</fpage>–<lpage>5497</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ganguli2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Short-term memory in neuronal networks through dynamical compressed sensing</article-title>. In: <source>NIPS. Curran Associates, Inc</source>. pp. <fpage>667</fpage>–<lpage>675</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Wiesel1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1960</year>) <article-title>Receptive fields of ganglion cells in the cat's retina</article-title>. <source>J Physiol</source> <volume>153</volume>: <fpage>583</fpage>–<lpage>594</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Hubel1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1960</year>) <article-title>Receptive fields of optic nerve fibres in the spider monkey</article-title>. <source>J Physiol</source> <volume>154</volume>: <fpage>572</fpage>–<lpage>580</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Olshausen1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1996</year>) <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Olshausen2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1997</year>) <article-title>Sparse coding with an overcomplete basis set: a strategy employed by V1</article-title>? <source>Vision Res</source> <volume>37</volume>: <fpage>3311</fpage>–<lpage>3325</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Wu1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>SM</given-names></name> (<year>2010</year>) <article-title>Synaptic organization of the vertebrate retina: general principles and species-specific variations: the friedenwald lecture</article-title>. <source>Invest Ophthalmol Vis Sci</source> <volume>51</volume>: <fpage>1263</fpage>–<lpage>1274</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Burkitt1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burkitt</surname><given-names>AN</given-names></name> (<year>2006</year>) <article-title>A review of the integrate-and-fire neuron model: I. homogeneous synaptic input</article-title>. <source>Biol Cybern</source> <volume>95</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Mirollo1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mirollo</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Strogatz</surname><given-names>SH</given-names></name> (<year>1990</year>) <article-title>Synchronization of pulse-coupled biological oscillators</article-title>. <source>SIAM Journal on Applied Mathematics</source> <volume>50</volume>: <fpage>1645</fpage>–<lpage>1662</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Somers1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Somers</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sur</surname><given-names>M</given-names></name> (<year>1995</year>) <article-title>An emergent model of orientation selectivity in cat visual cortical simple cells</article-title>. <source>Journal of Neuroscience</source> <volume>15</volume>: <fpage>5448</fpage>–<lpage>5465</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Mather1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mather</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Bennett</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Hasty</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tsimring</surname><given-names>LS</given-names></name> (<year>2009</year>) <article-title>Delay-induced degrade-and-fire oscillations in small genetic circuits</article-title>. <source>Phys Rev Lett</source> <volume>102</volume>: <fpage>068105</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Wang1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Cheng</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>L</given-names></name> (<year>2010</year>) <article-title>Review of pulse-coupled neural networks</article-title>. <source>Image and Vision Computing</source> <volume>28</volume>: <fpage>5</fpage>–<lpage>13</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Cai1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rangan</surname><given-names>A</given-names></name>, <name name-style="western"><surname>McLaughlin</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>Architectural and synaptic mechanisms underlying coherent spontaneous activity in V1</article-title>. <source>Proc Nat'l Acad Sci (USA)</source> <volume>102</volume>: <fpage>5868</fpage>–<lpage>5873</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Rangan1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rangan</surname><given-names>AV</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>D</given-names></name>, <name name-style="western"><surname>McLaughlin</surname><given-names>DW</given-names></name> (<year>2005</year>) <article-title>Modeling the spatiotemporal cortical activity associated with the line-motion illusion in primary visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>102</volume>: <fpage>18793</fpage>–<lpage>18800</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Zhou1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rangan</surname><given-names>AV</given-names></name>, <name name-style="western"><surname>McLaughlin</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>D</given-names></name> (<year>2013</year>) <article-title>Spatiotemporal dynamics of neuronal population response in the primary visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>110</volume>: <fpage>9517</fpage>–<lpage>9522</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Brette1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brette</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rudolph</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Carnevale</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hines</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Beeman</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Simulation of networks of spiking neurons: A review of tools and strategies</article-title>. <source>J Comput Neurosci</source> <volume>23</volume>: <fpage>349</fpage>–<lpage>398</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Newhall1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newhall</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Kovačič</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kramer</surname><given-names>PR</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Cascade-induced synchrony in stochastically-driven neuronal networks</article-title>. <source>Phys Rev E</source> <volume>82</volume>: <fpage>041903</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-DeBoer1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeBoer</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Vaney</surname><given-names>DI</given-names></name> (<year>2005</year>) <article-title>Gap-junction communication between subtypes of direction-selective ganglion cells in the developing retina</article-title>. <source>J Comp Neurol</source> <volume>482</volume>: <fpage>85</fpage>–<lpage>93</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Trong1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trong</surname><given-names>PK</given-names></name>, <name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name> (<year>2008</year>) <article-title>Origin of correlated activity between parasol retinal ganglion cells</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>1343</fpage>–<lpage>1351</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Trenholm1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trenholm</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schwab</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Awatramani</surname><given-names>GB</given-names></name> (<year>2013</year>) <article-title>Lag normalization in an electrically coupled neural network</article-title>. <source>Nat Neurosci</source> <volume>16</volume>: <fpage>154</fpage>–<lpage>156</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Feller1"><label>36</label>
<mixed-citation publication-type="book" xlink:type="simple">Feller W (1968) An Introduction to Probability Theory and Its Applications. New York: John Wiley.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Dunn1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunn</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name> (<year>2006</year>) <article-title>The impact of photoreceptor noise on retinal gain controls</article-title>. <source>Curr Opin Neurobiol</source> <volume>16</volume>: <fpage>363</fpage>–<lpage>370</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Thoreson1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thoreson</surname><given-names>WB</given-names></name> (<year>2007</year>) <article-title>Kinetics of synaptic transmission at ribbon synapses of rods and cones</article-title>. <source>Mol Neurobiol</source> <volume>36</volume>: <fpage>205</fpage>–<lpage>223</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Li1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>PH</given-names></name>, <name name-style="western"><surname>Verweij</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Schnapf</surname><given-names>JL</given-names></name> (<year>2012</year>) <article-title>Gap-junctional coupling of mammalian rod photoreceptors and its effect on visual detection</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>3552</fpage>–<lpage>3562</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Field2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Field</surname><given-names>GD</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2007</year>) <article-title>Information processing in the primate retina: circuitry and coding</article-title>. <source>Annu Rev Neurosci</source> <volume>30</volume>: <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Anderson1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>BW</given-names></name>, <name name-style="western"><surname>Watt</surname><given-names>CB</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>JH</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Exploring the retinal connectome</article-title>. <source>Mol Vis</source> <volume>17</volume>: <fpage>355</fpage>–<lpage>379</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Curcio1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Curcio</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Sloan</surname><given-names>KR</given-names></name>, <name name-style="western"><surname>Kalina</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Hendrickson</surname><given-names>AE</given-names></name> (<year>1990</year>) <article-title>Human photoreceptor topography</article-title>. <source>J Comp Neurol</source> <volume>292</volume>: <fpage>497</fpage>–<lpage>523</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Donoho1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donoho</surname><given-names>DL</given-names></name> (<year>2006</year>) <article-title>Compressed sensing</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>52</volume>: <fpage>1289</fpage>–<lpage>1306</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Rangan2"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rangan</surname><given-names>AV</given-names></name>, <name name-style="western"><surname>Cai</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Maximum-entropy closures for kinetic theories of neuronal network dynamics</article-title>. <source>Phys Rev Lett</source> <volume>96</volume>: <fpage>178101</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Cai2"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Shelley</surname><given-names>M</given-names></name>, <name name-style="western"><surname>McLaughlin</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>An effective representation of uctuation-driven neuronal networks with application to simple &amp; complex cells in visual cortex</article-title>. <source>Pro Nat Acad Sci (USA)</source> <volume>101</volume>: <fpage>7757</fpage>–<lpage>7762</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Zaghloul1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zaghloul</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Boahen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Demb</surname><given-names>JB</given-names></name> (<year>2003</year>) <article-title>Different circuits for ON and OFF retinal ganglion cells cause different contrast sensitivities</article-title>. <source>J Neurosci</source> <volume>23</volume>: <fpage>2645</fpage>–<lpage>2654</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Chen1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>SS</given-names></name>, <name name-style="western"><surname>Donoho</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Michael, Saunders</surname><given-names>A</given-names></name> (<year>1998</year>) <article-title>Atomic decomposition by basis pursuit</article-title>. <source>SIAM Journal on Scientific Computing</source> <volume>20</volume>: <fpage>33</fpage>–<lpage>61</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Tropp1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tropp</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Gilbert</surname><given-names>AC</given-names></name> (<year>2007</year>) <article-title>Signal Recovery From Random Measurements Via Orthogonal Matching Pursuit</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>53</volume>: <fpage>4655</fpage>–<lpage>4666</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ganmor1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganmor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>The architecture of functional interaction networks in the retina</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>3044</fpage>–<lpage>3054</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ganmor2"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganmor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>108</volume>: <fpage>9679</fpage>–<lpage>9684</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Cocco1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cocco</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Leibler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Monasson</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>106</volume>: <fpage>14058</fpage>–<lpage>14062</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Bloomfield1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bloomfield</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Volgyi</surname><given-names>B</given-names></name> (<year>2009</year>) <article-title>The diverse functional roles and regulation of neuronal gap junctions in the retina</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>495</fpage>–<lpage>506</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Amano1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amano</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Goda</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Nishida</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ejima</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Takeda</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Estimation of the timing of human visual perception from magnetoencephalography</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>3981</fpage>–<lpage>3991</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ando1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ando</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Yamada</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kokubu</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Reaction time to peripheral visual stimuli during exercise under hypoxia</article-title>. <source>J Appl Physiol</source> <volume>108</volume>: <fpage>1210</fpage>–<lpage>1216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Mitzdorf1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mitzdorf</surname><given-names>U</given-names></name> (<year>1985</year>) <article-title>Current source-density method and application in cat cerebral cortex: investigation of evoked potentials and EEG phenomena</article-title>. <source>Physiol Rev</source> <volume>65</volume>: <fpage>37</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Henrie1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henrie</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>Lfp power spectra in v1 cortex: the graded effect of stimulus contrast</article-title>. <source>J Neurophysiol</source> <volume>94</volume>: <fpage>479</fpage>–<lpage>490</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Rieke1"><label>57</label>
<mixed-citation publication-type="book" xlink:type="simple">Rieke F, Warland D, de Ruyter van Steveninck R, Bialek W (1996) Spikes: Exploring the Neural Code. Computational Neuroscience. Cambridge: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Nemenman1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Entropy and information in neural spike trains: Progress on the sampling problem</article-title>. <source>Phys Rev E</source> <volume>69</volume>: <fpage>056111</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-FabreThorpe1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fabre-Thorpe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Delorme</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Marlot</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2001</year>) <article-title>A limit to the speed of processing in ultra-rapid visual categorization of novel natural scenes</article-title>. <source>J Cogn Neurosci</source> <volume>13</volume>: <fpage>171</fpage>–<lpage>180</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Rolls1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Tovee</surname><given-names>MJ</given-names></name> (<year>1994</year>) <article-title>Processing speed in the cerebral cortex and the neurophysiology of visual masking</article-title>. <source>Proc Biol Sci</source> <volume>257</volume>: <fpage>9</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Thorpe1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname><given-names>C</given-names></name> (<year>1996</year>) <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>520</fpage>–<lpage>522</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Wiener1"><label>62</label>
<mixed-citation publication-type="book" xlink:type="simple">Wiener N (1958) Nonlinear Problems in Random Theory. Technology Press Research Monographs. Cambridge: The Technology Press of Massachusetts Institute of Technology and John Wiley &amp; Sons.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Victor1"><label>63</label>
<mixed-citation publication-type="book" xlink:type="simple">Victor J (1992) Nonlinear systems analysis in vision: Overview of kernel methods. In: RB Pinter BN, Nonlinear vision: Determination of Neural Receptive Fields, Function, and Networks, Boca Raton: CRC Press. pp. 1–37.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Ostojic1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ostojic</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2011</year>) <article-title>From spiking neuron models to linear-nonlinear models</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1001056</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003793-Duarte1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Duarte</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Davenport</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Takhar</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Laska</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Single-Pixel Imaging via Compressive Sampling</article-title>. <source>Signal Processing Magazine, IEEE</source> <volume>25</volume>: <fpage>83</fpage>–<lpage>91</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>