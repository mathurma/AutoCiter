<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-1923R2</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000930</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Psychology</subject><subject>Neuroscience/Natural and Synthetic Vision</subject></subj-group></article-categories><title-group><article-title>Evolution and Optimality of Similar Neural Mechanisms for Perception and Action during Search</article-title><alt-title alt-title-type="running-head">Similar Neural Mechanisms for Perception &amp; Action</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>Sheng</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Eckstein</surname><given-names>Miguel P.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1">          <addr-line>Vision and Image Understanding, Department of Psychology, University of California, Santa Barbara, California, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Maloney</surname><given-names>Laurence T.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">New York University, United States of America</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">eckstein@psych.ucsb.edu</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: SZ MPE. Performed the experiments: SZ MPE. Analyzed the data: SZ MPE. Contributed reagents/materials/analysis tools: SZ MPE. Wrote the paper: SZ MPE.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>9</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>9</day><month>9</month><year>2010</year></pub-date><volume>6</volume><issue>9</issue><elocation-id>e1000930</elocation-id><history>
<date date-type="received"><day>12</day><month>3</month><year>2010</year></date>
<date date-type="accepted"><day>10</day><month>8</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Zhang, Eckstein</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>A prevailing theory proposes that the brain's two visual pathways, the ventral and dorsal, lead to differing visual processing and world representations for conscious perception than those for action. Others have claimed that perception and action share much of their visual processing. But which of these two neural architectures is favored by evolution? Successful visual search is life-critical and here we investigate the evolution and optimality of neural mechanisms mediating perception and eye movement actions for visual search in natural images. We implement an approximation to the ideal Bayesian searcher with two separate processing streams, one controlling the eye movements and the other stream determining the perceptual search decisions. We virtually evolved the neural mechanisms of the searchers' two separate pathways built from linear combinations of primary visual cortex receptive fields (V1) by making the simulated individuals' probability of survival depend on the perceptual accuracy finding targets in cluttered backgrounds. We find that for a variety of targets, backgrounds, and dependence of target detectability on retinal eccentricity, the mechanisms of the searchers' two processing streams converge to similar representations showing that mismatches in the mechanisms for perception and eye movements lead to suboptimal search. Three exceptions which resulted in partial or no convergence were a case of an organism for which the targets are equally detectable across the retina, an organism with sufficient time to foveate all possible target locations, and a strict two-pathway model with no interconnections and differential pre-filtering based on parvocellular and magnocellular lateral geniculate cell properties. Thus, similar neural mechanisms for perception and eye movement actions during search are optimal and should be expected from the effects of natural selection on an organism with limited time to search for food that is not equi-detectable across its retina and interconnected perception and action neural pathways.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>The brain has two processing pathways of visual information, the ventral and dorsal streams. A prevailing theory proposes that this division leads to different world representations for conscious perception than those for actions such as grasping or eye movements. Perceptual tasks such as searching for our car keys in a living room requires the brain to coordinate eye movement actions to point the high resolution center of the eye, the fovea, to regions of interest in the scene to extract information used for a subsequent decision, such as identifying or localizing the keys. Does having different neural representations of the world for eye movement actions and perception have any costs for performance during visual search? We use computer vision algorithms that simulate components of the human visual system with the two separate processing streams and search for simple targets added to thousands of natural images. We simulate the process of evolution to show that the neural mechanisms of the perception and action processing streams co-evolve similar representations of the target suggesting that discrepancies in the neural representations of the world for perception and eye movements lead to lower visual search performance and are not favored by evolution.</p>
</abstract><funding-group><funding-statement>This research was supported by NSF 0819592 to MPE. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="11"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Neurophysiology studies of the macaque monkey <xref ref-type="bibr" rid="pcbi.1000930-Ungerleider1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1000930-Snyder1">[3]</xref> support the existence of two functionally distinct neural pathways in the brain mediating the processing of visual information. The behavior of patients with brain damage has led to the proposal that perception is mediated by the ventral stream projecting from the primary visual cortex to the inferior temporal cortex, and that action is mediated by the dorsal stream projecting from the primary visual cortex to the posterior parietal cortex <xref ref-type="bibr" rid="pcbi.1000930-Goodale1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1000930-Milner1">[6]</xref> (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1a</xref>). Although there has been debate about whether this separation into ventral/dorsal streams implies that the brain contains two distinct neural representations of the visual world <xref ref-type="bibr" rid="pcbi.1000930-Krliczak1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1000930-Krauzlis1">[12]</xref>, there has been no formal theoretical analysis about the functional consequences of the two different neural architectures on an animal's survival. Visual search requires animals to move their eyes to point the high-resolution region of the eye, the fovea, to potentially interesting regions of the scene to sub-serve perceptual decisions such as localizing food or a predator. What is the impact of having similar versus different neural mechanisms guiding eye movements and mediating perceptual decisions on visual search performance for an organism with a foveated visual system? We consider two leading computational models of multiple-fixation human visual search, the Bayesian ideal searcher (IS) <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref> and the ideal saccadic targeting model (maximum a posteriori probability, MAP <xref ref-type="bibr" rid="pcbi.1000930-Rao1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Beutter1">[17]</xref>) for a search task of a target in one of eight locations equidistant from initial fixation (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1b</xref>). The ideal searcher uses knowledge of how the detectability of a target varies with retinal eccentricity (visibility map) and statistics of the scenes to move the fovea to spatial locations which maximize the accuracy of the perceptual decision at the end of search <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref> (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1b</xref>). The saccadic targeting model (MAP) makes eye movements to the most probable target location <xref ref-type="bibr" rid="pcbi.1000930-Milner1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Beutter1">[17]</xref> which is optimal if the goal was to saccade to the target rather than collect information to optimize a subsequent perceptual decision <xref ref-type="bibr" rid="pcbi.1000930-Ungerleider1">[1]</xref> (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1b</xref>). Depending on the spatial layout of the possible target locations and the visibility map, the IS and MAP strategies lead to similar (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref>) or diverging eye-fixations (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1d–e</xref>). For example for a steeply varying visibility map (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref>) both models make eye movements to the possible target locations while for a broader visibility map (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1d–e</xref>) the ideal searcher tends to make eye movements in between the possible target locations attempting to obtain simultaneous close-to-fovea processing for more than one location. Covert attention allows both models to select possible target locations and ignore locations that are unlikely to contain the target when deciding on saccade endpoints and making perceptual search decisions <xref ref-type="bibr" rid="pcbi.1000930-Palmer1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Eckstein2">[19]</xref>. Perceptual target localization decisions for both models are based on visual information collected in parallel over the whole retina, temporally integrated across saccades, and based on the location with highest sensory evidence for the presence of the target. Critically, we implemented the models to have two processing pathways, one determining where to move the fovea and the other stream processing visual information to reach a final perceptual decision about the target location. Rather than having a single linear mechanism or perceptual template (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1b</xref>), each pathway in the model had its own neural mechanism which is compared to the incoming visual data at each possible target location. Likelihood ratios <xref ref-type="bibr" rid="pcbi.1000930-Green1">[20]</xref> of the observed responses for each of the mechanisms under the hypothesis that the target is present or absent at that location are used to make decisions about where to move the eyes and perceptual decisions (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p>
<fig id="pcbi-1000930-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g001</object-id><label>Figure 1.Virtual</label><caption>
<title>evolution of perception and saccade with different visibility maps, eye movement models and configurations.</title>
<p>a. Ventral (perception) and dorsal (action) streams projecting from the primary visual cortex (V1). b. Flow chart for two models of human eye-movement search: Ideal Bayesian Searcher (IS) and the Saccadic targeting model (maximum a posteriori probability model, MAP). c. 8 alternative forced choice target search for steep visibility map. d. 8 alternative forced choice target search for broad visibility map. e. 4 alternative forced choice target search for broad visibility map. Light blue circles outline possible target locations. Location of fixations for 1<sup>st</sup> (blue) and 2<sup>nd</sup> saccades (red) for three models: IS, MAP and Entropy Limit Minimization (ELM) in white noise The MAP model simulations include small random saccade endpoint errors to facilitate visualization of the different fixations. Central cross indicates initial fixation point for all models.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g001" xlink:type="simple"/></fig>
<p>We used a genetic algorithm as a method to find near-optimal solutions for perception and action mechanisms but also to simulate the effects of the evolutionary process of natural selection on the neural mechanisms driving saccadic eye movements and perceptual decisions during search. The computational complexity of the ideal Bayesian searcher makes it difficult to virtually evolve the model (see note 1 in <xref ref-type="supplementary-material" rid="pcbi.1000930.s004">Text S2</xref>) and thus we used a recently proposed approximation to the ideal searcher that is computationally faster (Entropy Limit Minimization, ELM <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Renninger1">[21]</xref>). The ELM model chooses the fixation location that minimizes the uncertainty of posterior probabilities over the potential target locations. The decision rule can be simplified to choose the fixation location with the maximum sum of likelihood ratios across potential target locations, each weighted by its squared detectability given the fixation location <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref>. The ELM model can be shown to approximate the fixation patterns of the ideal searcher <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref> and capture the main characteristics of the fixation patterns of the IS for our task and visibility maps (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c–e</xref>; ELM) (see note 2 in <xref ref-type="supplementary-material" rid="pcbi.1000930.s004">Text S2</xref>). The process of virtual evolution started with the creation of one thousand simulated individuals with separate linear mechanisms for perception (ventral) and eye movement programming (dorsal; <xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2a</xref>). Each pathway's template for each individual was created from independent random combinations of the receptive fields of twenty four V1 simple cells. Each simulated individual was allowed two eye movements (see note 3 in <xref ref-type="supplementary-material" rid="pcbi.1000930.s004">Text S2</xref>) before making a final perceptual search decision about the location of the target. Performance finding the target in one of eight locations for five thousand test-images (one thousand for natural images) was evaluated and the probability of survival of an individual was proportional to its performance accuracy. A new generation was then created from the surviving individuals through the process of reproduction, mutation and cross-over (<xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2a</xref>). The process was repeated for up to 500 generations.</p>
<fig id="pcbi-1000930-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g002</object-id><label>Figure 2</label><caption>
<title>Virtual evolution of two separate streams with the genetic algorithms for three different targets.</title>
<p>a. Virtual evolution of the perception (ventral stream) and saccade (dorsal stream) templates constructed from different linear combinations of twenty four different V1 simple cells which spanned the target (Gabor functions with center frequencies, 0.5, 1, 2, 4 cycles/degree for 6 different orientations, 30 degrees apart, and octave bandwidths). Probability of survival of an individual depends on search accuracy of the ideal searcher approximation (ELM model) with the two templates. b. Top row three different targets (from right to left: isotropic Gaussian, vertical elongated Gaussian and the difference of a vertical and horizontal elongated Gaussians) used in different evolution simulations for search in 1/f noise and a steep visibility map (See <xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref>, left). All targets are luminance grey patterns but are shown in pseudo-color and scaled for each image to maximize the use of the color scale.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g002" xlink:type="simple"/></fig></sec><sec id="s2">
<title>Results</title>
<p>We first evolved the ideal searcher approximation (ELM model) for different shape luminance targets (isotropic Gaussian, vertical elongated Gaussian and cross pattern consisting of a positive and negative polarity elongated Gaussian) embedded in 1/f noise and a steep visibility map (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref>). Irrespective of the target shape, virtual evolution led to converging perception (ventral) and saccade (dorsal) mechanisms that are similar to the target (<xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2b</xref>; see <xref ref-type="supplementary-material" rid="pcbi.1000930.s005">Video S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1000930.s006">Video S2</xref>, and <xref ref-type="supplementary-material" rid="pcbi.1000930.s007">Video S3</xref> for virtual evolution). To further investigate the generality of the result we evolved the ELM model to search a circular Gaussian target added to backgrounds with different statistical properties: white noise, 1/f noise and importantly, a calibrated set of natural image backgrounds <xref ref-type="bibr" rid="pcbi.1000930-vanHateren1">[22]</xref>. <xref ref-type="fig" rid="pcbi-1000930-g003">Figure 3</xref> (2<sup>nd</sup> row) presents the distribution of perceptual decision accuracies across individuals in a generation and shows that perceptual performances of simulated individuals in the population improve with generations and then converge to an asymptote. We characterized the similarity between the perception and saccade mechanisms by computing the correlations between the 2 dimensional linear mechanisms for each individual in each generation. <xref ref-type="fig" rid="pcbi-1000930-g003">Figure 3</xref> (3<sup>rd</sup> row) shows that the distribution of correlations across individuals in the population evolves to unity irrespective of the background type. To visualize in detail the shape of the evolved templates, we analyzed the radial profile of the templates of the highest performing simulated individuals in the last generation (<xref ref-type="fig" rid="pcbi-1000930-g003">Figure 3</xref>; 4<sup>th</sup> row). For all three backgrounds the saccade and perception templates converge to similar shapes (perception and saccade 2-D template correlations for the best performing templates in the last generation: 0.990±0.006, 0.986±0.013, 0.982±0.013). In addition, the linear mechanisms for the 1/f noise and natural scenes are narrower than those for the white noise and show an inhibitory surround (<xref ref-type="fig" rid="pcbi-1000930-g003">Figure 3</xref>).</p>
<fig id="pcbi-1000930-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g003</object-id><label>Figure 3</label><caption>
<title>Evolution plots for detecting the isotropic Gaussian target embedded in three different backgrounds.</title>
<p>1<sup>st</sup> row: Sample images for the 8 alternative forced choice (AFC) search task for an isotropic Gaussian shaped luminance target with a steep visibility map (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref> left) added to white noise, 1/f noise, and natural images. Center of circles indicate the possible target locations and the central cross is the initial fixation position for the models. 2<sup>nd</sup> row: Distribution of search accuracies for simulated individuals as a function of generation. 3<sup>rd</sup> row: Distribution of correlations between perception and saccade templates of individuals in each generation. Bottom row: Perception (red) and saccade (blue) templates radial profiles (averaged across all angles) of best performing simulated individual for each background type. Results are averages across ten different virtual evolution runs each with 500 generations. Plots only show data up to the 200<sup>th</sup> generation for which convergence has occurred. Radial profile of the Gaussian signal is shown in a dashed line for comparison.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g003" xlink:type="simple"/></fig>
<p>These previous results were based on a visibility map that steeply declines with eccentricity and rely on the assumption that humans are near-ideal searchers. We, thus, evolved the mechanisms for the case of a broader visibility map that is similar to that measured for human observers in 1/f noise <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref> (<xref ref-type="fig" rid="pcbi-1000930-g004">Figure 4a</xref>) and showed that the convergence of neural mechanisms generalizes to different visibility maps (<xref ref-type="fig" rid="pcbi-1000930-g004">Figure 4a</xref>) and also to a model in which eye movement planning is assumed to follow a saccadic targeting strategy (MAP) rather than approximating an ideal strategy (<xref ref-type="fig" rid="pcbi-1000930-g004">Figure 4a</xref>). Furthermore, <xref ref-type="fig" rid="pcbi-1000930-g004">Figure 4b</xref> shows that there is nothing particular about the symmetry of the eight location configuration search task since similar convergent evolution is observed for an asymmetric four location task (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1e</xref>).</p>
<fig id="pcbi-1000930-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g004</object-id><label>Figure 4</label><caption>
<title>Evolution plots for different models and scenarios detecting the elongated Gaussian target (<xref ref-type="fig" rid="pcbi-1000930-g002"><bold>Figure 2b</bold></xref>; middle).</title>
<p>a. 8 AFC search with a broad visibility map using 1/f noise for the Entropy Limit Minimization model (ELM) and the Saccade Targeting model (MAP); b. 4 AFC with broad visibility map using 1/f noise for the ELM and MAP model. All results based on averages across 10 virtual evolution runs.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g004" xlink:type="simple"/></fig>
<p>We also evaluated whether our results would change if the model included the increasing size of V1 receptive fields and lower frequency tuning with retinal eccentricity (see note 4 in <xref ref-type="supplementary-material" rid="pcbi.1000930.s004">Text S2</xref>). <xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5a</xref> (right graph) shows the center frequency and bandwidth (standard deviation) of the oriented Gabor receptive fields as a function of retinal eccentricity. The computational time demands of this simulation restricted us to evaluate this model for a fixed set of receptive field weights across eccentricities (see note 5 in <xref ref-type="supplementary-material" rid="pcbi.1000930.s004">Text S2</xref>) and limited set of scenarios: 1/f noise, steep visibility map and two targets: a low frequency Gaussian (<xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5b</xref>; left) and a Difference of Gaussians (DoG) with a center frequency of 8 c/deg (<xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5b</xref>; right). Due to the fixed set of weights across eccentricity, in this model the spatial profile of the linear combination of receptive fields scales up with eccentricity. Thus, for each retinal eccentricity category there was a pair of evolved template profiles. <xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5c</xref> shows that convergent evolution still results when receptive field size increases with eccentricity and irrespective of the spatial frequency of the target. <xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5d</xref> presents the similar radial profiles of the of evolved perception and saccade mechanisms for the fovea and a sample peripheral retinal location (perception and saccade 2-D template correlations for the best performing templates in the last generation averaged across retinal eccentricities were: Gaussian target: 0.963±0.008; DoG target: 0.961±0.004).</p>
<fig id="pcbi-1000930-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g005</object-id><label>Figure 5</label><caption>
<title>Evolution plots for a model with changing V1 receptive field size/spatial frequency with retinal eccentricity.</title>
<p>a. 8 AFC search task in 1/f noise (left) and graph (right) showing the change in central spatial frequency and width of channel in the frequency domain of oriented Gabor functions with retinal eccentricity. b. Radial profiles in the frequency domain of Gaussian target (left) and DoG target (right) with a center frequency of 8 cycles/degree. c. Distribution of correlations between perception and saccade templates of individuals in each generation for Gaussian target (left) and DoG target (right). d. Perception and saccade templates radial profiles (averaged across all angles) of best performing simulated individual for low-frequency Gaussian target (left) and higher frequency DoG target (right).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g005" xlink:type="simple"/></fig>
<p>Do all scenarios lead to converging evolution of the perception (ventral) and action (dorsal) pathways? No, if we take a case in which the sought target is equally detectable across the retina (flat visibility map), the results show the correlations between the perceptual and saccade templates do not converge to unity (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6a</xref>). A second example is a case in which the organism makes a decision after eight eye movements rather than two eye movements (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6b</xref>). Because the organism gets to fixate on all eight target locations, there is little added benefit of an efficient saccadic system and the co-evolution is much slower (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6b</xref>). A third scenario of partial convergence results if we adopt a strong model of two visual processing streams which spatially pre-filter the visual input based on the properties of the cells in the parvocellular and magnocellular lateral geniculate nucleus (LGN) (<xref ref-type="bibr" rid="pcbi.1000930-Derrington1">[23]</xref>; see <xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6d</xref>) and assume no further interaction across pathways. The differential spatial frequency filtering of the two pathways can introduce constraints in the frequency content of the evolved mechanisms preventing a full convergence of the templates (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6e</xref>; perception and saccade 2-D template correlations for the best performing templates in the last generation for: 1/f noise: 0.603±0.082). A similar simulation with the same target but white noise instead of 1/f noise also resulted in partial convergence (perception/saccade 2-D template correlation of 0.856±0.046).</p>
<fig id="pcbi-1000930-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000930.g006</object-id><label>Figure 6</label><caption>
<title>Evolution plots for scenarios which resulted in partial or no convergence of two templates.</title>
<p>All proportion correct and correlation plots shows the distribution for individuals in each generation. All results based on averages across 10 virtual evolution runs. a. 8 AFC search of the elongated Gaussian signal for a flat visibility map (ELM model); b. 8 AFC search of the elongated Gaussian signal for a broad visibility map, natural images, but with 8 eye movements which allows the model to fixate on all possible target locations (ELM model); c. 8 AFC search of an isotropic Gaussian signal for a steep visibility map using 1/f noise for the ELM model and considering two visual processing streams with different spatial pre-filtering based on LGN parvocellular and magnocelluar properties; d. Normalized frequency amplitude for Gaussian target, parvocellular LGN cell and magnocellular LGN cell; e. Perception and saccade templates radial profiles (averaged across all angles) of best performing simulated individual for the model with pathway LGN pre-filtering.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.g006" xlink:type="simple"/></fig></sec><sec id="s3">
<title>Discussion</title>
<p>We used an approximation to an Ideal Bayesian Searcher (Entropy Limit Minimization model; ELM) to virtually evolve separate linear mechanisms for eye movements and perceptual decisions during visual search for a variety of targets embedded in various synthetic and natural image backgrounds. Evolved templates contain similarities to the target but for the 1/f and natural images they are narrower than the target and contain a subtle inhibitory surrounding not present in the signals but often present in monkey neuronal receptive fields and human behavioral receptive fields <xref ref-type="bibr" rid="pcbi.1000930-Eckstein1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Eckstein2">[19]</xref> (see blue outline in <xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2b</xref>). A previous study has shown that such inhibitory surrounds serve to suppress high amplitude noise in the low frequencies and optimize the detection of spatially compact signals in natural images <xref ref-type="bibr" rid="pcbi.1000930-Zhang1">[24]</xref>. The current result extends previous results <xref ref-type="bibr" rid="pcbi.1000930-Zhang1">[24]</xref> to show the optimality of inhibitory surrounds during visual search in natural images for an organism with a foveated visual system and saccadic eye movements.</p>
<p>Central to this paper, the mechanisms for perception and saccades evolved to similar representations. This result is robust across different types of backgrounds, signals, visibility maps, and spatial distributions of possible target locations. Due to computational constraints we did not investigate the more general case of allowing the target to appear at any location within the image but there is no particular reason to suggest that our result would differ for this latter general case. In addition, similar convergence between mechanisms was found for what arguably are the most common contender algorithms to model how humans plan eye movements during search: an approximation to the ideal searcher, ELM and a saccadic targeting model; MAP model; <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref>. For simplicity our original models did not include receptive fields that increased with retinal eccentricity but an implementation of such a model led to similar convergent evolution for a low and a higher spatial frequency target.</p>
<p>The scenarios for which we did not find full convergent evolution of the linear mechanisms were for cases for which the target was either equi-detectable across the retina or the organism had enough time to fixate all of the possible target locations. Note, however, that for both cases, performance of the evolved individuals does improve with increasing generations (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6a–b</xref>) through the evolution of the perceptual template to a target-like structure. Yet, there is no performance advantage for evolving a neural mechanism for saccades that encodes target information because, for these cases different eye movement patterns have little or no impact on perceptual performance. A third scenario which resulted in partial convergence was a two stream model with pathway-specific pre-filtering of the visual input. A strong assumption that there are no interconnections between the two pathways would result in processing constraints based on the early stages of visual processing of both pathways. Inclusion of pre-filtering properties of the parvocellular and magnocellular LGN cells restricted the full convergence of the evolved mechanisms. These finding suggest that if we adopt a strict separation of pathways and take into account properties of LGN cells we should not always expect similar mechanisms driving perception and saccadic decisions during search. The specific circumstances for which we will not find convergent evolution and the degree of similarity between evolved templates will depend on the spatial frequency of the target and background statistical properties (see results for 1/f noise vs. white noise). Yet, is the strict separation of pathways and constraints to the filtering properties of parvocellular (perception) and magnocellular (action) LGN cells tenable for the case of eye movements and perceptual decisions during search? A recent psychophysical study <xref ref-type="bibr" rid="pcbi.1000930-Eckstein1">[9]</xref> used the same Gaussian target as in the simulations and reverse correlation to show that estimated underlying templates mediating human saccadic actions and perceptual search decisions are similar. Thus, these psychophysical findings would suggest that the strong assumption of no interconnections across pathways and constraints by the early LGN processing might not hold at least for the case of perception and eye movements during visual search.</p>
<p>Together, our present results suggest a theory of why evolution would favor similar neural mechanisms for perception and action during search <xref ref-type="bibr" rid="pcbi.1000930-Eckstein1">[9]</xref> and provide an explanation for the recent study finding similar estimated underlying templates mediating human saccadic decisions and perceptual decisions. Our findings and theory do not necessarily imply either that one pathway mediates both perception and action nor are they incompatible with the existence of separate magnocellular and parvocellular pathways. Instead, our theory would be consistent with the idea that pathways for perception and oculomotor largely overlap, leading to significant sharing of visual information across pathways <xref ref-type="bibr" rid="pcbi.1000930-Gegenfurtner1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Krauzlis1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Stone1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Dassonville1">[26]</xref>. For the case of saccadic eye movements, visual cortical pathways through the frontal eye fields <xref ref-type="bibr" rid="pcbi.1000930-Schall1">[27]</xref> and the lateral intra-parietal cortex <xref ref-type="bibr" rid="pcbi.1000930-Goldberg1">[28]</xref> play critical roles, as well as brainstem and cortical pathways through the superior colliculus <xref ref-type="bibr" rid="pcbi.1000930-McPeek1">[29]</xref>. In addition, studies have related areas in the ventral stream (V4) to target selection of saccades <xref ref-type="bibr" rid="pcbi.1000930-Moore1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Wolfe1">[31]</xref>. In addition, the results do not prohibit small differences in visual processing for perception and saccadic action but provide functional constraints on how much discrepancy can exist between neural mechanisms without jeopardizing the survival of the organism.</p>
<p>In the larger context, the similar neural mechanisms for perception and saccade actions should be understood as another effective strategy implemented in the brain, in addition to guidance by target properties <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Legge1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Rajashekar1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Tavassoli1">[33]</xref>, optimal saccade planning <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref>, contextual cues <xref ref-type="bibr" rid="pcbi.1000930-Torralba1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Eckstein3">[35]</xref> and miniature eye movements <xref ref-type="bibr" rid="pcbi.1000930-Rucci1">[36]</xref> to ensure successful visual search. Finally, the approach of the present study demonstrates how the rising field of natural systems analysis <xref ref-type="bibr" rid="pcbi.1000930-Geisler1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1000930-Geisler2">[38]</xref> can be used in conjunction with virtual evolution and physiological components of the visual system to evaluate whether properties of the human brain might reflect evolved strategies to optimize perceptual decisions and actions that are critical to survival.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Targets and backgrounds</title>
<p>We assumed a viewing distance of 50cm for the models. Search targets for simulations were: a) A Gaussian target with 0.5539 square root contrast energy (SCE) and a standard deviation of 0.1376 degrees (<xref ref-type="fig" rid="pcbi-1000930-g001">Figure 1c</xref>; 2b left column; 3); b) An elongated Gaussian with 0.9594 SCE, standard deviations of 0.4128 deg. in the vertical direction and 0.1376 degrees in the horizontal direction (<xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2b</xref> center column, <xref ref-type="fig" rid="pcbi-1000930-g004">Figure 4</xref>); c) The difference of a vertically oriented and a horizontally oriented elongated Gaussians with 0.8581 SCE (<xref ref-type="fig" rid="pcbi-1000930-g002">Figure 2b</xref>, right column). The white noise root mean square contrast (rms) was 0.0781. The same rms was used for white noise filtered with the 1/f function (1/f noise). Possible target locations were equidistant 7 degrees from the center fixation cross. Independent external and internal noise samples were refreshed with each saccade for the white and 1/f noise. For the natural images the external backgrounds were fixed but the internal noise refreshed across saccades.</p>
</sec><sec id="s4b">
<title>Model simulations</title>
<p>Here, we briefly describe the models implementations (see <xref ref-type="supplementary-material" rid="pcbi.1000930.s003">Text S1</xref> for detailed mathematical development and details). The initial stage of all three models investigated (ideal searcher, IS; entropy limit minimization, ELM; and saccadic targeting, MAP) is the dot product of a perceptual and saccade template (<bold>w</bold>) with the image data (<bold>g</bold>) at all possible target locations, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e001" xlink:type="simple"/></inline-formula> where r is the resulting scalar response and <bold>w</bold> and <bold>g</bold> are expressed as 1-D vectors. The templates for the perceptual decisions and saccade planning were independent and random linear combinations of 24 Gabor functions that spanned the targets: spatial frequencies, 0.5, 1, 2, 4 cycles/degree for 6 different orientations, 30 degrees apart, and with octave bandwidths. A subset of simulations (<xref ref-type="fig" rid="pcbi-1000930-g006">Figure 6</xref>) also modeled pre-processing of the image by separate LGN cells corresponding to the magnocellular (dorsal) and parvocellular (ventral) cells. The filtering was done using DoG functions with different center frequencies (see <xref ref-type="supplementary-material" rid="pcbi.1000930.s003">Text S1</xref> for mathematical details) prior to the processing by the Gabor functions.</p>
<p>Use of a larger number of Gabor functions did not significantly change the evolved templates for the targets considered but required prohibitively longer computational times due to the dimensionality explosion. For the template derived for the case of the isotropic Gaussian target we used an additional constraint of equal weighting for all orientations of the Gabor functions for a given spatial frequency. Most of the simulations used the fixed 24 Gabor functions irrespective of retinal eccentricity. A subset of simulations (see <xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5</xref>) used sets of 24 Gabor functions that increased linearly in size and also decreased in the central frequency tuning with retinal eccentricity (see details in effects of retinal eccentricity section). Template responses were integrated across saccades. Calculation of likelihood ratios use Gaussian probability density functions which depend on the image parameters for the white and 1/f Gaussian noisy images. For the natural images, the likelihood calculation required estimating the probability density function from a training set of 3000 images and fitting the probability density functions with Laplacian distributions convolved with a Gaussian distribution representing the internal noise (see <xref ref-type="supplementary-material" rid="pcbi.1000930.s003">Text S1</xref>).</p>
</sec><sec id="s4c">
<title>Effects of retinal eccentricity</title>
<p>Two methods were used to model the detrimental effect of retinal eccentricity on the detectability of the target. The first method which is similar to Najemnik and Geisler <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref> was implemented by adding internal noise to the scalar template response: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e002" xlink:type="simple"/></inline-formula>, where the additive internal noise scalar value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e003" xlink:type="simple"/></inline-formula> is sampled from a Gaussian distribution which standard deviation (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e004" xlink:type="simple"/></inline-formula>) is dependent on the distance ( i.e. retinal eccentricity) between the t<sup>th</sup> fixation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e005" xlink:type="simple"/></inline-formula> and the template response location <italic>i</italic> out of <italic>m</italic> possible target locations. Also the internal noise was proportional to the template's response standard deviation resulting from the external image variability. The visibility maps referred to as steep and broad (see also <xref ref-type="supplementary-material" rid="pcbi.1000930.s001">Figure S1</xref>) were obtained with internal noise standard deviations given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e006" xlink:type="simple"/><label>(1a)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e007" xlink:type="simple"/><label>(1b)</label></disp-formula>where σ<sub>o</sub> is the standard deviation of the template response due to external noise, <italic>e</italic> is the eccentricity in degrees, and the subscripts <italic>k</italic> refer to the fixation location, and <italic>i</italic> to the possible target location. For all models, independent samples of internal noise were used for each saccade and pathways.</p>
<p>The second method to model the effects of retinal eccentricity included internal noise (see above) and also varied the sets of 24 Gabor functions with retinal eccentricity.</p>
<p>The size of Gabor functions increased with the retinal eccentricity (<italic>e</italic>) so that the standard deviation of the spatial Gaussian envelope is given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e008" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e009" xlink:type="simple"/></inline-formula> is the bandwidth and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e010" xlink:type="simple"/></inline-formula> is the center frequency of Gabor function in the fovea. Thus, the standard deviation in the frequency domain of each Gabor function (<xref ref-type="fig" rid="pcbi-1000930-g005">Figure 5a</xref>; right graph) decreases as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e011" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>The center frequency tuning of the Gabor functions (s) linearly decreased with retinal eccentricity: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e012" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4d">
<title>The saccadic targeting model</title>
<p>The saccadic targeting or maximum a posteriori probability model (MAP) chooses the location of the next fixation with the maximum product of likelihoods ratios (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e013" xlink:type="simple"/></inline-formula>) across previous and present fixation (t = 1,…, T):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e014" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>For the case of white noise and 1/f Gaussian noise the expression can be simplified to the sum of log-likelihood ratios:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e015" xlink:type="simple"/><label>(5)</label></disp-formula>where Δμ is the difference in mean response of the template to the signal plus background and background only and all other symbols are defined above.</p>
</sec><sec id="s4e">
<title>The ideal searcher (IS)</title>
<p>The ideal searcher selects as the next fixation the location that will maximize the probability of finding the target after the eye movement is made:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e016" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e017" xlink:type="simple"/></inline-formula> is the proportion correct (PC) given that the target location is <italic>i</italic>, and the next fixation is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e018" xlink:type="simple"/></inline-formula>. The term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e019" xlink:type="simple"/></inline-formula> is the prior that the <italic>i</italic><sup>th</sup> location contains the target given the sensory evidence collected up to the present fixation: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e020" xlink:type="simple"/></inline-formula>and <italic>m</italic> is the number of possible target locations. For white noise and 1/f noise Gaussian noise, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e021" xlink:type="simple"/></inline-formula> becomes:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e022" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e023" xlink:type="simple"/></inline-formula> is the probability density function of the Gaussian function in Equation (9a), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e024" xlink:type="simple"/></inline-formula> the cumulative density function of the Gaussian function in Equation (9b), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e025" xlink:type="simple"/></inline-formula>and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e026" xlink:type="simple"/></inline-formula>, are the log-likelihood ratios which are known scalar values based on acquired visual information,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e027" xlink:type="simple"/><label>(8a)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e028" xlink:type="simple"/><label>(8b)</label></disp-formula>while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e029" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e030" xlink:type="simple"/></inline-formula> are random variables describing log-likelihoods after the next fixation and described by normal probability density functions:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e031" xlink:type="simple"/><label>(9a)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e032" xlink:type="simple"/><label>(9b)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e033" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e034" xlink:type="simple"/></inline-formula>) is the detectability at target location <italic>i</italic> (<italic>j</italic>), given fixation at location <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e035" xlink:type="simple"/></inline-formula>. The present formulation is identical to that of Najemink and Gielser <xref ref-type="bibr" rid="pcbi.1000930-Najemnik1">[13]</xref> but uses likelihood ratios rather than product of posteriors.</p>
</sec><sec id="s4f">
<title>Entropy limit minimization model (ELM model)</title>
<p>The entropy limit minimization model chooses as the next fixation the locations that minimize the expected entropy and can be approximated by maximizing the expected information gain. This can be shown to be approximated by calculating for each potential fixation location, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e036" xlink:type="simple"/></inline-formula>, a sum of the posterior probability for each location weighted by the squared detectability given the fixation location <xref ref-type="bibr" rid="pcbi.1000930-Najemnik2">[15]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e037" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e038" xlink:type="simple"/></inline-formula> is the Shannon entropy of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e039" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e040" xlink:type="simple"/></inline-formula> is the information gain.</p>
</sec><sec id="s4g">
<title>Perceptual decisions</title>
<p>For all models, the final perceptual decision about the target location was obtained by combining the likelihood ratios for each possible target locations across all fixations and choosing the location with the highest product of likelihood ratios:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e041" xlink:type="simple"/><label>(11)</label></disp-formula>where the likelihoods of the responses given the background only and the target are given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e043" xlink:type="simple"/></inline-formula> which are the probability density functions (pdf) assumed to be Gaussian (white noise and 1/f noise) or empirically estimated from samples (see next section) for the natural images.</p>
</sec><sec id="s4h">
<title>Natural images</title>
<p>The distribution of template responses for the natural image dataset <xref ref-type="bibr" rid="pcbi.1000930-vanHateren1">[22]</xref> were estimated from 24,000 image patches extracted from the eight possible target locations for 3000 natural images. We fit the distribution of these responses for each template of each simulated individual with a Laplacian distribution:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e044" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e045" xlink:type="simple"/></inline-formula> is the mean parameter and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e046" xlink:type="simple"/></inline-formula> is a scale parameter. To take into account the effect of additive Gaussian internal noise on the probability density function of the template responses we convolved the Laplacian distribution with the Gaussian distributions:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e047" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e048" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000930.e049" xlink:type="simple"/></inline-formula> are Gaussian and Laplace probability density functions respectively (see <xref ref-type="supplementary-material" rid="pcbi.1000930.s002">Figure S2</xref>).</p>
</sec><sec id="s4i">
<title>Genetic algorithm</title>
<p>We used the Genetic Algorithm Optimization Toolbox (GAOT) <xref ref-type="bibr" rid="pcbi.1000930-Houck1">[39]</xref>. Arithmetic crossover parameter was set to operate 50 times per generation, and uniform mutation to operate 50 times per generation. The selection process used a real-valued roulette wheel selection <xref ref-type="bibr" rid="pcbi.1000930-Geisler2">[38]</xref>. A generation consisted of 1,000 individual parameter settings. All individuals were randomly initialized, and allowed to evolve over 500 generations (see <xref ref-type="supplementary-material" rid="pcbi.1000930.s003">Text S1</xref> for additional details). Reported results for each scenario/model were averages across ten simulated evolution runs.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1000930.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s001" xlink:type="simple"><label>Figure S1</label><caption>
<p>Three visibility maps used in present paper.</p>
<p>(0.07 MB TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s002" xlink:type="simple"><label>Figure S2</label><caption>
<p>The probability density function of natural images is estimated from empirical distributions. (a) Gaussian and Laplace distributions fit to the distribution of template responses to natural images. (b) Convolution of Laplacian distribution with a Gaussian internal noise distribution.</p>
<p>(0.15 MB TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s003" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s003" xlink:type="simple"><label>Text S1</label><caption>
<p>A detailed description of the methods used in the paper.</p>
<p>(0.39 MB DOC)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s004" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s004" xlink:type="simple"><label>Text S2</label><caption>
<p>Some notes for the manuscript.</p>
<p>(0.03 MB DOC)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s005" mimetype="image/gif" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s005" xlink:type="simple"><label>Video S1</label><caption>
<p>Virtual evolution of linear neural mechanisms (templates) for perception (ventral stream) and saccadic action (dorsal stream) for search of an elongated Gaussian target. Video shows for each generation a perception and saccade template of a randomly sampled simulated individual. Legend below video indicates the generation number.</p>
<p>(7.83 MB GIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s006" mimetype="image/gif" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s006" xlink:type="simple"><label>Video S2</label><caption>
<p>Virtual evolution of linear neural mechanisms (templates) for perception (ventral stream) and saccadic action (dorsal stream) for search of a cross pattern consisting of a positive and negative polarity elongated Gaussian. Video shows for each generation a perception and saccade template of a randomly sampled simulated individual. Legend below video indicates the generation number.</p>
<p>(8.65 MB GIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1000930.s007" mimetype="image/gif" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000930.s007" xlink:type="simple"><label>Video S3</label><caption>
<p>Virtual evolution of linear neural mechanisms (templates) for perception (ventral stream) and saccadic action (dorsal stream) for search of an elongated Gaussian target in natural images. Video compares a pair of evolved perception and saccade templates and a pair of randomly generated templates.</p>
<p>(3.71 MB GIF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank the UCSB Grid Computing for access to their systems and three anonymous reviewers for their useful suggestions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1000930-Ungerleider1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name>
<name name-style="western"><surname>Mishkin</surname><given-names>M</given-names></name>
</person-group>             <year>1982</year>             <article-title>Two cortical visual systems.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Ingle</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Mansfield</surname><given-names>RW</given-names></name>
</person-group>             <source>Analysis of Visual Behavior</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>549</fpage>             <lpage>586</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Sakata1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sakata</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Taira</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Kusunoki</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Murata</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Tanaka</surname><given-names>Y</given-names></name>
</person-group>             <year>1997</year>             <article-title>The parietal association cortex in depth perception and visual control of hand action.</article-title>             <source>Trends Neurosci</source>             <volume>20</volume>             <fpage>350</fpage>             <lpage>357</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Snyder1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Snyder</surname><given-names>LH</given-names></name>
<name name-style="western"><surname>Batista</surname><given-names>AP</given-names></name>
<name name-style="western"><surname>Andersen</surname><given-names>RA</given-names></name>
</person-group>             <year>1997</year>             <article-title>Coding of intention in the posterior parietal cortex.</article-title>             <source>Nature</source>             <volume>386</volume>             <fpage>167</fpage>             <lpage>170</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Goodale1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Milner</surname><given-names>AD</given-names></name>
<name name-style="western"><surname>Jakobson</surname><given-names>LS</given-names></name>
<name name-style="western"><surname>Carey</surname><given-names>DP</given-names></name>
</person-group>             <year>1991</year>             <article-title>A neurological dissociation between perceiving objects and grasping them.</article-title>             <source>Nature</source>             <volume>349</volume>             <fpage>154</fpage>             <lpage>156</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Goodale2"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Milner</surname><given-names>AD</given-names></name>
</person-group>             <year>1992</year>             <article-title>Separate visual pathways for perception and action.</article-title>             <source>Trends Neurosci</source>             <volume>15</volume>             <fpage>20</fpage>             <lpage>25</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Milner1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Milner</surname><given-names>AD</given-names></name>
<name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name>
</person-group>             <year>1997</year>             <source>The Visual Brain in Action</source>             <publisher-loc>Oxford</publisher-loc>             <publisher-name>Oxford University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000930-Krliczak1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Króliczak</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Heard</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Goodale</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Gregory</surname><given-names>RL</given-names></name>
</person-group>             <year>2006</year>             <article-title>Dissociation of perception and action unmasked by the hollow-face illusion.</article-title>             <source>Brain Res</source>             <volume>1080</volume>             <fpage>9</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Gegenfurtner1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gegenfurtner</surname><given-names>KR</given-names></name>
<name name-style="western"><surname>Xing</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Scott</surname><given-names>BH</given-names></name>
<name name-style="western"><surname>Hawken</surname><given-names>MJ</given-names></name>
</person-group>             <year>2003</year>             <article-title>A comparison of pursuit eye movement and perceptual performance in speed discrimination.</article-title>             <source>J Vision</source>             <volume>3</volume>             <fpage>865</fpage>             <lpage>876</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Eckstein1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name>
<name name-style="western"><surname>Beutter</surname><given-names>BR</given-names></name>
<name name-style="western"><surname>Pham</surname><given-names>BT</given-names></name>
<name name-style="western"><surname>Shimozaki</surname><given-names>SS</given-names></name>
<name name-style="western"><surname>Stone</surname><given-names>LS</given-names></name>
</person-group>             <year>2007</year>             <article-title>Similar neural representations of the target for saccades and perception during search.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>1266</fpage>             <lpage>1270</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Volker1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Volker</surname><given-names>FH</given-names></name>
<name name-style="western"><surname>Gegenfurtner</surname><given-names>KR</given-names></name>
</person-group>             <year>2008</year>             <article-title>Grasping visual illusions: Consistent data and no dissociation.</article-title>             <source>Cogn Neuropsychol</source>             <volume>25</volume>             <fpage>920</fpage>             <lpage>950</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Farivar1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Farivar</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Blanke</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Chaudhuri</surname><given-names>A</given-names></name>
</person-group>             <year>2009</year>             <article-title>Dorsal-ventral integration in the recognition of motion-defined unfamiliar faces.</article-title>             <source>J Neurosci</source>             <volume>29</volume>             <fpage>5336</fpage>             <lpage>5342</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Krauzlis1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Krauzlis</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Stone</surname><given-names>LS</given-names></name>
</person-group>             <year>1999</year>             <article-title>Tracking with the mind's eye.</article-title>             <source>Trends Neurosci</source>             <volume>22</volume>             <fpage>544</fpage>             <lpage>550</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Najemnik1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Najemnik</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name>
</person-group>             <year>2005</year>             <article-title>Optimal eye movement strategies in visual search.</article-title>             <source>Nature</source>             <volume>434</volume>             <fpage>387</fpage>             <lpage>391</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Legge1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Legge</surname><given-names>GE</given-names></name>
<name name-style="western"><surname>Klitz</surname><given-names>TS</given-names></name>
<name name-style="western"><surname>Tjan</surname><given-names>BS</given-names></name>
</person-group>             <year>1997</year>             <article-title>Mr Chips: An ideal observer model of reading.</article-title>             <source>Psychol Rev</source>             <volume>104</volume>             <fpage>524</fpage>             <lpage>553</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Najemnik2"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Najemnik</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name>
</person-group>             <year>2009</year>             <article-title>Simple summation rule for optimal fixation selection in visual search.</article-title>             <source>Vision Res</source>             <volume>49</volume>             <fpage>1286</fpage>             <lpage>1294</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Rao1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RN</given-names></name>
<name name-style="western"><surname>Zelinsky</surname><given-names>GJ</given-names></name>
<name name-style="western"><surname>Hayhoe</surname><given-names>MM</given-names></name>
<name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name>
</person-group>             <year>2002</year>             <article-title>Eye movements in iconic visual search.</article-title>             <source>Vision Res</source>             <volume>43</volume>             <fpage>1447</fpage>             <lpage>1463</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Beutter1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beutter</surname><given-names>BR</given-names></name>
<name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name>
<name name-style="western"><surname>Stone</surname><given-names>LS</given-names></name>
</person-group>             <year>2003</year>             <article-title>Saccadic and perceptual performance in visual search tasks. I. Contrast detection and discrimination.</article-title>             <source>J Opt Soc Am A</source>             <volume>20</volume>             <fpage>1341</fpage>             <lpage>1355</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Palmer1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Palmer</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Verghese</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Pavel</surname><given-names>M</given-names></name>
</person-group>             <year>2000</year>             <article-title>The psychophysics of visual search.</article-title>             <source>Vision Res</source>             <volume>40</volume>             <fpage>1227</fpage>             <lpage>1268</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Eckstein2"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name>
<name name-style="western"><surname>Shimozaki</surname><given-names>SS</given-names></name>
<name name-style="western"><surname>Abbey</surname><given-names>CK</given-names></name>
</person-group>             <year>2002</year>             <article-title>The footprints of visual attention in the Posner paradigm revealed by classification images.</article-title>             <source>J Vision</source>             <volume>2</volume>             <fpage>25</fpage>             <lpage>45</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Green1"><label>20</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Green</surname><given-names>DM</given-names></name>
<name name-style="western"><surname>Swets</surname><given-names>JA</given-names></name>
</person-group>             <year>1966</year>             <source>Signal detection theory and psychophysics</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Wiley</publisher-name>          </element-citation></ref>
<ref id="pcbi.1000930-Renninger1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Renninger</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Verghese</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Coughlan</surname><given-names>J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Where to look next? Eye movements reduce local uncertainty.</article-title>             <source>J Vision</source>             <volume>7</volume>             <fpage>1</fpage>             <lpage>17</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-vanHateren1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name>
<name name-style="western"><surname>van der Schaaf</surname><given-names>A</given-names></name>
</person-group>             <year>1998</year>             <article-title>Independent component filters of natural images compared with simple cells in primary visual cortex.</article-title>             <source>P Roy Soc Lond B Bio</source>             <volume>265</volume>             <fpage>359</fpage>             <lpage>366</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Derrington1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Derrington</surname><given-names>AM</given-names></name>
<name name-style="western"><surname>Lennie</surname><given-names>P</given-names></name>
</person-group>             <year>1984</year>             <article-title>Spatial and temporal contrast sensitivities of neurones in lateral geniculate nucleus of macaque.</article-title>             <source>J Physiol</source>             <volume>357</volume>             <fpage>219</fpage>             <lpage>240</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Zhang1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zhang</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Abbey</surname><given-names>CK</given-names></name>
<name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name>
</person-group>             <year>2009</year>             <article-title>Virtual evolution for visual search in natural images results in behavioral receptive fields with inhibitory surrounds.</article-title>             <source>Visual Neurosci</source>             <volume>26</volume>             <fpage>93</fpage>             <lpage>108</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Stone1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stone</surname><given-names>LS</given-names></name>
<name name-style="western"><surname>Krauzlis</surname><given-names>RJ</given-names></name>
</person-group>             <year>2003</year>             <article-title>Shared motion signals for human perceptual decisions and oculomotor actions.</article-title>             <source>J Vision</source>             <volume>3</volume>             <fpage>725</fpage>             <lpage>736</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Dassonville1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dassonville</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Bala</surname><given-names>JK</given-names></name>
</person-group>             <year>2004</year>             <article-title>Action, perception and the Roelofs effect: A mere illusion of dissociation.</article-title>             <source>PLoS Biol</source>             <volume>2</volume>             <fpage>1936</fpage>             <lpage>1945</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Schall1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schall</surname><given-names>JD</given-names></name>
</person-group>             <year>2002</year>             <article-title>The neural selection and control of saccades by the frontal eye field.</article-title>             <source>Philos T Roy Soc B</source>             <volume>57</volume>             <fpage>1073</fpage>             <lpage>1082</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Goldberg1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Goldberg</surname><given-names>ME</given-names></name>
<name name-style="western"><surname>Bisley</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Powell</surname><given-names>KD</given-names></name>
<name name-style="western"><surname>Gottlieb</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kusunoki</surname><given-names>M</given-names></name>
</person-group>             <year>2002</year>             <article-title>The role of lateral intraparietal area of the monkey in the generation of saccades and visuospatial attention.</article-title>             <source>Ann NY Acad Sci</source>             <volume>956</volume>             <fpage>205</fpage>             <lpage>215</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-McPeek1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McPeek</surname><given-names>RM</given-names></name>
<name name-style="western"><surname>Keller</surname><given-names>EL</given-names></name>
</person-group>             <year>2004</year>             <article-title>Deficits in saccade target selection after inactivitation of superior colliculus.</article-title>             <source>Nat Neurosci</source>             <volume>7</volume>             <fpage>757</fpage>             <lpage>763</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Moore1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Moore</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Shape representations and visual guidance of saccadic eye movements.</article-title>             <source>Science</source>             <volume>285</volume>             <fpage>1914</fpage>             <lpage>1917</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Wolfe1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wolfe</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Cave</surname><given-names>KR</given-names></name>
<name name-style="western"><surname>Franzel</surname><given-names>SL</given-names></name>
</person-group>             <year>1989</year>             <article-title>Guided search: An alternative to the feature integration model for visual search.</article-title>             <source>J Exp Psychol: Human</source>             <volume>15</volume>             <fpage>419</fpage>             <lpage>433</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Rajashekar1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rajashekar</surname><given-names>U</given-names></name>
<name name-style="western"><surname>Bovik</surname><given-names>AC</given-names></name>
<name name-style="western"><surname>Cormack</surname><given-names>LK</given-names></name>
</person-group>             <year>2006</year>             <article-title>Visual search in noise: Revealing the influence of structural cues by gaze–contingent classification image analysis.</article-title>             <source>J Vision</source>             <volume>6</volume>             <fpage>379</fpage>             <lpage>386</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Tavassoli1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tavassoli</surname><given-names>A</given-names></name>
<name name-style="western"><surname>van der Linde</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Bovik</surname><given-names>AC</given-names></name>
<name name-style="western"><surname>Cormack</surname><given-names>LK</given-names></name>
</person-group>             <year>2009</year>             <article-title>Eye movements selective for spatial frequency and orientation during active visual search.</article-title>             <source>Vision Res</source>             <volume>49</volume>             <fpage>173</fpage>             <lpage>181</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Torralba1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Castelhano</surname><given-names>MS</given-names></name>
<name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name>
</person-group>             <year>2006</year>             <article-title>Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search.</article-title>             <source>Psychol Rev</source>             <volume>113</volume>             <fpage>766</fpage>             <lpage>786</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Eckstein3"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name>
<name name-style="western"><surname>Drescher</surname><given-names>BA</given-names></name>
<name name-style="western"><surname>Shimizaki</surname><given-names>SS</given-names></name>
</person-group>             <year>2006</year>             <article-title>Attentional cues in real scenes, saccadic targeting and Bayesian priors.</article-title>             <source>Psychol Sci</source>             <volume>17</volume>             <fpage>973</fpage>             <lpage>980</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Rucci1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rucci</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Iovin</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Poletti</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Santini</surname><given-names>F</given-names></name>
</person-group>             <year>2007</year>             <article-title>Miniature eye movements enhance fine spatial detail.</article-title>             <source>Nature</source>             <volume>447</volume>             <fpage>851</fpage>             <lpage>854</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Geisler1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name>
<name name-style="western"><surname>Ringach</surname><given-names>D</given-names></name>
</person-group>             <year>2009</year>             <article-title>Natural systems.</article-title>             <source>Visual Neurosci</source>             <volume>26</volume>             <fpage>1</fpage>             <lpage>3</lpage>          </element-citation></ref>
<ref id="pcbi.1000930-Geisler2"><label>38</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name>
<name name-style="western"><surname>Perry</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Ing</surname><given-names>AD</given-names></name>
</person-group>             <year>2008</year>             <article-title>Natural systems analysis.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Rogowitz</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Pappas</surname><given-names>T</given-names></name>
</person-group>             <source>Human Vision and Electronic Imaging</source>             <comment>SPIE Proceedings, SPIE Vol. 6806</comment>          </element-citation></ref>
<ref id="pcbi.1000930-Houck1"><label>39</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houck</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Joines</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kay</surname><given-names>M</given-names></name>
</person-group>             <year>1995</year>             <article-title>A genetic algorithm for function optimization: a Matlab implementation.</article-title>             <comment>NCSU-IE Technical Report 95–09</comment>          </element-citation></ref>
</ref-list>

</back>
</article>