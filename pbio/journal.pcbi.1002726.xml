<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00339</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002726</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Cognitive neuroscience</subject>
              <subj-group>
                <subject>Cognition</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory perception</subject>
              <subj-group>
                <subject>Psychophysics</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Visual system</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Behavioral neuroscience</subject>
              <subject>Neuroimaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories</article-title><alt-title alt-title-type="running-head">Contrast Responses Predict Perceptual Similarity</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Groen</surname>
            <given-names>Iris I. A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Ghebreab</surname>
            <given-names>Sennay</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Lamme</surname>
            <given-names>Victor A. F.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Scholte</surname>
            <given-names>H. Steven</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands</addr-line>
      </aff><aff id="aff2">
        <label>2</label>
        <addr-line>Intelligent Systems Lab Amsterdam, Institute of Informatics, University of Amsterdam, Amsterdam, The Netherlands</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">
        <addr-line>Indiana University, United States of America</addr-line>
      </aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">i.i.a.groen@uva.nl</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: SG VAFL HSS. Performed the experiments: IIAG. Analyzed the data: IIAG. Contributed reagents/materials/analysis tools: SG HSS. Wrote the paper: IIAG SG VAFL HSS.</p>
        </fn>
      </author-notes><pub-date pub-type="collection">
        <month>10</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>18</day>
        <month>10</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>10</issue><elocation-id>e1002726</elocation-id><history>
        <date date-type="received">
          <day>29</day>
          <month>2</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>2</day>
          <month>8</month>
          <year>2012</year>
        </date>
      </history><permissions>
        
        <copyright-holder>Groen et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions><abstract>
        <p>The visual world is complex and continuously changing. Yet, our brain transforms patterns of light falling on our retina into a coherent percept within a few hundred milliseconds. Possibly, low-level neural responses already carry substantial information to facilitate rapid characterization of the visual input. Here, we computationally estimated low-level contrast responses to computer-generated naturalistic images, and tested whether spatial pooling of these responses could predict image similarity at the neural and behavioral level. Using EEG, we show that statistics derived from pooled responses explain a large amount of variance between single-image evoked potentials (ERPs) in individual subjects. Dissimilarity analysis on multi-electrode ERPs demonstrated that large differences between images in pooled response statistics are predictive of more dissimilar patterns of evoked activity, whereas images with little difference in statistics give rise to highly similar evoked activity patterns. In a separate behavioral experiment, images with large differences in statistics were judged as different categories, whereas images with little differences were confused. These findings suggest that statistics derived from low-level contrast responses can be extracted in early visual processing and can be relevant for rapid judgment of visual similarity. We compared our results with two other, well- known contrast statistics: Fourier power spectra and higher-order properties of contrast distributions (skewness and kurtosis). Interestingly, whereas these statistics allow for accurate image categorization, they do not predict ERP response patterns or behavioral categorization confusions. These converging computational, neural and behavioral results suggest that statistics of pooled contrast responses contain information that corresponds with perceived visual similarity in a rapid, low-level categorization task.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Humans excel in rapid and accurate processing of visual scenes. However, it is unclear which computations allow the visual system to convert light hitting the retina into a coherent representation of visual input in a rapid and efficient way. Here we used simple, computer-generated image categories with similar low-level structure as natural scenes to test whether a model of early integration of low-level information can predict perceived category similarity. Specifically, we show that summarized (<italic>spatially pooled</italic>) responses of model neurons covering the entire visual field (<italic>the population response</italic>) to low-level properties of visual input (<italic>contrasts</italic>) can already be informative about differences in early visual evoked activity as well as behavioral confusions of these categories. These results suggest that low-level population responses can carry relevant information to estimate similarity of controlled images, and put forward the exciting hypothesis that the visual system may exploit these responses to rapidly process real natural scenes. We propose that the spatial pooling that allows for the extraction of this information may be a plausible first step in extracting scene gist to form a rapid impression of the visual input.</p>
      </abstract><funding-group>
        <funding-statement>This work is part of the Research Priority Program ‘Brain &amp; Cognition’ at the University of Amsterdam and was supported by an Advanced Investigator grant from the European Research Council (<ext-link ext-link-type="uri" xlink:href="http://erc.europa.eu/" xlink:type="simple">http://erc.europa.eu/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group><counts>
        <page-count count="16"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Complex natural images are categorized remarkably fast <xref ref-type="bibr" rid="pcbi.1002726-Potter1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Greene1">[2]</xref>, sometimes even faster than simple artificial stimuli <xref ref-type="bibr" rid="pcbi.1002726-FeiFei1">[3]</xref>. For animal and non-animal scenes, differences in EEG responses are found within 150 ms <xref ref-type="bibr" rid="pcbi.1002726-Thorpe1">[4]</xref> and a correct saccade is made within 120 ms <xref ref-type="bibr" rid="pcbi.1002726-Kirchner1">[5]</xref>. This speed of processing is also found for other scene categories <xref ref-type="bibr" rid="pcbi.1002726-VanRullen1">[6]</xref> and may require less attentional resources compared to artificial images <xref ref-type="bibr" rid="pcbi.1002726-Rousselet1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Peelen1">[8]</xref>. This suggests that relevant visual information is rapidly and efficiently extracted from early visual responses to natural scenes. However, the neural computations involved in this process are not known.</p>
      <p>Importantly, natural images differ from other image types such as white noise in low-level properties (e.g., sparseness), leading to the suggestion that the visual system has adapted to these low-level properties <xref ref-type="bibr" rid="pcbi.1002726-Field1">[9]</xref>. This idea paved the way for optimal coding models for natural images <xref ref-type="bibr" rid="pcbi.1002726-Vinje1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Schwartz1">[11]</xref> and successful predictions of response properties of visual neurons <xref ref-type="bibr" rid="pcbi.1002726-Olshausen1">[12]</xref>. Recent work identified statistical properties that differ even within the class of natural images, e.g. between natural scene parts <xref ref-type="bibr" rid="pcbi.1002726-Karklin1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Frazor1">[14]</xref> or natural image categories <xref ref-type="bibr" rid="pcbi.1002726-Torralba1">[15]</xref>, showing that image statistics such as power spectra of spatial frequency content or distributions of local image features are informative about scene category.</p>
      <p>The fact that it is mathematically possible to distinguish categories based on image statistics, however, does not imply that they are used for categorization in the brain. Image statistics may not be sufficiently reliable, or their computation may not be suitable for neural implementation <xref ref-type="bibr" rid="pcbi.1002726-Olshausen1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Graham1">[16]</xref>. We recently showed that statistics derived from the frequency histogram of local contrast – summarized by two parameters of a Weibull fit, <xref ref-type="fig" rid="pcbi-1002726-g001"><bold>Fig. 1A</bold></xref> – explain up to 50% of the variance of event-related potentials (ERPs) recorded from visual cortex <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref>. These parameters inform about the width and shape of the histogram, respectively, and appear to describe meaningful variability between images (<xref ref-type="fig" rid="pcbi-1002726-g001"><bold>Fig. 1B</bold></xref>). Importantly, we found that these parameters can be reliably approximated by linear summation of the output of localized difference-of-Gaussians filters modeled after X- and Y-type LGN cells, suggesting that this global information may be available to visual cortex directly from its early low-level contrast responses <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref>.</p>
      <fig id="pcbi-1002726-g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Contrast histograms of natural images follow a Weibull distribution.</title>
          <p>(<bold>A</bold>), Three natural images with varying degrees of details and scene fragmentation. The homogenous, texture-like image of grass (upper row) contains many edges of various strengths; its contrast distribution approaches a Gaussian. The strongly segmented image of green leaves against a uniform background (bottom row) contains very few, strong edges that are highly coherent; its distribution approaches power law. Most natural images, however, have distributions in between (middle row). The degree to which images vary between these two extremes is reflected in the free parameters of a Weibull fit to the contrast histogram: β (beta) and γ (gamma). (<bold>B</bold>), For each of 200 natural scenes, the beta and gamma values were derived from fitting the Weibull distribution to their contrast histogram. Beta describes the width of the histogram: it varies with the distribution of local contrasts strengths. Gamma describes the shape of the histogram: it varies with the amount of scene clutter. Four representative pictures are shown in each corner of the parameter space. Images with a high degree of scene segmentation, e.g. a leaf on top of snow, are found in the lower left corner, whereas highly cluttered images are on the right. Images with more depth are located on the top, whereas flat images are found at the bottom. Images are from the McGill Calibrated Colour Image Database <xref ref-type="bibr" rid="pcbi.1002726-Olmos1">[86]</xref>.</p>
        </caption>
        <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g001" xlink:type="simple"/>
      </fig>
      <p>Moreover, we found that output of contrast filters with a larger range of receptive field sizes captures additional image information <xref ref-type="bibr" rid="pcbi.1002726-Ghebreab1">[18]</xref>. This is not surprising since objects in natural scenes appear at many distances and hence spatial scales <xref ref-type="bibr" rid="pcbi.1002726-Oliva1">[19]</xref>. In the present implementation, the model first estimates at which scale relevant contrast information is present, as well as characteristics of the distribution of contrast strengths at those scales. This model, which approximates early visual population responses based on spatially pooled contrasts, was able to explain almost 80% of ERP variance to natural images <xref ref-type="bibr" rid="pcbi.1002726-Ghebreab1">[18]</xref>.</p>
      <p>These previous findings suggest that images with more similar contrast response statistics evoke more similar early visual activity. Could these responses already contain relevant information about the stimulus for rapid categorization? The two parameters appear to index meaningful information such as degree of clutter, depth and figure-ground segmentation <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref>, but how the two dimensions in <xref ref-type="fig" rid="pcbi-1002726-g001"><bold>Fig. 1B</bold></xref> influence perception has not been examined. The goal of the current study was thus to explore what type of visual information is contained in the variance of the earliest visual contrast responses that is so well described by these two parameters. Specifically, we were interested in whether these parameters cannot only predict variance in visual activity, but also ‘variance in perception’. In other words, do images with more similar contrast statistics also lead to more similar perceptual representations, and perhaps ultimately, to similar images being considered a single category?</p>
      <p>We aimed to answer this question in a data-driven manner, by investigating 1) which images group by similarity early in visual processing and 2) whether this grouping matches with perceived similarity of those images. For the first part of this question, we obtained reliable evoked responses to individual images. The advantage of this approach relative to traditional ERP analysis (which is based on averaging many trials across individual images within an a priori determined condition) is that it provides much richer data <xref ref-type="bibr" rid="pcbi.1002726-vanRijsbergen1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-Groen1">[24]</xref> that can be used for model selection. We used these single-image evoked responses to compute dissimilarities in ‘neural space’, similar to the pattern analysis approach used in fMRI <xref ref-type="bibr" rid="pcbi.1002726-Kriegeskorte1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Kravitz1">[26]</xref>. This allowed us to track, over the course of the ERP, to what extent the representation of an image is (dis)similar to all images in the data set.</p>
      <p>For the second part of the question, we needed to obtain an image-specific behavioral judgment of perceived visual similarity. However, simply judging similarity of natural scenes is problematic, because these images obviously contain rich semantic content: there are many features of natural scenes that can be similar or dissimilar, which is likely to lead to different categorization strategies by different subjects. Also, it is uncertain to what extent specific semantic tags that are provided by the researcher (e.g. ‘openness’ or ‘naturalness’, <xref ref-type="bibr" rid="pcbi.1002726-Ross1">[27]</xref>), can be uniformly interpreted as a relevant stimulus dimension that has a linear mapping to processing in early vision. Therefore, to explore the variance explained by contrast response statistics in a bottom-up way, we used stimuli that were simplified model images of natural scenes (‘dead leaves’, <xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2A</bold></xref>), which have similar low-level structure as natural scenes (e.g. 1/f power spectra) but are devoid of semantic content. These images are created by filling a frame with objects - much like fallen leaves can fill a forest floor – and are used in computer vision to study, for example, how the appearance and the distribution of these objects influences the low-level structure of natural scenes <xref ref-type="bibr" rid="pcbi.1002726-Hsiao1">[28]</xref>. By manipulating properties of the objects in a controlled manner, we created distinct image categories, and then tested whether differences between these categories in contrast statistics matched with behaviorally perceived similarity by letting human observers perform a same-different categorization task on all combinations of image categories.</p>
      <fig id="pcbi-1002726-g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Example stimuli and computation of contrast statistics.</title>
          <p>(<bold>A</bold>), Example images of each of the 16 categories used in the behavioral and EEG experiment. Images contained randomly placed disks that differed in distribution, opacity, depth and size. Each category contained 16 unique images. (<bold>B</bold>), Consecutive steps in computing various contrast statistics. Weibull statistics are computed by filtering the image with a range of contrast filters with LGN-like scale- and gain properties, after which for each image location, the filter containing the minimal reliable response is selected. Responses of all selected filters are summed in a histogram to which the Weibull function is fitted, from which the beta and gamma parameters are derived using maximum likelihood estimation. (<bold>C</bold>), Power spectra parameters (top row) are extracted by taking the Fourier transform, averaging across directions, and computing the intercept and slope values of a line fitted to the average power spectrum. Higher-order properties of the contrast distribution (bottom row) are computed by filtering with a single-scale center-surround filter, after which skewness and kurtosis of the resulting contrast distribution are derived. Weibull statistics (multiscale local contrast) presumably contain information present in Fourier parameters (scale statistics) as well as local contrast distribution parameters (distribution statistics).</p>
        </caption>
        <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g002" xlink:type="simple"/>
      </fig>
      <p>Specifically, we used the space formed by the two Weibull parameters to compute geometric distances between images in contrast statistics, and used these distances as quantitative predictors of dissimilarity <xref ref-type="bibr" rid="pcbi.1002726-Shepard1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-OpdeBeeck1">[31]</xref>. We thus tested whether these parameters can predict the extent to which image categories induced dissimilar single-image EEG responses (experiment 1) and whether they match with perceptual categorization at the behavioral level (experiment 2). We predicted that images with very different Weibull statistics would appear less similar, i.e. be less often confused than images from categories with similar statistics.</p>
      <p>By using controlled images that we quantified using a model originally derived from contrast responses to natural images, we aim to build a bridge between findings obtained with systematic manipulation of artificial stimuli and those obtained with more data-driven natural scene studies. For purpose of comparison, and to better understand which statistical information is captured by the Weibull parameters, we also tested two other global contrast statistics (<xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2C</bold></xref>). Following <xref ref-type="bibr" rid="pcbi.1002726-Oliva2">[32]</xref> we calculated the intercept and slope of the average power spectrum to parameterize spatial frequency information, a commonly used measure of low-level information in scene perception. In addition, we followed <xref ref-type="bibr" rid="pcbi.1002726-Tadmor1">[33]</xref> to derive the skewness and kurtosis of the contrast distribution for a range of spatial scales: these higher-order properties of distributions have previously been suggested (e.g. <xref ref-type="bibr" rid="pcbi.1002726-Brady1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Kingdom1">[35]</xref> to reflect low-level differences between images that are relevant for perceptual processing.</p>
      <p>We find that Weibull statistics explain substantial variance in evoked response amplitude to the dead leaves images, predicting clustering-by-category of occipital ERP patterns within 100 ms of visual processing. In addition, they correlate with human categorization behavior: specific confusions were made between categories with similar Weibull statistics. By comparison, Fourier power spectra and skewness and kurtosis can be used for accurate classification of image category, but fail to predict neural clustering and behavioral categorization. These convergent results provide evidence for relevance of pooled contrast response statistics in rapid neural computation of perceptual similarity.</p>
    </sec>
    <sec id="s2" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s2a">
        <title>Ethics statement</title>
        <p>The experiments reported here were approved by the Ethical Committee of the Psychology Department at the University of Amsterdam; all participants gave written informed consent prior to participation and were rewarded with study credits or financial compensation (7 euro/hour).</p>
      </sec>
      <sec id="s2b">
        <title>Stimuli</title>
        <p>Gray-scale dead leaves images (512×512 pixels, bit depth 24) were generated using Matlab. Images contained randomly placed disks that were manipulated along 4 dimensions (opacity, depth, size and distribution) to create 16 categories. Disks were either opaque or transparent; intensity at the outer edges of the disk was either constant (leading to a 2D appearance) or decaying (3D appearance), and disk size was determined by drawing randomly from a range of small, medium or large diameters (exact settings as in <xref ref-type="bibr" rid="pcbi.1002726-Hsiao1">[28]</xref>. Twelve categories were created by systematically varying these properties of power-law distributed disks. Four more categories were created using medium-diameter, exponentially distributed disks that could be 2D or 3D and opaque or transparent. For each category, 16 images were created using these category-specific settings: the random placement and use of ranges of diameter sizes ensured that each of these 16 images was unique. This procedure thus resulted in a total of unique 256 images, divided into 16 distinct categories, which were used for experimentation (<xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2A</bold></xref>).</p>
        <sec id="s2b1">
          <title>Computation of contrast statistics</title>
          <p>In the Weibull model, local contrast is computed at multiple spatial scales, after which a single optimal scale for each image location is selected. Subsequently, contrast responses are collected in a histogram that is summarized using a Weibull fit, yielding two statistical parameters: beta and gamma (<xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2B</bold></xref>). For comparison with other contrast statistics, we computed spatial frequency statistics (using power spectra) and higher-order statistics (third moments of the contrast distribution) for various receptive field sizes (<xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2C</bold></xref>). The computational steps of each method are described in detail below.</p>
        </sec>
        <sec id="s2b2">
          <title>Weibull contrast statistics</title>
          <p>We computed image contrast according to the standard linear-nonlinear model. For the initial linear filtering step we used contrast filters modeled after well-known receptive fields of LGN-neurons <xref ref-type="bibr" rid="pcbi.1002726-Bonin1">[36]</xref>. As described in detail in <xref ref-type="bibr" rid="pcbi.1002726-Ghebreab1">[18]</xref> each location in the image was filtered using Gaussian second-order derivative filters spanning multiple octaves in spatial scale <xref ref-type="bibr" rid="pcbi.1002726-Croner1">[37]</xref>. Based on our previous result <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref> that the beta parameter was best approximated by a linear summation of X-like receptive field size output, whereas the gamma parameter correlated highest with Y-like receptive field size contrast, two separate spatial scale octave ranges were applied to derive the two summary parameters in the present multi-scale model. For the beta parameter, a bank of filters with 5 octave scales (4, 8, 16, 32, 64) standard deviation in pixels was used; for the gamma parameter, the filter bank consisted of octave scales 5, 10, 20, 40 and 80. The output of each filter was normalized with a Naka-Rushton function with 5 semi-saturation constants between 0.15 and 1.6 to cover the spectrum from linear to non-linear contrast gain control in LGN.</p>
          <p>From the population of gain- and scale-specific filters, one filter response was selected for each location in the image using minimum reliable scale selection <xref ref-type="bibr" rid="pcbi.1002726-Elder1">[38]</xref>, a spatial scale control mechanism in which the smallest filter with output higher than what is expected to be noise for that specific filter is selected. The rationale behind this approach is that to arrive at a faithful scale-invariant contrast representation, the visual system selects spatial scale by minimizing receptive field size while simultaneously maximizing response reliability. Noise thresholds for each filter were determined in a separate set of stimuli (1800 natural images from the ImageNet natural scene database, <xref ref-type="bibr" rid="pcbi.1002726-Deng1">[39]</xref>) and set to half a standard deviation of the average contrast present in that dataset for a given scale and gain. Applying the selected filter for each location in the image resulted in a 512×512 pixel contrast magnitude map, which was converted in a 256-bin histogram summarizing the contrast distribution of the image, to which the Weibull function was fitted by a maximum likelihood estimator (MLE). The Weibull function is given by:<disp-formula id="pcbi.1002726.e001"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002726.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where c is a normalization constant and μ, ß (beta) and γ (gamma) are the free parameters that represent the origin, scale and shape of the distribution, respectively. The value of the origin parameter μ is influenced by uneven illumination and generally close to zero for natural images. To achieve illumination invariance, this value was estimated and averaged out, leaving only the beta and gamma values as free parameters for each image.</p>
        </sec>
        <sec id="s2b3">
          <title>Fourier power statistics</title>
          <p>A two-parameter Fourier statistic was derived for each image by computing the intercept and slope of a line fitted to its power spectrum. We determined the power spectrum of the largest concentric square portion of the image (in this case, the entire image), excluding its outer edges to prevent edge artifacts. The cropped image was transformed into the frequency domain using the Fast Fourier Transform. Slope and intercept were estimated from the regression line fitted to the log-log representation of the power law-dependence:<disp-formula id="pcbi.1002726.e002"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002726.e002" xlink:type="simple"/><label>(2)</label></disp-formula>The rotationally averaged power-law spectrum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002726.e003" xlink:type="simple"/></inline-formula> is defined as<disp-formula id="pcbi.1002726.e004"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002726.e004" xlink:type="simple"/><label>(3)</label></disp-formula>where F<italic><sub>I</sub> (f, θ)</italic> is the Fourier transform spectrum of the input image <italic>I</italic>; <italic>(f, θ)</italic> are the cylindrical polar coordinates in Fourier space and 〈 〉<italic><sub>θ</sub></italic> denotes averaging over <italic>θ</italic>.</p>
        </sec>
        <sec id="s2b4">
          <title>Contrast distribution statistics</title>
          <p>Following <xref ref-type="bibr" rid="pcbi.1002726-Tadmor1">[33]</xref>, we used center-surround difference-of-Gaussian (DoG) filters to extract contrast values. Center receptive field sizes ranged between 2 and 4 pixels, and surround-to-center size ranged between 3 and 9, resulting in 21 different combinations of center size and surround-to-center ratio, referred to as receptive-field models. For each model, a scaling factor was used to set the integrated sensitivity of the surround to be 85% of that of the center. Per image, contrast responses were computed by convolving each pixel value with each of these 21 models separately. Responses were normalized using center-surround divisive normalization, where the difference in output of the center and surround is divided by their summed output. From the response distribution of responses across the image one skewness and one kurtosis value was derived for each image and for each receptive field model, resulting in 21 skewness and kurtosis values per image. Of these 21 values, results are reported for the skewness and kurtosis values that explained most EEG variance (center radius of 4 pixels with surround-center ratio 3); see next section (Experiment 1: EEG). This measure, computed exactly as reported in <xref ref-type="bibr" rid="pcbi.1002726-Tadmor1">[33]</xref>, has two important distinctions with the Weibull model, namely 1) the method does not incorporate scale selection; each receptive field model has one specific receptive field size that is used across the entire image and 2) only one parameter (skewness or kurtosis) is used to describe the response distribution that results from contrast filtering, compared to the separate scale (beta) and shape (gamma) parameters used in the Weibull model.</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Experiment 1: EEG</title>
        <sec id="s2c1">
          <title>Experimental procedure</title>
          <p>Nineteen subjects took part in this experiment. The dead leaves images were presented on a 19 inch Ilyama monitor, whose resolution was set at 1024×768 pixels with a frame rate of 60 Hz. Subjects were seated 90 cm from the monitor such that stimuli subtended 11×11° of visual angle. During EEG acquisition, a single image was presented in the center of the screen on a grey background for 100 ms, on average every 1500 ms (range 1000–2000 ms; <xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3A</bold></xref>). Each stimulus was presented twice, in two separate runs. Stimuli were presented intermixed with phase-scrambled versions of grayscale natural images; subjects were instructed to indicate which type of image they were shown. This instruction was intended to ensure that subjects attended to the stimuli: the required discrimination between the dead leaves and phase-scrambled natural images did not correspond to any distinction between the categories of dead leaves themselves. Examples of the two types of images were displayed prior to the experiment. Each run was subdivided in 8 blocks across which response mappings were counterbalanced. Stimuli were presented using the software package Presentation (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com" xlink:type="simple">www.neurobs.com</ext-link>).</p>
          <fig id="pcbi-1002726-g003" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Methods and experimental design.</title>
              <p>(<bold>A</bold>), Experimental set-up of experiment 1 (EEG experiment). Subjects were presented with individual images of dead leaves while EEG was recorded. Single-image evoked responses (ERPs) were computed for each electrode, by averaging two repeated presentations of each individual image. Regression analyses of ERP amplitude on contrast statistics were performed at each time sample and electrode. (<bold>B</bold>), Representational dissimilarity matrices (RDMs) were computed at each sample of the ERP. A single RDM displays Euclidean distance (red = high, blue = low) between multiple-electrode patterns of ERP amplitude between all pairs of stimuli at a specific moment in time. The (cartoon) inset demonstrates how dissimilarities can cluster by category: all images from one category are in consecutive rows and can be ‘similarly dissimilar’ to other categories. (<bold>C</bold>), Experimental set-up of experiment 2 (behavioral experiment). On each trial, subjects were presented with a pair of stimuli for 50 ms, followed by a mask after an interval of 100 ms. Subjects were presented 8 times with all possible pairings of stimuli and were instructed to indicate whether stimuli were the same or different. (<bold>D</bold>), Cartoon example of leave-one-out classification based on contrast statistics. One stimulus is selected in turn, after which the median (thumbnail) of the remaining stimuli of its category is computed, as well as the median of other categories (here, just one). Classification accuracy reflects how many stimuli are closer to the median of other categories instead of its own category in terms of distance in image statistics.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g003" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2c2">
          <title>EEG data acquisition</title>
          <p>EEG Recordings were made with a Biosemi 64-channel Active Two EEG system (Biosemi Instrumentation BV, Amsterdam, NL, <ext-link ext-link-type="uri" xlink:href="http://www.biosemi.com" xlink:type="simple">www.biosemi.com</ext-link>), with sintered Ag/AgCl electrodes at scalp positions including the standard 10-10 system along with intermediate positions and two additional occipital electrodes (I1 and I2), which replaced two frontal electrodes (F5 and F6). During recording, a CMS/DRL feedback loop was used as an active ground, followed by offline referencing to electrodes placed on the earlobes. The Biosemi hardware is completely DC-coupled, so no high-pass filter is applied during recording of the raw data. A Bessel low-pass filter was applied starting at 1/5<sup>th</sup> of the sample rate. Eye movements were monitored with a horizontal electro-oculogram (hEOG) placed lateral to both eyes and a vertical electro-oculogram (vEOG) positioned above and below the left eye, aligned with the pupil location when the participants looked straight ahead. Data was sampled at 256 Hz.</p>
        </sec>
        <sec id="s2c3">
          <title>EEG data preprocessing</title>
          <p>The raw data was pre-processed using Brain Vision Analyzer by applying a high-pass filter at 0.1 Hz (12 dB/octave) and a low-pass filter at 30 Hz (24 dB/octave). Since this low-pass filter has a graded descent, it cannot be guaranteed that all high-frequency noise is removed; therefore, we additionally applied two notch filters at 50 (for line noise) and 60 Hz (for monitor noise). Deflections larger than 300 mV were automatically removed. Trials were segmented into epochs starting 100 ms before stimulus onset and ending at 500 ms after stimulus onset. These epochs were corrected for eye movements by removing the influence of ocular-generated EEG using a regression analysis based on the two horizontal and vertical EOG channels <xref ref-type="bibr" rid="pcbi.1002726-Gratton1">[40]</xref>. Baseline correction was performed based on the data between −100 ms and 0 ms relative to stimulus onset; artifacts were rejected using maximal allowed voltage steps of 50 µV, minimal and maximal allowed amplitudes of −75 and 75 µV and a lowest allowed activity of 0.50 µV (median rejection rate across subjects was 7%, with a range of 1%–38%). The resulting event-related potentials (ERPs) were converted to Current Source Density (CSD) responses <xref ref-type="bibr" rid="pcbi.1002726-Perrin1">[41]</xref>. This conversion results in a signal that is more localized in space, which has the advantage of more reliably reflecting activity of neural tissue underlying the recording electrode <xref ref-type="bibr" rid="pcbi.1002726-Nunez1">[42]</xref>.</p>
          <p>Trials in which the same individual image was presented were averaged over the two runs, resulting in a single event-related potential (ERP) for each image and each subject. To address the concern that regression results (see below) might be artificially high due to averaging of ERPs over repetitions, we also conducted all analyses using first-trial estimates only; these are reported in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s004">Fig. S4</xref></bold> and <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s005">S5</xref></bold>; the results were very similar to those obtained with repetition-averaged ERPs.</p>
        </sec>
        <sec id="s2c4">
          <title>Regression on single-image ERPs</title>
          <p>To test whether differences between evoked neural responses could be predicted by differences in contrast statistics between images, we conducted regression analyses on the single-image ERPs (<xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3A</bold></xref>). The preprocessed ERPs were read into Matlab, where we conducted linear regression analyses of ERP amplitude on image parameters using the Statistics Toolbox. For each subject, each channel and each time-point, two image parameters (Weibull parameters; Fourier parameters; skewness/kurtosis) were entered together as linear regressors on ERP amplitude. This analysis results in a measure of model fit (r<sup>2</sup>) over time (each sample of the ERP) and space (each electrode) for each individual subject.</p>
          <p>To compare the results between different sets of statistics directly (within each subject), we used the Akaike information criterion (AIC, <xref ref-type="bibr" rid="pcbi.1002726-Akaike1">[43]</xref> which measures the information contained in each set of predictors. In this procedure, we transformed the residual sum of squares (RSS) of the regression analysis based on each set of statistics into AIC-values using AIC = n*log(RSS/n)+2k where n = number of images and k is the number of predictors. AIC can be used for model selection given a set of candidate models of the same data, where the preferred model has minimum AIC-value <xref ref-type="bibr" rid="pcbi.1002726-Burnham1">[44]</xref>.</p>
          <p>To test whether the various image parameters explained any unique variance, we ran an additional regression analysis using a full model in which all three sets of image statistics were entered simultaneously (resulting in a 6 parameter model). We compared the results obtained with the full model with models for which, in turn, each parameter was left out; by subtracting the r<sup>2</sup> values of each of these partial models from the full model, we quantified unique variance explained by individual predictors.</p>
          <p>To correct for multiple comparisons, the p-values associated with the regression results were FDR-corrected at α = 0.05, unless stated otherwise.</p>
        </sec>
        <sec id="s2c5">
          <title>Representational similarity analysis</title>
          <p>To examine how variance between individual visual stimuli arises over time and space, we computed representational dissimilarity matrices (RDMs; <xref ref-type="bibr" rid="pcbi.1002726-Kriegeskorte1">[25]</xref>) based on spatial patterns of evoked ERP amplitude. In this type of analysis, dissimilarity between patterns of activity evoked by individual images (measured as 1-correlation or Euclidean distance) is determined across multiple recording sites simultaneously (e.g., voxels in fMRI, <xref ref-type="bibr" rid="pcbi.1002726-Kriegeskorte2">[45]</xref>). Here, we computed RDMs based on ERP amplitude at each time-point, using the spatial pattern of evoked activity across multiple electrode sites; we did this for each subject separately. Only electrodes showing substantial variance across the entire stimulus- and dataset were included (<bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s001">Fig. S1</xref></bold>); these were I1, I2, Iz, O1, O2, Oz, POz, PO7, PO8, P6 and P8. Based on this multi-electrode data, we computed (per subject and time-point) for all pairs of images the Euclidean distance between their evoked ERP amplitude patterns. As a result, we obtained RDMs containing 256×256 ‘dissimilarity’ values at each time-point of the ERP (<xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3B</bold></xref>). Within one RDM, each cell reflects similarity in ERP amplitude patterns of the corresponding two images indicated by the row- and column number. We used Euclidean distance to quantify dissimilarity rather than the 1–correlation measure recommended for fMRI data <xref ref-type="bibr" rid="pcbi.1002726-Kriegeskorte2">[45]</xref> because it corresponds more closely to the distance measure taken for the contrast statistics matrices (see below).</p>
        </sec>
        <sec id="s2c6">
          <title>Comparison with distance matrices based on contrast statistics</title>
          <p>To examine whether the dissimilarities between ERP patterns evoked by individual images could be predicted based on differences in contrast statistics, we computed pair-wise dissimilarity matrices based on the three sets of parameter values (Weibull statistics; Fourier statistics; distribution statistics). We computed the sum of the absolute differences between the (normalized) parameter values of each pair of images (reflecting distance in the parameter space formed by the image parameters, <xref ref-type="fig" rid="pcbi-1002726-g001"><bold>Fig. 1B</bold></xref>), resulting in one difference value between those two images. The matrices based on contrast statistics were compared with the RDMs based on the ERP data using a Mantel test for two-dimensional correlations <xref ref-type="bibr" rid="pcbi.1002726-Daniels1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Mantel1">[47]</xref>, denoted as r<sub>m</sub>. We computed these correlations for the average RDM across subjects as well as for single subjects RDMs. For the former, 95% confidence intervals for each correlation were assessed using a percentile bootstrap on the dissimilarity values <xref ref-type="bibr" rid="pcbi.1002726-Garthwaite1">[48]</xref> with number of bootstraps = 10.000 (∼40 * number of images).</p>
        </sec>
      </sec>
      <sec id="s2d">
        <title>Experiment 2: Behavior</title>
        <sec id="s2d1">
          <title>Behavioral data acquisition</title>
          <p>Twelve participants took part in the behavioral experiment; none of them had participated in the EEG experiment. The dead leaves images were presented on a 19-inch Dell monitor with a resolution of 1280×1024 pixels and a frame rate of 60 Hz. On each trial, a fixation cross appeared at the center of the screen; after an interval of 500 ms, a pair of images was presented simultaneously for 50 ms, separated by a gap of 236 pixels (<xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3C</bold></xref>). A mask followed after 100 ms, and stayed on screen for 200 ms. Participants were seated approximately 90 cm from the monitor; the stimulus display subtended 27×11° of visual angle. Subjects were instructed to indicate if the images were from the same or a different category by pressing one of two designated buttons on a keyboard (‘z’ and ‘m’) that were mapped to the left or the right hand. They completed four blocks of 256 trials each. In each block, the 256 trials were determined as follows: of the 16 images per category, 15 were paired with a randomly drawn image from another category (different-category comparisons); the 16th was paired with a randomly drawn image from the other 15 of its own category (same-category comparisons). Images were drawn without replacement, such that each image occurred only once in each block (with exception of the images that were selected for the same-category comparisons, which therefore occurred more often). Every possible different-category comparison thus occurred twice per block, and the ratio of different-category vs. same-category comparisons was 15∶1.</p>
          <p>Before testing, subjects were informed that for most trials the stimuli were different, and that only some were the same, preventing them from adopting a balanced response (50-50) strategy. Also, subjects were shown a few example stimuli and performed 20 practice trials (none of which appeared in the main experiment) before starting the actual experiment. Masks were created by randomly placing four mini-blocks of 16×16 pixels from each of the 256 stimuli in a 512×512 frame. Unique masks were randomly assigned to each trial. The same mask was presented at the location of both stimuli. Stimuli were presented using the Matlab Psychophysics Toolbox <xref ref-type="bibr" rid="pcbi.1002726-Pelli1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Brainard1">[50]</xref>.</p>
        </sec>
        <sec id="s2d2">
          <title>Behavioral data analysis</title>
          <p>In total, each possible combination of the 16 categories was presented 8 times in 4 consecutive blocks. Trials at which the subject failed to respond (&lt;1% for all subjects) within 1500 ms were discarded. Accuracy was determined by averaging across the four blocks. A mean confusion matrix was calculated by averaging accuracies across subjects separately for each specific combination of categories; we also calculated these matrices for each individual subject. We correlated both the mean confusion matrix and the individual matrices with classification accuracy based on contrast statistics (see below) using the Mantel test, resulting in one ‘mean’ and 12 individual correlation values. For these comparisons, the same-category comparisons were excluded (the Mantel test requires zero-values on the diagonal); they are included in the overall accuracy scores. Confidence intervals were determined using a percentile bootstrap (with number of bootstraps = 1000), which results in a 95% confidence interval along with the correlation.</p>
        </sec>
        <sec id="s2d3">
          <title>Classification analysis on contrast statistics</title>
          <p>To compare the behavioral performance with distance in contrast statistics, we performed leave-one-out classification analyses based on the parameter values of each set of contrast statistics (Weibull statistics; Fourier statistics; skewness/kurtosis). We used a simple algorithm that determines a single measure of classification accuracy based on the amount of overlap between different categories in parameter values. This involved the following steps: First, the median parameter values of each category were calculated. In turn, one of the 256 stimuli was selected, after which a temporary median of other 15 stimuli of its own category was determined. Next, the difference between its parameter values (beta and gamma for Weibull statistics; intercept and slope for Fourier statistics; skewness and kurtosis for distribution statistics) and the temporary median of its own category was calculated, as well as the difference with the median of all other categories. If the difference with its own category was less than the difference with any other category, this stimulus was counted as a ‘hit’, otherwise it was assigned a ‘miss’ (a cartoon example is shown in <xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3D</bold></xref>). Classification accuracy was determined by counting the percentage of hits out of all comparisons. To determine significance, binomial density probabilities across all combinations in the dataset were calculated (the likelihood of a hit occurring rather than a miss) based on which an FDR-threshold was established that was used to correct the pair-wise classification accuracy values for multiple comparisons. Using the mean values for each category rather than the median to determine distances between images between yielded very similar results as those reported here.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Contrast statistics</title>
        <p>If we set out all 256 dead leaves images against the three sets of image statistics (Weibull parameters, Fourier parameters and skewness/kurtosis), stimuli cluster by category in all cases, with Fourier parameters leading to the most separable clusters (<xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4A–C</bold></xref>). There were considerable correlations between the various parameters (<xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4D</bold></xref>; individual correlations plots in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s002">Fig. S2</xref></bold>). Skewness and kurtosis correlated highly (ρ = 0.91, p&lt;0.0001), but other significant correlations are observed as well, for example between Fourier slope and the Weibull beta parameter (ρ = 0.57, p&lt;0.0001) and also between the two Weibull parameters (ρ = 0.48, p&lt;0.001). A correlation of similar magnitude was also observed <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref> for natural scenes, supporting the notion that the dead leaves stimuli used here have similar low-level structure as natural stimuli.</p>
        <fig id="pcbi-1002726-g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Stimuli set out against their respective contrast statistics.</title>
            <p>Each data-point reflects parameter values for a single image, color-coded by category. Individual images are displayed against their (<bold>A</bold>), Weibull parameters beta and gamma, (<bold>B</bold>), Fourier parameters intercept and (increasing negative) slope and (<bold>C</bold>), distribution properties skewness and kurtosis. In all cases, clustering by category based on parameter values is evident. (<bold>D</bold>), Non-parametric correlations between the six image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), Skewness (Sk) and Kurtosis (Ku).</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g004" xlink:type="simple"/>
        </fig>
        <p>Interestingly, however, the ‘similarity spaces’ formed by each set of parameters are quite different between the various models. If Weibull parameters determine the axes of the similarity space (<xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4A</bold></xref>), highly cluttered images with many strong edges (e.g. 2D opaque stimuli with small disks) are located in the upper right corner (high gamma, high beta); images containing fewer edges (e.g. with larger disks) are found more on the left (low gamma); and most of the transparent stimuli, with weak edges, cluster together in the bottom of the space (low beta). For Fourier intercept and slope (<xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4B</bold></xref>), transparent categories are highly separated across the space: however, most images with strong edges end up in a similar part of the space (low slope, high intercept). Based on either skewness or kurtosis (<xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4C</bold></xref>), a few categories are distinct, but most tend to cluster together. These qualitative results suggest that all parameters are informative about clustering of image categories, but that they index different image properties.</p>
        <p>Importantly, they give rise to different predictions about which categories should lead to similar evoked responses based on overlapping parameter values. We tested these predictions using the single-image ERP data.</p>
      </sec>
      <sec id="s3b">
        <title>Experiment 1</title>
        <sec id="s3b1">
          <title>Contrast statistics explain variance in occipital ERPs</title>
          <p>Regression of single-image ERP amplitude (per subject, electrode and time-point) on contrast statistics showed that Weibull statistics explain a substantial amount of variance between individual images. Highest values were found at occipital channel Oz, where explained variance for all subjects reached a maximum between 100 and 210 ms after stimulus onset; maximal values ranged between r<sup>2</sup> = 0.12–0.80 (<xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5A</bold></xref>) and were highly significant (all p&lt;0.0001, FDR-corrected). For Fourier parameters (<xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5B</bold></xref>), somewhat lower values were found (max r<sup>2</sup> between 0.08–0.59, 100–210 ms; all p&lt;0.0001). For skewness and kurtosis (<xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5C</bold></xref>), explained variance was much lower and did not reach a consistent maximum during a specific time frame (max r<sup>2</sup> between 0.02–0.23 at 78–421 ms; maximal values were significant for 11 out of 19 subjects).</p>
          <fig id="pcbi-1002726-g005" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g005</object-id>
            <label>Figure 5</label>
            <caption>
              <title>Regression analysis of EEG data: single subject results.</title>
              <p>Explained variance of ERP amplitude at channel Oz over time, for each individual subject (colored thin lines) and mean across subjects (black thick line), using as regressors either (<bold>A</bold>), Weibull parameters beta and gamma, (<bold>B</bold>), Fourier parameters intercept and slope and (<bold>C</bold>), skewness and kurtosis; single-trial results of these analyses can be found in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s004">Fig. S4</xref></bold>. Insets display scalp plots of r<sup>2</sup> values for all electrodes at the time of maximal explained variance averaged over subjects (113 ms for Weibull/Fourier, 254 ms for skewness/kurtosis. (<bold>D</bold>), Grand average ERP amplitude (averaged over all subjects and all images) for an early and a late time-point of peak explained variance displayed in <bold>A–C</bold>.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g005" xlink:type="simple"/>
          </fig>
          <p>If we average the explained variance across subjects for each electrode separately at the time-points of maximal explained variance (113 ms for Weibull and Fourier statistics, 254 ms for skewness/kurtosis), we see (insets <xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5A–C</bold></xref>) that for all three sets of statistics, explained variance clusters around the midline occipital channels (Oz). Two weaker clusters were located near parietal electrodes, likely reflecting a dipole effect: both the early and late signals appear to originate from early visual areas (<xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5D</bold></xref>).</p>
          <p>These results demonstrate substantial differences in maximum explained variance between individual subjects. Inspection of the ERP recordings of each subject revealed a similarly large variability in subjects' signal-to-noise ratio (SNR, measured as the difference in ERP amplitude relative to pre-stimulus variability, reflecting the degree to which an evoked response is present). Indeed, the rank correlation between SNR and maximal explained variance by Weibull statistics was ρ = 0.69, p&lt;0.0014; see <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s003">Fig. S3</xref></bold>, which includes examples of subject-specific r<sup>2</sup> values alongside their single-image ERPs). This suggests that the observed variability in maximum explained variance is related to these subject-specific differences in SNR, which are in turn likely due to individual differences in cortical folding, scalp conductivity and recording conditions.</p>
          <p>In an alternative analysis performed on single-trial rather than single-image data (in which repeated presentations of the same stimulus were averaged, see <xref ref-type="sec" rid="s2">Materials and Methods</xref>), we found slightly lower explained variance for all models (maximal r<sup>2</sup> values: 0.71 for Weibull statistics, 0.52 for Fourier statistics, and 0.16 for skewness/kurtosis, respectively; see <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s004">Fig. S4</xref></bold>). Importantly, however, the relative differences between the sets of image parameters were fully consistent with those reported here.</p>
          <p>Overall, the regression results show that Weibull contrast statistics, but also Fourier statistics, reliably predict activity evoked by individual dead leaves images at the individual subject level. To investigate differences between the contributions of the different image predictors, we ran several additional analyses that are described below.</p>
        </sec>
        <sec id="s3b2">
          <title>Comparisons between different image parameters</title>
          <p>In order to compare differences in explained variance for Weibull statistics compared to the other statistics (<xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6A</bold></xref>), we used Akaike's information criterion (AIC) to evaluate the relative ‘goodness of fit’ of each of the three sets of contrast statistics. AIC is computed from the residuals of regression analyses (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>) and can be used for model selection given a set of candidate models of the same data, where the preferred model has minimum AIC-value. If we compare the mean AIC-value across individual subjects of Weibull, Fourier and skewness/kurtosis parameters over time, we find that the model fits start to diverge around 100 ms, with Weibull statistics leading to the lowest values (<xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6B</bold></xref>). It thus appears that Weibull parameters provide a better fit to the data than the other two sets of statistics. This could be related to the fact that the Weibull parameters characterize the histogram of contrast responses at a selected spatial scale, and may thus contain information reflected in both Fourier power spectra and higher-order properties of the contrast distribution. Therefore, we also computed AIC-values for intercept, slope, skewness and kurtosis combined into one regressor (<xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6B</bold></xref>, black line); the obtained values from this regression analysis are however still higher than those obtained from the Weibull parameters (significant differences between 117–140 ms, all t(19)&lt;−2.8, all p&lt;0.01). At the time-point of (mean) maximal explained variance (113 ms), the ordering of the different models in terms of AIC-values is consistent over subjects (<xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6C</bold></xref>): in all subjects, Weibull parameters lead to the best model fit, although differences are minimal for low SNR subjects. Interestingly, for subjects with high SNR, the distance between AIC-values for the Weibull model compared to the other contrast statistics appears to increase. These findings suggest that Weibull statistics capture additional variance relative to the other contrast statistics parameters considered here.</p>
          <fig id="pcbi-1002726-g006" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g006</object-id>
            <label>Figure 6</label>
            <caption>
              <title>AIC (Akaike information criterion) and unique explained variance analyses at channel Oz.</title>
              <p>(<bold>A</bold>), Mean explained variance across single subjects for Weibull (red), Fourier (blue) and skewness/kurtosis (green), respectively; shaded areas indicate S.E.M. (<bold>B</bold>), Mean AIC-value across single subjects computed from the residuals of each of the three regression models, as well as an additional model (black) consisting of Fourier and skewness/kurtosis values combined, showing that Weibull parameters provide the best fit to the data (low AIC-value); shaded areas indicate S.E.M. (<bold>C</bold>), Single subject AIC-values for the models displayed in <bold>B</bold> at the time-point of maximal explained variance for Weibull and Fourier statistics (113 ms); subjects are sorted based on independently determined SNR ratio (reported in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s002">Fig. S2</xref></bold>). (<bold>D</bold>), Unique explained variance by each set of contrast statistics. (<bold>E</bold>), Absolute, non-parametric correlations (Spearman's ρ) with ERP amplitude for the individual image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), distribution Skewness (Sk) and Kurtosis (Ku). Absolute values are plotted for convenience; shaded areas indicate S.E.M. (<bold>F</bold>), Unique explained variance by each individual parameter. Results for <bold>A–E</bold> based on single-trial rather than single-image data were highly similar (<bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s005">Fig. S5</xref></bold>).</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g006" xlink:type="simple"/>
          </fig>
          <p>To demonstrate this in a different way, we computed the unique variance contributed by each set of contrast statistics (r<sup>2</sup><sub>unique</sub>) by comparing partial models with a full model consisting of all 6 parameters (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). Unique explained variance for each set of statistics was low (r<sup>2</sup><sub>unique</sub> for Weibull parameters reached a maximum of 0.07 at 109 ms; for Fourier, max r<sup>2</sup><sub>unique</sub> was 0.05 at 180 ms; for skewness/kurtosis, max r<sup>2</sup><sub>unique</sub> was 0.04 at 203 ms), but clearly highest for the Weibull parameters in an extended early time interval (∼100–180 ms; <xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6D</bold></xref>). Given the substantial correlations between the various image parameters (reported in <xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4D</bold></xref>), we also tested the contribution of each parameter individually. From the correlations of individual parameters with ERP amplitude (<xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6E</bold></xref>), it can be readily seen that out of all parameters, the Weibull beta parameter correlates highest with the evoked activity in the early time-interval (max ρ = 0.57 at 121 ms, p&lt;0.001 in 18 out of 19 subjects, FDR-corrected); it also has highest unique explained variance (r<sup>2</sup><sub>unique</sub> reaching a max of 0.05 at 109 ms, <xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6F</bold></xref>), whereas the gamma parameter contributes unique variance somewhat later in time (max r<sup>2</sup><sub>unique</sub> was 0.04 at 164 ms), just before the Fourier parameters (a max of 0.03–0.04, around 175–180 ms).</p>
          <p>Taken together, these additional analyses suggest that the differences in regression results between the various sets of contrast statistics reflect reliable and consistent differences in information about the stimulus carried by these statistics, with Weibull statistics resulting in the best fit to the differences observed in the neural data.</p>
        </sec>
        <sec id="s3b3">
          <title>Clustering-by-category of ERPs is predicted by Weibull statistics</title>
          <p>The regression results indicate that the Weibull parameters are predictive of ERP amplitude, but do not reveal whether any categorical differences between ERPs are reflected in these parameters. To address this, we constructed representational dissimilarity matrices (RDMs) based on EEG activity. In this analysis, we computed RDMs of ERP amplitude using multiple electrodes as input (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>) for each subject separately. This approach is akin to performing multi-voxel pattern analysis in fMRI and calculating the dissimilarity between these activity patterns, but now comparing ERP amplitude differences across electrodes instead of voxels. We computed one RDM for each time-point of the ERP and averaged RDMs over subjects.</p>
          <p>To demonstrate how these matrices can convey information about categorical properties of evoked responses, we selected the time-point at which maximal dissimilarities were found (<xref ref-type="fig" rid="pcbi-1002726-g007"><bold>Fig. 7A</bold></xref>; 101 ms after stimulus-onset). In this subject-averaged RDM (<xref ref-type="fig" rid="pcbi-1002726-g007"><bold>Fig. 7B</bold></xref>), we observe clustering by category: the matrix appears to consist of small blocks of 16×16 images that are minimally dissimilar amongst themselves (diagonal values), but that tend to differ from other categories (off-diagonal values). Moreover, differences between these blocks show that some categories are more dissimilar than others. Specifically, opaque categories (upper left quadrant) differ from one another and from transparent categories (lower left/upper right quadrant) whereas the transparent categories themselves tend to be minimally dissimilar (lower right quadrant).</p>
          <fig id="pcbi-1002726-g007" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g007</object-id>
            <label>Figure 7</label>
            <caption>
              <title>Results of RDM analysis.</title>
              <p>(<bold>A</bold>), Maximum and mean Euclidean distance for the subject-averaged RDM: for both measures, highest dissimilarity between images was found at 101 ms after stimulus-onset. (<bold>B</bold>), Mean RDM across subjects at the moment of maximal Euclidean distance. Each cell of the matrix reflects the dissimilarity (red = high, blue = low) between two individual images, whose category is indexed on the x- and y-axis. (<bold>C</bold>), Dissimilarity matrices based on difference in contrast statistics between individual images. Color values indicate the summed difference between two individual images in beta and gamma (Weibull statistics), intercept and slope (Fourier statistics), skewness and kurtosis (distribution statistics). (<bold>D</bold>), Correlation between the RDM and each of the three dissimilarity matrices at each time-point. Highest correlation is found for Weibull statistics at 109 ms. Shaded areas reflect 95% confidence intervals obtained from a percentile bootstrap on the dissimilarity values.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g007" xlink:type="simple"/>
          </fig>
          <p>Next, we tested to what extent these category-specific differences between images in the ERP were predicted by contrast statistics. We calculated 256×256 distance matrices for each set of image parameters, in which we subtracted the parameter values of each image from the values of each other image (<xref ref-type="fig" rid="pcbi-1002726-g007"><bold>Fig. 7C</bold></xref>, see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). For example, for the first cell in the upper left corner of the Weibull statistics distance matrix, we summed the difference in beta and gamma values between image 1 and 2 (<sub>bim1</sub>−b<sub>im2</sub>+g<sub>im1</sub>−g<sub>im2</sub>), for the cell next to it between image 1 and 3, etc. For the other two sets of statistics, beta and gamma were replaced by intercept and slope or skewness and kurtosis.</p>
          <p>By visual inspection alone, it is clear that distances between individual images in Weibull statistics are most similar to the ERP dissimilarities. Inter-matrix correlations (Mantel tests, <xref ref-type="bibr" rid="pcbi.1002726-Burnham1">[44]</xref>) reveal that at nearly all time-points there is a substantially higher correlation of the RDM of the ERP signal with the distance matrix based on Weibull, relative to the other two statistics (<xref ref-type="fig" rid="pcbi-1002726-g007"><bold>Fig. 7D</bold></xref>). The highest correlations for Weibull and Fourier are found shortly after 100 ms (Weibull: r<sub>m</sub> = 0.67, 109 ms; Fourier: r<sub>m</sub> = 0.22, 113 ms) and are both significant after FDR-correction (p-values&lt;0.001), whereas the correlation between the RDMs and the skewness/kurtosis distance matrix does not reach significance. We also correlated the distance matrices based on contrast statistics with the subject-specific RDMs, confirming this result to be consistent over subjects; see <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s006">Fig. S6</xref></bold>. RDMs at all ERP time-points are provided in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s007">Video S1</xref></bold> in the form of a short movie clip.</p>
          <p>These results show that differences between image categories in ERP amplitude map onto differences in underlying Weibull statistics of individual images. Throughout the ERP, this model of low-level visual responses provides a better prediction of differences between images in neural response patterns than the other image parameters considered here. Moreover, the highest correlation between differences in Weibull statistics and ERP amplitude is near the time-point of maximal dissimilarity, where clustering by category in the ERP is clearly present. This clustering corresponds to the categorical organization in Weibull parameter space (<xref ref-type="fig" rid="pcbi-1002726-g002"><bold>Fig. 2A</bold></xref>), in which transparent categories were largely overlapping whereas stimuli with strong edges were more differentiated. In the next experiment, we asked whether this similarity space could not only predict early differences in ERP amplitude, but also behaviorally perceived similarity: do image categories with overlapping parameter values also look more alike?</p>
        </sec>
      </sec>
      <sec id="s3c">
        <title>Experiment 2</title>
        <sec id="s3c1">
          <title>Prediction of behavioral confusions</title>
          <p>Participants indicated for each possible combination of the 16 dead leaves categories whether these were the same or different category. Behavioral accuracy was high across all subjects (mean 93% correct, range 0.88–0.98), suggesting that subjects were well able to categorize these stimuli (<xref ref-type="fig" rid="pcbi-1002726-g008"><bold>Fig. 8A</bold></xref>). To generate specific predictions about categorical similarity based on contrast statistics, we conducted classification analyses using the distance between images in each of the three similarity spaces, testing how often proximity in parameter values resulted in classification of an image to another category than its own (see <xref ref-type="sec" rid="s2">Materials and Methods</xref> and <xref ref-type="fig" rid="pcbi-1002726-g003"><bold>Fig. 3D</bold></xref>). Mean classification accuracy based on distance in contrast statistics was high for all three sets of contrast statistics, with highest accuracy for the Fourier parameters (99%), subsequently for the Weibull parameters (94%) and finally for skewness/kurtosis (93%). Despite these high accuracies, errors were made in both behavior and classification: to test whether these errors occurred for specific combinations of categories, we summarized the average number of errors for each specific combination of categories in confusion matrices.</p>
          <fig id="pcbi-1002726-g008" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002726.g008</object-id>
            <label>Figure 8</label>
            <caption>
              <title>Behavioral results and comparison with classification.</title>
              <p>(<bold>A</bold>), Accuracy of behavioral categorization (open circles: single subjects, filled circle: mean) and of classification based on Weibull parameters, Fourier parameters or skewness and kurtosis. (<bold>B</bold>), Behavioral confusion matrix, displaying mean categorization accuracy for specific comparisons of categories. For each pair of categories the percentage of correct answers is displayed as a grayscale value. (<bold>C</bold>), Comparison of mean behavioral confusion matrix with classification results based on the three sets of contrast statistics. (<bold>D</bold>), Inter-matrix correlations of the classification errors for each set of statistics with the mean behavioral confusion matrix (left, mean) as well as those of individual participants (right, single subjects). For the mean correlation, error bars indicate 95% confidence intervals obtained using a percentile bootstrap on values within the mean confusion matrix.</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.g008" xlink:type="simple"/>
          </fig>
          <p>From the mean behavioral confusion matrix (<xref ref-type="fig" rid="pcbi-1002726-g008"><bold>Fig. 8B</bold></xref>), it is clear that subjects systematically confused certain categories more often than others. Specifically, transparent two- and three- dimensional images (dark squares in lower right quadrant) are more often confused than their opaque counterparts, although there were also some specific errors within opaque categories (upper left quadrant). Few errors were made between transparent and opaque categories. Although mean classification performance based on the Fourier parameters is highest, it is clear that the pattern of classification errors based on Weibull statistics most resembles the pattern of categorical confusions in behavior (<xref ref-type="fig" rid="pcbi-1002726-g008"><bold>Fig. 8C</bold></xref>). As expected, the behavioral confusion matrix correlated significantly with classification errors made based on Weibull parameters (r<sub>m</sub> = 0.46, p&lt;0.001), whereas classification based on differences in Fourier parameters or skewness/kurtosis did not correlate with human performance (r<sub>m</sub> = 0.07, p = 0.21 and r<sub>m</sub> = −0.20, p = 0.03, respectively; although significant, a negative correlation indicates that classification errors are <italic>opposed</italic> to categorization errors made by human participants). Correlations of individual confusion matrices confirm this result across all subjects (<xref ref-type="fig" rid="pcbi-1002726-g008"><bold>Fig. 8D</bold></xref>; range individual Weibull r<sub>m</sub>-values 0.33–0.46, all p&lt;0.005, FDR-corrected).</p>
          <p>These results show that perceived similarity of dead leaves image categories can be predicted based on differences in statistics of low-level contrast responses. Whereas mean classification accuracy for all image parameters was high, the different image parameters yielded different predictions about expected errors if categorization were to be based on these values. In the case of Fourier statistics, classification predicted that subjects would hardly confuse any categories at all, whereas skewness/kurtosis classification predicted that other categories would be confused with each other than those that subjects actually judged as similar. Only the Weibull parameters correlated with specific errors made by human subjects during rapid categorization.</p>
          <p>This suggests that out of the three similarity spaces presented in <xref ref-type="fig" rid="pcbi-1002726-g004"><bold>Fig. 4</bold></xref>, the arrangement of categories in Weibull space corresponds most closely to the actual perceptual similarity experienced by human subjects during a rapid categorization task.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>Low-level contrast statistics, derived from pooling of early visual responses, can predict similarity of early visual evoked responses as well as perceptual similarity of model natural scene images. We show that Weibull statistics, derived from the output of contrast filters modeled after LGN receptive fields, correlate with perceived similarity of computationally defined dead leaves categories. These statistics explain a significant amount of variance in the early visual ERP signal and correlate with behavioral categorization performance. Based on differences in these statistics, we were able to predict specific dissimilarities in the neural signal as well as specific category confusions.</p>
      <p>Interestingly, if we compare the results of experiment 1 and 2, we observe that subjects confused categories that were minimally dissimilar in ERP amplitude, which in turn were minimally different in Weibull statistics. Conversely, subjects accurately distinguished categories that were separable in their statistics, which was mirrored in high ERP dissimilarities. Also, correlations between Weibull statistics and neural responses were highest between 100 and 200 ms, well within the time frame that rapid categorization of natural images is thought to be constrained to <xref ref-type="bibr" rid="pcbi.1002726-Thorpe2">[51]</xref>.</p>
      <p>This work extends recent findings that statistical variations in low-level properties are important for understanding categorical generalization over single images <xref ref-type="bibr" rid="pcbi.1002726-Karklin1">[13]</xref>. It has been demonstrated before that behavioral categorization can be predicted using computational modeling of low-level information: a neural network consisting of local filters that were first allowed to adapt to natural scene statistics could predict behavioral performance on an object categorization task <xref ref-type="bibr" rid="pcbi.1002726-Serre1">[52]</xref>, and a computational model based on texture statistics accurately predicted human natural scene categorization performance <xref ref-type="bibr" rid="pcbi.1002726-Renninger1">[53]</xref>. Here, we expand on these results by showing that a geometric ‘similarity space’ formed by low-level contrast statistics can predict a complex pattern of categorization confusions of model natural scene images.</p>
      <sec id="s4a">
        <title>Implications for processing of real natural scenes</title>
        <p>Whether low-level statistics are indeed actively exploited during scene or object categorization is a topic of considerable debate. Whereas some studies report that manipulation of low-level properties influences rapid categorization accuracy <xref ref-type="bibr" rid="pcbi.1002726-Johnson1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Kaping1">[55]</xref> as well as early EEG responses <xref ref-type="bibr" rid="pcbi.1002726-Hansen1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Martinovic1">[57]</xref>, other studies have shown that not all early visual activity is obliterated by equation of those properties <xref ref-type="bibr" rid="pcbi.1002726-Philiastides1">[58]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-Rousselet4">[60]</xref> and, conversely, that early sensitivity to diagnostic information is revealed in stimuli that do not differ in low-level statistics <xref ref-type="bibr" rid="pcbi.1002726-vanRijsbergen1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Schyns1">[61]</xref>. We find that, at least for our set of simplified models of natural scene images, early differences in ERPs are correlated with low-level contrast statistics that are themselves also directly predictive of perceptual similarity.</p>
        <p>It is however likely that the degree to which low-level properties are relevant for processing of natural image categories is highly dependent on stimulus type and context, even within actual natural scene stimuli: for example, low-level information may influence rapid detection of faces to a larger extent than objects <xref ref-type="bibr" rid="pcbi.1002726-Gaspar1">[22]</xref> and the effects of low-level statistics on animal detection may interact with scene category (man-made vs. natural) <xref ref-type="bibr" rid="pcbi.1002726-Honey1">[62]</xref>. In addition, the present work is very different from these previous reports in that our experiments did not require formation of a high-level representation but only a same-different judgment. There are also notable differences between our ERP effects and those obtained with standardized object/scene categories: our maximum explained variance was found at around 100 ms, whereas those studies report sensitivity starting at 120 ms and onwards <xref ref-type="bibr" rid="pcbi.1002726-BaconMac1">[63]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-Smith1">[66]</xref>. Maximal sensitivity of evoked activity to faces and objects is found at lateral-occipital and parietal electrodes (PO, e.g. <xref ref-type="bibr" rid="pcbi.1002726-Philiastides1">[58]</xref>), whereas our correlations are clustered around occipital electrode Oz. This suggests that the dead leaves images may mostly engage mid-level areas of visual processing, such as those sensitive to textural information, e.g. V2 <xref ref-type="bibr" rid="pcbi.1002726-Groen1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Kastner1">[67]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-Freeman1">[69]</xref>. Our results implicate that clustering of image similarities at this level of processing can, in principle, already predict perceptual similarity – in turn, these similarities can be derived from Weibull contrast statistics. Given that for natural scenes, the Weibull statistics explain similar amounts of variance in EEG activity as reported here, we can hypothesize that image similarities as predicted by Weibull statistics are also present in evoked activity to actual natural scenes.</p>
      </sec>
      <sec id="s4b">
        <title>Information contained in contrast statistics</title>
        <p>If Weibull statistics indeed approximate meaningful global information in natural images, which image features do they convey? By manipulating computational image categories in their perceptual appearance, we were able to get a better understanding of the information contained in the Weibull parameters. They appear to index the amount of clutter, i.e. are related to occlusion and object size. These properties may be relevant for natural scene categorization: a forest has a higher degree of clutter (high gamma) and lower mean edge strength (high beta) compared to a beach scene. An image containing a few strong edges (low beta) that are sparsely distributed (low gamma) has high probability of coinciding with a single salient object, for example a single bird against an empty sky, suggesting that these statistics may be relevant for object detection in natural scenes. Here, behavioral confusions (and corresponding dissimilarities in ERP signals) were found between stimuli without coherent edge information (transparent stimuli with either large or small disks), or that were highly cluttered (opaque stimuli with small disks) which were exactly the categories that overlapped in Weibull parameter values.</p>
        <p>For comparison, we computed Fourier power spectra and higher-order properties of the contrast distribution (skewness and kurtosis), two sets of statistics that each index different sources of information in natural images: spatial frequency content and central moments of the contrast distribution, respectively. Deviations in the power spectra of natural images inform about variations in contrast across spatial scales: the slope and intercept parameters describe the ‘spectral signature’ of images <xref ref-type="bibr" rid="pcbi.1002726-Oliva2">[32]</xref> which is diagnostic of scene category <xref ref-type="bibr" rid="pcbi.1002726-Torralba1">[15]</xref>. Skewness and kurtosis were proposed to be relevant for texture perception <xref ref-type="bibr" rid="pcbi.1002726-Kingdom1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Ruderman1">[70]</xref> which in turn can be important for feature detection <xref ref-type="bibr" rid="pcbi.1002726-Renninger1">[53]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Malik1">[71]</xref> and the presence of featureless regions of images <xref ref-type="bibr" rid="pcbi.1002726-Brady1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Landy1">[72]</xref>. Our results confirm that both frequency content and central moments of the contrast distribution inform about image properties: both lead to accurate image classification. However, in the present study they did not predict neural and behavioral categorization patterns, suggesting that these statistics may not be plausible computations involved in visual processing of the dead leaves images.</p>
        <p>Even though we used controlled, computationally defined image categories, it is still possible that an image property other that the contrast statistics tested here will provide a better prediction of the (neural and behavioral) data, for example one of the manipulations used to create the image categories (e.g., opacity). However, neither the observed clustering-by-category of ERPs in the RDM, nor the pattern of categorization errors in behavior mapped clearly onto one of the manipulations used to create the categories (e.g., opaque vs. transparent; as is visible in <xref ref-type="fig" rid="pcbi-1002726-g007"><bold>Fig. 7B</bold></xref>, there are also differences <italic>within</italic> opaque and transparent categories, and this complex pattern of clustering is only predicted by Weibull statistics).</p>
      </sec>
      <sec id="s4c">
        <title>Explaining the advantage of Weibull statistics</title>
        <p>Why is the Weibull model better than widely used contrast statistics in predicting early neural and perceptual similarity? Although higher order moments of distributions can be diagnostic of textural differences, they may in practice be difficult for the visual system to represent <xref ref-type="bibr" rid="pcbi.1002726-Kingdom1">[35]</xref>. In addition, it has been suggested that rather than amplitude spectra, phase information derived from the Fourier transform <xref ref-type="bibr" rid="pcbi.1002726-Wichmann1">[73]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Loschky1">[74]</xref>, or the interaction between these two <xref ref-type="bibr" rid="pcbi.1002726-Gaspar2">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Joubert1">[76]</xref> contains diagnostic scene information. The reason that higher-order statistics derived from the phase spectrum may contain perceptually relevant information <xref ref-type="bibr" rid="pcbi.1002726-Doi1">[77]</xref> is that they carry edge information. In the Weibull model, contrasts, i.e. non-oriented edges, are explicitly computed (as the response of LGN-type neurons) and evaluated at multiple spatial scales. The model may thus be able to capture information contained both in power spectra (scale statistics) as well as central moments (distribution statistics). The Weibull parameters appear to reflect different aspects of low-level information: the beta parameter varies with the range of contrast strengths present in the image, reflecting overall <italic>contrast energy</italic>, whereas the gamma parameter varies with the degree of correlation between local contrast values, reflecting clutter or <italic>spatial coherence</italic>.</p>
        <p>Obviously, the Weibull fit is still a mathematical construct. However, the two parameters can also be approximated in a more biologically plausible way: with our previous single-scale model <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref>, we demonstrated that simple summation of X- and Y-type LGN output corresponded strikingly well with the fitted Weibull parameters. Similarly, if the outputs of the multi-scale filter banks used here (reflecting the entire range of receptive field sizes of the LGN) are linearly summed, we again obtain values that correlate highly with the Weibull parameters obtained from the contrast histogram at minimal reliable scale (S. Ghebreab, H.S. Scholte, V.A.F. Lamme, A.W.M Smeulders, under review). This suggests that Weibull estimation can in fact be reduced to pooling of neuronal population responses by summation, which is a biologically realistic operation.</p>
        <p>Why would summation of contrast responses of low-level neurons convey the same information as the Weibull parameters? This is likely a result of the structure of the world itself: distributions of contrast in natural images tend to range between power-law and Gaussian, which is the family of distributions that the Weibull function can capture <xref ref-type="bibr" rid="pcbi.1002726-Geusebroek1">[78]</xref>. It appears that this statistic simply provides a good characterization of the dynamic range of the low-level input to the visual cortex when viewing natural images. Since our brain developed in a natural world, early visual processing may take advantage of this regularity in estimating global properties to arrive at a first impression of scene content.</p>
      </sec>
      <sec id="s4d">
        <title>Outlook</title>
        <p>The present results extend our previous findings <xref ref-type="bibr" rid="pcbi.1002726-Scholte1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Ghebreab1">[18]</xref> with natural images to other image types (computational categories) and to prediction of behavioral categorization. Interestingly, even though the subjects in experiment 1 (EEG) were not engaged in categorization of the dead leaves images, their results generalize to the behavioral categorization patterns that were found in experiment 2, suggesting that similarity of bottom-up responses measured in EEG - in a different person - can be predictive of the perceived similarity during categorization of these images. This observation is now restricted to computationally defined categories. An interesting question for future work is whether in construction of high-level categorical representations of natural stimuli - considered a computationally challenging task - the brain actively exploits the pattern of variability of the population response to low-level information, estimated from early receptive field output. Contrary to the classical view of the visual hierarchy (e.g., <xref ref-type="bibr" rid="pcbi.1002726-Riesenhuber1">[79]</xref>) it has been proposed that a rapid, global percept of the input (gist) precedes a slow and detailed analysis of the scene <xref ref-type="bibr" rid="pcbi.1002726-Biederman1">[80]</xref>–<xref ref-type="bibr" rid="pcbi.1002726-Hochstein1">[83]</xref>. Natural image statistics provide a pointer to information that could be relevant for such a global percept <xref ref-type="bibr" rid="pcbi.1002726-Simoncelli1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1002726-Geisler1">[85]</xref>. However, the mechanism by which global information can be rapidly extracted from low-level properties is not directly evident from natural image statistics alone. As explained above, in our model, the statistics are derived from a biologically realistic substrate (the response of early visual contrast filters). We suggest that to build a realistic model of natural image categorization, it is essential to understand how statistics derived from very early, simple low-level responses can contribute to gist extraction.</p>
        <p>In conclusion, our findings suggest that global information based on low-level contrast can be available very early in visual processing and that this information can be relevant for judgment of perceptual similarity of controlled image categories.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002726.s001" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s001" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Selection of electrodes (Iz, I1, I2, Oz, O1, O2, POz, PO7, PO8, P6, P8) that were used as input to compute RDMs (dissimilarity matrices). Selection was based on standard deviation in ERP amplitude across the whole data set (all subjects and all images). Each line corresponds to a single electrode: only electrodes whose standard deviations crossed the dashed line were selected.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s002" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s002" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>Correlations of individual image parameters Weibull beta (<bold>A</bold>) and gamma (<bold>B</bold>) with Fourier intercept, Fourier slope, skewness and kurtosis values.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s003" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s003" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p>Left: Correlation between subject-specific signal-to-noise ratio (SNR) and maximal explained variance (across all electrodes). SNR was computed by 1) per electrode, averaging the mean ERP amplitude across the 256 images over all post-stimulus time-points, 2) dividing the absolute value of this average by the standard deviation of all pre-stimulus time-points and 3) averaging the resulting SNR values over electrodes. The SNR-values thus reflect the degree to which stimulus-related ERP amplitude is present relative to baseline fluctuations. Right: two examples of evoked responses (CSD-transformed) for the 256 individual stimuli and corresponding explained variance values at channel Oz. Top: example of high SNR single-subject data; an ERP is clearly visible in individual trials; explained variance based on contrast statistics is high. Bottom: example of low SNR single-subject data; an evoked response is hardly discernable in the individual trials; explained variance based on contrast statistics is low. This result elegantly shows that if there is no evoked response present in the EEG signal, there is no stimulus-related variance to be explained by differences in contrast statistics.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s004" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s004" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p>Explained variance values at channel Oz as reported in <xref ref-type="fig" rid="pcbi-1002726-g005"><bold>Fig. 5A–C</bold></xref>, but now computed based on non-averaged single-trial ERPs (compared to single-image ERPs that are averaged over repeats). As regressors, we used either (<bold>A</bold>), Weibull beta and gamma, (<bold>B</bold>), Fourier intercept and slope and (<bold>C</bold>), skewness and kurtosis. Colored thin lines: r<sup>2</sup> values for individual subjects. Black thick line: mean r<sup>2</sup> across subjects.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s005" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s005" xlink:type="simple">
        <label>Figure S5</label>
        <caption>
          <p>AIC and unique variance analyses at channel Oz as reported in <xref ref-type="fig" rid="pcbi-1002726-g006"><bold>Fig. 6</bold></xref>, but now computed based on non-averaged single-trial ERPs (compared to single-image ERPs that are averaged over repeats). (<bold>A</bold>), Mean explained variance across subjects for Weibull (red), Fourier (blue) and skewness/kurtosis (green); shaded areas indicate S.E.M. (<bold>B</bold>), Mean AIC-value across single subjects computed from the residuals of each of the three regression models, as well as an additional model (black) consisting of Fourier and skewness/kurtosis values combined, shaded areas indicate S.E.M. (<bold>C</bold>), Single subject AIC-values at the time-point of maximal explained variance for Weibull and Fourier statistics (113 ms); subjects are sorted based on SNR ratio (reported in <bold><xref ref-type="supplementary-material" rid="pcbi.1002726.s002">Fig. S2</xref></bold>). (<bold>D</bold>), Unique explained variance by each set of contrast statistics. (<bold>E</bold>), Absolute, non-parametric correlations (Spearman's ρ) with ERP amplitude for the individual image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), distribution Skewness (Sk) and Kurtosis (Ku). Absolute values are plotted for convenience; shaded areas indicate S.E.M. (<bold>F</bold>), Unique explained variance by each individual image parameter.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s006" mimetype="image/tiff" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s006" xlink:type="simple">
        <label>Figure S6</label>
        <caption>
          <p>Single-subject correlations of dissimilarity matrices (RDMs) of ERPs with distance matrices based on the three sets of contrast statistics: (<bold>A</bold>), Weibull parameters, (<bold>B</bold>), Fourier parameters and (<bold>C</bold>), skewness and kurtosis.</p>
          <p>(TIF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002726.s007" mimetype="video/mpeg" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002726.s007" xlink:type="simple">
        <label>Video S1</label>
        <caption>
          <p>Representational dissimilarity matrices at each sample in time of the ERP, starting 50 ms before until 350 ms after stimulus-onset. Dissimilarity between stimuli is measured as Euclidean distance (red = maximal, blue = minimal values in entire data set) between ERP patterns across occipital electrodes (see <xref ref-type="sec" rid="s2">Materials and Methods</xref>). Categories are labeled on the x- and y-axis; each cell of the matrix indexes the dissimilarity between two individual stimuli. Differences between images suddenly emerge around 90 ms after stimulus-onset and disappear again about 60 ms later. These differences cluster in 16×16 blocks, suggesting that categorical information is present in this time period. Later in time, weaker differences arise, but not as large as before, suggesting that category-specific dissimilarities between stimuli are evoked early in time.</p>
          <p>(MPG)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Judith Tankink for EEG data collection and Romke Rouw and Olympia Colizoli for helpful comments on earlier versions of this manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002726-Potter1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potter</surname><given-names>MC</given-names></name> (<year>1975</year>) <article-title>Meaning in visual search</article-title>. <source>Science</source> <volume>187</volume>: <fpage>965</fpage>–<lpage>966</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Greene1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greene</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>The briefest of glances: the time course of natural scene understanding</article-title>. <source>Psych Sci</source> <volume>20</volume>: <fpage>464</fpage>–<lpage>472</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-FeiFei1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Perona</surname><given-names>P</given-names></name> (<year>2002</year>) <article-title>Rapid natural scene categorization in the near absence of attention</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>99</volume>: <fpage>9596</fpage>–<lpage>9601</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Thorpe1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Fize</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Marlot</surname><given-names>C</given-names></name> (<year>1996</year>) <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>520</fpage>–<lpage>522</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kirchner1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kirchner</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2006</year>) <article-title>Ultra-rapid object detection with saccadic eye movements: visual processing speed revisited</article-title>. <source>Vision Res</source> <volume>46</volume>: <fpage>1762</fpage>–<lpage>1776</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-VanRullen1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2001</year>) <article-title>The time course of visual processing: from early perception to decision-making</article-title>. <source>J Cogn Neurosci</source> <volume>13</volume>: <fpage>454</fpage>–<lpage>461</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Rousselet1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2002</year>) <article-title>Parallel processing in high-level categorization of natural images</article-title>. <source>Nature Neurosci</source> <volume>5</volume>: <fpage>629</fpage>–<lpage>630</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Peelen1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peelen</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kastner</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Neural mechanisms of rapid natural scene categorization in human visual cortex</article-title>. <source>Nature</source> <volume>460</volume>: <fpage>94</fpage>–<lpage>97</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Field1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1987</year>) <article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title>. <source>J Opt Soc Am A</source> <volume>4</volume>: <fpage>2379</fpage>–<lpage>2394</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Vinje1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vinje</surname><given-names>WE</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2000</year>) <article-title>Sparse coding and decorrelation in primary visual cortex during natural vision</article-title>. <source>Science</source> <volume>287</volume>: <fpage>1273</fpage>–<lpage>1276</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Schwartz1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2001</year>) <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nature Neurosci</source> <volume>4</volume>: <fpage>819</fpage>–<lpage>825</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Olshausen1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1996</year>) <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>607</fpage>–<lpage>610</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Karklin1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karklin</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name> (<year>2009</year>) <article-title>Emergence of complex cell properties by learning to generalize in natural scenes</article-title>. <source>Nature</source> <volume>457</volume>: <fpage>83</fpage>–<lpage>86</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Frazor1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frazor</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name> (<year>2006</year>) <article-title>Local luminance and contrast in natural images</article-title>. <source>Vision Res</source> <volume>46</volume>: <fpage>1585</fpage>–<lpage>1598</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Torralba1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Statistics of natural image categories</article-title>. <source>Network</source> <volume>14</volume>: <fpage>391</fpage>–<lpage>412</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Graham1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graham</surname><given-names>N</given-names></name> (<year>1979</year>) <article-title>Does the brain perform a Fourier analysis of the visual scene?</article-title> <source>Trends Neurosci</source> <volume>2</volume>: <fpage>207</fpage>–<lpage>208</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Scholte1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scholte</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Ghebreab</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Waldorp</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Smeulders</surname><given-names>AWM</given-names></name>, <name name-style="western"><surname>Lamme</surname><given-names>VAF</given-names></name> (<year>2009</year>) <article-title>Brain responses strongly correlate with Weibull image statistics when processing natural images</article-title>. <source>J Vis</source> <volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Ghebreab1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ghebreab</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Smeulders</surname><given-names>AWM</given-names></name>, <name name-style="western"><surname>Scholte</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Lamme</surname><given-names>VAF</given-names></name> (<year>2009</year>) <article-title>A biologically plausible model for rapid natural image identification</article-title>. <source>Adv Neural Inf Process Syst</source> <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Oliva1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Building the gist of a scene: the role of global image features in recognition</article-title>. <source>Prog Brain Res</source> <volume>155</volume>: <fpage>23</fpage>–<lpage>36</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-vanRijsbergen1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Rijsbergen</surname><given-names>NJ</given-names></name>, <name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name> (<year>2009</year>) <article-title>Dynamics of trimming the content of face representations for categorization in the brain</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000561</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Rousselet2">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Pernet</surname><given-names>CR</given-names></name> (<year>2011</year>) <article-title>Quantifying the time course of visual object processing using ERPs: It's time to up the game</article-title>. <source>Front Psych</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Gaspar1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaspar</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Pernet</surname><given-names>CR</given-names></name> (<year>2011</year>) <article-title>Reliability of ERP and single-trial analyses</article-title>. <source>Neuroimage</source> <volume>58</volume>: <fpage>620</fpage>–<lpage>629</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kahn1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kahn</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Wolk</surname><given-names>DA</given-names></name> (<year>2010</year>) <article-title>Temporally distinct neural coding of perceptual similarity and prototype bias</article-title>. <source>J Vis</source> <volume>10</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Groen1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groen</surname><given-names>IIA</given-names></name>, <name name-style="western"><surname>Ghebreab</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lamme</surname><given-names>VAF</given-names></name>, <name name-style="western"><surname>Scholte</surname><given-names>HS</given-names></name> (<year>2012</year>) <article-title>Low-level contrast statistics are diagnostic of invariance of natural textures</article-title>. <source>Front Comput Neurosci</source> <volume>6</volume>: <fpage>34</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kriegeskorte1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Front Syst Neurosci</source> <volume>2</volume>: <fpage>4</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kravitz1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kravitz</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Peng</surname><given-names>CS</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>CI</given-names></name> (<year>2011</year>) <article-title>Real-world scene representations in high-level visual cortex: it's the spaces more than the places</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>7322</fpage>–<lpage>7333</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Ross1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ross</surname><given-names>MG</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2010</year>) <article-title>Estimating perception of scene layout properties from global image features</article-title>. <source>J Vis</source> <volume>10</volume>: <fpage>1</fpage>–<lpage>25</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Hsiao1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hsiao</surname><given-names>WH</given-names></name>, <name name-style="western"><surname>Millane</surname><given-names>RP</given-names></name> (<year>2005</year>) <article-title>Effects of occlusion, edges, and scaling on the power spectra of natural images</article-title>. <source>J Opt Soc Am A</source> <volume>22</volume>: <fpage>1789</fpage>–<lpage>1797</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Shepard1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepard</surname><given-names>RN</given-names></name> (<year>1964</year>) <article-title>Attention and the metric structure of the stimulus space</article-title>. <source>J Math Psych</source> <volume>1</volume>: <fpage>54</fpage>–<lpage>87</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Drucker1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Drucker</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Aguirre</surname><given-names>GK</given-names></name> (<year>2009</year>) <article-title>Different spatial scales of shape similarity representation in lateral and ventral LOC</article-title>. <source>Cereb Cortex</source> <volume>19</volume>: <fpage>2269</fpage>–<lpage>2280</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-OpdeBeeck1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Op de Beeck</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Wagemans</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vogels</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>The representation of perceived shape similarity and its role for category learning in monkeys: a modeling study</article-title>. <source>Vision Res</source> <volume>48</volume>: <fpage>598</fpage>–<lpage>610</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Oliva2">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Modeling the shape of the scene: A holistic representation of the spatial envelope</article-title>. <source>Int J Comput Vis</source> <volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Tadmor1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tadmor</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Tolhurst</surname><given-names>DJ</given-names></name> (<year>2000</year>) <article-title>Calculating the contrasts that retinal ganglion cells and LGN neurones encounter in natural scenes</article-title>. <source>Vision Res</source> <volume>40</volume>: <fpage>3145</fpage>–<lpage>3157</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Brady1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brady</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>2000</year>) <article-title>Local contrast in natural images: normalisation and coding efficiency</article-title>. <source>Perception</source> <volume>29</volume>: <fpage>1041</fpage>–<lpage>1055</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kingdom1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kingdom</surname><given-names>FAA</given-names></name>, <name name-style="western"><surname>Hayes</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>2001</year>) <article-title>Sensitivity to contrast histogram differences in synthetic wavelet-textures</article-title>. <source>Vision Res</source> <volume>41</volume>: <fpage>585</fpage>–<lpage>598</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Bonin1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bonin</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>The suppressive field of neurons in lateral geniculate nucleus</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>10844</fpage>–<lpage>10856</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Croner1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Croner</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Kaplan</surname><given-names>E</given-names></name> (<year>1995</year>) <article-title>Receptive fields of P and M ganglion cells across the primate retina</article-title>. <source>Vision Res</source> <volume>35</volume>: <fpage>7</fpage>–<lpage>24</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Elder1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elder</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Zucker</surname><given-names>SW</given-names></name> (<year>1998</year>) <article-title>Local scale control for edge detection and blur estimation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source> <volume>20</volume>: <fpage>699</fpage>–<lpage>716</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Deng1">
        <label>39</label>
        <mixed-citation publication-type="other" xlink:type="simple">Deng J, Dong W, Socher R, Li L-J, Li K, <etal>et al</etal>.. (2009) ImageNet: A large-scale hierarchical image database. In: Proceedings of the Conference on Computer Vision and Pattern Recognition; 2–25 June 2009; Miami, FL, 248–255. CPVR 2009.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Gratton1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gratton</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Coles</surname><given-names>MGH</given-names></name>, <name name-style="western"><surname>Donchin</surname><given-names>E</given-names></name> (<year>1983</year>) <article-title>A new method for off-line removal of ocular artifact</article-title>. <source>Electroencephalogr Clin Neurophysiol</source> <volume>55</volume>: <fpage>468</fpage>–<lpage>484</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Perrin1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrin</surname><given-names>F</given-names></name> (<year>1989</year>) <article-title>Spherical splines for scalp potential and current density mapping</article-title>. <source>Electroencephalogr Clin Neurophysiol</source> <volume>72</volume>: <fpage>184</fpage>–<lpage>187</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Nunez1">
        <label>42</label>
        <mixed-citation publication-type="other" xlink:type="simple">Nunez PL, Srinivasan R (2006) The neurophysics of EEG. 2nd ed,. Oxford, UK: Oxford University Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Akaike1">
        <label>43</label>
        <mixed-citation publication-type="other" xlink:type="simple">Akaike H (1973) Information theory and an extension of the maximum likelihood principle. In: Petrov BN, Csaki F, editors. Second International Symposium on Information Theory. Budapest: Akademiai Kiado. pp. 267–281.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Burnham1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burnham</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>DR</given-names></name> (<year>2004</year>) <article-title>Multimodel Inference: Understanding AIC and BIC in Model Selection</article-title>. <source>Sociol Methods Res</source> <volume>33</volume>: <fpage>261</fpage>–<lpage>304</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kriegeskorte2">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mur</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ruff</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Kiani</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bodurka</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title>. <source>Neuron</source> <volume>60</volume>: <fpage>1126</fpage>–<lpage>1141</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Daniels1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daniels</surname><given-names>HE</given-names></name> (<year>1944</year>) <article-title>The relation between measures of correlation in the universe of sample permutations</article-title>. <source>Biometrika</source> <volume>33</volume>: <fpage>129</fpage>–<lpage>135</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Mantel1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mantel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Valand</surname><given-names>RS</given-names></name> (<year>1970</year>) <article-title>A technique of nonparametric multivariate analysis</article-title>. <source>Biometrics</source> <volume>26</volume>: <fpage>547</fpage>–<lpage>558</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Garthwaite1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garthwaite</surname><given-names>PH</given-names></name> (<year>1996</year>) <article-title>Confidence intervals from randomization tests</article-title>. <source>Biometrics</source> <volume>52</volume>: <fpage>1387</fpage>–<lpage>1393</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Pelli1">
        <label>49</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelli</surname><given-names>DG</given-names></name> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source> <volume>10</volume>: <fpage>437</fpage>–<lpage>442</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Brainard1">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname><given-names>DH</given-names></name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>: <fpage>433</fpage>–<lpage>436</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Thorpe2">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thorpe</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>The speed of categorization in the human visual system</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>168</fpage>–<lpage>170</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Serre1">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2007</year>) <article-title>A feedforward architecture accounts for rapid categorization</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>104</volume>: <fpage>6424</fpage>–<lpage>6429</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Renninger1">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Renninger</surname><given-names>LW</given-names></name>, <name name-style="western"><surname>Malik</surname><given-names>J</given-names></name> (<year>2004</year>) <article-title>When is scene identification just texture recognition?</article-title> <source>Vision Res</source> <volume>44</volume>: <fpage>2301</fpage>–<lpage>2311</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2004.04.006" xlink:type="simple">10.1016/j.visres.2004.04.006</ext-link>.</comment></mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Johnson1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name> (<year>2003</year>) <article-title>Timecourse of neural signatures of object recognition</article-title>. <source>J Vis</source> <volume>3</volume>: <fpage>499</fpage>–<lpage>512</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kaping1">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaping</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tzvetanov</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Treue</surname><given-names>S</given-names></name> (<year>2007</year>) <article-title>Adaptation to statistical properties of visual scenes biases rapid categorization</article-title>. <source>Vis Cogn</source> <volume>15</volume>: <fpage>12</fpage>–<lpage>19</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Hansen1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Jacques</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Ellemberg</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>From spatial frequency contrast to edge preponderance: the differential modulation of early visual evoked potentials by natural scene stimuli</article-title>. <source>Vis Neurosci</source> <volume>28</volume>: <fpage>221</fpage>–<lpage>237</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Martinovic1">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martinovic</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mordal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wuerger</surname><given-names>SM</given-names></name> (<year>2011</year>) <article-title>Event-related potentials reveal an early advantage for luminance contours in the processing of objects</article-title>. <source>J Vis</source> <volume>11</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Philiastides1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname><given-names>MG</given-names></name>, <name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sajda</surname><given-names>P</given-names></name> (<year>2006</year>) <article-title>Neural representation of task difficulty and decision making during perceptual categorization: A timing diagram</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>8965</fpage>–<lpage>8975</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Rousselet3">
        <label>59</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Husk</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Bennett</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname><given-names>AB</given-names></name> (<year>2008</year>) <article-title>Time course and robustness of ERP object and face differences</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Rousselet4">
        <label>60</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Husk</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Bennett</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname><given-names>AB</given-names></name> (<year>2005</year>) <article-title>Spatial scaling factors explain eccentricity effects on face ERPs</article-title>. <source>J Vis</source> <volume>5</volume>: <fpage>755</fpage>–<lpage>763</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Schyns1">
        <label>61</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>, <name name-style="western"><surname>Thut</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gross</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Cracking the code of oscillatory activity</article-title>. <source>PLoS Biol</source> <volume>9</volume>: <fpage>e1001064</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Honey1">
        <label>62</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Honey</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kirchner</surname><given-names>H</given-names></name>, <name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Faces in the cloud: Fourier power spectrum biases ultrarapid face detection</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-BaconMac1">
        <label>63</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bacon-Macé</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Macé</surname><given-names>MJ-M</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name> (<year>2005</year>) <article-title>The time course of visual processing: backward masking and natural scene categorisation</article-title>. <source>Vision Res</source> <volume>45</volume>: <fpage>1459</fpage>–<lpage>1469</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Philiastides2">
        <label>64</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philiastides</surname><given-names>MG</given-names></name>, <name name-style="western"><surname>Sajda</surname><given-names>P</given-names></name> (<year>2006</year>) <article-title>Temporal characterization of the neural correlates of perceptual decision making in the human brain</article-title>. <source>Cereb Cortex</source> <volume>16</volume>: <fpage>509</fpage>–<lpage>518</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Rousselet5">
        <label>65</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Pernet</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Bennett</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Sekuler</surname><given-names>AB</given-names></name> (<year>2008</year>) <article-title>Parametric study of EEG sensitivity to phase noise during face processing</article-title>. <source>BMC Neurosci</source> <volume>9</volume>: <fpage>98</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Smith1">
        <label>66</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name> (<year>2009</year>) <article-title>Inverse mapping the neuronal substrates of face categorizations</article-title>. <source>Cereb Cortex</source> <volume>19</volume>: <fpage>2428</fpage>–<lpage>2438</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Kastner1">
        <label>67</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kastner</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Weerd</surname><given-names>PD</given-names></name>, <name name-style="western"><surname>Ungerleider</surname><given-names>LG</given-names></name> (<year>2000</year>) <article-title>Texture segregation in the human visual cortex: A functional MRI study</article-title>. <source>J Neurophys</source> <volume>83</volume>: <fpage>2453</fpage>–<lpage>2457</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Scholte2">
        <label>68</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scholte</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Jolij</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Fahrenfort</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Lamme</surname><given-names>VAF</given-names></name> (<year>2008</year>) <article-title>Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging</article-title>. <source>J Cogn Neurosci</source> <volume>20</volume>: <fpage>2097</fpage>–<lpage>2109</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Freeman1">
        <label>69</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freeman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2011</year>) <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neurosci</source> <volume>14</volume>: <fpage>1195</fpage>–<lpage>1201</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Ruderman1">
        <label>70</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruderman</surname><given-names>D</given-names></name> (<year>1994</year>) <article-title>The statistics of natural images</article-title>. <source>Network</source> <volume>5</volume>: <fpage>517</fpage>–<lpage>548</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Malik1">
        <label>71</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malik</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Perona</surname><given-names>P</given-names></name> (<year>1990</year>) <article-title>Preattentive texture discrimination with early vision mechanisms</article-title>. <source>J Opt Soc Am A</source> <volume>7</volume>: <fpage>923</fpage>–<lpage>932</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Landy1">
        <label>72</label>
        <mixed-citation publication-type="other" xlink:type="simple">Landy MS, Graham N (2004) Visual perception of texture. In: Chalupa LM, Werner JS, editors. The visual neurosciences. Cambridge, MA; MIT Press. pp. 1106–1118.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Wichmann1">
        <label>73</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wichmann</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Braun</surname><given-names>DI</given-names></name>, <name name-style="western"><surname>Gegenfurtner</surname><given-names>KR</given-names></name> (<year>2006</year>) <article-title>Phase noise and the classification of natural images</article-title>. <source>Vision Res</source> <volume>46</volume>: <fpage>1520</fpage>–<lpage>1529</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Loschky1">
        <label>74</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loschky</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Larson</surname><given-names>AM</given-names></name> (<year>2008</year>) <article-title>Localized information is necessary for scene categorization, including the natural/man-made distinction</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Gaspar2">
        <label>75</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaspar</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name> (<year>2009</year>) <article-title>How do amplitude spectra influence rapid animal detection?</article-title> <source>Vision Res</source> <volume>49</volume>: <fpage>3001</fpage>–<lpage>3012</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Joubert1">
        <label>76</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joubert</surname><given-names>OR</given-names></name>, <name name-style="western"><surname>Rousselet</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Fabre-Thorpe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fize</surname><given-names>D</given-names></name> (<year>2009</year>) <article-title>Rapid visual categorization of natural scene contexts with equalized amplitude spectrum and increasing phase noise</article-title>. <source>J Vis</source> <volume>9</volume>: <fpage>2.1</fpage>–<lpage>16</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Doi1">
        <label>77</label>
        <mixed-citation publication-type="other" xlink:type="simple">Doi E, Lewicki MS (2005) Relations between the statistical regularities of natural images and the response properties of the early visual system. In: Proceedings of the Workshop of Special Interest Group of Pattern Recognition and Perception Model (SIG P&amp;P); 28 July 2005; Kyoto University 2005. Japanese Cognitive Science Society, pp. 1–8.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Geusebroek1">
        <label>78</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geusebroek</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Smeulders</surname><given-names>AWM</given-names></name> (<year>2005</year>) <article-title>A six-stimulus theory for stochastic texture</article-title>. <source>Int J Comput Vis</source> <volume>62</volume>: <fpage>7</fpage>–<lpage>16</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Riesenhuber1">
        <label>79</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature Neurosci</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Biederman1">
        <label>80</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name> (<year>1972</year>) <article-title>Perceiving real world scenes</article-title>. <source>Science</source> <volume>177</volume>: <fpage>77</fpage>–<lpage>80</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Oliva3">
        <label>81</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name> (<year>1997</year>) <article-title>Coarse blobs or fine edges? Evidence that information diagnosticity changes the perception of complex visual stimuli</article-title>. <source>Cogn Psychol</source> <volume>34</volume>: <fpage>72</fpage>–<lpage>107</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Oliva4">
        <label>82</label>
        <mixed-citation publication-type="other" xlink:type="simple">Oliva A (2005) Gist of the Scene. In: Itti L, Rees G, Tsotsos JK, editors. The Encyclopedia of neurobiology of attention. San Francisco, CA: Elsevier. pp. 251–257.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Hochstein1">
        <label>83</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ahissar</surname><given-names>M</given-names></name> (<year>2002</year>) <article-title>View from the top: Hierarchies and reverse hierarchies in the visual system</article-title>. <source>Neuron</source> <volume>36</volume>: <fpage>791</fpage>–<lpage>804</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Simoncelli1">
        <label>84</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name> (<year>2001</year>) <article-title>Natural image statistics and neural representation</article-title>. <source>Annu Rev Neurosci</source> <volume>24</volume>: <fpage>1193</fpage>–<lpage>1216</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Geisler1">
        <label>85</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geisler</surname><given-names>WS</given-names></name> (<year>2008</year>) <article-title>Visual perception and the statistical properties of natural scenes</article-title>. <source>Annu Rev Psychol</source> <volume>59</volume>: <fpage>167</fpage>–<lpage>192</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002726-Olmos1">
        <label>86</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olmos</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kingdom</surname><given-names>FAA</given-names></name> (<year>2004</year>) <article-title>A biologically inspired algorithm for the recovery of shading and reflectance images</article-title>. <source>Perception</source> <volume>33</volume>: <fpage>1463</fpage>–<lpage>1473</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>