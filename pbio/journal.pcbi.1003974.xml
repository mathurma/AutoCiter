<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00608</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003974</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Biophysics</subject><subj-group><subject>Biophysics theory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Thermodynamic Costs of Information Processing in Sensory Adaptation</article-title>
<alt-title alt-title-type="running-head">Information Costs in Sensing</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sartori</surname><given-names>Pablo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Granger</surname><given-names>Léo</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>Chiu Fan</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Horowitz</surname><given-names>Jordan M.</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Max Planck Institute for the Physics of Complex Systems, Dresden, Germany</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Departamento de Física Atómica, Molecular y Nuclear and GISC, Universidad Complutense de Madrid, Madrid, Spain</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department of Bioengineering, Imperial College London, London, United Kingdom</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Department of Physics, University of Massachusetts at Boston, Boston, Massachusetts, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Beard</surname><given-names>Daniel A.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Michigan, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">pablosv@pks.mpg.de</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: PS LG CFL JMH. Performed the experiments: PS LG CFL JMH. Analyzed the data: PS LG CFL JMH. Contributed reagents/materials/analysis tools: PS LG CFL JMH. Wrote the paper: PS LG CFL JMH.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>12</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>11</day><month>12</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>12</issue>
<elocation-id>e1003974</elocation-id>
<history>
<date date-type="received"><day>7</day><month>4</month><year>2014</year></date>
<date date-type="accepted"><day>8</day><month>10</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Sartori et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Biological sensory systems react to changes in their surroundings. They are characterized by fast response and slow adaptation to varying environmental cues. Insofar as sensory adaptive systems map environmental changes to changes of their internal degrees of freedom, they can be regarded as computational devices manipulating information. Landauer established that information is ultimately physical, and its manipulation subject to the entropic and energetic bounds of thermodynamics. Thus the fundamental costs of biological sensory adaptation can be elucidated by tracking how the information the system has about its environment is altered. These bounds are particularly relevant for small organisms, which unlike everyday computers, operate at very low energies. In this paper, we establish a general framework for the thermodynamics of information processing in sensing. With it, we quantify how during sensory adaptation information about the past is erased, while information about the present is gathered. This process produces entropy larger than the amount of old information erased and has an energetic cost bounded by the amount of new information written to memory. We apply these principles to the <italic>E. coli</italic>'s chemotaxis pathway during binary ligand concentration changes. In this regime, we quantify the amount of information stored by each methyl group and show that receptors consume energy in the range of the information-theoretic minimum. Our work provides a basis for further inquiries into more complex phenomena, such as gradient sensing and frequency response.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The ability to process information is a ubiquitous feature of living organisms. Indeed, in order to survive, every living being, from the smallest bacterium to the biggest mammal, has to gather and process information about its surrounding environment. In the same way as our everyday computers need power to function, biological sensors need energy in order to gather and process this sensory information. How much energy do living organisms have to spend in order to get information about their environment? In this paper, we show that the minimum energy required for a biological sensor to detect a change in some environmental signal is proportional to the amount of information processed during that event. In order to know how far a real biological sensor operates from this minimum, we apply our predictions to chemo-sensing in the bacterium <italic>Escherichia Coli</italic> and find that the theoretical minimum corresponds to a sizable portion of the energy spent by the bacterium.</p>
</abstract>
<funding-group><funding-statement>This work was partially supported by a Max Planck society (<ext-link ext-link-type="uri" xlink:href="http://www.mpg.de" xlink:type="simple">www.mpg.de</ext-link>) scholarship to PS and LG, by grant ENFASIS (Spanish government: <ext-link ext-link-type="uri" xlink:href="http://www.idi.mineco.gob.es/" xlink:type="simple">www.idi.mineco.gob.es/</ext-link>) to LG and JMH, and by Army Research Office (<ext-link ext-link-type="uri" xlink:href="http://www.arl.army.mil" xlink:type="simple">http://www.arl.army.mil</ext-link>) MURI grant W911NF-11-1-0268 to JMH. The funders had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts><custom-meta-group><custom-meta id="data-availability" xlink:type="simple"><meta-name>Data Availability</meta-name><meta-value>The authors confirm that all data underlying the findings are fully available without restriction. All relevant data are within the paper and its Supporting Information files.</meta-value></custom-meta></custom-meta-group></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>In order to perform a variety of tasks, living organisms continually respond and adapt to their changing surroundings through diverse electrical, chemical and mechanical signaling pathways, called sensory systems <xref ref-type="bibr" rid="pcbi.1003974-Koshland1">[1]</xref>. In mammals, prominent examples are the neurons involved in the visual, olfactory, and somatic systems <xref ref-type="bibr" rid="pcbi.1003974-Gillespie1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1003974-Abraira1">[5]</xref>. But also unicellular organisms lacking a neuronal system sense their environment: Yeast can sense osmotic pressure <xref ref-type="bibr" rid="pcbi.1003974-Muzzy1">[6]</xref>, and <italic>E. coli</italic> can monitor chemical gradients <xref ref-type="bibr" rid="pcbi.1003974-Shimizu1">[7]</xref>, temperatures <xref ref-type="bibr" rid="pcbi.1003974-Paster1">[8]</xref> and pH <xref ref-type="bibr" rid="pcbi.1003974-Yang1">[9]</xref>. Despite the diversity in biochemical details, sensory adaptation systems (SAS) exhibit a common behavior: long-term storage of the state of the environment and rapid response to its changes <xref ref-type="bibr" rid="pcbi.1003974-CUM1">[10]</xref>. Intuitively, one expects that for these SAS to function, an energy source – such as ATP or SAM – is required; but is there a fundamental minimum energy needed? To tackle this question, we first relate a generic SAS to a binary information processing device, which is tasked to perform fast information acquisition on the environment (response) and to record subsequently the information into its longer term memory (adaptation). Since the foundational works of Maxwell, Szilard and Landauer, the intimate relationship between thermodynamic costs and information processing tasks has been intensely studied <xref ref-type="bibr" rid="pcbi.1003974-Leff1">[11]</xref>–<xref ref-type="bibr" rid="pcbi.1003974-Granger2">[17]</xref>. As a result, the natural mapping between a generic SAS and an information processing device allows us to quantify the minimal energetic costs of sensory adaptation.</p>
<p>The idea of viewing biological processes as information processing tasks is not new <xref ref-type="bibr" rid="pcbi.1003974-Shimizu1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Bennett1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Mehta1">[18]</xref>. However, rationalizing sensory adaptation is complicated by recent studies that have revealed that motifs in the underlying biochemical networks play a fundamental role in the thermodynamic costs. For instance, the steady state of feedback adaptive systems must be dissipative, with more dissipation leading to better adaptation <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>, an observation echoed in the analysis of a minimal model of adaptive particle transport <xref ref-type="bibr" rid="pcbi.1003974-Allahverdyan1">[20]</xref>. Other studies have suggested that some feedforward adaptive systems may require dissipation to sustain their steady state <xref ref-type="bibr" rid="pcbi.1003974-Lan2">[21]</xref>, while some may not <xref ref-type="bibr" rid="pcbi.1003974-Buijsman1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-DePalo1">[23]</xref>. Furthermore, past studies <xref ref-type="bibr" rid="pcbi.1003974-Mehta1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Tostevin1">[24]</xref> have approached the notion of information by considering noisy inputs due to stochastic binding, a realm in which adaptation may not be relevant due to the separation of time-scales <xref ref-type="bibr" rid="pcbi.1003974-Sartori1">[25]</xref>. Here, we develop a different approach that avoids these caveats by considering a thermodynamically consistent notion of information that naturally incorporates the costs of sensing in sensory adaptation. Specifically, we derive a collection of universal bounds that relate the thermodynamic costs of sensing to the information processed. These bounds reveal for the first time that for a generic SAS, measuring an environmental change is energetically costly [(6) below], while to erase the memory of the past is energetically free, but necessarily irreversible [(5) below]. By formalizing and linking the information processing and thermodynamics of sensory systems, our work shows that there is an intrinsic cost of sensing due to the necessity to process information.</p>
<p>To illustrate our generic approach, we study first a minimal four-state feedforward model and then a detailed ten-state feedback model of <italic>E. coli</italic> chemotaxis. Owing to the symmetry of its motif's topology the four-state feedforward model does not require energy to sustain its adapted state. Instead, all the dissipation arises from information processing: acquiring new information consumes energy, while erasing old information produces entropy. By contrast, the <italic>E. coli</italic> model sustains its nonequilibrium steady state (NESS) by constantly dissipating energy, a requirement for adaptation with a feedback topology <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>. In this nonequilibrium setting, we generalize our thermodynamic bounds in order to pinpoint the additional energy for sensing over that required to maintain the steady state. We find with this formalism that in <italic>E. coli</italic> chemotaxis the theoretical minimum demanded by our bounds accounts for a sizable portion of the energy spent by the bacterium on its SAS.</p>
</sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Universal traits of sensory adaptation</title>
<p>To respond and adapt to changes in an environmental signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e001" xlink:type="simple"/></inline-formula>, a SAS requires a fast variable, the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e002" xlink:type="simple"/></inline-formula>; and a slow variable, the memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e003" xlink:type="simple"/></inline-formula>. For example, in <italic>E. coli</italic> the activity is the conformational state of the receptor, the memory the number of methyl groups attached to it, and the signal is the ligand concentration <xref ref-type="bibr" rid="pcbi.1003974-Shimizu1">[7]</xref>. Without loss of generality, we consider in the following all three variables normalized such that they only lie between 0 and 1, and that the signal can only alternate between two values: a low value 0 and a high value 1.</p>
<p>As a result of thermal fluctuations, the time-dependent activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e004" xlink:type="simple"/></inline-formula> and memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e005" xlink:type="simple"/></inline-formula> are stochastic variables. Yet, the defining characteristics of sensory adaptation are captured by their ensemble averages <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e006" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e007" xlink:type="simple"/></inline-formula>, both at the steady state and in response to changes in the signal.</p>
<p>At a constant environmental signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e008" xlink:type="simple"/></inline-formula>, the system relaxes to an adapted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e009" xlink:type="simple"/></inline-formula>-dependent steady state, which may be far from equilibrium <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>. In this state, the memory is correlated with the signal, with an average value close to the signal, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e010" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e011" xlink:type="simple"/></inline-formula> is a small error. The average activity however is <italic>adapted</italic>, taking a value roughly independent of the signal, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e012" xlink:type="simple"/></inline-formula>, with adaption error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e013" xlink:type="simple"/></inline-formula>.</p>
<p>Besides the ability to adapt, SAS are also defined by their multiscale response to abrupt signal changes, which is illustrated in <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1</xref>. For example, given a sharp increase in the signal from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e014" xlink:type="simple"/></inline-formula> to 1 the average activity quickly grows from its adapted value to a peak <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e015" xlink:type="simple"/></inline-formula> characterized by the gain error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e016" xlink:type="simple"/></inline-formula>. This occurs in a time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e017" xlink:type="simple"/></inline-formula>, before the memory responds. After a longer time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e018" xlink:type="simple"/></inline-formula>, the memory starts to track the signal, and the activity gradually recovers to its adapted value (see <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1A</xref>). For a sharp decrease in the signal, the behavior is analogous (see <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1B</xref>).</p>
<fig id="pcbi-1003974-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g001</object-id><label>Figure 1</label><caption>
<title>Generic traits of sensory adaptive systems.</title>
<p>(A/B) Typical time evolution of the average activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e019" xlink:type="simple"/></inline-formula> (dark blue) and average memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e020" xlink:type="simple"/></inline-formula> (red) of a SAS in response to an abrupt increase or decrease in the signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e021" xlink:type="simple"/></inline-formula> (orange). (C) Schematic states of a chemical receptor (black) embedded in a cell (light blue) during the four key phases of adaptation. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e022" xlink:type="simple"/></inline-formula> the system is adapted; at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e023" xlink:type="simple"/></inline-formula> there is a sudden increase in the signal ligand concentration (orange flecks); at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e024" xlink:type="simple"/></inline-formula> the receptor responds increasing its activity (full blue circle); and at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e025" xlink:type="simple"/></inline-formula> it is adapted (the memory is full, red; while the activity is half full blue).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g001" position="float" xlink:type="simple"/></fig>
<p>We identify a SAS as any device that exhibits the described adapted states for low and high signals (0 or 1) and that reproduces the desired behavior to abrupt increases and decreases in the signal (see <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1C</xref> for a cartoon biochemical example). While SAS typically exhibit additional features (such as wide range sensitivity <xref ref-type="bibr" rid="pcbi.1003974-Mello1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Tu1">[27]</xref>), they all exhibit the universal features illustrated in <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1</xref>.</p>
</sec><sec id="s3b">
<title>Minimal SAS: Equilibrium feedforward model</title>
<p>To facilitate the development of our formalism, we first present a minimal stochastic model of a SAS, where the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e026" xlink:type="simple"/></inline-formula> and memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e027" xlink:type="simple"/></inline-formula> are binary variables (0 or 1). This model is minimal, since it has the least number of degrees of freedom (or states) possible and still exhibits the required response and adaptive behavior. Treating the environmental signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e028" xlink:type="simple"/></inline-formula> as an external field that drives the SAS, the system can be viewed as evolving by jumping stochastically between its four states depicted in <xref ref-type="fig" rid="pcbi-1003974-g002">Fig. 2A</xref>. The rates for activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e029" xlink:type="simple"/></inline-formula> transitions from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e030" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e031" xlink:type="simple"/></inline-formula> at fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e032" xlink:type="simple"/></inline-formula> are denoted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e033" xlink:type="simple"/></inline-formula>, and those for memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e034" xlink:type="simple"/></inline-formula> transitions from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e035" xlink:type="simple"/></inline-formula> given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e036" xlink:type="simple"/></inline-formula> are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e037" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003974-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g002</object-id><label>Figure 2</label><caption>
<title>Equilibrium adaptation in a symmetric feedforward SAS.</title>
<p>(A) Reaction network of the four states in activity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e038" xlink:type="simple"/></inline-formula>, memory, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e039" xlink:type="simple"/></inline-formula>, space, with kinetic rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e040" xlink:type="simple"/></inline-formula> indicated for each transitions. (B) Topology of the model: feedforward with mutual inhibition. For a fixed signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e041" xlink:type="simple"/></inline-formula>, a sudden increase in the memory makes the average activity drop, and vice versa for activity changes. This symmetry of the topology, which is at the core of detailed balance, allows an equilibrium construction. (C/D) Representation of steady state probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e042" xlink:type="simple"/></inline-formula> for low/high <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e043" xlink:type="simple"/></inline-formula> signals using the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e044" xlink:type="simple"/></inline-formula> space in (A). Wider state diameter represents higher probability, thus lower energy.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g002" position="float" xlink:type="simple"/></fig>
<p>As an equilibrium model, it is completely characterized by a free energy function, which we have constructed in the Methods by requiring the equilibrium steady state to have the required signal correlations of a SAS,<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e045" position="float" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e046" xlink:type="simple"/></inline-formula> is the energy penalty for the memory to mistrack the signal, ensuring adaptation (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e047" xlink:type="simple"/></inline-formula> the temperature and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e048" xlink:type="simple"/></inline-formula> Boltzmann's constant). In fact, one can show that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e049" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e050" xlink:type="simple"/></inline-formula> is the penalty for the activity to mistrack the signal when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e051" xlink:type="simple"/></inline-formula>; it thus becomes relevant after a signal change, but before the memory adapts to the new signal, ensuring response. In <xref ref-type="fig" rid="pcbi-1003974-g002">Figs. 2C and D</xref> the energy landscape <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e052" xlink:type="simple"/></inline-formula> is represented for low and high signals (smaller radius corresponds to less probability and larger energy). Note that for fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e053" xlink:type="simple"/></inline-formula>, the adaptation error is zero when the energy penalty to misstrack the signal becomes large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e054" xlink:type="simple"/></inline-formula>, the system's configuration is then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e055" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e056" xlink:type="simple"/></inline-formula> takes on the values 0 and 1 with equal probability. Finally, the dynamics are set by fixing the kinetic rates using detailed balance, <italic>e.g.</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e057" xlink:type="simple"/></inline-formula>, and then choosing well-separated bare rates to set the timescale of jumps: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e058" xlink:type="simple"/></inline-formula> for activity transitions and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e059" xlink:type="simple"/></inline-formula> for memory transitions, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e060" xlink:type="simple"/></inline-formula>, thereby enforcing the well-separated time-scales of adaptation.</p>
<p>When there is a change in the signal, this model exhibits response and adaptation as characterized in <xref ref-type="fig" rid="pcbi-1003974-g001">Figs. 1A and B</xref> (verified in <xref ref-type="supplementary-material" rid="pcbi.1003974.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003974.s002">S2 Figures</xref>), and relaxes towards a <italic>dissipationless</italic> equilibrium steady state in which detailed balance is respected. This is in contrast to previous studies on adaptive systems, which demonstrated that maintaining the steady state for a generic feedback system breaks detailed balance <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Allahverdyan1">[20]</xref>. Our model, however, differs by its network topology. As depicted in <xref ref-type="fig" rid="pcbi-1003974-g002">Fig. 2B</xref>, it is a mutually repressive feedforward (all rates depend explicitly on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e061" xlink:type="simple"/></inline-formula>, and the actions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e063" xlink:type="simple"/></inline-formula> on each other are symmetric). Similar topologies also underly recent suggestions for biochemical networks that allow for adaptation with dissipationless steady states <xref ref-type="bibr" rid="pcbi.1003974-Buijsman1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-DePalo1">[23]</xref>.</p>
</sec><sec id="s3c">
<title>Information processing in sensory adaptation</title>
<p>Any sensory system that responds and adapts can naturally be viewed as an information processing device. In the steady state, information about the signal is stored in the memory, since knowledge of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e064" xlink:type="simple"/></inline-formula> allows one to accurately infer the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e065" xlink:type="simple"/></inline-formula>. The activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e066" xlink:type="simple"/></inline-formula>, on the other hand, possesses very little information about the signal, since it is adapted and almost independent of the signal. When confronted by an abrupt signal change, the activity rapidly responds by gathering information about the new signal value. As the activity decays back to its adapted value, information is stored in the memory. However, to make room for this new information, the memory must decorrelate itself with the initial signal, thereby erasing the old information. Thus sensory adaptation involves measurement as well as erasure of information.</p>
<p>To make this intuitive picture of information processing precise, let us focus on a concrete experimental situation where the signal is manipulated by an outside observer. This is the setup common in experiments on <italic>E. coli</italic> chemotaxis where the signal (the ligand concentration) is varied in a prescribed, deterministic way <xref ref-type="bibr" rid="pcbi.1003974-Segall1">[28]</xref>. To be specific, the initial random signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e067" xlink:type="simple"/></inline-formula> is fixed to an arbitrary value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e068" xlink:type="simple"/></inline-formula>, either 0 or 1, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e069" xlink:type="simple"/></inline-formula>, and the system is prepared in the corresponding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e070" xlink:type="simple"/></inline-formula>-dependent steady state, characterized by the probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e071" xlink:type="simple"/></inline-formula>. Then, at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e072" xlink:type="simple"/></inline-formula>, the signal is randomly switched to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e073" xlink:type="simple"/></inline-formula> with final value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e074" xlink:type="simple"/></inline-formula> (which may be the same as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e075" xlink:type="simple"/></inline-formula>) according to the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e076" xlink:type="simple"/></inline-formula>. The signal is held there while the system's time-dependent probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e077" xlink:type="simple"/></inline-formula>, which conditionally depends on both the initial and final signals, irreversibly relaxes to the final steady state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e078" xlink:type="simple"/></inline-formula>. During this relaxation correlations between the system and the final signal value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e079" xlink:type="simple"/></inline-formula> develop while the correlations with the past value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e080" xlink:type="simple"/></inline-formula> are lost. As we will see, the measure of information that captures this evolution of correlations <italic>and</italic> naturally enters the thermodynamics of sensory adaptation is the mutual information between the system and the signal.</p>
<p>The <italic>mutual information</italic> is an information-theoretic quantification of how much a random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e081" xlink:type="simple"/></inline-formula> (such as the system) knows about another variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e082" xlink:type="simple"/></inline-formula> (such as the signal),<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e083" position="float" xlink:type="simple"/><label>(2)</label></disp-formula>measured in nats <xref ref-type="bibr" rid="pcbi.1003974-Cover1">[29]</xref>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e084" xlink:type="simple"/></inline-formula> is the Shannon entropy, which is a measure of uncertainty. Thus, the mutual information measures the reduction in uncertainty of one variable given knowledge of the other. Of note, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e085" xlink:type="simple"/></inline-formula> with equality only when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e087" xlink:type="simple"/></inline-formula> are independent.</p>
<p>There are two key appearances of mutual information in sensory adaptation capturing how information about the present is acquired, while knowledge of the past is lost, which we now describe. At the beginning of our experiment at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e088" xlink:type="simple"/></inline-formula>, the SAS is correlated with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e089" xlink:type="simple"/></inline-formula>, simply because the SAS is in a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e090" xlink:type="simple"/></inline-formula>-dependent steady state. Thus there is an initial information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e091" xlink:type="simple"/></inline-formula> that the SAS has about the initial value of the signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e092" xlink:type="simple"/></inline-formula>. The signal is then switched; yet immediately after, the SAS has no information about the new signal value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e093" xlink:type="simple"/></inline-formula>, so <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e094" xlink:type="simple"/></inline-formula>. Then for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e095" xlink:type="simple"/></inline-formula> the SAS evolves, becoming correlated with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e096" xlink:type="simple"/></inline-formula>, thereby gathering (or measuring) information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e097" xlink:type="simple"/></inline-formula>, which grows with time. Concurrently it decorrelates from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e098" xlink:type="simple"/></inline-formula>, thus erasing information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e099" xlink:type="simple"/></inline-formula> about the old signal, which also grows with time. This conditioning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e100" xlink:type="simple"/></inline-formula> only takes into account direct correlations between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e101" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e102" xlink:type="simple"/></inline-formula>, excluding indirect ones through <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e103" xlink:type="simple"/></inline-formula>.</p>
<p>To illustrate this, we calculate the flow of information in the non-disspative feedforward model for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e104" xlink:type="simple"/></inline-formula>, which is a 1-bit operation (because <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e105" xlink:type="simple"/></inline-formula>). <xref ref-type="fig" rid="pcbi-1003974-g003">Fig. 3A</xref> displays the evolution of the measured information (in black), which we decomposed as<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e106" position="float" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e107" xlink:type="simple"/></inline-formula> (red) is the information stored in the memory and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e108" xlink:type="simple"/></inline-formula> (blue) in the activity. We see the growth of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e109" xlink:type="simple"/></inline-formula> proceeds first by a rapid (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e110" xlink:type="simple"/></inline-formula>) increase as information is stored in the activity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e111" xlink:type="simple"/></inline-formula> grows) while the system responds, followed by a slower growth as adaptation sets in (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e112" xlink:type="simple"/></inline-formula>), and the memory begins to track the signal. At the end, the system is adapted, and there is almost no information in the activity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e113" xlink:type="simple"/></inline-formula>. With the small errors we have, the information acquired reaches nearly the maximum value of 1 bit, which is stored in the memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e114" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1003974-g003">Fig. 3B</xref> shows the erasure of information, visible by the decrease of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e115" xlink:type="simple"/></inline-formula> from an initial value of nearly one bit to zero when the system has decorrelated from the initial signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e116" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003974-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g003</object-id><label>Figure 3</label><caption>
<title>Information measurement and erasure in sensory adaptation.</title>
<p>(A) Information acquired about the new signal as a function of time. The information stored in the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e117" xlink:type="simple"/></inline-formula> (dark blue) grows as the system responds, and then goes down as it adapts, when the information in the memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e118" xlink:type="simple"/></inline-formula> (red) grows. The total information measured <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e119" xlink:type="simple"/></inline-formula> (black) shows the effect of both. (B) Information lost about the old signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e120" xlink:type="simple"/></inline-formula> (black), and its decomposition in memory (red) and activity (blue) information. Model parameters are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e121" xlink:type="simple"/></inline-formula> for x = a, m, g; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e123" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s3d">
<title>Thermodynamic costs to sensory adaptation</title>
<p>We have seen that through an irreversible relaxation, an SAS first acquires and then erases information in the registry of the activity, followed by the memory. The irreversibility of these information operations is quantified by the entropy production, which we now analyze in order to pinpoint the thermodynamic costs of sensing. Specifically, we demonstrate in Methods that for a system performing sensory adaptation in response to an abrupt change in the environment, the total entropy production can be partitioned in two positive parts: one caused by measurement (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e124" xlink:type="simple"/></inline-formula>) and the other by erasure (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e125" xlink:type="simple"/></inline-formula>). The second law thus becomes<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e126" position="float" xlink:type="simple"/><label>(4)</label></disp-formula>with the reference set to an initial state at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e127" xlink:type="simple"/></inline-formula>. The erasure piece<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e128" position="float" xlink:type="simple"/><label>(5)</label></disp-formula>is purely entropic in the sense that it contains no energetic terms. It solely results from the loss of information (or correlation) about the initial signal. By contrast, the energetics are contained in the measurement portion,<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e129" position="float" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e130" xlink:type="simple"/></inline-formula> is the change in Shannon entropy of the system and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e131" xlink:type="simple"/></inline-formula> is the average heat flow into the system from the thermal reservoir.</p>
<p>A useful alternative formulation can be obtained once we identify the internal energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e132" xlink:type="simple"/></inline-formula>. For example, in the equilibrium feedforward model, a sensible choice is the average energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e133" xlink:type="simple"/></inline-formula> (1). (Recall, that there is no unique division into internal energy and work, though any choice once made is thermodynamically consistent <xref ref-type="bibr" rid="pcbi.1003974-Jarzynski1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Horowitz1">[31]</xref>.) By substituting in the first law of thermodynamics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e134" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e135" xlink:type="simple"/></inline-formula> the work, we arrive at<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e136" position="float" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>This equation shows how the measured information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e137" xlink:type="simple"/></inline-formula> bounds the minimum energy required for sensing, which must be supplied as either work <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e138" xlink:type="simple"/></inline-formula> or free energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e139" xlink:type="simple"/></inline-formula>. Thus, <italic>to measure is energetically costly; whereas, erasure is energetically free, but necessarily irreversible.</italic> In particular, for sensing to occur, the old information must be erased (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e140" xlink:type="simple"/></inline-formula>), implying that the process is inherently irreversible,<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e141" position="float" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>Together (5) and (7) quantify the thermodynamic cost of sensing an abrupt change in the environment by an arbitrary sensory system.</p>
<p>We have demonstrated from fundamental principles that sensing generically requires energy. However, (7) does not dictate the source of that energy: It can be supplied by the environment itself or by the SAS. The distinction originates because the definition of internal energy is not unique, a point to which we come back in our analysis of <italic>E. coli</italic> chemotaxis.</p>
<p>Using again our equilibrium feedforward model as an example, we apply our formalism to investigate the costs of sensory adaptation. Since this model sustains its steady state at no energy cost, the ultimate limit lies in the sensing process itself. We see this immediately in <xref ref-type="fig" rid="pcbi-1003974-g004">Fig. 4</xref> where we verify the inequalities in (4) and (7). Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e142" xlink:type="simple"/></inline-formula> in (1) is explicitly a function of the environmental signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e143" xlink:type="simple"/></inline-formula>, the sudden change in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e144" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e145" xlink:type="simple"/></inline-formula> does work on the system, which is captured in <xref ref-type="fig" rid="pcbi-1003974-g004">Fig. 4A</xref> by the initial jump in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e146" xlink:type="simple"/></inline-formula>. This work is instantaneously converted into free energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e147" xlink:type="simple"/></inline-formula> and is then consumed as the system responds and adapts in order to measure. Thus, in this example the work to sense is supplied by the signal (the environment) itself and not the SAS, which is consistent with other equilibrium models of SAS <xref ref-type="bibr" rid="pcbi.1003974-DePalo1">[23]</xref>. Furthermore, <xref ref-type="fig" rid="pcbi-1003974-g004">Fig. 4B</xref> confirms that the erasure of information leads to an irreversible process with net entropy production. The bounds of (4) and (7) are not tightly met in our model, since we are sensing a sudden change in the signal that necessitates a dissipative response. Nonetheless, the total entropy production and energetic cost are on the order of the information erased and acquired. This indicates that these information theoretic bounds can be a limiting factor for the operation of adaptive systems. We now show that this is the case for <italic>E. coli</italic> chemotaxis, a fundamentally different system as it operates far from equilibrium.</p>
<fig id="pcbi-1003974-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g004</object-id><label>Figure 4</label><caption>
<title>Thermodynamics of adaptation in an equilibrium SAS.</title>
<p>(A) Energetic cost as a function of time given by the work <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e148" xlink:type="simple"/></inline-formula> provided by the environment (red), free energy change of the system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e149" xlink:type="simple"/></inline-formula> (orange), and dissipated work <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e150" xlink:type="simple"/></inline-formula> (black), compared to the measured information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e151" xlink:type="simple"/></inline-formula> (grey dashed), which gives the lower bound at every time. (B) Total entropic cost <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e152" xlink:type="simple"/></inline-formula> (black) and decomposition in measurement <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e153" xlink:type="simple"/></inline-formula> (gray) and erasure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e154" xlink:type="simple"/></inline-formula> (yellow). Parameters as in <xref ref-type="fig" rid="pcbi-1003974-g003">Fig. 3</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s3e">
<title>Extension to NESS and application to <italic>E. coli</italic> chemotaxis</title>
<p>We have quantified the thermodynamic costs in any sensory adaptation system; however, for systems that break detailed balance and maintain their steady state far from equilibrium, (5) – (8) are uninformative, because of the constant entropy production. A case in point is <italic>E. coli</italic>'s SAS, which enables it to perform chemotaxis by constantly consuming energy and producing entropy through the continuous hydrolysis of SAM.</p>
<p>Nevertheless, there is a refinement of the second law for genuine NESS in terms of the nonadiabatic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e155" xlink:type="simple"/></inline-formula> and adiabatic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e156" xlink:type="simple"/></inline-formula> entropy productions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e157" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003974-Esposito1">[32]</xref>. Crudely speaking, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e158" xlink:type="simple"/></inline-formula> is the entropy required to sustain a nonequilibrium steady state and is never null for a genuine NESS; whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e159" xlink:type="simple"/></inline-formula> is the entropy produced by the transient time evolution. When the system satisfies detailed balance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e160" xlink:type="simple"/></inline-formula> always, be it at its equilibrium steady state or not; when its surroundings change, the entropy production is entirely captured by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e161" xlink:type="simple"/></inline-formula>. We can refine our predictions for a NESS by recognizing that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e162" xlink:type="simple"/></inline-formula> captures the irreversibility due to a transient relaxation, just as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e163" xlink:type="simple"/></inline-formula> does for systems satisfying detailed balance. Analogously to Eqs. (6) and (8), we derive (see <xref ref-type="sec" rid="s5">Methods</xref>):<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e164" position="float" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e165" position="float" xlink:type="simple"/><label>(10)</label></disp-formula></p>
<p>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e166" xlink:type="simple"/></inline-formula> is the excess heat flow into the system, roughly the extra heat flow during a driven, nonautonomous process over that required to maintain the steady state <xref ref-type="bibr" rid="pcbi.1003974-Ge1">[33]</xref>. As a result, it remains finite during an irreversible relaxation to a NESS, even though the NESS may break detailed balance.</p>
<p><italic>E. coli</italic> is a bacterium that can detect changes in the concentration of nearby ligands in order to perform chemotaxis: the act of swimming up a ligand attractor gradient. It is arguably the best studied example of a SAS. At a constant ligand concentration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e167" xlink:type="simple"/></inline-formula>, chemoreceptors in <italic>E. coli</italic> – such as the one in <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1C</xref> – have a fixed average activity, which through a phosphorylation cascade translates into a fixed switching rate of the bacterial flagellar motor. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e168" xlink:type="simple"/></inline-formula> changes, the activity of the receptor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e169" xlink:type="simple"/></inline-formula> (which is a binary variable labeling two different receptor conformations) increases on a time-scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e170" xlink:type="simple"/></inline-formula>. On a longer time-scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e171" xlink:type="simple"/></inline-formula>, the methylesterase CheR and methyltransferase CheB alter the methylation level of the receptor in order to recover the adapted activity value. In this way, the methylation level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e172" xlink:type="simple"/></inline-formula> (which ranges from none to four methyl groups for a single receptor) is a representation of the environment, acting as the long-term memory (see diagram in <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5A</xref>). One important difference with the previous equilibrium model is that the chemotaxis pathway operates via a feedback. The memory is not regulated by the receptor's signal, but rather by the receptor's activity (see motif in <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5B</xref>). The implication is that energy must constantly be dissipated to sustain the steady state <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>, thus (9) and (10) are the appropriate tools for a thermodynamic analysis.</p>
<fig id="pcbi-1003974-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g005</object-id><label>Figure 5</label><caption>
<title>Energetic costs of adaptation in an <italic>E. coli</italic> chemotaxis SAS.</title>
<p>(A) Network representation of the nonequilibrium receptor model with five methylation and two activity states. Green arrows represent the addition/removal of methyl groups driven by the chemical fuel SAM. (B) Corresponding negative feedback topology, displaying the dissipative energy cycle (green arrow) sustained by adiabatic entropy production, due to the consumption of chemical fuel. (C) Energetics of nonequilibrium measurement in the chemotaxis pathway for a ligand concentration change of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e173" xlink:type="simple"/></inline-formula> (other parameters in Materials and Methods). The instantaneous change in ligand concentration performs chemical work on the cell, which increases its free energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e174" xlink:type="simple"/></inline-formula> as the cell responds. To adapt, the bacterium has to provide excess work <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e175" xlink:type="simple"/></inline-formula> from its own chemical reservoir, the fuel SAM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g005" position="float" xlink:type="simple"/></fig>
<p>There is a consensus kinetic model of <italic>E. coli</italic> chemoreceptors <xref ref-type="bibr" rid="pcbi.1003974-Shimizu1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Tu1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Keymer1">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003974-Tu2">[36]</xref> whose biochemical network is in <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5A</xref>. The free energy landscape of the receptor coupled to its environment is<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e176" position="float" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e177" position="float" xlink:type="simple"/><label>(12)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e178" xlink:type="simple"/></inline-formula> the receptor's characteristic energy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e179" xlink:type="simple"/></inline-formula> the reference methylation level, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e180" xlink:type="simple"/></inline-formula> the active/inactive dissociation constants (values in Methods). In (11) the first term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e181" xlink:type="simple"/></inline-formula> corresponds to the energy of the receptor, and the second <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e182" xlink:type="simple"/></inline-formula> comes from the interaction with the environment (<italic>de facto</italic> a ligand reservoir). The dynamics of this receptor consist of thermal transitions between the states with different activity, while transitions between the different methylation levels are powered by a chemical potential gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e183" xlink:type="simple"/></inline-formula> due to hydrolisis of the methyl donor SAM (see <xref ref-type="sec" rid="s5">Methods</xref>). Continuous hydrolysis of SAM at the steady state sustains the feedback at the expense of energy, allowing accurate adaptation in the ligand concentration range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e184" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5B</xref>.</p>
<p>To begin our study, we develop an equation analogous to (7), which requires identifying the internal energy of our system. As stated above, we consider the binding and unbinding of ligands as external stimuli, and thus define the internal energy as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e185" xlink:type="simple"/></inline-formula>. Using the excess heat <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e186" xlink:type="simple"/></inline-formula>, we consistently define the excess work through <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e187" xlink:type="simple"/></inline-formula>, analogous to the first law. Upon substitution into (9) gives<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e188" position="float" xlink:type="simple"/><label>(13)</label></disp-formula>showing just as in (7) that measuring requires excess work and free energy. Because here the internal energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e189" xlink:type="simple"/></inline-formula> is <italic>not</italic> a function of the ligand concentration, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e190" xlink:type="simple"/></inline-formula> is not due to signal variation: It represents the energy expended by the cell to respond and adapt to the external chemical force.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5C</xref>, we compare <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e191" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e192" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e193" xlink:type="simple"/></inline-formula> during a ligand change of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e194" xlink:type="simple"/></inline-formula>. The sudden change in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e195" xlink:type="simple"/></inline-formula> produces a smooth, fast (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e196" xlink:type="simple"/></inline-formula>) increase in the free energy as the activity transiently equilibrates with the new environment. The excess work driving this response comes mainly from the interaction with environment. As adaptation sets in (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e197" xlink:type="simple"/></inline-formula>), the receptor utilizes that stored free energy, but in addition burns energy by the consumption of SAM. Thus, in order to adapt the cell consumes the free energy stored from the environment, as well as additional excess work coming now mostly from the hydrolysis of SAM molecules. The inequality in (7) with the measured information is satisfied at all times.</p>
<p>The energetic cost of responding and adapting to the ligand change is roughly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e198" xlink:type="simple"/></inline-formula>, of which much has already been used by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e199" xlink:type="simple"/></inline-formula>. In comparison, the cost to sustain the chemotaxis pathway during this time is roughly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e200" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s5">Methods</xref>). This means that the cost to sensing a step change is about 10% of the cost to sustain the sensing apparatus at steady-state. During this process the cell measures (and erases) roughly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e201" xlink:type="simple"/></inline-formula> bits, less than the maximum of 1 bit despite its very high adaptation accuracy. This limitation comes from the finite number of discrete methylation levels, so that the probability distributions in <italic>m</italic>-space for large and low ligand concentrations have large overlaps (<xref ref-type="supplementary-material" rid="pcbi.1003974.s003">S3 Figure</xref>). In other words, it is difficult to discriminate these distributions, even though the averages are very distinct, which results in lower correlation between the methylation level and signal. The minimal energetic cost associated to measuring these <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e202" xlink:type="simple"/></inline-formula> bits (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e203" xlink:type="simple"/></inline-formula> nats) is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e204" xlink:type="simple"/></inline-formula>. <italic>E. coli</italic> dissipates roughly <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e205" xlink:type="simple"/></inline-formula> during this process, thus the energetic cost of sensory adaptation is slightly larger than twice its thermodynamic lower bound (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e206" xlink:type="simple"/></inline-formula>).</p>
<p>We further explored the cost of sensing in <italic>E. coli</italic> by examining the net entropy production for ligand changes of different intensity. In <xref ref-type="fig" rid="pcbi-1003974-g006">Fig. 6A</xref>, we plot the amount of information erased/measured for different step changes of the signal up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e207" xlink:type="simple"/></inline-formula> taking as lower base <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e208" xlink:type="simple"/></inline-formula>. The green shading highlights the region where adaptation is accurate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e209" xlink:type="simple"/></inline-formula>). The information erased is always below 1 bit and saturates for high ligand concentrations, for which the system is not sensitive. The total entropic cost (that is, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e210" xlink:type="simple"/></inline-formula>) and its relation with the information erased appears in <xref ref-type="fig" rid="pcbi-1003974-g006">Fig. 6B</xref>. The dependence is monotonic, and thus reveals a trade-off between information processing and dissipation in sensory adaptation. Notably, for small acquisition of information (small ligand steps) it grows linearly with the information, an effect observed in ideal measurement systems <xref ref-type="bibr" rid="pcbi.1003974-Granger2">[17]</xref>.</p>
<fig id="pcbi-1003974-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003974.g006</object-id><label>Figure 6</label><caption>
<title>Information-dissipation trade-off in <italic>E. coli</italic> chemotaxis.</title>
<p>(A) Relationship between information erased/acquired and size of the signal increase. Shaded in green is the region of accurate adaptation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e211" xlink:type="simple"/></inline-formula>). (B) Entropy production as a function of information erased/acquired as step size is varied. The more information is processed by the cell the higher the entropic cost. Notice the linear scaling between dissipation and information for small information (small ligand changes). Dashed lines refer to values in <xref ref-type="fig" rid="pcbi-1003974-g005">Fig. 5C</xref>. Parameters as in Methods.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003974.g006" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s4">
<title>Discussion</title>
<p>We have derived generic information-theoretic bounds to sensory adaptation. We have focused on response-adaptive sensory systems subject to an abrupt environmental switch. This was merely a first step, but the procedure we have outlined here only relies on the validity of the second law of thermodynamics, and therefore can be extend to any small system affected by a random external perturbation to which we can apply stochastic thermodynamics, which is reviewed in <xref ref-type="bibr" rid="pcbi.1003974-Seifert1">[37]</xref>.</p>
<p>Our predictions are distinct from (although reminiscent of) Landauer's principle <xref ref-type="bibr" rid="pcbi.1003974-Leff1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Bennett1">[12]</xref>, which bounds the minimum energy required to reset an isolated memory. By contrast, the information erased in our system is its correlations with the signal. There is another important distinction from the setup of Landauer, and more broadly the traditional setup in the thermodynamics of computation <xref ref-type="bibr" rid="pcbi.1003974-Leff1">[11]</xref> as well as the more recent advancements on the thermodynamics of information processing in the context of measurement and feedback <xref ref-type="bibr" rid="pcbi.1003974-Sagawa1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Sagawa2">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1003974-Ito1">[45]</xref>. There the memory is reset by changing or manipulating it by varying its energy landscape. In our situation, the erasure comes about because the signal is switched. The loss of correlations is stimulated by a change in the measured system – that is the environmental signal; erasure does not occur because the memory itself is altered. Also relevant is <xref ref-type="bibr" rid="pcbi.1003974-Still1">[46]</xref>, which addresses the minimum dissipated work for a system to make predictions about the future fluctuations of the environmental signal, in contrast to the measured information about the current signal, which we have considered.</p>
<p>Our results predict that energy is required to sense changes in the environment, but do not dictate that source of energy. Our equilibrium feedforward model is able to sense and adapt by consuming energy provided by the environment. <italic>E. coli</italic>'s feedback, however, uses mostly external energy to respond, but must consume energy of its own to adapt. The generic bounds here established apply to these two distinct basic topologies, irrespective of their fundamentally different energetics. For <italic>E. coli</italic>, to quantify to what extent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e212" xlink:type="simple"/></inline-formula> is affected by SAM consumption and ligand binding, a more detailed chemical model is required in conduction with a partitioning of the excess work into distinct terms. An interesting open question in this regard, is why nature would choose the dissipative steady state of <italic>E. coli</italic>, when theoretically the cost of sensing could be paid by the environment.</p>
<p>For a ligand change of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e213" xlink:type="simple"/></inline-formula>, in the region of high adaptation, the information measured/erased is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e214" xlink:type="simple"/></inline-formula> bits. We observed that the corresponding average change in the methylation level for a chemoreceptor is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e215" xlink:type="simple"/></inline-formula>, suggesting that a methylation level can store <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e216" xlink:type="simple"/></inline-formula> bits for such 1-bit step response operations. Despite the small adaptation error, information storage is limited by fluctuations arising from the finite number of discrete methylation levels. Receptors' cooperativity, which is known to reduce fluctuations of the collective methylation level, may prevent this allowing them to store more information. On the energetic side, we have shown that the cost of sensing these ligand changes per receptor is around 10% of the cost of sustaining the corresponding adaptive machinery. We also showed that the energetic cost of binary operations is roughly twice beyond its minimum for large ligand changes, in stark contrast with everyday computers for which the difference is orders of magnitude. Taken together these numbers suggest that 5% of the energy a cell uses in sensing is determined by information-thermodynamic bounds, and is thus unavoidable.</p>
<p>Future work should include addressing sensory adaptation in more complex scenarios. One which has recently aroused attention is fluctuating environments, which so far has been addressed using trajectory information <xref ref-type="bibr" rid="pcbi.1003974-Barato1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Ito1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003974-Diana1">[47]</xref>. However, under physiological conditions this is unlikely to play a significant role given the large separation of time-scales between binding, response, and adaptation <xref ref-type="bibr" rid="pcbi.1003974-Sartori1">[25]</xref>. Another scenario is a many bits step operation, in which instead of high and low signals a large discrete set of ligand concentrations is considered. Frequency response and gradient sensing are also appealing <xref ref-type="bibr" rid="pcbi.1003974-Tu1">[27]</xref>, since in them the system is in a dynamic steady state in which the memory is continuously erased and rewritten. Analysis of such scenarios is far from obvious, but the tools developed in this work constitute the first step in developing their theoretical framework.</p>
</sec><sec id="s5" sec-type="methods">
<title>Methods</title>
<sec id="s5a">
<title>Kinetics of equilibrium feedforward model</title>
<p>We determine a collection of rates that exhibit response and adaptation as in <xref ref-type="fig" rid="pcbi-1003974-g001">Fig. 1</xref> by first decomposing the steady state distribution as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e217" xlink:type="simple"/></inline-formula>. As a requirement to show adaptation, the memory must correlate with the signal, which we impose by fixing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e218" xlink:type="simple"/></inline-formula>. Next, in the steady state the activity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e219" xlink:type="simple"/></inline-formula>, or since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e220" xlink:type="simple"/></inline-formula> is binary the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e221" xlink:type="simple"/></inline-formula> is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e222" xlink:type="simple"/></inline-formula>. Recognizing that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e223" xlink:type="simple"/></inline-formula> is small, the average <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e224" xlink:type="simple"/></inline-formula> is dominated by adapted configurations with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e225" xlink:type="simple"/></inline-formula>. Thus, adaption will occur by demanding that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e226" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e227" xlink:type="simple"/></inline-formula>, with a model parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e228" xlink:type="simple"/></inline-formula>. Finally, to fix the activity distribution for non-adapted configurations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e229" xlink:type="simple"/></inline-formula>, we exploit the time-scale separation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e230" xlink:type="simple"/></inline-formula>. In this limit, after an abrupt change in the signal, the activity rapidly relaxes. To guarantee the proper response, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e231" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e232" xlink:type="simple"/></inline-formula>. Using the symmetry condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e233" xlink:type="simple"/></inline-formula> we complete knowledge of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e234" xlink:type="simple"/></inline-formula>. The energy levels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e235" xlink:type="simple"/></inline-formula> are obtained using the equilibrium condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e236" xlink:type="simple"/></inline-formula>, where we choose as reference <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e237" xlink:type="simple"/></inline-formula>. Equation (1) is an approximation of this energy to lowest order in the small errors. Finally, the kinetic rates are obtained using either the approximate or exact energy function, imposing detailed balance, and keeping two bare rates, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e238" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e239" xlink:type="simple"/></inline-formula>, for activity and memory transitions: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e240" xlink:type="simple"/></inline-formula> for activity transitions and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e241" xlink:type="simple"/></inline-formula> for memory transitions.</p>
</sec><sec id="s5b">
<title>Information bounds on the thermodynamics of sensory adaptation</title>
<p>The bounds in (5) and (6) follow from a rearrangement of the second law of thermodynamics <xref ref-type="bibr" rid="pcbi.1003974-Esposito2">[48]</xref>. Consider a system with states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e242" xlink:type="simple"/></inline-formula> [<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e243" xlink:type="simple"/></inline-formula> for SAS] with signal-dependent (free) energy function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e244" xlink:type="simple"/></inline-formula> in contact with a thermal reservoir at temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e245" xlink:type="simple"/></inline-formula>. The system is subjected to a random abrupt change in the signal. Specifically, the initial signal is a random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e246" xlink:type="simple"/></inline-formula> with values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e247" xlink:type="simple"/></inline-formula> (which are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e248" xlink:type="simple"/></inline-formula> in the main text), which we randomly change at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e249" xlink:type="simple"/></inline-formula> to a new random signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e250" xlink:type="simple"/></inline-formula> with values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e251" xlink:type="simple"/></inline-formula>. For times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e252" xlink:type="simple"/></inline-formula>, we model the evolution of the system's stochastic time-dependent state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e253" xlink:type="simple"/></inline-formula> as a continuous-time Markov chain.</p>
<p>We begin our analysis by imagining for the moment that the signal trajectory is fixed to a particular sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e254" xlink:type="simple"/></inline-formula>. Then our thermodynamic process begins prior to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e255" xlink:type="simple"/></inline-formula> by initializing the system in its <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e256" xlink:type="simple"/></inline-formula>-dependent steady state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e257" xlink:type="simple"/></inline-formula>. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e258" xlink:type="simple"/></inline-formula>, the signal changes to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e259" xlink:type="simple"/></inline-formula> and remains fixed while the system's probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e260" xlink:type="simple"/></inline-formula>, which conditionally depends on the <italic>entire</italic> signal trajectory, evolves according to the master equation <xref ref-type="bibr" rid="pcbi.1003974-VanKampen1">[49]</xref><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e261" position="float" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e262" xlink:type="simple"/></inline-formula> is the signal-dependent transition rate for an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e263" xlink:type="simple"/></inline-formula> transition. The transition rates are assumed to satisfy a local detailed balance condition, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e264" xlink:type="simple"/></inline-formula>, which allows us to identify the energy exchanged as heat with the thermal reservoir in each jump. Eventually, the system relaxes to the steady state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e265" xlink:type="simple"/></inline-formula> corresponding to the final signal value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e266" xlink:type="simple"/></inline-formula>.</p>
<p>Since the signal trajectory is fixed, this process is equivalent to a deterministic drive by an external field, and therefore the total entropy production rate will satisfy the second law <xref ref-type="bibr" rid="pcbi.1003974-Esposito2">[48]</xref><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e267" position="float" xlink:type="simple"/><label>(15)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e268" xlink:type="simple"/></inline-formula> is the rate of change of the Shannon entropy of the system conditioned on the entire signal trajectory; and<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e269" position="float" xlink:type="simple"/><label>(16)</label></disp-formula>is the heat current into the system from the thermal reservoir given the signal trajectory. Since (15) holds for any signal trajectory, it remains true after averaging over all signal trajectories sampled from the probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e270" xlink:type="simple"/></inline-formula>:<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e271" position="float" xlink:type="simple"/><label>(17)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e272" xlink:type="simple"/></inline-formula>, and nonconditioned thermodynamic quantities, such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e273" xlink:type="simple"/></inline-formula>, denote signal averages. We next proceed by two judicious substitutions of the definition of the mutual information (2) that tweeze out the contributions from the measured and erased information. First, we replace the Shannon entropy rate as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e274" xlink:type="simple"/></inline-formula>, and then immediately repeat <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e275" xlink:type="simple"/></inline-formula>. The result is a splitting of the total entropy production rate as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e276" xlink:type="simple"/></inline-formula>, with one part due to erasure<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e277" position="float" xlink:type="simple"/><label>(18)</label></disp-formula>and one due to measurement</p>
<p><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e278" position="float" xlink:type="simple"/><label>(19)</label></disp-formula>The bounds in (5) and (6) follow by integrating (18) and (19) from time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e279" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e280" xlink:type="simple"/></inline-formula>.</p>
<p>To prove the positivity of (18) and (19), we use the definition of entropy and heat to recast them in terms of a relative entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e281" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003974-Cover1">[29]</xref> as<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e282" position="float" xlink:type="simple"/><label>(20)</label></disp-formula><disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e283" position="float" xlink:type="simple"/><label>(21)</label></disp-formula></p>
<p>Positivity then follows, since the relative entropy decreases whenever the probability density evolves according to a master equation, as in (14) <xref ref-type="bibr" rid="pcbi.1003974-Sagawa5">[50]</xref>.</p>
<p>To arrive at (9) and (10) for genuine NESS, we repeat the analysis above applied to the average nonadiabatic entropy production rate (cf. (17))<disp-formula><graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e284" position="float" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e285" xlink:type="simple"/></inline-formula> is the excess heat flow into the system <xref ref-type="bibr" rid="pcbi.1003974-Ge1">[33]</xref>, taking special note that now <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e286" xlink:type="simple"/></inline-formula> is the nonequilibrium stationary state and cannot be related to the energy, as in the equilibrium case above (16).</p>
</sec><sec id="s5c">
<title>Description of the chemotaxis model</title>
<p>The parameters for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e287" xlink:type="simple"/></inline-formula> in (11) are taken from <xref ref-type="bibr" rid="pcbi.1003974-Shimizu1">[7]</xref> for a Tar receptor: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e288" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e289" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e290" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e291" xlink:type="simple"/></inline-formula>. The kinetic rates are obtained using local detailed balance and restricting to two characteristic time-scales. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e292" xlink:type="simple"/></inline-formula>-transitions, the rates are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e293" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e294" xlink:type="simple"/></inline-formula> the typical activation time. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e295" xlink:type="simple"/></inline-formula>-transitions, the rates for active states are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e296" xlink:type="simple"/></inline-formula>, and for inactive states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e297" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e298" xlink:type="simple"/></inline-formula> is the chemical potential force for the hydrolyzation of a SAM fuel molecule, which occurs when a methyl group is added or removed by CheR and CheB respectively <xref ref-type="bibr" rid="pcbi.1003974-Lan1">[19]</xref>, and at the steady state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003974.e299" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s6">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003974.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003974.s001" position="float" xlink:type="simple"><label>S1 Figure</label><caption>
<p><bold>Adaptation in equilibrium feedforward SAS to a step increase.</bold> Time evolution of average activity (left) and memory (right) during an increase from 0 to 1 of the environmental signal at time <italic>t = 0</italic> for the equilibrium feed-forward model.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003974.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003974.s002" position="float" xlink:type="simple"><label>S2 Figure</label><caption>
<p><bold>Adaptation in equilibrium feedforward SAS to a step decrease.</bold> Time evolution of average activity (left) and memory (right) during a decrease from 0 to 1 of the environmental signal at time <italic>t = 0</italic> for the equilibrium feed-forward model.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003974.s003" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003974.s003" position="float" xlink:type="simple"><label>S3 Figure</label><caption>
<p><bold>Probability distributions of the methylation level for low and high signals.</bold> Probability distribution of methylation levels for low (orange) and high (blue) ligand concentration levels in the chemotaxis pathway. To the left, ligand concentrations of <italic>[L] = 94µM</italic> and <italic>[L] = 720µM</italic> were used, which are in the adaptive region <italic>K<sub>I</sub>&lt;&lt;L&lt;&lt;K<sub>A</sub></italic>. To the right ligand concentrations of <italic>[L] = 720µM</italic> and <italic>[L] = 5760µM</italic>, outside the adaptive region. Notice the large overlap of the distributions. This effect reduces the memory capacity of <italic>E. coli</italic>.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We are grateful to Y. Tu, D. Zwicker, R. Ma, R.G. Endres, G. Aquino, G. de Palo and S. Pigolotti for comments on this manuscript, and P. Mehta for helpful discussions.</p>
<sec>
<title>Contributions</title>
<p>Conceived and designed the experiments: PS LG CFL JMH. Performed the experiments: PS LG CFL JMH. Analyzed the data: PS LG CFL JMH. Contributed reagents/materials/analysis tools: PS LG CFL JMH. Wrote the paper: PS LG CFL JMH.</p>
</sec></ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003974-Koshland1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koshland</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Goldbeter</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Stock</surname><given-names>JB</given-names></name> (<year>1982</year>) <article-title>Amplification and adaptation in regulatory and sensory systems</article-title>. <source>Science</source> <volume>217</volume>: <fpage>220</fpage>–<lpage>225</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Gillespie1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gillespie</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Corey</surname><given-names>D</given-names></name> (<year>1997</year>) <article-title>Myosin and adaption by hair cells</article-title>. <source>Neuron</source> <volume>19</volume>: <fpage>955</fpage>–<lpage>958</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Laughlin1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name> (<year>1989</year>) <article-title>The role of sensory adaptation in the retina</article-title>. <source>Journal of Experimental Biology</source> <volume>146</volume>: <fpage>39</fpage>–<lpage>62</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Martelli1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martelli</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Carlson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Emonet</surname><given-names>T</given-names></name> (<year>2013</year>) <article-title>Intensity invariant dynamics and odor-specific latencies in olfactory receptor neuron response</article-title>. <source>The Journal of Neuroscience</source> <volume>33</volume>: <fpage>6285</fpage>–<lpage>6297</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Abraira1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraira</surname><given-names>VE</given-names></name>, <name name-style="western"><surname>Ginty</surname><given-names>DD</given-names></name> (<year>2013</year>) <article-title>The sensory neurons of touch</article-title>. <source>Neuron</source> <volume>79</volume>: <fpage>618</fpage>–<lpage>639</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Muzzy1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muzzy</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gómez-Uribe</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Mettetal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>van Oudenaarden</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>A systems-level analysis of perfect adaption in yeast osmoregulation</article-title>. <source>Cell</source> <volume>138</volume>: <fpage>160</fpage>–<lpage>171</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Shimizu1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shimizu</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Berg</surname><given-names>HC</given-names></name> (<year>2010</year>) <article-title>A modular gradient-sensing network for chemotaxis in escherichia coli revealed by responses to time-varying stimuli</article-title>. <source>Molecular systems biology</source> <volume>6</volume>: <fpage>382</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Paster1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paster</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Ryu</surname><given-names>WS</given-names></name> (<year>2008</year>) <article-title>The thermal impulse response of escherichia coli</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>: <fpage>5373</fpage>–<lpage>5377</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Yang1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Sourjik</surname><given-names>V</given-names></name> (<year>2012</year>) <article-title>Opposite responses by different chemoreceptors set a tunable preference point in escherichia coli ph taxis</article-title>. <source>Molecular microbiology</source> <volume>86</volume>: <fpage>1482</fpage>–<lpage>1489</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-CUM1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">CUM S (2008) Biology of Sensory Systems. Wile-Blackwell, Chichester, 2nd edition.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Leff1"><label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">Leff HS, Rex AF, editors (1990) Maxwell's Demon: Entropy, Information, Computing. Princeton University Press, New Jersey.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Bennett1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bennett</surname><given-names>C</given-names></name> (<year>1982</year>) <article-title>The thermodynamics of computation—a review</article-title>. <source>Int J Theor Phys</source> <volume>21</volume>: <fpage>905</fpage>–<lpage>940</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Piechocinska1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Piechocinska</surname><given-names>B</given-names></name> (<year>2000</year>) <article-title>Information erasure</article-title>. <source>Phys Rev A</source> <volume>61</volume>: <fpage>062314</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Dillenschneider1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dillenschneider</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lutz</surname><given-names>E</given-names></name> (<year>2009</year>) <article-title>Memory erasure in small systems</article-title>. <source>Phys Rev Lett</source> <volume>102</volume>: <fpage>210601</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sagawa1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sagawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ueda</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Minimal energy cost for thermodynamic information processing: Measurement and information erasure</article-title>. <source>Phys Rev Lett</source> <volume>102</volume>: <fpage>250602</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Granger1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Granger</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kantz</surname><given-names>H</given-names></name> (<year>2011</year>) <article-title>Thermodynamics of measurements</article-title>. <source>Phys Rev E</source> <volume>84</volume>: <fpage>061110</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Granger2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Granger</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kantz</surname><given-names>H</given-names></name> (<year>2013</year>) <article-title>Differential landauer's principle</article-title>. <source>Europhys Lett</source> <volume>101</volume>: <fpage>50004</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Mehta1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mehta</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Schwab</surname><given-names>DJ</given-names></name> (<year>2012</year>) <article-title>Energetic costs of cellular computation</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume>: <fpage>17978</fpage>–<lpage>17982</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Lan1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lan</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sartori</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Neumann</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sourjik</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>The energy-speed-accuracy trade-off in sensory adaptation</article-title>. <source>Nature physics</source> <volume>8</volume>: <fpage>422</fpage>–<lpage>428</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Allahverdyan1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Allahverdyan</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>QA</given-names></name> (<year>2013</year>) <article-title>Adaptive machine and it thermodynamic costs</article-title>. <source>Phys Rev E</source> <volume>87</volume>: <fpage>032139</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Lan2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lan</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name> (<year>2013</year>) <article-title>The cost of sensitive response and accurate adaptation in networks with an incoherent type-1 feed-forward loop</article-title>. <source>Journal of The Royal Society Interface</source> <volume>10</volume>: <fpage>20130489</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Buijsman1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buijsman</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sheinman</surname><given-names>M</given-names></name> (<year>2014</year>) <article-title>Efficient fold-change detection based on protein-protein interactions</article-title>. <source>Phys Rev E</source> <volume>89</volume>: <fpage>022712</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-DePalo1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Palo</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Endres</surname><given-names>RG</given-names></name> (<year>2013</year>) <article-title>Unraveling adaptation in eukaryotic pathways: Lessons from protocells</article-title>. <source>PLOS Computational Biology</source> <volume>9</volume>: <fpage>e1003300</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Tostevin1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tostevin</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Ten Wolde</surname><given-names>PR</given-names></name> (<year>2009</year>) <article-title>Mutual information between input and output trajectories of biochemical networks</article-title>. <source>Physical review letters</source> <volume>102</volume>: <fpage>218101</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sartori1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sartori</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Noise filtering strategies in adaptive biochemical signaling networks</article-title>. <source>Journal of statistical physics</source> <volume>142</volume>: <fpage>1206</fpage>–<lpage>1217</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Mello1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mello</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name> (<year>2007</year>) <article-title>Effects of adaptation in maintaining high sensitivity over a wide range of backgrounds for escherichia coli chemotaxis</article-title>. <source>Biophysical journal</source> <volume>92</volume>: <fpage>2329</fpage>–<lpage>2337</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Tu1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Shimizu</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Berg</surname><given-names>HC</given-names></name> (<year>2008</year>) <article-title>Modeling the chemotactic response of escherichia coli to time-varying stimuli</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>105</volume>: <fpage>14855</fpage>–<lpage>14860</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Segall1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segall</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Block</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Berg</surname><given-names>HC</given-names></name> (<year>1986</year>) <article-title>Temporal comparisons in bacterial chemotaxis</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>83</volume>: <fpage>8987</fpage>–<lpage>8991</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Cover1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Cover TM, Thomas JA (2006) Elements of Information Theory. Wiley-Interscience, second edition.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Jarzynski1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jarzynski</surname><given-names>C</given-names></name> (<year>2007</year>) <article-title>Comparison of far-from-equilibrium work relations</article-title>. <source>Comptes Rendus Physique</source> <volume>8</volume>: <fpage>495</fpage>–<lpage>506</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Horowitz1"><label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Horowitz J, Jarzynski C (2007) Comparison of work fluctuation relations. J Stat Mech: Theor Exp: P11002.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Esposito1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Esposito</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Van den Broeck</surname><given-names>C</given-names></name> (<year>2010</year>) <article-title>Three detailed fluctuation theorems</article-title>. <source>Phys Rev Lett</source> <volume>104</volume>: <fpage>090601</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Ge1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ge</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Qian</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Physical origins of entropy produciton, free energy dissipation, and their mathematical representations</article-title>. <source>Phys Rev E</source> <volume>81</volume>: <fpage>051133</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Keymer1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keymer</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Endres</surname><given-names>RG</given-names></name>, <name name-style="western"><surname>Skoge</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Meir</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wingreen</surname><given-names>NS</given-names></name> (<year>2006</year>) <article-title>Chemosensing in escherichia coli: two regimes of two-state receptors</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>103</volume>: <fpage>1786</fpage>–<lpage>1791</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Segel1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segel</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Goldbeter</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Devreotes</surname><given-names>PN</given-names></name>, <name name-style="western"><surname>Knox</surname><given-names>BE</given-names></name> (<year>1986</year>) <article-title>A mechanism for exact sensory adaptation based on receptor modification</article-title>. <source>Journal of theoretical biology</source> <volume>120</volume>: <fpage>151</fpage>–<lpage>179</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Tu2"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tu</surname><given-names>Y</given-names></name> (<year>2013</year>) <article-title>Quantitative modeling of bacterial chemotaxis: Signal amplification and accurate adaptation</article-title>. <source>Annual review of biophysics</source> <volume>42</volume>: <fpage>337</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Seifert1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seifert</surname><given-names>U</given-names></name> (<year>2012</year>) <article-title>Stochastic thermodynamics, fluctuation theorems, and molecular motors</article-title>. <source>Rep Prog Phys</source> <volume>75</volume>: <fpage>126001</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sagawa2"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sagawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ueda</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Second law of thermodynamics with discrete quantum feedback control</article-title>. <source>Phys Rev Lett</source> <volume>100</volume>: <fpage>080403</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sagawa3"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sagawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ueda</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Nonequilibrium thermodynamics of feedback control</article-title>. <source>Phys Rev E</source> <volume>85</volume>: <fpage>021104</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Horowitz2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horowitz</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Vaikuntanathan</surname><given-names>S</given-names></name> (<year>2010</year>) <article-title>Nonequilibrium detailed fluctuation theorem for discrete feedback</article-title>. <source>Phys Rev E</source> <volume>82</volume>: <fpage>061120</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sagawa4"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sagawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ueda</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>Role of mutual information in entropy production under information exchanges</article-title>. <source>New J Phys</source> <volume>15</volume>: <fpage>125012</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Horowitz3"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horowitz</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Esposito</surname><given-names>M</given-names></name> (<year>2014</year>) <article-title>Thermodynamics with continuous information flow</article-title>. <source>Phys Rex X</source> <volume>4</volume>: <fpage>031015</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Cao1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname><given-names>FJ</given-names></name>, <name name-style="western"><surname>Feito</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Thermodynamics of feedback controlled systems</article-title>. <source>Phys Rev E</source> <volume>79</volume>: <fpage>041118</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Barato1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Barato AC, Hartich D, Seifert U (2013) Information-theoretic vs. thermodynamic entropy production in autonomous sensory networks. Phys Rev E 87.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Ito1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sagawa</surname><given-names>T</given-names></name> (<year>2013</year>) <article-title>Information thermodynamics on causal networks</article-title>. <source>Phys Rev Lett</source> <volume>111</volume>: <fpage>180603</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Still1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Still</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sivak</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Crooks</surname><given-names>GE</given-names></name> (<year>2012</year>) <article-title>Thermodynamics of prediction</article-title>. <source>Phys Rev Lett</source> <volume>109</volume>: <fpage>120604</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Diana1"><label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Diana G, Esposito M (2014) Mutual entropy-production in bipartite systems. J Stat Mech: Theor Exp: P04010.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Esposito2"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Esposito</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Van den Broeck</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Second law and landauer principle far from equilibrium</article-title>. <source>Europhys Lett</source> <volume>95</volume>: <fpage>40004</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003974-VanKampen1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Van Kampen NG (2007) Stochastic Processes in Physics and Chemistry. Elsevier Ltd., New York, 3rd edition.</mixed-citation>
</ref>
<ref id="pcbi.1003974-Sagawa5"><label>50</label>
<mixed-citation publication-type="book" xlink:type="simple">Sagawa T (2013) Second law-like inequalitites with quantum relative entropy: An introduction. In: Nakahara M, editor, Lectures on quantum computing, thermodynamics and statistical physics, World Scientific New Jersey, volume 8 of <italic>Kinki University Series on Quantum Computing</italic>.</mixed-citation>
</ref>
</ref-list></back>
</article>