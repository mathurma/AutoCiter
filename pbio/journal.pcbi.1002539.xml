<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01210</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002539</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Visual system</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability theory</subject>
            <subj-group>
              <subject>Probability distribution</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical methods</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>A Maximum Entropy Test for Evaluating Higher-Order Correlations in Spike Counts</article-title><alt-title alt-title-type="running-head">Maximum Entropy Test for Spike Count Correlations</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Onken</surname>
            <given-names>Arno</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Dragoi</surname>
            <given-names>Valentin</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Obermayer</surname>
            <given-names>Klaus</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Technische Universit√§t Berlin, Berlin, Germany</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Bernstein Center for Computational Neuroscience Berlin, Berlin, Germany</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>University of Texas, Houston Medical School, Houston, Texas, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Brown</surname>
            <given-names>Emery N.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">MIT, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">arno.onken@unige.ch</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: VD. Performed the experiments: VD. Analyzed the data: AO. Contributed reagents/materials/analysis tools: AO KO. Wrote the paper: AO VD KO.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>6</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>7</day>
        <month>6</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>6</issue><elocation-id>e1002539</elocation-id><history>
        <date date-type="received">
          <day>11</day>
          <month>8</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>4</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Onken et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Evaluating the importance of higher-order correlations of neural spike counts has been notoriously hard. A large number of samples are typically required in order to estimate higher-order correlations and resulting information theoretic quantities. In typical electrophysiology data sets with many experimental conditions, however, the number of samples in each condition is rather small. Here we describe a method that allows to quantify evidence for higher-order correlations in exactly these cases. We construct a family of reference distributions: maximum entropy distributions, which are constrained only by marginals and by linear correlations as quantified by the Pearson correlation coefficient. We devise a Monte Carlo goodness-of-fit test, which tests - for a given divergence measure of interest - whether the experimental data lead to the rejection of the null hypothesis that it was generated by one of the reference distributions. Applying our test to artificial data shows that the effects of higher-order correlations on these divergence measures can be detected even when the number of samples is small. Subsequently, we apply our method to spike count data which were recorded with multielectrode arrays from the primary visual cortex of anesthetized cat during an adaptation experiment. Using mutual information as a divergence measure we find that there are spike count bin sizes at which the maximum entropy hypothesis can be rejected for a substantial number of neuronal pairs. These results demonstrate that higher-order correlations can matter when estimating information theoretic quantities in V1. They also show that our test is able to detect their presence in typical in-vivo data sets, where the number of samples is too small to estimate higher-order correlations directly.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>Populations of neurons signal information by their joint activity. Dependencies between the activity of multiple neurons are typically described by the linear correlation coefficient. However, this description of the dependencies is not complete. Dependencies beyond the linear correlation coefficient, so-called higher-order correlations, are often neglected because too many experimental samples are required in order to estimate them reliably. Evaluating the importance of higher-order correlations for the neural representation has therefore been notoriously hard. We devise a statistical test that can quantify evidence for higher-order correlations without estimating higher-order correlations directly. The test yields reliable results even when the number of experimental samples is small. The power of the method is demonstrated on data which were recorded from a population of neurons in the primary visual cortex of cat during an adaptation experiment. We show that higher-order correlations can have a substantial impact on the encoded stimulus information which, moreover, is modulated by stimulus adaptation.</p>
      </abstract><funding-group><funding-statement>This work was supported by BMBF grant 01GQ1001B. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="12"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Neural coding examines the way that populations of neurons represent and signal information. A central topic in population coding is the impact of spike count correlations following repeated presentation of the same stimulus (noise correlations). How important are these dependencies for the information carried by the neural population response? Is it important for a decoder to take noise correlations into account? Typically, a distinction is made between (easy to estimate) linear and (hard to estimate) higher-order correlations. We define higher-order correlations as a statistical moment of order greater than two that is not uniquely determined by the first- and second-order statistics. According to this definition, higher-order correlations go beyond the linear correlation and can - but not necessarily have to - involve more than two neurons. These higher-order correlations can exist between pairs of neurons. They refer to all dependencies that are not already characterized by the correlation coefficients.</p>
      <p>The linear correlation coefficient is central to many studies dealing with neural coding. Substantial correlations were found in many cortical areas (see e.g. <xref ref-type="bibr" rid="pcbi.1002539-Gutnisky1">[1]</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1002539-Bair1">[3]</xref>) with a notable exception being reported in <xref ref-type="bibr" rid="pcbi.1002539-Ecker1">[4]</xref>. Furthermore, characteristics of linear noise correlations change with adaptation <xref ref-type="bibr" rid="pcbi.1002539-Gutnisky1">[1]</xref>. Theoretical studies have revealed a strong impact of linear correlations on information measures and optimal decoding <xref ref-type="bibr" rid="pcbi.1002539-Averbeck1">[5]</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1002539-Abbott1">[12]</xref>. However, these studies have focused exclusively on linear correlations.</p>
      <p>Higher-order correlations are notoriously difficult to estimate. The number of samples required for reliable estimation increases exponentially with the order of correlations <xref ref-type="bibr" rid="pcbi.1002539-Staude1">[13]</xref>. Recently, statistical tests were developed for detection of higher-order correlations and cumulants <xref ref-type="bibr" rid="pcbi.1002539-Staude1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Staude2">[14]</xref>. These tests are based on the compound Poisson process as an underlying model and may fail if the model assumptions are not justified. Furthermore, the tests were designed to <italic>detect</italic> higher-order dependencies and not to evaluate their impact on a neural coding measure of interest.</p>
      <p>In order to <italic>assess</italic> the impact of correlations on neural coding, a performance measure must be evaluated which quantifies the ‚Äúquality‚Äù of the neural code. Common measures include the decoding error (e.g. the averaged error of an optimal estimator), the Fisher information (for continuous variables), or the mutual information <xref ref-type="bibr" rid="pcbi.1002539-Averbeck2">[15]</xref>. Calculation of most of these measures, however, requires full knowledge of the probability distribution of the data.</p>
      <p>Probability distributions can be estimated without parametric assumptions using histograms. The histograms can then be used to estimate information theoretic quantities such as the mutual information. However, estimators based on histograms are biased if the sample size is small <xref ref-type="bibr" rid="pcbi.1002539-Panzeri1">[16]</xref>. Bias correction techniques have been developed for alleviating this problem <xref ref-type="bibr" rid="pcbi.1002539-Panzeri1">[16]</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1002539-Paninski1">[18]</xref>. Breaking down the mutual information into several blocks that can be attributed to different statistics of the distribution can also help in reducing the number of required samples <xref ref-type="bibr" rid="pcbi.1002539-Pola1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Scaglione1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Scaglione2">[20]</xref>. Nevertheless, the number of samples required for non-parametric estimators is still on the order of hundreds.</p>
      <p>A complementary approach to estimate probability distributions involves model-based techniques with parametrized higher-order correlations. The most common way to analyze the impact of higher-order correlations is to fit a second-order model (which is based on first- and second-order statistics) to the data and to make a comparison to a higher-order model <xref ref-type="bibr" rid="pcbi.1002539-Ohiorhenuan1">[21]</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1002539-Shlens1">[29]</xref>. The distribution is typically assumed to be stationary over time. The method is restricted to situations in which it is possible to collect a sufficient number of samples for all stimulus conditions. For retinal ganglion cells, for instance, between 100 and 1000 samples can be collected for each bin of the neural activity distribution <xref ref-type="bibr" rid="pcbi.1002539-Shlens1">[29]</xref>.</p>
      <p>Maximum entropy distributions form a family of parametric models that allow to reliably quantify the impact of linear correlations <xref ref-type="bibr" rid="pcbi.1002539-Montemurro1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Jaynes1">[30]</xref>, because single neuron distributions (marginal distributions) and the linear correlation coefficient can be estimated reliably given a small number of samples. The model distribution can then be used to determine above mentioned measures quantifying the quality of the neural code. In principle, maximum entropy methods allow for the inclusion of higher-order correlations also <xref ref-type="bibr" rid="pcbi.1002539-Amari1">[24]</xref>. However, these correlations have to be estimated from the data in order to be included as proper constraints requiring larger amounts of data. Ignoring these higher-order correlations, on the other hand, could possibly lead to biased results. Rich parametric families of distributions, such as copulas, reduce the number of required samples as a trade-off for parametric assumptions <xref ref-type="bibr" rid="pcbi.1002539-Onken1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Onken2">[31]</xref>. Nevertheless, the number of samples that is required to reliably estimate the model parameters is still on the order of hundreds. In summary, previously described methods for detecting and evaluating the impact of higher-order correlations exhibit a number of shortcomings. They either (1) detect higher-order correlations without evaluating their impact on a neural coding measure; (2) are based on strong parametric assumptions; or (3) require a substantial number of samples to construct models that explicitly take higher-order correlations into account, which can be time-consuming and expensive to obtain. In these situations, it would be important to develop methods that assess, based on small sample sizes, whether higher-order correlations may have an impact on the conclusion to be drawn from a particular study or whether a collection of a large number of samples would be indeed required. For instance, let us assume that mutual information between an ensemble of stimuli and the responses of a small population of simultaneously recorded neurons is evaluated. It would then be desirable to design a test based on a small number of samples to assess whether higher-order correlations are present and they lead to conclusions substantially different from the conclusions obtained using linear correlations only.</p>
      <p>Here, we introduce a statistical test to assess whether linear correlations are sufficient for analyzing population spike counts (null hypothesis). To this end, we construct a set of distributions which includes all maximum entropy distributions with linear correlations and a parametric family of marginals and test whether the data is consistent with or rejects the null hypothesis for the selected divergence measures.</p>
      <p>This test is applied to all neuronal pairs for a given population. Hence, the test is sensitive to higher-order correlations between pairs only. The null hypothesis cannot be rejected if all distributions for the pairwise elements are consistent with the maximum entropy distribution. If, however, the null hypothesis is rejected for some of the pairs we can conclude that higher-order correlations are essential and need to be determined using a larger number of samples.</p>
      <p>The paper is organized as follows. The next section contains a detailed description of the statistical test for maximum entropy distributions and the recording procedures. In Section ‚Äú<xref ref-type="sec" rid="s3">Results</xref>‚Äù we verify the statistical test for maximum entropy distributions on various dependency structures that were artificially generated. We then describe the results of the application to recordings from cat V1. The paper concludes with a discussion of the advantages and limitations of the approach and of the findings in V1.</p>
    </sec>
    <sec id="s2" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s2a">
        <title>Ethics Statement</title>
        <p>All experiments were performed under protocols approved by MITs Animal Care and Use Committee.</p>
      </sec>
      <sec id="s2b">
        <title>A Monte Carlo Maximum Entropy Test</title>
        <p>Here we describe a novel test for bivariate spike count distributions that determines whether the dependence structure is sufficiently well characterized by the correlation coefficient. We will first give an intuitive description of the test followed by a rigorous mathematical description.</p>
        <p>The test consists of two parts: (1) construction of a reference distribution which is based on the single neuron spike count distributions and the correlation coefficient and (2) a goodness-of-fit test to calculate a <italic>p</italic>-value and eventually reject the reference distribution.</p>
        <p>The reference distribution formalizes the linear dependency assumption. For this purpose, we apply a maximum entropy model subject to a set of constraints. The constraints contain the complete single neuron spike count distributions and the linear correlation coefficient. Everything is therefore fixed by the distribution constraints except for the higher-order correlations. If this reference distribution can be statistically rejected then we can conclude that higher-order correlations do matter.</p>
        <p>The single neuron spike count distributions and the correlation coefficient are not known a priori. Instead, they must be estimated from the data. For simplicity, we assume that the single neuron distributions are Poisson distributed. This leaves us with the estimation of firing rates and the correlation coefficient. The test should be applicable even when the number of samples is very small. Therefore, any estimates of distribution parameters are not reliable. Instead of relying on specific estimates of these parameters, we maximize the <italic>p</italic>-value over these parameters and then use the most conservative <italic>p</italic>-value.</p>
        <p><xref ref-type="fig" rid="pcbi-1002539-g001">Figure 1</xref> shows a flow diagram of the test. A step-by-step procedure is provided in <xref ref-type="table" rid="pcbi-1002539-t001">Table 1</xref>. In step (1) the Poisson parameters and the correlation coefficient are initialized with their sample means that are obtained from the data set. The <italic>p</italic>-value is then maximized by applying an optimization algorithm like simulated annealing (step (2), cf. Section ‚ÄúOptimization of the Nuisance Parameters‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). To this end, we estimate the <italic>p</italic>-value based on the maximum entropy distribution subject to the optimization parameters. First, we calculate a divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e001" xlink:type="simple"/></inline-formula> between the data and the maximum entropy distribution (step 2.2). We then draw many samples from the maximum entropy distribution and estimate an empirical distribution over divergences (step 2.3). By comparing the divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e002" xlink:type="simple"/></inline-formula> to this empirical distribution we can assess how likely it is that the maximum entropy distribution generated the data. This gives us a <italic>p</italic>-value for a particular set of Poisson rates and a correlation coefficient (step 2.4). The maximization over these parameters then yields the most conservative <italic>p</italic>-value. In step (3) the second-order assumption is rejected if the <italic>p</italic>-value is below the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e003" xlink:type="simple"/></inline-formula> significance level.</p>
        <fig id="pcbi-1002539-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Flow diagram of the maximum entropy test.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g001" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002539-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.t001</object-id><label>Table 1</label><caption>
            <title>Step procedure of the Monte Carlo Maximum Entropy Test.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002539-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">(1) Initialize spike count rates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e004" xlink:type="simple"/></inline-formula> and correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e005" xlink:type="simple"/></inline-formula> with their sample estimates from the data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e006" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(2) Maximize the <italic>p</italic>-value over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e008" xlink:type="simple"/></inline-formula> using simulated annealing:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(2.1) Compute the maximum entropy distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e009" xlink:type="simple"/></inline-formula> subject to Poisson marginals with rates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e011" xlink:type="simple"/></inline-formula> and correlation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e012" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(2.2) Calculate the divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e013" xlink:type="simple"/></inline-formula> between the empirical distribution of the data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e014" xlink:type="simple"/></inline-formula> and the maximum entropy distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e015" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(2.3) For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e016" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e017" xlink:type="simple"/></inline-formula>:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">‚ÄÉ(2.3.1) Draw <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e018" xlink:type="simple"/></inline-formula> samples from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e019" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e020" xlink:type="simple"/></inline-formula> is the number of samples in the data set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e021" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">‚ÄÉ(2.3.2) Calculate the divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e022" xlink:type="simple"/></inline-formula> between the empirical distribution of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e023" xlink:type="simple"/></inline-formula> drawn samples and the maximum entropy distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e024" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(2.4) Estimate the <italic>p</italic>-value based on the number of indices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e025" xlink:type="simple"/></inline-formula> for which <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e026" xlink:type="simple"/></inline-formula>; if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e027" xlink:type="simple"/></inline-formula> then toss a coin to decide whether to include the index</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">(3) Reject <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e028" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e029" xlink:type="simple"/></inline-formula></td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
        <p>An implementation of the test including an application scenario for MathWorks MATLAB and GNU Octave is available online at a software directory of the Technische Universit√§t Berlin (<ext-link ext-link-type="uri" xlink:href="http://www.ni.tu-berlin.de/menue/software/monte_carlo_maximum_entropy_test/" xlink:type="simple">http://www.ni.tu-berlin.de/menue/software/monte_carlo_maximum_entropy_test/</ext-link>).</p>
        <sec id="s2b1">
          <title>Formal test description</title>
          <p>We will first describe the maximum entropy reference distribution and then the goodness-of-fit test based on a Monte Carlo procedure.</p>
          <p>Consider a bivariate distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e030" xlink:type="simple"/></inline-formula> over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e031" xlink:type="simple"/></inline-formula> with marginals<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e032" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e033" xlink:type="simple"/></inline-formula> denotes the support of the random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e034" xlink:type="simple"/></inline-formula>. The linear correlation coefficient is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e035" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e036" xlink:type="simple"/></inline-formula> denotes the expectation and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e037" xlink:type="simple"/></inline-formula> denotes the variance of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e038" xlink:type="simple"/></inline-formula>. The maximum entropy distribution constrained by the marginals (Equation 1) of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e039" xlink:type="simple"/></inline-formula> and the correlation coefficient (Equation 2) is the distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e040" xlink:type="simple"/></inline-formula> that maximizes the entropy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e041" xlink:type="simple"/></inline-formula> subject to the constraints:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e042" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e043" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e044" xlink:type="simple"/><label>(5)</label></disp-formula>It was shown <xref ref-type="bibr" rid="pcbi.1002539-Pasha1">[32]</xref> that this distribution is uniquely given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e045" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e046" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e047" xlink:type="simple"/></inline-formula> are obtained by solving:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e048" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e049" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e050" xlink:type="simple"/><label>(9)</label></disp-formula></p>
          <p>In order to examine whether a given data set is consistent with such a maximum entropy reference distribution we apply the multinomial distribution. Every bivariate discrete distribution with finite support can be represented by a table containing the probability distribution mass function. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e051" xlink:type="simple"/></inline-formula> denote the number of boxes of this table with frequencies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e052" xlink:type="simple"/></inline-formula>. For a given number of draws <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e053" xlink:type="simple"/></inline-formula> from this distribution the probability mass function of these entire draws follows a multinomial distribution:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e054" xlink:type="simple"/><label>(10)</label></disp-formula>The number of draws, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e055" xlink:type="simple"/></inline-formula>, corresponds to the number of observations in the data set. The maximum entropy distributions constrained up to second-order constitute a subset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e056" xlink:type="simple"/></inline-formula> of the set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e057" xlink:type="simple"/></inline-formula> of all multinomial distributions. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e058" xlink:type="simple"/></inline-formula> denote the frequencies of the true distribution of the observed data. We then consider the null hypothesis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e059" xlink:type="simple"/></inline-formula>, that the observed data are drawn from a distribution from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e060" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e061" xlink:type="simple"/><label>(11)</label></disp-formula>If the null hypothesis can be rejected, then the marginals and linear correlation coefficient are not sufficient to characterize the underlying distribution of the data. This is formalized as the alternative hypothesis:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e062" xlink:type="simple"/><label>(12)</label></disp-formula></p>
          <p>In order to test the null hypothesis the distributions of the test statistics are approximated by Monte Carlo sampling over nuisance parameters. The nuisance parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e063" xlink:type="simple"/></inline-formula> are unknown parameters such as the correlation coefficient or parameters of the marginal distributions. The Monte Carlo goodness-of-fit test with nuisance parameters follows <xref ref-type="bibr" rid="pcbi.1002539-Dufour1">[33]</xref>: for a given set of nuisance parameters and resulting reference distribution (in our case the maximum entropy distribution), Monte Carlo samples are drawn. A single Monte Carlo sample consists of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e064" xlink:type="simple"/></inline-formula> samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e065" xlink:type="simple"/></inline-formula> from the reference distribution. A total of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e066" xlink:type="simple"/></inline-formula> Monte Carlo samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e067" xlink:type="simple"/></inline-formula> are drawn. Each Monte Carlo sample gives rise to an empirical distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e068" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e069" xlink:type="simple"/></inline-formula> denotes the cardinality of the set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e070" xlink:type="simple"/></inline-formula>. In order to obtain a distribution over test statistics for the reference distribution under review, the divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e071" xlink:type="simple"/></inline-formula> between the original reference distribution and each of the empirical distributions of the samples <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e072" xlink:type="simple"/></inline-formula> is calculated. This distribution is compared to the divergence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e073" xlink:type="simple"/></inline-formula> between the reference distribution and the empirical distribution of the observed data. The location of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e074" xlink:type="simple"/></inline-formula> within the empirical distribution of test statistics <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e075" xlink:type="simple"/></inline-formula> yields a <italic>p</italic>-value:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e076" xlink:type="simple"/><label>(14)</label></disp-formula></p>
          <p>The spike count distribution is a discrete distribution and, therefore, the distribution of test statistics is also discrete. The formalism requires a properly randomized distribution to be rigorous <xref ref-type="bibr" rid="pcbi.1002539-Dufour1">[33]</xref>. Hence, we need to take into account ties of the test statistics. To accomplish this a simple procedure called tie breaking can be applied <xref ref-type="bibr" rid="pcbi.1002539-Dufour1">[33]</xref>. Essentially, the procedure orders test statistics randomly whenever they are the same.</p>
          <p>Formally, for each test statistic <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e077" xlink:type="simple"/></inline-formula> an i.i.d. random variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e078" xlink:type="simple"/></inline-formula> is drawn from the uniform distribution on the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e079" xlink:type="simple"/></inline-formula>. The test statistics are then reordered according to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e080" xlink:type="simple"/></inline-formula>.</p>
          <p>The <italic>p</italic>-value has to be corrected for the finite number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e081" xlink:type="simple"/></inline-formula> of Monte Carlo samples and is given by <xref ref-type="bibr" rid="pcbi.1002539-Dufour1">[33]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e082" xlink:type="simple"/><label>(15)</label></disp-formula></p>
          <p>The <italic>p</italic>-value is then maximized over the nuisance parameters (i.e., the correlation coefficient or parameters of the marginals) that span the space of maximum entropy distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e083" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e084" xlink:type="simple"/><label>(16)</label></disp-formula>The procedure results in a test in which the false rejection rate is guaranteed to be below the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e085" xlink:type="simple"/></inline-formula>-level <xref ref-type="bibr" rid="pcbi.1002539-Dufour1">[33]</xref>, thus allowing us to examine evidence for higher-order correlations that is reflected in the particular divergence measure of interest. Note that the power of the test slightly increases with the number of Monte Carlo samples. However, the power is primarily affected by the sample size (cf. Section ‚ÄúComparison to Likelihood Ratio Test‚Äù).</p>
          <p>In this paper we consider two divergence measures that are based on common information measures: The entropy difference:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e086" xlink:type="simple"/><label>(17)</label></disp-formula>and the mutual information difference:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e087" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e088" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e089" xlink:type="simple"/><label>(19)</label></disp-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e090" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e091" xlink:type="simple"/></inline-formula> are random variables with conditional distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e092" xlink:type="simple"/></inline-formula> of a given realization <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e093" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e094" xlink:type="simple"/></inline-formula>.</p>
          <p>If the test rejects the second-order hypothesis then we can conclude that significant differences exist in terms of the information measure between the data and the predictions of models neglecting higher-order correlations. For the mutual information between stimuli and neural responses, for example, this means that the mutual information estimates obtained for data and models are significantly different. Hence our approach tests for higher-order correlations which are relevant for a particular analysis task. Consider the extreme example that only the probability of both neurons being silent (no spikes of both neurons) is of interest. Then one could apply a divergence measure which takes only the probability of that spike count pair into account. In contrast, other common approaches would just quantify a general divergence of the distributions which might not be of interest at all.</p>
          <p>Additional divergence measures are discussed in the supporting information (cf. Section ‚ÄúAlternative Divergence Measures‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>).</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Recording Procedures</title>
        <p>The experimental procedures have been described previously <xref ref-type="bibr" rid="pcbi.1002539-Dragoi1">[34]</xref>. Here, we briefly repeat the description for completeness. The experiment was performed under protocols approved by MIT's Animal Care and Use Committee. The animal was anesthetized and paralyzed. Neural responses were measured to drifting high-contrast square-wave gratings of 16 directions. Each drifting grating had a frequency of 1 Hz. In the control conditions the 16 drifting gratings were presented for 10 trials each for a total of 160 trials, 2.5 s each presentation. We selected the first 7 trials each to match the number of trials of the second condition. In the adaptation condition one grating of fixed orientation moving randomly in two opposite directions was presented for a duration of 2 min. Afterward, the drifting test gratings with 16 different directions were presented randomly for 2.5 s each (7 trials per grating), preceded by a 5 s ‚Äútopping-up‚Äù presentation of the adapting orientation. Multiple simultaneous extra-cellular recordings were made using tungsten microelectrodes at cortical depths between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e095" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e096" xlink:type="simple"/></inline-formula> in the primary visual cortex. Responses from 11 cells were recorded whose orientation preferences in the control condition covered the entire orientation range. The signal was amplified and thresholded to obtain spike trains.</p>
      </sec>
      <sec id="s2d">
        <title>Data Analysis</title>
        <p>The spike trains were binned into non-overlapping time intervals and the number of spikes was counted within these intervals. The length of the intervals was varied between 10 ms and 400 ms. Three recurrences of the same grating appeared in each of the 2.5 s presentations. The sample pool of each model and each test was based on the orientation of the grating stimulus (trials with opposing directions of movement were combined), on the time points within an iteration of the drifting grating and on the two experimental conditions. This yielded a total of 42 repetitions each for the control and adaptation conditions, the 8 orientations and for each of the time intervals with stimulus presentation. Samples from within an iteration were not mixed to prevent confounding effects from varying rates. Therefore, separate tests were used for every time step and every neuron pair. The false discovery rate of the multiple testing rejections was corrected using the Benjamini-Hochberg procedure <xref ref-type="bibr" rid="pcbi.1002539-Benjamini1">[35]</xref> with a significance level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e097" xlink:type="simple"/></inline-formula>. This multiple testing procedure controls the expected proportion of falsely rejected hypotheses in a multiple inference setting. It was applied over all pairs, all time bins for a given bin size and, in the case of entropy difference as the divergence measure, over all stimulus orientations. The correction was not applied over all bin sizes, because the potential absence of higher-order correlations is treated as a separate hypothesis for every bin size.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <sec id="s3a">
        <title>Validation of the Test on Artificial Data</title>
        <p>We devised a test that determines whether the dependence structure of spike count distributions is sufficiently well characterized by the second-order statistics (cf. ‚Äú<xref ref-type="sec" rid="s2">Materials and Methods</xref>‚Äù). The test was applied to spike count samples drawn from three different families of bivariate distributions: (1) a family of maximum entropy distributions constrained by rates and linear correlations only (ME), (2) a family of distributions with higher-order correlations but vanishing linear correlations (M1), and (3) a family of distributions with higher-order correlations in the presence of limited linear correlations (M2). Marginals <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e098" xlink:type="simple"/></inline-formula> were always Poisson distributed, i.e.<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e099" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e100" xlink:type="simple"/></inline-formula> is the mean and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e101" xlink:type="simple"/></inline-formula> is the spike count variable.</p>
        <p>The maximum entropy distributions ME served as reference distributions and were constructed according to Equations 6‚Äì9 and varying correlation coefficient. In order to investigate the power of the test we constructed two distribution families M1 and M2, which included higher-order correlations. These families are based on so-called copulas, which allow us to construct multivariate distributions with Poisson marginals and higher-order correlations <xref ref-type="bibr" rid="pcbi.1002539-Onken1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002539-Nelsen1">[36]</xref>. The families M1 and M2 consisted of two components:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e102" xlink:type="simple"/><label>(21)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e103" xlink:type="simple"/></inline-formula> is a mixture parameter, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e104" xlink:type="simple"/></inline-formula> are spike counts. The two components were defined as a maximum entropy distribution of the family ME (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e105" xlink:type="simple"/></inline-formula>, cf. Equations 6‚Äì9 with Poisson marginals as in Equation 20 and <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 A</xref>, left, and <xref ref-type="fig" rid="pcbi-1002539-g002">2 B</xref>, left) and a copula-based distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e106" xlink:type="simple"/></inline-formula> (cf. <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 A</xref>, right, for M1 and <xref ref-type="fig" rid="pcbi-1002539-g002">2 B</xref>, right, for M2) which was a mixture distribution by itself and which showed significant higher-order correlations <xref ref-type="bibr" rid="pcbi.1002539-Onken1">[23]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e107" xlink:type="simple"/><label>(22)</label></disp-formula>The cumulative distribution function (CDF) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e108" xlink:type="simple"/></inline-formula> is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e109" xlink:type="simple"/><label>(23)</label></disp-formula>where the CDF's of the Poisson marginals are defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e110" xlink:type="simple"/><label>(24)</label></disp-formula><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e111" xlink:type="simple"/></inline-formula> is the rate parameter of element <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e112" xlink:type="simple"/></inline-formula>. Equations 22 and 23 hold for every copula-based distribution with discrete marginals. Linear and higher-order correlations are specified by the applied copula. The copula <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e113" xlink:type="simple"/></inline-formula> of the model is defined as a Gaussian mixture copula<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e114" xlink:type="simple"/><label>(25)</label></disp-formula>The bivariate Gaussian copula family is defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e115" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e116" xlink:type="simple"/></inline-formula> is the CDF of the bivariate zero-mean unit-variance normal distribution with correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e118" xlink:type="simple"/></inline-formula> is the inverse of the CDF of the univariate zero-mean unit-variance Gaussian distribution.</p>
        <fig id="pcbi-1002539-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Evaluation of the maximum entropy test on artificial data.</title>
            <p>(A) Probability mass functions of the maximum entropy distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e119" xlink:type="simple"/></inline-formula> (left) and the Gaussian copula based distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e120" xlink:type="simple"/></inline-formula> (right) of the mixture distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e121" xlink:type="simple"/></inline-formula> with a linear correlation coefficient of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e122" xlink:type="simple"/></inline-formula>. The Poisson marginals (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e123" xlink:type="simple"/></inline-formula>) are plotted along the axes. (B) Same as A but for the mixture distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e124" xlink:type="simple"/></inline-formula> with a linear correlation coefficient of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e125" xlink:type="simple"/></inline-formula>. (C) Percent rejections of the null hypothesis using the entropy difference as the divergence measure. Significance level was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e126" xlink:type="simple"/></inline-formula>. Rejection rates were estimated over 100 tests. Different lines correspond to different numbers of samples drawn from the candidate distribution: 10 (red dotted line), 50 (green dash-dotted line), 100 (blue dashed line), and 200 (black solid line). (Left) Results for the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e127" xlink:type="simple"/></inline-formula> family for varying correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e128" xlink:type="simple"/></inline-formula>. (Center) Results for distributions from the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e129" xlink:type="simple"/></inline-formula> family (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e130" xlink:type="simple"/></inline-formula>) for varying mixture parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e131" xlink:type="simple"/></inline-formula> (cf. <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 A</xref>). (Right) Same for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e132" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e133" xlink:type="simple"/></inline-formula>, cf. <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 B</xref>)). Poisson rate was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e134" xlink:type="simple"/></inline-formula> for all candidate distributions (corresponding to 30 Hz and 100 ms bins). Simulated annealing <xref ref-type="bibr" rid="pcbi.1002539-Kirkpatrick1">[45]</xref> was applied to maximize the <italic>p</italic>-value (cf. <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e135" xlink:type="simple"/></inline-formula> of Monte Carlo samples was 1000.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g002" xlink:type="simple"/>
        </fig>
        <p>We chose the Gaussian copula model to construct <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e136" xlink:type="simple"/></inline-formula>, because it can be used to systematically introduce higher-order correlations while keeping the marginal distributions unchanged. The marginal Poisson distributions have one rate parameter each whereas the Gaussian copula family has one parameter which affects the linear correlation coefficient and higher-order correlations of the bivariate model, but not the marginal distributions. The resulting linear correlation coefficients can be positive or negative. We can therefore apply a mixture of two Gaussian copulas with two opposite correlation coefficients. This yields a model with an overall linear correlation coefficient that is arbitrarily small. The higher-order correlations, on the other hand, can still be strong. These dependencies are visible as a cross in the probability mass function (<xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 A</xref>, right).</p>
        <p>In order to generate strong higher-order correlations we set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e137" xlink:type="simple"/></inline-formula> and numerically adjusted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e138" xlink:type="simple"/></inline-formula> for each <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e139" xlink:type="simple"/></inline-formula> to obtain a certain linear correlation coefficient for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e140" xlink:type="simple"/></inline-formula> (correlation coefficient 0 for model M1 and 0.2 for model M2).</p>
        <p>For family M1 the correlation coefficient of both <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e141" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e142" xlink:type="simple"/></inline-formula> was set to 0 (no linear correlation) while for family M2 the correlation coefficient was set to 0.2 (weak positive linear correlation). Therefore, the linear correlation coefficient (and the marginals) of families (M1) and (M2) were by construction independent of the mixture parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e143" xlink:type="simple"/></inline-formula>. The mixture parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e144" xlink:type="simple"/></inline-formula>, however, controlled the strength of the higher-order correlations. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e145" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e146" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e147" xlink:type="simple"/></inline-formula> corresponded to maximum entropy distributions with linear correlations only, while <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e148" xlink:type="simple"/></inline-formula> led to distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e149" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e150" xlink:type="simple"/></inline-formula> with higher-order correlations. A mixture distribution can be interpreted as a model with multiple common inputs which are active at different times. In the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e151" xlink:type="simple"/></inline-formula> model there were two mixture elements with opposite correlation coefficients corresponding to inputs that produce correlated or anticorrelated responses. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e152" xlink:type="simple"/></inline-formula> these inputs were absent. Increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e153" xlink:type="simple"/></inline-formula> corresponded to increasing the strengths of these inputs.</p>
        <p><xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref> shows the results of the maximum entropy test for higher-order correlations for several members of the M1 and M2 families of bivariate spike count distributions for the entropy difference (Equation 17) as the divergence measure. All subfigures show the percent rejections of the null hypothesis <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e154" xlink:type="simple"/></inline-formula> (cf. previous section) on a significance level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e155" xlink:type="simple"/></inline-formula>, i.e. the rejections of the hypothesis that higher-order correlations did not significantly influence the estimated values for the entropy of the distributions M1 and M2. The rejection rates were estimated over 100 trials. Different lines represent different sample sizes (10, 50, 100 and 200) to which the test was applied.</p>
        <p><xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref> (left) shows the percent rejections of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e156" xlink:type="simple"/></inline-formula> for data samples from maximum entropy distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e157" xlink:type="simple"/></inline-formula> with different linear, but no higher-order, correlations present. As expected, the achieved Type I error (i.e. rejections despite absence of higher-order correlations in the underlying distribution) was small and the acceptance rate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e158" xlink:type="simple"/></inline-formula> was close to the desired value of 95%. The center and left subfigures in <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref> show the percent rejections of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e159" xlink:type="simple"/></inline-formula> for different strengths of higher-order correlations of samples drawn from the M1- (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e160" xlink:type="simple"/></inline-formula>) and M2-distributions (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e161" xlink:type="simple"/></inline-formula>). The larger the mixture parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e162" xlink:type="simple"/></inline-formula> the higher the percent rejection of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e163" xlink:type="simple"/></inline-formula>, i.e. percent rejections increases for increasing strength of the higher-order correlations. Moreover, Type II errors (no rejections despite presence of higher-order correlations in the underlying distribution) decrease for increasing sample sizes. Therefore, the test can successfully detect moderately strong higher-order correlations in artificial data even when the sample size is on the order of 50. Since the results for the M1- (<xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref>, center) and M2-distributions (<xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref>, right) were similar, the test was insensitive to the presence of linear correlations. Additional results for a Poisson rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e164" xlink:type="simple"/></inline-formula> (corresponding to 50 Hz and 100 ms bins) are shown in Figure S2 in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref> and resemble those of <xref ref-type="fig" rid="pcbi-1002539-g002">Figure 2 C</xref>.</p>
        <sec id="s3a1">
          <title>Comparison to likelihood ratio test</title>
          <p>We compared the proposed Monte Carlo maximum entropy test to a standard approach for statistical testing: the likelihood ratio test <xref ref-type="bibr" rid="pcbi.1002539-Wald1">[37]</xref>. This test evaluates whether a reduced model provides a fit that is as good as a full model. In this case, the full model is the multinomial distribution and the reduced model is the maximum entropy distribution. Parameters are estimated by maximizing the likelihood of the respective model. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e165" xlink:type="simple"/></inline-formula> denote the likelihood of the maximum entropy distribution and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e166" xlink:type="simple"/></inline-formula> the likelihood function of the multinomial distribution (cf. Equations 6 and 10). Then the likelihood ratio statistic is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e167" xlink:type="simple"/><label>(27)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e168" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e169" xlink:type="simple"/></inline-formula> are the maximum likelihood estimators of the distribution parameters. For sufficiently large sample size, the LR test statistic is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e170" xlink:type="simple"/></inline-formula> distributed with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e171" xlink:type="simple"/></inline-formula> degrees of freedom, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e172" xlink:type="simple"/></inline-formula> denotes the number of additional free parameters in the full model compared to the maximum entropy model. Note that for limited sample sizes the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e173" xlink:type="simple"/></inline-formula> distribution is not necessarily exact.</p>
          <p>We compared the likelihood ratio test to the proposed Monte Carlo test for varying sample sizes. <xref ref-type="fig" rid="pcbi-1002539-g003">Figure 3 A</xref> shows the percentages of rejections of the maximum entropy hypothesis for data that were sampled from a maximum entropy model (Type I error). For both tests the percentages do not depend on the number of samples. The percentages of rejections are generally smaller for the likelihood ratio test. Both tests, however, have a rejection rate below the significance level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e174" xlink:type="simple"/></inline-formula>.</p>
          <fig id="pcbi-1002539-g003" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Effect of sample size on the Monte Carlo maximum entropy test results (solid black line) and on the maximum likelihood ratio test results (dashed blue line) with Poisson rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e175" xlink:type="simple"/></inline-formula>. </title>
              <p>The entropy difference was used as a divergence measure. Significance level was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e176" xlink:type="simple"/></inline-formula>. Rates were estimated over 100 trials. ( A) Percent rejections of the maximum entropy hypothesis. Data were sampled from maximum entropy distributions with random correlation strengths. ( B) Percent rejections of the null hypothesis. Data were sampled from a copula-based mixture model with uniformly random mixture parameter (cf. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e177" xlink:type="simple"/></inline-formula>, Equation 21).</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g003" xlink:type="simple"/>
          </fig>
          <p>We also evaluated the power of the tests as a function of sample size. We expected that the Monte Carlo test would perform better for small sample sizes, because the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e178" xlink:type="simple"/></inline-formula> distribution is less accurate in these cases. <xref ref-type="fig" rid="pcbi-1002539-g003">Figure 3 B</xref> shows the power with respect to the copula mixture models (cf. Section ‚ÄúValidation of the Test on Artificial Data‚Äù) with a mixture parameter that was sampled uniformly. The figure shows that the power of the proposed test is indeed much greater when the sample size is small. The likelihood ratio test has almost no rejections for sample sizes below 50, whereas the Monte Carlo test rejects between 5% and 20% of the maximum entropy hypotheses. But also for medium sample sizes the power of the Monte Carlo test surpasses that of the likelihood ratio test.</p>
        </sec>
        <sec id="s3a2">
          <title>Impact of autocorrelations</title>
          <p>Neural spike trains typically have autocorrelation structure which is known to affect estimates of correlations <xref ref-type="bibr" rid="pcbi.1002539-Tetzlaff1">[38]</xref>. We explored the impact of autocorrelations on the proposed Monte Carlo maximum entropy test by simulating gamma processes with varying refractory periods. Interspike intervals were sampled from the gamma distribution<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e179" xlink:type="simple"/><label>(28)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e180" xlink:type="simple"/></inline-formula> is the rate of the process (set to 30 Hz) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e181" xlink:type="simple"/></inline-formula> is the gamma function. The exponential distribution is a special case of the gamma distribution when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e182" xlink:type="simple"/></inline-formula>. In this case we obtain a Poisson process, whereas for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e183" xlink:type="simple"/></inline-formula> the gamma process has a refractory period. We varied <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e184" xlink:type="simple"/></inline-formula> between 1 and 2. The processes become more regular when we increase <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e185" xlink:type="simple"/></inline-formula>. Autocorrelations therefore increase with increasing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e186" xlink:type="simple"/></inline-formula>.</p>
          <p>In each trial we simulated two concurrent spike trains. The goodness-of-fit of a Poisson process was assessed with a Kolmogorov-Smirnov test at a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e187" xlink:type="simple"/></inline-formula> significance level (c.f. Section ‚ÄúPoisson Goodness-of-fit Tests‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Moreover, we binned the spike trains into 100 ms intervals and calculated simultaneous spike count pairs. We then applied our proposed maximum entropy test to the spike count pairs. <xref ref-type="fig" rid="pcbi-1002539-g004">Figure 4</xref> shows the rejections rates for ( A) 50 samples (corresponding to spike trains of length 5 s) and ( B) 100 samples (corresponding to spike trains of length 10 s) over 100 trials. The rejection rates of both tests increase with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e188" xlink:type="simple"/></inline-formula>. This reflects that the deviation from Poisson processes increases. Furthermore, both tests have greater rejection rates when applied to 100 samples ( B) than when applied to 50 samples ( A).</p>
          <fig id="pcbi-1002539-g004" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g004</object-id>
            <label>Figure 4</label>
            <caption>
              <title>Effect of autocorrelations on the Monte Carlo maximum entropy test results (blue) and on the discrete Kolmogorov-Smirnov test results (red).</title>
              <p>Interspike intervals of two concurrent spike trains were sampled from a gamma distribution with constant rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e189" xlink:type="simple"/></inline-formula> and gamma parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e190" xlink:type="simple"/></inline-formula>. Spike counts were calculated over subsequent 100 ms bins. The entropy difference was used as a divergence measure. Significance level was <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e191" xlink:type="simple"/></inline-formula>. Rates were estimated over 100 trials. ( A) 50 spike count pairs were sampled for each test trial. ( B) 100 spike count pairs were sampled for each test trial.</p>
            </caption>
            <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g004" xlink:type="simple"/>
          </fig>
          <p>In almost every case, rejection rates of the maximum entropy test are lower compared to the Kolmogorov-Smirnov test. This shows that the Kolmogorov-Smirnov test is more sensitive to autocorrelations of gamma processes and has more statistical power than our proposed maximum entropy test. This comes as no surprise, since the Kolmogorov-Smirnov test operates directly on the spike trains whereas the maximum entropy test assesses the spike counts without any information of spike timing.</p>
        </sec>
      </sec>
      <sec id="s3b">
        <title>Application to Data Recorded from Cat V1</title>
        <p>The new maximum entropy test was applied to neural spike trains recorded from the primary visual cortex of anesthetized cat during visual stimulation <xref ref-type="bibr" rid="pcbi.1002539-Dragoi1">[34]</xref>. The protocols of the neurophysiological experiments are depicted in <xref ref-type="fig" rid="pcbi-1002539-g005">Figure 5</xref>. Drifting gratings of random orientations between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e192" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e193" xlink:type="simple"/></inline-formula> (resolution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e194" xlink:type="simple"/></inline-formula>) were presented during two conditions. In the control condition, each (test) orientation was presented for 2.5 s. In the adaptation condition, an initial block (2 min) of one grating of fixed orientation was followed by random presentations of the 8 orientations (2.5 s). Each of these (test) gratings was preceded by a 5 s presentation of the adapted grating in order to maintain the orientation effects. Simultaneous neural activity from 11 cells was recorded by multiple electrodes in V1. The resulting spike trains were binned and transformed to spike count sequences. We thereby obtained a total of 42 repetitions for each condition, orientation and non-overlapping spike train bin of varying length.</p>
        <fig id="pcbi-1002539-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Illustration of the control and adaptation protocols.</title>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g005" xlink:type="simple"/>
        </fig>
        <p>Application of the maximum entropy test requires a maximization of the <italic>p</italic>-values over the nuisance parameters (Equation 16) which include the marginal distributions. Because a maximization over all possible marginal distributions would have been unfeasible, we made a parametric assumption and described all marginal distributions by Poisson distributions (Equation 20) with rate parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e195" xlink:type="simple"/></inline-formula> as the only nuisance parameters. Note that parameter values differed between neurons, conditions, orientations and spike train bins.</p>
        <p>The stimulus grating was drifting with a frequency of 1 Hz. The size of the spike count bins was varied between 10 ms and 400 ms. The changing stimulus-driven rate might therefore violate the Poisson assumption depending on the size of the bin. Several statistical tests were applied to check whether our assumption should be rejected (cf. Section ‚ÄúPoisson Goodness-of-fit Tests‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Indeed, neither a single neuron Monte Carlo goodness-of-fit test for Poisson statistics nor a multivariate Monte Carlo goodness-of-fit test for the product distribution (after removing all dependencies) led to rejections of the Poisson hypothesis for any of the bin sizes. Taken together, these findings do not provide any evidence against our assumption of Poisson-distributed marginals.</p>
        <p>Furthermore, we applied Kolmogorov-Smirnov tests based on the discrete time rescaling theorem <xref ref-type="bibr" rid="pcbi.1002539-Haslinger1">[39]</xref> quantifying the interspike interval statistics (cf. ‚ÄúPoisson Goodness-of-fit Tests‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Although a discrete Poisson distribution of the spike counts does not necessarily imply interspike interval statistics that follow a Poisson spike generating process, the reverse always holds. For rates estimated in 100 ms bins, the rejection rates of the Poisson process hypothesis were below 5%. For greater bin sizes (200 ms, 400 ms), the rejection rates increased. For a detailed discussion, see ‚ÄúPoisson Goodness-of-fit Tests‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>.</p>
        <p>We applied separate maximum entropy tests to all 55 neuronal pairs and to all time bins (non-overlapping time intervals locked to the start of a grating presentation at varying latencies, cf. Section ‚ÄúData Analysis‚Äù). <xref ref-type="fig" rid="pcbi-1002539-g006">Figure 6 A</xref> shows the results for the entropy difference (Equation 17) as the divergence measure. The rejections were corrected for multiple inferences and averaged over neuron pairs, stimuli and time bins for given bin sizes (cf. Section ‚ÄúData Analysis‚Äù). The fraction of rejected pairs increased with increasing bin size until it reached a maximum at 200 ms. Therefore, as bin size increases, more and more neuron pairs show significant differences between the entropy estimated directly from the data and the entropy estimated using models which neglect higher-order correlations.</p>
        <fig id="pcbi-1002539-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Results of the maximum entropy test for data recorded from area V1 of anesthetized cat.</title>
            <p>The evaluation was performed separately for the control and adaptation conditions. ( A) Fraction of neuron pairs rejected by the Monte Carlo maximum entropy test with the entropy difference as the divergence measure (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e196" xlink:type="simple"/></inline-formula>) and for different bin sizes. ( B) Same as in A but using the mutual information difference. Rejection rates were averaged over all neuron pairs and all time bins. Simulated annealing <xref ref-type="bibr" rid="pcbi.1002539-Kirkpatrick1">[45]</xref> was applied to maximize the <italic>p</italic>-value (cf. <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e197" xlink:type="simple"/></inline-formula> of Monte Carlo samples was 1000. The false discovery rate of the rejections was corrected using the Benjamini-Hochberg procedure <xref ref-type="bibr" rid="pcbi.1002539-Benjamini1">[35]</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g006" xlink:type="simple"/>
        </fig>
        <p>The smaller value for a bin size of 400 ms could already be a consequence of the central limit theorem: For finite rates and in the limit of large bin sizes the distribution of the spike counts converges to a bivariate normal distribution which is an instance of a second-order maximum entropy distribution.</p>
        <p><xref ref-type="fig" rid="pcbi-1002539-g006">Figure 6 B</xref> shows the results for the difference in mutual information (Equation 18) as the divergence measure. The mutual information is calculated between the orientations of the test-gratings and the corresponding spike counts: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e198" xlink:type="simple"/></inline-formula> were the flat distributions over gratings, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e199" xlink:type="simple"/></inline-formula> was the maximum entropy reference distribution subject to Poisson rates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e200" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e201" xlink:type="simple"/></inline-formula> and a correlation coefficient <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e202" xlink:type="simple"/></inline-formula> for each of the eight stimulus values. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e203" xlink:type="simple"/></inline-formula> was the empirical distribution over the data set or Monte Carlo sample.</p>
        <p>The fraction of rejected pairs increased with bin size showing that for many bin sizes there was significant evidence for higher-order correlations that was reflected in the mutual information for a substantial number of pairs. However, contrary to the results for the entropy difference, the number of rejections was significantly higher in the adapted condition than in the control condition for bin sizes 80 ms, 100 ms and 200 ms (paired <italic>t</italic>-test, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e204" xlink:type="simple"/></inline-formula>). For the adaptation condition, there was much more evidence for higher-order correlations that was reflected in the mutual information than in the entropy. This indicates that for many bin sizes divergences from the maximum entropy distribution were more stimulus specific after adaptation even though they were smaller.</p>
        <p>For the entropy difference as the divergence measure, rejection rates do not vary with stimulus orientation. For both the entropy difference and the mutual information difference, the data suggest that rejection rates tend to increase towards the end of the trial (data not shown).</p>
        <p><xref ref-type="fig" rid="pcbi-1002539-g007">Figure 7 A</xref> shows the overall firing rates of the individual neurons in the control and the adaptation conditions. The data suggest the existence of a high firing rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e205" xlink:type="simple"/></inline-formula>) and a low firing rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e206" xlink:type="simple"/></inline-formula>) population. <xref ref-type="fig" rid="pcbi-1002539-g007">Figures 7 B, C</xref> show the results of the maximum entropy test for the difference in the mutual information as a divergence measure separately for both populations. The figures show that the rejected pairs were significantly higher for the high firing rate population for bin sizes <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e207" xlink:type="simple"/></inline-formula> in both the control and the adaptation condition (paired <italic>t</italic>-test, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e208" xlink:type="simple"/></inline-formula>). Moreover, the rejection rates in this subpopulation were significantly higher in the adaptation condition than in the control condition for these bin sizes (paired <italic>t</italic>-test, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e209" xlink:type="simple"/></inline-formula>), which did not hold for the low firing rate population.</p>
        <fig id="pcbi-1002539-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002539.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Subpopulation analysis of the data that are presented in <xref ref-type="fig" rid="pcbi-1002539-g006">Figure 6</xref> C.</title>
            <p>(A) Overall firing rates of the 11 neurons in the data set from <xref ref-type="fig" rid="pcbi-1002539-g006">Figure 6</xref> for the control and adaptation conditions. The rates were averaged over all stimuli. ( B) Fraction of neuronal pairs rejected by the maximum entropy test with the mutual information difference as the divergence measure (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e210" xlink:type="simple"/></inline-formula>) for the high firing rate (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e211" xlink:type="simple"/></inline-formula>, cf. A) population of neurons. ( C) Same as in B but for the low firing rate population (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e212" xlink:type="simple"/></inline-formula>). Rejection rates were averaged over all neuron pairs and all time bins. Simulated annealing <xref ref-type="bibr" rid="pcbi.1002539-Kirkpatrick1">[45]</xref> was applied to maximize the <italic>p</italic>-value (cf. <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). Number <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e213" xlink:type="simple"/></inline-formula> of Monte Carlo samples was 1000. The false discovery rate of the rejections was corrected using the Benjamini-Hochberg procedure <xref ref-type="bibr" rid="pcbi.1002539-Benjamini1">[35]</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.g007" xlink:type="simple"/>
        </fig>
        <p>We explicitly estimated the mutual information between the bivariate spike counts of neuronal pairs and the stimulus set using (1) the best fitting second-order maximum entropy distribution, which neglects higher-order correlations and (2) a non-parametric method involving a bias correction for small sample sizes, where higher-order correlations are included in principle (cf. Section ‚ÄúSubpopulation Structure of Recorded Neurons‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). The relation between these estimates can roughly illustrate the order of impact of higher-order correlations even though the number of samples is insufficient to obtain unbiased mutual information estimates. Results show that when higher-order correlations are taken into account a bimodal distribution of mutual information values emerges, with modes coinciding with the low firing rate (for small mutual information values) and high firing rate (for high mutual information values) population (cf. Figure S5 in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>).</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>We devised a maximum entropy test that assesses higher-order correlations in terms of an information theoretic analysis. The biggest advantage of the method is the small number of samples that is required. We demonstrate that the test can be useful even when the number of samples is on the order of 50. Our approach has the advantage of being able to test for higher-order correlations, which are relevant for a particular analysis task, rather than in terms of a general divergence measure, which might not be of interest at all.</p>
      <p>A divergence measure that is based on mutual information can be applied to quantify evidence for higher-order correlations that is reflected in mutual information. Suppose two stimuli <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e214" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e215" xlink:type="simple"/></inline-formula> are present with conditional bivariate spike count distributions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e216" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e217" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e218" xlink:type="simple"/></inline-formula> follow a maximum entropy distribution and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e219" xlink:type="simple"/></inline-formula> follow the mixture distribution of Section ‚ÄúValidation of the Test on Artificial Data‚Äù with the same correlation coefficient. Then the mutual information is 0 for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e220" xlink:type="simple"/></inline-formula> and increases with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e221" xlink:type="simple"/></inline-formula>. The mutual information difference can therefore quantify divergence in terms of a measure of interest.</p>
      <p>The test that we presented is restricted to neuronal pairs. Therefore, multivariate higher-order correlations that are not detectable in bivariate distributions would be overseen. The maximum entropy hypothesis would be accepted in spite of such correlations being present. The number of rejections, however, could only increase but not decrease. In principle, a theoretical generalization of the test to an arbitrary number of neurons is straightforward. Practical computation time and memory requirements, however, increase exponentially with the number of neurons. It is not clear whether higher-order correlations that are not detectable in bivariate distributions are particularly important for neural systems. From a generative viewpoint, however, this condition would impose strong constraints on the statistics of the neural responses: on the order of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e222" xlink:type="simple"/></inline-formula> constraints would be necessary for a population of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002539.e223" xlink:type="simple"/></inline-formula> neurons to keep the neurons linearly uncorrelated while higher-order correlations would need to be stimulus specific.</p>
      <p>If the number of samples is on the order of 50 and higher-order correlations are present then mutual information estimations are unreliable. The test with the mutual information difference as the divergence measure includes the calculation of mutual information values. This is acceptable for several reasons: (1) The goal of the method is not to yield an estimate of the mutual information, but rather to quantify the divergence of the data compared to the reference family of distributions. (2) Simple bias correction techniques that depend on the number of samples (e.g. the Miller-Madow bias corrections) are implicitly present in the test because the divergence is the difference of the mutual information and therefore, any additive biases vanish. (3) The test searches in the parameter space of maximum entropy distributions and calculates their mutual information values. Under the null hypothesis this means that the true mutual information is among those that we consider. Since we use the worst case <italic>p</italic>-value, the test is reliable even if the number of samples is insufficient to estimate mutual information.</p>
      <p>We cannot make sure that the spike counts are actually Poisson distributed even though we applied several statistical tests to ascertain that this assumption is not unreasonable. We explored the impact of deviations from the Poisson distribution by applying the maximum entropy test to gamma processes. Naturally, the test rejects the maximum entropy distribution with Poisson marginals if the deviations of the gamma process from the Poisson process are too strong. However, we also compared the rejection rates of the maximum entropy test to the rejection rates of the Kolmogorov-Smirnov test. The Kolmogorov-Smirnov test turned out to be more sensitive to these deviations than the proposed maximum entropy test. This suggests that the V1 rejection rates of the Kolmogorov-Smirnov test should be much greater if the Poisson assumption was the reason for the strong rejection rates of the maximum entropy test. In general, one could assume more flexible marginals if the Poisson hypothesis must be rejected. We propose the negative binomial and the binomial distribution as alternatives in the supporting information (cf. Section ‚ÄúAlternative Marginal Distributions‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>).</p>
      <p>It might come as a surprise that the application of the Poisson goodness-of-fit tests to the V1 data yielded so little Poisson rejections, given that spike trains are non-stationary and typically have strong autocorrelations. We emphasize that we applied separate tests for every bin: spike counts from subsequent bins of a single trial were not modeled by a single distribution but by separate distributions. We make no assumptions about the relation of these subsequent models. The test, therefore, cannot detect any higher-order correlations that have a time lag beyond the length of the bin. Moreover, we assume that spike counts of a given bin can be described by a single stationary distribution across trials. In principle, additional tests could be applied to bin pairs with a fixed lag in order to detect higher-order correlations that have a time lag beyond the length of the bin.</p>
      <p>Spike counts are calculated within subsequent bins of a given length. It is well known that higher-order correlations are of no relevance if the bin size is very small. In this case the marginals are essentially binary: either there was a spike present in the bin or not. The bivariate distribution table of these spike counts is characterized by only three probabilities (the table has four values but the last one is fixed by the constraint that the probabilities sum to one). Thus the correlation coefficient is sufficient to characterize the dependency structure for very small bin sizes. For larger bin sizes higher-order correlations can be important, however, we assume that the bin size is already fixed before the test is applied. The goal of our analysis is not to find a parameter regime where higher-order correlations are necessary. Instead, there was a separate hypothesis inference for each bin size. As such, we applied a multiple inference procedure to analyze the data that were recorded from V1. Multiple inference procedures do not need a multiple testing correction <xref ref-type="bibr" rid="pcbi.1002539-Benjamini2">[40]</xref>. Instead, we applied a multiple testing correction over all pairs and all time bins for a given bin size but not over all bin sizes. If the goal of the analysis would have been to identify a particular bin size for which higher-order correlations do matter or if the test is applied in an exploratory study over multiple bin sizes to determine whether more data should be collected, then the multiple testing correction should as well be applied over all bin sizes.</p>
      <p>Previously, it was shown that orientation adaptation in cat V1 neurons results in shifts of the preferred orientation <xref ref-type="bibr" rid="pcbi.1002539-Dragoi1">[34]</xref> and in changes of the distribution of linear correlations between neuronal pairs <xref ref-type="bibr" rid="pcbi.1002539-Gutnisky1">[1]</xref>. In our ‚Äúproof of principle‚Äù example we investigated whether higher-order correlations change and whether they have a significant impact on information theoretic quantities. Taken together, the results of our analysis provide evidence for condition-dependent influences of higher-order correlations on the estimation of entropy and mutual information for many spike count bin sizes. Furthermore, our analysis suggests the existence of different subpopulations of neurons with a different higher-order correlation structure.</p>
      <p>The purpose of the test is to show whether higher-order correlations must be taken into account when a particular kind of analysis is planned for an experimental data set and not to provide in depth insight into the structure of these associations. The advantage of the test lies in the small numbers of samples it needs to detect the presence of analysis-relevant higher-order correlations, hence it can be applied to a smaller exploratory study. If the maximum entropy hypothesis is rejected, one learns two things: (1) One should not perform the planned analysis on the given data and (2) one should redo the experiment and increase the number of data, such that higher-order associations can be reliably estimated. In the supporting information, we briefly describe generalized linear models (GLMs) as one particular option for modeling higher-order associations (cf. Section ‚ÄúModeling Higher-order Correlations‚Äù in <xref ref-type="supplementary-material" rid="pcbi.1002539.s001">Text S1</xref>). For a more detailed description of GLMs we refer the reader to previous studies <xref ref-type="bibr" rid="pcbi.1002539-Kass1">[41]</xref>‚Äì<xref ref-type="bibr" rid="pcbi.1002539-Pillow2">[44]</xref>.</p>
      <p>The new test provides a convenient way to investigate the sufficiency of second-order dependency models, and is especially useful when the number of samples per condition is small - a typical situation in electrophysiology. The application of the test to data recorded in primary visual cortex provides a proof of principle for the usefulness of our method.</p>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002539.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002539.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Supporting text providing a detailed description of the optimization procedure, of Poisson goodness-of-fit tests, of alternative marginal distributions and divergence measures. It also provides a discussion of modeling higher-order correlations and of the subpopulation structure of the recorded neurons.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Nicolas Neubauer for visualizing the subpopulation structure and the anonymous reviewers for their constructive comments.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002539-Gutnisky1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gutnisky</surname><given-names>DA</given-names></name><name name-style="western"><surname>Dragoi</surname><given-names>V</given-names></name></person-group>             <year>2008</year>             <article-title>Adaptive coding of visual information in neural populations.</article-title>             <source>Nature</source>             <volume>452</volume>             <fpage>220</fpage>             <lpage>224</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Kohn1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kohn</surname><given-names>A</given-names></name><name name-style="western"><surname>Smith</surname><given-names>MA</given-names></name></person-group>             <year>2005</year>             <article-title>Stimulus dependence of neuronal correlation in primary visual cortex of the macaque.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>3661</fpage>             <lpage>3673</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Bair1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bair</surname><given-names>W</given-names></name><name name-style="western"><surname>Zohary</surname><given-names>E</given-names></name><name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name></person-group>             <year>2001</year>             <article-title>Correlated firing in macaque visual area MT: time scales and relationship to behavior.</article-title>             <source>J Neurosci</source>             <volume>21</volume>             <fpage>1676</fpage>             <lpage>1697</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Ecker1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ecker</surname><given-names>AS</given-names></name><name name-style="western"><surname>Berens</surname><given-names>P</given-names></name><name name-style="western"><surname>Keliris</surname><given-names>GA</given-names></name><name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name><name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Decorrelated neuronal firing in cortical microcircuits.</article-title>             <source>Science</source>             <volume>327</volume>             <fpage>584</fpage>             <lpage>587</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Averbeck1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name><name name-style="western"><surname>Lee</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>Neural noise and movement-related codes in the macaque supplementary motor area.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>7630</fpage>             <lpage>7641</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Sompolinsky1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name><name name-style="western"><surname>Yoon</surname><given-names>H</given-names></name><name name-style="western"><surname>Kang</surname><given-names>K</given-names></name><name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name></person-group>             <year>2001</year>             <article-title>Population coding in neuronal systems with correlated noise.</article-title>             <source>Phys Rev E</source>             <volume>64</volume>             <fpage>051904</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Macke1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Macke</surname><given-names>JH</given-names></name><name name-style="western"><surname>Berens</surname><given-names>P</given-names></name><name name-style="western"><surname>Ecker</surname><given-names>AS</given-names></name><name name-style="western"><surname>Tolias</surname><given-names>AS</given-names></name><name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name></person-group>             <year>2009</year>             <article-title>Generating spike trains with specified correlation coefficients.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>397</fpage>             <lpage>423</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Shamir1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shamir</surname><given-names>M</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>2006</year>             <article-title>Implications of neuronal diversity on population coding.</article-title>             <source>Neural Comput</source>             <volume>18</volume>             <fpage>1951</fpage>             <lpage>1986</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Series1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Series</surname><given-names>P</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2004</year>             <article-title>Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations.</article-title>             <source>Nat Neurosci</source>             <volume>7</volume>             <fpage>1129</fpage>             <lpage>1135</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Nirenberg1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name></person-group>             <year>2003</year>             <article-title>Decoding neuronal spike trains: how important are correlations?</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>100</volume>             <fpage>7348</fpage>             <lpage>7353</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Pola1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pola</surname><given-names>G</given-names></name><name name-style="western"><surname>Thiele</surname><given-names>A</given-names></name><name name-style="western"><surname>Hoffmann</surname><given-names>KP</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name></person-group>             <year>2003</year>             <article-title>An exact method to quantify the information transmitted by different mechanisms of correlational coding.</article-title>             <source>Network</source>             <volume>14</volume>             <fpage>35</fpage>             <lpage>60</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Abbott1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name></person-group>             <year>1999</year>             <article-title>The effect of correlated variability on the accuracy of a population code.</article-title>             <source>Neural Comput</source>             <volume>11</volume>             <fpage>91</fpage>             <lpage>101</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Staude1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Staude</surname><given-names>B</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name><name name-style="western"><surname>Gr√ºn</surname><given-names>S</given-names></name></person-group>             <year>2010</year>             <article-title>Cubic: cumulant based inference of higher-order correlations.</article-title>             <source>J Comput Neurosci</source>             <volume>29</volume>             <fpage>327</fpage>             <lpage>350</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Staude2">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Staude</surname><given-names>B</given-names></name><name name-style="western"><surname>Gr√ºn</surname><given-names>S</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name></person-group>             <year>2010</year>             <article-title>Higher-order correlations in non-stationary parallel spike trains: statistical modeling and inference.</article-title>             <source>Front Comput Neurosci</source>             <volume>4</volume>             <fpage>p11:16</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Averbeck2">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2006</year>             <article-title>Neural correlations, population coding and computation.</article-title>             <source>Nat Rev Neurosci</source>             <volume>7</volume>             <fpage>358</fpage>             <lpage>366</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Panzeri1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name><name name-style="western"><surname>Senatore</surname><given-names>R</given-names></name><name name-style="western"><surname>Montemurro</surname><given-names>MA</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>RS</given-names></name></person-group>             <year>2007</year>             <article-title>Correcting for the sampling bias problem in spike train information measures.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>1064</fpage>             <lpage>1072</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Montemurro1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Montemurro</surname><given-names>MA</given-names></name><name name-style="western"><surname>Senatore</surname><given-names>R</given-names></name><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name></person-group>             <year>2007</year>             <article-title>Tight data-robust bounds to mutual information combining shuffling and model selection techniques.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2913</fpage>             <lpage>2957</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Paninski1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name></person-group>             <year>2003</year>             <article-title>Estimation of entropy and mutual information.</article-title>             <source>Neural Comput</source>             <volume>15</volume>             <fpage>1191</fpage>             <lpage>1253</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Scaglione1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Scaglione</surname><given-names>A</given-names></name><name name-style="western"><surname>Moxon</surname><given-names>KA</given-names></name><name name-style="western"><surname>Foffani</surname><given-names>G</given-names></name></person-group>             <year>2010</year>             <article-title>General Poisson exact breakdown of the mutual information to study the role of correlations in populations of neurons.</article-title>             <source>Neural Comput</source>             <volume>22</volume>             <fpage>1445</fpage>             <lpage>1467</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Scaglione2">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Scaglione</surname><given-names>A</given-names></name><name name-style="western"><surname>Foffani</surname><given-names>G</given-names></name><name name-style="western"><surname>Scannella</surname><given-names>G</given-names></name><name name-style="western"><surname>Cerutti</surname><given-names>S</given-names></name><name name-style="western"><surname>Moxon</surname><given-names>KA</given-names></name></person-group>             <year>2008</year>             <article-title>Mutual information expansion for studying the role of correlations in population codes.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>2662</fpage>             <lpage>2695</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Ohiorhenuan1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ohiorhenuan</surname><given-names>IE</given-names></name><name name-style="western"><surname>Mechler</surname><given-names>F</given-names></name><name name-style="western"><surname>Purpura</surname><given-names>KP</given-names></name><name name-style="western"><surname>Schmid</surname><given-names>AM</given-names></name><name name-style="western"><surname>Hu1</surname><given-names>Q</given-names></name><etal/></person-group>             <year>2010</year>             <article-title>Sparse coding and high-order correlations in fine-scale cortical networks.</article-title>             <source>Nature</source>             <volume>466</volume>             <fpage>617</fpage>             <lpage>621</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Montani1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Montani</surname><given-names>F</given-names></name><name name-style="western"><surname>Ince</surname><given-names>RAA</given-names></name><name name-style="western"><surname>Senatore</surname><given-names>R</given-names></name><name name-style="western"><surname>Arabzadeh</surname><given-names>E</given-names></name><name name-style="western"><surname>Diamond</surname><given-names>ME</given-names></name><etal/></person-group>             <year>2009</year>             <article-title>The impact of high-order interactions on the rate of synchronous discharge and information transmission in somatosensory cortex.</article-title>             <source>Philos Transact A Math Phys Eng Sci</source>             <volume>367</volume>             <fpage>3297</fpage>             <lpage>3310</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Onken1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Onken</surname><given-names>A</given-names></name><name name-style="western"><surname>Gr√ºnew√§lder</surname><given-names>S</given-names></name><name name-style="western"><surname>Munk</surname><given-names>MHJ</given-names></name><name name-style="western"><surname>Obermayer</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>Analyzing short-term noise dependencies of spike-counts in macaque prefrontal cortex using copulas and the flashlight transformation.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000577</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Amari1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>Information geometry on hierarchy of probability distributions.</article-title>             <source>IEEE T Inform Theory</source>             <volume>47</volume>             <fpage>1701</fpage>             <lpage>1711</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Roudi1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tyrcha</surname><given-names>J</given-names></name><name name-style="western"><surname>Hertz</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Ising model for neural data: Model quality and approximate methods for extracting functional connectivity.</article-title>             <source>Phys Rev E</source>             <volume>79</volume>             <fpage>e051915</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Roudi2">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name><name name-style="western"><surname>Aurell</surname><given-names>E</given-names></name><name name-style="western"><surname>Hertz</surname><given-names>JA</given-names></name></person-group>             <year>2009</year>             <article-title>Statistical physics of pairwise probability models.</article-title>             <source>Front Comput Neurosci</source>             <volume>3</volume>             <fpage>22</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Roudi3">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name><name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name></person-group>             <year>2009</year>             <article-title>Pairwise maximum entropy models for studying large biological systems: when they can work and when they can't.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000380</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Schneidman1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name><name name-style="western"><surname>II</surname><given-names>MJB</given-names></name><name name-style="western"><surname>Segev</surname><given-names>R</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>             <year>2006</year>             <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population.</article-title>             <source>Nature</source>             <volume>440</volume>             <fpage>1007</fpage>             <lpage>1012</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Shlens1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Field</surname><given-names>GD</given-names></name><name name-style="western"><surname>Gauthier</surname><given-names>JL</given-names></name><name name-style="western"><surname>Grivich</surname><given-names>MI</given-names></name><name name-style="western"><surname>Petrusca</surname><given-names>D</given-names></name><etal/></person-group>             <year>2006</year>             <article-title>The structure of multineuron firing patterns in primate retina.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>2006</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Jaynes1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name></person-group>             <year>1957</year>             <article-title>Information theory and statistical mechanics.</article-title>             <source>Phys Rev</source>             <volume>106</volume>             <fpage>620</fpage>             <lpage>630</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Onken2">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Onken</surname><given-names>A</given-names></name><name name-style="western"><surname>Obermayer</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>A Frank mixture copula family for modeling higher-order correlations of neural spike counts.</article-title>             <source>J Phys Conf Ser</source>             <volume>197</volume>             <fpage>012019</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Pasha1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pasha</surname><given-names>E</given-names></name><name name-style="western"><surname>Mansoury</surname><given-names>S</given-names></name></person-group>             <year>2008</year>             <article-title>Determination of maximum entropy multivariate probability distribution under some constraints.</article-title>             <source>Appl Math Sci</source>             <volume>2</volume>             <fpage>2843</fpage>             <lpage>2849</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Dufour1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dufour</surname><given-names>JM</given-names></name></person-group>             <year>2006</year>             <article-title>Monte Carlo tests with nuisance parameters: a general approach to finite-sample inference and nonstandard asymptotics.</article-title>             <source>J Econometrics</source>             <volume>133</volume>             <fpage>443</fpage>             <lpage>477</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Dragoi1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dragoi</surname><given-names>V</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>J</given-names></name><name name-style="western"><surname>Sur</surname><given-names>M</given-names></name></person-group>             <year>2000</year>             <article-title>Adaptation-induced plasticity of orientation tuning in adult visual cortex.</article-title>             <source>Neuron</source>             <volume>28</volume>             <fpage>287</fpage>             <lpage>298</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Benjamini1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Benjamini</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yekutieli</surname><given-names>D</given-names></name></person-group>             <year>2001</year>             <article-title>The control of the false discovery rate in multiple testing under dependency.</article-title>             <source>Ann Stat</source>             <volume>29</volume>             <fpage>1165</fpage>             <lpage>1188</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Nelsen1">
        <label>36</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nelsen</surname><given-names>RB</given-names></name></person-group>             <year>2006</year>             <source>An Introduction to Copulas. 2nd edition</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Wald1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wald</surname><given-names>A</given-names></name></person-group>             <year>1943</year>             <article-title>Tests of statistical hypotheses concerning several parameters when the number of observations is large.</article-title>             <source>T Am Math Soc</source>             <volume>54</volume>             <fpage>426</fpage>             <lpage>482</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Tetzlaff1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tetzlaff</surname><given-names>T</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name><name name-style="western"><surname>Stark</surname><given-names>E</given-names></name><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name><name name-style="western"><surname>Aertsen</surname><given-names>A</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Dependence of neuronal correlations on filter characteristics and marginal spike train statistics.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>2133</fpage>             <lpage>2184</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Haslinger1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haslinger</surname><given-names>R</given-names></name><name name-style="western"><surname>Pipa</surname><given-names>G</given-names></name><name name-style="western"><surname>Brown</surname><given-names>E</given-names></name></person-group>             <year>2010</year>             <article-title>Discrete time rescaling theorem: determining goodness of fit for discrete time statistical models of neural spiking.</article-title>             <source>Neural Comput</source>             <volume>22</volume>             <fpage>2477</fpage>             <lpage>2506</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Benjamini2">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Benjamini</surname><given-names>Y</given-names></name><name name-style="western"><surname>Hochberg</surname><given-names>Y</given-names></name></person-group>             <year>1995</year>             <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing.</article-title>             <source>J R Statist Soc B</source>             <volume>57</volume>             <fpage>289</fpage>             <lpage>300</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Kass1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kass</surname><given-names>RE</given-names></name><name name-style="western"><surname>Ventura</surname><given-names>V</given-names></name></person-group>             <year>2001</year>             <article-title>A spike-train probability model.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>1713</fpage>             <lpage>1720</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Truccolo1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name><name name-style="western"><surname>Eden</surname><given-names>UT</given-names></name><name name-style="western"><surname>Fellows</surname><given-names>MR</given-names></name><name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name><name name-style="western"><surname>Brown</surname><given-names>EN</given-names></name></person-group>             <year>2005</year>             <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects.</article-title>             <source>J Neurophysiol</source>             <volume>93</volume>             <fpage>1074</fpage>             <lpage>1089</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Pillow1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name><name name-style="western"><surname>Sher</surname><given-names>A</given-names></name><name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name><etal/></person-group>             <year>2008</year>             <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population.</article-title>             <source>Nature</source>             <volume>454</volume>             <fpage>995</fpage>             <lpage>999</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Pillow2">
        <label>44</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pillow</surname><given-names>J</given-names></name><name name-style="western"><surname>Latham</surname><given-names>P</given-names></name></person-group>             <year>2008</year>             <article-title>Neural characterization in partially observed populations of spiking neurons.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Platt</surname><given-names>JC</given-names></name><name name-style="western"><surname>Koller</surname><given-names>D</given-names></name><name name-style="western"><surname>Singer</surname><given-names>Y</given-names></name><name name-style="western"><surname>Roweis</surname><given-names>S</given-names></name></person-group>             <source>Advances in Neural Information Processing Systems</source>             <publisher-loc>Cambridge, MA</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>1161</fpage>             <lpage>1168</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002539-Kirkpatrick1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kirkpatrick</surname><given-names>S</given-names></name><name name-style="western"><surname>Gelatt</surname><given-names>CD</given-names></name><name name-style="western"><surname>Vecchi</surname><given-names>MP</given-names></name></person-group>             <year>1983</year>             <article-title>Optimization by simulated annealing.</article-title>             <source>Science</source>             <volume>220</volume>             <fpage>671</fpage>             <lpage>680</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>