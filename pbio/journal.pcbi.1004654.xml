<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00767</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004654</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Consciousness</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Consciousness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information theory</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Consciousness</subject><subj-group><subject>Theories of consciousness</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Consciousness</subject><subj-group><subject>Theories of consciousness</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Mammals</subject><subj-group><subject>Primates</subject><subj-group><subject>Monkeys</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Measuring Integrated Information from the Decoding Perspective</article-title>
<alt-title alt-title-type="running-head">Measuring Integrated Information from the Decoding Perspective</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Oizumi</surname> <given-names>Masafumi</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Amari</surname> <given-names>Shun-ichi</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Yanagawa</surname> <given-names>Toru</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Fujii</surname> <given-names>Naotaka</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Tsuchiya</surname> <given-names>Naotsugu</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>RIKEN Brain Science Institute, Wako, Saitama, Japan</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>School of Psychological Sciences, Faculty of Biomedical and Psychological Sciences, Monash University, Clayton, Victoria, Australia</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Japan Science and Technology Agency, Kawaguchi, Saitama, Japan</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Monash Institute of Cognitive and Clinical Neurosciences, Monash University, Clayton, Victoria, Australia</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Polani</surname> <given-names>Daniel</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Hertfordshire, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MO SiA NT. Performed the experiments: MO TY NF. Analyzed the data: MO. Contributed reagents/materials/analysis tools: MO SiA NT. Wrote the paper: MO NT.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">oizumi@brain.riken.jp</email> (MO); <email xlink:type="simple">naotsugu.tsuchiya@monash.edu</email> (NT)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>1</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>21</day>
<month>1</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>1</issue>
<elocation-id>e1004654</elocation-id>
<history>
<date date-type="received">
<day>9</day>
<month>5</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>11</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Oizumi et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004654"/>
<abstract>
<p>Accumulating evidence indicates that the capacity to integrate information in the brain is a prerequisite for consciousness. Integrated Information Theory (IIT) of consciousness provides a mathematical approach to quantifying the information integrated in a system, called integrated information, Φ. Integrated information is defined theoretically as the amount of information a system generates as a whole, above and beyond the amount of information its parts independently generate. IIT predicts that the amount of integrated information in the brain should reflect levels of consciousness. Empirical evaluation of this theory requires computing integrated information from neural data acquired from experiments, although difficulties with using the original measure Φ precludes such computations. Although some practical measures have been previously proposed, we found that these measures fail to satisfy the theoretical requirements as a measure of integrated information. Measures of integrated information should satisfy the lower and upper bounds as follows: The lower bound of integrated information should be 0 and is equal to 0 when the system does not generate information (no information) or when the system comprises independent parts (no integration). The upper bound of integrated information is the amount of information generated by the whole system. Here we derive the novel practical measure Φ* by introducing a concept of mismatched decoding developed from information theory. We show that Φ* is properly bounded from below and above, as required, as a measure of integrated information. We derive the analytical expression of Φ* under the Gaussian assumption, which makes it readily applicable to experimental data. Our novel measure Φ* can generally be used as a measure of integrated information in research on consciousness, and also as a tool for network analysis on diverse areas of biology.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Integrated Information Theory (IIT) of consciousness attracts scientists who investigate consciousness owing to its explanatory and predictive powers for understanding the neural properties of consciousness. IIT predicts that the levels of consciousness are related to the quantity of information integrated in the brain, which is called integrated information Φ. Integrated information measures excess information generated by a system as a whole above and beyond the amount of information independently generated by its parts. Although IIT predictions are indirectly supported by numerous experiments, validation is required through quantifying integrated information directly from experimental neural data. Practical difficulties account for the absence of direct, quantitative support. To resolve these difficulties, several practical measures of integrated information have been proposed. However, we found that these measures do not satisfy the theoretical requirements of integrated information: First, integrated information should not be below 0; and second, integrated information should not exceed the quantity of information generated by the whole system. Here, we propose a novel practical measure of integrated information, designated as Φ* that satisfies these theoretical requirements by introducing the concept of mismatched decoding developed from information theory. Φ* creates the possibility of empirical and quantitative validations of IIT to gain novel insights into the neural basis of consciousness.</p>
</abstract>
<funding-group>
<funding-statement>MO was supported by a Grant-in-Aid for Young Scientists (B) from the Ministry of Education, Culture, Sports, Science, and Technology of Japan (26870860) (<ext-link ext-link-type="uri" xlink:href="https://www.jsps.go.jp/english/" xlink:type="simple">https://www.jsps.go.jp/english/</ext-link>). NT was supported by Precursory Research for Embryonic Science and Technology from Japan Science and Technology Agency (3630) (<ext-link ext-link-type="uri" xlink:href="http://www.jst.go.jp/EN/" xlink:type="simple">http://www.jst.go.jp/EN/</ext-link>), Future Fellowship (FT120100619) and Discovery Project (DP130100194) from Australian Research Council (<ext-link ext-link-type="uri" xlink:href="http://www.arc.gov.au/" xlink:type="simple">http://www.arc.gov.au/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The details on the recording apparatus and the dataset is available online (<ext-link ext-link-type="uri" xlink:href="http://neurotycho.org" xlink:type="simple">http://neurotycho.org</ext-link>). 
</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Although its neurobiological basis remains unclear, consciousness may be related to certain aspects of information processing [<xref ref-type="bibr" rid="pcbi.1004654.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref002">2</xref>]. In particular, Integrated Information Theory of consciousness (IIT) developed by Tononi and colleagues [<xref ref-type="bibr" rid="pcbi.1004654.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref009">9</xref>] predicts that the amount of information integrated among the components of a system, called integrated information Φ, is related to the level of consciousness of the system. The level of consciousness in the brain varies from a very high level, as in full wakefulness, to a very low level, as in deeply anesthetized states or dreamless sleep. When consciousness changes from high to low, IIT predicts that the amount of integrated information changes from high to low, accordingly. This prediction is indirectly supported by recent neuroimaging experiments that combine noninvasive magnetic stimulation of the brain (transcranial magnetic stimulation, TMS) with electrophysiological recordings of stimulation-evoked activity (electroencephalography) [<xref ref-type="bibr" rid="pcbi.1004654.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref014">14</xref>]. Such evidence implies that if there is a practical method to estimate the amount of integrated information from neural activities, we may be able to measure levels of consciousness using integrated information.</p>
<p>IIT provides several versions of mathematical formulations to calculate integrated information [<xref ref-type="bibr" rid="pcbi.1004654.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref008">8</xref>]. Although the detailed mathematical formulations are different, the central philosophy of integrated information does not vary among different versions of IIT. Integrated information is mathematically defined as the amount of information generated by a system as a whole above and beyond the amount of information generated independently by its parts. If the parts are independent, no integrated information should exist.</p>
<p>Despite its potential importance, the empirical calculation of integrated information is difficult. For example, one difficulty involves making an assumption when integrated information is calculated according to the informational relationship between the past and present states of a system. The distribution of the past states is assumed to maximize entropy, which is called the maximum entropy distribution. The assumption of the maximum entropy distribution severely limits the applicability of the original integrated information measure Φ as indicated by [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>]. First, the concept of the maximum entropy distribution cannot be applied to a system that comprises elements whose states are continuous, because there is no unique maximum entropy distribution for continuous variables [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref016">16</xref>]. Second, information under the assumption of the maximum entropy distribution can be computed only when there is complete knowledge about the transition probability matrix that describes how the system transits between states. However, the transition probability matrix for actual neuronal systems is practically impossible to estimate.</p>
<p>To overcome these problems, Barrett and Seth [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] proposed using the empirical distribution estimated from experimental data, thereby removing the requirement to rely on the assumption of the maximum entropy distribution. Although we believe that their approach does lead to practical computation of integrated information, we found that their proposed measures based on the empirical distribution [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] do not satisfy key theoretical requirements as a measure of integrated information. Two theoretical requirements should be satisfied as a measure of integrated information. First, the amount of integrated information should not be negative. Second, the amount of integrated information should never exceed information generated by the whole system. These theoretical requirements, which are satisfied by the original measure Φ, are required so that a measure of integrated information is interpretable in accordance with the original philosophy of integrated information.</p>
<p>Here, we propose a novel practical measure of integrated information, Φ*, by introducing the concept of mismatched decoding developed from information theory [<xref ref-type="bibr" rid="pcbi.1004654.ref017">17</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref020">20</xref>]. Φ* represents the difference between “actual” and “hypothetical” mutual information between the past and present states of the system. The actual mutual information corresponds to the amount of information that can be extracted about the past states by knowing the present states (or vice versa) when the actual probability distribution of a system is used for decoding. In contrast, hypothetical mutual information corresponds to the amount of information that can be extracted about the past states by knowing the present states when the “mismatched” probability distribution is used for decoding where a system is partitioned into hypothetical independent parts. Decoding with a mismatched probability distribution is called mismatched decoding. Φ* quantifies the amount of loss of information caused by the mismatched decoding where interactions between the parts are ignored. We show here that Φ* satisfies the theoretical requirements as a measure of integrated information. Further, we derive the analytical expression of Φ* under the Gaussian assumption and make this measure feasible for practical computation. We also compute Φ* and the previously proposed measures in electrocorticogram (ECoG) data recorded in monkeys to demonstrate that the previous measures violate the theoretical requirements even in real brain recordings.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>While its central ideas are unchanged, IIT updated measures of integrated information. The original formulation, IIT 1.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref002">2</xref>], underwent major developments leading to IIT 2.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>] and the latest version IIT 3.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref008">8</xref>]. In the present study, we focus on the version in IIT 2.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>], because the measure of integrated information proposed in IIT 2.0 is simpler and more feasible to calculate compared with that in IIT 3.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref008">8</xref>].</p>
<p>Here, we briefly review the original measure of integrated information, Φ, in IIT 2.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>] and describe its limitations for practical application [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>]. From the concept of the original measure, we point out the lower and upper bounds that a measure of integrated information should satisfy. We introduce next two practical measures of integrated information, Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub>, proposed by [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] and show that Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> fail to satisfy the lower and upper bounds of integrated information. Finally, we derive a novel measure of integrated information, Φ*, from the decoding perspective, which is properly bounded from below and above.</p>
<sec id="sec003">
<title>Intrinsic information and extrinsic information</title>
<p>In IIT, information refers to intrinsic information as opposed to extrinsic information (See <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref> for details). Intrinsic information is quantified from the intrinsic perspective of a system itself and only depends on internal variables of the system. On the other hand, extrinsic information is quantified from the extrinsic perspective of an external observer and depends on external variables. For example, in neuroscience, extrinsic information is quantified as mutual information between neural states <italic>X</italic> and external stimuli <italic>S</italic>, <italic>I</italic>(<italic>X</italic>;<italic>S</italic>) [<xref ref-type="bibr" rid="pcbi.1004654.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref024">24</xref>]. In contrast, intrinsic information can be quantified by the mutual information between the past states <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> and the present states <italic>X</italic><sup><italic>t</italic></sup> of the system, <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>). The mutual information, <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), is expressed by
<disp-formula id="pcbi.1004654.e001"><alternatives><graphic id="pcbi.1004654.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>H</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is the entropy of the past states and <italic>H</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>|<italic>X</italic><sup><italic>t</italic></sup>) is the conditional entropy of the past states given the present states. In IIT, the distribution of the past states is assumed to be the maximum entropy distribution so that the entropy of the past states is maximized, i.e., the past states are maximally uncertain. We can interpret that intrinsic information, <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), quantifies to what extent uncertainty of the past states can be reduced by knowing the present states from the system’s intrinsic point of view. IIT considers such quantity as the amount of information intrinsically generated by the system.</p>
</sec>
<sec id="sec004">
<title>Measure of integrated information with the maximum entropy distribution</title>
<p>Consider partitioning a system into <italic>m</italic> parts such as <italic>M</italic><sub>1</sub>, <italic>M</italic><sub>2</sub>, ⋯, and <italic>M</italic><sub><italic>m</italic></sub> and computing the quantity of information that is integrated across the <italic>m</italic> parts of a system. As detailed in <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref>, the measure of integrated information proposed in IIT 2.0 can be expressed as follows:
<disp-formula id="pcbi.1004654.e002"><alternatives><graphic id="pcbi.1004654.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where the superscript<sup>max</sup> indicates that the distribution of the past states is the maximum entropy distribution. The first term of <xref ref-type="disp-formula" rid="pcbi.1004654.e002">Eq 2</xref>, <italic>I</italic>(<sup>max</sup> <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), represents the mutual information between the past and present states in the whole system, and the second term represents the sum of the mutual information between the past and present states in the <italic>i</italic>-th part of the system <inline-formula id="pcbi.1004654.e003"><alternatives><graphic id="pcbi.1004654.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mi>I</mml:mi> <mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Thus, Φ, the difference between them, gives the information generated by the whole system above and beyond the information generated independently by its parts. If the parts are independent, no extra information is generated, and the integrated information is 0. We can rewrite <xref ref-type="disp-formula" rid="pcbi.1004654.e002">Eq 2</xref> in terms of entropy <italic>H</italic> as follows:
<disp-formula id="pcbi.1004654.e004"><alternatives><graphic id="pcbi.1004654.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mo>Φ</mml:mo> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
To derive the above expression, we use the fact that the entropy of the whole system <italic>H</italic>(<sup>max</sup> <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) equals the sum of the entropy of the subsystems <inline-formula id="pcbi.1004654.e005"><alternatives><graphic id="pcbi.1004654.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:msubsup> <mml:mi>H</mml:mi> <mml:mrow><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> when the maximum entropy distribution is assumed.</p>
</sec>
<sec id="sec005">
<title>Theoretical requirements as a measure of integrated information</title>
<p>To interpret a measure of integrated information as the “extra” information generated by a system as a whole above and beyond its parts, it should satisfy theoretical requirements, as follows: First, integrated information should not be negative because information independently generated by the parts should never exceed information generated by the whole. Integrated information should equal 0 when the amount of information generated by the whole system equals 0 (no information) or when the amount of information generated by the whole is equal to that generated by its parts (no integration). Second, integrated information should not exceed the amount of information generated by the whole system because the information generated by the parts should not be negative. In short, integrated information should be lower-bounded by 0 and upper-bounded by the information generated by the whole system.</p>
<p>One can check the original measure Φ satisfies the lower and upper bounds.
<disp-formula id="pcbi.1004654.e006"><alternatives><graphic id="pcbi.1004654.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>≤</mml:mo> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:msup><mml:mo>(</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msup> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
As shown in <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref>, Φ can be written as the Kullback-Leibler divergence. Thus, Φ is positive or equal to 0. Further, as can be seen from <xref ref-type="disp-formula" rid="pcbi.1004654.e002">Eq 2</xref>, the upper bound of Φ is the mutual information in the entire system, because the sum of mutual information in the parts is larger than or equal to 0.</p>
<sec id="sec006">
<title>Practical measures of integrated information with empirical distribution</title>
<p>The original measure Φ assumes the distribution of the past states to be the maximum entropy distribution, which limits the practical application of Φ for two reasons. First, the maximum entropy distribution can be applied only when the states of a system are discrete. If the states are represented by discrete variables, the maximum entropy distribution is the uniform distribution over all possible states of <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>. When the states of a system are described by continuous variables, the maximum entropy distribution cannot be uniquely defined [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref016">16</xref>]. Second, the transition probability matrix of a system, <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) must be known for all possible past states <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> for obtaining the mutual information <italic>I</italic>(<sup>max</sup> <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>). However, it is nearly impossible to estimate such a complete transition probability matrix experimentally in an actual neural system, because some states may not occur during a reasonable period of observation.</p>
<p>A simple remedy for the limitations of the original measure Φ is not to impose the maximum entropy distribution on the past states but instead to use the probability distributions obtained from empirical observations of the system. Barrett and Seth [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] adopted this strategy to derive two practical measures of integrated information from Eqs <xref ref-type="disp-formula" rid="pcbi.1004654.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1004654.e004">3</xref> by substituting the maximum entropy distribution with the empirical distribution as follows:
<disp-formula id="pcbi.1004654.e007"><alternatives><graphic id="pcbi.1004654.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>I</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
<disp-formula id="pcbi.1004654.e008"><alternatives><graphic id="pcbi.1004654.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>H</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
Note that Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> are not equal when the empirical distribution is used for the past states, because the entropy of the whole system <italic>H</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is not equal to the sum of the entropy of the subsystems, <inline-formula id="pcbi.1004654.e009"><alternatives><graphic id="pcbi.1004654.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:msub> <mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Φ<sub><italic>H</italic></sub> was also derived from a different perspective from IIT, i.e. the perspective of information geometry, as a measure of spatio-temporal interdependencies and is termed “stochastic interaction” [<xref ref-type="bibr" rid="pcbi.1004654.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref026">26</xref>].</p>
<p>Although these two measures appear as natural modifications of the original measure, they do not satisfy the theoretical requirements as a measure of integrated information. We discuss the problems of Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> in detail later.</p>
</sec>
<sec id="sec007">
<title>Integrated information measure based on mismatched decoding</title>
<p>Here, we propose an alternative practical measure of integrated information that satisfies the theoretical requirements which we call Φ* (phi star) (<xref ref-type="fig" rid="pcbi.1004654.g001">Fig 1</xref>). Φ*, which uses the empirical distribution, can be applied to actual neuronal recordings. Similar to Φ<sub><italic>I</italic></sub>, we will derive Φ* based on the original measure Φ in <xref ref-type="disp-formula" rid="pcbi.1004654.e002">Eq 2</xref> based on mutual information. Given the problem of Φ<sub><italic>I</italic></sub> in <xref ref-type="disp-formula" rid="pcbi.1004654.e007">Eq 5</xref>, we should refine the second term of <xref ref-type="disp-formula" rid="pcbi.1004654.e007">Eq 5</xref>, while the first term, the mutual information in the whole system, is unchanged. The second term should be a quantity that can be interpreted as information generated independently by the parts of a system and should be less than information generated by the system as a whole.</p>
<fig id="pcbi.1004654.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004654.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Integrated information based on the concept of mismatched decoding.</title>
<p>The figure shows a system with five neurons in which the arrows represent directed connectivity and the colors represent the states of the neurons (black: silence, white: firing, gray: unknown). The past states <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> are decoded given the present states <italic>X</italic><sup><italic>t</italic></sup>. The “true” conditional distribution <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is used for matched decoding, while a “false” conditional distribution <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is used for mismatched decoding where the parts of a system <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> are assumed independent. The amount of information about the past states that can be extracted from the present states using matched and mismatched decoding is quantified by the mutual information <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) and the “hypothetical” mutual information <italic>I</italic>*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) for mismatched decoding, respectively. In this framework, integrated information, Φ*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), is defined as the difference between <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) and <italic>I</italic>*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.g001" xlink:type="simple"/>
</fig>
<p>To derive a proper second term in <xref ref-type="disp-formula" rid="pcbi.1004654.e007">Eq 5</xref>, we interpret the mutual information from a decoding perspective and introduce the concept of “mismatched decoding”, which was developed by information theory [<xref ref-type="bibr" rid="pcbi.1004654.ref017">17</xref>] (see <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref> for details). Consider that the past states <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> are decoded given the present states <italic>X</italic><sup><italic>t</italic></sup>. From the decoding perspective, the mutual information can be interpreted as the maximum information about the past states that can be obtained knowing the present states. To extract the maximum information, the decoding must be performed optimally using the “true” conditional distribution,
<disp-formula id="pcbi.1004654.e010"><alternatives><graphic id="pcbi.1004654.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>m</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
Note that the expression on the right explicitly accounts for interactions among all the parts. The optimal decoding can be performed using the maximum likelihood estimation. In the above setting, the maximum likelihood estimation chooses the past state that maximizes <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) given a present state. Decoding that uses the true distribution, <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>), is called “matched decoding” because the probability distribution used for decoding matches the actual probability distribution.</p>
<p>Decoding that uses a “false” conditional distribution, <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>), is called “mismatched” decoding. To quantify integrated information, we consider specifically the mismatched decoding that uses the “partitioned” probability distribution <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>),
<disp-formula id="pcbi.1004654.e011"><alternatives><graphic id="pcbi.1004654.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:mi>q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula> 
where a system is partitioned into parts and the parts <italic>M</italic><sub><italic>i</italic></sub> are assumed to be independent. <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is the product of the conditional probability distribution in each part <inline-formula id="pcbi.1004654.e012"><alternatives><graphic id="pcbi.1004654.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The distribution, <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>), is “mismatched” with the actual probability distribution, because parts are generally not independent in reality. As is matched decoding, mismatched decoding is also performed using the maximum likelihood estimation, wherein the past state that maximizes <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is selected. The amount of information obtained from mismatched decoding is necessarily degraded compared with that obtained from matched decoding. The best decoding performance can be achieved only when matched decoding is used with the actual probability distribution <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>).</p>
<p>We consider the amount of information that can be obtained from mismatched decoding, <italic>I</italic>*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), as a proper second term of <xref ref-type="disp-formula" rid="pcbi.1004654.e007">Eq 5</xref> (see <xref ref-type="sec" rid="sec014">Methods</xref> for the mathematical expression of <italic>I</italic>*). The difference between <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) and <italic>I</italic>*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) provides a new practical measure of integrated information (<xref ref-type="fig" rid="pcbi.1004654.g001">Fig 1</xref>),
<disp-formula id="pcbi.1004654.e013"><alternatives><graphic id="pcbi.1004654.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>Φ* quantifies the information loss caused by mismatched decoding where a system is partitioned into independent parts, and the interactions between the parts are ignored. Φ* satisfies the theoretical requirements, because <italic>I</italic>* is greater than or equal to 0 and is less than or equal to the information in the whole system <italic>I</italic>. Φ* is equivalent to the original measure Φ if the maximum entropy distribution is imposed on the past states instead of an empirical distribution (see <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref> for the proof). Thus, we can consider Φ* as a natural extension of Φ to the case when the empirical distribution is used.</p>
</sec>
</sec>
<sec id="sec008">
<title>Analytical computation of Φ* using Gaussian approximation</title>
<p>Although using an empirical distribution instead of the maximum entropy distribution makes integrated information feasible to calculate, it is still difficult to compute Φ* in a large system, because the summation over all possible states must be calculated. The number of all possible states grows exponentially with the size of the system and therefore, computational costs for computing Φ* also grow exponentially. Thus, for practical calculation of Φ*, we need to approximate Φ* in some way such as approximating the probability distribution of neural states using the Gaussian distribution [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>]. Φ* can be analytically computed using the Gaussian approximation (see <xref ref-type="sec" rid="sec014">Methods</xref>). The Gaussian approximation significantly reduces the computational costs and makes Φ* practically computable even in a large system.</p>
</sec>
<sec id="sec009">
<title>Theoretical requirements are not satisfied by previously proposed measures</title>
<p>In this section, by considering two extreme cases, we demonstrate that the previously proposed measures Φ<sub><italic>H</italic></sub> and Φ<sub><italic>I</italic></sub>[<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] do not satisfy either the lower or upper bound.</p>
<sec id="sec010">
<title>When there is no information</title>
<p>First, we consider the case where there is no information between the past and present states of a system, i.e. <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) = 0. In this case, integrated information should be 0. As expected, Φ* and Φ<sub><italic>I</italic></sub> are 0, because the amount of information for mismatched decoding, <italic>I</italic>*(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), and the mutual information in each part, <inline-formula id="pcbi.1004654.e014"><alternatives><graphic id="pcbi.1004654.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>I</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, are both 0 when <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) = 0;
<disp-formula id="pcbi.1004654.e015"><alternatives><graphic id="pcbi.1004654.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula> <disp-formula id="pcbi.1004654.e016"><alternatives><graphic id="pcbi.1004654.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>I</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
However, Φ<sub><italic>H</italic></sub> is not 0. Φ<sub><italic>H</italic></sub> can be written as
<disp-formula id="pcbi.1004654.e017"><alternatives><graphic id="pcbi.1004654.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>H</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>Φ<sub><italic>H</italic></sub> is not 0 even when the information <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) is 0 because Φ<sub><italic>H</italic></sub> is not based on the mutual information but on the conditional entropy (see <xref ref-type="disp-formula" rid="pcbi.1004654.e008">Eq 6</xref>). Therefore, Φ<sub><italic>H</italic></sub> does not necessarily reflect the amount of information in a system.</p>
<p>As a simple example that shows the above problem of Φ<sub><italic>H</italic></sub>, consider the following linear regression model,
<disp-formula id="pcbi.1004654.e018"><alternatives><graphic id="pcbi.1004654.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>E</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
Here, <italic>X</italic> is the state of units, <italic>A</italic> is a connectivity matrix, and <italic>E</italic><sup><italic>t</italic></sup> is multivariate Gaussian noise with zero mean and covariance Σ(<italic>E</italic>). <italic>E</italic><sup><italic>t</italic></sup> is uncorrelated over time. For simplicity, consider a system composed of two units (the following argument can be easily generalized to a system with more than two units). We set the connectivity matrix <italic>A</italic> and the covariance matrix of noise Σ(<italic>E</italic>) as follows:
<disp-formula id="pcbi.1004654.e019"><alternatives><graphic id="pcbi.1004654.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mrow><mml:mi>A</mml:mi> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi><mml:mspace width="1pt"/><mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
<disp-formula id="pcbi.1004654.e020"><alternatives><graphic id="pcbi.1004654.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mo>Σ</mml:mo><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mi>c</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>c</mml:mi></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where <italic>a</italic> and <italic>c</italic> are parameters that control the strengths of connections and noise correlation, respectively. We compute measures of integrated information using the above model. The time difference <italic>τ</italic> is set to 1. We assume that the prior distribution of the system is the steady state distribution, where the covariance of the past states, Σ(<italic>X</italic><sup><italic>t</italic>−1</sup>), and that of the present states, Σ(<italic>X</italic><sup><italic>t</italic></sup>), are equal, i.e. Σ(<italic>X</italic><sup><italic>t</italic>−1</sup>) = Σ(<italic>X</italic><sup><italic>t</italic></sup>) = Σ(<italic>X</italic>). The covariance of the steady state distribution Σ(<italic>X</italic>) can be calculated by taking the covariance of both sides of <xref ref-type="disp-formula" rid="pcbi.1004654.e018">Eq 13</xref>,
<disp-formula id="pcbi.1004654.e021"><alternatives><graphic id="pcbi.1004654.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mrow><mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>A</mml:mi> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi>A</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>We consider a case where the connection strength <italic>a</italic> is 0. <xref ref-type="fig" rid="pcbi.1004654.g002">Fig 2</xref> shows an exemplar time series when the strength of noise correlation <italic>c</italic> is 0.9. Because there are no connections, including self-connections within each unit, each unit has no information between the past and present states, i.e., <italic>I</italic><sub>1</sub> = <italic>I</italic><sub>2</sub> = 0. As can be seen from <xref ref-type="fig" rid="pcbi.1004654.g002">Fig 2</xref>, however, the two time series correlate at each moment because of the high noise correlation.</p>
<fig id="pcbi.1004654.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004654.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Exemplar time series when there is no information between the past and present states.</title>
<p>The connection strength <italic>a</italic> and the strength of noise correlation <italic>c</italic> are set to 0 and 0.9, respectively in the linear regression model (<xref ref-type="disp-formula" rid="pcbi.1004654.e018">Eq 13</xref>). <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub> represent the mutual information in units 1 and 2. Because there is no connection, there is no information between the past and present states of the system: <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub> are both 0. In this case, Φ* and Φ<sub><italic>I</italic></sub> are 0 as they should be, yet Φ<sub><italic>H</italic></sub> is positive.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.g002" xlink:type="simple"/>
</fig>
<p>We varied the degree of noise correlation, <italic>c</italic>, from 0 to 1 while keeping the connection strength <italic>a</italic> as 0 (<xref ref-type="fig" rid="pcbi.1004654.g003">Fig 3(A)</xref>). Φ* and Φ<sub><italic>I</italic></sub> stay 0 independent of noise correlation. However, an entropy-based measure, Φ<sub><italic>H</italic></sub>, increases monotonically with <italic>c</italic>, irrespective of the amount of information in the whole system (<xref ref-type="fig" rid="pcbi.1004654.g003">Fig 3(A)</xref>). As shown in <xref ref-type="disp-formula" rid="pcbi.1004654.e017">Eq 12</xref>, Φ<sub><italic>H</italic></sub> is the difference between the sum of entropy within each part and entropy in the whole system. When the parts correlate, the entropy in the whole system decreases. In contrast, the sum of entropy of each part does not change, because the degree of noise within each part (the diagonal elements of <italic>E</italic><sup><italic>t</italic></sup>) is fixed. Thus, Φ<sub><italic>H</italic></sub> increases as the degree of noise correlation <italic>c</italic> increases without reflecting the amount of information in the system.</p>
<fig id="pcbi.1004654.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004654.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Violation of theoretical requirements as a measure of integrated information.</title>
<p>The behaviors of Φ*, Φ<sub><italic>I</italic></sub>, and Φ<sub><italic>H</italic></sub> are shown in the left, middle, and right panels, respectively, when the strength of noise correlation <italic>c</italic> is varied in a linear regression model (<xref ref-type="disp-formula" rid="pcbi.1004654.e018">Eq 13</xref>). Red lines indicate the regime where the theoretical requirements are violated, and the blue lines indicate that the theoretical requirements are satisfied. Dotted black lines are drawn at 0. (A) Violation of the upper bound. The strength of connections <italic>a</italic> is set to 0. In this case, there is no information between the past and present states of the system but Φ<sub><italic>H</italic></sub> is not 0, i.e., Φ<sub><italic>H</italic></sub> violates the upper bound. (B) Violation of the lower bound. The strength of connections <italic>a</italic> is set to 0.4. At the right ends of the figures where <italic>c</italic> is 1, the two units in the system are perfectly correlated. Φ<sub><italic>I</italic></sub> is negative, i.e., Φ<sub><italic>I</italic></sub> violates the lower bound when the degree of correlation is high.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011">
<title>When parts are perfectly correlated</title>
<p>Next, we consider the case where the parts are perfectly correlated. More specifically, consider the case where the two parts <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> are equal at every time, i.e. <inline-formula id="pcbi.1004654.e022"><alternatives><graphic id="pcbi.1004654.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004654.e023"><alternatives><graphic id="pcbi.1004654.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>2</mml:mn> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. Here, Φ* is 0 because the amount of information extracted by mismatched decoding would not degrade even if the other part is ignored for decoding (see <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref> for the mathematical proof).
<disp-formula id="pcbi.1004654.e024"><alternatives><graphic id="pcbi.1004654.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
Regarding Φ<sub><italic>I</italic></sub>, the mutual information of each part is equal to each other, <inline-formula id="pcbi.1004654.e025"><alternatives><graphic id="pcbi.1004654.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>2</mml:mn> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and the mutual information in the whole system is equal to the mutual information of each part, <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>) = <italic>I</italic>(<italic>M</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>M</italic><sup><italic>t</italic></sup>). Thus, the second term in <xref ref-type="disp-formula" rid="pcbi.1004654.e007">Eq 5</xref> is twice the value of the first, and Φ<sub><italic>I</italic></sub> is the negative value of the mutual information in one part,
<disp-formula id="pcbi.1004654.e026"><alternatives><graphic id="pcbi.1004654.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>I</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula>
Thus, Φ<sub><italic>I</italic></sub> does not satisfy the lower bound as a measure of integrated information. Φ<sub><italic>H</italic></sub> is given by
<disp-formula id="pcbi.1004654.e027"><alternatives><graphic id="pcbi.1004654.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mrow><mml:msub><mml:mo>Φ</mml:mo> <mml:mi>H</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>H</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
which is larger than or equal to 0 (Φ<sub><italic>H</italic></sub> is always larger than or equal to 0 because it can be written as the Kullback-Leibler divergence.).</p>
<p>We considered again the same linear regression model presented in the previous section (<xref ref-type="disp-formula" rid="pcbi.1004654.e018">Eq 13</xref>). We varied the degree of noise correlation, <italic>c</italic>, from 0 to 1 while keeping connection strength <italic>a</italic> as 0.4. When <italic>c</italic> is 1, the two units correlate perfectly. <xref ref-type="fig" rid="pcbi.1004654.g004">Fig 4</xref> shows an exemplar time series when <italic>c</italic> is 0.4 and <italic>a</italic> is 0.4. Φ<sub><italic>I</italic></sub> takes positive values when <italic>c</italic> is less than ∼0.2 but takes negative values when <italic>c</italic> is greater (<xref ref-type="fig" rid="pcbi.1004654.g003">Fig 3(B)</xref>). Φ* decreases monotonically with <italic>c</italic> and becomes 0 when <italic>c</italic> is 1. Φ<sub><italic>H</italic></sub> increases monotonically with <italic>c</italic> reflecting the degree of correlation between the units. The detailed behaviors of Φ*, Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> when <italic>a</italic> and <italic>c</italic> are both varied are shown in <xref ref-type="supplementary-material" rid="pcbi.1004654.s002">S1 Fig</xref>.</p>
<fig id="pcbi.1004654.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004654.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Exemplar time series when correlation is high.</title>
<p>The strength of noise correlation <italic>c</italic> and the connection strength <italic>a</italic> are set to both 0.4 in the linear regression model (<xref ref-type="disp-formula" rid="pcbi.1004654.e018">Eq 13</xref>). <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub> represent the mutual information in unit 1 and 2, and <italic>I</italic> represents the mutual information in the whole system. In this case, the sum of the mutual information in the parts exceeds the mutual information in the whole system and Φ<sub><italic>I</italic></sub> is negative.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012">
<title>Electrocorticogram data analysis</title>
<p>The problems of Φ<sub><italic>H</italic></sub> and Φ<sub><italic>I</italic></sub> can manifest in their application to real neural recordings from the brain. <xref ref-type="fig" rid="pcbi.1004654.g005">Fig 5</xref> shows the measures of integrated information, Φ*, Φ<sub><italic>I</italic></sub>, Φ<sub><italic>H</italic></sub>, and the mutual information <italic>I</italic> computed from the electrocorticogram (ECoG) recordings in an awake monkey as a function of the time lag <italic>τ</italic> (See <xref ref-type="sec" rid="sec014">Methods</xref> for details).</p>
<fig id="pcbi.1004654.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004654.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Measures of integrated information and mutual information computed in monkey ECoG data.</title>
<p>Time lag <italic>τ</italic> is varied from 1 to 500 ms. The behaviors of Φ* (red line), Φ<sub><italic>I</italic></sub> (green line), Φ<sub><italic>H</italic></sub> (blue line), and mutual information <italic>I</italic> (black line) are shown. Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> violate the theoretical requirements.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.g005" xlink:type="simple"/>
</fig>
<p>As we can see, the mutual information between <italic>X</italic><sup><italic>t</italic></sup> and <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> monotonically decreases as <italic>τ</italic> increases. Φ* is positive, peaks around <italic>τ</italic> = 20 ms, and less than the mutual information, always satisfying the theoretical requirements. However, Φ<sub><italic>I</italic></sub> is negative when <italic>τ</italic> is small and Φ<sub><italic>H</italic></sub> remains large even when <italic>I</italic> approaches 0 with increasing <italic>τ</italic>, both violating the theoretical requirements.</p>
</sec>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>In this study, we consider the two theoretical requirements that a measure of integrated information should satisfy, as follows: The lower and upper bounds of integrated information should be 0 and the amount of information generated by the whole system, respectively. The theoretical requirements are naturally derived from the original philosophy of integrated information [<xref ref-type="bibr" rid="pcbi.1004654.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>], which states that integrated information is the information generated by a system as a whole above and beyond its parts. The original measure of integrated information Φ satisfies the theoretical requirements so that we can interpret a measure of integrated information according to the original philosophy. To derive a practical measure of integrated information that satisfies the required lower and upper bounds, we introduced a concept of mismatched decoding. We defined our measure of integrated information Φ* as the amount of information lost when a mismatched probability distribution, where a system is partitioned into “independent” parts, is used for decoding instead of the actual probability distribution. In this framework, Φ* quantifies the amount of information loss associated with mismatched decoding where interactions between the parts of a system are ignored and therefore quantifies the amount of information integrated by the interactions. We show that Φ* satisfies the lower and upper bounds, that Φ<sub><italic>I</italic></sub> does not satisfy the lower bound, and that Φ<sub><italic>H</italic></sub> does not satisfy the upper bound. We consider Φ* a proper measure of integrated information that can be generally used for practical applications.</p>
<p>Here, we briefly note a potential reason why the previous study [<xref ref-type="bibr" rid="pcbi.1004654.ref015">15</xref>] failed to identify these problems of Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub>. Although they calculated their measures in small networks by using the autoregressive model in <xref ref-type="disp-formula" rid="pcbi.1004654.e017">Eq 12</xref>, they did not extensively vary the connectivity matrix <italic>A</italic> and the Gaussian noise <italic>E</italic>. In particular, they fixed the covariance of the Gaussian noise <italic>E</italic> to 0. As we can clearly see in <xref ref-type="fig" rid="pcbi.1004654.g003">Fig 3</xref> and <xref ref-type="supplementary-material" rid="pcbi.1004654.s002">S1 Fig</xref>, both connectivity strength <italic>a</italic> and the covariance of the noise <italic>c</italic> strongly affect the amount of integrated information. In particular, when the covariance of <italic>E</italic> is large, Φ<sub><italic>I</italic></sub> and Φ<sub><italic>H</italic></sub> violate the theoretical requirements. For future investigations of calculating integrated information in networks described by autoregressive model, we should note that it is very important to take account of not only the effects of connectivity matrix <italic>A</italic> but also the effects of covariance of <italic>E</italic> on the amount of integrated information.</p>
<p>The basic concept of Integrated Information Theory (IIT) was tested by conducting empirical experiments, and the evidence accumulated supports the conclusion that when consciousness is lost, integration of information is lost [<xref ref-type="bibr" rid="pcbi.1004654.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref014">14</xref>]. In particular, Casali and colleagues [<xref ref-type="bibr" rid="pcbi.1004654.ref014">14</xref>] found that a complexity measure, motivated by IIT, successfully separates conscious awake states from various unconscious states due to deep sleep, anesthesia, and traumatic brain injuries. Although their measure is inspired by the concept of integrated information, it measures the complexity of averaged neural responses to one particular type of external perturbation (e.g. a TMS pulse to a target region) and does not directly measure integrated information.</p>
<p>There are few studies that directly estimate integrated information in the brain [<xref ref-type="bibr" rid="pcbi.1004654.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref028">28</xref>] using the measure introduced in IIT 1.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref002">2</xref>] or Φ<sub><italic>H</italic></sub>. Our new measure of integrated information, Φ*, will contribute to experiments designed to test whether integrated information is a key to distinguishing conscious states from unconscious states [<xref ref-type="bibr" rid="pcbi.1004654.ref029">29</xref>–<xref ref-type="bibr" rid="pcbi.1004654.ref031">31</xref>].</p>
<p>We considered the measure of integrated information proposed in IIT 2.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>], because its computations are feasible. There are several updates in the latest version, IIT 3.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref008">8</xref>]. In IIT 2.0, integrated information is quantified by measuring how the distribution of the past states differs when a present state is given (see <xref ref-type="supplementary-material" rid="pcbi.1004654.s001">S1 Text</xref> for details) whereas in IIT 3.0, it is quantified by measuring how the distribution of the past and future states differs when a present state is given. In other words, IIT 2.0 considers only the information flow from the present to the past while IIT 3.0 additionally considers the information flow from the present to the future. Our measure Φ* does not asymmetrically quantify integrated information from the present to the past or from the present to the future, because the mutual information is a symmetric measure for the time points <italic>t</italic> − <italic>τ</italic> and <italic>t</italic>. An unanswered question is how integrated information should be practically calculated taking account of the both directions of information flow, using an empirical distribution.</p>
<p>An unresolved difficulty that impedes practical calculation of integrated information is how to partition a system. In the present study, we considered only the quantification of integrated information when a partition of a system is given. IIT requires that integrated information should be quantified using the partition where information is least integrated, called the minimum information partition (MIP) [<xref ref-type="bibr" rid="pcbi.1004654.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>]. To find the MIP, every possible partition must be examined, yet the number of possible partitions grows exponentially with the size of the system. One way to work around this difficulty would be to develop optimization algorithms to quickly find a partition that well approximates the MIP.</p>
<p>Besides the practical problem of finding the MIP, there remains a theoretical problem of how to compare integrated information across different partitions. Integrated information increases as the number of parts gets larger, because more information is lost by partitioning the system. Further, integrated information is expected to be larger in a symmetric partition where a system is partitioned into two parts of equal size than in an asymmetric partition. IIT 2.0 [<xref ref-type="bibr" rid="pcbi.1004654.ref006">6</xref>] proposes a normalization factor, which considers these issues. However, there might be other possible ways to perform normalization. It is unclear whether there is a reasonable theoretical foundation that adjudicates the best normalization scheme. Moreover, it is unclear if the normalization factor, which is proposed for systems whose states are represented by discrete variables, is appropriate for systems whose states are represented by continuous variables. The normalization factor, which is based on the entropies of the parts of a system, can be negative because entropy can be negative for continuous variables. Thus, we need a different normalization factor when we deal with continuous variables. Further investigations are required to resolve the practical and theoretical issues related to the MIP.</p>
<p>Although we derived Φ*, because we were motivated by IIT and its potential relevance to consciousness, Φ* has unique meaning from the perspective of information theory, which is independent of IIT. Thus, it can be applied to research fields other than research on consciousness [<xref ref-type="bibr" rid="pcbi.1004654.ref032">32</xref>]. Φ* quantifies the loss of information when interactions or connections between the units in a system are ignored. Thus, Φ* is expected to be related to connectivity measures such as Granger causality [<xref ref-type="bibr" rid="pcbi.1004654.ref033">33</xref>] or transfer entropy [<xref ref-type="bibr" rid="pcbi.1004654.ref034">34</xref>]. It will be interesting to clarify mathematical relationships between Φ* and the other connectivity measures. We expect that information geometry [<xref ref-type="bibr" rid="pcbi.1004654.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref036">36</xref>] plays an important role for studying the properties of these quantities. Here, we indicate only an apparent difference between them as follows: Φ* intends to measure global integrations in a system as a whole, while traditional bivariate measures such as Granger causality or transfer entropy intends to measure local interactions between elements of the system. Consider that we divide a system into parts <italic>A</italic>, <italic>B</italic>, and <italic>C</italic>. Using integrated information, our goal is to quantify the information integrated among <italic>A</italic>, <italic>B</italic>, and <italic>C</italic> as a whole. In contrast, what we quantify using Granger causality or transfer entropy is the influence of <italic>A</italic> on <italic>B</italic>, <italic>B</italic> on <italic>C</italic>, <italic>C</italic> on <italic>A</italic> and the reverse. It is not obvious how a measure of global interactions in the whole system should be defined and derived theoretically from measures of the local interactions. As an example, one possibility is simply summing up all local interactions and considering the sum as a global measure [<xref ref-type="bibr" rid="pcbi.1004654.ref037">37</xref>]. Yet, more research is required to determine whether such an approach is a valid method to define global interactions [<xref ref-type="bibr" rid="pcbi.1004654.ref036">36</xref>]. Φ*, in contrast, is not derived from the local interaction measures but is derived directly by comparing the total mutual information in the whole system with hypothetical mutual information when the system is assumed to be partitioned into independent parts. Thus, the interpretation of Φ* is straightforward from an information theoretical viewpoint. Our measure, which we consider a measure of the global interaction, may provide new insights into diverse research subjects as a novel tool for network analysis.</p>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Mathematical expression of <italic>I</italic>*</title>
<p>The amount of information for mismatched decoding can be evaluated using the following equation,
<disp-formula id="pcbi.1004654.e028"><alternatives><graphic id="pcbi.1004654.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>I</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>q</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>β</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>q</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>β</mml:mi></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
where <italic>β</italic> is the value that maximizes <italic>I</italic>*. The maximization of <italic>I</italic>* with respect to <italic>β</italic> is performed by differentiating <italic>I</italic>* and solving the equation, <italic>dI</italic>*(<italic>β</italic>)/<italic>dβ</italic> = 0. In general, the solution of the equation can be found using the standard gradient ascent method, because <italic>I</italic>* is a convex function with respect to <italic>β</italic>[<xref ref-type="bibr" rid="pcbi.1004654.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref018">18</xref>].</p>
<p>For comparison, the mutual information is given by
<disp-formula id="pcbi.1004654.e029"><alternatives><graphic id="pcbi.1004654.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mrow><mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:munder> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
If a mismatched probability distribution <italic>q</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is replaced by the actual distribution <italic>p</italic>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) in <xref ref-type="disp-formula" rid="pcbi.1004654.e028">Eq 20</xref>, the derivative of <italic>I</italic>* becomes 0 when <italic>β</italic> = 1. By substituting <italic>q</italic> = <italic>p</italic> and <italic>β</italic> = 1 into <xref ref-type="disp-formula" rid="pcbi.1004654.e028">Eq 20</xref>, one can check that <italic>I</italic>* is equal to <italic>I</italic> in <xref ref-type="disp-formula" rid="pcbi.1004654.e029">Eq 21</xref>, as it should be. The amount of information for mismatched decoding, <italic>I</italic>*, was first derived in the field of information theory as an extension of the mutual information in the case of mismatched decoding [<xref ref-type="bibr" rid="pcbi.1004654.ref017">17</xref>]. <italic>I</italic>* was first introduced into neuroscience in [<xref ref-type="bibr" rid="pcbi.1004654.ref018">18</xref>] and was first applied to the analysis of neural data by [<xref ref-type="bibr" rid="pcbi.1004654.ref019">19</xref>]. However, <italic>I</italic>* in the prior neuroscience application [<xref ref-type="bibr" rid="pcbi.1004654.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004654.ref019">19</xref>] was quantified between stimuli and neural states, not between the past and present states of a system, as described in the present study.</p>
</sec>
<sec id="sec016">
<title>Analytical computation of Φ* under the Gaussian assumption</title>
<p>Assume that the probability distribution of neural states X is the Gaussian distribution,
<disp-formula id="pcbi.1004654.e030"><alternatives><graphic id="pcbi.1004654.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="normal">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>N</mml:mi></mml:msup> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo></mml:mrow></mml:mfenced> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="normal">X</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="normal">X</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>Σ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="normal">X</mml:mi> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="normal">X</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
where <italic>N</italic> is the number of variables in X, <inline-formula id="pcbi.1004654.e031"><alternatives><graphic id="pcbi.1004654.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mover accent="true"><mml:mi mathvariant="normal">X</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the mean value of X, and Σ(<italic>X</italic>) is the covariance matrix of X. The Gaussian assumption allows us to analytically compute Φ*, which substantially reduces the costs for computing Φ*. When <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> and <italic>X</italic><sup><italic>t</italic></sup> are both multivariate Gaussian variables, the mutual information between <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> and <italic>X</italic><sup><italic>t</italic></sup>, <italic>I</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>;<italic>X</italic><sup><italic>t</italic></sup>), can be analytically computed as
<disp-formula id="pcbi.1004654.e032"><alternatives><graphic id="pcbi.1004654.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mrow><mml:mi>I</mml:mi><mml:mspace width="1pt"/><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
where Σ(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>|<italic>X</italic><sup><italic>t</italic></sup>) is the covariance matrix of the conditional distribution, <italic>p</italic>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>|<italic>X</italic><sup><italic>t</italic></sup>), which is expressed as
<disp-formula id="pcbi.1004654.e033"><alternatives><graphic id="pcbi.1004654.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>Σ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>Σ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
where Σ(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>, <italic>X</italic><sup><italic>t</italic></sup>) is the cross covariance matrix between <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup> and <italic>X</italic><sup><italic>t</italic></sup>, whose element Σ(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>, <italic>X</italic><sup><italic>t</italic></sup>)<sub><italic>ij</italic></sub> is given by <inline-formula id="pcbi.1004654.e034"><alternatives><graphic id="pcbi.1004654.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mtext>cov</mml:mtext> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>X</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>X</mml:mi> <mml:mi>j</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Similarly, we can obtain the analytical expression of <italic>I</italic>* as follows:
<disp-formula id="pcbi.1004654.e035"><alternatives><graphic id="pcbi.1004654.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:msup><mml:mi>I</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>Tr</mml:mtext><mml:mspace width="1pt"/><mml:mfenced close=")" open="(" separators=""><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo> <mml:mi>R</mml:mi></mml:mfenced> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mo>|</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>|</mml:mo> <mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo> <mml:mo>|</mml:mo></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>β</mml:mi> <mml:mi>N</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
where <inline-formula id="pcbi.1004654.e036"><alternatives><graphic id="pcbi.1004654.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mtext>Tr</mml:mtext></mml:math></alternatives></inline-formula> stands for trace. <italic>Q</italic> and <italic>R</italic> are given by
<disp-formula id="pcbi.1004654.e037"><alternatives><graphic id="pcbi.1004654.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>=</mml:mo> <mml:mo>Σ</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(26)</label></disp-formula>
<disp-formula id="pcbi.1004654.e038"><alternatives><graphic id="pcbi.1004654.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>R</mml:mi> <mml:mo>=</mml:mo> <mml:mi>β</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>-</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(27)</label></disp-formula>
where Σ<sub><italic>D</italic></sub>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>), Σ<sub><italic>D</italic></sub>(<italic>X</italic><sup><italic>t</italic></sup>, <italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) and Σ<sub><italic>D</italic></sub>(<italic>X</italic><sup><italic>t</italic></sup>|<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) are diagonal block matrices. Each block matrix is a covariance matrix of each part, <inline-formula id="pcbi.1004654.e039"><alternatives><graphic id="pcbi.1004654.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004654.e040"><alternatives><graphic id="pcbi.1004654.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pcbi.1004654.e041"><alternatives><graphic id="pcbi.1004654.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msubsup> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <italic>M</italic><sub><italic>i</italic></sub> is a subsystem. For example, Σ<sub><italic>D</italic></sub>(<italic>X</italic><sup><italic>t</italic>−<italic>τ</italic></sup>) is given by
<disp-formula id="pcbi.1004654.e042"><alternatives><graphic id="pcbi.1004654.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mrow><mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd/><mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mn>2</mml:mn> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd/><mml:mtd><mml:mtext>0</mml:mtext></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mtext>0</mml:mtext></mml:mtd> <mml:mtd/><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd/></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd/><mml:mtd/><mml:mtd><mml:mrow><mml:mo>Σ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>m</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p>The maximization of <italic>I</italic>* with respect to <italic>β</italic> is performed by solving the equation <italic>dI</italic>*(<italic>β</italic>)/<italic>dβ</italic> = 0. The derivative of <italic>I</italic>*(<italic>β</italic>) with respect to <italic>β</italic> is given by
<disp-formula id="pcbi.1004654.e043"><alternatives><graphic id="pcbi.1004654.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msup><mml:mi>I</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>Tr</mml:mtext><mml:mspace width="1pt"/><mml:mfenced close=")" open="(" separators=""><mml:mo>Σ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>R</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext>Tr</mml:mtext> <mml:mspace width="1pt"/><mml:mfenced close=")" open="(" separators=""><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(29)</label></disp-formula>
where
<disp-formula id="pcbi.1004654.e044"><alternatives><graphic id="pcbi.1004654.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>R</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mi>β</mml:mi> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>-</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(30)</label></disp-formula> <disp-formula id="pcbi.1004654.e045"><alternatives><graphic id="pcbi.1004654.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(31)</label></disp-formula>
and
<disp-formula id="pcbi.1004654.e046"><alternatives><graphic id="pcbi.1004654.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mfrac><mml:mrow><mml:mi>d</mml:mi> <mml:mi>Q</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:mfrac> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1004654.e047"><alternatives><graphic id="pcbi.1004654.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004654.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mrow><mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>|</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mi>t</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>Σ</mml:mo> <mml:mi>D</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>X</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mi>τ</mml:mi></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(33)</label></disp-formula>
Inspection of the above equations reveals that <italic>dI</italic>*(<italic>β</italic>)/<italic>dβ</italic> = 0 is a quadratic equation with respect to <italic>β</italic>. Thus, <italic>β</italic> can be analytically computed without resorting to numerical optimization such as gradient ascent.</p>
</sec>
<sec id="sec017">
<title>Electrocorticogram (ECoG) recording</title>
<p>The detailed recording protocols were described in [<xref ref-type="bibr" rid="pcbi.1004654.ref038">38</xref>]. Here, we briefly describe the aspects of the protocols that are relevant for our analysis. We used customized multichannel ECoG electrode arrays. An array of ECoG electrodes was embedded in an insulating silicone sheet. The surface of the sheet was dimpled to expose the surface of ECoG electrodes with the diameter of 1 mm. The electrodes were made of platinum discs, and inter-electrode distance was 5 mm. We implanted 128 ECoG electrodes in the subdural space in four adult macaque monkeys. The ECoG electrodes covered the left hemisphere over the frontal, parietal, temporal, and occipital lobes. ECoG signal was recorded at a sampling rate of 1 kHz. All experimental and surgical procedures were performed in accordance with the protocols approved by the RIKEN ethics committee. During the experiments, the monkeys were seated in a primate chair with both arms and head restrained. We analyzed the data recorded when the monkeys were awake.</p>
</sec>
<sec id="sec018">
<title>Data processing and calculation of integrated information Φ*</title>
<p>To remove line noise and reduce artifacts in the ECoG data, we computed bipolar re-referenced signals between two neighboring electrodes. We calculated integrated information Φ* using all the bipolar re-referenced signals (64 in total). We considered the simplest partition scheme, “atomic partition” [<xref ref-type="bibr" rid="pcbi.1004654.ref039">39</xref>], in which the system is partitioned into its individual elements. For this data set, it meant that we computed Φ* assuming that all the 64 channels are independent. The atomic partition gives the upper bound of Φ* among all the possible partitions because it quantifies the amount of information loss when all the interactions in the system are ignored for decoding.</p>
<p>We approximated the probability distributions of the continuous ECoG signals with the Gaussian distribution. Under the Gaussian assumption, we analytically computed Φ* by using the equations derived in Methods. We estimated the covariance matrices of the data with a time window of 2s and a time step of 2s. Then, we averaged the covariance matrices over 600s and used the average of the covariance matrices for computation of Φ*.</p>
</sec>
</sec>
<sec id="sec019">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004654.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Mathematical details of integrated information.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004654.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004654.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Theoretical requirements are not satisfied by previously proposed measures.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004654.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chalmers</surname> <given-names>DJ</given-names></name>. <article-title>Facing up to the problem of consciousness</article-title>. <source>J Conscious Stud</source>. <year>1995</year>; <volume>2</volume>: <fpage>200</fpage>–<lpage>219</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>An information integration theory of consciousness</article-title>. <source>BMC Neurosci</source> <year>2004</year>; <volume>5</volume>: <fpage>42</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2202-5-42" xlink:type="simple">10.1186/1471-2202-5-42</ext-link></comment> <object-id pub-id-type="pmid">15522121</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Consciousness as integrated information: a provisional manifesto</article-title>. <source>Biol Bull</source>. <year>2008</year>; <volume>215</volume>: <fpage>216</fpage>–<lpage>242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/25470707" xlink:type="simple">10.2307/25470707</ext-link></comment> <object-id pub-id-type="pmid">19098144</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Information integration: its relevance to brain function and consciousness</article-title>. <source>Arch Ital Biol</source>. <year>2010</year>; <volume>148</volume>: <fpage>299</fpage>–<lpage>322</lpage>. <object-id pub-id-type="pmid">21175016</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Integrated information theory of consciousness: an updated account</article-title>. <source>Arch Ital Biol</source>. <year>2012</year>; <volume>150</volume>: <fpage>56</fpage>–<lpage>90</lpage>. <object-id pub-id-type="pmid">23165867</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balduzzi</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Integrated information in discrete dynamical systems: Motivation and theoretical framework</article-title>. <source>PLoS Comput Biol</source> <year>2008</year>; <volume>4</volume>: <fpage>e1000091</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000091" xlink:type="simple">10.1371/journal.pcbi.1000091</ext-link></comment> <object-id pub-id-type="pmid">18551165</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balduzzi</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Qualia: the geometry of integrated information</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>; <volume>5</volume>: <fpage>e1000462</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000462" xlink:type="simple">10.1371/journal.pcbi.1000462</ext-link></comment> <object-id pub-id-type="pmid">19680424</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oizumi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Albantakis</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>From the phenomenology to the mechanisms of consciousness: Integrated Information Theory 3.0</article-title>. <source>PLoS Comp Biol</source>. <year>2014</year>; <volume>10</volume>: <fpage>e1003588</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003588" xlink:type="simple">10.1371/journal.pcbi.1003588</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Consciousness: here, there and everywhere?</article-title> <source>Phil Trans R Soc B</source>. <year>2015</year>; <volume>19</volume>: <fpage>370</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Massimini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ferrarelli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Esser</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Breakdown of cortical effective connectivity during sleep</article-title>. <source>Science</source>. <year>2005</year>; <volume>309</volume>: <fpage>2228</fpage>–<lpage>2232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1117256" xlink:type="simple">10.1126/science.1117256</ext-link></comment> <object-id pub-id-type="pmid">16195466</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Massimini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ferrarelli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Esser</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Riedner</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Huber</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Peterson</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Triggering sleep slow waves by transcranial magnetic stimulation</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2007</year>; <volume>104</volume>: <fpage>8496</fpage>–<lpage>8501</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0702495104" xlink:type="simple">10.1073/pnas.0702495104</ext-link></comment> <object-id pub-id-type="pmid">17483481</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ferrarelli</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Massimini</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sarasso</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Casali</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Riedner</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Angelini</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Pearce</surname> <given-names>RA</given-names></name>. <article-title>Breakdown in cortical effective connectivity during midazolam-induced loss of consciousness</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2010</year>; <volume>107</volume>: <fpage>2681</fpage>–<lpage>2686</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0913008107" xlink:type="simple">10.1073/pnas.0913008107</ext-link></comment> <object-id pub-id-type="pmid">20133802</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rosanova</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gosseries</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Casarotto</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Boly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Casali</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Bruno</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Mariotti</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boveroux</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Laureys</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Massimini</surname> <given-names>M</given-names></name>. <article-title>Recovery of cortical effective connectivity and recovery of consciousness in vegetative patients</article-title>. <source>Brain</source>. <year>2012</year>; <volume>135</volume>. <fpage>1308</fpage>–<lpage>1320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/awr340" xlink:type="simple">10.1093/brain/awr340</ext-link></comment> <object-id pub-id-type="pmid">22226806</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Casali</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Gosseries</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rosanova</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Boly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sarasso</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>A theoretically based index of consciousness independent of sensory processing and behavior</article-title>. <source>Sci Transl Med</source>. <year>2013</year>; <volume>5</volume>: <fpage>198ra105</fpage>. <object-id pub-id-type="pmid">23946194</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barrett</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Seth</surname> <given-names>AK</given-names></name>. <article-title>Practical measures of integrated information for time-series data</article-title>. <source>PLoS Comput Biol</source> <year>2011</year>; <volume>7</volume>. <fpage>e1001052</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001052" xlink:type="simple">10.1371/journal.pcbi.1001052</ext-link></comment> <object-id pub-id-type="pmid">21283779</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cover</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JA</given-names></name>. <source>Elements of information theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Merhav</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kaplan</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Lapidoth</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shamai Shitz</surname> <given-names>S</given-names></name>. <article-title>On information rates for mismatched decoders</article-title>. <source>IEEE Trans Inform Theory</source>. <year>1994</year>; <volume>40</volume>: <fpage>1953</fpage>–<lpage>1967</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/18.340469" xlink:type="simple">10.1109/18.340469</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Nirenberg</surname> <given-names>S</given-names></name>. <article-title>Synergy, redundancy, and independence in population codes, revisited</article-title>. <source>J Neurosci</source>. <year>2005</year>; <volume>25</volume>: <fpage>5195</fpage>–<lpage>5206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5319-04.2005" xlink:type="simple">10.1523/JNEUROSCI.5319-04.2005</ext-link></comment> <object-id pub-id-type="pmid">15917459</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oizumi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ishii</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ishibashi</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hosoya</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>M</given-names></name>. <article-title>Mismatched decoding in the brain</article-title>. <source>J Neurosci</source>. <year>2010</year>; <volume>30</volume>: <fpage>4815</fpage>–<lpage>4826</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4360-09.2010" xlink:type="simple">10.1523/JNEUROSCI.4360-09.2010</ext-link></comment> <object-id pub-id-type="pmid">20357132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oizumi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>. <article-title>Information loss associated with imperfect observation and mismatched decoding</article-title>. <source>Front Comput Neurosci</source>. <year>2011</year>; <volume>5</volume>: <fpage>9</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2011.00009" xlink:type="simple">10.3389/fncom.2011.00009</ext-link></comment> <object-id pub-id-type="pmid">21629857</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref021">
<label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rieke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Warland</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname> <given-names>W</given-names></name>. <source>Spikes: Exploring the neural code</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1997</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref022">
<label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <source>Theoretical Neuroscience. Computational and Mathematical Modeling of Neural Systems</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Averbeck</surname> <given-names>BB</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name>. <article-title>Neural correlations, population coding and computation</article-title>. <source>Nat Rev Neurosci</source>. <year>2006</year>; <volume>7</volume>: <fpage>358</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1888" xlink:type="simple">10.1038/nrn1888</ext-link></comment> <object-id pub-id-type="pmid">16760916</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Quian Quiroga</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Panzeri</surname> <given-names>S</given-names></name>. <article-title>Extracting information from neuronal populations: information theory and decoding approaches</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year>; <volume>10</volume>: <fpage>173</fpage>–<lpage>185</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2578" xlink:type="simple">10.1038/nrn2578</ext-link></comment> <object-id pub-id-type="pmid">19229240</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Ay N. Information geometry on complexity and stochastic interaction. 2001. MPI MIS Preprint 95. Available: <ext-link ext-link-type="uri" xlink:href="http://www.mis.mpg.de/publications/preprints/2001/prepr2001-95.html" xlink:type="simple">http://www.mis.mpg.de/publications/preprints/2001/prepr2001-95.html</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ay</surname> <given-names>N</given-names></name>. <article-title>Information geometry on complexity and stochastic interaction</article-title>. <source>Entropy</source>. <year>2015</year>; <volume>17</volume>: <fpage>2432</fpage>–<lpage>2458</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3390/e17042432" xlink:type="simple">10.3390/e17042432</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Mashour</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Noh</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>BM</given-names></name>. <article-title>Propofol induction reduces the capacity for neural information integration: Implications for the mechanism of consciousness and general anesthesia</article-title>. <source>Conscious Cogn</source>. <year>2009</year>; <volume>18</volume>: <fpage>56</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.concog.2008.10.005" xlink:type="simple">10.1016/j.concog.2008.10.005</ext-link></comment> <object-id pub-id-type="pmid">19054696</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chang</surname> <given-names>JY</given-names></name>, <etal>et al</etal>. <article-title>Multivariate autoregressive models with exogenous inputs for intracerebral responses to direct electrical stimulation of the human brain</article-title>. <source>Front Hum Neurosci</source>. <year>2012</year>; <volume>6</volume>: <fpage>317</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnhum.2012.00317" xlink:type="simple">10.3389/fnhum.2012.00317</ext-link></comment> <object-id pub-id-type="pmid">23226122</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alkire</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Hudetz</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>. <article-title>Consciousness and anesthesia</article-title>. <source>Science</source>. <year>2008</year>; <volume>322</volume>: <fpage>876</fpage>–<lpage>880</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1149213" xlink:type="simple">10.1126/science.1149213</ext-link></comment> <object-id pub-id-type="pmid">18988836</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boly</surname> <given-names>M</given-names></name>. <article-title>Measuring the fading consciousness in the human brain</article-title>. <source>Curr Opin Neurol</source>. <year>2011</year>; <volume>24</volume>: <fpage>394</fpage>–<lpage>400</lpage>. <object-id pub-id-type="pmid">21577107</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sanders</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Laureys</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sleigh</surname> <given-names>J</given-names></name>. <article-title>Unresponsiveness ≠ unconsciousness</article-title>. <source>Anesthesiology</source>. <year>2012</year>; <volume>116</volume>: <fpage>946</fpage>–<lpage>959</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/ALN.0b013e318249d0a7" xlink:type="simple">10.1097/ALN.0b013e318249d0a7</ext-link></comment> <object-id pub-id-type="pmid">22314293</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boly</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sasai</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gosseries</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Oizumi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Casali</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Massimini</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Stimulus set meaningfulness and neurophysiological differentiation: A functional magnetic resonance imaging study</article-title>. <source>PLoS ONE</source>. <year>2015</year>; <volume>10</volume>: <fpage>e0125337</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0125337" xlink:type="simple">10.1371/journal.pone.0125337</ext-link></comment> <object-id pub-id-type="pmid">25970444</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref033">
<label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ding</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Bressler</surname> <given-names>SL</given-names></name>. <chapter-title>Granger causality: Basic theory and application to neuroscience</chapter-title>. In: <name name-style="western"><surname>Schelter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Winterhalder</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Timmer</surname> <given-names>J</given-names></name>, editors. <source>Handbook of time series analysis</source>. <publisher-loc>Wienheim</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>2006</year>. pp. <fpage>438</fpage>–<lpage>460</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vicente</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wibral</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lindner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pipa</surname> <given-names>G</given-names></name>. <article-title>Transfer entropy–a model-free measure of effective connectivity for the neurosciences</article-title>. <source>J Comput Neurosci</source>. <year>2011</year>; <volume>30</volume>: <fpage>45</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10827-010-0262-3" xlink:type="simple">10.1007/s10827-010-0262-3</ext-link></comment> <object-id pub-id-type="pmid">20706781</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref035">
<label>35</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Amari</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nagaoka</surname> <given-names>H</given-names></name>. <source>Methods of information geometry</source>. <publisher-name>AMS and Oxford University Press</publisher-name>; <year>2000</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Oizumi M, Tsuchiya N, Amari S. A unified framework for information integration based on information geometry. 2015. Preprint. Available: arXiv:1510.04455.</mixed-citation>
</ref>
<ref id="pcbi.1004654.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seth</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Barrett</surname> <given-names>AB</given-names></name>, <name name-style="western"><surname>Barnett</surname> <given-names>L</given-names></name>. <article-title>Causal density and integrated information as measures of conscious level</article-title>. <source>Philos Transact A Math Phys Eng Sci</source>. <year>2011</year>; <volume>369</volume>: <fpage>3748</fpage>–<lpage>3767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsta.2011.0079" xlink:type="simple">10.1098/rsta.2011.0079</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yanagawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Chao</surname> <given-names>ZC</given-names></name>, <name name-style="western"><surname>Hasegawa</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Fujii</surname> <given-names>N</given-names></name>. <article-title>Large-scale information flow in conscious and unconscious states: an ECoG study in monkeys</article-title>. <source>PLoS One</source>. <year>2013</year>; <volume>8</volume>: <fpage>e80845</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0080845" xlink:type="simple">10.1371/journal.pone.0080845</ext-link></comment> <object-id pub-id-type="pmid">24260491</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004654.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Edlund</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chaumont</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hintze</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Adami</surname> <given-names>C</given-names></name>. <article-title>Integrated information increases with fitness in the evolution of animats</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>; <volume>7</volume>: <fpage>e1002236</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002236" xlink:type="simple">10.1371/journal.pcbi.1002236</ext-link></comment> <object-id pub-id-type="pmid">22028639</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>