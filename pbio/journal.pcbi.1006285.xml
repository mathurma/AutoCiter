<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00953</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006285</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Chemical compounds</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Chemistry</subject><subj-group><subject>Organic chemistry</subject><subj-group><subject>Organic compounds</subject><subj-group><subject>Amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurotransmitters</subject><subj-group><subject>Biogenic amines</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Hormones</subject><subj-group><subject>Catecholamines</subject><subj-group><subject>Dopamine</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Biochemistry</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurochemistry</subject><subj-group><subject>Neurochemicals</subject><subj-group><subject>Dopaminergics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Basal ganglia</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Basal ganglia</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subj-group><subject>Synaptic plasticity</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Learning the payoffs and costs of actions</article-title>
<alt-title alt-title-type="running-head">Learning payoffs and costs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0399-574X</contrib-id>
<name name-style="western">
<surname>Möller</surname> <given-names>Moritz</given-names></name>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8994-1661</contrib-id>
<name name-style="western">
<surname>Bogacz</surname> <given-names>Rafal</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>MRC Brain Network Dynamics Unit, Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gutkin</surname> <given-names>Boris S.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>École Normale Supérieure, College de France, CNRS, FRANCE</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">rafal.bogacz@ndcn.ox.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>2</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>28</day>
<month>2</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>2</issue>
<elocation-id>e1006285</elocation-id>
<history>
<date date-type="received">
<day>6</day>
<month>6</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Möller, Bogacz</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006285"/>
<abstract>
<p>A set of sub-cortical nuclei called basal ganglia is critical for learning the values of actions. The basal ganglia include two pathways, which have been associated with approach and avoid behavior respectively and are differentially modulated by dopamine projections from the midbrain. Inspired by the influential opponent actor learning model, we demonstrate that, under certain circumstances, these pathways may represent learned estimates of the positive and negative consequences (payoffs and costs) of individual actions. In the model, the level of dopamine activity encodes the motivational state and controls to what extent payoffs and costs enter the overall evaluation of actions. We show that a set of previously proposed plasticity rules is suitable to extract payoffs and costs from a prediction error signal if they occur at different moments in time. For those plasticity rules, successful learning requires differential effects of positive and negative outcome prediction errors on the two pathways and a weak decay of synaptic weights over trials. We also confirm through simulations that the model reproduces drug-induced changes of willingness to work, as observed in classical experiments with the D2-antagonist haloperidol.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>The basal ganglia are structures underneath the surface of the vertebrate brain, associated with error-driven learning. Much is known about the anatomical and biological features of the basal ganglia; scientists now try to understand the algorithms implemented by these structures. Numerous models aspire to capture the learning functionality, but many of them only cover some specific aspect of the algorithm. Instead of further adding to that pool of partial models, we unify two existing ones—one which captures what the basal ganglia learn, and one that describes the learning mechanism itself. The first model suggests that the basal ganglia weigh positive against negative consequences of actions according to the motivational state. It hints how payoff and cost might be represented, but does not explain how those representations arise. The other model consists of biologically plausible plasticity rules, which describe how learning takes place, but not how the brain makes use of what is learned. We show that the two theories are compatible. Together, they form a model of learning and decision making that integrates the motivational state as well as the learned payoffs and costs of opportunities.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000265</institution-id>
<institution>Medical Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>MC UU 12024/5</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8994-1661</contrib-id>
<name name-style="western">
<surname>Bogacz</surname> <given-names>Rafal</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by Medical Research Council (<ext-link ext-link-type="uri" xlink:href="https://mrc.ukri.org/" xlink:type="simple">https://mrc.ukri.org/</ext-link>) grant MC_UU_12024/5 awarded to RB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="11"/>
<table-count count="0"/>
<page-count count="32"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-03-12</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data are contained within the manuscript.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>What guides rational behavior in a complex environment? Certainly, knowledge of the typical payoffs and costs of acting a certain way is critical for successful action selection. Those payoffs and costs do not only depend on the action that is carried out, but also on the environmental state, henceforth referred to a ‘situation’. If payoffs and costs are represented separately in the animal’s brain, they can be weighted depending on animal’s motivational (i.e. internal) state, which can vary independently of the environmental situation. For example, consider the action ‘harvesting fruit from a tree’ in the situation ‘close to a fruit-bearing tree’. It has a payoff connected with the nutrients in the fruit, but also costs related to the effort, the risk of pain and the exposure to predators associated with climbing a tree. The nutrients in the fruit are only valuable for the animal if it is hungry, i.e. if it is in the appropriate internal state. So, when it is hungry, the payoffs of climbing a tree which was identified as fruit-bearing should be weighted more than the costs, to ensure that the animal searches for food. By contrast, when the animal is not hungry at all, the payoffs should be weighed less than the costs, to make sure that it does not climb the tree without necessity. In summary, the payoffs and costs of a specific action (‘climbing a nearby tree’) carried out in a certain environmental situation (‘near fruit-bearing tree’) should be weighed against each other according to the motivational state (‘hunger’) to correctly asses the action’s utility.</p>
<p>In all vertebrates, an important role in this process of action evaluation and selection is played by a set of subcortical structures called the basal ganglia [<xref ref-type="bibr" rid="pcbi.1006285.ref001">1</xref>]. The basal ganglia are organized into two main pathways shown schematically in green and red in <xref ref-type="fig" rid="pcbi.1006285.g001">Fig 1</xref>. The Go or direct pathway is related to the initiation of movements, while activation of the No-Go or indirect pathway results in targeted movement inhibition [<xref ref-type="bibr" rid="pcbi.1006285.ref002">2</xref>]. These two pathways include two separate populations of striatal neurons expressing different dopaminergic receptors [<xref ref-type="bibr" rid="pcbi.1006285.ref003">3</xref>]. The striatal Go neurons express D1 receptors and are excited by dopamine, while the striatal No-Go neurons express D2 receptors and are inhibited by dopamine [<xref ref-type="bibr" rid="pcbi.1006285.ref004">4</xref>]. Thus dopamine changes the balance between the two pathways and promotes action initiation over inhibition.</p>
<fig id="pcbi.1006285.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The organization of the basal ganglia.</title>
<p>Circles denote neural populations in the areas indicated by labels next to them, where D1 and D2 correspond to striatal neurons expressing D1 and D2 receptors respectively, STN stands for the subthalamic nucleus, GPe for the external segment of globus pallidus, and Output for the output nuclei of the basal ganglia, i.e. the internal segment of globus pallidus and the substantia nigra pars reticulata. Arrows and lines ending with circles denote excitatory and inhibitory connections respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g001" xlink:type="simple"/>
</fig>
<p>The competition between Go and No-Go pathways during action selection and the role of dopaminergic modulation are subject of many interpretations and models, e.g. [<xref ref-type="bibr" rid="pcbi.1006285.ref005">5</xref>–<xref ref-type="bibr" rid="pcbi.1006285.ref007">7</xref>]. In particular, the Opponent Actor Learning (OpAL) hypothesis suggests that the Go and No-Go neurons specialise in encoding the values of actions with positive or negative consequences respectively [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]. We extend the OpAL hypothesis further by proposing that for each individual action, the direct and indirect pathway separately encode the learned positive and negative consequences. As the dopaminergic neurons modulate the Go and No-Go neurons in opposite ways, dopamine controls the extent to which positive and negative consequences affect the activity in the thalamus, through the output of the basal ganglia [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]. For example, when motivation is high, the dopaminergic neurons will excite the Go neurons and inhibit the No-Go neurons. Consequently, positive action values will influence the decision more than negative action values. By contrast, when the motivation is low, the Go neurons tend to be excited to a smaller degree, but the No-Go neurons will be released from inhibition, such that negative values are weighted stronger.</p>
<p>Much research has also focused on how the synapses of Go and No-Go neurons are modified by experience. A systematic investigation revealed that bursts of activity of dopaminergic neurons encode outcome prediction errors, which measure the difference between outcome (typically rewards) obtained and expected [<xref ref-type="bibr" rid="pcbi.1006285.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref010">10</xref>]. Note that we use the phrases ‘outcome prediction error’ and ‘reinforcement’ instead of the more common ‘reward prediction error’ and ‘reward’ respectively. This use of language emphasizes that in our theory, the feedback signal is informative of both positive and negative events and that not only rewards but any outcome will be compared with predictions. That perspective is well supported by experimental results; see <xref ref-type="sec" rid="sec010">Discussion</xref> for a review of evidence for negative prediction errors (e.g. pauses in dopaminergic firing) caused by negative experiences.</p>
<p>Such bursts of dopaminergic activity produce distinct changes in the synaptic weights of Go and No-Go neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref011">11</xref>]. Several computational models have attempted to describe the learning process of the synapses of Go and No-Go neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1006285.ref015">15</xref>]. Among these models, the OpAL model provided simple and analytically tractable rules describing the changes in weights of Go and No-Go neurons as a function of outcome prediction errors [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]. However, no-one so far examined how the basal ganglia might estimate payoff and cost if they are both associated with the same action.</p>
<p>The goal of this paper is to show how the Go and No-Go neurons can learn the payoffs and costs of individual actions through local synaptic plasticity rules. We argue that the payoffs and costs of individual actions are not necessarily correlated (for instance, two actions might have comparable benefits, but very different costs), and strive to construct a model that is able to represent those independent dimensions of reinforcement for every single action. Ultimately, we confront the resulting model with experimental results.</p>
<p>Instead of constructing a new set of learning rules from scratch, we will employ the theory of striatal learning described in [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>], which has been shown to account for diverse observations. That theory was originally developed to explain how the mean and the spread of the reinforcement signal could be learned by the basal ganglia network. In this article, we will prove that if the weights of Go and No-Go neurons change according to these rules, they can eventually represent payoff and cost. In summary, we show that a set of learning rules, originally constructed to estimate statistical properties of the reinforcement signal, can be reinterpreted as rules to estimate payoffs and costs. We thus extend both the interpretation of the striatal pathways of Collins and Frank and the striatal learning rules of Mikhael and Bogacz to ultimately obtain a consistent theory of learning the payoffs and costs of actions.</p>
<p>According to the experimental and modeling work mentioned above, dopaminergic activity encodes both information about motivational state and the outcome prediction error. However, if the dopaminergic neurons carried both signals, the striatal neurons would need a way to decode each signal and react appropriately, i.e. change their activity according to the motivation signal, and change the synaptic weights according to the prediction error. The prominent suggestion that motivation might be encoded in the average or tonic dopamine level, and outcome prediction errors in the burst or phasic activity [<xref ref-type="bibr" rid="pcbi.1006285.ref017">17</xref>] is hotly debated; it seems to be contradicted by the observation of fast-changing dopaminergic activity that encodes motivation [<xref ref-type="bibr" rid="pcbi.1006285.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1006285.ref020">20</xref>]. Note, though, that these apparently divergent views could potentially be reconciled–see e.g. [<xref ref-type="bibr" rid="pcbi.1006285.ref021">21</xref>]. Anyhow, the motivation and teaching signals could both be provided by other means. For example, the activity of striatal cholinergic neurons may inform what the dopaminergic neurons encode at the moment [<xref ref-type="bibr" rid="pcbi.1006285.ref020">20</xref>]. In this paper, we assume that striatal neurons can read out both motivation and teaching signals encoded by dopaminergic neurons, and we leave the details of the mechanisms by which they can be distinguished to future work.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Inspired by the OpAL model [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>], we assume that synaptic weights within the Go pathway encode positive consequences of actions, that is the positive reinforcement caused by food, drink or other appetitive stimuli obtained through actions. More precisely, we claim that the typical payoff of a particular action <italic>a</italic> in a particular situation <italic>s</italic> is encoded in the strength of the connections from the cortical neurons selective for the situation to the striatal Go neurons selective for the action. We denote these weights by <italic>G</italic>(<italic>s</italic>, <italic>a</italic>) (see <xref ref-type="fig" rid="pcbi.1006285.g001">Fig 1</xref>), and propose that after learning, the weights <italic>G</italic> represent the mean payoff for an action. Mathematically, the collective strength of the weights <italic>G</italic> corresponds to a single, non-negative number. The negative consequences, on the other hand, are encoded in the synaptic connections of striatal No-Go neurons. Negative consequences should be understood as the negative reinforcement induced by aversive stimuli such as pain, effort or disgust. We denote their weights by <italic>N</italic>(<italic>s</italic>, <italic>a</italic>), and propose that after learning, they represent the mean cost of an action. Just as with <italic>G</italic>, we mathematically represent the collective strength of the weights <italic>N</italic> by a single, non-negative number.</p>
<p>To learn the positive and negative consequences of actions respectively, the striatal neurons can take advantage of the fact that these consequences typically occur in different moments in time. Let us consider a situation in which an animal performs an action that involves an effort in order to obtain a reward: <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2a</xref> sketches a task in which a rat is given the opportunity to press a lever in order to obtain a food pellet. Due to the effort, the instantaneous reinforcement during the course of this action is negative at first, while pressing the lever. Then, it turns positive at the time the payoff is received. <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2b</xref> sketches the resulting changes in the synaptic weights. The leftmost display shows the initial weights. While making an effort to perform an action, the outcome prediction error is negative. Similarly as in previous models [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref012">12</xref>], we assume that the negative prediction error results in a strengthening of <italic>N</italic> (compare the red arrows in the middle and the left displays in <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2b</xref>). This allows the weights <italic>N</italic> to encode negative consequences. Later, reception of the payoff causes a positive prediction error, which strengthens <italic>G</italic>. This leads the weights <italic>G</italic> to encode the positive consequences. Here, we assumed that–at baseline dopamine level–positive prediction errors trigger more plasticity in the Go pathway than in the No-Go pathway, while negative prediction errors affect the No-Go pathway more than the Go pathway. In Discussion, we will review data suggesting that the properties of D1 and D2 receptors allow this assumption. Generally, if an experience involves both positive and negative consequences, both weights are increased during the experience (compare the right and the left displays in <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2b</xref>).</p>
<fig id="pcbi.1006285.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Qualitative description of learning payoffs and costs.</title>
<p>(a) Operant conditioning chamber setup: a rat obtains a food pellet by pressing a lever. (b) Diagrams of changes in the weights <italic>G</italic> and <italic>N</italic> associated with lever-pressing at each stage of the experience presented in panel (a). In all diagrams, the black circles represent the cortical neurons selective for the state (being in the operant box), and the green and red circles represent the Go and No-Go populations of striatal neurons, respectively, selective for the action (pressing the lever). The thickness of the arrows linking the circles represents the connection strength between the respective neuron populations. The blue shading in the background indicates the strength of the immediate reinforcement, with a colour intensity proportional to the magnitude of reinforcement.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g002" xlink:type="simple"/>
</fig>
<p>To mathematically implement these ideas, we need to model the weighs of the Go pathway <italic>G</italic>(<italic>s</italic>, <italic>a</italic>), the weighs of the No-Go pathway <italic>N</italic>(<italic>s</italic>, <italic>a</italic>), and the prediction error. The outcome prediction error, which we denote by <italic>δ</italic>, quantifies the difference between the expected reinforcement and the received reinforcement <italic>r</italic> after executing action <italic>a</italic> in situation <italic>s</italic>. If <italic>r</italic> is negative, we shall speak of cost, and when <italic>r</italic> is positive, we shall speak of payoff or reward. The expected reinforcement, on the other hand, directly corresponds to the expected payoffs and costs, which–according to our theory–are represented by the synaptic weights <italic>G</italic> and <italic>N</italic>. We take the expected reinforcement to be the average over the expected payoff and the expected cost. Altogether, we model the outcome prediction error as
<disp-formula id="pcbi.1006285.e001"><alternatives><graphic id="pcbi.1006285.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>δ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>r</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula></p>
<p>It should be clarified that this definition of the prediction error differs from one in the original model [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>], in that we introduced here a factor 1/2. This factor allows <italic>G</italic> and <italic>N</italic> to converge to the exact payoffs and cost, and not to values proportional payoffs and costs, and hence increases the clarity of the exposition. However, since value cannot be measured directly, the overall scaling of values through this factor is not observable, but a mere convention.</p>
<p>Equipped with the quantities <italic>δ</italic>, <italic>G</italic> and <italic>N</italic>, we can formulate our theory of learning payoff and cost. To present the theory, we simply describe how the collective connection strengths <italic>G</italic>(<italic>s</italic>, <italic>a</italic>) and <italic>N</italic>(<italic>s</italic>, <italic>a</italic>) change when a prediction error <italic>δ</italic> is received after executing action <italic>a</italic> in situation <italic>s</italic>; we use Δ<italic>G</italic>(<italic>s</italic>, <italic>a</italic>) and Δ<italic>N</italic>(<italic>s</italic>, <italic>a</italic>) to denote the changes in relevant connection strengths. Note that any update only applies if the resulting weights are still positive—if an update would render a weight negative, that weight is set to zero instead. In all other cases, we follow Mikhael and Bogacz [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>] in prescribing
<disp-formula id="pcbi.1006285.e002"><alternatives><graphic id="pcbi.1006285.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>G</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo><mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>G</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
<disp-formula id="pcbi.1006285.e003"><alternatives><graphic id="pcbi.1006285.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo><mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>α</italic> is the learning rate, <italic>ϵ</italic> is the slope parameter and λ the decay rate. The slope parameter <italic>ϵ</italic> controls the strength of the nonlinearity exhibited by the function <italic>f</italic><sub><italic>ϵ</italic></sub>, which we introduce in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3d and 3e</xref>. The nonlinearity of the function <italic>f</italic><sub><italic>ϵ</italic></sub> accounts for the fact that positive and negative prediction errors affect the weights differently. From here on, we drop the dependency of <italic>G</italic> and <italic>N</italic> on <italic>a</italic> and <italic>s</italic> to simplify notation. The dependency is still implicitly assumed unless stated otherwise.</p>
<fig id="pcbi.1006285.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The incremental construction of the learning rules.</title>
<p>(a)–(c) The different stages in the construction of the learning rules. All panels feature a mathematical formulation of the rules at the given stage and a simulation of these rules. The reinforcements in those simulations, indicated by black dots, alternate between a fixed payoff of magnitude 20 and a fixed cost of −20. The Go weights <italic>G</italic> are depicted in green, the negative No-Go weights −<italic>N</italic> are depicted in red. The parameters used in the simulations were <italic>α</italic> = 0.300, <italic>ϵ</italic> = 0.443 and λ = 0.093. (d)–(f) Definition, visualization and properties of the nonlinear function <italic>f</italic><sub><italic>ϵ</italic></sub>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g003" xlink:type="simple"/>
</fig>
<p>There is a normative intuition for each term in the rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>. These intuitions are most easily gained by following through a couple of steps to reconstruct the rules from scratch. We may start by observing that several models of learning in Go and No-Go neurons assume the effect of the prediction error on <italic>G</italic> to be opposite to its effect on <italic>N</italic> [<xref ref-type="bibr" rid="pcbi.1006285.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]. We thus propose that Δ<italic>G</italic> and Δ<italic>N</italic> might simply be proportional to the prediction error and its negative, respectively. To see whether this proposal works, we formulate it mathematically and simulate the learning of an alternating sequence of costs −<italic>n</italic> and payoffs <italic>p</italic>. <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3a</xref> shows both the mathematical formulation and the simulation. There is a problem: the strengthening of <italic>N</italic> due to negative prediction error, caused by the cost, is always immediately reversed by the following positive prediction error caused by the payoff. The same is true for the changes in <italic>G</italic>. As illustrated by the simulation, there is no net effect of learning.</p>
<p>To overcome this problem, we proceed by damping the impact of negative prediction errors (which are usually caused by costs) on <italic>G</italic>, and the impact of positive prediction errors on <italic>N</italic>, by introducing a nonlinear transformation of the prediction errors. This transformation is given in form of a piecewise-linear function <italic>f</italic><sub><italic>ϵ</italic></sub>, defined and depicted in panels d and e of <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3</xref>. The transformation leaves positive prediction errors invariant (<italic>f</italic><sub><italic>ϵ</italic></sub> (<italic>δ</italic>) is just the identity for <italic>δ</italic> &gt; 0) but reduces the impact of negative prediction errors by scaling them down (for <italic>δ</italic> &lt; 0, <italic>f</italic><sub><italic>ϵ</italic></sub> (<italic>δ</italic>) is linear with slope <italic>ϵ</italic> &lt; 1). Hence, <italic>f</italic><sub><italic>ϵ</italic></sub> introduces a pathway-specific imbalance between learning from positive prediction errors and learning from negative prediction errors (which, as we point in Discussion, is in accordance with the properties of dopaminergic receptors on these pathways). For the construction at hand, it is also logical, since costs should not alter the estimate <italic>G</italic> of the payoffs and vice versa. Such damping can be achieved by replacing the simple proportionality to <italic>δ</italic> in the first proposal by a nonlinear dependence, mediated by the functions depicted in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3e</xref>. We update our mathematical formulation accordingly, and again simulate the effects of the previously used reinforcement sequence—both these steps are illustrated in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3b</xref>. The simulation shows that, while producing the appropriate tendencies, these rules cause unconstrained, ongoing strengthening of both connections. Such dynamics are neither biologically plausible nor useful to infer the actual payoff and cost.</p>
<p>Finally, to stop unconstrained strengthening and stabilize the weighs, we balance growth with decay. Adding decay terms to the mathematical formulation of the rules yields their final form Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>. The simulation in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3c</xref> suggests that the construction was successful: the final version of the rules allows the weights to converge to <italic>p</italic> and <italic>n</italic> respectively.</p>
<sec id="sec003">
<title>Mathematical analysis</title>
<p>After providing an intuitive understanding of the learning rules and their mathematical formulation, we proceed to a more rigorous analytical treatment. We saw the potential of Mikhael and Bogacz’ [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>] rules to learn payoffs and costs. Appropriate choice of parameters is key to unlock that potential, and we shall now investigate how that choice must be made. In particular, we will derive certain relations between parameters that must be satisfied for payoff and cost to be learned.</p>
<p>Originally, the rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> were meant to describe learning of reinforcements statistics. Mikhael and Bogacz [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>] showed that after learning, particular combinations of <italic>G</italic> and <italic>N</italic> will encode the mean <inline-formula id="pcbi.1006285.e004"><alternatives><graphic id="pcbi.1006285.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and the mean spread <inline-formula id="pcbi.1006285.e005"><alternatives><graphic id="pcbi.1006285.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>R</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the received reinforcements. For further reference, we denote these important statistics by <inline-formula id="pcbi.1006285.e006"><alternatives><graphic id="pcbi.1006285.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>q</mml:mi> <mml:mo>≔</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006285.e007"><alternatives><graphic id="pcbi.1006285.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mi>s</mml:mi> <mml:mo>≔</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. How are the mean and the mean spread of received reinforcements related to payoff and cost? Consider the reinforcement statistics of an action that reliably requires effort (corresponding to negative reinforcement) to produce a payoff (which corresponds to positive reinforcement). Assume that these reinforcements are clearly negative and positive respectively, such that one will not be confused for the other even in the presence of noise. Repeat that action multiple times, and record all received reinforcements, the costs as well as the payoffs. Finally, analyze how all these received reinforcements are distributed. If an effort was required to earn the payoff, the distribution of reinforcements will turn out bimodal, as schematically shown in <xref ref-type="fig" rid="pcbi.1006285.g004">Fig 4</xref>. It features two peaks, one centered around the mean payoff <italic>p</italic>, and one centered around the mean cost −<italic>n</italic>, respectively. <xref ref-type="fig" rid="pcbi.1006285.g004">Fig 4</xref> also shows the mean <italic>q</italic> and the mean spread <italic>s</italic> of that distribution. We observe that payoffs and costs are both exactly one mean spread <italic>s</italic> away from the center <italic>q</italic> of the distribution—the payoff above, and the cost below. This implies that there is, at least in this representative case, a strong connection between payoffs and costs and the reinforcement statistics:
<disp-formula id="pcbi.1006285.e008"><alternatives><graphic id="pcbi.1006285.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>p</mml:mi><mml:mrow><mml:mo>=</mml:mo> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
<disp-formula id="pcbi.1006285.e009"><alternatives><graphic id="pcbi.1006285.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo> <mml:mi>n</mml:mi><mml:mo>=</mml:mo> <mml:mi>q</mml:mi> <mml:mo>−</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula></p>
<fig id="pcbi.1006285.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The relation of reinforcement statistics to payoff and cost.</title>
<p>The graph shows a representative reinforcement distribution over the magnitude <italic>r</italic> of all received reinforcements. The parts of the distribution that indicate negative reinforcements (costs) are colored red, while the parts that indicate positive reinforcements (payoffs) are colored green. The mean <italic>q</italic> and the mean spread <italic>s</italic> are indicated above the distribution, the mean cost −<italic>n</italic> and the mean payoff <italic>p</italic> are indicated below the distribution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g004" xlink:type="simple"/>
</fig>
<p>This connection allows us to set up conditions for the result of learning: if <italic>G</italic> and <italic>N</italic> are to represent payoff and cost, they must approach <italic>q</italic> + <italic>s</italic> and −<italic>q</italic> + <italic>s</italic> respectively. Equivalently, we can ask for 1/2(<italic>G</italic> − <italic>N</italic>) and 1/2(<italic>G</italic> + <italic>N</italic>) to approach <italic>q</italic> and <italic>s</italic> in the course of learning.</p>
<p>After revealing the link between reinforcement statistics and payoff and cost, we are ready to derive the relations necessary to learn the latter. To that end, we first determine the connection strengths <italic>G</italic> and <italic>N</italic> that result from training on stochastic reinforcements. Such uncertain reinforcements are sampled at random from a fixed distribution. Then, we implement the newly identified conditions, demanding for 1/2(<italic>G</italic> − <italic>N</italic>) to approximate <italic>q</italic> and 1/2(<italic>G</italic> + <italic>N</italic>) to approximate <italic>s</italic> after training is finished. From these conditions, we will be able to derive the desired parameter relations.</p>
<p>Working through these steps is simpler after changing variables from <italic>G</italic> and <italic>N</italic> to <italic>Q</italic> ≔ 1/2(<italic>G</italic> − <italic>N</italic>) and <italic>S</italic> ≔ 1/2(<italic>G</italic> + <italic>N</italic>) right away. We saw that the new variables <italic>Q</italic> and <italic>S</italic> have a clear computational interpretation: if learning goes as planned, <italic>Q</italic> and <italic>S</italic> track the mean <italic>q</italic> and the mean spread <italic>s</italic> of the experienced reinforcement. To determine how <italic>Q</italic> and <italic>S</italic> change due to prediction errors <italic>δ</italic>, we simply add and subtract the update rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>. Certain convenient properties of the nonlinear functions <italic>f</italic><sub><italic>ϵ</italic></sub> help to further simplify the resulting equations: <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3f</xref> shows that subtracting and adding functions depicted in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3e</xref> give functions proportional to identity and absolute value, respectively. Explicitly, <italic>f</italic><sub><italic>ϵ</italic></sub>(<italic>x</italic>) − <italic>f</italic><sub><italic>ϵ</italic></sub>(−<italic>x</italic>) = (1 + <italic>ϵ</italic>)<italic>x</italic> and <italic>f</italic><sub><italic>ϵ</italic></sub>(<italic>x</italic>) + <italic>f</italic><sub><italic>ϵ</italic></sub>(−<italic>x</italic>) = (1 − <italic>ϵ</italic>) |<italic>x</italic>|. Exploiting these properties, we obtain
<disp-formula id="pcbi.1006285.e010"><alternatives><graphic id="pcbi.1006285.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>δ</mml:mi> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
<disp-formula id="pcbi.1006285.e011"><alternatives><graphic id="pcbi.1006285.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>S</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Here, for brevity of notation, we introduced the effective learning rates <italic>α</italic><sub><italic>Q</italic></sub> = <italic>α</italic>(1 + <italic>ϵ</italic>)/2 and <italic>α</italic><sub><italic>S</italic></sub> = <italic>α</italic>(1 − <italic>ϵ</italic>)/2. Note that the changes of <italic>Q</italic> and <italic>S</italic> are proportional either to the prediction error itself or to its absolute value, in contrast to the changes of <italic>G</italic> and <italic>N</italic>.</p>
<p>Now, let us determine the strengths of the weights <italic>G</italic> and <italic>N</italic>, or equivalently of the variables <italic>Q</italic> and <italic>S</italic>, after many encounters with an action. When learning the reinforcements of a previously unknown action, <italic>Q</italic> and <italic>S</italic> typically change a lot during the first trials. These changes then get smaller and smaller as more experience is integrated—the learning curve plateaus. After enough trials, <italic>Q</italic> and <italic>S</italic> stop changing systematically, and start to merely fluctuate about some constant values, which we denote by <italic>Q</italic>* and <italic>S</italic>* and refer to as equilibrium points. In mathematical terms, directed learning stops when we may expect <italic>Q</italic> and <italic>S</italic> to remain unchanged by another trial, i.e. when <inline-formula id="pcbi.1006285.e012"><alternatives><graphic id="pcbi.1006285.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>S</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. If that stage is reached, the equilibrium points can be inferred by computing the mean value of the fluctuating variables: <inline-formula id="pcbi.1006285.e013"><alternatives><graphic id="pcbi.1006285.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>Q</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006285.e014"><alternatives><graphic id="pcbi.1006285.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>S</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. With these identities and the learning rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e010">6</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e011">7</xref>, we can determine the equilibrium points <italic>Q</italic>* and <italic>S</italic>*:
<disp-formula id="pcbi.1006285.e015"><alternatives><graphic id="pcbi.1006285.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>[</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>q</mml:mi> <mml:mo>−</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
<disp-formula id="pcbi.1006285.e016"><alternatives><graphic id="pcbi.1006285.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>Δ</mml:mo> <mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>[</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:mi>S</mml:mi> <mml:mo>]</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula></p>
<p>To solve these equations, we shall make the additional assumption that the fluctuations of <italic>Q</italic> about <italic>Q</italic>* are small. This assumption is justified whenever <italic>α</italic> is sufficiently small, and allows us to approximate <inline-formula id="pcbi.1006285.e017"><alternatives><graphic id="pcbi.1006285.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>≈</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mspace width="0.166667em"/><mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Collecting all those intermediate results, we may solve Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e015">8</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e016">9</xref> for the equilibrium points. The solutions read
<disp-formula id="pcbi.1006285.e018"><alternatives><graphic id="pcbi.1006285.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
<disp-formula id="pcbi.1006285.e019"><alternatives><graphic id="pcbi.1006285.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
with <italic>c</italic><sub><italic>Q</italic></sub> = <italic>α</italic><sub><italic>Q</italic></sub>/(<italic>α</italic><sub><italic>Q</italic></sub> + λ) and <italic>c</italic><sub><italic>S</italic></sub> = <italic>α</italic><sub><italic>S</italic></sub>/λ. Those are the approximate values of <italic>Q</italic> and <italic>S</italic> after learning.</p>
<p>Next, we need to implement the conditions we inferred from <xref ref-type="fig" rid="pcbi.1006285.g004">Fig 4</xref>. Thanks to our choice of variables, this simply amounts to requiring that <italic>Q</italic> converge to the mean reinforcement <italic>q</italic>, and <italic>S</italic> to the mean spread <italic>s</italic>, i.e. requiring <italic>Q</italic>* = <italic>q</italic> and <italic>S</italic>* = <italic>s</italic>. Inserting the approximate values from Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e018">10</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e019">11</xref> produced by the learning rules, we obtain
<disp-formula id="pcbi.1006285.e020"><alternatives><graphic id="pcbi.1006285.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>q</mml:mi><mml:mo>=</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
<disp-formula id="pcbi.1006285.e021"><alternatives><graphic id="pcbi.1006285.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo><mml:mo>=</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p>These equations are central to this publication. Their left-hand side represents the result of learning according to Mikhael and Bogacz’ [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>] rules. Their right-hand side specifies what needs to be learned if <italic>G</italic> and <italic>N</italic> really represented payoffs and costs, as Collins and Frank hypothesized [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]. Equating the left-hand and the right-hand side amounts to merging both theories. It allows us to determine how the parameters would be related if both theories were exactly true: for Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e020">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e021">13</xref> to hold, <italic>α</italic>, λ and <italic>ϵ</italic> must take values such that <italic>c</italic><sub><italic>Q</italic></sub> = 1 and <italic>c</italic><sub><italic>S</italic></sub> = 1.</p>
<p>This result evokes several questions: Is it at all possible to satisfy the derived conditions? What do the conditions mean with respect to the parameters <italic>α</italic>, λ and <italic>ϵ</italic>? And finally, is there a practical way to determine sets of parameters <italic>α</italic>, λ and <italic>ϵ</italic> which—at least approximately—satisfy the conditions? We discuss each of these questions in the following paragraphs.</p>
<p>Firstly, is it possible to satisfy <italic>c</italic><sub><italic>Q</italic></sub> = 1 and <italic>c</italic><sub><italic>S</italic></sub> = 1 exactly? Examining the definition <italic>c</italic><sub><italic>Q</italic></sub> = <italic>α</italic><sub><italic>Q</italic></sub>/(<italic>α</italic><sub><italic>Q</italic></sub> + λ) quickly reveals that letting <italic>c</italic><sub><italic>Q</italic></sub> → 1 would amount to letting λ → 0. To see why this is the case, consider that <italic>c</italic><sub><italic>Q</italic></sub> → 1 amounts to λ/<italic>α</italic><sub><italic>Q</italic></sub> → 0. However, <italic>α</italic><sub><italic>Q</italic></sub> is an effective learning rate, and so must take values smaller then one. Thus, we really need to let λ → 0. Now, we derived above that after learning, <italic>S</italic> will fluctuate about its equilibrium point <inline-formula id="pcbi.1006285.e022"><alternatives><graphic id="pcbi.1006285.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with <italic>c</italic><sub><italic>S</italic></sub> = <italic>α</italic><sub><italic>S</italic></sub>/λ. In order to keep the equilibrium point <italic>S</italic>* finite as λ → 0, we would therefore be forced to have <italic>α</italic><sub><italic>S</italic></sub> → 0 also. This, though, would pose a real problem: <italic>α</italic><sub><italic>S</italic></sub> is the effective learning rate for <italic>S</italic>—having it vanish would imply stopping learning in <italic>S</italic> all together. We must conclude that strict satisfaction of the constraints <italic>c</italic><sub><italic>Q</italic></sub> = 1 and <italic>c</italic><sub><italic>S</italic></sub> = 1 is not compatible with non-vanishing learning rates that lead to a finite equilibrium. Specifically, <italic>c</italic><sub><italic>Q</italic></sub> = 1 can only ever hold approximately if the spread <italic>s</italic> is to be learned in finite time. Nevertheless, no such problem arises when <italic>c</italic><sub><italic>S</italic></sub> is set to 1 exactly.</p>
<p>Now, what do the constraints <italic>c</italic><sub><italic>Q</italic></sub> ≈ 1 and <italic>c</italic><sub><italic>S</italic></sub> = 1 mean in terms of the parameters <italic>α</italic>, λ and <italic>ϵ</italic>? In the previous paragraph, we saw that <italic>c</italic><sub><italic>Q</italic></sub> ≈ 1 is equivalent to λ/<italic>α</italic><sub><italic>Q</italic></sub> ≈ 0. Since both λ (a decay constant) and <italic>α</italic><sub><italic>Q</italic></sub> (an effective learning rate) are inherently positive, we may rewrite this as λ/<italic>α</italic><sub><italic>Q</italic></sub> ≪ 1. Inserting the definition <italic>α</italic><sub><italic>Q</italic></sub> = <italic>α</italic>(1 + <italic>ϵ</italic>)/2 immediately yields
<disp-formula id="pcbi.1006285.e023"><alternatives><graphic id="pcbi.1006285.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn> <mml:mo>λ</mml:mo> <mml:mo>≪</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
<p>The other condition, <italic>c</italic><sub><italic>S</italic></sub> = 1, is easily translated analogously. We need only use the definitions <italic>c</italic><sub><italic>S</italic></sub> = <italic>α</italic><sub><italic>S</italic></sub>/λ and <italic>α</italic><sub><italic>S</italic></sub> = <italic>α</italic>(1 − <italic>ϵ</italic>)/2 to obtain
<disp-formula id="pcbi.1006285.e024"><alternatives><graphic id="pcbi.1006285.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn> <mml:mo>λ</mml:mo> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula></p>
<p>Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e023">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e024">15</xref> constitute the exact relations between the parameters <italic>α</italic>, λ and <italic>ϵ</italic> that need to hold for payoffs and costs to be estimated accurately. They cannot be further simplified, but we may use them to gain some more insight into the required magnitudes of the individual parameters: by substituting 2λ according to <xref ref-type="disp-formula" rid="pcbi.1006285.e024">Eq 15</xref> on the right-hand side of <xref ref-type="disp-formula" rid="pcbi.1006285.e023">Eq 14</xref>, one obtains a condition of the form 1 − <italic>ϵ</italic> ≪ 1 + <italic>ϵ</italic>. Now, given that the intended range for <italic>ϵ</italic> is [0, 1], one quickly reaches the conclusion that <italic>ϵ</italic> ≈ 1. Reinserting this into <xref ref-type="disp-formula" rid="pcbi.1006285.e023">Eq 14</xref> yields λ ≪ <italic>α</italic>. In conclusion, we found that it is necessary (though not sufficient) for accurate learning of payoffs and costs to maintain a small, but non vanishing nonlinearity <italic>ϵ</italic> in the transmission of the prediction error signal, as well as a non vanishing decay rate λ, which is much smaller than the learning rate <italic>α</italic>.</p>
<p>Finally, how can such parameters <italic>α</italic>, λ and <italic>ϵ</italic> practically be determined? To implement the conditions <italic>c</italic><sub><italic>Q</italic></sub> ≈ 1 and <italic>c</italic><sub><italic>S</italic></sub> = 1, one can for instance express λ and <italic>ϵ</italic> in terms of <italic>α</italic>, <italic>c</italic><sub><italic>Q</italic></sub> and <italic>c</italic><sub><italic>S</italic></sub>. It is tedious, but without conceptual difficulty to invert the definitions of <italic>c</italic><sub><italic>Q</italic></sub> and <italic>c</italic><sub><italic>S</italic></sub> in order to yield <italic>ϵ</italic> = (1 − <italic>c</italic><sub><italic>S</italic></sub>(1/<italic>c</italic><sub><italic>Q</italic></sub> − 1))/(1 + <italic>c</italic><sub><italic>S</italic></sub>(1/<italic>c</italic><sub><italic>Q</italic></sub> − 1)) and λ = <italic>α</italic>(1 − <italic>ϵ</italic>)/(2<italic>c</italic><sub><italic>S</italic></sub>). Then, one chooses <italic>α</italic> freely at one’s convenience, and <italic>c</italic><sub><italic>Q</italic></sub> and <italic>c</italic><sub><italic>S</italic></sub> close (or, in case of <italic>c</italic><sub><italic>S</italic></sub>, equal) to one. Importantly, <italic>c</italic><sub><italic>Q</italic></sub> must be chosen smaller then one to result in a positive λ. From these choices, one finally obtains <italic>ϵ</italic> and λ to work with the chosen <italic>α</italic>. Our simulations suggest that even values such as <italic>c</italic><sub><italic>Q</italic></sub> = 0.7 and <italic>c</italic><sub><italic>S</italic></sub> = 0.9, in combination with a learning rate of, say <italic>α</italic> = 0.3, are close enough to one to allow reasonably accurate estimations of payoff and cost. This can be seen in <xref ref-type="fig" rid="pcbi.1006285.g003">Fig 3</xref>: the simulations shown in there used those exact settings, which equivalently means that <italic>ϵ</italic> = 0.443 and λ = 0.093.</p>
<p>In summary, we used a statistical argument–the connection between payoffs and costs and the reinforcement statistics–to determine conditions under which payoffs and costs can be learned with the update rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>.</p>
</sec>
<sec id="sec004">
<title>Deterministic reinforcement sequences</title>
<p>In the preceding section, we derived relations that are necessary for successful learning of payoff and cost. If rewards are awarded stochastically, those relations are also sufficient for successful learning. But what happens to the weighs <italic>G</italic> and <italic>N</italic> if the received reinforcements follow a strong pattern? Assume, for instance, that an action reliably yields a fixed cost −<italic>n</italic> followed by a fixed payoff <italic>p</italic>. Under which additional conditions do <italic>G</italic> and <italic>N</italic> then still reflect the magnitudes of payoff and cost after learning?</p>
<p>To answer that question, we must again determine the connection strengths that result from experiencing the action time and again. Now, we do not have to rely on a probabilistic treatment—when the pattern of the reinforcements is fully known, it is possible to determine the evolution of <italic>G</italic> and <italic>N</italic> exactly. As in the previous section, we will concentrate on the result of learning rather than on its dynamics. Here, this amounts to determine the fixed points of the learning rules. These fixed points are simply those values of <italic>G</italic> and <italic>N</italic> (or equivalently of the alternative variables <italic>Q</italic> and <italic>S</italic> we defined above) that are invariant under the updates caused by the action. We denote the fixed points by <italic>G</italic>* and <italic>N</italic>*, or <italic>Q</italic>* and <italic>S</italic>*. During learning, the variables converge to their respective fixed points and cease to change notably once they arrive in their vicinity.</p>
<p>First, we focus on determining the fixed point of <italic>Q</italic>. Note that each encounter with the action yields two updates of <italic>Q</italic>: one due to the cost and one due to the payoff. Mathematically, we can formulate this as
<disp-formula id="pcbi.1006285.e025"><alternatives><graphic id="pcbi.1006285.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>cost</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>payoff</mml:mtext></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>To find <italic>Q</italic>*, demand that these successive updates have no net effect on <italic>Q</italic>: If <italic>Q</italic><sub>after action</sub> equals <italic>Q</italic><sub>before action</sub>, then <italic>Q</italic><sub>before action</sub> can rightfully be called fixed point. If this is so, the two updates must have canceled each other:
<disp-formula id="pcbi.1006285.e026"><alternatives><graphic id="pcbi.1006285.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>cost</mml:mtext></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>payoff</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula></p>
<p>This condition, in combination with the update rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>, allows to determine <italic>Q</italic>* in terms of <italic>p</italic>, <italic>n</italic> and the parameters <italic>α</italic>, <italic>ϵ</italic> and λ. First, we use the update rule <xref ref-type="disp-formula" rid="pcbi.1006285.e010">Eq 6</xref> for <italic>Q</italic> to write (Δ<italic>Q</italic>)<sub><italic>cost</italic></sub> as
<disp-formula id="pcbi.1006285.e027"><alternatives><graphic id="pcbi.1006285.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>cost</mml:mtext></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>cost</mml:mtext></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mo>−</mml:mo> <mml:mi>n</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Then, one uses the rule again to write (Δ<italic>Q</italic>)<sub>payoff</sub> as
<disp-formula id="pcbi.1006285.e028"><alternatives><graphic id="pcbi.1006285.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>payoff</mml:mtext></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mtext>payoff</mml:mtext></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>cost</mml:mtext></mml:msub> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>Q</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mrow><mml:mo>(</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mtext>cost</mml:mtext></mml:msub> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>Finally, one substitutes (Δ<italic>Q</italic>)<sub>cost</sub> from above into this expression, and then inserts (Δ<italic>Q</italic>)<sub>cost</sub> and (Δ<italic>Q</italic>)<sub>payoff</sub> into <xref ref-type="disp-formula" rid="pcbi.1006285.e026">Eq 17</xref>. Solving the equation for <italic>Q</italic><sub>before action</sub>, which in case of <xref ref-type="disp-formula" rid="pcbi.1006285.e026">Eq 17</xref> is identical to <italic>Q</italic>*, yields
<disp-formula id="pcbi.1006285.e029"><alternatives><graphic id="pcbi.1006285.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where <italic>α</italic><sub><italic>Q</italic></sub> = <italic>α</italic>(1 + <italic>ϵ</italic>)/2. Now, recall that the definition of <italic>Q</italic> in terms of <italic>G</italic> and <italic>N</italic> is <italic>Q</italic> = 1/2(<italic>G</italic> − <italic>N</italic>), and that true payoffs and costs of in this model are <italic>p</italic> and <italic>n</italic>. If <italic>G</italic> and <italic>N</italic> represented the true payoffs and costs after learning, it must be true that <italic>G</italic>* ≈ <italic>p</italic> and <italic>N</italic>* ≈ <italic>n</italic>, and thereby
<disp-formula id="pcbi.1006285.e030"><alternatives><graphic id="pcbi.1006285.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo></mml:mrow></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo> <mml:mo>≈</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>Just as Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e020">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e021">13</xref>, this equation is an interface between the results of Mikhael and Bogacz’ [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>] update rules on the left-hand side and the requirement that Go and No-Go weights encode payoffs and costs on the right-hand side. For both sides to agree, we must have
<disp-formula id="pcbi.1006285.e031"><alternatives><graphic id="pcbi.1006285.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>α</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula></p>
<p>This is a novel condition for learning the correct magnitudes of payoffs and costs from a deterministic reinforcement pattern. The definition of <italic>α</italic><sub><italic>Q</italic></sub> and the previously derived conditions in Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e023">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e024">15</xref> may be used to transform this novel condition into the simpler form <italic>α</italic> ≪ 1.</p>
<p>Next, we repeat the same analysis for <italic>S</italic>. Since we search for additional conditions on the parameters, we are free to use the original conditions in Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e023">14</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e024">15</xref> to simplify our calculations. The only complication we encounter is the appearance of <italic>Q</italic> in the update rules of <italic>S</italic>, which we resolve by substituting <italic>Q</italic> with <italic>Q</italic>*, acknowledging that the fixed points of <italic>S</italic> and <italic>Q</italic> depend on each other. We arrive at
<disp-formula id="pcbi.1006285.e032"><alternatives><graphic id="pcbi.1006285.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≈</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>Again, using the definition <italic>S</italic> = 1/2(<italic>G</italic> + <italic>N</italic>) allows comparing the result of learning with the strengths required to represent payoffs and costs. We immediately find that <italic>G</italic>* ≈ <italic>p</italic> and <italic>N</italic>* ≈ <italic>n</italic> already hold. Thus, <xref ref-type="disp-formula" rid="pcbi.1006285.e031">Eq 20</xref> is the only additional condition for successful learning of payoff and cost from reinforcements that follow a strong pattern.</p>
<p>From the results presented in this section, we conclude that the learning rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> facilitate learning of the magnitudes of fixed payoffs and costs that occur reliably one after the other. However, we also saw that this is only true if <xref ref-type="disp-formula" rid="pcbi.1006285.e031">Eq 20</xref> holds in addition to the conditions that we derived in the previous section.</p>
</sec>
<sec id="sec005">
<title>Summary of analytic results</title>
<p>The analysis above revealed the conditions under which the striatal plasticity rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> could learn the magnitudes of the payoffs and costs of actions. We identified the conditions in two different paradigms: first, we investigated learning from purely stochastic reinforcements sampled from a fixed distribution. Then, we considered a deterministic pattern of reinforcements. We obtained two key results:
<list list-type="bullet">
<list-item><p>Consider a reinforcement distribution—obtained from multiple encounters with an action—that is shaped by payoffs and cost, as the one shown in <xref ref-type="fig" rid="pcbi.1006285.g004">Fig 4</xref>. If trained on reinforcements sampled from that distribution, the plasticity rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> will enable learning of the mean payoffs and costs if
<disp-formula id="pcbi.1006285.e033"><alternatives><graphic id="pcbi.1006285.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn> <mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mo>≪</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
<disp-formula id="pcbi.1006285.e034"><alternatives><graphic id="pcbi.1006285.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn> <mml:mo>λ</mml:mo><mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
hold. These conditions imply–but do not follow from–a non-vanishing but small nonlinearity in the transmission of the prediction error, and a non-vanishing but small decay of the connection weights. Here, a small decay is characterized by a decay rate λ which is small compared to the learning rate <italic>α</italic>.</p></list-item>
<list-item><p>If trained on a pattern of reinforcements that alternates between payoffs of magnitude <italic>p</italic> and costs of magnitude <italic>n</italic>, the plasticity rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> will capture those exact payoffs and costs if, in addition to Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e033">22</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e034">23</xref>,
<disp-formula id="pcbi.1006285.e035"><alternatives><graphic id="pcbi.1006285.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>α</mml:mi> <mml:mo>≪</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
holds. In words, unbiased learning of payoffs and costs in deterministic scenarios explicitly requires a small learning rate <italic>α</italic>.</p></list-item>
</list></p>
</sec>
<sec id="sec006">
<title>Simulations of learning</title>
<p>The previous sections revealed what to expect from training the learning rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> on certain types of reinforcement. Specifically, we investigated the connection strengths <italic>G</italic> and <italic>N</italic> after many experiences of either totally predictable or totally random reinforcements. In this section, we aim to confirm and extend those results using numerical simulations rather than analytic methods.</p>
<p><xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5</xref> shows the results of simulating the gradual change of connection weights in four different tasks. In all those simulations, <italic>G</italic> and <italic>N</italic> change according to the learning rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>. The parameters we used roughly fulfill the conditions Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e020">12</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e021">13</xref> for learning of the correct magnitudes of payoffs and costs, but are also chosen to facilitate quick convergence. The values presented in <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5a</xref> mirror that compromise.</p>
<fig id="pcbi.1006285.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Simulations of learning.</title>
<p>In all graphs, the collective strength <italic>G</italic> of the Go weights is depicted in green, while the negative collective strength −<italic>N</italic> of the No-Go weights is depicted in red. The received reinforcements are indicated by solid black dots in the panels on the left, and by transparent black dots in the panels on the right. Each simulation shows how <italic>G</italic> and <italic>N</italic> change due to the reception of 30 prediction errors. Panel (a) contains a simulation based on predictable, alternating reinforcements. It also contains the parameter values used for the simulations. Panels (b) to (d) show both single and averaged simulations of stochastic reinforcements: On the left, we show a single sequence of learning, with reinforcements sampled from different distributions. On the right, we show averages over many such sequences of learning. There, the mean weights are depicted as green and red lines, while the shaded green and red areas around these lines of <italic>G</italic> and <italic>N</italic> in the right column indicate one standard deviation. The bars to the right of the averaged learning curves indicate the mean and mean spreads of the respective reinforcement distributions.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g005" xlink:type="simple"/>
</fig>
<p>The simulation in <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5a</xref> is based on a repeating an action that reliably results in a cost −<italic>n</italic>, followed by a payoff <italic>p</italic>. An analytic treatment of that case can be found in the previous sections. Both weights constantly oscillate due to the alternation of payoff and costs. This oscillating behavior is superimposed with learning curves that take the weights from their initial values towards the magnitudes of the payoffs and costs respectively. After 30 trials, <italic>G</italic> and <italic>N</italic> represent good approximations of <italic>p</italic> and <italic>n</italic>. <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5b</xref> is similar to <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5a</xref>, with a slight variation: Just as in <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5a</xref>, payoffs and costs alternate reliably. But while the cost is again held constant at −<italic>n</italic>, this time the payoff <italic>P</italic> is sampled from a fixed distribution (a normal distribution with mean <italic>p</italic> and non-vanishing variance) in each trial. Thus, the task includes both stochastic and deterministic components: each repetition of an action results in a fixed cost, which is followed by an uncertain reinforcement. The depicted simulations show that under such conditions, <italic>N</italic> eventually represents the cost <italic>n</italic>, while <italic>G</italic> converges towards the mean payoff <inline-formula id="pcbi.1006285.e036"><alternatives><graphic id="pcbi.1006285.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<p>Finally, <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5c and 5d</xref> contain simulations of repeated actions with reinforcements drawn completely at random from fixed distributions. In <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5c</xref>, the obtained reinforcements are valued either <italic>p</italic> or −<italic>n</italic>, with probabilities 1/2 each. In <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5d</xref>, reinforcements are sampled from a normal distribution with mean <italic>μ</italic><sub><italic>r</italic></sub> = 1/2(<italic>p</italic> − <italic>n</italic>) and standard deviation of <inline-formula id="pcbi.1006285.e037"><alternatives><graphic id="pcbi.1006285.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:msqrt><mml:mrow><mml:mi>π</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msqrt> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. We simulate the experience resulting from such actions by sampling reinforcements from a fixed distribution on each trial. The stochastic nature of this procedure causes the evolution of the weights <italic>G</italic> and <italic>N</italic> to be different each time the simulation is run. To overcome that effect and segregate random fluctuations from reproducible effects, we collect and average a large number of runs. Each row in <xref ref-type="fig" rid="pcbi.1006285.g005">Fig 5b–5d</xref> contains both a single run of the simulation and an average of 500 successive runs. In the above sections, we proved that in purely stochastic tasks, the weights would approximate key statistics of the reinforcement distribution after convergence. Those statistics are indeed approximated in the simulations, confirming the results of the analytic treatment above.</p>
</sec>
<sec id="sec007">
<title>Simulations of the effect of D2 blocking</title>
<p>In the previous sections, we focused on the change of the synaptic weights associated with a single action during the accumulation of experience. In this section, we redirect our attention. Instead of considering one action during learning, we now consider multiple actions after learning, and ask: can effects of dopamine depletion on choice behavior be explained in terms of payoffs versus costs?</p>
<p>In a classic experiment illustrated in <xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6a</xref>, rats were given a choice between pressing a lever in order to obtain a nutritious pellet and freely available lab chow [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]. Normal animals were willing to work for pellets, but after blocking D2 receptors with the drug haloperidol they were not any more willing to make an effort and preferred a less valuable but free option. Collins and Frank [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>] provided a mechanical explanation for this surprising effect. The theory proposed in this paper accounts for it in a conceptually similar but slightly simpler way. Here, we explain our modeling of the experiment and then describe the simulations—the differences to the account of OpAL model are pointed out in Discussion.</p>
<fig id="pcbi.1006285.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Effects of D2 blocking on the willingness to exert effort.</title>
<p>(a) Schematic illustration of the experimental setup. (b) Action selection in control state. Green and red circles on the left denote striatal Go and No-Go neurons associated with pressing the lever, while the green and red circles on the right denote the neurons associated with approaching the free food. The strengths of the synaptic connections, which result from simulated learning, are indicated by the thickness of the arrows, and the labels. The parameters used for the simulations were obtained through a fit of the model to the experimental data. The blue circle represents a population of dopaminergic neurons, and its shading indicates the level of activity. (c) Action selection in the dopamine-depleted state. The notation is the same as in panel (b), with the thick red circles indicating enhanced activity in the No-Go population, which results from blocked dopaminergic inhibition (symbolised by the smaller inhibitory projections of the dopamine neurons).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g006" xlink:type="simple"/>
</fig>
<p>To model the experiment, we need to specify how the striatal weights <italic>G</italic> and <italic>N</italic> and the motivation signal transmitted by dopamine affect the output of the basal ganglia system, and how that output then affects choice. We refer to the output of the basal ganglia as the thalamic activity, denoted by <italic>T</italic>. <italic>T</italic> depends on the cortico-striatal weights <italic>G</italic> and <italic>N</italic>, and dopaminergic motivation signal denoted by <italic>D</italic>. Even though this relationship might admittedly be complex, we restrict ourselves to just capture the signs of the dependencies by using a linear approximation:
<disp-formula id="pcbi.1006285.e038"><alternatives><graphic id="pcbi.1006285.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>=</mml:mo> <mml:mi>D</mml:mi> <mml:mi>G</mml:mi> <mml:mo>−</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo> <mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
<p>In the above equation, the first term <italic>DG</italic> corresponds to the input from the striatal Go neurons. This term is positive because the projection from striatal Go neurons to the thalamus involves double inhibitory connections (see <xref ref-type="fig" rid="pcbi.1006285.g001">Fig 1</xref>) resulting in an overall excitatory effect. The activity of the Go neurons depends on synaptic weights <italic>G</italic>. We assume that their gain is modulated by the dopaminergic input <italic>D</italic>, extrapolated from the observation of an increased slope of the firing-input relationship in the presence of dopamine in pyramidal neurons expressing D1 receptors [<xref ref-type="bibr" rid="pcbi.1006285.ref023">23</xref>]. These data are replotted in <xref ref-type="fig" rid="pcbi.1006285.g007">Fig 7a</xref>. The second term −(1 − <italic>D</italic>)<italic>N</italic> corresponds to the input from the striatal No-Go neurons. It has a negative sign because the projection form the No-Go neurons to the thalamus includes three inhibitory connections. The activity of the striatal No-Go neurons depends on their synaptic weights <italic>N</italic>, and we assume that their gain is reduced by dopamine, so the synaptic input is scaled by (1 − <italic>D</italic>). This assumption is based on data showing that agonists reduce the slope of the firing-input relationship of striatal No-Go neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref024">24</xref>], which are replotted in <xref ref-type="fig" rid="pcbi.1006285.g007">Fig 7b</xref>. Those assumptions about the impact of dopamine on the activity of striatal neurons are backed up by detailed modeling studies [<xref ref-type="bibr" rid="pcbi.1006285.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref026">26</xref>], which predict precisely that dopamine enhances activity in the Go- and inhibits activity in the No-Go pathway. In <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref>, we further assume that <italic>D</italic> ∈ [0, 1] and that the value of <italic>D</italic> = 0.5 corresponds to a baseline level of dopamine for which both striatal populations equally affect the thalamic activity.</p>
<fig id="pcbi.1006285.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g007</object-id>
<label>Fig 7</label>
<caption>
<title>fI-curves of D1- and D2-expressing neurons at different levels of receptor activation.</title>
<p>(a) fI-curves of a D1-expressing pyramidal neuron, replotted from [<xref ref-type="bibr" rid="pcbi.1006285.ref023">23</xref>]. The blue points are recorded from a neuron at a higher level of D1 receptor activation (e.g. with dopamine present), the black points are recorded at a lower level of receptor activation (e.g. without dopamine). Smooth curves have been obtained from the data through LOESS regression to serve as visual guides (black and blue lines). (b) fI-curves of a D2-expressing neuron, replotted from [<xref ref-type="bibr" rid="pcbi.1006285.ref024">24</xref>]. The blue points are recorded from a neuron at a higher level of D2 receptor activation (e.g. in the presence of the D2 agonist quinpirole), the black points are recorded from a neuron in the control group at a lower level of D2 activation (e.g. in the absence of the agonist). As in panel (a), LOESS curves (black and blue lines) have been added as visual guides.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g007" xlink:type="simple"/>
</fig>
<p>Although arising from a slightly different induction, the action value defined by <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref> is directly proportional to the action value proposed by Collins and Frank, which is defined by Eq 4 of their publication [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]: <italic>Q</italic> ∝ <italic>β</italic><sub><italic>G</italic></sub><italic>G</italic> − <italic>β</italic><sub><italic>N</italic></sub><italic>N</italic>. One easily verifies the direct proportionality of the two expressions by rewriting
<disp-formula id="pcbi.1006285.e039"><alternatives><graphic id="pcbi.1006285.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>How does thalamic activity affect choice? Again, we use a very simple dependency to capture the key aspects of that relationship: In our model of the experiment, we calculate the thalamic activity for each option. Then, we add some random noise independently to each option. Finally, all options with negative noisy thalamic activity are discarded, and the option with the highest noisy thalamic activity is chosen. If the noisy thalamic activity is negative for all available options, no choice will be made; the model defaults to staying inactive.</p>
<p>Often in similar situations, the softmax rule is the preferred choice procedure. According to that rule, one should first transform the set of different action values (or thalamic activities in this case) into a probability distribution over the available actions, by use of the softmax function. Then, one should sample an action from that distribution, and declare it the choice of that trial. Collins and Frank’s OpAL model [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>] exemplifies the use of the softmax rule.</p>
<p>We deliberately decided against this conventional approach and in favor of the above-described procedure to accommodate a certain feature of the data presented in [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]: The group with D2 antagonist differed from the control group not only in their willingness to work for food but also in their overall food consumption. The rats with D2 antagonist consumed less food in total (see <xref ref-type="fig" rid="pcbi.1006285.g008">Fig 8c</xref>). We can hope to capture this effect with our model, since it allows for the possibility to make no choice at all, and thus consume neither of the food items. A softmax decision rule, on the other hand, forces a choice on each trial, and must therefore always lead to the same number of consumed food items.</p>
<fig id="pcbi.1006285.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Frequency of choosing pellets (dark blue) and lab chow (light blue) in control and D2-blocked states.</title>
<p>The top displays (a) and (b) correspond to a condition with free pellets, while the bottom displays (c) and (d) correspond to a condition where pressing a lever was required to obtain a pellet. The left displays (a) and (c) re-plot experimental data. The values in (a) were taken from Figure 1 in the paper by Salamone et al. [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]: pellet consumption was 15.5g and 15.7g in control and D2-blocked state, while chow consumption was 0.2g and 0.8g respectively. The values in (c) were taken from Figure 4 in [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]: pellet consumption was 7.2g and 2.1g in control and D2-blocked state, while chow consumption was 3.9g and 7g respectively. The right displays (b) and (d) show the results of simulations. The parameters used to simulate learning were <italic>α</italic> = 0.1, <italic>ϵ</italic> = 0.6327 and λ = 0.0204.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g008" xlink:type="simple"/>
</fig>
<p>Finally, how does the drug haloperidol affect the thalamic activity, and hence choice? Haloperidol is a D2 antagonist; it blocks the D2 receptors on the medial spiny neurons of the No-Go pathway. This blocking reduces the (inhibiting) impact of dopamine on the activity <italic>N</italic> of that pathway. To account for this in our model, we introduce another factor <italic>κ</italic><sub><italic>N</italic></sub> ∈ [0, 1] into our expression for the thalamic activity:
<disp-formula id="pcbi.1006285.e040"><alternatives><graphic id="pcbi.1006285.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>=</mml:mo> <mml:mi>D</mml:mi> <mml:mi>G</mml:mi> <mml:mo>−</mml:mo> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>κ</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mi>D</mml:mi> <mml:mo>)</mml:mo> <mml:mi>N</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula></p>
<p>The parameter <italic>κ</italic><sub><italic>N</italic></sub> controls the how much dopamine affects the activity of the No-Go pathway <italic>N</italic>, and is hence suitable to model D2-blocking: <italic>κ</italic><sub><italic>N</italic></sub> = 1 recovers the normal thalamic activity given in <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref>, while <italic>κ</italic><sub><italic>N</italic></sub> = 0 (total blocking) fully removes the impact of dopamine on the indirect pathway, leading to completely uninhibited activity <italic>N</italic>. In the control group of the experiment, <italic>κ</italic><sub><italic>N</italic></sub> is set to 1 (no medication is administered, no blocking happens). In the group that received the medication, <italic>κ</italic><sub><italic>N</italic></sub> is a free parameter that must be fitted to the data. The best fit featured <italic>κ</italic><sub><italic>N</italic></sub> = 0.7507, corresponding to blocking of D2 receptors with an efficiency of roughly 25%.</p>
<p><xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6b</xref> illustrates how the model can account for the behaviour when the dopamine level has a normal baseline value. In the figure, the strength of the cortico-striatal connections is denoted by the labels and the thickness of arrows. Pressing the lever gives a high payoff, so the weights of Go neurons selective for this action are strong, but it also has a substantial cost, so the No-Go weights are also present. On the other hand, the free food is not particularly nutritious so the Go weights are weak, and there is no cost, so the No-Go weight is negligible. When no medication is administered, the positive and negative consequences are weighted equally, so the thalamic neurons selective for pressing the lever have overall slightly higher activity, which ultimately leads to a higher likelihood for this action to be chosen over the free option. By contrast, <xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6c</xref> shows that when the D2 receptors are blocked, costs are weighted more than payoffs, and the thalamic activity associated with pressing the lever decreases. Approaching free food has only negligible cost; therefore, the activity of thalamic neurons selective for this option is now higher, and this action is overall more likely to be chosen.</p>
<p>A quantitative fit of our model to Salamone et al.’s experimental results [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>] is illustrated in <xref ref-type="fig" rid="pcbi.1006285.g008">Fig 8</xref>. The panels on the left side in <xref ref-type="fig" rid="pcbi.1006285.g008">Fig 8</xref> summarize experimental data: the top-left display corresponds to a condition in which both high-valued pellets and the low-valued lab chow were freely available. In this case, the animals preferred pellets irrespective of the dopamine level. The bottom-left panel corresponds to the condition in which the animal had to press a lever in order to obtain a pellet, and as mentioned before, after injections of a D2 antagonist they started to prefer the lab chow.</p>
<p>In our model of the experiment, we run through a sequence of trials mimicking those illustrated in <xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6</xref>: on each trial, the model makes a choice between two actions—pressing a lever or approaching lab chow—or remains inactive. Before the main experiments, the animals were trained to press a lever to obtain rewards and were exposed to the lab chow [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]. To parallel this in simulations, the model was first trained such that it experienced each action a number of times, received corresponding payoffs and costs, and updated its weights according to Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>. The weights resulting from that learning are reported in <xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6b and 6c</xref>. Then, the model was tested with and without blocking, e.g. with <italic>κ</italic><sub><italic>N</italic></sub> a variable and <italic>κ</italic><sub><italic>N</italic></sub> fixed to one. As described in Materials and Methods, the parameters of the model were optimized to match experimentally observed behavior. As shown in the right displays in <xref ref-type="fig" rid="pcbi.1006285.g008">Fig 8</xref>, the model was able to reproduce the observed pattern of behavior. This illustrates the model’s ability to capture both learning about payoffs and costs associated with individual actions and the effects of the dopamine level on choices.</p>
</sec>
<sec id="sec008">
<title>Robustness</title>
<p>Above, we dedicated a whole section to derive conditions for the parameters of the learning rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref> to guarantee correct (i.e. unbiased) estimation of payoffs and costs. We also pointed out that these conditions cannot be satisfied exactly even in theory; in fact, our own simulations throughout this paper yield parameter settings that significantly violate the conditions. The proposed biological implementation of the rules, certainly imperfect and subject to unpredictable influences, is yet less likely to feature parameters close to the constraint surface. How robust is the presented learning algorithm under parameter detuning? How much variation around the conditions can the rules take without breaking? Here, we first describe the effect of parameter detuning on the values to which Go and No-Go weights converge. Then, we argue that the algorithm will still produce useful results even under substantial detuning of the parameters.</p>
<p>We are interested in the coding of payoffs and costs after learning, and should therefore investigate the equilibrium values <italic>G</italic>* and <italic>N</italic>* of <italic>G</italic> and <italic>N</italic>. Those equilibrium values may be obtained via combination of the equilibrium values of <italic>Q</italic>* and <italic>S</italic>* given in Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e015">8</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e019">11</xref>:
<disp-formula id="pcbi.1006285.e041"><alternatives><graphic id="pcbi.1006285.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>G</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>=</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula>
<disp-formula id="pcbi.1006285.e042"><alternatives><graphic id="pcbi.1006285.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>−</mml:mo> <mml:msup><mml:mi>S</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>|</mml:mo> <mml:mi>R</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>|</mml:mo> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mi>q</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mi>s</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p>Here, we assumed that the average spread around <italic>c</italic><sub><italic>Q</italic></sub> <italic>q</italic> is approximately equal to the average spread around <italic>q</italic>, which is a good approximation if the spread of a distribution is comparable to the mean. Next, we can use the relation of payoffs <italic>p</italic> and costs <italic>n</italic> to the statistics <italic>q</italic> and <italic>s</italic> of the reinforcement distribution they generate. These relations are given in Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e008">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e009">5</xref>; inverting and inserting those yields
<disp-formula id="pcbi.1006285.e043"><alternatives><graphic id="pcbi.1006285.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e043" xlink:type="simple"/><mml:math display="block" id="M43"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>G</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>≈</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula>
<disp-formula id="pcbi.1006285.e044"><alternatives><graphic id="pcbi.1006285.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>N</mml:mi> <mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>≈</mml:mo> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mi>n</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula></p>
<p>We observe that as long as <italic>c</italic><sub><italic>Q</italic></sub> = <italic>c</italic><sub><italic>S</italic></sub>, the Go and No-Go weights converge to the vicinity of values proportional to the payoffs and costs. Thus, as long as <italic>c</italic><sub><italic>Q</italic></sub> = <italic>c</italic><sub><italic>S</italic></sub>, the payoffs and costs are encoded separately in the two pathways.</p>
<p>Expressed in terms of the elementary parameters <italic>α</italic>, λ and <italic>ϵ</italic>, and solved for <italic>ϵ</italic>, this condition becomes
<disp-formula id="pcbi.1006285.e045"><alternatives><graphic id="pcbi.1006285.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e045" xlink:type="simple"/><mml:math display="block" id="M45"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ϵ</mml:mi> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>λ</mml:mo> <mml:mo>/</mml:mo> <mml:mi>α</mml:mi><mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msqrt> <mml:mo>−</mml:mo> <mml:mn>2</mml:mn> <mml:mo>λ</mml:mo> <mml:mo>/</mml:mo> <mml:mi>α</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>A second solution of the condition exists; however, it yields <italic>ϵ</italic> &lt; 0, which is biologically implausible. Hence, we ignore that second solution and focus our attention on <xref ref-type="disp-formula" rid="pcbi.1006285.e045">Eq 31</xref>: if λ/<italic>α</italic> is very small (i.e. if decay is weak relative to learning), then <italic>ϵ</italic> approaches one, rendering the learning rules approximately linear. If, on the other hand, λ/<italic>α</italic> is very large (i.e. decay is very strong compared to learning), then <italic>ϵ</italic> approaches zero, rendering the learning rules maximally non-linear. This relationship between <italic>ϵ</italic> and λ is not surprising; in fact, we have seen in <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2</xref> that decay is necessary to balance the unconstrained strengthening of the weighs that results from introducing the nonlinearity (compare <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2b</xref> and <xref ref-type="fig" rid="pcbi.1006285.g002">Fig 2c</xref>). <xref ref-type="disp-formula" rid="pcbi.1006285.e045">Eq 31</xref> makes this manifest: the stronger the nonlinearity (i.e. the closer <italic>ϵ</italic> gets to zero), the stronger the decay relative to learning–and vice versa.</p>
<p>Now, after investigating the effect of detuning on <italic>G</italic>* and <italic>N</italic>*, let us explore the effect of detuning on the thalamic activity <italic>T</italic>, which is the relevant output of our model as far as action selection is concerned. Substituting the above equations into the definition of thalamic activity in <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref> we obtain:
<disp-formula id="pcbi.1006285.e046"><alternatives><graphic id="pcbi.1006285.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>D</mml:mi> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msub><mml:mi>c</mml:mi> <mml:mi>Q</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:mi>D</mml:mi> <mml:msub><mml:mi>c</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula>
When <italic>c</italic><sub><italic>Q</italic></sub> = <italic>c</italic><sub><italic>S</italic></sub> ≠ 1, the thalamic activity becomes scaled by a constant <italic>c</italic><sub><italic>S</italic></sub>, but as this scaling constant is the same for all actions, the network can still select actions on the basis of payoffs and costs modulated by motivation signal <italic>D</italic>, in the same way as described in the previous subsection. Importantly, the effect of dopamine–to emphasize the payoff when increased, and emphasize the cost when decreased–is present as long as <italic>c</italic><sub><italic>S</italic></sub> &gt; 0 even if <italic>c</italic><sub><italic>Q</italic></sub> ≠ <italic>c</italic><sub><italic>S</italic></sub>. These signature effects of the proposed mechanism are thus robust even under significant detuning. However, the disadvantage of setting parameters such that <italic>c</italic><sub><italic>Q</italic></sub> ≠ <italic>c</italic><sub><italic>S</italic></sub> is that the dopaminergic motivation signal <italic>D</italic> would have a relatively smaller effect on changing the weighting between payoffs and costs; for example the payoffs or costs could no longer be ignored by setting <italic>D</italic> to its extreme values of 0 or 1. From this analysis, we may conclude that while action selection is quite robust under violation of the derived conditions, dopaminergic regulation works most effectively if the conditions are met approximately.</p>
</sec>
<sec id="sec009">
<title>An actor-critic variation</title>
<p>So far, we assumed that the outcome prediction is computed by the same striatal neurons that encode the payoffs and costs of actions. Only one network was involved: that which is responsible for the choice of action. We refer to such a network as ‘actor’ in the remainder of this exposition. In this section, we look at how the theory described above generalizes to the actor-critic framework [<xref ref-type="bibr" rid="pcbi.1006285.ref027">27</xref>]. That framework assumes that the outcome prediction is not computed by the actor, but by a separate group of striatal patch neurons called the ‘critic’. More formally, the purpose of that critic is to learn the value <italic>V</italic> of the current state.</p>
<p>One way to generalize our theory in this direction is to keep the actor network unaltered, while supplementing it with a similar critic network that learns by the very similar rules Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e002">2</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e003">3</xref>:
<disp-formula id="pcbi.1006285.e047"><alternatives><graphic id="pcbi.1006285.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e047" xlink:type="simple"/><mml:math display="block" id="M47"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula>
<disp-formula id="pcbi.1006285.e048"><alternatives><graphic id="pcbi.1006285.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mo>−</mml:mo> <mml:mi>δ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula></p>
<p>The crucial difference between the actor and the critic is that the critic network is not selective for the action, but only for the situation (note that <italic>G</italic><sub><italic>critic</italic></sub>(<italic>s</italic>) and <italic>N</italic><sub><italic>critic</italic></sub>(<italic>s</italic>) depend on <italic>s</italic>, but not on <italic>a</italic>, as opposed to <italic>G</italic><sub><italic>actor</italic></sub> (<italic>s</italic>, <italic>a</italic>) and <italic>N</italic><sub><italic>actor</italic></sub> (<italic>s</italic>, <italic>a</italic>)). It thus learns the value of a situation irrespective of the actions chosen. Importantly, the critic is in charge of supplying the outcome predictions. Those predictions are compared to the actual outcomes to produce the outcome prediction errors <italic>δ</italic> from which both networks learn.</p>
<p>We take the state value to be encoded in the difference of <italic>G</italic><sub><italic>critic</italic></sub>(<italic>s</italic>) and <italic>N</italic><sub><italic>critic</italic></sub>(<italic>s</italic>): <italic>V</italic><sub><italic>critic</italic></sub>(<italic>s</italic>) = 1/2(<italic>G</italic><sub><italic>critic</italic></sub>(<italic>s</italic>) − <italic>N</italic><sub><italic>critic</italic></sub>(<italic>s</italic>)). The change of the state value on each trial can be obtained by subtracting Eqs <xref ref-type="disp-formula" rid="pcbi.1006285.e047">33</xref> and <xref ref-type="disp-formula" rid="pcbi.1006285.e048">34</xref>:
<disp-formula id="pcbi.1006285.e049"><alternatives><graphic id="pcbi.1006285.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e049" xlink:type="simple"/><mml:math display="block" id="M49"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>α</mml:mi> <mml:mfrac><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>ε</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mo>−</mml:mo> <mml:mo>λ</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula></p>
<p>The prediction error <italic>δ</italic>—which teaches the actor as well—is the difference between the obtained reinforcement <italic>r</italic> and the reinforcement prediction by the critic:
<disp-formula id="pcbi.1006285.e050"><alternatives><graphic id="pcbi.1006285.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>δ</mml:mi> <mml:mo>=</mml:mo> <mml:mi>r</mml:mi> <mml:mo>−</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>r</mml:mi> <mml:mi>i</mml:mi> <mml:mi>t</mml:mi> <mml:mi>i</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula></p>
<p>What would be learned with that architecture? If the same action is selected on each trial, the actor will learn in exactly the same way as the critic. Then, the prediction error in the actor-critic model is the same as in the actor-only model described above, and the weights of the actor in the actor-critic model converge to exactly the same values as for the actor-only model. However, this reasoning does not seem to apply if more than one action is available: empirically, animals then select the actions that maximize their rewards in their own perception. In the process of learning, they will likely sample all available actions.</p>
<p>If such behavior generates input for an actor-critic model, the critic will integrate the experience of all those trials, and will thus represent a mixture of the expected reinforcements associated with the available actions. This generally interferes with correct learning of the payoffs and costs of the different actions. However, there is a caveat: one of the available actions will eventually prove most useful; as soon as the animal has determined that best action, it will select it in the majority of cases. That, in turn, forces the critic into mainly representing the expected reinforcement of this best action. As a final consequence, also payoff and cost of that best action are inferred correctly.</p>
<p>We confirmed this empirically for the model specified above: in <xref ref-type="fig" rid="pcbi.1006285.g009">Fig 9</xref>, we present simulations of a task in which the subject must choose between two actions. Both actions reliably yield a constant cost followed by a constant payoff each time they are selected. One of the actions is unambiguously superior to the other: its payoff is larger and its cost is lower.</p>
<fig id="pcbi.1006285.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Actor-only in comparison with actor-critic learning.</title>
<p>The columns labeled with ‘action value 1’ (panels a and d) and ‘action value 2’ (panels b and d) show the simulated evolution of the collective synaptic weights <italic>G</italic> and <italic>N</italic> of the actor network over 30 successive trials. The first row (panels a and b) shows the evolution of the actor network in an actor-only architecture, while the second row (panels d and e) provides the evolution of the actor in an actor-critic architecture. The weights <italic>G</italic> are drawn as solid green lines, the negative weights-<italic>N</italic> are drawn as solid red lines. The reinforcements obtained by choosing the respective actions are indicated by black dots. For the actor-critic simulations (second row), we additionally provide the evolution of the state value in panel c. There, the state value <italic>V</italic><sub><italic>critic</italic></sub> is represented by a solid purple line. The expected reinforcements of both actions are indicated by dashed horizontal lines. The parameter settings used in these simulations were <italic>α</italic> = 0.4, <italic>ϵ</italic> = 0.519, λ = 0.1013 and <italic>β</italic> = 0.9. The same set of parameters was used for both the actor-only and the actor-critic model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g009" xlink:type="simple"/>
</fig>
<p>Both an actor-only model and an actor-critic model interacted with that task. On each trial, an action was selected by sampling from a softmax distribution over all available actions: the probability of choosing action <italic>a</italic> in situation <italic>s</italic> was proportional to exp (<italic>βQ</italic>(<italic>s</italic>, <italic>a</italic>)), where <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>) = 1/2(<italic>G</italic>(<italic>s</italic>, <italic>a</italic>) − <italic>N</italic>(<italic>s</italic>, <italic>a</italic>)) was the action value, and <italic>β</italic> was the softmax temperature. <xref ref-type="fig" rid="pcbi.1006285.g009">Fig 9</xref> shows the temporal evolution of the involved synaptic weights over the course of learning. <xref ref-type="fig" rid="pcbi.1006285.g009">Fig 9a and 9b</xref> depict the actor-only evolution of the weights <italic>G</italic> and <italic>N</italic> that encode the payoffs and costs of actions 1 and 2, respectively. For both actions, payoffs and costs are learned correctly. Learning is notably slower for action 1. This is easily explained: action 1 is the worse of the two options and thus chosen much less frequent. In contrast, the actor-critic driven evolution of the same weights presented in <xref ref-type="fig" rid="pcbi.1006285.g009">Fig 9d and 9e</xref> leads to a correct estimate of the payoff and cost only for the superior action 1. Learning is impaired for the inferior action 2, as anticipated in the qualitative discussion above. The state value, presented in <xref ref-type="fig" rid="pcbi.1006285.g009">Fig 9c</xref>, provides further confidence in the validity of that discussion: Instead of encoding a mixture of the values of all available actions, it converges to the value of the superior action, indicated by the higher of the two dashed lines.</p>
<p>What have we learned in this section? We set out to analyze an actor-critic formulation of our model, where the feedback signal that teaches the actor is computed by a different network called the critic. We found that our formulation (which is by no means the only possible one) enables the actor to learn accurate estimates of the payoffs and costs of the most advantageous action from the critic’s feedback. The payoffs and costs of the other actions were not estimated as accurately, which was due to a sampling bias towards more rewarding options. This does not necessarily compromise behavior–after all, one may trust the model to provide accurate information on the actions that are most frequently picked, and thus to be helpful in the majority of cases. However, we believe that a more sophisticated actor-critic variant of our model could conceivably provide good estimates of the payoffs and costs of all actions. The development of this improved actor-critic variant is left to future work; here we merely demonstrate that our model is not meant to compete with actor-critic models, but rather to complement them.</p>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>This article describes how the positive and negative consequences of actions can be separately learned on the basis of a single teaching signal encoding outcome prediction error. In this section, we relate the theory with data and other models, state experimental predictions, and highlight the directions in which the theory needs to be developed further.</p>
<sec id="sec011">
<title>Relationship to experimental data</title>
<p>The model described in this paper was shown in simulations to avoid actions that require effort when the motivational signal was reduced. The unwillingness to make an effort for reward in dopamine-depleted state has also been observed in other paradigms: During a choice in a T-maze, dopamine-depleted animals were less likely to go to an arm with more pellets behind the barrier, but rather chose the arm with easily accessible but fewer pellets [<xref ref-type="bibr" rid="pcbi.1006285.ref028">28</xref>]. Parkinson’s patients were not willing to exert as much physical effort by squeezing a handle in order to obtain reward as healthy controls, especially if they were off medications [<xref ref-type="bibr" rid="pcbi.1006285.ref029">29</xref>]. These effects can be explained in an analogous way [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>] by assuming that in the dopamine-depleted state the effort of crossing the barrier or squeezing a handle is weighted more, resulting in lower activity of thalamic neurons selective for this option. Both in OpAL and the model proposed here, reducing the dopamine level reduces the tendency to choose actions involving costs, and thus changes preferences.</p>
<p>Let us now consider how the weight changes in our model relate to known data on synaptic plasticity in the striatum. <xref ref-type="fig" rid="pcbi.1006285.g010">Fig 10b</xref> illustrates the weight changes when an animal performs an action involving a cost <italic>n</italic> in order to obtain a payoff <italic>p</italic> (<xref ref-type="fig" rid="pcbi.1006285.g010">Fig 10a</xref>), e.g. pressing a lever in order to obtain a pellet. The direction of changes in <italic>G</italic> and <italic>N</italic> depending on the sign of <italic>δ</italic> are consistent with the changes of synaptic weights of Go and No-Go neurons observed at different dopamine concentrations. <xref ref-type="fig" rid="pcbi.1006285.g010">Fig 10c</xref> shows experimentally observed changes in synaptic strengths when the level of dopamine is low (displays with white background) and in the presence of agonists (blue background) [<xref ref-type="bibr" rid="pcbi.1006285.ref011">11</xref>]. Note that the directions of change match those in the corresponding displays above, in <xref ref-type="fig" rid="pcbi.1006285.g010">Fig 10b</xref>.</p>
<fig id="pcbi.1006285.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Relationship of learning rules to synaptic plasticity and receptor properties.</title>
<p>(a) Instantaneous reinforcement <italic>r</italic> when an action with effort <italic>n</italic> is selected to obtain payoff <italic>p</italic>. (b) Cortico-striatal weights before the action, after performing the action, and after obtaining the payoff. Red and green circles correspond to striatal Go and No-Go neurons, and the thickness of the lines indicates the strength of synaptic connections. The intensity of the blue background indicates the dopaminergic teaching signal at different moments of time. (c) The average excitatory post-synaptic potential (EPSP) in striatal neurons produced by cortical stimulation as a function of time in the experiment reported in [<xref ref-type="bibr" rid="pcbi.1006285.ref011">11</xref>]. The vertical black lines indicate the time when synaptic plasticity was induced by successive stimulation of cortical and striatal neurons. The amplitude of EPSPs is normalized to the baseline before the stimulation indicated by horizontal dashed lines. The green and red dots indicate the EPSPs of Go and No-Go neurons respectively. Displays with white background show the data from experiments with rat models of Parkinson’s disease, while the displays with blue background show the data from experiments in the presence of corresponding dopamine receptor agonists. The four displays re-plot the data from Figures 3E, 3B, 3F and 1H in [<xref ref-type="bibr" rid="pcbi.1006285.ref011">11</xref>]. (d) Changes in dopamine receptor occupancy. The green and red curves show the probabilities of D1 and D2 receptor occupancies in a biophysical model [<xref ref-type="bibr" rid="pcbi.1006285.ref030">30</xref>]. The two dashed blue lines in each panel indicate the levels of dopamine in dorsal (60 nM) and ventral (85 nM) striatum estimated on the basis of spontaneous firing of dopaminergic neurons using the biophysical model [<xref ref-type="bibr" rid="pcbi.1006285.ref032">32</xref>]. Displays with white and blue backgrounds illustrate changes in receptor occupancy when the level of dopamine is reduced or increased respectively.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g010" xlink:type="simple"/>
</fig>
<p>These directions of changes in striatal weights are also consistent with other models of the basal ganglia [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref012">12</xref>], but the unique prediction of the rules described in this paper is that the increase in dopaminergic teaching signal should mainly affect changes in <italic>G</italic>, while the decrease in dopamine should primarily affect <italic>N</italic>. Thus, the dopamine receptors on the Go and No-Go neurons should be most sensitive to increases and decreases in dopamine level respectively. This matches the properties of these receptors. The D2 receptors on No-Go neurons have a higher affinity and therefore are sensitive to low levels of dopamine compared to D1 receptors on Go neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref031">31</xref>]. This property is illustrated in <xref ref-type="fig" rid="pcbi.1006285.g010">Fig 10d</xref> where the green and red curves show the probabilities of D1 and D2 receptors being occupied as a function of dopamine concentration. The blue dashed lines indicate the levels of dopamine in the striatum predicted to result from the spontaneous firing of dopaminergic neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref032">32</xref>]. At these levels, most D1 receptors are deactivated. Thus the D1 receptor activation will change when the dopamine goes up, but not when it goes down, as indicated by the black arrows. This is consistent with the stronger impact of positive prediction errors on the weight changes of the Go neurons implemented in <xref ref-type="disp-formula" rid="pcbi.1006285.e002">Eq 2</xref>. By contrast, the D2 receptors are activated at baseline dopamine levels, so their activation is affected by the decreases in dopamine level but little by increases, in agreement with a stronger impact of positive prediction errors on the No-Go neurons implemented in <xref ref-type="disp-formula" rid="pcbi.1006285.e003">Eq 3</xref>.</p>
<p>Our model further requires decay of relevant weights whenever prediction errors are absent. In terms of neural implementation, this translates into mild LTD resulting from co-activation of the pre- and post-synaptic cells at baseline dopamine levels. Recently, this effect has been observed at cortico-striatal synapses in vivo [<xref ref-type="bibr" rid="pcbi.1006285.ref033">33</xref>]: in anesthetized rats, presynaptic activity followed by postsynaptic activity caused LTD in the absence of induced dopaminergic response.</p>
<p>In summary, the plasticity rules allowing learning positive and negative consequences are consistent with the observed plasticity and the receptor properties.</p>
<p>Recently, there has been a debate concerning the fundamental concept of basal ganglia function, i.e. the relationship between the Go and No-Go neurons: on one hand they have the opposite effects on a tendency to make movements [<xref ref-type="bibr" rid="pcbi.1006285.ref002">2</xref>], but on the other hand they are co-activated during action selection [<xref ref-type="bibr" rid="pcbi.1006285.ref034">34</xref>]. The presented theory is consistent with both observations: It assumes that Go and No-Go neurons have opposite effects on movement initiation. But during action selection, the basal ganglia need to calculate the utility which combines information encoded by both populations, so may require their co-activation.</p>
<p>The proposed model assumes that while an animal makes an effort, the outcome prediction error should be negative, thus the dopamine level should decrease. However, at the time of lever pressing the system needs to be energized to perform a movement, so one could expect an increased level of dopamine. Furthermore, voltammetry studies measuring dopamine concentration in the striatum did not observe a decrease in dopamine level during lever pressing [<xref ref-type="bibr" rid="pcbi.1006285.ref035">35</xref>]. Nevertheless a recent study recording activity of single dopaminergic neurons that provided a better temporal resolution reported that dopaminergic neurons increased the activity before movement, and then decreased it below baseline during movement [<xref ref-type="bibr" rid="pcbi.1006285.ref032">32</xref>]. The increase before movement may be related with energizing system for movement, while the decrease during movement may be related with representing effort.</p>
<p>In addition to effort, other negative experiences lead to phasic decreases in dopaminergic activity as well: the unexpected experience of pain [<xref ref-type="bibr" rid="pcbi.1006285.ref036">36</xref>], aversive stimuli such as air puffs [<xref ref-type="bibr" rid="pcbi.1006285.ref037">37</xref>] and, for humans, monetary losses (literal costs) [<xref ref-type="bibr" rid="pcbi.1006285.ref038">38</xref>] all coincide with decreased activity of dopamine neurons. This supports the general idea that the No-Go pathway encode costs of all kinds.</p>
</sec>
<sec id="sec012">
<title>Experimental predictions</title>
<p>A direct test of the proposed model could involve the recording of the activity of Go and No-Go neurons (e.g. with photometry) during a task in which an animal learns the payoffs and costs associated with an action. Assuming that <italic>G</italic> and <italic>N</italic> are reflected in the activity of the Go and No-Go neurons while the animal evaluates an action (i.e. just before its selection), one could analyze the changes in the activity of Go and No-Go neurons across trials. One could compare if they follow the pattern predicted by the rules given in this paper, or rather by other rules proposed to describe learning in striatal neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref007">7</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref014">14</xref>].</p>
<p>Just as the OpAL model [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>], the theory proposes that the positive and negative consequences are separately encoded by the Go and No-Go neurons which are differentially modulated by dopamine. The theory predicts that agonists specific to just one of the striatal populations change the effect of consequences encoded by this population without changing the impact of the other population. For instance, a D1 antagonist would suppress the reception of dopamine in the direct pathway. There, dopamine increases activity. Hence, the D1 antagonist would diminish the impact of the direct pathway, and therefore of learned positive consequences, on choices. However, it would not change the impact of the indirect pathway, i.e. the impact of learned negative consequences. This prediction could be tested in an experiment involving choices between options with both payoff and cost. Consider, for instance, the decision between a neutral option (<italic>p</italic> = 1, <italic>n</italic> = 1) and a high-payoff option (<italic>p</italic> = 2, <italic>n</italic> = 1). Since a D1 antagonist decreases the impact of payoffs on decisions, it should decrease the preference for the high-payoff option. On the other hand, the avoidance of a high-cost option (<italic>p</italic> = 1, <italic>n</italic> = 2) over the neutral option should not be affected by the D1 antagonist, since it does not affect the impact of costs on decisions.</p>
<p>It could also be worthwhile to investigate whether changing the influence of positive and negative consequences on choice can not only be achieved by pharmacological manipulations, but also by changing a behavioral context such as hunger, or reward rate which has been shown to affect the average dopamine level [<xref ref-type="bibr" rid="pcbi.1006285.ref019">19</xref>].</p>
<p>The theory assumes that the synaptic plasticity rules include a decay term proportional to the value of the synaptic weights themselves. Decay terms are also present in other models of learning in basal ganglia [<xref ref-type="bibr" rid="pcbi.1006285.ref015">15</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006285.ref040">40</xref>]. This class of models predicts that the synaptic weights of striatal neurons which are already high increase less during potentiation than the smaller weights (an opposite prediction is made by the OpAL model [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>], where the weights scale the prediction error in the update rule). This prediction could be tested by observing the Excitatory Post-Synaptic Currents (EPSCs) evoked at individual spines. The class of model including decay predicts that the spines with smaller evoked EPSCs before inducing plasticity should be more likely to potentiate.</p>
</sec>
<sec id="sec013">
<title>Relationship to other theories</title>
<p>The proposed model builds on the seminal work of Collins and Frank [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>], who proposed that the Go and No-Go neurons learn the tendency to execute and inhibit movements, and how the level of dopamine changes the influence of the Go and No-Go pathways on choice. The key new feature of the present model is the ability to learn both payoffs and costs associated with a single action. We demonstrated above that when the model repeatedly selects an action resulting first in a cost and then in the payoff, <italic>G</italic> and <italic>N</italic>—under certain conditions that we specified—converge to the magnitudes of that payoff and cost. This is not so in the original OpAL model, as we shall show in a brief analysis.</p>
<p>Collins and Frank [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>] demonstrated that when the environment is stationary and prediction error <italic>δ</italic> converges to zero, then the weights <italic>G</italic> and <italic>N</italic> in the OpAL model converge to bounded values. However, we will show that Go and No-Go weights converge to zero when an action that results first in a cost and then in the payoff is repeatedly selected.</p>
<p>The OpAL model is based on the actor-critic framework; hence, the prediction error is defined as in <xref ref-type="disp-formula" rid="pcbi.1006285.e050">Eq 36</xref>. The weights of the critic are modified simply as Δ<italic>V</italic> = <italic>αδ</italic>. The weights of the actor are modified according to the following equations [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>]:
<disp-formula id="pcbi.1006285.e051"><alternatives><graphic id="pcbi.1006285.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>G</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>α</mml:mi> <mml:mi>G</mml:mi> <mml:mi>δ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula>
<disp-formula id="pcbi.1006285.e052"><alternatives><graphic id="pcbi.1006285.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e052" xlink:type="simple"/><mml:math display="block" id="M52"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>N</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>−</mml:mo> <mml:mi>α</mml:mi> <mml:mi>N</mml:mi> <mml:mi>δ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula></p>
<p><xref ref-type="fig" rid="pcbi.1006285.g011">Fig 11</xref> shows how the weights change in a simulation of the OpAL model. The weights of the critic approach a value close to the average of payoff and cost. Let us consider what happens in the model once the critic weight stops changing between trials (i.e. from ∼10th trial onward in <xref ref-type="fig" rid="pcbi.1006285.g011">Fig 11</xref>). The weight of the critic still changes within a trial, i.e. decreases when cost is incurred and increases after a payoff. This happens because the prediction error oscillates around 0, i.e. it is equal to <italic>δ</italic> = −<italic>d</italic> while incurring a cost and <italic>δ</italic> = <italic>d</italic> while receiving a payoff, where <italic>d</italic> is a constant. If so, let us consider how a Go weight changes within a trial. According to <xref ref-type="disp-formula" rid="pcbi.1006285.e051">Eq 37</xref> the weight changes as follows:
<disp-formula id="pcbi.1006285.e053"><alternatives><graphic id="pcbi.1006285.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mi>d</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula>
<disp-formula id="pcbi.1006285.e054"><alternatives><graphic id="pcbi.1006285.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>payoff</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>cost</mml:mtext></mml:mrow></mml:msub> <mml:mi>d</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(40)</label></disp-formula></p>
<fig id="pcbi.1006285.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006285.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Changes in the weight <italic>G</italic> of the Go neurons, <italic>N</italic> of the No-Go neurons and <italic>V</italic> of the critic in the OpAL model over the course of simulations.</title>
<p>(a) The purple line represents the evolving critic weight. The experienced reinforcements are indicated by black dots. (b) The actor weights, represented by a green and a red line respectively, were initialized to <italic>G</italic> = <italic>N</italic> = 1. Again, the black dots indicate the received reinforcements. The simulation was run with learning rate <italic>α</italic> = 0.3.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006285.g011" xlink:type="simple"/>
</fig>
<p>Substituting <xref ref-type="disp-formula" rid="pcbi.1006285.e053">Eq 39</xref> into <xref ref-type="disp-formula" rid="pcbi.1006285.e054">Eq 40</xref> we obtain:
<disp-formula id="pcbi.1006285.e055"><alternatives><graphic id="pcbi.1006285.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>after</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>payoff</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mi>d</mml:mi> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mi>d</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:msup><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:msub><mml:mi>G</mml:mi> <mml:mrow><mml:mtext>before</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>action</mml:mtext></mml:mrow></mml:msub> <mml:msup><mml:mi>d</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula></p>
<p>We see that within a trial a Go weight decays proportionally to is value, resulting in an exponential decay across trials seen in <xref ref-type="fig" rid="pcbi.1006285.g011">Fig 11</xref>. Analogous calculations show that the No-Go weight decays in the same way. We conclude that the OpAL model is unable to estimate positive and negative consequences for actions which result in both payoffs and costs. It is worth noting that the decay of actor weights to zero demonstrated above is specific to the version of basal ganglia model proposed by Collins and Frank [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>], but would not be present in another version of the model [<xref ref-type="bibr" rid="pcbi.1006285.ref039">39</xref>] where the learning rules include a special term preventing the weights from approaching zero. On the other hand, nothing in the above calculation depended on <italic>G</italic>, <italic>N</italic> and <italic>V</italic> updating at the same learning rate <italic>α</italic>–the derivation can be carried out in exactly the same way assuming <italic>α</italic><sub><italic>V</italic></sub> ≠ <italic>α</italic><sub><italic>N</italic></sub> ≠ <italic>α</italic><sub><italic>G</italic></sub>. Hence, we may summarise that even such generalised OpAL models must fail to learn payoffs and costs of actions, irrespective of the specific parameter values unless further terms are added to the learning rules. Our analysis suggests that learning payoffs and costs can be enabled by different effective learning rates after positive versus negative feedback for Go and No-Go synapses, which in our model is achieved by setting <italic>ϵ</italic> &lt; 1.</p>
<p>To interpret this result, note that we do not claim that the OpAL model is not capable of optimizing the policy. It is set up as a policy improving algorithm, and might even reflect the payoffs and costs of actions in the weights <italic>G</italic> and <italic>N</italic> in certain situations. However, as we have shown there is also situations in which OpAL is not able to encode the payoffs and costs. In contrast, we showed above the model presented in this paper does encode payoffs and costs in any situation, given a suitable set of parameters and enough time to learn.</p>
<p>The model described in this paper has been shown to account for the effects of dopamine depletion on the willingness to make effort, which have also been simulated with the OpAL model. To simulate the effects of dopamine depletion on the choice between an arm of a T-maze with more pellets behind a barrier and an arm with fewer pellets, [<xref ref-type="bibr" rid="pcbi.1006285.ref008">8</xref>] trained a model on three separate actions: eating in the left arm, eating in the right arm, and crossing a barrier. In this way, it was ensured that each action had just payoff or just cost, and the model could learn them. Subsequently, during choice, the model was deciding between a combination of two actions (e.g. crossing a barrier and eating in the left arm) and the other action. By contrast, the model proposed in this paper was choosing just between the two options available to an animal in an analogous task (<xref ref-type="fig" rid="pcbi.1006285.g006">Fig 6</xref>), because it was able to learn both payoffs and costs associated with each option. This is a useful ability, as most real-world actions have both payoffs and costs.</p>
<p>In the original paper introducing the plasticity rules [<xref ref-type="bibr" rid="pcbi.1006285.ref016">16</xref>], it was proposed that the rules allow the Go and No-Go neurons to encode reinforcement variability because when an action results in variable reinforcements, both <italic>G</italic> and <italic>N</italic> increase during learning. It was further proposed that the tonic level of dopamine controls the tendency to make risky choices, as observed in experiments [<xref ref-type="bibr" rid="pcbi.1006285.ref041">41</xref>], because it leads to emphasizing potential gains, and under-weighting potential losses. However, here it is proposed that the striatal learning rules primarily sub-serve a function more fundamental for survival, i.e. learning payoffs and costs of actions. From this perspective, the influence of dopamine level on the tendency to make risky choices arises as a by-product of a system primarily optimized to weight payoffs and costs according to the current motivational state.</p>
</sec>
<sec id="sec014">
<title>Directions for the future work</title>
<p>There are multiple directions in which the presented theory could be extended. For example, the theory has to be integrated with the models of action selection in the basal ganglia to describe how the circuit selects the action with the best trade-off of payoffs and costs. Furthermore, the theory may be extended to describe the dependence of the dopaminergic teaching signal on the motivational state. Learning experiments in which an animal may be deprived of physiologically required substances suggest that both terms in the outcome prediction error encoded by dopamine (i.e. the reinforcement and the expected outcome) are scaled by motivation [<xref ref-type="bibr" rid="pcbi.1006285.ref042">42</xref>]. It would be interesting to incorporate such scaling in our model, where the direct pathway, as well as the indirect pathway, contribute to the outcome estimate, which is then compared to the experienced reinforcement to compute the prediction error. If dopaminergic modulation is taken into account also at this stage, the dopaminergic motivation signal should affect the outcome estimate, and hence influence learning.</p>
<p>A limitation to our current model is the rudimentary form of the basal ganglia output, given in <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref>. It is known that the effect of dopamine on the activity in the two pathways is not linear (as assumed in this paper), but exhibits saturation effects. The fact that the reception of dopamine is nonlinear plays a crucial role in the learning part of our model (the piecewise linear functions <italic>f</italic><sub><italic>ϵ</italic></sub> introduce exactly that nonlinearity), and could also be implemented at the decision-making stage, if the activity of Go and No-Go neurons (combined in <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref>) depended nonlinearly on the dopamine level. In such more elaborate formulation, the fine-tuning of the baseline dopamine level then becomes critical. Including nonlinear effects of dopamine on activity during choice would allow studying interactions between learning and decision making, which would both be affected by the position of the baseline and the strength of the nonlinearity.</p>
<p>It is intriguing to ask whether the evaluation of actions combining separately encoded positive and negative consequences is also performed by areas beyond the basal ganglia. Indeed, positive and negative associations are encoded by different populations of neurons in the amygdala [<xref ref-type="bibr" rid="pcbi.1006285.ref043">43</xref>]. Moreover, an imaging study [<xref ref-type="bibr" rid="pcbi.1006285.ref044">44</xref>] suggests that costs and payoffs are predicted by the amygdala and the ventral striatum respectively, and ultimately compared in the prefrontal cortex. Furthermore, different cortical regions preferentially project to Go or No-Go neurons [<xref ref-type="bibr" rid="pcbi.1006285.ref045">45</xref>], raising the possibility that the positive and negative consequences are also encoded separately in the cortex. Therefore, it seems promising to investigate if similar plasticity rules could also describe learning beyond the basal ganglia.</p>
</sec>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Materials and methods</title>
<p>During simulations of an experiment by Salamone et al. [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>], the model received payoff <italic>p</italic><sub>chow</sub> = 1 for approaching the lab chow, and payoff <italic>p</italic><sub>pellet</sub> for choosing a pellet. The model was simulated in two conditions differing in the cost of choosing a pellet which was equal to <italic>n</italic><sub>pellet</sub> = 0 in the free-pellet condition, and to <italic>n</italic><sub>pellet</sub> = <italic>n</italic><sub>lever</sub> in a condition requiring lever pressing to obtain a pellet. There was no cost of choosing lab chow (<italic>n</italic><sub>chow</sub> = 0) in either condition.</p>
<p>For each condition, the model was simulated in two operational modes: in the control state, the coupling <italic>κ</italic><sub><italic>N</italic></sub> of dopamine to the D2-expressing neurons was fixed at <italic>κ</italic><sub><italic>N</italic></sub> = 1 during choice (making manifest the assumed fully functioning dopaminergic modulation in the control group). Conversely, in the state corresponding to the presence of the D2-antagonist haloperidol, <italic>κ</italic><sub><italic>N</italic></sub> was treated as a variable valued in [0, 1], now allowing for impaired dopaminergic regulation. The level of dopamine <italic>D</italic> was kept fixed at <italic>D</italic> = 0.5 throughout, assuming largely an unaltered baseline level for both groups.</p>
<p>For each condition and state, the behavior of <italic>N</italic><sub><italic>rats</italic></sub> was simulated. Each simulation consisted of 180 training and 180 testing trials (as each animal in the experiment of [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>] was tested for 30 minutes, so 180 trials correspond to an assumption that a single trial took 10s). At the start of each simulation, the weights were initialized to <italic>G</italic><sub>pellet</sub> = <italic>N</italic><sub>pellet</sub> = <italic>G</italic><sub>chow</sub> = <italic>N</italic><sub>chow</sub> = 0.1. During each training trial, the model experienced choosing a pellet as well as approaching the lab chow. In detail, it received the cost <italic>n</italic><sub>pellet</sub>, modified the weights <italic>G</italic><sub>pellet</sub> and <italic>N</italic><sub>pellet</sub>, then received the payoff <italic>p</italic><sub>pellet</sub> and modified the weight again, and analogously for the lab chow. During each testing trial, the thalamic activity for each option was calculated from <xref ref-type="disp-formula" rid="pcbi.1006285.e038">Eq 25</xref>), and Gaussian noise with standard deviation <italic>σ</italic> was added. An option with the highest thalamic activity was selected, and if this activity was positive, the action was executed, resulting in the corresponding cost and payoff and weight modification. If thalamic activity for both options was negative, no action was executed and no weights were updated.</p>
<p>The values of model parameters: <italic>p</italic><sub>pellet</sub>, <italic>n</italic><sub>lever</sub>, <italic>κ</italic><sub><italic>N</italic></sub>, <italic>σ</italic> were optimized to match the choices made by the animals. In particular, for each set of parameters, the model was simulated <italic>N</italic><sub><italic>rats</italic></sub> = 100 times, and the average number of choices <inline-formula id="pcbi.1006285.e056"><alternatives><graphic id="pcbi.1006285.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> of option <italic>i</italic> in dopamine state <italic>j</italic> and experimental condition <italic>k</italic> was computed. The mismatch with corresponding consumption in experiment <inline-formula id="pcbi.1006285.e057"><alternatives><graphic id="pcbi.1006285.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> was quantified by a normalized summed squared error:
<disp-formula id="pcbi.1006285.e058"><alternatives><graphic id="pcbi.1006285.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mi>o</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:mo>(</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>Z</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mfrac> <mml:mo>−</mml:mo> <mml:mfrac><mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>Z</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mfrac><mml:msup><mml:mo>)</mml:mo> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula></p>
<p>In the above equation <inline-formula id="pcbi.1006285.e059"><alternatives><graphic id="pcbi.1006285.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msubsup><mml:mi>Z</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is a normalization term equal to the total number of choices or consumption in a particular condition:
<disp-formula id="pcbi.1006285.e060"><alternatives><graphic id="pcbi.1006285.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006285.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:msubsup><mml:mi>c</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula></p>
<p>The values of parameters minimizing the cost function were sought using the Simplex optimization algorithm implemented in Matlab, and the following values were found: <italic>p</italic><sub>pellet</sub> = 15.511751, <italic>n</italic><sub>lever</sub> = 14.510517, <italic>κ</italic><sub><italic>N</italic></sub> = 0.7507 and <italic>σ</italic> = 1.066246. Subsequently, the model with these optimized parameters was simulated with <italic>N</italic><sub><italic>rats</italic></sub> = 6, which was the number of animals tested by [<xref ref-type="bibr" rid="pcbi.1006285.ref022">22</xref>]. The resulting mean number of choices across animals are shown in <xref ref-type="fig" rid="pcbi.1006285.g008">Fig 8</xref>.</p>
</sec>
</body>
<back>
<ack>
<p>The authors wish to thank Jacqueline Pumphrey for composing a lay summary of this article.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006285.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>. <article-title>The basal ganglia: a vertebrate solution to the selection problem?</article-title> <source>Neuroscience</source>. <year>1999</year>;<volume>89</volume>:<fpage>1009</fpage>–<lpage>1023</lpage>. <object-id pub-id-type="pmid">10362291</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kravitz</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Freeze</surname> <given-names>BS</given-names></name>, <name name-style="western"><surname>Parker</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Kay</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Thwin</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Regulation of parkinsonian motor behaviours by optogenetic control of basal ganglia circuitry</article-title>. <source>Nature</source>. <year>2010</year>;<volume>466</volume>:<fpage>622</fpage>–<lpage>626</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature09159" xlink:type="simple">10.1038/nature09159</ext-link></comment> <object-id pub-id-type="pmid">20613723</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Beyan</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Shink</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bolam</surname> <given-names>JP</given-names></name>. <article-title>Microcircuitry of the direct and indirect pathways of the basal ganglia</article-title>. <source>Neuroscience</source>. <year>1998</year>;<volume>86</volume>:<fpage>353</fpage>–<lpage>388</lpage>. <object-id pub-id-type="pmid">9881853</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Surmeier</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Day</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>W</given-names></name>. <article-title>D1 and D2 dopamine-receptor modulation of striatal glutamatergic signaling in striatal medium spiny neurons</article-title>. <source>Trends Neurosci</source>. <year>2007</year>;<volume>30</volume>:<fpage>228</fpage>–<lpage>235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2007.03.008" xlink:type="simple">10.1016/j.tins.2007.03.008</ext-link></comment> <object-id pub-id-type="pmid">17408758</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Prescott</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>. <article-title>A computational model of action selection in the basal ganglia. I. A new functional anatomy</article-title>. <source>Biol Cybernetics</source>. <year>2001</year>;<volume>84</volume>:<fpage>401</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/PL00007984" xlink:type="simple">10.1007/PL00007984</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Khamassi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>. <article-title>Dopaminergic control of the exploration-exploitation trade-off via the basal ganglia</article-title>. <source>Frontiers in Neurosci</source>. <year>2012</year>;<volume>6</volume>:<fpage>9</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2012.00009" xlink:type="simple">10.3389/fnins.2012.00009</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schroll</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Vitay</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hamker</surname> <given-names>H</given-names></name>. <article-title>Dysfunctional and compensatory synaptic plasticity in Parkinson’s disease</article-title> <source>European Journal of Neuroscience</source>. <year>2014</year>;<volume>39</volume>:<fpage>688</fpage>–<lpage>702</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/ejn.12434" xlink:type="simple">10.1111/ejn.12434</ext-link></comment> <object-id pub-id-type="pmid">24313650</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Collins</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>Opponent actor learning (OpAL): Modeling interactive effects of striatal dopamine on reinforcement learning and choice incentive</article-title>. <source>Psychol Rev</source>. <year>2014</year>;<volume>121</volume>:<fpage>337</fpage>–<lpage>366</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0037015" xlink:type="simple">10.1037/a0037015</ext-link></comment> <object-id pub-id-type="pmid">25090423</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schultz</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Montague</surname> <given-names>PR</given-names></name>. <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>. <year>1997</year>;<volume>275</volume>:<fpage>1593</fpage>–<lpage>1599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.275.5306.1593" xlink:type="simple">10.1126/science.275.5306.1593</ext-link></comment> <object-id pub-id-type="pmid">9054347</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eshel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bukwich</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Uchida</surname> <given-names>N</given-names></name>. <article-title>Dopamine neurons share common response function for reward prediction error</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>:<fpage>479</fpage>–<lpage>486</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4239" xlink:type="simple">10.1038/nn.4239</ext-link></comment> <object-id pub-id-type="pmid">26854803</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Flajolet</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Greengard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Surmeier</surname> <given-names>DJ</given-names></name>. <article-title>Dichotomous dopaminergic control of striatal synaptic plasticity</article-title>. <source>Science</source>. <year>2008</year>;<volume>321</volume>:<fpage>848</fpage>–<lpage>851</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1160575" xlink:type="simple">10.1126/science.1160575</ext-link></comment> <object-id pub-id-type="pmid">18687967</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Seeberger</surname> <given-names>LC</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name>. <article-title>By carrot or by stick: cognitive reinforcement learning in parkinsonism</article-title>. <source>Science</source>. <year>2004</year>;<volume>306</volume>:<fpage>1940</fpage>–<lpage>1943</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1102941" xlink:type="simple">10.1126/science.1102941</ext-link></comment> <object-id pub-id-type="pmid">15528409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hong</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>. <article-title>Dopamine-mediated learning and switching in cortico-striatal circuit explain behavioral changes in reinforcement learning</article-title>. <source>Frontiers Behav Neurosci</source>. <year>2011</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2011.00015" xlink:type="simple">10.3389/fnbeh.2011.00015</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gurney</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>. <article-title>A new framework for cortico-striatal plasticity: behavioural theory meets in vitro data at the reinforcement-action interface</article-title>. <source>PLoS Biology</source>. <year>2015</year>;<volume>13</volume>:<fpage>e1002034</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.1002034" xlink:type="simple">10.1371/journal.pbio.1002034</ext-link></comment> <object-id pub-id-type="pmid">25562526</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yttri</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Dudman</surname> <given-names>JT</given-names></name>. <article-title>Opponent and bidirectional control of movement velocity in the basal ganglia</article-title>. <source>Nature</source>. <year>2016</year>;<volume>533</volume>(<issue>7603</issue>):<fpage>402</fpage>–<lpage>406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature17639" xlink:type="simple">10.1038/nature17639</ext-link></comment> <object-id pub-id-type="pmid">27135927</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mikhael</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>. <article-title>Learning reward uncertainty in the basal ganglia</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>:<fpage>e1005062</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005062" xlink:type="simple">10.1371/journal.pcbi.1005062</ext-link></comment> <object-id pub-id-type="pmid">27589489</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>. <article-title>Cost, benefit, tonic, phasic</article-title>. <source>Ann NY Acad Sci</source>. <year>2007</year>;<volume>1104</volume>:<fpage>357</fpage>–<lpage>376</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1196/annals.1390.018" xlink:type="simple">10.1196/annals.1390.018</ext-link></comment> <object-id pub-id-type="pmid">17416928</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dombeck</surname> <given-names>D</given-names></name>. <article-title>Rapid signalling in distinct dopaminergic axons during locomotion and reward</article-title>. <source>Nature</source>. <year>2016</year>;<volume>535</volume>:<fpage>505</fpage>–<lpage>510</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature18942" xlink:type="simple">10.1038/nature18942</ext-link></comment> <object-id pub-id-type="pmid">27398617</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hamid</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Pettibone</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Mabrouk</surname> <given-names>OS</given-names></name>, <name name-style="western"><surname>Hetrick</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Vander Weele</surname> <given-names>CM</given-names></name>, <etal>et al</etal>. <article-title>Mesolimbic dopamine signals the value of work</article-title>. <source>Nat Neurosci</source>. <year>2016</year>;<volume>19</volume>:<fpage>117</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4173" xlink:type="simple">10.1038/nn.4173</ext-link></comment> <object-id pub-id-type="pmid">26595651</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Berke</surname> <given-names>JD</given-names></name>. <article-title>What does dopamine mean?</article-title> <source>Nat Neurosci</source>. <year>2018</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41593-018-0152-y" xlink:type="simple">10.1038/s41593-018-0152-y</ext-link></comment> <object-id pub-id-type="pmid">29760524</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maia</surname> <given-names>TV</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>An integrative perspective on the role of dopamine in schizophrenia</article-title>. <source>Biological psychiatry</source>. <year>2017</year>;<volume>81</volume>:<fpage>52</fpage>–<lpage>66</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.biopsych.2016.05.021" xlink:type="simple">10.1016/j.biopsych.2016.05.021</ext-link></comment> <object-id pub-id-type="pmid">27452791</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salamone</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Steinpreis</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>McCullough</surname> <given-names>LD</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Grebel</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mahan</surname> <given-names>K</given-names></name>. <article-title>Haloperidol and nucleus accumbens dopamine depletion suppress lever pressing for food but increase free food consumption in a novel food choice procedure</article-title>. <source>Psychopharmacology</source>. <year>1991</year>;<volume>104</volume>:<fpage>515</fpage>–<lpage>521</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF02245659" xlink:type="simple">10.1007/BF02245659</ext-link></comment> <object-id pub-id-type="pmid">1780422</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thurley</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Lüscher</surname> <given-names>HR</given-names></name>. <article-title>Dopamine increases the gain of the input-output response of rat prefrontal pyramidal neurons</article-title>. <source>J Neurophysiol</source>. <year>2008</year>;<volume>99</volume>:<fpage>2985</fpage>–<lpage>2997</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.01098.2007" xlink:type="simple">10.1152/jn.01098.2007</ext-link></comment> <object-id pub-id-type="pmid">18400958</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hernández-López</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tkatch</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Perez-Garci</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Galarraga</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bargas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hamm</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Surmeier</surname> <given-names>JD</given-names></name>. <article-title>D2 dopamine receptors in striatal medium spiny neurons reduce L-Type Ca2+ currents and excitability vía a novel PLC<italic>β</italic>1–IP3–calcineurin-signaling cascade</article-title>. <source>Journal of Neuroscience</source>. <year>2000</year>;<volume>20</volume>:<fpage>8987</fpage>–<lpage>8995</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.20-24-08987.2000" xlink:type="simple">10.1523/JNEUROSCI.20-24-08987.2000</ext-link></comment> <object-id pub-id-type="pmid">11124974</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Humphries</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Lepora</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wood</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gurney</surname> <given-names>K</given-names></name>. <article-title>Capturing dopaminergic modulation and bimodal membrane behaviour of striatal medium spiny neurons in accurate, reduced models</article-title> <source>Frontiers in computational neuroscience</source>. <year>2009</year>;<volume>26</volume></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moyer</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Finkel</surname> <given-names>LH</given-names></name>. <article-title>Effects of dopaminergic modulation on the integrative properties of the ventral striatal medium spiny neuron</article-title>. <source>Journal of neurophysiology</source>. <year>2007</year>;<volume>98</volume>:<fpage>3731</fpage>–<lpage>3748</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00335.2007" xlink:type="simple">10.1152/jn.00335.2007</ext-link></comment> <object-id pub-id-type="pmid">17913980</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?</article-title> <source>Neural Networks</source>. <year>1999</year>;<volume>12</volume>:<fpage>961</fpage>–<lpage>974</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(99)00046-5" xlink:type="simple">10.1016/S0893-6080(99)00046-5</ext-link></comment> <object-id pub-id-type="pmid">12662639</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Salamone</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Correa</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yohn</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cruz</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>San Miguel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Alatorre</surname> <given-names>L</given-names></name>. <article-title>The pharmacology of effort-related choice behavior: Dopamine, depression, and individual differences</article-title>. <source>Behav Process</source>. <year>2016</year>;<volume>127</volume>:<fpage>3</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.beproc.2016.02.008" xlink:type="simple">10.1016/j.beproc.2016.02.008</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chong</surname> <given-names>TTJ</given-names></name>, <name name-style="western"><surname>Bonnelle</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Manohar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Veromann</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Muhammed</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Tofaris</surname> <given-names>GK</given-names></name>, <etal>et al</etal>. <article-title>Dopamine enhances willingness to exert effort for reward in Parkinson’s disease</article-title>. <source>Cortex</source>. <year>2015</year>;<volume>69</volume>:<fpage>40</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2015.04.003" xlink:type="simple">10.1016/j.cortex.2015.04.003</ext-link></comment> <object-id pub-id-type="pmid">25967086</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dreyer</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Herrik</surname> <given-names>KF</given-names></name>, <name name-style="western"><surname>Berg</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Hounsgaard</surname> <given-names>JD</given-names></name>. <article-title>Influence of phasic and tonic dopamine release on receptor activation</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>:<fpage>14273</fpage>–<lpage>14283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1894-10.2010" xlink:type="simple">10.1523/JNEUROSCI.1894-10.2010</ext-link></comment> <object-id pub-id-type="pmid">20962248</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Richfield</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Penney</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>AB</given-names></name>. <article-title>Anatomical and affinity state comparisons between dopamine D1 and D2 receptors in the rat central nervous system</article-title>. <source>Neuroscience</source>. <year>1989</year>;<volume>30</volume>:<fpage>767</fpage>–<lpage>777</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0306-4522(89)90168-1" xlink:type="simple">10.1016/0306-4522(89)90168-1</ext-link></comment> <object-id pub-id-type="pmid">2528080</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dodson</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Dreyer</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Jennings</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Syed</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Wade-Martins</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cragg</surname> <given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Representation of spontaneous movement by dopaminergic neurons is cell-type selective and disrupted in parkinsonism</article-title>. <source>P Natl Acad Sci USA</source>. <year>2016</year>;<volume>113</volume>:<fpage>E2180</fpage>–<lpage>E2188</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1515941113" xlink:type="simple">10.1073/pnas.1515941113</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fisher</surname> <given-names>SD</given-names></name>, <name name-style="western"><surname>Robertson</surname> <given-names>PB</given-names></name>, <name name-style="western"><surname>Black</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Redgrave</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sagar</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Abraham</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>JNJ</given-names></name>. <article-title>Reinforcement determines the timing dependence of corticostriatal synaptic plasticity in vivo</article-title>. <source>Nature communications</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>334</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-017-00394-x" xlink:type="simple">10.1038/s41467-017-00394-x</ext-link></comment> <object-id pub-id-type="pmid">28839128</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cui</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jun</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Jin</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Pham</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Vogel</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Lovinger</surname> <given-names>DM</given-names></name>, <etal>et al</etal>. <article-title>Concurrent activation of striatal direct and indirect pathways during action initiation</article-title>. <source>Nature</source>. <year>2013</year>;<volume>494</volume>:<fpage>238</fpage>–<lpage>242</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature11846" xlink:type="simple">10.1038/nature11846</ext-link></comment> <object-id pub-id-type="pmid">23354054</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Syed</surname> <given-names>EC</given-names></name>, <name name-style="western"><surname>Grima</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Magill</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Walton</surname> <given-names>ME</given-names></name>. <article-title>Action initiation shapes mesolimbic dopamine encoding of future rewards</article-title>. <source>Nature Neurosci</source>. <year>2016</year>;<volume>19</volume>:<fpage>34</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4187" xlink:type="simple">10.1038/nn.4187</ext-link></comment> <object-id pub-id-type="pmid">26642087</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ungless</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Magill</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Bolam</surname> <given-names>JP</given-names></name>. <article-title>Uniform inhibition of dopamine neurons in the ventral tegmental area by aversive stimuli</article-title>. <source>Science</source>. <year>2004</year>;<volume>303</volume>:<fpage>2040</fpage>–<lpage>2042</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1093360" xlink:type="simple">10.1126/science.1093360</ext-link></comment> <object-id pub-id-type="pmid">15044807</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Matsumoto</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>. <article-title>Two types of dopamine neuron distinctly convey positive and negative motivational signals</article-title>. <source>Nature</source>. <year>2009</year>;<volume>459</volume>:<fpage>837</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature08028" xlink:type="simple">10.1038/nature08028</ext-link></comment> <object-id pub-id-type="pmid">19448610</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zaghloul</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Blanco</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Weidemann</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>McGill</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Jaggi</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Baltuch</surname> <given-names>GH</given-names></name>, <name name-style="western"><surname>Kahana</surname> <given-names>MJ</given-names></name>. <article-title>Human substantia nigra neurons encode unexpected financial rewards</article-title>. <source>Science</source>. <year>2009</year>;<volume>323</volume>:<fpage>1496</fpage>–<lpage>1499</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1167342" xlink:type="simple">10.1126/science.1167342</ext-link></comment> <object-id pub-id-type="pmid">19286561</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Franklin</surname> <given-names>NT</given-names></name>, <name name-style="western"><surname>Frank</surname> <given-names>MJ</given-names></name>. <article-title>A cholinergic feedback circuit to regulate striatal population uncertainty and optimize reinforcement learning</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e12029</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.12029" xlink:type="simple">10.7554/eLife.12029</ext-link></comment> <object-id pub-id-type="pmid">26705698</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kato</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Morita</surname> <given-names>K</given-names></name>. <article-title>Forgetting in Reinforcement Learning Links Sustained Dopamine Signals to Motivation</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>:<fpage>e1005145</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005145" xlink:type="simple">10.1371/journal.pcbi.1005145</ext-link></comment> <object-id pub-id-type="pmid">27736881</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rutledge</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Skandali</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Dopaminergic modulation of decision making and subjective well-being</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>:<fpage>9811</fpage>–<lpage>9822</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0702-15.2015" xlink:type="simple">10.1523/JNEUROSCI.0702-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26156984</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cone</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Fortin</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>McHenry</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Stuber</surname> <given-names>GD</given-names></name>, <name name-style="western"><surname>McCutcheon</surname> <given-names>JE</given-names></name>, <name name-style="western"><surname>Roitman</surname> <given-names>MF</given-names></name>. <article-title>Physiological state gates acquisition and expression of mesolimbic reward prediction signals</article-title>. <source>P Natl Acad Sci USA</source>. <year>2016</year>;<volume>113</volume>:<fpage>1943</fpage>–<lpage>1948</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1519643113" xlink:type="simple">10.1073/pnas.1519643113</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Namburi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Beyeler</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Yorozu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Calhoon</surname> <given-names>GG</given-names></name>, <name name-style="western"><surname>Halbert</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Wichmann</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>A circuit mechanism for differentiating positive and negative associations</article-title>. <source>Nature</source>. <year>2015</year>;<volume>520</volume>:<fpage>675</fpage>–<lpage>678</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14366" xlink:type="simple">10.1038/nature14366</ext-link></comment> <object-id pub-id-type="pmid">25925480</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Basten</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Biele</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Heekeren</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Fiebach</surname> <given-names>CJ</given-names></name> <article-title>How the brain integrates costs and benefits during decision making</article-title>. <source>PNAS</source>. <year>2010</year>; <volume>107</volume>:<fpage>21767</fpage>–<lpage>21772</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0908104107" xlink:type="simple">10.1073/pnas.0908104107</ext-link></comment> <object-id pub-id-type="pmid">21118983</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006285.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wall</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>De La Parra</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Callaway</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Kreitzer</surname> <given-names>AC</given-names></name>. <article-title>Differential innervation of direct-and indirect-pathway striatal projection neurons</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>:<fpage>347</fpage>–<lpage>360</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.05.014" xlink:type="simple">10.1016/j.neuron.2013.05.014</ext-link></comment> <object-id pub-id-type="pmid">23810541</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>