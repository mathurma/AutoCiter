<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00790</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006518</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject><subj-group><subject>Animal navigation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject><subj-group><subject>Animal navigation</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal migration</subject><subj-group><subject>Animal navigation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Modeling sensory-motor decisions in natural behavior</article-title>
<alt-title alt-title-type="running-head">Modeling sensory-motor decisions</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6681-3360</contrib-id>
<name name-style="western">
<surname>Zhang</surname> <given-names>Ruohan</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zhang</surname> <given-names>Shun</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tong</surname> <given-names>Matthew H.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cui</surname> <given-names>Yuchen</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rothkopf</surname> <given-names>Constantin A.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ballard</surname> <given-names>Dana H.</given-names></name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hayhoe</surname> <given-names>Mary M.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, The University of Texas at Austin, Austin, TX, USA</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Computer Science and Engineering, University of Michigan, Ann Arbor, MI, USA</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Center for Perceptual Systems, The University of Texas at Austin, Austin, TX, USA</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Cognitive Science Center and Institute of Psychology, Technical University Darmstadt, Darmstadt, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gershman</surname> <given-names>Samuel J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Harvard University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">zharu@utexas.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>10</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="epub">
<day>25</day>
<month>10</month>
<year>2018</year>
</pub-date>
<volume>14</volume>
<issue>10</issue>
<elocation-id>e1006518</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>9</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Zhang et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006518"/>
<abstract>
<p>Although a standard reinforcement learning model can capture many aspects of reward-seeking behaviors, it may not be practical for modeling human natural behaviors because of the richness of dynamic environments and limitations in cognitive resources. We propose a modular reinforcement learning model that addresses these factors. Based on this model, a modular inverse reinforcement learning algorithm is developed to estimate both the rewards and discount factors from human behavioral data, which allows predictions of human navigation behaviors in virtual reality with high accuracy across different subjects and with different tasks. Complex human navigation trajectories in novel environments can be reproduced by an artificial agent that is based on the modular model. This model provides a strategy for estimating the subjective value of actions and how they influence sensory-motor decisions in natural behavior.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>It is generally agreed that human actions can be formalized within the framework of statistical decision theory, which specifies a cost function for actions choices, and that the intrinsic value of actions is controlled by the brain’s dopaminergic reward machinery. Given behavioral data, the underlying subjective reward value for an action can be estimated through a machine learning technique called inverse reinforcement learning. Hence it is an attractive method for studying human reward-seeking behaviors. Standard reinforcement learning methods were developed for artificial intelligence agents, and incur too much computation to be a viable model for real-time human decision making. We propose an approach called modular reinforcement learning that decomposes a complex task into independent decision modules. This model includes a frequently overlooked variable called the discount factor, which controls the degree of impulsiveness in seeking future reward. We develop an algorithm called modular inverse reinforcement learning that estimates both the reward and the discount factor. We show that modular reinforcement learning may be a useful model for natural navigation behaviors. The estimated rewards and discount factors explain human walking direction decisions in a virtual-reality environment, and can be used to train an artificial agent that can accurately reproduce human navigation trajectories.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
<institution>National Science Foundation</institution>
</institution-wrap>
</funding-source>
<award-id>CNS 1624378</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Hayhoe</surname> <given-names>Mary M.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
<institution>National Institutes of Health</institution>
</institution-wrap>
</funding-source>
<award-id>EY05729</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ballard</surname> <given-names>Dana H.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100006785</institution-id>
<institution>Google</institution>
</institution-wrap>
</funding-source>
<award-id>AR/VR Research Award</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Ballard</surname> <given-names>Dana H.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by National Institute of Health (<ext-link ext-link-type="uri" xlink:href="https://www.nih.gov/" xlink:type="simple">https://www.nih.gov/</ext-link>) Grant EY05729 (MMH, DHB), National Science Foundation (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/" xlink:type="simple">https://www.nsf.gov/</ext-link>) Grant CNS 1624378 (DHB, MMH), and Google AR/VR Research Award (DHB, MMH). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="2"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2018-11-06</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available from the Zenodo database at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.255882" xlink:type="simple">https://doi.org/10.5281/zenodo.255882</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Modeling and predicting visually guided behavior in humans is challenging. In various contexts, it is unclear what information is being acquired and how it is being used to control behaviors. Empirical investigation of natural behavior has been limited, largely because it requires immersion in natural environments and monitoring of ongoing behavior. However, recent technical developments have allowed more extensive investigation of visually guided behavior in natural contexts [<xref ref-type="bibr" rid="pcbi.1006518.ref001">1</xref>]. At the empirical level it appears that complex behaviors can be broken down into a set of subgoals, each of which requires specific visual information [<xref ref-type="bibr" rid="pcbi.1006518.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>]. In a complex task such as crossing a road, a person must simultaneously determine the direction of heading, avoid tripping over the curb, locate other pedestrians or vehicles, and plan for future trajectory. Each of these particular goals requires some visual evaluation of the state of the world in order to make an appropriate action choice in the moment. A fundamental problem for understanding natural behavior is thus to be able to predict which subgoals are currently being considered, and how these sequences of visuomotor decisions unfold in time.</p>
<p>A theoretical basis for modeling such behavioral sequences is reinforcement learning (RL). Since the breakthrough work by [<xref ref-type="bibr" rid="pcbi.1006518.ref005">5</xref>], a rapidly increasing number of studies have used a formal reinforcement learning framework to model reward-seeking behaviors. Numerous studies have linked sensory-motor decisions to the underlying dopaminergic reward machinery [<xref ref-type="bibr" rid="pcbi.1006518.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref006">6</xref>]. The basic mechanisms of reinforcement learning, such as reward estimation, temporal-difference error, model-free and model-based learning, and discount factor, have been linked to a broad range of brain regions [<xref ref-type="bibr" rid="pcbi.1006518.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref016">16</xref>]. Because studies of the neural circuitry involve very restrictive behavioral paradigms, it is not known how these effects play out in the context of natural visually guided behavior. Similarly, the application of RL models to human behavior has been restricted almost exclusively to simple laboratory paradigms, and there are few formal attempts to model natural behaviors [<xref ref-type="bibr" rid="pcbi.1006518.ref017">17</xref>]. The goal of the presented work is to predict action choices in a virtual walking setting by estimating the subjective value of some of the sub-tasks that the sensory-motor system must perform in this context. We show that it is possible to estimate the subjective reward values of behaviors such as obstacle avoidance and path following, and accurately predict the trajectories walkers take through the environment. This demonstration suggests a potential analytical tool for the exploration of natural behavioral sequences.</p>
<sec id="sec002">
<title>Modular reinforcement learning for modeling natural behaviors</title>
<p>The primary focus of reinforcement learning has been on forward models that, given reward signals, can learn to produce policies, which specify action choices when immersed in an environment state. A <italic>state</italic> refers to information about the environment that is needed for decision making. An important breakthrough of RL in behavior modeling is inverse reinforcement learning (IRL), which aims to estimate the underlying subjective reward of decision makers given behavioral data [<xref ref-type="bibr" rid="pcbi.1006518.ref018">18</xref>]. IRL is an appealing tool for modeling human behavior: A behavioral model can be quantitatively evaluated by comparing human behaviors with reproduced behaviors by an artificial agent trained using the RL model with the estimated reward function.</p>
<p>An important factor that makes standard RL difficult in modeling natural behaviors is its sophistication and resulting computational burden as a model for general reward-seeking behaviors. The natural environment has at least two features that could make RL/IRL algorithms computationally intractable. First, a large number of task-relevant objects may be present, hence the decision state space is likely to be high-dimensional. Standard RL suffers from the <italic>curse of dimensionality</italic> with high-dimensional state space, where the computational burden grows exponentially with the number of state variables [<xref ref-type="bibr" rid="pcbi.1006518.ref005">5</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>]. Second, the natural environment is ever-changing such that humans must make decisions under different situations although these situations might have similar components. Living in a natural environment requires a decision maker to be able to <italic>transfer</italic> knowledge learned from previous experience to a new situation. In contrast an RL agent is often trained and tested repeatedly in a fixed environment. The optimal behavior is obtained through either a model-based dynamic programming approach that requires full knowledge of the environment, or a model-free learning approach that requires a large amount of experience. Both approaches generally put a heavy burden on memory storage or computation in order to calculate the optimal behavior. Consequently both of them may not be suitable for the real-time decision-making strategy in natural conditions since decision makers encounter new environment all the time and need to make decisions with reasonable cognitive load. For these reasons, standard RL must be extended to make computation tractable.</p>
<p>An extension of standard RL named <italic>modular</italic> reinforcement learning utilizes divide-and-conquer as an approximation strategy [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref021">21</xref>]. The modular RL takes the statistical structure present in the environment, decomposes a task into <italic>modules</italic> where each module solves a subgoal of the original task. Generally an arbitrator is required to synthesize module policies and make final decisions. Modularization alleviates the problem of curse of dimensionality since each module only concerns a subset of state variables. Introducing a new state variable may not affect the entire state space and cause its size to grow exponentially. Additionally, the decomposition naturally allows the decision maker to learn a behavior specifically for a module and reuse it later in a new environment. Under the modular RL framework, a more sample-efficient IRL algorithm is possible [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>], which matters for modeling natural human behaviors since such behavioral data is often expensive to collect.</p>
<p>Recent studies have explored the plausibility of a modular architecture for natural visually guided behavior where complex tasks can be broken down into concurrent execution of modules, or microbehaviors [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref022">22</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref023">23</xref>]. Thus in the example of walking across the street, each particular behavioral subgoal such as avoiding obstacles can be treated as an independent module. This leads to a view of the human brain as the centralized arbitrator that divides and coordinates these modules in a hierarchical manner. The current investigation explores the modular architecture in more detail.</p>
</sec>
<sec id="sec003">
<title>Estimating the discount factor</title>
<p>A frequently overlooked variable in RL is the discount factor that determines how much a decision-maker weighs future reward compared to immediate reward. In the agent-environment interaction paradigm, a standard RL model typically treats the discount factor as a part of the environment and as fixed. The alternative approach is to view the discount factor as a subjective decision-making variable that is part of the agent and may vary. Behavioral neuroscience studies suggest that the magnitude of the discount factor is correlated with serotonin level in human subjects [<xref ref-type="bibr" rid="pcbi.1006518.ref024">24</xref>]. As a consequence decision-makers may exhibit between-subject variations [<xref ref-type="bibr" rid="pcbi.1006518.ref025">25</xref>]. At the same time, between-task variation may also exist, i.e., the same decision maker may use different discount factors for various tasks. An fMRI study by [<xref ref-type="bibr" rid="pcbi.1006518.ref016">16</xref>] suggests that different cortico-basal ganglia loops are responsible for reward prediction at different time scales, allowing multiple discount factors to be implemented. Hence it is necessary to extend the standard RL model to adapt discount factors to different human subjects and tasks. A modular approach is ideal for this modeling effort. Allowing different modules to have their own discount factors makes the model flexible in modeling potential variations in human data.</p>
<p>Spatial navigation has been used as a canonical benchmark task for standard RL/IRL algorithms in machine learning, and therefore is selected as the experimental domain for testing our model. The task is an ideal testbed for modular RL since it is convenient for introducing multiple (sub-)tasks. In following sections of this paper, computer simulations are conducted first to validate the correctness of the proposed algorithm and to compare with existing methods. We then use human behavioral data previously collected in an immersive virtual environment [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>] to show that the proposed sparse modular IRL algorithm allows prediction of human walking trajectories by estimating the subjective reward values and discount factors of different modules. By demonstrating the ability to model naturalistic human sensory-motor behavior we lay the ground work for future analysis of similar behaviors.</p>
</sec>
</sec>
<sec id="sec004" sec-type="materials|methods">
<title>Methods</title>
<p>We introduce the experimental designs and computational models first since they are necessary to understand the results.</p>
<sec id="sec005">
<title>Experiments</title>
<p>Virtual reality (VR) and motion tracking were employed to create a naturalistic environment with a rich stimulus array, while maintaining experimental control. <xref ref-type="fig" rid="pcbi.1006518.g001">Fig 1</xref> shows the basic setup. The subject wore a binocular head-mounted display (the nVisor SX111 by NVIS) that showed a virtual room (8.5 × 7.3 meters). The subject’s eye, head, and body motion were tracked while walking through the virtual room. Subjects were recruited from a subject pool of undergraduates at the University of Texas at Austin, and were naive to the nature of the experiment. The human subject research is approved by the University of Texas at Austin Institutional Review Board with approval number 2006-06-0085 [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>].</p>
<fig id="pcbi.1006518.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g001</object-id>
<label>Fig 1</label>
<caption>
<title>The virtual-reality human navigation experiment with motion tracking.</title>
<p>(A) A human subject wears a head mounted display (HMD) and trackers for eyes, head, and body. (B) The virtual environment as seen through the HMD. The red cubes are obstacles and the blue spheres are targets. There is also a gray path on the ground leading to a goal (the green disk). At the green disk the subject is ‘transported’ to a new ‘level’ in a virtual elevator for another trial with a different arrangement of objects.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g001" xlink:type="simple"/>
</fig>
<p>Although we do not know the set of normal subtasks involved in walking through a room like this, three plausible candidates might be following a path across the room, avoiding obstacles, and perhaps heading towards target objects. To capture some of this natural behavior we asked subjects to collect the targets (blue spheres) by intercepting them, follow the path (the gray line), and/or avoid the obstacles (red cubes). Objects disappeared after collision. This type of state transition function encourages subjects to navigate through the virtual room instead of sticking at a single target.</p>
<p>The global task has at least three <italic>modules</italic>: following the path, collecting targets, and avoiding obstacles. We gave subjects four types of instructions that attempt to manipulate their reward functions (and potentially the discount factors), resulting in four experimental task conditions:</p>
<list list-type="order">
<list-item>
<p><bold>Task 1</bold>: Follow the path only</p>
</list-item>
<list-item>
<p><bold>Task 2</bold>: Follow the path and avoid the obstacles</p>
</list-item>
<list-item>
<p><bold>Task 3</bold>: Follow the path and collect the targets</p>
</list-item>
<list-item>
<p><bold>Task 4</bold>: Follow, avoid, and collect together</p>
</list-item>
</list>
<p>There were no monetary rewards in the task. Since following paths, avoiding obstacles, and heading towards targets are frequent natural behaviors, we assume that subjects have some learned, and perhaps context-specific subjective values associated with the three task components, and our goal was to modulate these intrinsic values using the instructions. The instructions were to walk normally, but to give some priority to the particular task components in the different conditions. To encourage such prioritization, Subjects received auditory feedback when colliding with obstacles or targets. When objects were task-relevant, this feedback was positive (a fanfare) or negative (a buzzer), while collisions to task-irrelevant objects resulted in a neutral sound (a soft bubble pop) [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>]. The color of the targets and obstacles was counterbalanced in another version of the experiment and was found not to affect task performance or the distribution of eye fixations so the control was not repeated in the present experiment [<xref ref-type="bibr" rid="pcbi.1006518.ref026">26</xref>]. The order of the task was Task 1, 2, 3, and 4. This order was chosen so as not to influence the single task conditions by doing the double task. Thus it is possible there are some order effects. In another experiment in the environment the order of the conditions was counterbalanced and no obvious order effects were observed [<xref ref-type="bibr" rid="pcbi.1006518.ref026">26</xref>].</p>
<p>We analyze data collected from 25 human subjects. A single experimental trial consisted of a subject traversing the room, with the trial ending when the goal at the end of the path is reached. Objects’ positions and the path’s shape differed on every trial. Each subject performed four trials for each task condition.</p>
<sec id="sec006">
<title>Data availability</title>
<p>This general paradigm of navigation with targets and obstacles has been used to evaluate modular RL and IRL algorithms [<xref ref-type="bibr" rid="pcbi.1006518.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>] and to study human navigation and gaze behaviors [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref027">27</xref>]. The data that support the findings of this study are made public and available at [<xref ref-type="bibr" rid="pcbi.1006518.ref028">28</xref>].</p>
</sec>
</sec>
<sec id="sec007">
<title>Modular reinforcement learning</title>
<sec id="sec008">
<title>Reinforcement learning basics</title>
<p>A standard reinforcement learning model is formalized as a Markov decision process (MDP). The MDP models the interaction between the environment and a decision maker which will be referred as an agent. Formally, an MDP is defined as a tuple <inline-formula id="pcbi.1006518.e001"><alternatives><graphic id="pcbi.1006518.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mo>〈</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">P</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">R</mml:mi> <mml:mo>,</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>〉</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1006518.ref005">5</xref>], where:</p>
<list list-type="bullet">
<list-item>
<p>
<inline-formula id="pcbi.1006518.e002">
<alternatives>
<graphic id="pcbi.1006518.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e002" xlink:type="simple"/>
<mml:math display="inline" id="M2">
<mml:mi mathvariant="script">S</mml:mi>
</mml:math>
</alternatives>
</inline-formula> is a finite set of environment states. Let <italic>s</italic><sub><italic>t</italic></sub> denote the agent’s state at discrete time step <italic>t</italic>. The state encodes relevant information for an agent’s decision.</p>
</list-item>
<list-item>
<p>
<inline-formula id="pcbi.1006518.e003">
<alternatives>
<graphic id="pcbi.1006518.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e003" xlink:type="simple"/>
<mml:math display="inline" id="M3">
<mml:mi mathvariant="script">A</mml:mi>
</mml:math>
</alternatives>
</inline-formula> is a finite set of available actions. Let <italic>a</italic><sub><italic>t</italic></sub> be the action agent chooses to take at time <italic>t</italic>. The agent interacts with the environment by taking an action in its observed state.</p>
</list-item>
<list-item>
<p>
<inline-formula id="pcbi.1006518.e004">
<alternatives>
<graphic id="pcbi.1006518.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e004" xlink:type="simple"/>
<mml:math display="inline" id="M4">
<mml:mi mathvariant="script">P</mml:mi>
</mml:math>
</alternatives>
</inline-formula> is the state transition function which specifies the probability <italic>P</italic>(<italic>s</italic>′|<italic>s</italic>, <italic>a</italic>), i.e., the probability of entering state <italic>s</italic>′ when agent takes action <italic>a</italic> in state <italic>s</italic>. The state transition function describes the dynamics of the environment that are influenced by an agent’s action.</p>
</list-item>
<list-item>
<p>
<inline-formula id="pcbi.1006518.e005">
<alternatives>
<graphic id="pcbi.1006518.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e005" xlink:type="simple"/>
<mml:math display="inline" id="M5">
<mml:mi mathvariant="script">R</mml:mi>
</mml:math>
</alternatives>
</inline-formula> is a reward function. <italic>r</italic><sub><italic>t</italic></sub> denotes the scalar reward agent received at time step <italic>t</italic>.</p>
</list-item>
<list-item>
<p><italic>γ</italic> ∈ [0,1) is a discount factor. The agent values future rewards less than an immediate reward, therefore future rewards are discounted by parameter <italic>γ</italic> atevery discrete time step. <italic>γ</italic> = 0 indicates that the agent is myopic and only seeks to maximize the immediate reward.</p>
</list-item>
<list-item>
<p>
<inline-formula id="pcbi.1006518.e006">
<alternatives>
<graphic id="pcbi.1006518.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e006" xlink:type="simple"/>
<mml:math display="inline" id="M6">
<mml:mrow>
<mml:mi>π</mml:mi>
<mml:mo>:</mml:mo>
<mml:mi mathvariant="script">S</mml:mi>
<mml:mo>↦</mml:mo>
<mml:mi mathvariant="script">A</mml:mi>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula> is called a policy of the agent, which specifies the probability of chosen each action in each state.</p>
</list-item>
</list>
<p>In machine learning, the purpose of a reinforcement learning algorithm is to find an optimal policy <italic>π</italic>* that maximizes the longterm cumulative reward. Many RL algorithms are based on value function estimation. The action-value function (also called Q-value function) estimates the expected longterm reward for taking an action in a given state, and follow policy <italic>π</italic> afterwards. Formally, the Q-value function conditioned on policy <italic>π</italic> is defined as [<xref ref-type="bibr" rid="pcbi.1006518.ref005">5</xref>]:
<disp-formula id="pcbi.1006518.e007"><alternatives><graphic id="pcbi.1006518.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mi>π</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mi>π</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:munderover> <mml:msup><mml:mi>γ</mml:mi> <mml:mi>k</mml:mi></mml:msup> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>k</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
Given the Q-value function it is convenient for an agent to select the action that maximizes expected future returns.</p>
</sec>
<sec id="sec009">
<title>Modular reinforcement learning</title>
<p>The divide-and-conquer approximation of RL leads to modular reinforcement learning, in which a <italic>module</italic> is a subtask of the original task. Each module is hence a simpler problem, so that its value function and policy can be learned or calculated efficiently. A module is also modeled by an MDP <inline-formula id="pcbi.1006518.e008"><alternatives><graphic id="pcbi.1006518.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mo>〈</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>〉</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>n</italic> is the index of the <italic>n</italic>th module. Note that each module has its own state space, transition function, reward function, and discount factor, but the action space is shared between modules because all modules reside in a single agent.</p>
<p>Let <italic>N</italic> be the number of modules and <inline-formula id="pcbi.1006518.e009"><alternatives><graphic id="pcbi.1006518.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mi>π</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> denote module Q-value function of the <italic>n</italic>th module conditioned on module policy <italic>π</italic><sup>(<italic>n</italic>)</sup>. For simplicity, we will drop <italic>π</italic><sup>(<italic>n</italic>)</sup> and write <italic>Q</italic><sup>(<italic>n</italic>)</sup>. Let <italic>Q</italic> without superscription denote the global Q function (also drop global policy <italic>π</italic>). Modular RL sums module Q functions to obtain the global Q function [<xref ref-type="bibr" rid="pcbi.1006518.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref029">29</xref>]:
<disp-formula id="pcbi.1006518.e010"><alternatives><graphic id="pcbi.1006518.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
There can be multiple <italic>module objects</italic> of a module, e.g., several identical obstacles nearby to avoid. The number of objects of each module is denoted as <italic>M</italic><sup>(1)</sup>, …, <italic>M</italic><sup>(<italic>N</italic>)</sup>. Note that for a given module, its module objects share the same <italic>Q</italic><sup>(<italic>n</italic>)</sup> since their module MDPs are identical. But at a given time they could be in different states relative to the agent’s reference frame which can be denoted as <italic>s</italic><sup>(<italic>n</italic>,<italic>m</italic>)</sup> for module <italic>n</italic> object <italic>m</italic>. To generalize the above equation:
<disp-formula id="pcbi.1006518.e011"><alternatives><graphic id="pcbi.1006518.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msup><mml:mi>M</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:munderover> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
This assumes independent transition functions between module objects [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>]. A module action-value function <italic>Q</italic><sup>(<italic>n</italic>)</sup> may be calculated from solving Bellman equations using dynamic programming or through standard learning algorithms with enough experience data, which we argue to be infeasible for human performing natural tasks. <italic>Q</italic><sup>(<italic>n</italic>)</sup> needs to be calculated efficiently with reasonable cognitive load.</p>
<p>In our experiments, both the state transition function and reward function are deterministic hence the expectation in <xref ref-type="disp-formula" rid="pcbi.1006518.e007">Eq (1)</xref> can be dropped. Since each module Q function only considers a single source of reward from a single module object, and assuming a policy that leads the agent directly to the module object, <italic>Q</italic><sup>(<italic>n</italic>)</sup>(<italic>s</italic><sup>(<italic>n</italic>,<italic>m</italic>)</sup>, <italic>a</italic>) takes the following simple form:
<disp-formula id="pcbi.1006518.e012"><alternatives><graphic id="pcbi.1006518.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msup><mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>r</italic><sup>(<italic>n</italic>)</sup> is the reward for the <italic>n</italic>th module, <italic>γ</italic><sup>(<italic>n</italic>)</sup> is its discount factor, and <italic>d</italic>(<italic>s</italic><sup>(<italic>n</italic>, <italic>m</italic>)</sup>, <italic>a</italic>) is the spatial or temporal distance between the agent and the module object <italic>m</italic> after taking action <italic>a</italic> at state <italic>s</italic><sup>(<italic>n</italic>, <italic>m</italic>)</sup>. Note <xref ref-type="disp-formula" rid="pcbi.1006518.e012">Eq (4)</xref> converts value function back to its simplest form in [<xref ref-type="bibr" rid="pcbi.1006518.ref015">15</xref>]. This simple form allows a decision maker to calculate the action-value for a state efficiently when needed instead of beforehand. This matters when humans need to make decisions fast and when it is computationally expensive to calculate value functions using a standard RL algorithm. It is also unlikely for a human to pre-compute the values for all future states and use dynamic programming to obtain a global policy when they visit the environment for the first time. Doing so would at least require a human to store Q-values for relevant states (a Q-table) in its memory system, which is convenient for an artificial agent but would be difficult for a real-time human decision maker.</p>
<p>Why does modular RL alleviate the problem of curse of dimensionality? Consider the joint state space of a standard RL which can be represented as the Cartesian product of the module state spaces: <inline-formula id="pcbi.1006518.e013"><alternatives><graphic id="pcbi.1006518.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi mathvariant="script">S</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>×</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>×</mml:mo> <mml:mo>…</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The computation cost for one iteration in value iteration (a popular RL algorithm) is <inline-formula id="pcbi.1006518.e014"><alternatives><graphic id="pcbi.1006518.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="script">S</mml:mi> <mml:msup><mml:mo>|</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where |⋅| denotes the cardinality of a set [<xref ref-type="bibr" rid="pcbi.1006518.ref030">30</xref>]. When a new module <inline-formula id="pcbi.1006518.e015"><alternatives><graphic id="pcbi.1006518.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is added, the cost of standard RL becomes <inline-formula id="pcbi.1006518.e016"><alternatives><graphic id="pcbi.1006518.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>×</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>×</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>×</mml:mo> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mo>|</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>|</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, while the cost of modular RL becomes <inline-formula id="pcbi.1006518.e017"><alternatives><graphic id="pcbi.1006518.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>|</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>|</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>+</mml:mo> <mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>|</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>|</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Therefore the computational cost increases additively in modular RL instead of multiplicatively.</p>
</sec>
<sec id="sec010">
<title>Visualizing modular reinforcement learning</title>
<p>
<xref ref-type="disp-formula" rid="pcbi.1006518.e012">Eq (4)</xref> bridges modular RL with an important planning method called artificial potential field [<xref ref-type="bibr" rid="pcbi.1006518.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref033">33</xref>]. Similar to a potential field, we use a value surface to visualize the value function. Each module objects is associated with a value surface. The module reward controls the maximum absolute height of the surface, and the discount factor controls temporal or spatial discounting rates. Module value surfaces can be composed directly by summation or integration to produce a multi-module value surface. The concept of value surfaces and their combination is illustrated in <xref ref-type="fig" rid="pcbi.1006518.g002">Fig 2</xref>. Given a composed value surface as in <xref ref-type="fig" rid="pcbi.1006518.g002">Fig 2F</xref>, a modular RL agent would choose actions that lead to a local minima on the surface. A sequence of actions could construct a trajectory in <xref ref-type="fig" rid="pcbi.1006518.g003">Fig 3A</xref> which traverses through a sequence of local minima.</p>
<fig id="pcbi.1006518.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The concept of modular reinforcement learning illustrated using value surfaces.</title>
<p>(A) The value surface is flat without any reward signal. (B) A module object with positive reward has positive weight, and one with negative reward has negative weight. They bend the value surface to have negative and positive curvatures respectively. Therefore, an agent desires to follow the steepest descent to minimize energy, or equivalently, to maximize reward. (C) An object with larger weight bends the surface more. (D) An object with greater discount factor <italic>γ</italic> has larger influence over distance. (E,F) Composing different objects with different rewards and <italic>γ</italic>s results complicated value surfaces that can model an agent’s value function over the entire state space.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g002" xlink:type="simple"/>
</fig>
<fig id="pcbi.1006518.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Maximum likelihood modular inverse reinforcement learning.</title>
<p>(A) From an observed trajectory (a sequence of state-action pairs), the goal of modular IRL is to recover the underlying value surface. (B) Maximum likelihood IRL assumes that the probability of observing a particular action (red) in a state is proportional to its Q-value among all possible actions as in <xref ref-type="disp-formula" rid="pcbi.1006518.e018">Eq (5)</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g003" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec011">
<title>Modular inverse reinforcement learning</title>
<p>While reinforcement learning aims at finding the optimal policy given a reward function, inverse reinforcement learning (IRL) attempts to infer the unknown reward function given the agent behavioral data in the form of state-action pairs (<italic>s</italic><sub><italic>t</italic></sub>, <italic>a</italic><sub><italic>t</italic></sub>) [<xref ref-type="bibr" rid="pcbi.1006518.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref034">34</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>]. Our work is largely based on the modular IRL algorithm by [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>] which pioneered the first modular IRL algorithm. Given the modular RL formulation in the previous section, the goal of modular IRL is to estimate the underlying reward and discount factor for each module to recover the value function, given a sequence of observed state-action pairs, i.e., a trajectory that traverses through the state space, as shown in <xref ref-type="fig" rid="pcbi.1006518.g003">Fig 3A</xref>.</p>
<p>We follow the Bayesian formulation of IRL [<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref037">37</xref>], Maximum Likelihood IRL [<xref ref-type="bibr" rid="pcbi.1006518.ref038">38</xref>], and improve the modular IRL algorithm in [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>]. These approaches assume that the higher the <italic>Q</italic>-value for an action <italic>a</italic><sub><italic>t</italic></sub> in state <italic>s</italic><sub><italic>t</italic></sub>, the more likely action <italic>a</italic><sub><italic>t</italic></sub> is observed in behavioral data. Let <italic>η</italic> denote the confidence level in optimality (the extent to which an agent selects actions greedily, default to be 1), and let exp(⋅) denote the exponential function. The likelihood of observing a certain state-action pair is modeled by the softmax function with Gibbs (Boltzmann) distribution, as illustrated in <xref ref-type="fig" rid="pcbi.1006518.g003">Fig 3B</xref>:
<disp-formula id="pcbi.1006518.e018"><alternatives><graphic id="pcbi.1006518.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
Let <italic>T</italic> denote the total length of the trajectory. The overall likelihood <inline-formula id="pcbi.1006518.e019"><alternatives><graphic id="pcbi.1006518.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mi mathvariant="script">L</mml:mi></mml:math></alternatives></inline-formula> for observed data <italic>D</italic> = {(<italic>s</italic><sub>1</sub>, <italic>a</italic><sub>1</sub>), ⋯, (<italic>s</italic><sub><italic>T</italic></sub>, <italic>a</italic><sub><italic>T</italic></sub>)} is the product of the likelihood of individual state-action pairs, given the states are Markovian and action decisions are independent:
<disp-formula id="pcbi.1006518.e020"><alternatives><graphic id="pcbi.1006518.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>|</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub><mml:mspace width="4pt"/><mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
Next, the global action-value function <italic>Q</italic>(<italic>s</italic><sub><italic>t</italic></sub>, <italic>a</italic><sub><italic>t</italic></sub>) is decomposed using <xref ref-type="disp-formula" rid="pcbi.1006518.e011">Eq (3)</xref> with module Q functions <italic>Q</italic><sup>(1:<italic>N</italic>)</sup>, therefore the likelihood becomes:
<disp-formula id="pcbi.1006518.e021"><alternatives><graphic id="pcbi.1006518.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">L</mml:mi> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:mi>η</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msubsup><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Take the log of the likelihood function:
<disp-formula id="pcbi.1006518.e022"><alternatives><graphic id="pcbi.1006518.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mi>η</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>log</mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:munder> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>Q</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
Substituting <xref ref-type="disp-formula" rid="pcbi.1006518.e012">Eq (4)</xref> into <xref ref-type="disp-formula" rid="pcbi.1006518.e022">Eq (8)</xref>:
<disp-formula id="pcbi.1006518.e023"><alternatives><graphic id="pcbi.1006518.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="4pt"/><mml:mi mathvariant="script">L</mml:mi> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mi>η</mml:mi> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>log</mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:munder> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
The variables to be estimated from the data are module rewards <italic>r</italic><sup>(1: <italic>N</italic>)</sup> and discount factors <italic>γ</italic><sup>(1: <italic>N</italic>)</sup>. The number of modules <italic>N</italic>, the number of objects for each module <inline-formula id="pcbi.1006518.e024"><alternatives><graphic id="pcbi.1006518.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, and distances <inline-formula id="pcbi.1006518.e025"><alternatives><graphic id="pcbi.1006518.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for each object are all state information and can be observed from the environment. This formulation follows closely the work by [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>], extending it to use the new formulation of modular RL, handle multiple objects of each module, estimate the discount factors, and derive a slightly different objective function.</p>
<sec id="sec012">
<title>Sparse modular inverse reinforcement learning</title>
<p>Modular IRL can only guess which objects are actually being considered by the decision maker when chosen an action. To address this problem, we can further add a <italic>L</italic><sub>1</sub> regularizer <inline-formula id="pcbi.1006518.e026"><alternatives><graphic id="pcbi.1006518.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msub><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> to <xref ref-type="disp-formula" rid="pcbi.1006518.e023">Eq (9)</xref>, which causes some module rewards to become 0 so these modules would be ignored in decision making. This is an extension of using a Laplacian prior in Bayesian IRL [<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>]. In addition to the benefit from an optimization perspective, the regularization term has the following important interpretation in terms of explaining natural behaviors.</p>
<p>A <italic>hypothetical module set</italic> is a set <inline-formula id="pcbi.1006518.e027"><alternatives><graphic id="pcbi.1006518.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi mathvariant="script">H</mml:mi> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mi>N</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> contains <italic>N</italic> modules that could potentially be of an agent’s interest. However, due to the limitations in computational resource, the agent can only consider a subset of <inline-formula id="pcbi.1006518.e028"><alternatives><graphic id="pcbi.1006518.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula> at a time, denoted <inline-formula id="pcbi.1006518.e029"><alternatives><graphic id="pcbi.1006518.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msup><mml:mi mathvariant="script">H</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:math></alternatives></inline-formula>. In a rich environment many modules’ rewards would be effectively zero at current decision step, hence <inline-formula id="pcbi.1006518.e030"><alternatives><graphic id="pcbi.1006518.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi mathvariant="script">H</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>⪡</mml:mo> <mml:mo>|</mml:mo> <mml:mi mathvariant="script">H</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. For instance, a driving environment could contain hundreds of objects in <inline-formula id="pcbi.1006518.e031"><alternatives><graphic id="pcbi.1006518.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mi mathvariant="script">H</mml:mi></mml:math></alternatives></inline-formula>. But a driver may pay attention to only a few. The regularization constant λ serves as a cognitive capacity factor that helps determine <inline-formula id="pcbi.1006518.e032"><alternatives><graphic id="pcbi.1006518.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msup><mml:mi mathvariant="script">H</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:math></alternatives></inline-formula> from the observed behaviors. Therefore the final objective function of modular IRL is:
<disp-formula id="pcbi.1006518.e033"><alternatives><graphic id="pcbi.1006518.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:munder><mml:mtext>max</mml:mtext> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>:</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>T</mml:mi></mml:munderover> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mi>η</mml:mi> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mtext>log</mml:mtext> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>a</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:munder> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msubsup><mml:mi>M</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover> <mml:mtext>exp</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>η</mml:mi> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mi>a</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mo>λ</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msup><mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msub><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>s</mml:mi> <mml:mo>.</mml:mo> <mml:mi>t</mml:mi> <mml:mo>.</mml:mo> <mml:mspace width="4pt"/><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:msup><mml:mi>γ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
Note that if we are to fit <italic>r</italic><sup>(1:<italic>N</italic>)</sup> and <italic>γ</italic><sup>(1:<italic>N</italic>)</sup> simultaneously, the above objective function is non-convex. However, the objective becomes convex if only fitting <italic>r</italic><sup>(1:<italic>N</italic>)</sup>. Since <italic>γ</italic><sup>(<italic>n</italic>)</sup> is in range [0, 1), one can perform a grid search over values for <italic>γ</italic><sup>(1:<italic>N</italic>)</sup> with step size <italic>ϵ</italic> and fit <italic>r</italic><sup>(1: <italic>N</italic>)</sup> at each possible <italic>γ</italic><sup>(1: <italic>N</italic>)</sup> value. This allows us to find a solution within <italic>ϵ</italic>-precision of the true global optimum.</p>
<p>An accessible evaluation of the proposed algorithms in an artificial multitask navigation environment can be found in <xref ref-type="supplementary-material" rid="pcbi.1006518.s001">S1 Appendix</xref>. The environment is a 2D gridworld that resembles the virtual room we use for the human experiments. The validity of the modular IRL is proved empirically by showing its ability to recover true module rewards and discount factors with high accuracy given enough behavioral data. Meanwhile it requires significantly less data samples to obtain high prediction accuracy comparing to a standard Bayesian IRL algorithm [<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>], presumably because the state space is reduced significantly by modularization. Sparse modular IRL is shown to further improve sample efficiency if task-irrelevant modules are present. Unlike computer simulated experiments in which one can easily generate millions of behavioral data, human experiments have a more expensive data collection procedure in general. Therefore sample efficiency of sparse modular IRL is an important advantage in modeling natural human behaviors, which will be seen in the next section.</p>
</sec>
</sec>
</sec>
<sec id="sec013" sec-type="results">
<title>Results</title>
<p>Despite its computational advantages shown in simulation, the question remains whether modular IRL can be used as a decision-making model to explain human behaviors in the experiments. Sparse modular IRL (<xref ref-type="disp-formula" rid="pcbi.1006518.e033">Eq (10)</xref>) is used as the objective function to estimate reward <italic>r</italic> and discount factor <italic>γ</italic> for the target, obstacle, and path modules. However the regularization constant is found to be close to zero since there are only three modules. Recall that each subject performs each task four times, and each time the path and the arrangement of objects are different. We use leave-one-out cross evaluation, where <italic>r</italic>, <italic>γ</italic> are estimated using all-but-one training trials that are from the same subject and same task condition and evaluated on the remaining test trial. Since the parameter estimates are based on the other three trials, all of our prediction results shown below are for a <italic>novel</italic> environment with similar components—this requires the model to generalize across environments. The number of data samples obtained from a single trial is typically around 100 hence sample efficiency is critical for the performance of an algorithm.</p>
<p>Different <italic>r</italic> and <italic>γ</italic> are estimated for each subject under each task condition for each module, hence there are 25 subjects × 4 conditions × 3 modules × 4 trials = 1,200 different pairs of <italic>r</italic>, <italic>γ</italic> estimations. The state information for the model includes the distance and angle to the objects, while the state space is discretized using grids of size 0.572 by 0.572 meters, a parameter chosen empirically that produces the best modeling result. It also matches the approximate length of a step in VR, so is a suitable scale for human direction decisions. Empirically, as long as the grid size is within reasonable range of human stride length (0.3-0.9 meters) the algorithm’s performance is fairly robust.</p>
<p>The path is discretized into a sequence of waypoints which are removed after being visited (similar to the targets). The action space spans 360 degrees and is discretized to be 16 actions using bins of 22.5 degrees. This is a suitable discretization of the action space, given the size of the objects at the distance of 1-2 meters, where an action decision is most likely made.</p>
<sec id="sec014">
<title>Qualitative results and visualization</title>
<p>The most intuitive way to evaluate the modular RL model is to see whether the model can accurately reproduce human navigation trajectories. The Q-value function of a modular RL agent is calculated using <italic>r</italic> and <italic>γ</italic> estimated from human data. Next, the modular RL agent is placed at the same starting position as the human subject and starts to navigate the environment until it reaches the end of the path. The agent chooses an action probabilistically based on the Q-value of the current state, using a softmax action selection function as in <xref ref-type="disp-formula" rid="pcbi.1006518.e018">Eq (5)</xref>. The reason to let the agent choose actions with a certain degree of randomness is that the Q-values for multiple actions can be very close, e.g., turning left or turning right to avoid an obstacle, consequently a human subject may choose either. Therefore, a single greedy trajectory may not overlap with the actual human trajectory. The softmax action selection function generates a distribution of hypothetical trajectories, i.e., a trajectory cloud, by running an agent many times in the same environment. The actual human trajectory can be visualized in the context of this distribution.</p>
<p>
<xref ref-type="fig" rid="pcbi.1006518.g004">Fig 4</xref> shows generated trajectory clouds together with actual human trajectories, along with estimated rewards and discount factors. The agent trajectories are shown in semi-transparent green hence darker area represents trajectories with higher likelihood, and the human trajectory on that trial is shown in black. Each row of figures presents experimental trials from one experimental condition (Task 1-4), and three trials within each row are from different subjects but the same environment, i.e., the same arrangement of objects.</p>
<fig id="pcbi.1006518.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Bird’s-eye view of human trajectories and agent trajectory clouds across different subjects.</title>
<p>Black lines: human trajectories. Green lines: modular RL agent trajectory clouds generated using softmax action selection. The green is semi-transparent hence darker area represents trajectories with higher likelihood. Yellow circles: end of the path. Blue circles: targets. Red squares: obstacles. Gray dots: path waypoints used by the model (subjects see a continuous path). Below each graph are the rewards and discount factors estimated from human and used by the modular RL agent. The rewards and discount factors are shown in the order of (Target, Obstacle, Path). The module rewards that correspond to task instructions are bold. Obstacle module has negative reward, but to compare with the other two modules the absolute value is taken. Three trials within each row are from different subjects but the same environment. (A,B,C) show trials from <bold>Task 1: follow the path</bold>. (D,E,F) show trials from <bold>Task 2: follow the path and avoid obstacles</bold>. (G,H,I) show trials from <bold>Task 3: follow the path and collect targets</bold>. (J,K,L) show trials from <bold>Task 4: follow the path, collect targets, and avoid obstacles</bold>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g004" xlink:type="simple"/>
</fig>
<p>The figures demonstrate that the model’s generated trajectory clouds align well with observed human trajectories. When a local trajectory distribution is multi-modal, e.g., in <xref ref-type="fig" rid="pcbi.1006518.g004">Fig 4D, 4F, 4J, 4K, and 4L</xref>, the human trajectories align with one of the means. The next important observation is the between-subject variation. Trials within each row are from the same environment under the same task instruction. However, human trajectories can sometimes exhibit drastically different choices, e.g., <xref ref-type="fig" rid="pcbi.1006518.g004">Fig 4E versus 4F and 4J versus 4K</xref>. These differences are modeled by the underlying <italic>r</italic> and <italic>γ</italic>, and accurately reproduced by the distributions generated. This means that we can compactly model naturalistic, diverse human navigation behaviors using only a reward and a discount factor per module. The modeling power of modular RL is demonstrated by the observation that varying these two variables can produce a rich class of human-like navigation trajectories.</p>
</sec>
<sec id="sec015">
<title>Between-task and between-subject differences</title>
<p>We then look at the way average reward estimates vary between different tasks when aggregating data from all subjects. The results are shown in <xref ref-type="fig" rid="pcbi.1006518.g005">Fig 5A</xref>. Overall, the estimated <italic>r</italic> values vary in an appropriate manner with task instructions. Thus obstacles are valued higher when the instructions prioritize this task, and targets are valued higher when that task is prioritized. Note that the obstacle avoidance module is given some weight even when it is not explicitly prioritized—this is consistent with the observation that subjects deviates from the path to avoid obstacles even when obstacles are task-irrelevant. This may reflect a bias which is carried over from natural behavior with real obstacles. The relatively high value for the path may indicate that subjects see staying near the path as the primary goal.</p>
<fig id="pcbi.1006518.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g005</object-id>
<label>Fig 5</label>
<caption>
<title/>
<p>(A) Normalized average rewards across different task instructions. The error bar represents the standard error of the mean between subjects (<italic>N</italic> = 25). The obstacle module has negative reward, but to compare with the other two modules its absolute value is taken. The estimated reward agree with task instructions. (B) Average discount factors across different task instructions. The error bar represents the standard error of the mean between subjects (<italic>N</italic> = 25).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g005" xlink:type="simple"/>
</fig>
<p>The between-subject differences in reward are shown in <xref ref-type="supplementary-material" rid="pcbi.1006518.s002">S2 Appendix</xref> for all 25 subjects. At each individual subject’s level, changing in the relative reward between the modules is also consistent with task instructions. An one-way ANOVA test suggests that individual differences are evident across subjects under the same task instruction (see <xref ref-type="supplementary-material" rid="pcbi.1006518.s002">S2 Appendix</xref> for details).</p>
<p>
<xref ref-type="fig" rid="pcbi.1006518.g005">Fig 5B</xref> shows average discount factor estimates for different tasks. Although the reward evidently reflects and agrees with task instructions, the interpretation of the discount factor is more complicated. The discount factors vary across tasks for target and obstacle modules but are close to 1.0 and stable for the path module. This may also reflect the primacy of the task of getting across the room, and the need to plan ahead. Although the instructions do not directly manipulate discount factors, we will later show that estimating discount factors from data instead of holding them fixed is important for modeling accuracy.</p>
</sec>
<sec id="sec016">
<title>Stability of rewards and discount factors across tasks</title>
<p>An important observation from <xref ref-type="fig" rid="pcbi.1006518.g005">Fig 5</xref> is that <italic>task-relevant</italic> module rewards and discount factors are stable across task conditions. To show this quantitatively, for each subject, we combine module rewards from Task 2 (path + obstacle) and Task 3 (path + target) to synthesize the rewards for Task 4 (path + obstacle + target) in the following way:
<disp-formula id="pcbi.1006518.e034"><alternatives><graphic id="pcbi.1006518.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>4</mml:mn> <mml:mo>_</mml:mo> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>g</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>3</mml:mn> <mml:mo>_</mml:mo> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mi>g</mml:mi> <mml:mi>e</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula> <disp-formula id="pcbi.1006518.e035"><alternatives><graphic id="pcbi.1006518.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>4</mml:mn> <mml:mo>_</mml:mo> <mml:mi>o</mml:mi> <mml:mi>b</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>l</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn> <mml:mo>_</mml:mo> <mml:mi>o</mml:mi> <mml:mi>b</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>c</mml:mi> <mml:mi>l</mml:mi> <mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula> <disp-formula id="pcbi.1006518.e036"><alternatives><graphic id="pcbi.1006518.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006518.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>4</mml:mn> <mml:mo>_</mml:mo> <mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn> <mml:mo>_</mml:mo> <mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mi>a</mml:mi> <mml:mi>s</mml:mi> <mml:mi>k</mml:mi> <mml:mn>3</mml:mn> <mml:mo>_</mml:mo> <mml:mi>p</mml:mi> <mml:mi>a</mml:mi> <mml:mi>t</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
Then the discount factors are synthesized in the similar way. The synthesized rewards (re-normalized) and discount factors from Task 2 and 3 are found to be very close to those estimated from Task 4, as shown in <xref ref-type="table" rid="pcbi.1006518.t001">Table 1</xref>. However, task-irrelevant rewards and discount factors are not stable. This result indicates that task-relevant module rewards and discount factors generalize to a different task condition. Thus modules are independent and transferable in this particular scenario.</p>
<table-wrap id="pcbi.1006518.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.t001</object-id>
<label>Table 1</label>
<caption>
<title>Synthesized rewards and discount factors compared to the estimated ones.</title>
<p>Rewards are re-normalized. Results are presented as mean ± standard error between subjects (N = 25).</p>
</caption>
<alternatives>
<graphic id="pcbi.1006518.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="left"/>
<td align="left">Target <italic>r</italic></td>
<td align="left">Obstacle <italic>r</italic></td>
<td align="left">Path <italic>r</italic></td>
</tr>
<tr>
<td align="left">Task 2+3 synthesized</td>
<td align="left">0.177 ± 0.018</td>
<td align="left">0.415 ± 0.028</td>
<td align="left">0.408 ± 0.021</td>
</tr>
<tr>
<td align="left">Task 4</td>
<td align="left">0.180 ± 0.017</td>
<td align="left">0.422 ± 0.029</td>
<td align="left">0.398 ± 0.031</td>
</tr>
<tr>
<td align="left"/>
<td align="left">Target <italic>γ</italic></td>
<td align="left">Obstacle <italic>γ</italic></td>
<td align="left">Path <italic>γ</italic></td>
</tr>
<tr>
<td align="left">Task 2+3 synthesized</td>
<td align="left">0.773 ± 0.017</td>
<td align="left">0.689 ± 0.015</td>
<td align="left">0.928 ± 0.006</td>
</tr>
<tr>
<td align="left">Task 4</td>
<td align="left">0.768 ± 0.009</td>
<td align="left">0.679 ± 0.019</td>
<td align="left">0.936 ± 0.006</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec017">
<title>Quantitative results and comparisons to alternative models</title>
<p>Next we compare our model with several alternative hypotheses. The full modular IRL model chooses the action greedily that maximizes the Q-value function of each state using both estimated <italic>r</italic> and <italic>γ</italic>. An ablation study is conducted to demonstrate the relative importance of the variables in the model. The binary reward agent estimates <italic>γ</italic> only, and uses a unit reward of 1 for the module that is task-relevant, e.g., in Task 2 the path and the obstacle modules would have rewards of +1 and -1 respectively, and the target module would have a reward of 0. The fixed <italic>γ</italic> agents estimate <italic>r</italic> only, and use fixed <italic>γ</italic> = 0.1, 0.5, 0.99. A Bayesian IRL agent without modularization and assumes a fixed discount factor [<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>] is also implemented where the implementation details can be found in <xref ref-type="supplementary-material" rid="pcbi.1006518.s003">S3 Appendix</xref>.</p>
<p>We choose two performance metrics to evaluate these models. The first one is the number of objects intercepted by the agent’s entire trajectory under different task conditions. <xref ref-type="fig" rid="pcbi.1006518.g006">Fig 6</xref> shows the performance of different models ((A) targets and (B) obstacles). Overall, the modular IRL model has the closest performance to the human data across task conditions. Note that the number of targets collected is only a little affected by the avoid instruction and obstacles avoided do not change very much with the target instruction, supporting the previous claim that the modules in this experiment are independent hence task-relevant module values are stable. Bayesian IRL and fixed <italic>γ</italic> = 0.99 models perform poorly—the number of objects hit does not vary accordingly with task instructions. The binary reward model, <italic>γ</italic> = 0.1, 0.5 reflect task instructions correctly but are less accurate than the full modular IRL model.</p>
<fig id="pcbi.1006518.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Average number of targets collected/obstacles hit when different models perform the navigation task across all trials.</title>
<p>There are 12 targets/obstacles each in the virtual room. Error bars indicate standard error of the mean (<italic>N</italic> = 100).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g006" xlink:type="simple"/>
</fig>
<p>The second quantitative evaluation metric would be the angular difference, i.e., policy agreement, which is obtained by placing an agent in the same state as a human and measuring the angular difference between the agent’s action and the human subject’s action. This metric differs from the previous one because it emphasizes more on the accuracy of local decisions instead of the whole trajectory. Thus this angular difference is a local metric instead of a holistic one. The comparison results are shown in <xref ref-type="table" rid="pcbi.1006518.t002">Table 2</xref>. All modular RL agents are more accurate in predicting human actions comparing to the traditional Bayesian IRL algorithm. Again the full modular IRL model results in higher accuracy comparing to the alternative models. The binary reward model has comparable performance and is in general better than the models that have the discount factor fixed. This supports our claim that module-specific discount factor plays an important role in modeling human behaviors and should be estimated from data.</p>
<table-wrap id="pcbi.1006518.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.t002</object-id>
<label>Table 2</label>
<caption>
<title>Evaluation of the modular agent’s performance compared with baseline agents, measured by the average angular difference (in degrees) compared to actual human decisions.</title>
<p>The results are presented as mean ± standard error (<italic>N</italic> = 100). The agent that uses the full model outperforms all other models.</p>
</caption>
<alternatives>
<graphic id="pcbi.1006518.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left">Task 1</th>
<th align="left">Task 2</th>
<th align="left">Task 3</th>
<th align="left">Task 4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Bayesian IRL</td>
<td align="left">53.87±2.54</td>
<td align="left">53.37 ± 2.71</td>
<td align="left">59.86 ± 2.00</td>
<td align="left">51.09 ± 2.60</td>
</tr>
<tr>
<td align="left">Fixed <italic>γ</italic> = 0.1</td>
<td align="left">31.74±0.88</td>
<td align="left">39.43±1.18</td>
<td align="left">36.16±0.75</td>
<td align="left">41.40±0.88</td>
</tr>
<tr>
<td align="left">Fixed <italic>γ</italic> = 0.5</td>
<td align="left">21.46±0.46</td>
<td align="left">36.04±1.16</td>
<td align="left">34.20±0.78</td>
<td align="left">39.14±0.92</td>
</tr>
<tr>
<td align="left">Fixed <italic>γ</italic> = 0.99</td>
<td align="left">18.19±0.32</td>
<td align="left">27.63±1.41</td>
<td align="left">28.61±0.93</td>
<td align="left">31.63±1.08</td>
</tr>
<tr>
<td align="left">Binary Reward</td>
<td align="left">17.66±0.38</td>
<td align="left">27.66±1.44</td>
<td align="left">29.97±0.72</td>
<td align="left">29.80±0.95</td>
</tr>
<tr>
<td align="left">MIRL (Full Model)</td>
<td align="left">17.94±0.33</td>
<td align="left">27.39±1.46</td>
<td align="left">26.98±0.80</td>
<td align="left">27.65±1.02</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To summarize, we are able to predict human novel trajectories in different environments on the basis of rewards and discount factors estimated from behavioral data. Since we do not know the actual set of visual operations involved in walking through a cluttered room like this, the fact that we can reproduce the trajectories suggests that the three chosen modules can account for a substantial fraction of the behavior while vision may be used for other tasks. In fact, close to half the fixations made by the subject are on regions of the environment other than the path or objects [<xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>]. This suggests that there may be other visual computations going on but that they do not have much influence on the behavior. Thus the modular RL agents generate reasonable hypotheses about underlying human decision-making mechanism.</p>
<p>These results provides a strong support for using modular RL as the model for explaining such multitask navigation behaviors, and modular IRL as a sample efficient algorithm to estimate rewards and discount factors. Bayesian IRL has to deal with a complex high-dimensional state space and settle for its approximations for a dynamic multi-task problem with limited data, while modular RL can easily reduce the dimensionality of the state-space by factoring out sub-tasks. Therefore the algorithm significantly outperforms the previous standard IRL method in terms of the accuracy in reproducing human behaviors.</p>
</sec>
<sec id="sec018">
<title>Related work in reinforcement learning</title>
<p>The proposed modular IRL algorithm is an extension and refinement of [<xref ref-type="bibr" rid="pcbi.1006518.ref019">19</xref>] which introduced the first modular IRL and demonstrated its effectiveness using an simulated avatar. The navigation tasks are similar but we use data from actual human subjects. While they use a simulated human avatar and moving from the straight path, our curved path proves quite different in practice, as well, being significantly more challenging for both humans and virtual agents. We then generalize the state space to let the agent consider multiple objects for each module, while the original work assumes the agent considers one nearest object of each module.</p>
<p>Bayesian IRL was first introduced by [<xref ref-type="bibr" rid="pcbi.1006518.ref036">36</xref>] as a principled way of approaching an ill-posed reward learning problem. Existing works using Bayesian IRL usually experiment in discretized gridworlds with no more than 1000 states with an exception being the work of [<xref ref-type="bibr" rid="pcbi.1006518.ref039">39</xref>] which was able to test on a goal-oriented MDP with 20,518 states using hierarchical Bayesian IRL.</p>
<p>The modular RL architecture proposed in this work is most similar to a recent work in [<xref ref-type="bibr" rid="pcbi.1006518.ref040">40</xref>], in which they decompose the reward function in the same way as the modular reinforcement learning. Their focus is not on modeling human behavior, but rather on using deep reinforcement learning to learn a separate value function for each subtask and combining them to obtain a good policy. Other examples of divide-and-conquer approach in RL include factored MDP [<xref ref-type="bibr" rid="pcbi.1006518.ref041">41</xref>] and co-articulation [<xref ref-type="bibr" rid="pcbi.1006518.ref042">42</xref>].</p>
<p>Hierarchical RL [<xref ref-type="bibr" rid="pcbi.1006518.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref044">44</xref>] utilizes the idea of <italic>temporal abstraction</italic> to allow more efficient computation of the policy. [<xref ref-type="bibr" rid="pcbi.1006518.ref045">45</xref>] analyzes human decision data in spatial navigation tasks and the Tower of Hanoi; they suggest that human subjects learn to decompose tasks and construct action hierarchy in an optimal way. In contrast with that approach, modular RL assumes <italic>parallel decomposition</italic> of the task. The difference can be visualized in <xref ref-type="fig" rid="pcbi.1006518.g007">Fig 7</xref>. These two approaches are complementary, and are both important for understanding and reproducing natural behaviors. For example, a hierarchical RL agent could have multiple concurrent <italic>options</italic> [<xref ref-type="bibr" rid="pcbi.1006518.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref044">44</xref>] executing at a given time for different behavioral objectives. Another possibility is to extend the modular RL to a two-level hierarchical system. Learned module policies are stored and a higher-level scheduler or arbitrator decides which modules to activate or deactivate given the current context and the protocol to synthesize module policies. An example of this type of architecture can be found in [<xref ref-type="bibr" rid="pcbi.1006518.ref002">2</xref>].</p>
<fig id="pcbi.1006518.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006518.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Modular reinforcement learning (left) vs. hierarchical reinforcement learning (right).</title>
<p>Modular RL assumes modules run concurrently and do not extend over multiple time steps. Hierarchical RL assumes that a single option may extend over multiple time steps.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>This paper formalizes a modular reinforcement learning model for natural multitask behaviors. Modular RL is more suitable for modeling human behaviors in natural tasks while standard RL serves as a general model for reward-seeking behaviors. The two important variables in modular RL are module-specific reward and discount factor, which can be jointly estimated from behavioral data using the proposed modular IRL algorithm. A computer simulation demonstrated the validity and sample efficiency of the modular IRL. In a virtual-reality human navigation experiment, we showed multitask human navigation behaviors, across subjects and under different instructions, can be modeled and reproduced using modular RL.</p>
<p>Modular RL/IRL makes it possible to estimate the subjective value of particular human behavioral goals. Over the last 15 years it has become clear that the brain’s internal reward circuitry can provide a mechanism for the role of tasks on both gaze behavior and action choices. It is thought that the ventromedial prefrontal cortex and basal ganglia circuits encode the subjective values driving behavior [<xref ref-type="bibr" rid="pcbi.1006518.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1006518.ref048">48</xref>]. The present work shows that it is possible to get a realistic estimate of the subjective value of goals in naturalistic behavior, and these values might reflect the underlying reward machinery. Many of the reward effects observed for neurons have very simple choice response paradigms. Thus it is important to attempt to link the primary rewards used in experimental paradigms and the secondary rewards that operate in natural behavior. Previous human experiments have typically used simple behaviors with money or points as rewards. In our experiment we used instructions to bias particular aspects of basic natural behavior with no explicit rewards.</p>
<p>The results provide support for a modular cognitive architecture when modeling natural visually guided behaviors. Modularization reduces the size of state space and alleviates the curse of dimensionality. Consequently modular IRL is more sample efficient than the standard Bayesian IRL. In addition, modular RL estimates a discount factor for every module hence it is more flexible and powerful than a standard RL model in which the discount factor is unitary and fixed. The modeling result suggests having such flexibility is indeed helpful. It may also explain why basal ganglia has the mechanism to implement multiple discount factors [<xref ref-type="bibr" rid="pcbi.1006518.ref016">16</xref>].</p>
<p>The decomposition of global task also allows humans to reuse a learned module later in a new environment. This claim is supported by the observation that task-relevant module rewards and discount factors are stable and generalize to a different task condition. When immersed in a new environment, the simple form of <xref ref-type="disp-formula" rid="pcbi.1006518.e012">Eq (4)</xref> allows value function to be computed with reasonable cognitive load. It is possible that subjects learn stable values for the costs of particular actions like walking and obstacle avoidance and these subjective values factor into momentary action decisions [<xref ref-type="bibr" rid="pcbi.1006518.ref001">1</xref>]. For example, humans direct gaze to nearby pedestrians in a simple uninstructed walking context with a probability close to 0.5, with small variability between subjects [<xref ref-type="bibr" rid="pcbi.1006518.ref049">49</xref>] and a similar gaze distribution was found in a virtual environment [<xref ref-type="bibr" rid="pcbi.1006518.ref050">50</xref>]. These values may change in more complex contexts, as in the decoy effect for example [<xref ref-type="bibr" rid="pcbi.1006518.ref051">51</xref>]. The present work provides a way of testing the circumstances in which such subjective values might change.</p>
<p>Modular RL allows intuitive interpretation for multitask behaviors, where relative importance and reward discounting rates can be compared between modules directly. We expect this modular approach of RL can be applied to and can explain many natural tasks. [<xref ref-type="bibr" rid="pcbi.1006518.ref052">52</xref>] has shown that a wide range of human behaviors can be modeled as consisting of microbehaviors, so many behaviors are a mixture of simple modules and could potentially be modeled in this way.</p>
<p>A question remains of how these modules are formed originally. The intuition for a modularized strategy comes from two conjectures: learning is incremental and attentional resource is limited. From a developmental perspective, a complicated natural task is often divided in to subtasks when learning happens, e.g., curriculum learning [<xref ref-type="bibr" rid="pcbi.1006518.ref053">53</xref>], hence a real-time decision-making rule is likely to be a combination of pre-learned subroutines. A subtask is attended when needed to save computational resource.</p>
<sec id="sec020">
<title>Limitations of the model and future work</title>
<p>Although modular RL/IRL is able to produce trajectories that are similar to human behavior, the match was imperfect as demonstrated by the angular difference. One difficulty with modeling human behavior is that we defined the state space and a set of modules by hand without knowing the actual state representation or task decomposition that the human uses. This may account for the discrepancy between the human and agent policies. Ideally, we could learn state representation from data, but this involves the challenging task of combining representation learning and IRL. The work in [<xref ref-type="bibr" rid="pcbi.1006518.ref054">54</xref>] provides a potential method for inferencing goals and states for the modules. Recent development in deep reinforcement learning [<xref ref-type="bibr" rid="pcbi.1006518.ref055">55</xref>] may possibly lead to a data-driven approach to IRL that can learn state representation from data.</p>
<p>An important assumption about the centralized arbitrator of the modules needs to be examined more carefully in the future: In our model, an agent forms global Q-values by summing up module Q-values [<xref ref-type="bibr" rid="pcbi.1006518.ref021">21</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref029">29</xref>]. There has been work examining more sophisticated mechanisms for global decision making [<xref ref-type="bibr" rid="pcbi.1006518.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref057">57</xref>]. For example, one could schedule modules according to an attention mechanism [<xref ref-type="bibr" rid="pcbi.1006518.ref056">56</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref058">58</xref>]. Whether these mechanisms can better explain human behaviors remains an open question that should be explored.</p>
<p>An important consequence of being able to get a quantitatively estimated subjective reward and discount factor of a module is that it is possible to test whether these values are stable across contexts. For example, the value of avoiding an obstacle should be stable across moderate variations in the environment such as the changes in obstacle density or changes in the visual appearance of the environment. If this is true, then it is possible to make predictions about behavior in other contexts using learned modules. And it would also be possible to use the prediction error to indicate that other factors need to be considered.</p>
<p>Estimates of the value of the underlying behaviors will also allow prediction of the gaze patterns subjects make in the environment. It has been suggested that gaze patterns reflect both the subjective value of a target and uncertainty about task-relevant state [<xref ref-type="bibr" rid="pcbi.1006518.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref059">59</xref>, <xref ref-type="bibr" rid="pcbi.1006518.ref060">60</xref>]. For example, gaze should be frequently deployed to look at pedestrians in a crowded environment since it is important to avoid collisions and there is high uncertainty about their location. Also gaze is deployed very differently depending on the terrain and the need to locate stable footholds, reflecting the increased uncertainty of rocky terrain [<xref ref-type="bibr" rid="pcbi.1006518.ref061">61</xref>]. Estimates of the subjective value might thus allow inferences about uncertainty as well.</p>
<p>In conclusion, we have demonstrated that modular reinforcement learning can plausibly account for sequences of sensory-motor decisions in a natural context, and it is possible to estimate the internal reward value of behavioral components such as path following, target collection, and obstacle avoidance. The estimated reward values and discount factors enabled us to predict long walking trajectories in a novel environment. This framework provides a potentially useful tool for exploring the task structure of natural behavior, and investigating how momentary decisions are modulated by internal rewards and discount factors.</p>
</sec>
</sec>
<sec id="sec021">
<title>Supporting information</title>
<supplementary-material id="pcbi.1006518.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Simulation results.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006518.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.s002" xlink:type="simple">
<label>S2 Appendix</label>
<caption>
<title>One-way ANOVA for estimated rewards.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006518.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.s003" xlink:type="simple">
<label>S3 Appendix</label>
<caption>
<title>Bayesian inverse reinforcement learning.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1006518.s004" mimetype="video/quicktime" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006518.s004" xlink:type="simple">
<label>S1 Video</label>
<caption>
<title>A sample video from collected human data.</title>
<p>The attached video file shows a typical experimental trial from the subject’s point of view, with motion tracking eye tracking enabled (the white cross). The task of this particular trial is to collect the targets, avoid the obstacles, and follow the path at the same time.</p>
<p>(MOV)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1006518.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>. <article-title>Vision and action</article-title>. <source>Annual review of vision science</source>. <year>2017</year>;<volume>3</volume>:<fpage>389</fpage>–<lpage>413</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-vision-102016-061437" xlink:type="simple">10.1146/annurev-vision-102016-061437</ext-link></comment> <object-id pub-id-type="pmid">28715958</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sprague</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>A</given-names></name>. <article-title>Modeling embodied visual behaviors</article-title>. <source>ACM Transactions on Applied Perception (TAP)</source>. <year>2007</year>;<volume>4</volume>(<issue>2</issue>):<fpage>11</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1265957.1265960" xlink:type="simple">10.1145/1265957.1265960</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>. <article-title>Task and context determine where you look</article-title>. <source>Journal of vision</source>. <year>2007</year>;<volume>7</volume>(<issue>14</issue>):<fpage>16</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/7.14.16" xlink:type="simple">10.1167/7.14.16</ext-link></comment> <object-id pub-id-type="pmid">18217811</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tong</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Zohar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>. <article-title>Control of gaze while walking: task structure, reward, and uncertainty</article-title>. <source>Journal of Vision</source>. <year>2017</year>;. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/17.1.28" xlink:type="simple">10.1167/17.1.28</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref005">
<label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>. <source>Introduction to reinforcement learning</source>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolpert</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Landy</surname> <given-names>MS</given-names></name>. <article-title>Motor control is decision-making</article-title>. <source>Current opinion in neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>6</issue>):<fpage>996</fpage>–<lpage>1003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.05.003" xlink:type="simple">10.1016/j.conb.2012.05.003</ext-link></comment> <object-id pub-id-type="pmid">22647641</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haruno</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kuroda</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Toyama</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kimura</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>A neural correlate of reward-based behavioral learning in caudate nucleus: a functional magnetic resonance imaging study of a stochastic decision task</article-title>. <source>The Journal of Neuroscience</source>. <year>2004</year>;<volume>24</volume>(<issue>7</issue>):<fpage>1660</fpage>–<lpage>1665</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3417-03.2004" xlink:type="simple">10.1523/JNEUROSCI.3417-03.2004</ext-link></comment> <object-id pub-id-type="pmid">14973239</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Holroyd</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>Coles</surname> <given-names>MG</given-names></name>. <article-title>The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity</article-title>. <source>Psychological review</source>. <year>2002</year>;<volume>109</volume>(<issue>4</issue>):<fpage>679</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.109.4.679" xlink:type="simple">10.1037/0033-295X.109.4.679</ext-link></comment> <object-id pub-id-type="pmid">12374324</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>. <article-title>Efficient reinforcement learning: computational theories, neuroscience and robotics</article-title>. <source>Current opinion in neurobiology</source>. <year>2007</year>;<volume>17</volume>(<issue>2</issue>):<fpage>205</fpage>–<lpage>212</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2007.03.004" xlink:type="simple">10.1016/j.conb.2007.03.004</ext-link></comment> <object-id pub-id-type="pmid">17374483</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Foster</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>A model of hippocampally dependent navigation, using the temporal difference learning rule</article-title>. <source>Hippocampus</source>. <year>2000</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/(SICI)1098-1063(2000)10:1&lt;1::AID-HIPO1&gt;3.0.CO;2-1" xlink:type="simple">10.1002/(SICI)1098-1063(2000)10:1&lt;1::AID-HIPO1&gt;3.0.CO;2-1</ext-link></comment> <object-id pub-id-type="pmid">10706212</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Seo</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Jung</surname> <given-names>MW</given-names></name>. <article-title>Neural basis of reinforcement learning and decision making</article-title>. <source>Annual review of neuroscience</source>. <year>2012</year>;<volume>35</volume>:<fpage>287</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev-neuro-062111-150512" xlink:type="simple">10.1146/annurev-neuro-062111-150512</ext-link></comment> <object-id pub-id-type="pmid">22462543</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cardinal</surname> <given-names>RN</given-names></name>. <article-title>Neural systems implicated in delayed and probabilistic reinforcement</article-title>. <source>Neural Networks</source>. <year>2006</year>;<volume>19</volume>(<issue>8</issue>):<fpage>1277</fpage>–<lpage>1301</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2006.03.004" xlink:type="simple">10.1016/j.neunet.2006.03.004</ext-link></comment> <object-id pub-id-type="pmid">16938431</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>69</volume>(<issue>6</issue>):<fpage>1204</fpage>–<lpage>1215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.02.027" xlink:type="simple">10.1016/j.neuron.2011.02.027</ext-link></comment> <object-id pub-id-type="pmid">21435563</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Momennejad</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Russek</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Cheong</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Botvinick</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>. <article-title>The successor representation in human reinforcement learning</article-title>. <source>Nature Human Behaviour</source>. <year>2017</year>;<volume>1</volume>(<issue>9</issue>):<fpage>680</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41562-017-0180-8" xlink:type="simple">10.1038/s41562-017-0180-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>. <article-title>Modulators of decision making</article-title>. <source>Nature neuroscience</source>. <year>2008</year>;<volume>11</volume>(<issue>4</issue>):<fpage>410</fpage>–<lpage>416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn2077" xlink:type="simple">10.1038/nn2077</ext-link></comment> <object-id pub-id-type="pmid">18368048</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okada</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ueda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yamawaki</surname> <given-names>S</given-names></name>. <article-title>Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops</article-title>. <source>Nature neuroscience</source>. <year>2004</year>;<volume>7</volume>(<issue>8</issue>):<fpage>887</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1279" xlink:type="simple">10.1038/nn1279</ext-link></comment> <object-id pub-id-type="pmid">15235607</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>D</given-names></name>. <article-title>Modeling task control of eye movements</article-title>. <source>Current Biology</source>. <year>2014</year>;<volume>24</volume>(<issue>13</issue>):<fpage>R622</fpage>–<lpage>R628</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2014.05.020" xlink:type="simple">10.1016/j.cub.2014.05.020</ext-link></comment> <object-id pub-id-type="pmid">25004371</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Ng AY, Russell SJ. Algorithms for Inverse Reinforcement Learning. In: Proceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc.; 2000. p. 663–670.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Modular inverse reinforcement learning for visuomotor behavior</article-title>. <source>Biological cybernetics</source>. <year>2013</year>;<volume>107</volume>(<issue>4</issue>):<fpage>477</fpage>–<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-013-0562-6" xlink:type="simple">10.1007/s00422-013-0562-6</ext-link></comment> <object-id pub-id-type="pmid">23832417</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Samejima</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Doya</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kawato</surname> <given-names>M</given-names></name>. <article-title>Inter-module credit assignment in modular reinforcement learning</article-title>. <source>Neural Networks</source>. <year>2003</year>;<volume>16</volume>(<issue>7</issue>):<fpage>985</fpage>–<lpage>994</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(02)00235-6" xlink:type="simple">10.1016/S0893-6080(02)00235-6</ext-link></comment> <object-id pub-id-type="pmid">14692633</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Sprague N, Ballard D. Multiple-goal reinforcement learning with modular Sarsa (O). In: Proceedings of the 18th international joint conference on Artificial intelligence. Morgan Kaufmann Publishers Inc.; 2003. p. 1445–1447.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Kit</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sullivan</surname> <given-names>B</given-names></name>. <article-title>A hierarchical modular architecture for embodied cognition</article-title>. <source>Multisensory research</source>. <year>2013</year>;<volume>26</volume>:<fpage>177</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1163/22134808-00002414" xlink:type="simple">10.1163/22134808-00002414</ext-link></comment> <object-id pub-id-type="pmid">23713205</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gershman</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Pesaran</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Daw</surname> <given-names>ND</given-names></name>. <article-title>Human reinforcement learning subdivides structured action spaces by learning effector-specific values</article-title>. <source>The Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>43</issue>):<fpage>13524</fpage>–<lpage>13531</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2469-09.2009" xlink:type="simple">10.1523/JNEUROSCI.2469-09.2009</ext-link></comment> <object-id pub-id-type="pmid">19864565</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schweighofer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bertin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shishida</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okamoto</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Yamawaki</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Low-serotonin levels increase delayed reward discounting in humans</article-title>. <source>the Journal of Neuroscience</source>. <year>2008</year>;<volume>28</volume>(<issue>17</issue>):<fpage>4528</fpage>–<lpage>4532</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4982-07.2008" xlink:type="simple">10.1523/JNEUROSCI.4982-07.2008</ext-link></comment> <object-id pub-id-type="pmid">18434531</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Story</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Vlaev</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Seymour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Darzi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>. <article-title>Does temporal discounting explain unhealthy behavior? A systematic review and reinforcement learning perspective</article-title>. <source>Frontiers in behavioral neuroscience</source>. <year>2014</year>;<volume>8</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnbeh.2014.00076" xlink:type="simple">10.3389/fnbeh.2014.00076</ext-link></comment> <object-id pub-id-type="pmid">24659960</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hitzel</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tong</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schütz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>. <article-title>Objects in the peripheral visual field influence gaze location in natural vision</article-title>. <source>Journal of vision</source>. <year>2015</year>;<volume>15</volume>(<issue>12</issue>):<fpage>e783</fpage>–<lpage>e783</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.12.783" xlink:type="simple">10.1167/15.12.783</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rothkopf</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Image statistics at the point of gaze during human navigation</article-title>. <source>Visual neuroscience</source>. <year>2009</year>;<volume>26</volume>(<issue>01</issue>):<fpage>81</fpage>–<lpage>92</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0952523808080978" xlink:type="simple">10.1017/S0952523808080978</ext-link></comment> <object-id pub-id-type="pmid">19309533</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tong</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Zohar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>S</given-names></name>. <source>Multitask Human Navigation in VR with Motion Tracking</source>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.255882" xlink:type="simple">https://doi.org/10.5281/zenodo.255882</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Russell SJ, Zimdars A. Q-Decomposition for Reinforcement Learning Agents. In: Proceedings of the 20th International Conference on Machine Learning (ICML-03); 2003. p. 656–663.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaelbling</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>Littman</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>AW</given-names></name>. <article-title>Reinforcement learning: A survey</article-title>. <source>Journal of artificial intelligence research</source>. <year>1996</year>;<volume>4</volume>:<fpage>237</fpage>–<lpage>285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1613/jair.301" xlink:type="simple">10.1613/jair.301</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khatib</surname> <given-names>O</given-names></name>. <article-title>Real-time obstacle avoidance for manipulators and mobile robots</article-title>. <source>The international journal of robotics research</source>. <year>1986</year>;<volume>5</volume>(<issue>1</issue>):<fpage>90</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/027836498600500106" xlink:type="simple">10.1177/027836498600500106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Arkin</surname> <given-names>RC</given-names></name>. <article-title>Motor schema-based mobile robot navigation</article-title>. <source>The International journal of robotics research</source>. <year>1989</year>;<volume>8</volume>(<issue>4</issue>):<fpage>92</fpage>–<lpage>112</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/027836498900800406" xlink:type="simple">10.1177/027836498900800406</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Fajen</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>WH</given-names></name>. <article-title>Visual navigation and obstacle avoidance using a steering potential function</article-title>. <source>Robotics and Autonomous Systems</source>. <year>2006</year>;<volume>54</volume>(<issue>4</issue>):<fpage>288</fpage>–<lpage>299</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.robot.2005.11.004" xlink:type="simple">10.1016/j.robot.2005.11.004</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Abbeel P, Ng AY. Apprenticeship learning via inverse reinforcement learning. In: Proceedings of the twenty-first international conference on Machine learning. ACM; 2004. p. 1.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Ziebart BD, Maas A, Bagnell JA, Dey AK. Maximum entropy inverse reinforcement learning. In: Proceedings of the 23rd national conference on Artificial intelligence-Volume 3. AAAI Press; 2008. p. 1433–1438.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref036">
<label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Ramachandran D, Amir E. Bayesian inverse reinforcement learning. In: Proceedings of the 20th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc.; 2007. p. 2586–2591.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref037">
<label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lopes</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Melo</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Montesano</surname> <given-names>L</given-names></name>. <chapter-title>Active learning for reward estimation in inverse reinforcement learning</chapter-title>. In: <source>Machine Learning and Knowledge Discovery in Databases</source>. <publisher-name>Springer</publisher-name>; <year>2009</year>. p. <fpage>31</fpage>–<lpage>46</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref038">
<label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Babes M, Marivate V, Subramanian K, Littman ML. Apprenticeship learning about multiple intentions. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11); 2011. p. 897–904.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Choi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>KE</given-names></name>. <article-title>Hierarchical bayesian inverse reinforcement learning</article-title>. <source>IEEE transactions on cybernetics</source>. <year>2015</year>;<volume>45</volume>(<issue>4</issue>):<fpage>793</fpage>–<lpage>805</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TCYB.2014.2336867" xlink:type="simple">10.1109/TCYB.2014.2336867</ext-link></comment> <object-id pub-id-type="pmid">25291805</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Van Seijen H, Fatemi M, Romoff J, Laroche R, Barnes T, Tsang J. Hybrid reward architecture for reinforcement learning. In: Advances in Neural Information Processing Systems; 2017. p. 5392–5402.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Guestrin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Koller</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Parr</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Venkataraman</surname> <given-names>S</given-names></name>. <article-title>Efficient solution algorithms for factored MDPs</article-title>. <source>Journal of Artificial Intelligence Research</source>. <year>2003</year>; p. <fpage>399</fpage>–<lpage>468</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1613/jair.1000" xlink:type="simple">10.1613/jair.1000</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Rohanimanesh K, Mahadevan S. Coarticulation: An approach for generating concurrent plans in Markov decision processes. In: Proceedings of the 22nd International Conference on Machine Learning. ACM; 2005. p. 720–727.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dietterich</surname> <given-names>TG</given-names></name>. <article-title>Hierarchical reinforcement learning with the MAXQ value function decomposition</article-title>. <source>J Artif Intell Res(JAIR)</source>. <year>2000</year>;<volume>13</volume>:<fpage>227</fpage>–<lpage>303</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1613/jair.639" xlink:type="simple">10.1613/jair.639</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sutton</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Precup</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>S</given-names></name>. <article-title>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</article-title>. <source>Artificial intelligence</source>. <year>1999</year>;<volume>112</volume>(<issue>1</issue>):<fpage>181</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0004-3702(99)00052-1" xlink:type="simple">10.1016/S0004-3702(99)00052-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Solway</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Diuk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Córdova</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barto</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Niv</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Optimal behavioral hierarchy</article-title>. <source>PLoS computational biology</source>. <year>2014</year>;<volume>10</volume>(<issue>8</issue>):<fpage>e1003779</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003779" xlink:type="simple">10.1371/journal.pcbi.1003779</ext-link></comment> <object-id pub-id-type="pmid">25122479</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Levy</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Glimcher</surname> <given-names>PW</given-names></name>. <article-title>The root of all value: a neural common currency for choice</article-title>. <source>Current opinion in neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>6</issue>):<fpage>1027</fpage>–<lpage>1038</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.06.001" xlink:type="simple">10.1016/j.conb.2012.06.001</ext-link></comment> <object-id pub-id-type="pmid">22766486</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bogacz</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Moraud</surname> <given-names>EM</given-names></name>, <name name-style="western"><surname>Abdi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Magill</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Baufreton</surname> <given-names>J</given-names></name>. <article-title>Properties of neurons in external globus pallidus can support optimal action selection</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>7</issue>):<fpage>e1005004</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005004" xlink:type="simple">10.1371/journal.pcbi.1005004</ext-link></comment> <object-id pub-id-type="pmid">27389780</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zénon</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Duclos</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Carron</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Witjas</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Baunez</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Régis</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The human subthalamic nucleus encodes the subjective value of reward and the cost of effort during decision-making</article-title>. <source>Brain</source>. <year>2016</year>;<volume>139</volume>(<issue>6</issue>):<fpage>1830</fpage>–<lpage>1843</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/aww075" xlink:type="simple">10.1093/brain/aww075</ext-link></comment> <object-id pub-id-type="pmid">27190012</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jovancevic-Misic</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>. <article-title>Adaptive gaze control in natural environments</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>19</issue>):<fpage>6234</fpage>–<lpage>6238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5570-08.2009" xlink:type="simple">10.1523/JNEUROSCI.5570-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19439601</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jovancevic</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sullivan</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>. <article-title>Control of attention and gaze in complex environments</article-title>. <source>Journal of Vision</source>. <year>2006</year>;<volume>6</volume>(<issue>12</issue>):<fpage>9</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/6.12.9" xlink:type="simple">10.1167/6.12.9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huber</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Payne</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Puto</surname> <given-names>C</given-names></name>. <article-title>Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis</article-title>. <source>Journal of consumer research</source>. <year>1982</year>;<volume>9</volume>(<issue>1</issue>):<fpage>90</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/208899" xlink:type="simple">10.1086/208899</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref052">
<label>52</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <source>Brain computation as hierarchical abstraction</source>. <publisher-name>MIT Press</publisher-name>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref053">
<label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Bengio Y, Louradour J, Collobert R, Weston J. Curriculum learning. In: Proceedings of the 26th annual international conference on machine learning. ACM; 2009. p. 41–48.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref054">
<label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Baker CL, Tenenbaum JB, Saxe RR. Goal inference as inverse planning. In: Proceedings of the Annual Meeting of the Cognitive Science Society. vol. 29; 2007.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mnih</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Kavukcuoglu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Silver</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rusu</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Veness</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Bellemare</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>(<issue>7540</issue>):<fpage>529</fpage>–<lpage>533</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature14236" xlink:type="simple">10.1038/nature14236</ext-link></comment> <object-id pub-id-type="pmid">25719670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref056">
<label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Bhat S, Isbell CL, Mateas M. On the difficulty of modular reinforcement learning for real-world partial programming. In: Proceedings of the National Conference on Artificial Intelligence. vol. 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999; 2006. p. 318.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref057">
<label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Ring M, Schaul T. Q-error as a selection mechanism in modular reinforcement-learning systems. In: Proceedings of International Joint Conference on Artificial Intelligence. vol. 22; 2011. p. 1452.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref058">
<label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhang R, Song Z, Ballard DH. Global Policy Construction in Modular Reinforcement Learning. In: AAAI; 2015. p. 4226–4227.</mixed-citation>
</ref>
<ref id="pcbi.1006518.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Johnson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Sullivan</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>D</given-names></name>. <article-title>Predicting human visuomotor behaviour in a driving task</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2014</year>;<volume>369</volume>(<issue>1636</issue>):<fpage>20130044</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2013.0044" xlink:type="simple">10.1098/rstb.2013.0044</ext-link></comment> <object-id pub-id-type="pmid">24395971</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gottlieb</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hikosaka</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rangel</surname> <given-names>A</given-names></name>. <article-title>Attention, reward, and information seeking</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>46</issue>):<fpage>15497</fpage>–<lpage>15504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3270-14.2014" xlink:type="simple">10.1523/JNEUROSCI.3270-14.2014</ext-link></comment> <object-id pub-id-type="pmid">25392517</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006518.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Matthis</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Yates</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>. <article-title>Gaze and the control of foot placement when walking in natural terrain</article-title>. <source>Current Biology</source>. <year>2018</year>;<volume>28</volume>(<issue>8</issue>):<fpage>1224</fpage>–<lpage>1233</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2018.03.008" xlink:type="simple">10.1016/j.cub.2018.03.008</ext-link></comment> <object-id pub-id-type="pmid">29657116</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>