<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-02138</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003640</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A Normative Theory of Forgetting: Lessons from the Fruit Fly</article-title>
<alt-title alt-title-type="running-head">A Normative Theory of Forgetting</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Brea</surname><given-names>Johanni</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Urbanczik</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Senn</surname><given-names>Walter</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Physiology, University of Bern, Bern, Switzerland</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Physiology and Center for Cognition, Learning and Memory, University of Bern, Bern, Switzerland</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>Jean</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Brain and Spine Institute (ICM), France</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">johannibrea@gmail.com</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JB RU WS. Performed the experiments: JB. Analyzed the data: JB. Wrote the paper: JB RU WS.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>6</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>5</day><month>6</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>6</issue>
<elocation-id>e1003640</elocation-id>
<history>
<date date-type="received"><day>5</day><month>12</month><year>2013</year></date>
<date date-type="accepted"><day>9</day><month>4</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Brea et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Recent experiments revealed that the fruit fly <italic>Drosophila melanogaster</italic> has a dedicated mechanism for forgetting: blocking the G-protein Rac leads to slower and activating Rac to faster forgetting. This active form of forgetting lacks a satisfactory functional explanation. We investigated optimal decision making for an agent adapting to a stochastic environment where a stimulus may switch between being indicative of reward or punishment. Like <italic>Drosophila</italic>, an optimal agent shows forgetting with a rate that is linked to the time scale of changes in the environment. Moreover, to reduce the odds of missing future reward, an optimal agent may trade the risk of immediate pain for information gain and thus forget faster after aversive conditioning. A simple neuronal network reproduces these features. Our theory shows that forgetting in <italic>Drosophila</italic> appears as an optimal adaptive behavior in a changing environment. This is in line with the view that forgetting is adaptive rather than a consequence of limitations of the memory system.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The dominant perception of forgetting in science and society is that it is a nuisance in achieving better memory performance. However, recent experiments in the fruit fly show that the forgetting rate is biochemically adapted to the environment, raising doubts that slower forgetting <italic>per se</italic> is a desirable feature. Here we show that, in fact, optimal behavior in a stochastically changing environment requires a forgetting rate that is adapted to the time constant of the changes. The fruit fly behavior is compatible with the classical optimality criterion of choosing actions that maximize future rewards. A consequence of future reward maximization is that negative experiences that lead to timid behavior should be quickly forgotten in order to not miss rewarding opportunities. In economics this is called “minimization of opportunity costs”, and the fruit fly seems to care about it: punishment is forgotten faster than reward. Forgetting as a trait of optimality can further explain the different memory performances for multiple training sessions with varying inter-session intervals, as observed in a wide range of species from flies to humans. These aspects suggest to view forgetting as a dimension of adaptive behavior that is tuned to the environment to maximize subjective benefits.</p>
</abstract>
<funding-group><funding-statement>The work was supported by SNF grants 31003A_133094 and a SystemsX.ch grant (SynaptiX) evaluated by the SNF. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p><italic>Drosophila melanogaster</italic> forgets <xref ref-type="bibr" rid="pcbi.1003640-Tully1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Berry1">[2]</xref>. In itself this is unremarkable because forgetting as a behavioral phenomenon appears in any adaptive system of limited capacity; storing new associations will lead to interference with existing memories. Forgetting, in this sense, is just the flip side of learning. When capacity is not an issue, forgetting may nevertheless be caused by a useful mechanism: one that keeps a low memory load and thus prevents a slowdown of retrieval <xref ref-type="bibr" rid="pcbi.1003640-Rosenzweig1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Storm1">[4]</xref>. Consequently, capacity or retrieval limitations lie at the heart of standard theories of non-pathological forgetting <xref ref-type="bibr" rid="pcbi.1003640-Wixted1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Hardt1">[6]</xref>, which focus on interference and decay explanations. Alternatively, forgetting has been proposed to be an adaptive strategy that has evolved in response to the demands of a changing environment <xref ref-type="bibr" rid="pcbi.1003640-Kraemer1">[7]</xref>. It is the latter explanation that seems to apply to <italic>Drosophila</italic> where the experimental evidence suggests that the cause underlying forgetting is an active process which is modulated by the learning task and not by internal constraints of the memory system; in particular in olfactory conditioning tasks, reversal learning leads to faster forgetting <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref> whereas spaced training leads to slower forgetting compared to single or massed training <xref ref-type="bibr" rid="pcbi.1003640-Tully2">[9]</xref>. Further, forgetting in <italic>Drosophila</italic> seems rather idiosyncratic in that aversive conditioning is forgotten approximately twice as quickly as appetitive conditioning <xref ref-type="bibr" rid="pcbi.1003640-Tempel1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-CervantesSandoval1">[11]</xref>.</p>
<p>In psychology, the term forgetting commonly refers in “to the absence of expression of previously properly acquired memory in situations that normally cause such expression.” (<xref ref-type="bibr" rid="pcbi.1003640-Hardt1">[6]</xref>; see also <xref ref-type="bibr" rid="pcbi.1003640-Wixted2">[12]</xref>). Similarly, in conditioning experiments, one speaks of forgetting, when the conditioned stimulus fails to evoke the conditioned response at some point after successful conditioning <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Gonzalez1">[13]</xref>.</p>
<p>In the basic protocol for behavioral studies of memory in <italic>Drosophila</italic> <xref ref-type="bibr" rid="pcbi.1003640-Tully1">[1]</xref> a group of flies is placed into a tube for conditioning. There the flies are exposed to a specific odor and the exposure is paired with a reinforcer (sugar or electrical shock). Having experienced the pairing once or multiple times, the flies are removed from the conditioning tube. After a predefined delay time, the group is placed into the middle of a second, elongated tube for assessment. One side of the elongated tube is baited with the conditioned odor and, after a while, the fraction of flies is determined which exhibit the conditioned response by comparing the number of flies which are closer to the baited side of the tube with the number of flies closer to the un-baited side. The setup allows to measure memory performance (c.f. <xref ref-type="fig" rid="pcbi-1003640-g001">Fig. 1 D</xref>), i.e. expression of the conditioned response, as function of the delay time and of the conditioning protocol (e.g. magnitude of reinforcement, number of pairings). To check for bias in the setup, one typically in addition uses a second odor as a control which was not paired with a reinforcer.</p>
<fig id="pcbi-1003640-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003640.g001</object-id><label>Figure 1</label><caption>
<title>Agent acting in a changing environment.</title>
<p><bold>A</bold> The environmental state changes stochastically with rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e001" xlink:type="simple"/></inline-formula> between being rewarding, neutral or punishing. Unless mentioned otherwise, we choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e002" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e003" xlink:type="simple"/></inline-formula>. <bold>B</bold> Based on a policy (with forgetting, without forgetting) which may depend on past observations of the environmental state and current costs of responding, an agent shows the appetitive reaction (upward arrow) or the aversive reaction (downward arrow). The stochastic costs (i.i.d. with an exponential distribution with scale parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e004" xlink:type="simple"/></inline-formula>) for the appetitive/aversive reaction are shown above/below the white line. An agent with a policy that involves forgetting accumulates more reward than an agent without forgetting or immediate forgetting. <bold>C</bold> In an emulation of a classical conditioning experiment, the agent experiences a defined environmental state, and after a waiting period of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e005" xlink:type="simple"/></inline-formula> the agent has to react according to the internal policy. <bold>D</bold> Different policies lead to different outcomes in classical conditioning experiments. Shown is the fraction of agents choosing the conditioned response (conditioned resp.) at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e006" xlink:type="simple"/></inline-formula> after conditioning for agents subject to individual costs of responding.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003640.g001" position="float" xlink:type="simple"/></fig>
<p>That <italic>Drosophila</italic> has a dedicated mechanism to control forgetting was convincingly demonstrated by Shuai et al. <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref> and Berry et al. <xref ref-type="bibr" rid="pcbi.1003640-Berry1">[2]</xref>. Inhibition of the small G-protein Rac leads to slower decay of memory, extending it from a few hours to more than one day <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref>. Conversely, elevated Rac activity leads to faster forgetting <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref>. Similar results were achieved by modulation of a small subset of Dopamine neurons <xref ref-type="bibr" rid="pcbi.1003640-Berry1">[2]</xref>. Stimulating these neurons leads to faster forgetting after aversive and appetitive conditioning, while silencing these neurons leads to slower forgetting <xref ref-type="bibr" rid="pcbi.1003640-Berry1">[2]</xref>.</p>
<p>Given the importance of decision making, it appears unlikely that forgetting in <italic>Drosophila</italic> is a behavioral trait which is maladaptive in an ecological sense. Hence we investigated what generic model of the environment would justify the observed forgetting and in particular the asymmetry between aversive and appetitive conditioning. For this we mathematically determined optimal decision making strategies in environments with different associations between stimulus and reinforcement.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Basic model of decision making in a changing environment</title>
<p>For our model we assumed a simplified scenario where the conditioning pertains directly to the appetitive reaction. In particular, depending on the state of the environment, approaching the odor can lead to reward (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e007" xlink:type="simple"/></inline-formula>) or punishment (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e008" xlink:type="simple"/></inline-formula>) but it can also result in no reinforcement (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e009" xlink:type="simple"/></inline-formula>) (<xref ref-type="fig" rid="pcbi-1003640-g001">Fig. 1</xref>). Fleeing the odor, i.e the aversive reaction, never leads to reinforcement (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e010" xlink:type="simple"/></inline-formula>). An agent (fruit fly), whose goal is to maximize reinforcement, chooses between the appetitive and aversive reaction depending on past experience. To model the non-deterministic behavior observed in the experiments we assume that the two available behavioral options involve different costs of responding. These costs of responding, however, fluctuate from trial to trial causing no bias on average. For instance, a fly which happens to find itself to the right of the group initially could well have a smaller cost of responding for staying on this side of the assessment tube on this trial. More generally, the stochastic costs of responding can be seen as incorporating all other factors that also influence the behavior but do not depend on the past experiences that involve the conditioned stimulus. The total reward received by the agent is the external reinforcement (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e011" xlink:type="simple"/></inline-formula>) minus the cost of responding. Our agent takes this into account in decision making, and so the costs of responding result in trial to trial fluctuation in the behavior. Whether the appetitive reaction results in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e012" xlink:type="simple"/></inline-formula> depends on the state of the environment. This state changes slowly over time (according to a Markov chain, see <xref ref-type="sec" rid="s4">Methods</xref> and <xref ref-type="fig" rid="pcbi-1003640-g001">Fig. 1A</xref>). So when the appetitive reaction results in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e013" xlink:type="simple"/></inline-formula> on one trial, the same outcome is likely on an immediately subsequent trial, but as time goes by the odds increase that the appetitive reaction results in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e014" xlink:type="simple"/></inline-formula> or even punishment.</p>
</sec><sec id="s2b">
<title>The agent maintains a belief about the environmental state</title>
<p>If the agent knew the environmental state, the best policy would be simple: choose the appetitive (aversive) reaction if the environmental state is rewarding (punishing). Typically however, the agent does not know the actual environmental state but, at best, maintains a belief about it (see <xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2A</xref> and <xref ref-type="sec" rid="s4">Methods</xref>). In our model, the belief consists of the probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e015" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e017" xlink:type="simple"/></inline-formula> to receive rewarding, neutral or punishing reinforcement, respectively, after selecting the appetitive reaction. Geometrically, the belief can be represented as a position in a 2-dimensional belief space that stepwise changes after the appetitive reaction and thus gaining new information about the current environmental state and otherwise drifts towards an equilibrium (forgetting), see <xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2B</xref> (note that, since the three probabilities sum to one, the probability of the neutral state can be computed given the probabilities of the rewarding and punishing state, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e018" xlink:type="simple"/></inline-formula>).</p>
<fig id="pcbi-1003640-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003640.g002</object-id><label>Figure 2</label><caption>
<title>Belief and policy of an agent acting in a changing environment.</title>
<p><bold>A</bold> The belief about the environmental state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e019" xlink:type="simple"/></inline-formula> may influence the choice of the appetitive or aversive reaction. Only after the appetitive reaction, the agent gains new information about the true state of the environment. The belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e020" xlink:type="simple"/></inline-formula> and the agents knowledge about the transition probabilities of the environmental state combined with potentially new information determines the new belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e021" xlink:type="simple"/></inline-formula>. <bold>B</bold> The starting point of the arrows is a belief found by choosing the appetitive reaction once and receiving reward (green), punishment (red) or no reinforcement (blue). If the agent always chooses the aversive reaction thereafter, the belief drifts to the stationary state along the trajectories shown by the arrows. Possible belief states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e022" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e023" xlink:type="simple"/></inline-formula> can be represented as a point in the “belief space” (gray shaded triangle). <bold>C</bold> The regions in the belief space favoring the appetitive reaction (dark shading, upward arrow) over the aversive reaction (bright shading, downward arrow) depend on the policy and the costs of responding. The provident policy (lowest row) is biased towards the appetitive reaction. A larger cost for the aversive reaction than for the appetitive reaction (left column) decreases the region of the aversive reaction.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003640.g002" position="float" xlink:type="simple"/></fig>
<p>If e.g. a fly gets punished, the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e024" xlink:type="simple"/></inline-formula> to be punished again on the next trial is high (initial point of red trajectory in <xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2B</xref>). If subsequently the fly chooses the aversive reaction, the belief will drift towards a stationary value (end point of red trajectory in <xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2B</xref>). We assume that the agent has implicit knowledge, e.g. gathered by experience or through genetic encoding, about the transition rates of the environmental state.</p>
</sec><sec id="s2c">
<title>Acting according to a greedy policy leads to forgetting</title>
<p>Based on belief values and costs of responding one may define different policies. A greedy policy selects the appetitive reaction if the agent believes that reward is more probable than punishment and costs of responding are equal for both actions, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e025" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2C</xref> top, middle). If costs for one reaction are larger than for the other, the region in the belief space favoring this higher-cost reaction becomes smaller (<xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2C</xref> top, left and right). Immediately after conditioning, an agent has a strong belief that the environment is still in the same state as during conditioning. Thus, if the greedy policy determines action selection, an agent most likely chooses the conditioned response. As the belief drifts towards the stationary point, the stochastic costs of responding gain more influence on the decision making and thus an agent is more likely to have already forgotten the conditioning, i.e. the agent is more likely to choose the opposite of the conditioned response. We call this policy “greedy”, because it maximizes reward if only one choice is made but it is not necessarily optimal with respect to gaining future rewards. Technically, the greedy policy is equivalent to the optimal future discounted policy with discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e026" xlink:type="simple"/></inline-formula>, i.e. the policy that neglects future rewards.</p>
</sec><sec id="s2d">
<title>Dependence of the forgetting curve on parameter choices</title>
<p>In order to conveniently analyze the forgetting behavior under the greedy policy for different choices of the environmental parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e027" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e028" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003640-g001">Fig. 1A</xref>), we use a re-parametrization with the “probability of the neutral state” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e029" xlink:type="simple"/></inline-formula> and the “average reward” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e030" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e031" xlink:type="simple"/></inline-formula> denotes the stationary state probability of state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e032" xlink:type="simple"/></inline-formula> (see <xref ref-type="sec" rid="s4">Methods</xref> for the relationship between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e033" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e034" xlink:type="simple"/></inline-formula>). Changing the probability of the neutral state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e035" xlink:type="simple"/></inline-formula> has almost no effect on the forgetting curve (<xref ref-type="fig" rid="pcbi-1003640-g003">Fig. 3A</xref>, solid vs. dashed line). Increasing the average reward has the consequence that in the stationary state more agents select the appetitive reaction than the aversive reaction (<xref ref-type="fig" rid="pcbi-1003640-g003">Fig. 3A</xref>, solid vs. dotted line). The speed of forgetting a conditioned state (p, n or r) is determined by the rate of transitioning away from this state. <xref ref-type="fig" rid="pcbi-1003640-g003">Fig. 3A</xref> (solid vs. dash-dotted line) shows the effect of changing the rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e036" xlink:type="simple"/></inline-formula>, whose inverse is equal to the average number of timesteps the environment spends in the rewarding state: forgetting is faster for a larger rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e037" xlink:type="simple"/></inline-formula>. The variance of the costs of responding determines the impact of the costs of responding on decision making. For large variance the forgetting curve is closer to 0.5 than for small variance, since for large variance it is more likely that the costs of responding have a strong impact on decision making (<xref ref-type="fig" rid="pcbi-1003640-g003">Fig. 3B</xref>).</p>
<fig id="pcbi-1003640-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003640.g003</object-id><label>Figure 3</label><caption>
<title>Dependence of the forgetting curves on the model parameters.</title>
<p><bold>A</bold> Left: The stationary belief state in the absence of observations (indicated by dots) moves along the direction of the arrows for increasing probability of the neutral state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e038" xlink:type="simple"/></inline-formula> or increasing average reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e039" xlink:type="simple"/></inline-formula>. How fast the belief drifts towards the stationary state after receiving reward depends on the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e040" xlink:type="simple"/></inline-formula> that controls the “timescale of changes”. Right: Changing the probability of the neutral state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e041" xlink:type="simple"/></inline-formula> only marginally affects the forgetting curve (solid and dashed line). A smaller rate of changes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e042" xlink:type="simple"/></inline-formula> leads to slower forgetting (dash-dotted curve). A positive average reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e043" xlink:type="simple"/></inline-formula> leads to a higher fraction of agents choosing the appetitive reaction, which is here the conditioned response (dotted curve). <bold>B</bold> For a large variance of costs of responding (curve with scale parameter of the exponential distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e044" xlink:type="simple"/></inline-formula>) there are some agents that do not exhibit the conditioned response immediately after conditioning, since the costs of the conditioned response are too large. If the variance of the costs of responding is small (curve with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e045" xlink:type="simple"/></inline-formula>), most agents choose the conditioned response until their belief gets close to the stationary belief state.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003640.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Acting according to a provident policy leads to faster forgetting after aversive conditioning than after appetitive conditioning</title>
<p>While the difference in forgetting speed after appetitive and aversive forgetting could be a consequence of different transition rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e046" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e047" xlink:type="simple"/></inline-formula>, such a difference also arises if these rates are equal but the agent uses a provident policy, i.e. a policy that also takes into account future rewards. In the long run the provident policy is superior to the greedy policy (<xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4B</xref>). We therefore determined numerically a policy which approximately maximizes the reward rate, i.e. the total reward accumulated over a long period divided by the length of this period (see <xref ref-type="sec" rid="s4">Methods</xref>). The resulting policy is such that there are beliefs for which the appetitive reaction is chosen, even when the probability of punishment is larger than the probability of reward, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e048" xlink:type="simple"/></inline-formula>, and the costs of responding are equal for both actions (<xref ref-type="fig" rid="pcbi-1003640-g002">Fig. 2C</xref> bottom, middle). The reason for this becomes clearer when we look at what economists call the opportunity cost, i.e. the additional gain that has not been harvested because of missing to choose the (often by hindsight) better option <xref ref-type="bibr" rid="pcbi.1003640-Buchanan1">[14]</xref>. For the appetitive reaction, the agent's opportunity cost is given by the potentially lower cost for the aversive reaction. But for the aversive reaction, the agent's opportunity cost is not only the potentially lower cost for the appetitive reaction but also the lack of further information about the actual environmental state. This information is required for best exploitation in future trials. Assume, for instance, that at some point in time the agent believes that punishment is slightly more probable than reward and therefore sticks to the aversive reaction. Now, if the actual environmental state would be rewarding, the agent would not only miss the current reward but also misses subsequent rewards that could potentially be harvested while the state is still rewarding. When taking this opportunity cost into account, the agent will choose the appetitive reaction despite the belief state slightly favoring the aversive reaction. For an external observer this optimal choice behavior appears as a faster forgetting of the aversive memory. In short, the asymmetry in forgetting after aversive and appetitive conditioning (<xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4</xref>) arises because choosing the appetitive reaction is always informative about the current environmental state whereas choosing the aversive reaction is not.</p>
<fig id="pcbi-1003640-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003640.g004</object-id><label>Figure 4</label><caption>
<title>Asymmetry of behavior after aversive and appetitive conditioning.</title>
<p><bold>A</bold> An agent with a provident policy shows faster forgetting after aversive conditioning (red curve) than after appetitive conditioning (green curve). The boxes mark the behavior of the approximative model in C. <bold>B</bold> The total reward collected in free runs of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e049" xlink:type="simple"/></inline-formula> time bins (compare to <xref ref-type="fig" rid="pcbi-1003640-g001">Fig. 1B</xref>) is larger for the provident policy than for the greedy policy. Plotted are mean and s.e.m. for 40 trials. <bold>C</bold> Similar performances are obtained with a simple, approximative implementation of the optimal strategy with synaptic strengths <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e050" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e051" xlink:type="simple"/></inline-formula> connecting an odor detecting neuron (o) to action neurons “approach” (ap) and “avoid” (av). In the absence of any stimulus (odor) the synaptic strengths decay with different time constants for the approximative provident policy and with the same time constants for the approximative greedy policy. When an odor is present, the synaptic strengths change in a Hebbian way in the case of reward and in an anti-Hebbian way in the case of punishment, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e052" xlink:type="simple"/></inline-formula>/<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e053" xlink:type="simple"/></inline-formula> increase/decrease for reward and decrease/increase for punishment.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003640.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s2f">
<title>A simple mechanistic implementation results in close to optimal behavior</title>
<p>The probabilistic calculations needed to derive the optimal provident behavior can be quite involved. We do not suggest that there is a neuronal circuitry in <italic>Drosophila</italic> which actually does these calculations. Yet it is interesting to note that a much simpler mechanistic decision making model already results in close to optimal behavior (<xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4B</xref>). This simple model allows an interpretation of the variables as synaptic strengths from odor sensitive neurons to decision neurons (<xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4C</xref>). In the absence of odor and behavioral feedback the synaptic strengths decay with different time scales towards a stationary level: decay is faster for synapses targeting the “avoid” neurons than for the “approach” neurons. One could speculate that the speed of this decay is governed by e.g. the concentration of Rac <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref> or dopamine <xref ref-type="bibr" rid="pcbi.1003640-Berry1">[2]</xref>.</p>
</sec><sec id="s2g">
<title><italic>Drosophila</italic> adapts to changing environmental time scales</title>
<p>So far we have assumed that the transition rates between the environmental states are fixed. This is not an assumption <italic>Drosophila</italic> seems to make and in fact, would be an unrealistic model of the environment. The experiments by Tully et al. <xref ref-type="bibr" rid="pcbi.1003640-Tully2">[9]</xref> show that forgetting depends not only on the number of conditioning trials but also on their frequency. In particular, forgetting is slower when the same number of learning trials is spaced out over a longer period of time. Spaced training is more informative about the environment being in a slowly changing mode than the temporally compressed massed training. Furthermore, reversal training during which in fast succession an odor is aversively, neutral and again aversively conditioned <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref> results in faster forgetting and is informative about a fast changing environment. So the observed behavior provides rather direct evidence that adaptation in <italic>Drosophila</italic> does indeed take non-stationarity into account.</p>
</sec><sec id="s2h">
<title>Extended model with slow and fast transitions matches the observed behavior for different conditioning protocols</title>
<p>To include adaptation as a response to changing transition rates, we extended our model by a slowly varying meta variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e054" xlink:type="simple"/></inline-formula> which can either be in state “fast change” or “slow change” (<xref ref-type="fig" rid="pcbi-1003640-g005">Fig. 5A</xref>). The dynamics of the meta variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e055" xlink:type="simple"/></inline-formula> is governed by a Markov process with small transition rates. In state “fast change”, the environmental reward state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e056" xlink:type="simple"/></inline-formula> changes more rapidly than in state “slow change”. In this setting, an optimal agent maintains a belief about both the environmental reward state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e057" xlink:type="simple"/></inline-formula> and the “hidden” state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e058" xlink:type="simple"/></inline-formula> that sets the time scale of the changes in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e059" xlink:type="simple"/></inline-formula>. Spaced training increases the belief that the environment is in a slowly changing mode, whereas reversal learning leads to a strong belief about the environment being in the fast changing mode. The resulting greedy-optimal behavior is in qualitative agreement with the known behavior after spaced, massed and reversal learning (<xref ref-type="fig" rid="pcbi-1003640-g005">Fig. 5B</xref>) as observed for flies <xref ref-type="bibr" rid="pcbi.1003640-Shuai1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Tully2">[9]</xref>, honey bees <xref ref-type="bibr" rid="pcbi.1003640-Menzel1">[15]</xref>, pigeons <xref ref-type="bibr" rid="pcbi.1003640-Gonzalez1">[13]</xref>, and humans <xref ref-type="bibr" rid="pcbi.1003640-Hagman1">[16]</xref>.</p>
<fig id="pcbi-1003640-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003640.g005</object-id><label>Figure 5</label><caption>
<title>Behavior of agents that estimate the time scale of non-stationarity.</title>
<p><bold>A</bold> In an extended model the rate of change depends on a slowly changing meta variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e060" xlink:type="simple"/></inline-formula>, which can be in a slow or fast state. <bold>B</bold> As observed in experiments with <italic>Drosophila</italic>, our model agents show slowest forgetting after spaced training and fastest forgetting of the last association after reversal training. In our model, this result appears as a consequence of spaced training being most informative about slow transitions, whereas reversal training is most informative about fast transitions.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003640.g005" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We demonstrated that forgetting appears when an agent, subject to costs of responding, acts optimally in an environment with non-stationary stimulus-reinforcement associations. Based on reward maximization in a non-stationary environment, which is a reasonable objective not only for the fruit fly but for other species as well, our normative theory of forgetting includes an asymmetry in forgetting speed after aversive and appetitive conditioning and an adaptation of forgetting speed after spaced, massed and reversal learning. The asymmetry is the result of an economically optimal provident policy, which forages not only for immediate reward but also for information required for future exploitation. The adaptation of forgetting rate after spaced, massed and reversal learning is a consequence of the agents estimation of the current rate of environmental changes.</p>
<p>That costs of responding influence the action selection is an assumption which is in agreement with test-retest experiments <xref ref-type="bibr" rid="pcbi.1003640-Tully2">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-CervantesSandoval1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Beck1">[17]</xref>. In these classical conditioning experiment the flies are grouped according to whether they choose the conditioned response or not. Both groups are immediately retested to examine whether the flies stick to their decision. The outcome is: they do not. An equal fraction of flies chooses the conditioned response in both retest groups and this fraction is the same as in the first test containing all flies. This suggests that all flies maintain traces of the conditioning but that also other factors influence the choice in a stochastic way. Similarly, in our model the belief is a sufficient statistic of the past experiences that involve the conditioned stimulus and the stochastic costs of responding account for other factors that influence the choice.</p>
<p>A key assumption in our normative explanation of the differential forgetting in <italic>Drosophila</italic> is that the relationship between conditioned stimulus and reinforcement is non-stationary. Now, if this relationship were completely stationary, it would not need to be learned by the phenotype because it would already have been learned by the genotype, i.e. in this case the stimulus would be an unconditioned stimulus. Hence, from an evolutionary perspective, our assumption is close to being a truism. Nevertheless, many biological models of reinforcement learning have, for the sake of simplicity, assumed a stationary stimulus-reinforcement relationship <xref ref-type="bibr" rid="pcbi.1003640-Sutton1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Doya1">[19]</xref>.</p>
<p>Experiments and models with non-stationary stimulus-reinforcement associations have suggested, similar to our findings, that in a more volatile environment the learning should be faster <xref ref-type="bibr" rid="pcbi.1003640-Courville1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003640-Gallistel1">[24]</xref>. However, faster learning does not unconditionally imply faster forgetting. The asymmetry in forgetting speed after appetitive and aversive conditioning additionally requires an evaluation of the behavioral relevance of a specific memory content. Since the aversive reaction is not informative about the current state of association, aversive conditioning should be forgotten faster than appetitive conditioning.</p>
<p>Finding the optimal policy in an environment with a non-stationary stimulus-reinforcement relationship, as considered here, is computationally involving. As we have shown, however, approximately optimal decision making is still possible with a simplified neuronal model using experience induced synaptic updates. This model incorporates forgetting in the decay time constant of the synaptic strengths. As the parameters describing the changing environment are assumed to be constant across generations, the neuronal architecture and the forgetting rates can be considered to be genetically encoded.</p>
<p>Since the work of Ebbinghaus <xref ref-type="bibr" rid="pcbi.1003640-Ebbinghaus1">[25]</xref> on the forgetting rate of non-sense syllables and the observation of Jenkins and Dallenbach <xref ref-type="bibr" rid="pcbi.1003640-Jenkins1">[26]</xref> that sleep between learning and recalling reduces forgetting, cognitive psychologists debate about the role of natural decay and interference in explaining forgetting <xref ref-type="bibr" rid="pcbi.1003640-Wixted1">[5]</xref>. While interference based explanations are favored by many <xref ref-type="bibr" rid="pcbi.1003640-Wixted1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Wixted2">[12]</xref>, Hardt et al. <xref ref-type="bibr" rid="pcbi.1003640-Hardt1">[6]</xref> recently advocated active processes behind decay-driven forgetting. They suggested a memory system that engages in promiscuous encoding and uses a flexible mechanism to remove irrelevant information later, mostly during sleep phases. In their view, different forgetting rates are a sign of such a flexible removal mechanism. But why do biological organisms need to actively remove irrelevant memories at all? Popular answers so far implicitly assumed that forgetting is ultimately the result of some limitation of the memory system, for instance, limited storage capacity, a limit on the acceptable read-out time for the memory or a decay of the biological substrate similar to unused muscles atrophy <xref ref-type="bibr" rid="pcbi.1003640-Hardt1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Thorndike1">[27]</xref>. In our model, however, forgetting does not result from a memory limitation, but emerges as an adaptive feature of the memory system to optimally cope with a changing environment while accounting for the relevance of different memory contents.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Basic model of the environment</title>
<p>In time bin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e061" xlink:type="simple"/></inline-formula> an odor can be associated with one of three environmental states: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e062" xlink:type="simple"/></inline-formula> (reward), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e063" xlink:type="simple"/></inline-formula> (neutral), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e064" xlink:type="simple"/></inline-formula> (punishment). The time-discrete dynamics of the environmental state is given by a Markov Chain with state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e065" xlink:type="simple"/></inline-formula> and transition probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e067" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e068" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e069" xlink:type="simple"/></inline-formula>. For simplicity we did not include direct transitions between the rewarding and punishing state, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e070" xlink:type="simple"/></inline-formula>. Including them would also allow for a behavior where the preference switches from the conditioned response to the opposite of the conditioned response before reaching the stationary state. The stationary distribution of this Markov chain, satisfying the self-consistency equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e071" xlink:type="simple"/></inline-formula>, is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e073" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e074" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b">
<title>External reinforcement signal</title>
<p>In each time bin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e075" xlink:type="simple"/></inline-formula> the agent has two behavioral options: approach the odor (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e076" xlink:type="simple"/></inline-formula>) or avoid the odor (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e077" xlink:type="simple"/></inline-formula>). If the agent avoids, a neutral reinforcement signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e078" xlink:type="simple"/></inline-formula> is always returned. If the agent approaches, the external reinforcement signal depends on the environmental state: there will always be a positive signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e079" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e080" xlink:type="simple"/></inline-formula>, always a negative signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e081" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e082" xlink:type="simple"/></inline-formula> and if the odor is associated with the neutral state (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e083" xlink:type="simple"/></inline-formula>), the agent will stochastically get a neutral signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e084" xlink:type="simple"/></inline-formula> with probability 0.99, while with probability 0.005 the agent will get a positive or a negative reinforcement signal. Positive and negative reinforcement signals during the neutral state are included to model situations, where reward or punishment depends on odor unrelated factors. For further use we summarize the information in this paragraph in the probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e085" xlink:type="simple"/></inline-formula>, with non-zero entries <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e086" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e087" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e088" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e089" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e090" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e091" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c">
<title>Belief</title>
<p>The agent maintains a belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e092" xlink:type="simple"/></inline-formula> over the current environmental state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e093" xlink:type="simple"/></inline-formula> given past reinforcement <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e094" xlink:type="simple"/></inline-formula> and actions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e095" xlink:type="simple"/></inline-formula>. The belief state is updated by Bayesian filtering<disp-formula id="pcbi.1003640.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e096" xlink:type="simple"/><label>(1)</label></disp-formula>with normalization <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e097" xlink:type="simple"/></inline-formula>. We use the abbreviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e098" xlink:type="simple"/></inline-formula> to denote the update of the belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e099" xlink:type="simple"/></inline-formula> given action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e100" xlink:type="simple"/></inline-formula> and reinforcement signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e101" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4d">
<title>Costs of responding</title>
<p>We modeled costs of responding with exponentially distributed and uncorrelated random variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e102" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e103" xlink:type="simple"/></inline-formula> with parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e104" xlink:type="simple"/></inline-formula>, i.e. the probability density function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e105" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e106" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e107" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e108" xlink:type="simple"/></inline-formula> otherwise. This distribution has mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e109" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e110" xlink:type="simple"/></inline-formula>. We assumed, that the agent receives an effective reward, which is a sum of the external reinforcement signal and the momentary cost of responding for the action chosen. During decision making, the agent knows the costs of responding for both actions but only has an expectation of the external reinforcement signal.</p>
</sec><sec id="s4e">
<title>Greedy policy: Maximization of immediate reward</title>
<p>If the goal is to maximize immediate reward, the agent's policy depends on the expected return in the next step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e111" xlink:type="simple"/></inline-formula>, which for action ap can be simplified to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e112" xlink:type="simple"/></inline-formula> and for action av is always zero, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e113" xlink:type="simple"/></inline-formula>. Including costs of responding, the policy that maximizes immediate reward selects the action for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e114" xlink:type="simple"/></inline-formula> is maximal.</p>
</sec><sec id="s4f">
<title>Provident policy: Maximization of reward rate</title>
<p>A canonical choice of the objective to be maximized by a provident policy is the reward rate, i.e.<disp-formula id="pcbi.1003640.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e115" xlink:type="simple"/></disp-formula>with expected reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e116" xlink:type="simple"/></inline-formula> in time bin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e117" xlink:type="simple"/></inline-formula> when acting according to policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e118" xlink:type="simple"/></inline-formula>. We approximately determined the policy which maximizes the reward rate by two methods: dynamic programming and linear programming on a quantized space.</p>
<p>Dynamic Programming allows to find a policy that maximizes the future discounted values<disp-formula id="pcbi.1003640.e119"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e119" xlink:type="simple"/></disp-formula>with discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e120" xlink:type="simple"/></inline-formula> and expected reward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e121" xlink:type="simple"/></inline-formula> in time bin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e122" xlink:type="simple"/></inline-formula> after starting in belief state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e123" xlink:type="simple"/></inline-formula> and acting according to policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e124" xlink:type="simple"/></inline-formula>. For finite state spaces and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e125" xlink:type="simple"/></inline-formula> sufficiently close to 1 a policy that maximizes future discounted reward also maximizes the reward rate <xref ref-type="bibr" rid="pcbi.1003640-Flynn1">[28]</xref>. Without costs of responding one could directly apply the Incremental Pruning algorithm <xref ref-type="bibr" rid="pcbi.1003640-Cassandra1">[29]</xref> to find a policy that maximizes the future discounted values. Here we derive dynamical programming in the presence of costs of responding.</p>
<p>Dynamic programming proceeds by iteratively constructing optimal finite-horizon values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e126" xlink:type="simple"/></inline-formula> for some operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e127" xlink:type="simple"/></inline-formula>. Assume that we have the horizon-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e128" xlink:type="simple"/></inline-formula> policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e129" xlink:type="simple"/></inline-formula> that maximizes the future discounted values of an episode of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e130" xlink:type="simple"/></inline-formula>. The horizon-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e131" xlink:type="simple"/></inline-formula> policy consists of instructions for each step in the episode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e132" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e133" xlink:type="simple"/></inline-formula> tells which action to take at the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e134" xlink:type="simple"/></inline-formula>-th step before the end of the episode, given belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e135" xlink:type="simple"/></inline-formula> and costs of responding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e136" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e137" xlink:type="simple"/></inline-formula>. To construct the horizon-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e138" xlink:type="simple"/></inline-formula> policy we need to extend the horizon-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e139" xlink:type="simple"/></inline-formula> policy by the instruction for the first step, i.e. the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e140" xlink:type="simple"/></inline-formula>-th step before the end of the episode. Without considering the costs of responding in the first step, the expected future discounted values for choosing action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e141" xlink:type="simple"/></inline-formula> are given by<disp-formula id="pcbi.1003640.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e142" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e143" xlink:type="simple"/></inline-formula> and the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e144" xlink:type="simple"/></inline-formula> is given by (we will use the indicator function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e145" xlink:type="simple"/></inline-formula>, given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e146" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e147" xlink:type="simple"/></inline-formula> is true and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e148" xlink:type="simple"/></inline-formula> otherwise):<disp-formula id="pcbi.1003640.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e149" xlink:type="simple"/></disp-formula></p>
<p>With the change of variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e150" xlink:type="simple"/></inline-formula>, the resulting probability density function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e151" xlink:type="simple"/></inline-formula> (Laplace probability density), and the abbreviations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e152" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e153" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e154" xlink:type="simple"/></inline-formula>, we get<disp-formula id="pcbi.1003640.e155"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e155" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>In the same manner we find the value function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e156" xlink:type="simple"/></inline-formula>, which depends through <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e157" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e158" xlink:type="simple"/></inline-formula> (see <xref ref-type="disp-formula" rid="pcbi.1003640.e142">Eq. 2</xref>)<disp-formula id="pcbi.1003640.e159"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e159" xlink:type="simple"/><label>(4)</label></disp-formula>where now <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e160" xlink:type="simple"/></inline-formula>.</p>
<p>Due to the discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e161" xlink:type="simple"/></inline-formula> this recursion will eventually converge. In practice we will stop after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e162" xlink:type="simple"/></inline-formula> iterations and define the policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e163" xlink:type="simple"/></inline-formula>, which approximates the future discounted policy. Note that in contrast to the finite horizon policies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e164" xlink:type="simple"/></inline-formula> the policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e165" xlink:type="simple"/></inline-formula> is stationary: in a sequential setting there is no end of an episode on which the policy may depend.</p>
<p>The number of terms in a naive implementation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e166" xlink:type="simple"/></inline-formula> grows exponentially with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e167" xlink:type="simple"/></inline-formula>. Without costs of responding the exponential growth can sometimes be prohibited by Incremental Pruning <xref ref-type="bibr" rid="pcbi.1003640-Cassandra1">[29]</xref>. With costs of responding we are not aware of a way to prevent exponential growth. In <xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4</xref> we approximated the stationary policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e168" xlink:type="simple"/></inline-formula> by taking the policy after 5 iteration with discount factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e169" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e170" xlink:type="simple"/></inline-formula>. Since it is not clear whether for this choice of discount factor and number of iterations the resulting policy is a good approximation of the reward rate maximizing policy, we compared the result of dynamic programming with the policy obtained by linear programming on a quantized belief space.</p>
<p>For finite state and action space Markov Decision Processes linear programming can be used to find a policy that maximizes the reward rate <xref ref-type="bibr" rid="pcbi.1003640-Bello1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003640-Puterman1">[31]</xref>. In our case, the policies act on the continuous space of belief states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e171" xlink:type="simple"/></inline-formula> and cost of responding differences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e172" xlink:type="simple"/></inline-formula>. Analogous to the finite state space problem, the optimization problem could be formulated as: find functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e173" xlink:type="simple"/></inline-formula> that implicitly define the policy <xref ref-type="bibr" rid="pcbi.1003640-Bello1">[30]</xref> and satisfy<disp-formula id="pcbi.1003640.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e174" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003640.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e175" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003640.e176"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e176" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e177" xlink:type="simple"/></inline-formula> denotes the expected reward for action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e178" xlink:type="simple"/></inline-formula>, belief state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e179" xlink:type="simple"/></inline-formula> and costs of responding differences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e180" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e181" xlink:type="simple"/></inline-formula> denotes the probability density to transition from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e182" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e183" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e184" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e185" xlink:type="simple"/></inline-formula> given action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e186" xlink:type="simple"/></inline-formula>. A straightforward approach is to quantize the belief space and space of cost of responding differences, replace the integrals by sums and find through linear programming an approximation to the reward rate maximizing policy. We quantized the two dimensional belief simplex <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e187" xlink:type="simple"/></inline-formula> on a square lattice with different lattice spacings. Values that did not fall on lattice points where stochastically assigned to neighboring lattice points. The space of real valued cost of responding differences was quantized by segmenting the real line into adjacent intervals with equal mass of the probability density function. For each interval the average costs of responding for each action where computed. Using increasingly finer quantization we estimated the total reward to be between 600 and 655 for trials of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e188" xlink:type="simple"/></inline-formula> time bins, which is in agreement with the estimate obtained with dynamic programming (<xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4B</xref> provident).</p>
</sec><sec id="s4g">
<title>A simple, approximative implementation</title>
<p>In <xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4</xref> we demonstrate that also an agent with two uncoupled low-pass filters can show near to optimal behavior. The agent's decision to approach (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e189" xlink:type="simple"/></inline-formula>) or avoid (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e190" xlink:type="simple"/></inline-formula>) the odor depends on whether <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e191" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e192" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e193" xlink:type="simple"/></inline-formula>) are variables interpretable as synaptic strengths and where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e194" xlink:type="simple"/></inline-formula> represents stochastic input due to costs of responding. The values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e195" xlink:type="simple"/></inline-formula> decay with different time-constants, in the case of no feedback, because the agent either stays away or no odor is present. If the agent approaches the odor and experiences reward, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e196" xlink:type="simple"/></inline-formula> is set to a maximal value, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e197" xlink:type="simple"/></inline-formula> is set to zero; for odor plus punishment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e198" xlink:type="simple"/></inline-formula> is set to a maximal value, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e199" xlink:type="simple"/></inline-formula> is set to zero. Formally, with the subscript <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e200" xlink:type="simple"/></inline-formula> standing for either ap or av, we get<disp-formula id="pcbi.1003640.e201"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003640.e201" xlink:type="simple"/><label>(5)</label></disp-formula>The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e202" xlink:type="simple"/></inline-formula> controls the speed of forgetting, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e203" xlink:type="simple"/></inline-formula> sets a baseline value and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e204" xlink:type="simple"/></inline-formula> sets a maximum value. In <xref ref-type="fig" rid="pcbi-1003640-g004">Fig. 4</xref> the parameter values where fit to the curves in sub-figure A (approx provident: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e205" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e206" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e207" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e208" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e209" xlink:type="simple"/></inline-formula>) and to the curves in sub-figure B (approx greedy: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e210" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e211" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e212" xlink:type="simple"/></inline-formula>).</p>
</sec><sec id="s4h">
<title>Extended model of the environment</title>
<p>To study the behavior of an agent that additionally has to estimate the rate of change we extended the basic model of the environment with a meta variable that controls the rate of change of the environmental state. In time bin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e213" xlink:type="simple"/></inline-formula> the meta variable can be in one of two states: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e214" xlink:type="simple"/></inline-formula> (fast) or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e215" xlink:type="simple"/></inline-formula> (slow). The dynamics of the meta variable is described by a Markov Chain with transition probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e216" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e217" xlink:type="simple"/></inline-formula>. If the meta variable is in the slow (fast) state the transition parameters of the environmental state are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e218" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e219" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e220" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e221" xlink:type="simple"/></inline-formula>. In the extended model the state space is given by the product space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e222" xlink:type="simple"/></inline-formula> and the transition parameters are given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003640.e223" xlink:type="simple"/></inline-formula>. The agent maintains a belief about both the environmental state and the state of transition speed.</p>
</sec><sec id="s4i">
<title>Spaced, massed and reversal learning</title>
<p>In spaced training, the agent was aversively conditioned six times with intermittent waiting periods of 9 time bins. In massed training, the agent was aversively conditioned in 6 subsequent time bins. In reversal learning, the agent was exposed to the punishing, neutral and punishing environmental state in subsequent time bins. Forgetting curves are shown for the computationally less involving greedy policy. In order to compare massed with spaced training we choose a finer time discretization in the extended model, i.e. 10 time bins in the extended model correspond to 1 time bin in the basic model. In <xref ref-type="fig" rid="pcbi-1003640-g005">figure 5B</xref> the result is plotted in units of the basic model.</p>
</sec></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003640-Tully1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tully</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Quinn</surname><given-names>W</given-names></name> (<year>1985</year>) <article-title>Classical conditioning and retention in normal and mutant Drosophila melanogaster</article-title>. <source>Journal of Comparative Physiology A</source> <volume>157</volume>: <fpage>263</fpage>–<lpage>277</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Berry1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berry</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Cervantes-Sandoval</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Nicholas</surname><given-names>EP</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>RL</given-names></name> (<year>2012</year>) <article-title>Dopamine is required for learning and forgetting in Drosophila</article-title>. <source>Neuron</source> <volume>74</volume>: <fpage>530</fpage>–<lpage>542</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Rosenzweig1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenzweig</surname><given-names>ES</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>Ca</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name> (<year>2002</year>) <article-title>Making room for new memories</article-title>. <source>Nature neuroscience</source> <volume>5</volume>: <fpage>6</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Storm1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Storm</surname><given-names>BC</given-names></name> (<year>2011</year>) <article-title>The benefit of forgetting in thinking and remembering</article-title>. <source>Current Directions in Psychological Science</source> <volume>20</volume>: <fpage>291</fpage>–<lpage>295</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Wixted1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name> (<year>2004</year>) <article-title>The Psychology and Neuroscience of Forgetting</article-title>. <source>Annual review of psychology</source> <volume>55</volume>: <fpage>235</fpage>–<lpage>69</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Hardt1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hardt</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Nader</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Nadel</surname><given-names>L</given-names></name> (<year>2013</year>) <article-title>Decay happens: the role of active forgetting in memory</article-title>. <source>Trends in cognitive sciences</source> <volume>17</volume>: <fpage>109</fpage>–<lpage>118</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Kraemer1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kraemer</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Golding</surname><given-names>J</given-names></name> (<year>1997</year>) <article-title>Adaptive forgetting in animals</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>4</volume>: <fpage>480</fpage>–<lpage>491</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Shuai1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shuai</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Forgetting is Regulated through Rac Activity in Drosophila</article-title>. <source>Cell</source> <volume>140</volume>: <fpage>579</fpage>–<lpage>89</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Tully2"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tully</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Preat</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Boynton</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vecchio</surname><given-names>MD</given-names></name> (<year>1994</year>) <article-title>Genetic dissection of consolidated memory in Drosophila</article-title>. <source>Cell</source> <volume>79</volume>: <fpage>35</fpage>–<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Tempel1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tempel</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Bonini</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Dawson</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Quinn</surname><given-names>WG</given-names></name> (<year>1983</year>) <article-title>Reward learning in normal and mutant Drosophila</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>80</volume>: <fpage>1482</fpage>–<lpage>1486</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-CervantesSandoval1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cervantes-Sandoval</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>RL</given-names></name> (<year>2012</year>) <article-title>Distinct traces for appetitive versus aversive olfactory memories in DPM neurons of Drosophila</article-title>. <source>Current Biology</source> <volume>22</volume>: <fpage>1247</fpage>–<lpage>1252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Wixted2"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name> (<year>2005</year>) <article-title>A Theory About Why We Forget What We Once Knew</article-title>. <source>Current Directions in Psychological Science</source> <volume>14</volume>: <fpage>6</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Gonzalez1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gonzalez</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Behrend</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bitterman</surname><given-names>M</given-names></name> (<year>1967</year>) <article-title>Reversal learning and forgetting in bird and fish</article-title>. <source>Science</source> <volume>158</volume>: <fpage>519</fpage>–<lpage>521</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Buchanan1"><label>14</label>
<mixed-citation publication-type="book" xlink:type="simple">Buchanan JM (2008) opportunity cost. In: Durlauf SN, Blume LE, editors, The New Palgrave Dictionary of Economics, Palgrave Macmillan. Second edition.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Menzel1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Menzel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Manz</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Menzel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Greggers</surname><given-names>U</given-names></name> (<year>2001</year>) <article-title>Massed and Spaced Learning in Honeybees: The Role of CS, US, the Intertrial Interval, and the Test Interval</article-title>. <source>Learning &amp; Memory</source> <volume>8</volume>: <fpage>198</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Hagman1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Hagman J (1980) Effects of training schedule and equipment variety on retention and transfer of maintenance skill. US Army Research Institute for the Behavioral and Social Sciences, Alexandria, VA.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Beck1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Schroeder</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>RL</given-names></name> (<year>2000</year>) <article-title>Learning performance of normal and mutant Drosophila after repeated conditioning trials with discrete stimuli</article-title>. <source>The Journal of Neuroscience</source> <volume>20</volume>: <fpage>2944</fpage>–<lpage>2953</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Sutton1"><label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">Sutton R, Barto A (1998) Reinforcement learning: An introduction. Cambridge, MIT Press, 1 edition.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Doya1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Reinforcement learning: Computational theory and biological mechanisms</article-title>. <source>HFSP Journal</source> <volume>1</volume>: <fpage>30</fpage>–<lpage>40</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Courville1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Courville</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name> (<year>2006</year>) <article-title>Bayesian theories of conditioning in a changing world</article-title>. <source>Trends in cognitive sciences</source> <volume>10</volume>: <fpage>294</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Behrens1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name>, <name name-style="western"><surname>Woolrich</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Walton</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Rushworth</surname><given-names>MFS</given-names></name> (<year>2007</year>) <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nature neuroscience</source> <volume>10</volume>: <fpage>1214</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Nassar1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Heasly</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2010</year>) <article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>12366</fpage>–<lpage>78</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-PayzanLeNestour1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Payzan-LeNestour</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bossaerts</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Risk, unexpected uncertainty, and estimation uncertainty: Bayesian learning in unstable settings</article-title>. <source>PLoS computational biology</source> <volume>7</volume>: <fpage>e1001048</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Gallistel1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Mark</surname><given-names>Ta</given-names></name>, <name name-style="western"><surname>King</surname><given-names>aP</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2001</year>) <article-title>The rat approximates an ideal detector of changes in rates of reward: implications for the law of effect</article-title>. <source>Journal of experimental psychology Animal behavior processes</source> <volume>27</volume>: <fpage>354</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Ebbinghaus1"><label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">Ebbinghaus H (1885) Über das Gedächtnis. Leibzig: Duncker &amp; Humber.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Jenkins1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jenkins</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dallenbach</surname><given-names>K</given-names></name> (<year>1924</year>) <article-title>Obliviscence during Sleep and Waking</article-title>. <source>The American Journal of Psychology</source> <volume>35</volume>: <fpage>605</fpage>–<lpage>612</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Thorndike1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Thorndike EL (1923) Educational Psychology Volume II, The Psychology of Learning. Teachers College Columbia University.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Flynn1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flynn</surname><given-names>J</given-names></name> (<year>1974</year>) <article-title>Averaging vs. discounting in dynamic programming: a counterexample</article-title>. <source>The Annals of Statistics</source> <volume>2</volume>: <fpage>411</fpage>–<lpage>413</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Cassandra1"><label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">Cassandra A, Littman M, Zhang N (1997) Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. In: Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., pp. 54–61.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Bello1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bello</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Riano</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Linear Programming solvers for Markov Decision Processes</article-title>. <source>Systems and Information Engineering Design Symposium, 2006 IEEE</source> <fpage>90</fpage>–<lpage>95</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003640-Puterman1"><label>31</label>
<mixed-citation publication-type="book" xlink:type="simple">Puterman M (2005) Markov decision processes: discrete stochastic dynamic programming. John Wiley &amp; Sons.</mixed-citation>
</ref>
</ref-list></back>
</article>