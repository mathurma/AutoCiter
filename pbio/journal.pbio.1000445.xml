<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id><journal-id journal-id-type="pmc">plosbiol</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Biology</journal-title></journal-title-group><issn pub-type="ppub">1544-9173</issn><issn pub-type="epub">1545-7885</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLBI-RA-6831R2</article-id><article-id pub-id-type="doi">10.1371/journal.pbio.1000445</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject></subj-group></article-categories><title-group><article-title>Auditory Cortex Tracks Both Auditory and Visual Stimulus Dynamics Using Low-Frequency Neuronal Phase Modulation</article-title><alt-title alt-title-type="running-head">Representation of Multi-Sensory Speech</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Luo</surname><given-names>Huan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Liu</surname><given-names>Zuxiang</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences, Beijing, China</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Psychology, New York University, New York, New York, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Zatorre</surname><given-names>Robert</given-names></name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">McGill University, Canada</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">luohuan@gmail.com</email> (HL); <email xlink:type="simple">david.poeppel@nyu.edu</email> (DP)</corresp>
<fn fn-type="con"><p>The author(s) have made the following declarations about their contributions: Conceived and designed the experiments: HL DP. Performed the experiments: HL DP. Analyzed the data: HL ZL. Contributed reagents/materials/analysis tools: HL ZL DP. Wrote the paper: HL ZL DP.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>8</month><year>2010</year></pub-date><pub-date pub-type="epub"><day>10</day><month>8</month><year>2010</year></pub-date><volume>8</volume><issue>8</issue><elocation-id>e1000445</elocation-id><history>
<date date-type="received"><day>2</day><month>4</month><year>2010</year></date>
<date date-type="accepted"><day>24</day><month>6</month><year>2010</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2010</copyright-year><copyright-holder>Luo et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract abstract-type="toc">
<p>How is naturalistic multisensory information combined in the human brain? Based on MEG data we show that phase modulation of visual and auditory signals captures the dynamics of complex scenes.</p>
</abstract><abstract>
<p>Integrating information across sensory domains to construct a unified representation of multi-sensory signals is a fundamental characteristic of perception in ecological contexts. One provocative hypothesis deriving from neurophysiology suggests that there exists early and direct cross-modal phase modulation. We provide evidence, based on magnetoencephalography (MEG) recordings from participants viewing audiovisual movies, that low-frequency neuronal information lies at the basis of the synergistic coordination of information across auditory and visual streams. In particular, the phase of the 2–7 Hz delta and theta band responses carries robust (in single trials) and usable information (for parsing the temporal structure) about stimulus dynamics in both sensory modalities concurrently. These experiments are the first to show in humans that a particular cortical mechanism, delta-theta phase modulation across early sensory areas, plays an important “<italic>active</italic>” role in continuously tracking naturalistic audio-visual streams, carrying dynamic multi-sensory information, and reflecting cross-sensory interaction in real time.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>When faced with ecologically relevant stimuli in natural scenes, our brains need to coordinate information from multiple sensory systems in order to create accurate internal representations of the outside world. Unfortunately, we currently have little information about the neuronal mechanisms for this cross-modal processing during online sensory perception under natural conditions. Neurophysiological and human imaging studies are increasingly exploring the response properties elicited by natural scenes. In this study, we recorded magnetoencephalography (MEG) data from participants viewing audiovisual movie clips. We developed a phase coherence analysis technique that captures—in single trials of watching a movie—how the phase of cortical responses is tightly coupled to key aspects of stimulus dynamics. Remarkably, auditory cortex not only tracks auditory stimulus dynamics but also reflects dynamic aspects of the visual signal. Similarly, visual cortex mainly follows the visual properties of a stimulus, but also shows sensitivity to the auditory aspects of a scene. The critical finding is that cross-modal phase modulation appears to lie at the basis of this integrative processing. Continuous cross-modal phase modulation may permit the internal construction of behaviorally relevant stimuli. Our work therefore contributes to the understanding of how multi-sensory information is analyzed and represented in the human brain.</p>
</abstract><funding-group><funding-statement>This work was supported by National Institutes of Health 2R01DC05660 to DP as well as a 973 grant from the National Strategic Basic Research program of the Ministry of Science and Technology of China (2005CB522800), National Nature Science Foundation of China grants (30621004, 90820307), and Chinese Academy of Sciences grants (KSCX2-YW-R-122, KSCX2-YW-R-259). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>We do not experience the world as parallel sensory streams; rather, the information extracted from different modalities fuses to form a seamlessly unified multi-sensory percept dynamically evolving over time. There is a compelling benefit to multimodal information: behavioral studies show that combining information across sensory domains enhances unimodal detection ability—and can even induce new, integrated percepts <xref ref-type="bibr" rid="pbio.1000445-McGurk1">[1]</xref>–<xref ref-type="bibr" rid="pbio.1000445-vanWassenhove1">[4]</xref>. The relevant neuronal mechanisms have been widely investigated. One typical view posits that multisensory integration occurs at later stages of cortical processing, subsequent to unisensory analysis. This view has been supported by studies showing that higher, “association” areas in temporal, parietal, and frontal cortices receive inputs from multiple unimodal areas <xref ref-type="bibr" rid="pbio.1000445-Jones1">[5]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Macaluso1">[8]</xref> and respond to stimulation in manner that reflects multisensory convergence, for example with amplified or suppressed responses for multimodal over unimodal stimuli <xref ref-type="bibr" rid="pbio.1000445-Calvert1">[9]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Stein1">[12]</xref>.</p>
<p>A growing body of evidence provides a complementary view, suggesting that cross-modal interaction is not restricted to association areas and can occur at early, putatively unisensory cortical processing stages <xref ref-type="bibr" rid="pbio.1000445-Ghazanfar1">[11]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Schroeder1">[13]</xref>. For example, non-auditory stimulation (visual and somatosensory) has been found to drive auditory cortical activity, as observed in both humans and animals <xref ref-type="bibr" rid="pbio.1000445-vanWassenhove1">[4]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Calvert2">[14]</xref>–<xref ref-type="bibr" rid="pbio.1000445-FuhrmannAlpert1">[23]</xref>. Similarly, visual cortical responses are modulated by inputs from other modalities <xref ref-type="bibr" rid="pbio.1000445-Morrell1">[24]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Macaluso2">[25]</xref>. Importantly, independent anatomical evidence also reveals direct connections among early sensory areas <xref ref-type="bibr" rid="pbio.1000445-Falchier1">[26]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Rockland1">[27]</xref>. Therefore, multisensory integration may operate through lateral cross-sensory modulation, and there exist multiple integration pathways beyond purely hierarchical convergence <xref ref-type="bibr" rid="pbio.1000445-Stein1">[12]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Senkowski1">[28]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Arnal1">[29]</xref>.</p>
<p>How is early cortical activity coordinated? Beyond the classical examination of cross-modal influences on neuronal firing rate, recent studies suggest temporal coherence <xref ref-type="bibr" rid="pbio.1000445-Singer1">[30]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Engel1">[31]</xref> to underlie multisensory integration <xref ref-type="bibr" rid="pbio.1000445-Senkowski1">[28]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Maier1">[32]</xref>. This view posits that oscillations synchronous across different brain areas might serve an essential role in multisensory binding, similarly as that for feature binding and attentional selection <xref ref-type="bibr" rid="pbio.1000445-Singer1">[30]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Desimone1">[33]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Jensen1">[36]</xref>. Several EEG/MEG studies in humans implicate oscillations and cross-area coherence in multisensory integration <xref ref-type="bibr" rid="pbio.1000445-Arnal1">[29]</xref>,<xref ref-type="bibr" rid="pbio.1000445-vonStein1">[37]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Mishra1">[42]</xref>. However, most of the studies employed short, transient multisensory stimuli and focused on the evoked transient oscillatory power instead of examining sustained cross-modal modulation for long, naturalistic audiovisual streams.</p>
<p>Importantly, with regard to the cross-area modulation mechanism, it has recently been suggested that <italic>cross-sensory phase modulation</italic> may underlie this interaction <xref ref-type="bibr" rid="pbio.1000445-Senkowski1">[28]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Maier1">[32]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Driver2">[43]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Schroeder2">[44]</xref>. For example, non-auditory inputs (re)set the phase of ongoing local neuronal activity in auditory cortex to a high-excitability state (reflected in phase angle), effectively “selecting” or amplifying the response to subsequent auditory inputs <xref ref-type="bibr" rid="pbio.1000445-Ghazanfar1">[11]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Schroeder1">[13]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Lakatos1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser3">[45]</xref>. Whether such a mechanism is implemented in populations of neurons and could mediate the perception of audiovisual speech in human viewers/listeners is completely unknown.</p>
<p>In order to test directly the proposal of cross-modal phase modulation of oscillatory neural activity, we investigate online audiovisual interaction, in auditory and visual cortices simultaneously, by recording magnetoencephalography (MEG) responses from human participants presented with 30-s-long natural movie clips from the movie “Dumb and Dumber” (1994, New Line Platinum Series). These video segments had either “matched” (congruent audio-visual combinations, V1A1, V2A2, V3A3) or “mixed” streams (incongruent audio-visual, V1A3, V2A1, V3A2). Building on our previous results showing that the theta-band phase pattern in human <italic>auditory</italic> cortex reflects the dynamic structure of spoken sentences <xref ref-type="bibr" rid="pbio.1000445-Luo1">[46]</xref>, we employed a new trial-by-trial phase tracking analysis to explore multi-sensory integration. We conjectured that, in response to naturalistic audio-visual streams (movies), the low-frequency phase of auditory and visual sensory activity <italic>in single trials</italic> (i) will robustly track and discriminate (in a classification analysis) the sensory stream dynamics in each modality (“within-modality tracking”; i.e. auditory channel tracks auditory, visual tracks visual dynamics), (ii) may carry information about stimulus dynamics in the other modality (“cross-modality tracking”; e.g. an auditory channel can reflect visual dynamics), and (iii) that the efficacy of such cross-sensory phase modulation (trial-to-trial phase variance) depends on the relative audiovisual timing, such that a temporally matched audio-visual stream will enhance phase tracking reliability, compared to unmatched (mixed) pairs. Our data support these predictions, highlighting the critical role of cross-sensory phase modulation of oscillations in multisensory integration, commensurate with the hypothesis <xref ref-type="bibr" rid="pbio.1000445-Senkowski1">[28]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Schroeder2">[44]</xref>. We thus argue that multi-sensory integration may use cross-modal phase modulation as a basic mechanism to construct temporally aligned representations that facilitate perceptual decoding of audiovisual speech.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Low-Frequency Phase Patterns in Auditory <italic>and</italic> Visual Areas Carry Reliable Information about Audiovisual Movies</title>
<p>We first assessed whether MEG responses in single trials can reliably track the six movie clips we presented to participants (three Matched, three Mixed). The phase and power pattern of MEG responses to the movies (see illustration of cross-trial phase coherence analysis in <xref ref-type="fig" rid="pbio-1000445-g001">Figure 1a</xref>) and the corresponding discrimination ability were calculated as a function of frequency of the brain response (0–50 Hz) using previously developed methods <xref ref-type="bibr" rid="pbio.1000445-Luo1">[46]</xref>. We quantified stimulus-specific trial-by-trial phase and power pattern coherence in 20 auditory and 20 visual channels, which were defined in separate auditory (1 kHz tone pip) and visual (alternating checkerboard) localizer pretests for each subject (see <xref ref-type="supplementary-material" rid="pbio.1000445.s002">Figure S2</xref>). As illustrated in <xref ref-type="fig" rid="pbio-1000445-g002">Figure 2a</xref>, both auditory and visual cortical responses showed good discrimination ability in the delta-theta-band (2–7 Hz) phase pattern (above zero discrimination score, 2-way ANOVA, main effect of frequency, <italic>F</italic>(24, 840) = 7.94, <italic>p</italic>&lt;0.0001; post-hoc one-sample <italic>t</italic> test in delta-theta band (2∼7 Hz), Auditory: <italic>t</italic> = 11.57, <italic>df</italic> = 35, <italic>p</italic>&lt;0.0001, Visual: <italic>t</italic> = 11.16, <italic>df</italic> = 35, <italic>p</italic>&lt;0.0001). Critically, phase tracking was not accompanied by comparable power pattern tracking (<xref ref-type="fig" rid="pbio-1000445-g002">Figure 2b</xref>, 2-way ANOVA, main effect of frequency, <italic>F</italic>(24, 840) = 0.517, <italic>p</italic> = 0.97; <italic>t</italic> test in delta-theta band (2∼7 Hz), Auditory: <italic>t</italic> = 0.913, <italic>p</italic> = 0.368; Visual: <italic>t</italic> = 0.698, <italic>p</italic> = 0.49). These results demonstrate that the phase of ongoing auditory and visual cortical low-frequency oscillations is reliably modulated by the audio-visual stimuli, and thus conveys information about the rich naturalistic dynamics of these multi-sensory movies.</p>
<fig id="pbio-1000445-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g001</object-id><label>Figure 1</label><caption>
<title>Schematic illustrating experimental design and phase coherence analysis (for a single MEG channel).</title>
<p>The colors represent single-trial responses to each of the six audiovisual streams. The coherence analyses are performed on each of the 157 MEG channels separately. (a) The cross-trial phase coherence is calculated on all 15 trials of the same stimulus condition (same color) and compared to a mixture of trials (see <xref ref-type="sec" rid="s4">Methods</xref>) to get the phase-based and power-based movie discrimination ability (see <xref ref-type="fig" rid="pbio-1000445-g002">Figure 2ab</xref>). (b) Cross-movie phase coherence is calculated by combining response trials across two movie stimuli (two different colors in each column), where one dimension is matched in auditory (SameAud), visual (SameVis), or neither modality input (NoSame). See more equation details in <xref ref-type="sec" rid="s4">Methods</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g001" xlink:type="simple"/></fig><fig id="pbio-1000445-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g002</object-id><label>Figure 2</label><caption>
<title>Phase-based and Power-based movie discrimination ability.</title>
<p>Phase (a) and power (b) discrimination ability as a function of frequency (2–50 Hz) for 20 auditory (solid circles) and 20 visual channels (open circles) selected from separate auditory and visual localizer pretests for each participant. The gray box denotes the delta-theta range (2–7 Hz) selected for further analyses. The phase discrimination score in this range is significantly above 0. Error bars indicate the standard error across the 36 calculated samples (six stimulus conditions, six subjects).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g002" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Modality Specificity in Low-Frequency Phase Tracking</title>
<p>Having established the <italic>sensitivity of the low-frequency phase pattern</italic> to different audiovisual movie streams using the cross-trial phase coherence (<xref ref-type="fig" rid="pbio-1000445-g001">Figure 1a</xref>), we next evaluated its <italic>modality specificity</italic> in auditory and visual cortical responses, by employing a cross-movie coherence analysis (<xref ref-type="fig" rid="pbio-1000445-g001">Figure 1b</xref>; <xref ref-type="supplementary-material" rid="pbio.1000445.s003">Figure S3</xref> schematizes the logic). Given the predominantly unisensory characteristics of cortical responses early in the cortical processing hierarchy, the low-frequency phase pattern should be mainly driven by the stimulus in the corresponding sensory modality. We thus tested a double dissociation hypothesis, namely that in <italic>auditory</italic> channels, movie clips sharing the <italic>same</italic> auditory input regardless of visual input (stimuli we call “SameAud”) should induce a more similar low-frequency phase pattern response (and display higher cross-movie delta-theta phase coherence) than those containing the same visual but <italic>different</italic> auditory input (stimuli called “SameVis”); analogously, in <italic>visual</italic> channels, SameVis movies should yield higher cross-movie delta-theta phase coherence compared to SameAud movie pairs.</p>
<p>For the three matched clips (V1A1, V2A2, V3A3), we selected the corresponding SameVis and SameAud stimuli (see <xref ref-type="fig" rid="pbio-1000445-g001">Figure 1b</xref> and <xref ref-type="supplementary-material" rid="pbio.1000445.s003">Figure S3</xref> for visualization of the design; e.g., for matched clip V1A1, its SameVis counterpart is V1A3, its SameAud is V2A1); we then calculated the similarity or coherence between the responses to matched clips and the corresponding SameAud or SameVis mixed clips (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e001" xlink:type="simple"/></inline-formula>), separately for auditory and visual areas. The cross-movie low-frequency phase coherence results (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e002" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e003" xlink:type="simple"/></inline-formula>) show a double dissociation (<xref ref-type="fig" rid="pbio-1000445-g003">Figure 3a</xref>; condition×place interaction, <italic>F</italic>(1, 5) = 10.44, <italic>p</italic> = 0.023). This confirms the efficacy of the auditory and visual “functional channel localizers”; more importantly, though, this analysis suggests, plausibly, that the phase patterns over auditory and visual areas are predominantly driven by the sensory stimulus structure in the corresponding modality. Critically, the corresponding power coherence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e004" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e005" xlink:type="simple"/></inline-formula>) did not show the double dissociation pattern (<xref ref-type="fig" rid="pbio-1000445-g003">Figure 3b</xref>; condition×place interaction, <italic>F</italic>(1, 5) = 0.077, <italic>p</italic> = 0.79), confirming that <italic>precise timing</italic>—as reflected in the phase of delta and theta activity—plays a dominant role in sensory stream representation.</p>
<fig id="pbio-1000445-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g003</object-id><label>Figure 3</label><caption>
<title>Low-frequency band phase pattern reflects within- and across-modality tracking.</title>
<p>Cross-movie response coherence (how similar are the responses elicited by two movies) in delta-theta phase pattern (a) and power pattern (b) for the 20 auditory and 20 visual channels selected from independent localizer pretests (see <xref ref-type="fig" rid="pbio-1000445-g001">Figure 1b</xref> and <xref ref-type="sec" rid="s4">Methods</xref> for analysis illustration). SameVis: movie clip <italic>pair</italic> sharing the same visual but different auditory input; SameAud: movie <italic>pair</italic> sharing same auditory but different visual input; NoSame: movie pair differing in both auditory and visual inputs. For example, for movie clip V1A1, the SameVis, SameAud, and NoSame movies correspond to V1A3, V2A1, and V3A2, respectively. Error bars indicate the standard error across six subjects.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g003" xlink:type="simple"/></fig>
<p>The modality-<italic>dependent</italic> characteristics of the delta-theta phase pattern in all 157 recorded channels were verified by comparing the spatial distribution maps of the cross-movie delta-theta phase coherence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e006" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e007" xlink:type="simple"/></inline-formula>). We observed a lateral temporal origin of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e008" xlink:type="simple"/></inline-formula> and an occipital origin of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e009" xlink:type="simple"/></inline-formula> in every subject (<xref ref-type="fig" rid="pbio-1000445-g004">Figure 4</xref>). The spatial distribution results thus confirm the finding that in response to a multi-sensory audiovisual stream, the low-frequency phase of the auditory and visual cortical activities principally and concurrently tracks the <italic>respective</italic> sensory stimulus dynamics.</p>
<fig id="pbio-1000445-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g004</object-id><label>Figure 4</label><caption>
<title>Low-frequency cross-movie phase coherence distribution map.</title>
<p>Delta-theta cross-movie phase coherence distribution map for each of the six subjects, indicating within-modality tracking. In this flat map of the MEG recordings, left is left, right is right, and red indicates larger cross-movie phase coherence. Left: distribution map for larger cross-movie delta-theta phase response coherence of SameVis movie pair versus SameAud movie pair. The comparison implicates occipital (visual) cortex. Right: distribution map for larger cross-movie delta-theta phase coherence of SameAud movie pair versus SameVis movie pair. This analysis shows auditory activation.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g004" xlink:type="simple"/></fig></sec><sec id="s2c">
<title>Cross-Modality Low-Frequency Phase Tracking</title>
<p>We then examined the critical hypothesized <italic>cross-modality modulation effects</italic> in the low-frequency phase pattern, by studying whether naturalistic visual input can affect the phase of auditory cortical oscillations (as previously only observed using artificial stimuli and in animal data), and similarly whether the auditory dynamic structure influences the phase of ongoing rhythmic activities in visual cortex, to some extent. A cross-movie coherence analysis was again performed (<xref ref-type="fig" rid="pbio-1000445-g001">Figure 1b</xref>; <xref ref-type="supplementary-material" rid="pbio.1000445.s003">Figure S3</xref> schematizes the logic), by calculating the coherence or similarity between the responses to matched clips and the corresponding NoSame mixed clips, i.e. movie clip differing in both auditory and visual input (e.g., for matched clip V1A1, V2A2, V3A3, their respective NoSame counterpart is V3A2, V1A3, V2A1), in auditory and visual areas separately.</p>
<p>The logic of this analysis is as follows: If the low-frequency phase pattern in one sensory modality is systematically influenced by the other modality, movies sharing same visual input (SameVis) should show more similar low-frequency phase pattern in <italic>auditory</italic> cortex, compared to movies differing in both visual and auditory inputs (NoSame); similarly, in visual cortex, the SameAud movies should show higher cross-movie coherence than NoSame movies. <xref ref-type="fig" rid="pbio-1000445-g003">Figure 3a</xref> shows that the NoSame pair manifested the smallest cross-movie phase coherence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e010" xlink:type="simple"/></inline-formula>), supporting our hypothesis (3-way ANOVA, condition main effect, <italic>F</italic>(2, 10) = 36.394, <italic>p</italic>&lt;0.0001; post-hoc analysis, NoSame versus SameVis, <italic>p</italic>&lt;0.0001, NoSame versus SameAud, <italic>p</italic>&lt;0.0001; condition×place interaction, <italic>F</italic>(2, 10) = 8.467, <italic>p</italic> = 0.007). The delta-theta power pattern reflects no such effect (<xref ref-type="fig" rid="pbio-1000445-g003">Figure 3b</xref>). This suggests that in response to an audio-visual stream (e.g., V1A1), the phase of the cortical activity is driven and modulated not only by the input in the corresponding modality (double dissociation result discussed above) but also by input from another modality (cross-sensory phase modulation).</p>
</sec><sec id="s2d">
<title>Matched Movies Elicit Stronger Trial-to-Trial Low-Frequency Phase Pattern</title>
<p>The above <italic>cross-movie</italic> coherence results demonstrate that the phase pattern in response to an audiovisual stream carries information about both auditory and visual stimulus structure. We next ask whether multisensory tracking is simply a mixture of passive following responses to unisensory stimuli, or—more interestingly—whether phase-tracking plays an <italic>active</italic> role in multisensory integration, by establishing a cross-modal temporal context in which a unisensory stimulus unfolds and merges into a coherent perceptual representation. We first examined the similarity in the elicited phase pattern response in auditory and visual areas. Given the congruent temporal structure in matched audiovisual stimuli, together with the observed within-modality phase tracking, we predict that both auditory and visual areas show higher similarity in low-frequency phase responses for the matched conditions. The cross-movie analysis results support the hypothesis (<xref ref-type="fig" rid="pbio-1000445-g005">Figure 5c</xref>, paired <italic>t</italic> test, <italic>t</italic>(9) = 2.31, <italic>p</italic> = 0.046); the corresponding power coherence revealed no statistical difference (<xref ref-type="fig" rid="pbio-1000445-g005">Figure 5d</xref>, paired <italic>t</italic> test, <italic>t</italic>(9) = 1.93, <italic>p</italic> = 0.086).</p>
<fig id="pbio-1000445-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g005</object-id><label>Figure 5</label><caption>
<title>Inter-trial low-frequency phase coherence depends on audiovisual temporal.</title>
<p>Cross-trial coherence in delta-theta phase pattern (a) and power pattern (b) for Matched (black bar) and Mixed (grey bar) stimulus conditions, in 20 auditory and 20 visual channels (see <xref ref-type="fig" rid="pbio-1000445-g001">Figure 1a</xref> and <xref ref-type="sec" rid="s4">Methods</xref> for analysis illustration). Cross-area (auditory and visual) coherence in delta-theta phase pattern (c) and power pattern (d) for Matched (black bar) and Mixed (grey bar) stimulus conditions. Error bars indicate the standard error across 10 subjects.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g005" xlink:type="simple"/></fig>
<p>In light of the observed similarity between the phase response in the two modalities, we next conjecture that the cross-modality phase modulation will occur in a manner “temporally commensurate” to within-modality phase modulation, leading to more temporally reliable integration and consequently achieving a more <italic>robust</italic> low-frequency-based representation of audio-visual naturalistic stimuli (enhanced trial-to-trial response reliability) in both sensory areas (not between areas). Importantly, the <italic>cross-trial reliability enhancement</italic> hypothesis cannot be derived from a passive following response interpretation.</p>
<p>We compared the delta-theta cross-trial phase coherence for the three matched and three mixed movies separately, noting that the three movies in the mixed group contained exactly the same auditory and visual inputs as the matched one—but in incongruent audio-visual combinations (<xref ref-type="fig" rid="pbio-1000445-g001">Figure 1a</xref>). We observed stronger trial-by-trial delta-theta phase pattern coherence in the matched group than in the mixed group (2-way ANOVA, significant main effect of condition, <italic>F</italic>(1, 9) = 7.33, <italic>p</italic> = 0.024), in both auditory and visual areas (<xref ref-type="fig" rid="pbio-1000445-g005">Figure 5a</xref>). The cross-trial power coherence revealed no significant difference between the two conditions (<xref ref-type="fig" rid="pbio-1000445-g005">Figure 5b</xref>, condition main effect, 2-way ANOVA, <italic>F</italic>(1, 9) = 3.64, <italic>p</italic> = 0.09). The result that the trial-by-trial phase reliability depends on the relative audiovisual temporal relationship thus supports the “<italic>active cross-modal phase modulation</italic>” hypothesis for multisensory integration. In our view, sensory cortical activity builds a more efficient and robust continuous representation for a temporally congruent multi-sensory stream by mutually modulating the low-frequency phase of ongoing oscillatory activity in an <italic>active</italic> manner, perhaps facilitating temporal packaging of information that can then act “predicatively” across modalities.</p>
</sec><sec id="s2e">
<title>Classification Based on Low-Frequency Phase Pattern</title>
<p>To apply a unified analysis framework to our data, a classification analysis was employed based on the low-frequency (2–7 Hz) phase pattern in single response trials across all six movies. For each of the six movie clips, the delta-theta phase pattern as a function of time for one single trial response under one stimulus condition was arbitrarily chosen as a template response for that movie. The delta-theta phase pattern of the remaining trials of all stimulus conditions was calculated, and their similarity to each of the six templates was defined as the distance to the templates. Responses were then classified to the closest movie template. The classification was computed 100 times for each of the 20 auditory and 20 visual channels in each subject, by randomly choosing template combinations. This classifier analysis shows that the delta-theta phase pattern successfully discriminates among movies. The individual trial data for each condition were predominantly classified as belonging to that condition, for both auditory (<xref ref-type="fig" rid="pbio-1000445-g006">Figure 6a</xref>) and visual (<xref ref-type="fig" rid="pbio-1000445-g006">Figure 6b</xref>) areas. Second, the classification results support the tracking hypothesis for matched versus mixed conditions, revealing higher “self”-classification for matched than mixed movies. Third, the modality-specific characteristics of phase tracking were manifested in the classification in that in auditory areas, each of the six movies was categorized to the movie stimulus sharing the same auditory input (SameAud) with larger proportion than to SameVis input, and vice versa for visual areas. Finally, the classification results also support the elevated response reliability by congruent audiovisual stimuli. The response to each movie clip was primarily classified to itself, secondly to the clip sharing the same modality (e.g., SameAud for auditory channels), and thirdly to the movies sharing the same input in the other modality (e.g., SameVis in auditory area), which has a significantly better classification proportion than stimuli differing in both inputs (NoSame). A statistical analysis and summary of the classification data (<xref ref-type="fig" rid="pbio-1000445-g006">Figure 6c</xref>) underscores the effect of this cross-sensory phase modulation. The results demonstrate that the low-frequency phase pattern in sensory cortices can be relied on for audiovisual stream discrimination in single trial responses, and that it is modulated by input from multiple sensory domains, reflecting an <italic>active</italic> cross-sensory integration, dynamically evolving in time.</p>
<fig id="pbio-1000445-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g006</object-id><label>Figure 6</label><caption>
<title>Low-frequency phase-pattern-based classification performance.</title>
<p>Grand average of delta-theta-phase-based classification histograms for each of the six audiovisual stream conditions (3 matched and 3 mixed conditions) for auditory (a) and visual areas (b). Note that the sum of the clustered bar sums to 1. Error bars indicate the standard error across six subjects. (c) Generalization and statistical analysis of classification performance (ab).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g006" xlink:type="simple"/></fig></sec><sec id="s2f">
<title>Optimal Phase and Active Cross-Modal Low-Frequency Phase Modulation</title>
<p>Neurophysiological work in animal preparations suggests that non-auditory inputs can modulate auditory responses towards a preferred excitability state, by aligning the phase of ongoing low-frequency auditory activity with a specific phase angle known to elicit maximal stimulus-driven responses, resulting in the cross-sensory response amplification <xref ref-type="bibr" rid="pbio.1000445-Lakatos1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>. We hypothesize that stimulus-induced temporal regularization leads to robust phase tracking, by resetting the phase of the intrinsic low-frequency rhythmic activity to a preferred phase. We thus expect (i) that the cross-trial delta-theta phase coherence is phase dependent, and the phase values corresponding to high cross-trial phase coherence values are non-uniformly distributed and centered on a preferred phase angle, and (ii) that the matched movie elicits a larger fraction of optimal phase compared to the mixed condition, since a temporally congruent stream would achieve cross-sensory phase tracking enhancement, by regularizing low-frequency phase to the optimal phase angle more robustly in each response trial.</p>
<p>We explored the relationship between the cross-trial phase coherence and the corresponding phase angles and observed an increasingly clustered phase angle distribution (around 0 and ±<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e011" xlink:type="simple"/></inline-formula>) for higher phase coherence in both auditory and visual areas (<xref ref-type="fig" rid="pbio-1000445-g007">Figure 7a</xref>, upper and lower panel). As shown in <xref ref-type="fig" rid="pbio-1000445-g007">Figure 7b</xref>, we further quantified the deviation of phase distribution from uniform distribution as a function of cross-trial phase coherence values, and the results confirm that higher phase coherence corresponds to larger deviation from uniform distribution (2-way ANOVA, <italic>F</italic>(19, 95) = 67.99, <italic>p</italic>&lt;0.001), thus suggesting a trend of non-uniform phase clustering for the robust phase tracking pattern. (Note that the drop in the deviation values for the highest phase coherence (∼1) may be due to the artifacts produced by small samples and large variance across subjects during such a high coherence regime.) The findings demonstrate that it is mainly the stimulus-induced delta-theta phase resetting to the preferred phase angle (0 or ±<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e012" xlink:type="simple"/></inline-formula>) that regularizes the low-frequency phase pattern in each response trial to improve the phase tracking reliability. In addition, as shown in <xref ref-type="fig" rid="pbio-1000445-g007">Figure 7c</xref>, the matched movies showed a larger fraction of optimal phase angle (0 or ±<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e013" xlink:type="simple"/></inline-formula>) than mixed movies for higher phase coherence (&gt;0.7) in both auditory and visual areas, as hypothesized; statistical testing confirms that phase angle at ±<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e014" xlink:type="simple"/></inline-formula> was more relevant to preferred or optimal phase (2-way ANOVA, main effect of condition, <italic>F</italic>(1, 5) = 5.794, <italic>p</italic> = 0.06) than phase angle at 0 (2-way ANOVA, main effect of condition, <italic>F</italic>(1, 5) = 2.856, <italic>p</italic> = 0.152), commensurate with optimal phase findings in neurophysiological studies <xref ref-type="bibr" rid="pbio.1000445-Lakatos1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser3">[45]</xref>. The results support the view that the visual (auditory) stream in a matched movie modulates the auditory (visual) cortical activity by aligning the phase to the optimal phase angle so that the expected auditory (visual) input arrives during a high excitability state, to be amplified and achieve the cross-sensory enhancement. In contrast, mixed, incongruent audiovisual streams cannot benefit from the cross-sensory phase regularization and thus are driven to the preferred phase angle with a significantly smaller fraction than matched movie stimuli.</p>
<fig id="pbio-1000445-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pbio.1000445.g007</object-id><label>Figure 7</label><caption>
<title>Low-frequency phase coherence and “optimal phase.”</title>
<p>(a) Grand average of phase histograms (<italic>x</italic>-axis) as a function of inter-trial delta-theta phase coherence (<italic>y</italic>-axis, 0∼1) across six subjects in auditory channels (upper) and visual channels (lower). Note that the sum of each row is 1. (b) Deviation score from uniform distribution as a function of inter-trial delta-theta phase coherence (<italic>x</italic>-axis, 0–1). Error bars indicate the standard error across subjects. (c) “Optimal” phase (0 and ±<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e015" xlink:type="simple"/></inline-formula>) fraction for matched (black bar) versus mixed (grey bar) conditions in auditory (upper) and visual (lower) channels. Error bars indicate the standard error across six subjects.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.g007" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We examined multi-sensory interaction in early sensory areas in MEG responses recorded from human subjects viewing and listening to natural audio-visual movies. We show that the low-frequency, delta and theta phase pattern in early visual and auditory cortices tracks (and can discriminate among) naturalistic visual and auditory stimuli, respectively, in single MEG response trials. In addition, the low-frequency phase pattern in one sensory domain can, to some extent, represent and track the stimulus structure of the other modality. Importantly, temporally aligned audio-visual streams (“matched”) elicit stronger low-frequency trial-by-trial phase response reliability than non-aligned streams (“mixed”), supporting an active cross-modal phase modulation versus a “passive stimulus following response” interpretation. Finally, the delta-theta phase clusters for stronger phase tracking, indicating that it is phase resetting to the preferred or “optimal phase” that tracks the “within-modality” and “across-modality” stimulus structure. Congruent multisensory stimuli lead to mutual driving towards “optimal phase” more reliably, perhaps to achieve temporally optimized cross-sensory enhancement. We conjecture that the ongoing phase pattern of slow oscillatory activity in sensory cortices provides a unified temporal frame of reference in which continuous multi-sensory streams are seamlessly represented and integrated into a coherent percept.</p>
<sec id="s3a">
<title>Phase Tracking of Naturalistic Sensory Streams</title>
<p>Unlike pairings of transient artificial stimuli used in most previous audiovisual studies, we examined the cross-modal integration effects in presumptively unimodal areas by employing naturalistic audiovisual movies that are ethologically natural and extended in time (30-s film clips). Naturalistic stimuli contain complex structure and rich dynamics in the time domain, and it has been suggested that the relevant neural mechanisms are in part shaped by the statistical structure of natural environments <xref ref-type="bibr" rid="pbio.1000445-Simoncelli1">[47]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Karklin1">[48]</xref>. Our previous MEG studies revealed that the phase pattern of theta-band responses reliably tracks and discriminates natural spoken sentences <xref ref-type="bibr" rid="pbio.1000445-Luo1">[46]</xref>. Here we build on and extend the previous findings by showing that delta-theta phase tracking exists for multi-sensory streams and that the low-frequency phase response in auditory <italic>and</italic> visual cortices reliably tracks audio-visual movies concurrently. There is emerging consensus that the signals quantified in neuroimaging (e.g., MEG signals) reflect synchronized large-scale neuronal ensemble activity and have been found to mainly derive from LFP rather than spiking activity <xref ref-type="bibr" rid="pbio.1000445-Logothetis1">[49]</xref>. A recent neurophysiological study in monkeys quantified the information different codes carry about natural sounds in auditory cortex and found that spiking responses interpreted with regard to the relative phase of the accompanying slow ongoing LFP are more informative about the properties of the dynamic sound than spiking responses alone <xref ref-type="bibr" rid="pbio.1000445-Kayser4">[50]</xref>. The same encoding scheme has also been observed in visual cortex in response to natural movies <xref ref-type="bibr" rid="pbio.1000445-Montemurro1">[51]</xref>. Our results from human neuroimaging converge with these neurophysiological studies on low-frequency phase tracking for naturalistic streams and are commensurate with the observed essential role of brain oscillations in sensory processing, feature integration, and response selection within the various sensory modalities <xref ref-type="bibr" rid="pbio.1000445-Singer1">[30]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Fries1">[34]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Jensen1">[36]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Salinas1">[52]</xref>. It has been argued that intrinsic rhythms undergo significant phase resetting in response to stimulus presentation <xref ref-type="bibr" rid="pbio.1000445-Fries2">[35]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Makeig1">[53]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Fiser1">[54]</xref>, and crucially, some studies demonstrate that neuronal oscillations enhance the response robustness to natural stimulation by modulating the excitability state (phase resetting) for spiking activity <xref ref-type="bibr" rid="pbio.1000445-Schaefer1">[55]</xref>.</p>
</sec><sec id="s3b">
<title>Phase Tracking and Attention</title>
<p>Could one argue that the observed delta-theta phase tracking is due to different levels of attention to a given modality, given the important role of attention in multisensory integration <xref ref-type="bibr" rid="pbio.1000445-Macaluso2">[25]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Lakatos2">[56]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Lakatos3">[57]</xref>? Such a view cannot be a sufficient explanation because the low-frequency phase pattern distinguishes the audio-visual streams belonging to the matched <italic>or</italic> mixed conditions, both of which elicit similar attentional states. (The three matched (or mixed) movies should elicit similar attentional states, and therefore the delta-theta phase pattern should not be able to discriminate them only based on attentional state.) Interestingly, previous studies show that such cross-sensory interactions occur in anaesthetized animals <xref ref-type="bibr" rid="pbio.1000445-Bizley1">[19]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser1">[21]</xref>. These observations suggest that the general attentional level is not the main source underlying the observed delta-theta phase tracking. Recent studies <xref ref-type="bibr" rid="pbio.1000445-Lakatos2">[56]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Lakatos3">[57]</xref> revealed that the phase of low-frequency oscillations in auditory and visual cortex entrains to the rhythm of the attended sensory stream amidst multi-sensory inputs and thus could track either a visual or auditory stimulus. They suggest the phase modulation mechanism to underlie temporally based attention. Their results further challenge an attentional-load explanation for the present data, given the observed modality-specific characteristics (the double dissociation results), and support that the observed delta-theta phase tracking is not due to global modality-independent attentional modulation.</p>
<p>Uncontrolled eye movements also constitute a possible confounding factor, given previous findings reporting the effect of eye position on the auditory cortical responses <xref ref-type="bibr" rid="pbio.1000445-Fu1">[17]</xref>. We believe that the eye-movement-related activity <italic>may</italic> contribute to phase modulation in early sensory activity, but not in a dominant way, given that the cross-modal phase modulation exists under both anesthetized conditions <xref ref-type="bibr" rid="pbio.1000445-Bizley1">[19]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser1">[21]</xref> and controlled eye fixation conditions <xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>. Note that eye movements by themselves cannot account for the observed stronger modulation for matched over mixed audiovisual stimuli; both carry the <italic>same</italic> visual stream; which should result in a comparable pattern of eye movements. More generally, during the free viewing of movies, eye movements are argued to be tightly correlated with stimulus dynamics, which in turn induces phase tracking in brain signals, and therefore the phase modulation mechanism may also be integral to the temporally based attention. Fries <xref ref-type="bibr" rid="pbio.1000445-Fries2">[35]</xref> recently proposed a rhythmic input gain model to link attention to brain oscillations and suggested that the strength of gamma-band synchronization (binding by synchronization) is modulated with the theta rhythm, the phase of which makes or breaks selections of input segments, thus constituting a strong link to the “biased competition” modal in visual attention <xref ref-type="bibr" rid="pbio.1000445-Desimone1">[33]</xref>.</p>
</sec><sec id="s3c">
<title>Temporal Scales, Brain Oscillations, and Natural Statistics</title>
<p>We found that low-frequency phase patterns were sufficiently reliable to continuously track the naturalistic audiovisual streams. The crucial relevance of low-frequency oscillations to perceptual analysis has been observed in several studies <xref ref-type="bibr" rid="pbio.1000445-Lakatos1">[20]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Luo1">[46]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Kayser4">[50]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Montemurro1">[51]</xref>. The acoustic structure of both natural sounds and movies contain rich dynamics on multiple time scales, but with power dominance in the low-frequency range <xref ref-type="bibr" rid="pbio.1000445-Karklin1">[48]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Theunissen1">[58]</xref>–<xref ref-type="bibr" rid="pbio.1000445-Butts1">[60]</xref>. Accumulating evidence demonstrates that a coarse representation suffices for the comprehension of natural streams <xref ref-type="bibr" rid="pbio.1000445-Shannon1">[61]</xref>. For example, from the perspective of speech processing, a temporal window of ∼200 ms corresponds to mean syllable length across languages, and such a temporal window has been suggested as a fundamental unit for speech perception <xref ref-type="bibr" rid="pbio.1000445-Poeppel1">[62]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Hickok1">[63]</xref>. The observed tracking ability of slow quasi-rhythmic (and aperiodic) activity may be simply driven by the input temporal pattern, but we conjecture that it reflects an internal stable processing rhythm <xref ref-type="bibr" rid="pbio.1000445-Giraud1">[64]</xref> that is ideally suited to match the gross statistical temporal structure of natural streams. Recent data <xref ref-type="bibr" rid="pbio.1000445-Chandrasekaran1">[65]</xref> demonstrate robust temporal correspondence in the delta-theta range (2∼7 Hz) between visual and auditory streams in multisensory speech signals, supporting this interpretation.</p>
<p>In addition to the essential role of long-duration time scales in natural stimuli, the dynamic structure at other biologically relevant scales, especially the short windows (e.g., ∼25 ms) corresponding to gamma band oscillation, also carries important information <xref ref-type="bibr" rid="pbio.1000445-Poeppel1">[62]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Giraud1">[64]</xref>. Several previous studies show the relevance of gamma oscillations to multisensory integration, but in contexts of transient or evoked responses <xref ref-type="bibr" rid="pbio.1000445-Senkowski3">[40]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Mishra1">[42]</xref>, which is a very different approach from ours. In the current work, we examine the sustained response pattern to natural complex audiovisual scenes and the relevance to multisensory integration. A possible factor accounting for the absence of evidence for fast, gamma rhythms in tracking might lie in the task demands; subjects were only asked to passively view and listen to the audiovisual streams, without requiring their focused, selective attention to fast transitions, phonemes, any aspect of sublexical information, etc. Crucially, both unimodal and multimodal naturalistic streams contain various temporal scales that are nested within each other. For example, in human speech, high-frequency events (e.g., formant transitions) are temporally nested within low-frequency structures (e.g., syllables, phrases). Correspondingly, human cortical oscillations at different frequencies also manifest similar temporally nested relationships and tend to be phase-amplitude coupled <xref ref-type="bibr" rid="pbio.1000445-Canolty1">[66]</xref>. Such cross-scale coupling in both naturalistic extended stimuli and brain oscillations are consistent with the “sampling window hypothesis” for speech perception <xref ref-type="bibr" rid="pbio.1000445-Poeppel1">[62]</xref>, and further indicate a general cross-scale modulation mechanism underlying multi-sensory interaction <xref ref-type="bibr" rid="pbio.1000445-Lakatos2">[56]</xref>.</p>
</sec><sec id="s3d">
<title>Phase-Reset Mechanisms and Active Multisensory Interaction</title>
<p>The central finding concerns the hypothesis of active cross-modality phase modulation of endogenous oscillations in a multi-sensory context. Specifically, we observed that the auditory and visual modalities can mutually and actively modulate the phase of the internal low-frequency rhythms in early sensory cortical regions and that such cross-sensory driving efficiency depends on the relative audiovisual timing. A study recording A1 in awake macaques <xref ref-type="bibr" rid="pbio.1000445-Lakatos1">[20]</xref> revealed phase modulation in multi-sensory interaction: somatosensory inputs enhanced auditory processing by resetting the phase of ongoing neuronal oscillations in A1 so that the accompanying auditory input arrived during a high-excitability phase. A further neurophysiological experiment exploring the impact of visual stimulation on auditory responses demonstrated that visual stimuli modulated auditory cortex activity, at the level of both LFP and single-unit responses <xref ref-type="bibr" rid="pbio.1000445-Kayser2">[22]</xref>. Importantly, they too found that the observed cross-sensory enhancement correlated well with the resetting of slow oscillations to an optimal phase angle, and the multi-sensory interactions were sensitive to the audiovisual timing. Moreover, they discovered that matched audiovisual stimuli enhanced the trial-to-trial response reliability in auditory cortex of alert monkeys <xref ref-type="bibr" rid="pbio.1000445-Kayser3">[45]</xref>, precisely like one of our central findings of a tight link between cross-sensory modulation efficacy and relative audiovisual timing congruency. Our results in humans are thus in good agreement with these animal data and also implicate neural mechanisms accounting for previous behavioral results showing temporally matched visual amplification of auditory processing, in both monkeys <xref ref-type="bibr" rid="pbio.1000445-Ghazanfar3">[67]</xref> and human subjects <xref ref-type="bibr" rid="pbio.1000445-vanWassenhove1">[4]</xref>,<xref ref-type="bibr" rid="pbio.1000445-Fairhall1">[68]</xref>.</p>
<p>Given the simple binary design here (matched versus mixed), further studies need to be executed by continuously jittering the temporal relationship between auditory and visual stimuli and investigating the influences in both behavior and cross-modal low-frequency phase modulation in a more systematic way. Recently, Schroeder et al. <xref ref-type="bibr" rid="pbio.1000445-Schroeder2">[44]</xref> proposed a phase-resetting-based mechanism to solve the “cocktail party” problem using such a mechanism and hypothesized that the visual amplification of speech perception is operating through efficient modulation or “shaping” of ongoing neuronal oscillations. Our results support such a model and indicate that multi-sensory integration is at least in part based on a cross-modal phase resetting mechanism in early cortical sensory regions. The phase patterns of the ongoing rhythmic activity in early sensory areas help construct a temporal framework that reflects both unimodal information and multimodal context from which the unified multisensory perception is actively constructed. However, we do not exclude the existence of multiple multisensory integration pathways, as shown in a recent study <xref ref-type="bibr" rid="pbio.1000445-Arnal1">[29]</xref> demonstrating the convergence of lateral and feedback in multisensory integration, given the complex characteristics of integration. In a more general sense, we surmise that the dynamic interplay of neural populations <xref ref-type="bibr" rid="pbio.1000445-Senkowski1">[28]</xref> constitutes a unified temporal framework where the segmented senses unfold and merge, resulting in the seamless multisensory-integrated dynamic world we perceive. Further human studies with better spatial resolution (e.g., intracranial EEG in humans and fMRI+EEG recording) may help to address the issue in a more granular way. The results from this human MEG experiment suggest that neuroimaging data can make a fruitful contribution to our understanding of neural coding, building on concepts of neural timing that can be exploited productively at the levels of analysis of large neuronal populations.</p>
</sec></sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Subjects and MEG Data Acquisition</title>
<p>Six right-handed subjects provided informed consent before participating in the experiment. All subjects had normal vision and hearing. We have acquired data from additional four subjects (10 subjects in total then) to specifically investigate matched versus mixed cross-trial low-frequency phase coherence difference (as shown in <xref ref-type="fig" rid="pbio-1000445-g005">Figure 5</xref>). Neuromagnetic signals were recorded continuously with a 157 channel whole-head MEG system (5 cm baseline axial gradiometer SQUID-based sensors; KIT, Kanazawa, Japan) in a magnetically shielded room, using a sampling rate of 1,000 Hz and an online 100 Hz analog low-pass filter, with no high-pass filtering.</p>
</sec><sec id="s4b">
<title>Stimuli and Experimental Procedures</title>
<p>Three audio-visual movie clips (V1+A1, V2+A2, V3+A3) were selected from the movie “Dumb and Dumber” (1994, New Line Platinum Series) to form the three “Matched” movie stimuli (see <xref ref-type="supplementary-material" rid="pbio.1000445.s001">Figure S1</xref>). We constructed another three “Mixed” movie clips, by shuffling the auditory and visual combinations (V1+A3, V2+A1, V3+A2). All six movie clips contained natural conversation in an audiovisual setting and were 30 s in duration. Prior to the movie experiment, the subjects participated in one auditory localizer pretest in which they were presented with 1 kHz tone pips (duration 50 ms) and one visual localizer pretest in which they were presented with alternating checkerboard stimuli. Both pretests were performed to collect functional localization data for auditory and visual cortices (to identify the most responsive channels, <xref ref-type="supplementary-material" rid="pbio.1000445.s002">Figure S2</xref>). Subjects were told to passively view and listen to the six audio-visual stimulus streams (no explicit task) presented on a rear projection screen in the shielded room screen (the clips subtended ∼18 deg horizontal and 11 deg vertical visual angles, presented at typical photopic luminance values) without restriction on eye movements. Each of the six movie clips was presented 15 times, in two separate blocks (Matched block and Mixed block), with the audio track presented at a comfortable loudness level (∼70 dB).</p>
</sec><sec id="s4c">
<title>Data Analysis</title>
<p>In the auditory localizer pretest, the large electrophysiological response peak with latency around 100 ms after tone-pip onset was determined (M100 or N1m) and the 20 channels with largest response amplitude were defined as the <italic>auditory channels</italic>. These channels, unsurprisingly, largely lie over the temporal lobe. In the visual localizer pretest, the 20 channels with largest response amplitude at the response peak with latency around 150 ms were selected as <italic>visual channels</italic> (typically occipital). The channel selection procedure was performed for each subject separately, and all subsequent analysis was done on those independently selected channels to represent auditory and visual cortical activity, respectively. There was no overlap among the channel groups.</p>
<p>For each of the six audio-visual stimuli (15 trials of each), 12 out of 15 response trials were chosen and termed “within-group” signals (six within-group signals corresponding to six movie stimuli). Note that selecting 12 trials out of 15 trials here was simply due to this specific discrimination analysis that required trial number to be an integer number of 6 (the stimulus condition number); the following other analyses were performed on all the 15 response trials. Two response trials (one-sixth of the 12 trials for each stimulus condition) were chosen from each of the six groups and combined to construct a 12-trial “across-group” signal. Six across-group signals were constructed by repeating the combination procedure six times. For each of the twelve 12-trial signal groups (six <italic>within-group</italic> and six <italic>across-group</italic> signals), the spectrogram of the entire 30 s of each single trial response was calculated using a 500 ms time window in steps of 100 ms, for each of the 20 auditory channels and 20 visual channels defined for each subject. The phase and power were calculated as a function of frequency and time and were stored for further analysis. The “cross-trial phase coherence” (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e016" xlink:type="simple"/></inline-formula>) and “cross-trial power coherence” (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e017" xlink:type="simple"/></inline-formula>) were calculated as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.e018" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e019" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e020" xlink:type="simple"/></inline-formula> are the phase and absolute amplitude at the frequency bin i and temporal bin j in trial n, respectively. These calculated cross-trial coherence parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e021" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e022" xlink:type="simple"/></inline-formula>) are dimensionless quantity and were compared between each of six within-group signals and each of six across-group signals separately. The discrimination function (also dimensionless quantity) for each frequency bin i was defined as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.e023" xlink:type="simple"/></disp-formula></p>
<p>The resulting six discrimination functions for each of the six subjects were then averaged. A value significantly above 0 indicates larger cross-trial coherence of within-group signals than across-group signals. The average values within delta and delta-theta ranges (∼2–7 Hz) from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e025" xlink:type="simple"/></inline-formula> were then selected for further analysis, given the above-zero discrimination score in this frequency range in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e026" xlink:type="simple"/></inline-formula> function (upper panel of <xref ref-type="fig" rid="pbio-1000445-g001">Figure 1</xref>). Importantly, note the different formulas from which phase coherence and power difference are derived, due to their different characteristics. We calculated power coherence in terms of the cross-trial standard deviation of power pattern normalized by the power in each frequency band, similar to the Fano factor calculation in neurophysiology, but the value is in reversed direction (smaller Fano factor corresponds to larger reliability, and Fano factor can be below or above 1). Therefore, correspondingly, the power coherence values, as a result of the current computation, would not necessarily be smaller than 1, which is different from the phase coherence range (0–1), and therefore cannot be directly compared as quantities.</p>
<p>For the <italic>cross-movie coherence analysis</italic> (<xref ref-type="fig" rid="pbio-1000445-g003">Figure 3</xref>, <xref ref-type="fig" rid="pbio-1000445-g004">Figure 4</xref>), for each of the three matched movie clips (V1A1, V2A2, V3A3), we first selected the corresponding SameVis (V1A3, V2A1, V3A2), SameAud (V2A1, V3A2, V1A3), and NoSame (V3A2, V1A3, V2A1) movie stimulus in the mixed group, and then calculated the cross-movie delta-theta phase coherence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e027" xlink:type="simple"/></inline-formula>) and power coherence (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e028" xlink:type="simple"/></inline-formula>) (both of them are dimensionless quantities) for each of the 20 auditory and 20 visual channels defined in localizer pretest in each subject, by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.e029" xlink:type="simple"/></disp-formula></p>
<p>Note that the cross-movie coherence values derived from the above equation actually quantify the similarity extent of the response from two movies, in either phase or in power pattern (see <xref ref-type="supplementary-material" rid="pbio.1000445.s004">Text S1</xref> for the difference between the cross-movie analysis employed here and traditional cross-channel coherence analysis). For example, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e030" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e031" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e032" xlink:type="simple"/></inline-formula> indicate how similar the delta-theta phase responses elicited by two movies sharing the same visual stream but different auditory input are (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e033" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pbio-1000445-g003">Figure 3</xref>). We calculated it in auditory channels and visual channels separately.</p>
<p>The across-movie delta-theta phase coherence distribution maps (<xref ref-type="fig" rid="pbio-1000445-g004">Figure 4</xref>) for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e034" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e035" xlink:type="simple"/></inline-formula> conditions were constructed, respectively, in terms of the corresponding values of all 157 MEG channels for each subject.</p>
<p>To evaluate the low-frequency <italic>inter-trial phase and power coherence</italic> (<xref ref-type="fig" rid="pbio-1000445-g005">Figure 5ab</xref>) for matched (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e036" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e037" xlink:type="simple"/></inline-formula>) and mixed (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e038" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e039" xlink:type="simple"/></inline-formula>) conditions, we first calculated the low-frequency inter-trial phase coherence for each of the six movie stimuli (Movie1∼Movie6: V1A1, V2A2, V3A3, V1A3, V2A1, V3A2) and then averaged the inter-trial delta-theta phase coherence and power coherence for the three matched movies and the three mixed movies separately, by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.e040" xlink:type="simple"/></disp-formula></p>
<p>The cross-area analysis is similar to the cross-movie analysis but calculates the pattern similarity between auditory channels and visual channels, instead of that between movie 1 and movie 2 in auditory and visual channels separately in cross-movie analysis.</p>
<p>In the classification analysis (<xref ref-type="fig" rid="pbio-1000445-g006">Figure 6</xref>), for each of the six movies, the delta-theta phase pattern as a function of time for one single trial under one stimulus condition was arbitrarily chosen as a template response for that movie. The delta-theta phase pattern of the remaining trials of all stimulus conditions was calculated, and their similarity to each of the six templates was defined as the distance to the templates <xref ref-type="bibr" rid="pbio.1000445-Luo1">[46]</xref>. Responses were then classified to the closest movie template. The classification was computed 100 times for each of the 20 auditory and 20 visual channels in each subject, by randomly choosing template combinations.</p>
<p>In the optimal phase analysis (<xref ref-type="fig" rid="pbio-1000445-g007">Figure 7</xref>), for each of the 20 auditory and 20 visual channels in each subject, the calculated cross-trial phase coherence <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e041" xlink:type="simple"/></inline-formula> (i denotes time index and j denotes frequency index in range between 2∼7 Hz) was divided into 20 bins ranging from 0 to 1. The phase angle <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e042" xlink:type="simple"/></inline-formula> (n denotes the trial index) histograms in the range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e043" xlink:type="simple"/></inline-formula> in each of the 20 <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e044" xlink:type="simple"/></inline-formula> value ranges was then constructed, and the resulting matrix was averaged across six stimulus conditions and 20 selected channels for each subject (<xref ref-type="fig" rid="pbio-1000445-g006">Figure 6a</xref> shows the grand average of the matrices). The deviation of the phase histogram <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e045" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e046" xlink:type="simple"/></inline-formula> indicates the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e047" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e048" xlink:type="simple"/></inline-formula> indicates the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e049" xlink:type="simple"/></inline-formula>) from uniform distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e050" xlink:type="simple"/></inline-formula> was quantified by deviation function as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e051" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e052" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pbio-1000445-g006">Figure 6b</xref>.</p>
<p>We then selected all the phase angles with corresponding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e053" xlink:type="simple"/></inline-formula> above 0.7 for all the selected channels in each subject and quantified the number of phase angles around 0 and around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e054" xlink:type="simple"/></inline-formula> for the matched and mixed movie stimuli, respectively.</p>
<p>We also performed a control analysis to rule out “leaking” induced cross-modal modulation (see <xref ref-type="supplementary-material" rid="pbio.1000445.s005">Text S2</xref> for details).</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pbio.1000445.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.s001" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Audiovisual movie stimulus illustration.</bold> Three matched audiovisual movie clip illustration (V1+A1, V2+A2, V3+A3). The three mixed audiovisual movie stimuli are mixtures of V2+A1, V1+A3, and V3+A2.</p>
<p>(0.67 MB DOC)</p>
</caption></supplementary-material>
<supplementary-material id="pbio.1000445.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.s002" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Channel localization (linked to </bold><xref ref-type="fig" rid="pbio-1000445-g002"><bold>Figures 2</bold></xref><bold>, </bold><xref ref-type="fig" rid="pbio-1000445-g003"><bold>3</bold></xref><bold>, </bold><xref ref-type="fig" rid="pbio-1000445-g004"><bold>4</bold></xref><bold>, </bold><xref ref-type="fig" rid="pbio-1000445-g005"><bold>5</bold></xref><bold>, </bold><xref ref-type="fig" rid="pbio-1000445-g006"><bold>6</bold></xref><bold>, </bold><xref ref-type="fig" rid="pbio-1000445-g007"><bold>7</bold></xref><bold> in auditory and visual channels analysis).</bold> Auditory and visual localizer-based contour map for one representative subject. Red indicates a large absolute response value around the M100 peak latency (auditory localizer) and the M150 peak latency (visual localizer). Of the 157 recorded channels, 20 auditory and 20 visual channels were chosen based on the contour map for each subject—with no overlap allowed (i.e., the main analyses are based on spatially distinct sets of channels). Predictably, the visual localizer implicates occipital channels (both on the left and right of the midline), and the auditory localizer reflects the more anterior canonical (dipolar) distribution that has two channel groupings around a temporal lobe source (M100 dipole pattern). The color bar is in units of fT.</p>
<p>(0.16 MB DOC)</p>
</caption></supplementary-material>
<supplementary-material id="pbio.1000445.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.s003" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Cross-movie coherence analysis illustration (linked to </bold><xref ref-type="fig" rid="pbio-1000445-g003"><bold>Figure 3</bold></xref><bold> and </bold><xref ref-type="fig" rid="pbio-1000445-g006"><bold>Figure 6</bold></xref><bold>).</bold> Illustration of the logic of cross-movie phase coherence analysis. In each of the six movie stimuli (first row of <xref ref-type="supplementary-material" rid="pbio.1000445.s002">Figure S2</xref>), the solid bar represents the auditory stream and the hatched bar of the same color represents the corresponding visual stream. The middle and lower rows of <xref ref-type="supplementary-material" rid="pbio.1000445.s002">Figure S2</xref> indicate the hypothesized “representation ratio” of the stimulus in auditory and visual areas, respectively, in that the auditory stimulus dynamics will be more strongly represented in auditory cortex (solid bar) and the visual information (hatched bar) will be better represented in visual cortex. Crucially, if there exists direct modulation across sensory areas, the auditory area will <italic>also</italic> represent visual information, although to a lesser degree, and vice versa in the visual area. The figure illustrates an arbitrary hypothesized “representation distance” among the six movie stimuli in auditory and visual areas given the representation ratios in 2a. In this visualization, the distance between any two items corresponds to the similarity of the representation of the two movies, indicated by the arrow length between them (shorter distance means higher degree of similarity). D1, D2, and D3 correspond to the representation distance between one specific stimulus in the Matched group (A1V1 stimulus, for example) and the corresponding SameAud (A1V2), SameVis (A3V1), and NoSame (A2V3) counterparts in the Mixed group, respectively. A cross-modal representation results in the D2&lt;D3 prediction for the auditory area and the D1&lt;D3 prediction in the visual area. For example, the additional representation of visual information (hatched bar) in the auditory area makes the SameVis pair representation (D2) more similar (they both contain the representation for the same movie) compared to the NoSame pair. In contrast, as shown in <xref ref-type="supplementary-material" rid="pbio.1000445.s002">Figure S2c</xref>, if there is no significant cross-modal representation (either no or an ineffective visual representation in auditory area and vice versa in the visual area), there will be not much difference in the distance for the SameVis pair and the NoSame pair (similar D2 and D3) in auditory areas, and similarly D1 and D3 in visual areas. Therefore, in summary, by comparing whether <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e055" xlink:type="simple"/></inline-formula> (D2&lt;D3) in auditory channels and whether <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pbio.1000445.e056" xlink:type="simple"/></inline-formula> (D1&lt;D3) in visual channels, we can examine and quantify the <italic>cross-modal phase modulation</italic> effect.</p>
<p>(0.70 MB DOC)</p>
</caption></supplementary-material>
<supplementary-material id="pbio.1000445.s004" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.s004" xlink:type="simple"><label>Text S1</label><caption>
<p><bold>Cross-trial phase coherence versus traditional coherence analysis.</bold> Clarifying our cross-movie analysis, in comparison to traditional cross-channel coherence analysis.</p>
<p>(0.03 MB DOC)</p>
</caption></supplementary-material>
<supplementary-material id="pbio.1000445.s005" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pbio.1000445.s005" xlink:type="simple"><label>Text S2</label><caption>
<p><bold>Ruling out “leaking” induced cross-modal modulation.</bold> Control analysis.</p>
<p>(0.03 MB DOC)</p>
</caption></supplementary-material>
</sec></body>
<back>
<ack>
<p>Jeff Walker and Max Ehrmann provided expert technical support. We thank David Heeger, Mary Howard, Jonathan Simon, Xing Tian, and Elana Zion Golumbic for their critical comments and feedback.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pbio.1000445-McGurk1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McGurk</surname><given-names>H</given-names></name>
<name name-style="western"><surname>MacDonald</surname><given-names>J</given-names></name>
</person-group>             <year>1976</year>             <article-title>Hearing lips and seeing voices.</article-title>             <source>Nature</source>             <volume>264</volume>             <fpage>746</fpage>             <lpage>748</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Driver1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Spence</surname><given-names>C</given-names></name>
</person-group>             <year>1998</year>             <article-title>Crossmodal attention.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>8</volume>             <fpage>245</fpage>             <lpage>253</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Shams1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shams</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Kamitani</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Shimojo</surname><given-names>S</given-names></name>
</person-group>             <year>2000</year>             <article-title>Illusions. What you see is what you hear.</article-title>             <source>Nature</source>             <volume>408</volume>             <fpage>788</fpage>          </element-citation></ref>
<ref id="pbio.1000445-vanWassenhove1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>van Wassenhove</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Grant</surname><given-names>K. W</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2005</year>             <article-title>Visual speech speeds up the neural processing of auditory speech.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>102</volume>             <fpage>1181</fpage>             <lpage>1186</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Jones1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jones</surname><given-names>E. G</given-names></name>
<name name-style="western"><surname>Powell</surname><given-names>T. P</given-names></name>
</person-group>             <year>1970</year>             <article-title>An anatomical study of converging sensory pathways within the cerebral cortex of the monkey.</article-title>             <source>Brain</source>             <volume>93</volume>             <fpage>793</fpage>             <lpage>820</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Linden1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Linden</surname><given-names>J. F</given-names></name>
<name name-style="western"><surname>Grunewald</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Andersen</surname><given-names>R. A</given-names></name>
</person-group>             <year>1999</year>             <article-title>Responses to auditory stimuli in macaque lateral intraparietal area. II. Behavioral modulation.</article-title>             <source>J Neurophysiol</source>             <volume>82</volume>             <fpage>343</fpage>             <lpage>358</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fuster1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fuster</surname><given-names>J. M</given-names></name>
<name name-style="western"><surname>Bodner</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Kroger</surname><given-names>J. K</given-names></name>
</person-group>             <year>2000</year>             <article-title>Cross-modal and cross-temporal association in neurons of frontal cortex.</article-title>             <source>Nature</source>             <volume>405</volume>             <fpage>347</fpage>             <lpage>351</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Macaluso1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Macaluso</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>
</person-group>             <year>2005</year>             <article-title>Multisensory spatial interactions: a window onto functional integration in the human brain.</article-title>             <source>Trends Neurosci</source>             <volume>28</volume>             <fpage>264</fpage>             <lpage>271</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Calvert1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Calvert</surname><given-names>G. A</given-names></name>
</person-group>             <year>2001</year>             <article-title>Crossmodal processing in the human brain: insights from functional neuroimaging studies.</article-title>             <source>Cereb Cortex</source>             <volume>11</volume>             <fpage>1110</fpage>             <lpage>1123</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Beauchamp1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Beauchamp</surname><given-names>M. S</given-names></name>
</person-group>             <year>2005</year>             <article-title>See me, hear me, touch me: multisensory integration in lateral occipital-temporal cortex.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>15</volume>             <fpage>145</fpage>             <lpage>153</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Ghazanfar1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ghazanfar</surname><given-names>A. A</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
</person-group>             <year>2006</year>             <article-title>Is neocortex essentially multisensory?</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>278</fpage>             <lpage>285</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Stein1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Stein</surname><given-names>B. E</given-names></name>
<name name-style="western"><surname>Stanford</surname><given-names>T. R</given-names></name>
</person-group>             <year>2008</year>             <article-title>Multisensory integration: current issues from the perspective of the single neuron.</article-title>             <source>Nat Rev Neurosci</source>             <volume>9</volume>             <fpage>255</fpage>             <lpage>266</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Schroeder1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
<name name-style="western"><surname>Foxe</surname><given-names>J</given-names></name>
</person-group>             <year>2005</year>             <article-title>Multisensory contributions to low-level, ‘unisensory’ processing.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>15</volume>             <fpage>454</fpage>             <lpage>458</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Calvert2"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Calvert</surname><given-names>G. A</given-names></name>
<name name-style="western"><surname>Bullmore</surname><given-names>E. T</given-names></name>
<name name-style="western"><surname>Brammer</surname><given-names>M. J</given-names></name>
<name name-style="western"><surname>Campbell</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Williams</surname><given-names>S. C</given-names></name>
<etal/></person-group>             <year>1997</year>             <article-title>Activation of auditory cortex during silent lipreading.</article-title>             <source>Science</source>             <volume>276</volume>             <fpage>593</fpage>             <lpage>596</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Foxe1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foxe</surname><given-names>J. J</given-names></name>
<name name-style="western"><surname>Morocz</surname><given-names>I. A</given-names></name>
<name name-style="western"><surname>Murray</surname><given-names>M. M</given-names></name>
<name name-style="western"><surname>Higgins</surname><given-names>B. A</given-names></name>
<name name-style="western"><surname>Javitt</surname><given-names>D. C</given-names></name>
<etal/></person-group>             <year>2000</year>             <article-title>Multisensory auditory-somatosensory interactions in early cortical processing revealed by high-density electrical mapping.</article-title>             <source>Brain Res Cogn Brain Res</source>             <volume>10</volume>             <fpage>77</fpage>             <lpage>83</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Foxe2"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foxe</surname><given-names>J. J</given-names></name>
<name name-style="western"><surname>Wylie</surname><given-names>G. R</given-names></name>
<name name-style="western"><surname>Martinez</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
<name name-style="western"><surname>Javitt</surname><given-names>D. C</given-names></name>
<etal/></person-group>             <year>2002</year>             <article-title>Auditory-somatosensory multisensory processing in auditory association cortex: an fMRI study.</article-title>             <source>J Neurophysiol</source>             <volume>88</volume>             <fpage>540</fpage>             <lpage>543</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fu1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fu</surname><given-names>K. M</given-names></name>
<name name-style="western"><surname>Johnston</surname><given-names>T. A</given-names></name>
<name name-style="western"><surname>Shah</surname><given-names>A. S</given-names></name>
<name name-style="western"><surname>Arnold</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Smiley</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2003</year>             <article-title>Auditory cortical neurons respond to somatosensory stimulation.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>7510</fpage>             <lpage>7515</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Ghazanfar2"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ghazanfar</surname><given-names>A. A</given-names></name>
<name name-style="western"><surname>Maier</surname><given-names>J. X</given-names></name>
<name name-style="western"><surname>Hoffman</surname><given-names>K. L</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
</person-group>             <year>2005</year>             <article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>5004</fpage>             <lpage>5012</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Bizley1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bizley</surname><given-names>J. K</given-names></name>
<name name-style="western"><surname>Nodal</surname><given-names>F. R</given-names></name>
<name name-style="western"><surname>Bajo</surname><given-names>V. M</given-names></name>
<name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name>
<name name-style="western"><surname>King</surname><given-names>A. J</given-names></name>
</person-group>             <year>2007</year>             <article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>2172</fpage>             <lpage>2189</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Lakatos1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Chen</surname><given-names>C. M</given-names></name>
<name name-style="western"><surname>O'Connell</surname><given-names>M. N</given-names></name>
<name name-style="western"><surname>Mills</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
</person-group>             <year>2007</year>             <article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex.</article-title>             <source>Neuron</source>             <volume>53</volume>             <fpage>279</fpage>             <lpage>292</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Kayser1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Petkov</surname><given-names>C. I</given-names></name>
<name name-style="western"><surname>Augath</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
</person-group>             <year>2005</year>             <article-title>Integration of touch and sound in auditory cortex.</article-title>             <source>Neuron</source>             <volume>48</volume>             <fpage>373</fpage>             <lpage>384</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Kayser2"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Petkov</surname><given-names>C. I</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
</person-group>             <year>2008</year>             <article-title>Visual modulation of neurons in auditory cortex.</article-title>             <source>Cereb Cortex</source>             <volume>18</volume>             <fpage>1560</fpage>             <lpage>1574</lpage>          </element-citation></ref>
<ref id="pbio.1000445-FuhrmannAlpert1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fuhrmann Alpert</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Hein</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Tsai</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Naumer</surname><given-names>M. J</given-names></name>
<name name-style="western"><surname>Knight</surname><given-names>R. T</given-names></name>
</person-group>             <year>2008</year>             <article-title>Temporal characteristics of audiovisual information processing.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>5344</fpage>             <lpage>5349</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Morrell1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Morrell</surname><given-names>F</given-names></name>
</person-group>             <year>1972</year>             <article-title>Visual system's view of acoustic space.</article-title>             <source>Nature</source>             <volume>238</volume>             <fpage>44</fpage>             <lpage>46</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Macaluso2"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Macaluso</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Frith</surname><given-names>C. D</given-names></name>
<name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Modulation of human visual cortex by crossmodal spatial attention.</article-title>             <source>Science</source>             <volume>289</volume>             <fpage>1206</fpage>             <lpage>1208</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Falchier1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Falchier</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Clavagnier</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Barone</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Kennedy</surname><given-names>H</given-names></name>
</person-group>             <year>2002</year>             <article-title>Anatomical evidence of multimodal integration in primate striate cortex.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>5749</fpage>             <lpage>5759</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Rockland1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rockland</surname><given-names>K. S</given-names></name>
<name name-style="western"><surname>Ojima</surname><given-names>H</given-names></name>
</person-group>             <year>2003</year>             <article-title>Multisensory convergence in calcarine visual areas in macaque monkey.</article-title>             <source>Int J Psychophysiol</source>             <volume>50</volume>             <fpage>19</fpage>             <lpage>26</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Senkowski1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Senkowski</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Schneider</surname><given-names>T. R</given-names></name>
<name name-style="western"><surname>Foxe</surname><given-names>J. J</given-names></name>
<name name-style="western"><surname>Engel</surname><given-names>A. K</given-names></name>
</person-group>             <year>2008</year>             <article-title>Crossmodal binding through neural coherence: implications for multisensory processing.</article-title>             <source>Trends Neurosci</source>             <volume>31</volume>             <fpage>401</fpage>             <lpage>409</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Arnal1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arnal</surname><given-names>L. H</given-names></name>
<name name-style="western"><surname>Morillon</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Kell</surname><given-names>C. A</given-names></name>
<name name-style="western"><surname>Giraud</surname><given-names>A. L</given-names></name>
</person-group>             <year>2009</year>             <article-title>Dural neural routing of visual facilitation in speech processing.</article-title>             <source>Journal of Neuroscience</source>             <volume>29</volume>             <fpage>13445</fpage>             <lpage>13453</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Singer1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Gray</surname><given-names>C. M</given-names></name>
</person-group>             <year>1995</year>             <article-title>Visual feature integration and the temporal correlation hypothesis.</article-title>             <source>Annu Rev Neurosci</source>             <volume>18</volume>             <fpage>555</fpage>             <lpage>586</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Engel1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Engel</surname><given-names>A. K</given-names></name>
<name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>
</person-group>             <year>2001</year>             <article-title>Dynamic predictions: oscillations and synchrony in top-down processing.</article-title>             <source>Nat Rev Neurosci</source>             <volume>2</volume>             <fpage>704</fpage>             <lpage>716</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Maier1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Maier</surname><given-names>J. X</given-names></name>
<name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Ghazanfar</surname><given-names>A. A</given-names></name>
</person-group>             <year>2008</year>             <article-title>Integration of bimodal looming signals through neuronal coherence in the temporal lobe.</article-title>             <source>Curr Biol</source>             <volume>18</volume>             <fpage>963</fpage>             <lpage>968</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Desimone1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name>
</person-group>             <year>1995</year>             <article-title>Neural mechanisms of selective visual attention.</article-title>             <source>Annu Rev Neurosci</source>             <volume>18</volume>             <fpage>193</fpage>             <lpage>222</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fries1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>
</person-group>             <year>2005</year>             <article-title>A mechanism for cognitive dynamics: neuronal communication through neuronal coherence.</article-title>             <source>Trends Cogn Sci</source>             <volume>9</volume>             <fpage>474</fpage>             <lpage>480</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fries2"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Neuronal gamma-band synchronization as a fundamental process in cortical computation.</article-title>             <source>Annu Rev Neurosci</source>             <volume>32</volume>             <fpage>209</fpage>             <lpage>224</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Jensen1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jensen</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Kaiser</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Lachaux</surname><given-names>J. P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Human gamma-frequency oscillations associated with attention and memory.</article-title>             <source>Trends Neurosci</source>             <volume>30</volume>             <fpage>317</fpage>             <lpage>324</lpage>          </element-citation></ref>
<ref id="pbio.1000445-vonStein1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>von Stein</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Rappelsberger</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Sarnthein</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Petsche</surname><given-names>H</given-names></name>
</person-group>             <year>1999</year>             <article-title>Synchronization between temporal and parietal cortex during multimodal object processing in man.</article-title>             <fpage>137</fpage>             <lpage>150</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Sakowitz1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sakowitz</surname><given-names>O. W</given-names></name>
<name name-style="western"><surname>Quian Quiroga</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Schurmann</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Basar</surname><given-names>E</given-names></name>
</person-group>             <year>2005</year>             <article-title>Spatio-temporal frequency characteristics of intersensory components in audiovisually evoked potentials.</article-title>             <source>Brain Res Cogn Brain Res</source>             <volume>23</volume>             <fpage>316</fpage>             <lpage>326</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Senkowski2"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Senkowski</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Molholm</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Gomez-Ramirez</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Foxe</surname><given-names>J. J</given-names></name>
</person-group>             <year>2006</year>             <article-title>Oscillatory beta activity predicts response speed during a multisensory audiovisual reaction time task: a high-density electrical mapping study.</article-title>             <source>Cereb Cortex</source>             <volume>16</volume>             <fpage>1556</fpage>             <lpage>1565</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Senkowski3"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Senkowski</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Talsma</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Grigutsch</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Herrmann</surname><given-names>C. S</given-names></name>
<name name-style="western"><surname>Woldorff</surname><given-names>M. G</given-names></name>
</person-group>             <year>2007</year>             <article-title>Good times for multisensory integration: effects of the precision of temporal synchrony as revealed by gamma-band oscillations.</article-title>             <source>Neuropsychologia</source>             <volume>45</volume>             <fpage>561</fpage>             <lpage>571</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Giard1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Giard</surname><given-names>M. H</given-names></name>
<name name-style="western"><surname>Peronnet</surname><given-names>F</given-names></name>
</person-group>             <year>1999</year>             <article-title>Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study.</article-title>             <source>J Cogn Neurosci</source>             <volume>11</volume>             <fpage>473</fpage>             <lpage>490</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Mishra1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mishra</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Martinez</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T. J</given-names></name>
<name name-style="western"><surname>Hillyard</surname><given-names>S. A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Early cross-modal interactions in auditory and visual cortex underlie a sound-induced visual illusion.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>4120</fpage>             <lpage>4131</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Driver2"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Spence</surname><given-names>C</given-names></name>
</person-group>             <year>2000</year>             <article-title>Multisensory perception: beyond modularity and convergence.</article-title>             <source>Curr Biol</source>             <volume>10</volume>             <fpage>R731</fpage>             <lpage>R735</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Schroeder2"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
<name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Kajikawa</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Partan</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
</person-group>             <year>2008</year>             <article-title>Neuronal oscillations and visual amplification of speech.</article-title>             <source>Trends Cogn Sci</source>             <volume>12</volume>             <fpage>106</fpage>             <lpage>113</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Kayser3"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
<name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>
</person-group>             <year>2010</year>             <article-title>Visual enhancement of the information representation in auditory cortex.</article-title>             <source>Curr Biol</source>             <volume>20</volume>             <fpage>19</fpage>             <lpage>24</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Luo1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Luo</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2007</year>             <article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex.</article-title>             <source>Neuron</source>             <volume>54</volume>             <fpage>1001</fpage>             <lpage>1010</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Simoncelli1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simoncelli</surname><given-names>E. P</given-names></name>
<name name-style="western"><surname>Olshausen</surname><given-names>B. A</given-names></name>
</person-group>             <year>2001</year>             <article-title>Natural image statistics and neural representation.</article-title>             <source>Annu Rev Neurosci</source>             <volume>24</volume>             <fpage>1193</fpage>             <lpage>1216</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Karklin1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Karklin</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Lewicki</surname><given-names>M. S</given-names></name>
</person-group>             <year>2009</year>             <article-title>Emergence of complex cell properties by learning to generalize in natural scenes.</article-title>             <source>Nature</source>             <volume>457</volume>             <fpage>83</fpage>             <lpage>86</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Logothetis1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
<name name-style="western"><surname>Pauls</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Augath</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Trinath</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Oeltermann</surname><given-names>A</given-names></name>
</person-group>             <year>2001</year>             <article-title>Neurophysiological investigation of the basis of the fMRI signal.</article-title>             <source>Nature</source>             <volume>412</volume>             <fpage>150</fpage>             <lpage>157</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Kayser4"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Montemurro</surname><given-names>M. A</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
<name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns.</article-title>             <source>Neuron</source>             <volume>61</volume>             <fpage>597</fpage>             <lpage>608</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Montemurro1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montemurro</surname><given-names>M. A</given-names></name>
<name name-style="western"><surname>Rasch</surname><given-names>M. J</given-names></name>
<name name-style="western"><surname>Murayama</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
<name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>
</person-group>             <year>2008</year>             <article-title>Phase-of-firing coding of natural visual stimuli in primary visual cortex.</article-title>             <source>Curr Biol</source>             <volume>18</volume>             <fpage>375</fpage>             <lpage>380</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Salinas1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Salinas</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T. J</given-names></name>
</person-group>             <year>2001</year>             <article-title>Correlated neuronal activity and the flow of neural information.</article-title>             <source>Nat Rev Neurosci</source>             <volume>2</volume>             <fpage>539</fpage>             <lpage>550</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Makeig1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Makeig</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Westerfield</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Jung</surname><given-names>T. P</given-names></name>
<name name-style="western"><surname>Enghoff</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Townsend</surname><given-names>J</given-names></name>
<etal/></person-group>             <year>2002</year>             <article-title>Dynamic brain sources of visual evoked responses.</article-title>             <source>Science</source>             <volume>295</volume>             <fpage>690</fpage>             <lpage>694</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fiser1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Chiu</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Weliky</surname><given-names>M</given-names></name>
</person-group>             <year>2004</year>             <article-title>Small modulation of ongoing cortical dynamics by sensory input during natural vision.</article-title>             <source>Nature</source>             <volume>431</volume>             <fpage>573</fpage>             <lpage>578</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Schaefer1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schaefer</surname><given-names>A. T</given-names></name>
<name name-style="western"><surname>Angelo</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Spors</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Margrie</surname><given-names>T. W</given-names></name>
</person-group>             <year>2006</year>             <article-title>Neuronal oscillations enhance stimulus discrimination by ensuring action potential precision.</article-title>             <source>PLoS Biol</source>             <volume>4</volume>             <fpage>e163</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0040163" xlink:type="simple">10.1371/journal.pbio.0040163</ext-link></comment>          </element-citation></ref>
<ref id="pbio.1000445-Lakatos2"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Karmos</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Mehta</surname><given-names>A. D</given-names></name>
<name name-style="western"><surname>Ulbert</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>C. E</given-names></name>
</person-group>             <year>2008</year>             <article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection.</article-title>             <source>Science</source>             <volume>320</volume>             <fpage>110</fpage>             <lpage>113</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Lakatos3"><label>57</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lakatos</surname><given-names>P</given-names></name>
<name name-style="western"><surname>O'Connell</surname><given-names>M. N</given-names></name>
<name name-style="western"><surname>Barczak</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Mills</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Javitt</surname><given-names>D. C</given-names></name>
<etal/></person-group>             <year>2009</year>             <article-title>The leading sense: supramodal control of neurophysiological context by attention.</article-title>             <source>Neuron</source>             <volume>64</volume>             <fpage>419</fpage>             <lpage>430</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Theunissen1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Theunissen</surname><given-names>F. E</given-names></name>
<name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Doupe</surname><given-names>A. J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>2315</fpage>             <lpage>2331</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Smith1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Smith</surname><given-names>E. C</given-names></name>
<name name-style="western"><surname>Lewicki</surname><given-names>M. S</given-names></name>
</person-group>             <year>2006</year>             <article-title>Efficient auditory coding.</article-title>             <source>Nature</source>             <volume>439</volume>             <fpage>978</fpage>             <lpage>982</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Butts1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Butts</surname><given-names>D. A</given-names></name>
<name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Yeh</surname><given-names>C. I</given-names></name>
<name name-style="western"><surname>Lesica</surname><given-names>N. A</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>Temporal precision in the neural code and the timescales of natural vision.</article-title>             <source>Nature</source>             <volume>449</volume>             <fpage>92</fpage>             <lpage>95</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Shannon1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Shannon</surname><given-names>R. V</given-names></name>
<name name-style="western"><surname>Zeng</surname><given-names>F. G</given-names></name>
<name name-style="western"><surname>Kamath</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Wygonski</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ekelid</surname><given-names>M</given-names></name>
</person-group>             <year>1995</year>             <article-title>Speech recognition with primarily temporal cues.</article-title>             <source>Science</source>             <volume>270</volume>             <fpage>303</fpage>             <lpage>304</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Poeppel1"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2003</year>             <article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time.’</article-title>             <source>Speech Communication</source>             <volume>41</volume>             <fpage>245</fpage>             <lpage>255</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Hickok1"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
</person-group>             <year>2007</year>             <article-title>The cortical organization of speech processing.</article-title>             <source>Nat Rev Neurosci</source>             <volume>8</volume>             <fpage>393</fpage>             <lpage>402</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Giraud1"><label>64</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Giraud</surname><given-names>A. L</given-names></name>
<name name-style="western"><surname>Kleinschmidt</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Lund</surname><given-names>T. E</given-names></name>
<name name-style="western"><surname>Frackowiak</surname><given-names>R. S</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>Endogenous cortical rhythms determine cerebral specialization for speech perception and production.</article-title>             <source>Neuron</source>             <volume>56</volume>             <fpage>1127</fpage>             <lpage>1134</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Chandrasekaran1"><label>65</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chandrasekaran</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Trubanova</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Stillittano</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Ghazanfar</surname><given-names>A. A</given-names></name>
</person-group>             <year>2009</year>             <article-title>The natural statistics of audiovisual speech.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000436</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000436" xlink:type="simple">10.1371/journal.pcbi.1000436</ext-link></comment>          </element-citation></ref>
<ref id="pbio.1000445-Canolty1"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Canolty</surname><given-names>R. T</given-names></name>
<name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Dalal</surname><given-names>S. S</given-names></name>
<name name-style="western"><surname>Soltani</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Nagarajan</surname><given-names>S. S</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>High gamma power is phase-locked to theta oscillations in human neocortex.</article-title>             <source>Science</source>             <volume>313</volume>             <fpage>1626</fpage>             <lpage>1628</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Ghazanfar3"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ghazanfar</surname><given-names>A. A</given-names></name>
<name name-style="western"><surname>Logothetis</surname><given-names>N. K</given-names></name>
</person-group>             <year>2003</year>             <article-title>Neuroperception: facial expressions linked to monkey calls.</article-title>             <source>Nature</source>             <volume>423</volume>             <fpage>937</fpage>             <lpage>938</lpage>          </element-citation></ref>
<ref id="pbio.1000445-Fairhall1"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fairhall</surname><given-names>S. L</given-names></name>
<name name-style="western"><surname>Macaluso</surname><given-names>E</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spatial attention can modulate audiovisual integration at multiple cortical and subcortical sites.</article-title>             <source>Eur J Neurosci</source>             <volume>29</volume>             <fpage>1247</fpage>             <lpage>1257</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>