<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-01586</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003143</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Sensory systems</subject><subject>Single neuron function</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subject>Visual system</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs</article-title>
<alt-title alt-title-type="running-head">A Nonlinear Neuronal Model of Sensory Processing</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>McFarland</surname><given-names>James M.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Cui</surname><given-names>Yuwei</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Butts</surname><given-names>Daniel A.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Department of Biology and Program in Neuroscience and Cognitive Science, University of Maryland, College Park, Maryland, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Bethge</surname><given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jmmcfarl@umd.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Analyzed the data: JMM YC DAB. Wrote the paper: JMM DAB. Developed the model: JMM YC DAB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>7</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>18</day><month>7</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>7</issue>
<elocation-id>e1003143</elocation-id>
<history>
<date date-type="received"><day>3</day><month>10</month><year>2012</year></date>
<date date-type="accepted"><day>1</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>McFarland et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>The computation represented by a sensory neuron's response to stimuli is constructed from an array of physiological processes both belonging to that neuron and inherited from its inputs. Although many of these physiological processes are known to be nonlinear, linear approximations are commonly used to describe the stimulus selectivity of sensory neurons (i.e., linear receptive fields). Here we present an approach for modeling sensory processing, termed the Nonlinear Input Model (NIM), which is based on the hypothesis that the dominant nonlinearities imposed by physiological mechanisms arise from rectification of a neuron's inputs. Incorporating such ‘upstream nonlinearities’ within the standard linear-nonlinear (LN) cascade modeling structure implicitly allows for the identification of multiple stimulus features driving a neuron's response, which become directly interpretable as either excitatory or inhibitory. Because its form is analogous to an integrate-and-fire neuron receiving excitatory and inhibitory inputs, model fitting can be guided by prior knowledge about the inputs to a given neuron, and elements of the resulting model can often result in specific physiological predictions. Furthermore, by providing an explicit probabilistic model with a relatively simple nonlinear structure, its parameters can be efficiently optimized and appropriately regularized. Parameter estimation is robust and efficient even with large numbers of model components and in the context of high-dimensional stimuli with complex statistical structure (e.g. natural stimuli). We describe detailed methods for estimating the model parameters, and illustrate the advantages of the NIM using a range of example sensory neurons in the visual and auditory systems. We thus present a modeling framework that can capture a broad range of nonlinear response functions while providing physiologically interpretable descriptions of neural computation.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Sensory neurons are capable of representing a wide array of computations on sensory stimuli. Such complex computations are thought to arise in large part from the accumulation of relatively simple nonlinear operations across the sensory processing hierarchies. However, models of sensory processing typically rely on mathematical approximations of the overall relationship between stimulus and response, such as linear or quadratic expansions, which can overlook critical elements of sensory computation and miss opportunities to reveal how the underlying inputs contribute to a neuron's response. Here we present a physiologically inspired nonlinear modeling framework, the ‘Nonlinear Input Model’ (NIM), which instead assumes that neuronal computation can be approximated as a sum of excitatory and suppressive ‘neuronal inputs’. We show that this structure is successful at explaining neuronal responses in a variety of sensory areas. Furthermore, model fitting can be guided by prior knowledge about the inputs to a given neuron, and its results can often suggest specific physiological predictions. We illustrate the advantages of the proposed model and demonstrate specific parameter estimation procedures using a range of example sensory neurons in both the visual and auditory systems.</p>
</abstract>
<funding-group><funding-statement>All authors were supported by NSF IIS-0904430 (<ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov" xlink:type="simple">www.nsf.gov</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="18"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Sensory perception in the visual and auditory systems involves the detection of elemental features such as luminance and sound intensity, and their subsequent processing into more abstract representations such as “objects” that comprise our perception. The neuronal computations performed during such sensory processing must be nonlinear in order to generate more complex stimulus selectivity, such as needed to encode the conjunction of multiple sensory features <xref ref-type="bibr" rid="pcbi.1003143-Riesenhuber1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Mineault1">[3]</xref> as well as to develop invariance to irrelevant aspects of the raw sensory input <xref ref-type="bibr" rid="pcbi.1003143-DiCarlo1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Rust1">[5]</xref>. While these computations can appear inscrutably complex, they are necessarily constructed from the underlying neural circuitry, which exhibits several well-known and relatively straightforward nonlinear properties.</p>
<p>Nevertheless, characterizations of sensory neurons still typically rely on the assumption of linear stimulus processing, which is often implicit in standard approaches such as spike-triggered averaging and – more recently – generalized linear models (GLMs) <xref ref-type="bibr" rid="pcbi.1003143-Brillinger1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>. While such descriptions can often provide good predictions of the neuronal response <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Shapley1">[11]</xref>, they necessarily leave out the nonlinear elements of neuronal processing that likely play a major role in building the sensory percept.</p>
<p>Unfortunately, the space of possible nonlinear models is not bounded. While one might be inclined to incorporate details of the system and circuitry in question, more complicated models require more data for parameter estimation, and often involve poorly behaved or intractable optimization problems. As a result, practical nonlinear modeling approaches must make assumptions that limit the space of functions considered by restricting to a defined set of nonlinear interactions.</p>
<p>Several different approaches have been developed in this regard. The most common is to identify a low dimensional “feature space” to which the neuron is sensitive, with the assumption that its firing rate depends on a nonlinear function applied only to these stimulus features. Prominent examples of this approach include spike-triggered covariance (STC) analysis <xref ref-type="bibr" rid="pcbi.1003143-deRuytervanSteveninck1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Schwartz1">[13]</xref>, which uses the covariance of the stimuli that elicit spikes, and information-theoretic approaches such as maximally informative dimensions (MID) analysis <xref ref-type="bibr" rid="pcbi.1003143-Sharpee1">[14]</xref> and iSTAC <xref ref-type="bibr" rid="pcbi.1003143-Pillow1">[15]</xref>. With the subspace determined, other methods can be used to estimate a nonlinear mapping between the projection of the stimulus onto this low dimensional feature space and the firing rate <xref ref-type="bibr" rid="pcbi.1003143-Schwartz1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Truccolo2">[19]</xref>.</p>
<p>A second general approach is to assume the form of nonlinearities present, most commonly based on a second-order approximation of the nonlinear stimulus-response relationship, as with the Wiener-Volterra expansion <xref ref-type="bibr" rid="pcbi.1003143-Marmarelis1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Marmarelis2">[24]</xref>, and more recent versions cast in a probabilistic context <xref ref-type="bibr" rid="pcbi.1003143-Ahrens2">[25]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Rajan1">[28]</xref>. This category might also encompass neural network approaches, which characterize the stimulus-response relationship in terms of a set of fixed nonlinear basis functions, using either generic network elements <xref ref-type="bibr" rid="pcbi.1003143-Lau1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Prenger1">[30]</xref> or more specific nonlinear models of upstream sensory processing <xref ref-type="bibr" rid="pcbi.1003143-Nishimoto1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Mineault2">[32]</xref>.</p>
<p>A final commonly used approach assumes that relevant nonlinearities can be captured by directly augmenting the linear model to account for specific response properties, such as the addition of refractoriness to account for neural precision <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Berry1">[33]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Pillow2">[35]</xref>, feedback terms that account for adaptation to contrast <xref ref-type="bibr" rid="pcbi.1003143-Shapley2">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Mante1">[38]</xref>, and other nonlinearities to capture response properties such as sensitivity to stimulus intensity and local context <xref ref-type="bibr" rid="pcbi.1003143-Ahrens2">[25]</xref>.</p>
<p>Here, we present a probabilistic modeling framework inspired by all of these approaches, the ‘Nonlinear Input Model’ (NIM), which limits the space of nonlinear functions by assuming that nonlinearities in sensory processing are dominated by spike generation, resulting in both rectification of the inputs to the neuron, as well as rectification of the neuron's output. By assuming a neuron's inputs are rectified, the NIM implicitly describes neuronal processing as a sum over excitatory and inhibitory inputs, which is increasingly being seen as an important factor in sensory processing <xref ref-type="bibr" rid="pcbi.1003143-Wehr1">[39]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Dorrn1">[43]</xref>. The NIM expands directly on the GLM framework, and is able to utilize recent advances in the statistical modeling of neural responses <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Ahrens1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Pillow3">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, including the ability to model spike-refractoriness <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref> and multi-neuron correlations <xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1003143-Pillow3">[44]</xref>.</p>
<p>As we show here, this results in a parsimonious nonlinear description of a range of neurons in both the visual and auditory systems, and has several advantages over previous approaches. Because of its relatively simple model structure, parameter estimation is well-behaved and makes efficient use of the data, even when the number of relevant inputs is large and/or the stimulus is high-dimensional. Importantly, because its form is based on an integrate-and-fire neuron, model selection and parameter estimation can be guided by specific knowledge about the inputs to a given neuron, and the elements of the resulting model can often be related to specific physiological predictions. The NIM thus provides a powerful and general approach for nonlinear modeling that complements other methods that rely on more abstract formulations of nonlinear computation.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Nonlinear combination of multiple inputs: ON-OFF retinal ganglion cells</title>
<p>Perhaps the greatest success of linear models is in the retina, where it has been used primarily to describe the spike responses of retinal ganglion cells (RGCs) <xref ref-type="bibr" rid="pcbi.1003143-Carandini1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Shapley1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Chichilnisky1">[16]</xref>. For a given RGC, estimating the components of the linear model typically involves measuring its spiking response to a noise stimulus, and then computing the average stimulus that preceded its spikes: the spike-triggered average (STA). The STA linear filter can produce very good response predictions for typical RGCs under stationary stimulus conditions, but clearly fails for ON-OFF cells (commonly found in rodents), which respond to both increases and decreases of light intensity <xref ref-type="bibr" rid="pcbi.1003143-Carcieri1">[46]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Cantrell1">[49]</xref>. This failure for ON-OFF cells occurs simply because the STA identifies only a single stimulus dimension, and averages out the opposing stimulus features that evoke ON and OFF responses.</p>
<p>To explore this situation, we construct a basic model of an ON-OFF RGC, which receives separate ON and OFF inputs (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1A</xref>). If these two inputs were to combine linearly, their effect would be identical to that of a single input generated by the sum of the two stimulus filters, i.e., (<bold>s</bold>·<bold>k</bold><sub>ON</sub>)+(<bold>s</bold>·<bold>k</bold><sub>OFF</sub>) = <bold>s</bold>·(<bold>k</bold><sub>ON</sub>+<bold>k</bold><sub>OFF</sub>) = <bold>s</bold>·<bold>k</bold><sub>SUM</sub>. Here the stimulus <bold>s</bold> at a particular time is represented as a vector (which in general includes time-lagged elements to account for stimulus history) such that the operation of a linear filter <bold>k</bold> is given by a dot product. Because of the averaging implicit in linear processing, a nonlinear transformation must be applied to each input in order to enable the model ON-OFF neuron to respond to both types of stimuli: i.e., <italic>f</italic>(<bold>s</bold>·<bold>k</bold><sub>ON</sub>)+<italic>f</italic>(<bold>s</bold>·<bold>k</bold><sub>OFF</sub>). These <italic>f</italic>(.) are taken to be rectifying functions (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1A</xref>), as seen experimentally <xref ref-type="bibr" rid="pcbi.1003143-Demb1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Kim1">[51]</xref>, and as modeled in <xref ref-type="bibr" rid="pcbi.1003143-Geffen1">[52]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Schwartz2">[54]</xref>. As a result, the response of the neuron to increases or decreases of luminance is dominated by the ON or OFF pathways respectively (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1B</xref>), producing a response that is selective to both ON and OFF stimulus dimensions. As expected, the STA (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1C</xref>) for this neuron does not match either the ON or OFF stimulus filters, but rather reflects their average.</p>
<fig id="pcbi-1003143-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g001</object-id><label>Figure 1</label><caption>
<title>ON-OFF RGC simulation.</title>
<p><bold>A</bold>) Schematic showing the ON (top, red) and OFF (bottom, blue) inputs to the simulated ON-OFF RGC. The temporal filters (left) process the stimulus, and the upstream nonlinearities (black, middle) are then applied to the filter outputs. The sum of the two inputs is then passed through the spiking nonlinearity (black, right). The distributions of the stimulus filtered by the ON and OFF pathways, as well as the distribution of their summed input to the simulated neuron are shown as gray shaded regions. <bold>B</bold>) A simulation showing how the response to a 15 Hz (Gaussian) white noise stimulus is constructed. The stimulus (black) is filtered by the ON and OFF temporal kernels (dashed red and blue), and then transformed by the upstream nonlinearities (solid red and blue). The resulting instantaneous firing rate (green) is given by the sum of these inputs passed through the spiking nonlinearity. <bold>C</bold>) The STA (black) resembles the average of separate ON (red) and OFF (blue) filters of the generative model. <bold>D</bold>) Similar to panel C, the first STC filter (gray) resembles a mixture of the ON and OFF filters. <bold>E</bold>) Stimuli eliciting spikes (black dots) are projected onto the two-dimensional subspace spanned by the STA and first STC filter, shown in units of z-score. The distributions of stimuli corresponding to spikes (dashed lines on top and left) are compared to the marginal stimulus distributions (solid), demonstrating a systematic bias along the STA (horizontal) axis, and an increased variance along the STC (vertical) axis. The true ON and OFF filters (red and blue) are also contained in the STA/STC subspace, as indicated by the red and blue lines lying on the unit circle (green). The inner green circle has a radius of one standard deviation. <bold>F</bold>) The neuron's firing rate as a function of the stimulus projected into the 2-D STA/STC subspace (shaded color depicts firing rate: increasing from blue to red).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g001" position="float" xlink:type="simple"/></fig>
<p>Thus, this is a clear example where nonlinear characterization is necessary to capture the RGC's stimulus selectivity. One such approach that has been applied to ON-OFF cells is spike-triggered covariance (STC) analysis <xref ref-type="bibr" rid="pcbi.1003143-Cantrell1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Fairhall1">[55]</xref>, which identifies stimulus dimensions along which the variance of the spike-triggered ensemble is either increased or decreased relative to the stimulus distribution <xref ref-type="bibr" rid="pcbi.1003143-deRuytervanSteveninck1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Schwartz1">[13]</xref>. For the example neuron in <xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1</xref>, STC analysis identifies a stimulus dimension along which the variance of the spike-triggered ensemble is expanded (<xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1D, E</xref>). While neither the STA nor STC filters correspond to the true ON or OFF filters, together they define a stimulus subspace that contains the true filters (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1E</xref>).</p>
<p>Given the dimensionality reduction achieved in determining the STC subspace (or with other subspace identification methods), it is possible in principle to completely characterize the neural response function, i.e., <italic>r</italic> = <italic>F</italic>[<bold>k</bold><sub>1</sub>·<bold>s</bold>, <bold>k</bold><sub>2</sub>·<bold>s</bold>]. In two-dimensions, such as in this example, this nonlinear mapping from the subspace to a firing rate can be estimated non-parametrically <xref ref-type="bibr" rid="pcbi.1003143-Park1">[18]</xref>,<xref ref-type="bibr" rid="pcbi.1003143-Rad1">[56]</xref> given enough data, and potentially approximated in higher dimensions <xref ref-type="bibr" rid="pcbi.1003143-Schwartz1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Pillow1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Park1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Truccolo2">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>.</p>
<p>However, even if accurate estimation of this nonlinear mapping were possible, such functions are difficult to interpret, even when arising from the conjunction of simpler components. For example, in our simulated ON-OFF RGC, neither the STA/STC filters themselves nor the measured nonlinear mapping make it clear that the response is generated from separate inputs with relatively straightforward nonlinearities.</p>
<p>This example thus motivates the modeling framework that we present here, the Nonlinear Input Model (NIM), which describes a neuron's stimulus processing as a sum of nonlinear inputs, following the structure of the generative model shown in <xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1</xref>. Below, we first present procedures for estimating the parameters of the NIM before demonstrating its ability to recover the inputs to the ON-OFF RGC, as well as its application to a range of other simulated and measured data from both visual and auditory brain areas.</p>
</sec><sec id="s2b">
<title>Parameter estimation for the Nonlinear Input Model (NIM)</title>
<p>The computational challenges associated with parameter estimation are a significant barrier to the successful development and application of nonlinear models of sensory processing. In the standard linear-nonlinear (LN) model, the neuron's response is modeled by an initial stage of linear stimulus filtering, followed by a static nonlinear function (“spiking nonlinearity”) that maps the output to a firing rate (<xref ref-type="fig" rid="pcbi-1003143-g002">Fig. 2A</xref>). The more recent adaptation of probabilistic models based on spike train likelihoods, such as in the Generalized Linear Model (GLM) <xref ref-type="bibr" rid="pcbi.1003143-Brillinger1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>, allows for integration of other aspects of neuronal processing into the linear stimulus-processing framework, and can be used to model nonlinear stimulus processing through predefined nonlinear transformations <xref ref-type="bibr" rid="pcbi.1003143-Nishimoto1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Mineault2">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>. Importantly, this approach also provides a foundation for parameter estimation for the NIM.</p>
<fig id="pcbi-1003143-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g002</object-id><label>Figure 2</label><caption>
<title>Schematic of LN and NIM structures.</title>
<p>A) Schematic diagram of an LN model, with multiple filters (k<sub>1</sub>, k<sub>2</sub>, …) that define the linear stimulus subspace. The outputs of these linear filters (<italic>g</italic><sub>1</sub>, <italic>g</italic><sub>2</sub>, …) are then transformed into a firing rate prediction <italic>r</italic>(<italic>t</italic>) by the static nonlinear function <italic>F</italic>[<italic>g</italic><sub>1</sub>,<italic>g</italic><sub>2</sub>,…], depicted at right for a two-dimensional subspace. Note that while the general LN model thus allows for a nonlinear dependence on multiple stimulus dimensions, estimation of the function <italic>F</italic>[.] is typically only feasible for low (one- or two-) dimensional subspaces. B) Schematic illustration of a generic neuron that receives input from a set of ‘upstream’ neurons that are themselves driven by the stimulus s. Each of the upstream neurons provides input to the model neuron that is generally rectified due to spike generation (inset at left), and thus is either excitatory or inhibitory. The model neuron then integrates its inputs and produces a spiking output. C) Block diagram illustrating the structure of the NIM, based on (B). The set of inputs are represented as (one-dimensional) LN models, with a corresponding stimulus filter k<sub>i</sub>, and “upstream nonlinearity” <italic>f<sub>i</sub></italic>(.). These inputs are then linearly combined, with weights <italic>w</italic><sub>i</sub>, and fed into the spiking nonlinearity <italic>F</italic>[.], resulting in the predicted firing rate <italic>r</italic>(<italic>t</italic>). The NIM thus has a ‘second-order LN’ structure (or LNLN), with the neuron's own nonlinear processing shaped by the LN nature of its inputs.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g002" position="float" xlink:type="simple"/></fig>
<p>A principal motivation for the NIM structure is that if the neuronal output at one level is well described by an LN model, downstream neurons will receive inputs that are already rectified (or otherwise nonlinearly transformed). Thus, we use LN models to represent the inputs to the neuron in question, and the neuron's response is given by a summation over these LN inputs followed by the neuron's own spiking nonlinearity (<xref ref-type="fig" rid="pcbi-1003143-g002">Fig. 2B</xref>). Importantly, this allows us to account for the rectification of a neuron's inputs imposed by the spike-generation process. The NIM can thus be viewed as a ‘second-order’ generalization of the LN model, or an LNLN cascade <xref ref-type="bibr" rid="pcbi.1003143-Narendra1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Hunter1">[60]</xref>. Previous work from our lab <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref> cast this model structure in a probabilistic form, and suggested several statistical innovations in order to fit the models using neural data <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-SchinkelBielefeld1">[61]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>. Here, we present a general and detailed framework for NIM parameter estimation that greatly extends the applicability of the model. This model structure has also been suggested for applications outside of neuroscience in the form of projection pursuit regression <xref ref-type="bibr" rid="pcbi.1003143-Friedman1">[63]</xref>, including generalizations to response variables with distributions from the exponential family <xref ref-type="bibr" rid="pcbi.1003143-Lingjrde1">[64]</xref>.</p>
<p>The processing of the NIM is comprised of three stages (<xref ref-type="fig" rid="pcbi-1003143-g002">Fig. 2C</xref>): (a) the filters <bold>k</bold><italic><sub>i</sub></italic> that define the stimulus selectivity of each input; (b) the static ‘upstream’ nonlinearities <italic>f<sub>i</sub></italic>(.) and corresponding linear weights <italic>w<sub>i</sub></italic> which determine how each input contributes to the overall response; and (c) the spiking nonlinearity <italic>F</italic>[.] applied to the linear sum over the neuron's inputs. The predicted firing rate <italic>r</italic>(<italic>t</italic>) is then given as:<disp-formula id="pcbi.1003143.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <bold>s</bold>(<italic>t</italic>) is the (vector-valued) stimulus at time <italic>t</italic>, <bold>x</bold>(<italic>t</italic>) represents any additional covariates (such as the neuron's own spike history), and <bold>h</bold> is a linear filter operating on <bold>x</bold>. Note that <xref ref-type="disp-formula" rid="pcbi.1003143.e001">equation (1)</xref> reduces to a GLM when the <italic>f<sub>i</sub></italic>(.) are linear functions. The <italic>w<sub>i</sub></italic> can also be extended to include temporal convolution of the subunit contributions to model the time course of post-synaptic responses associated with individual inputs <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, as well as ‘spatial’ convolutions to account for multiple spatially distributed inputs with similar stimulus selectivity <xref ref-type="bibr" rid="pcbi.1003143-Vintch1">[65]</xref>. Since equivalent models can be produced by rescaling the <italic>w<sub>i</sub></italic>, and <italic>f<sub>i</sub></italic>(.) (see <xref ref-type="sec" rid="s3">Methods</xref>), we constrain the subunit weights <italic>w<sub>i</sub></italic> to be either +/−1. Because we generally assume the <italic>f</italic><sub>i</sub>(.) are rectifying functions, the <italic>w<sub>i</sub></italic> thus specify whether each subunit will have an ‘excitatory’ or ‘inhibitory’ influence on the neuron.</p>
<p>Parameter estimation for the NIM is based on maximum likelihood (or maximum a posteriori) methods similar to those used with the GLM <xref ref-type="bibr" rid="pcbi.1003143-Brillinger1">[6]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Truccolo1">[8]</xref>. Assuming that the neuron's spikes are described in discrete time by a conditionally inhomogeneous Poisson count process with rate function <italic>r</italic>(<italic>t</italic>), the log-likelihood (<italic>LL</italic>) of the model parameters given an observed set of spike counts <italic>R</italic><sub>obs</sub>(<italic>t</italic>) is given (up to an overall constant) by:<disp-formula id="pcbi.1003143.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e002" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>To find the set of parameters that maximize the likelihood (<xref ref-type="disp-formula" rid="pcbi.1003143.e002">eq. 2</xref>), we adapt methods that allow for efficient parameter optimization of the GLM <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>. First, we use a parametric spiking nonlinearity given by <italic>F</italic>[<italic>x</italic>] = <italic>α</italic>log[1+exp(<italic>β</italic>(<italic>x</italic>-<italic>θ</italic>))], with scale <italic>α</italic>, shape <italic>β</italic>, and offset <italic>θ</italic>. Other functions can be used, so long as they satisfy conditions specified in <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>. This ensures that the likelihood surface will be concave with respect to linear parameters inside the spiking nonlinearity <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref>, and in practice will be well-behaved for other model parameters (see <xref ref-type="supplementary-material" rid="pcbi.1003143.s001">Fig. S1</xref>; <xref ref-type="sec" rid="s3">Methods</xref>).</p>
<p>Because it is straightforward to estimate the linear term <bold>h</bold>, and the <italic>w<sub>i</sub></italic> are constrained to be +/−1, the upstream nonlinearities <italic>f<sub>i</sub></italic>(.) and the stimulus filters <bold>k</bold><sub>i</sub> are the key components that must be fit in the NIM. While it is typically not feasible to optimize the likelihood with respect to both sets of parameters simultaneously, an efficient strategy is to use block coordinate ascent <xref ref-type="bibr" rid="pcbi.1003143-Bertsekas1">[66]</xref>, alternating between optimizing the <bold>k</bold><italic><sub>i</sub></italic> and <italic>f<sub>i</sub></italic>(.), in each case holding the remaining set of parameters constant (see <xref ref-type="sec" rid="s3">Methods</xref>). ‘Linear’ parameters, such as <bold>h</bold> and <italic>θ</italic>, can be optimized simultaneously during either (or both) optimization stages.</p>
<p>While the set of ‘upstream nonlinearities’ <italic>f<sub>i</sub></italic>(.) can be represented as parametric functions such as rectified-linear or quadratic functions (see <xref ref-type="sec" rid="s3">Methods</xref>), a powerful approach is to represent them as a linear combination of basis functions <italic>φ<sub>j</sub></italic>(.) such as piecewise linear “tent” basis functions, i.e., <italic>f<sub>i</sub></italic>(<italic>g</italic>) = Σ<italic><sub>j</sub> a<sub>ij</sub>φ<sub>j</sub></italic>(<italic>g</italic>) <xref ref-type="bibr" rid="pcbi.1003143-Ahrens1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>. In doing so, estimation of the upstream nonlinearities reduces to estimating linear parameters <italic>a<sub>ij</sub></italic> inside the spiking nonlinearity, with a single global optimum of the likelihood function for a given set of stimulus filters <bold>k</bold><sub>i</sub>.</p>
<p>For a fixed set of upstream nonlinearities, the stimulus filters <bold>k</bold><sub>i</sub> can be similarly optimized, although the resulting likelihood surface will not in general be convex because the <bold>k</bold><sub>i</sub> operate inside the upstream nonlinearities. Nevertheless, we have found that in practice their optimization is well-behaved and that local minima can be avoided with appropriate optimization procedures (<xref ref-type="supplementary-material" rid="pcbi.1003143.s001">Fig. S1</xref>; see <xref ref-type="sec" rid="s3">Methods</xref>). Furthermore, it is straightforward to evaluate the likelihood function and its gradient with respect to the <bold>k</bold><sub>i</sub> analytically (see <xref ref-type="sec" rid="s3">Methods</xref>), allowing for efficient gradient-based optimization.</p>
<p>Thus, optimal parameter estimates for the NIM can be determined efficiently, even for models with large numbers of parameters (see examples below). The time required for filter estimation (typically the most time-consuming step) scales approximately linearly with the experiment duration, the dimensionality of the stimulus, and the number of model subunits (<xref ref-type="supplementary-material" rid="pcbi.1003143.s002">Fig. S2</xref>). This is very favorable compared with alternative nonlinear modeling approaches such as MID <xref ref-type="bibr" rid="pcbi.1003143-Sharpee1">[14]</xref>, which require using simulated annealing and quickly becomes intractable as the number of filters and/or stimulus dimensions is increased.</p>
<p>Furthermore, because the NIM provides an explicit probabilistic model for the neuronal spike response, regularization of the model components can be incorporated without adversely affecting the behavior of the optimization problem <xref ref-type="bibr" rid="pcbi.1003143-Paninski1">[7]</xref> (see <xref ref-type="sec" rid="s3">Methods</xref>). This is particularly important when optimizing high-dimensional spatiotemporal filters and/or models with many inputs, which are both discussed further below. Likewise, as with other probabilistic modeling approaches – but not those relying on spike-triggered measurements <xref ref-type="bibr" rid="pcbi.1003143-Paninski2">[67]</xref> – the model can be optimized using data recorded with natural stimulus ensembles (containing complex correlation structure, and non-Gaussian distributions) without introducing biases into the parameter estimates.</p>
<p>The NIM thus provides a nonlinear modeling framework in which large numbers of parameters can be efficiently estimated using data recorded with arbitrarily complex stimulus ensembles. In addition to this flexibility, the NIM provides model fits that are more directly interpretable due to its physiologically motivated model structure. To illustrate these advantages, below we first apply the NIM to the example ON-OFF RGC from <xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1</xref>, and then demonstrate its wide applicability on recorded and simulated neurons in several different sensory areas.</p>
</sec><sec id="s2c">
<title>Nonlinear models of the ON-OFF retinal ganglion cell example</title>
<p>Returning to the example ON-OFF RGC (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1</xref>), the NIM is a natural choice given that its structure matches that of the simulated neuron. Using the estimation procedures described above, the NIM is able to successfully capture the true stimulus selectivity of its individual inputs (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3A</xref>), including the ‘upstream nonlinearities’ associated with each input, as well as the form of the spiking nonlinearity (see <xref ref-type="sec" rid="s3">Methods</xref>).</p>
<fig id="pcbi-1003143-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g003</object-id><label>Figure 3</label><caption>
<title>Comparison of NIM and quadratic model.</title>
<p><bold>A</bold>) The filters fit by the NIM (green dots) are able to capture the true underlying ON and OFF filters (red and blue), as well as the shape of the upstream nonlinearities (right), which are shown relative to the corresponding distributions of the filtered stimulus (gray shaded). The ranges of the y-axes for different subunits are indicated by the numbers above, for comparison of their relative magnitudes. These ‘subunit weights’ are scaled so that their squared magnitude is one. <bold>B</bold>) The filters fit by the GQM, consisting of two (excitatory) squared filters (magenta and light blue) and a linear filter (green trace), are different than the true filters (red and blue), but are in the same subspace, as demonstrated in (E). <bold>C</bold>) The simulated neuron's response function (shaded color depicts firing rate) and true filters (red and blue) projected into the STC subspace (identical to <xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1F</xref>). <bold>D</bold>) Response function predicted by the NIM. The filters identified by the NIM (dashed green) overlay onto the true filters. <bold>E</bold>) Same as (D) for the GQM, with colored lines corresponding to the filters in (B). <bold>F</bold>) Model performance is plotted for the STC, GQM, and NIM fit with different numbers of filters (indicated by different circle sizes). Log-likelihood (relative to the null model) is shown on the x-axis, and the ‘predictive power’ <xref ref-type="bibr" rid="pcbi.1003143-Sahani2">[99]</xref> is shown on the y-axis; both were evaluated on a simulated cross-validation data set. The NIM (blue) outperforms the GQM (red), both of which outperform a nonlinear model based on the STC filters (black, see <xref ref-type="sec" rid="s3">Methods</xref>). The STC model and GQM achieve maximal performance with 3 filters, since this is sufficient for capturing the best-fit quadratic function in the relevant 2-D stimulus subspace, while the NIM achieves optimal performance with two filters, as expected. <bold>G</bold>) To determine how model performance depends on the stimulus distribution we simulated the same neuron's response to white noise luminance stimuli with Student's <italic>t</italic>-distributions, ranging from Gaussian (i.e., <italic>ν</italic> = ∞, dashed black) to “heavy tailed” (decreasing <italic>ν</italic> from red to blue). <bold>H</bold>) The log-likelihood improvement of the NIM over the GQM increases as a function of the tail thickness (parameterized by 1/<italic>ν</italic>) of the stimulus distribution (which also determines the tail thickness of the filtered stimulus distributions). The GQM is able to provide a very close approximation for large values of <italic>v</italic> (i.e., a more normally-distributed stimulus), but has lower performance compared to the NIM for more heavy-tailed stimuli.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g003" position="float" xlink:type="simple"/></fig>
<p>This example thus illustrates the core motivation behind the NIM of modeling a neuron's stimulus processing in terms of rectified neuronal inputs. While the structure of the simulated RGC neuron in this example may appear to be a convenient choice, its form is consistent with other models of ON-OFF processing <xref ref-type="bibr" rid="pcbi.1003143-Zhang1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gollisch1">[53]</xref>, and with models of RGCs in general <xref ref-type="bibr" rid="pcbi.1003143-Schwartz2">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-J1">[68]</xref>.</p>
<p>Thus, to understand the advantages and disadvantages of the NIM structure, it is useful to compare it with the dominant alternative approach for describing nonlinear stimulus processing: “quadratic models”. Such models have recently been cast in an information-theoretic context <xref ref-type="bibr" rid="pcbi.1003143-Pillow1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Fitzgerald1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Rajan1">[28]</xref>, as well as in the form of an explicit probabilistic model <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref> which has been referred to as the ‘Generalized Quadratic Model’ (GQM). The GQM can be viewed as a probabilistic generalization of STA/STC analysis <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref> and of the second-order Wiener-Volterra expansion <xref ref-type="bibr" rid="pcbi.1003143-Marmarelis1">[20]</xref>. The GQM can also be written in the form of a NIM where the upstream nonlinearities <italic>f<sub>i</sub></italic>(.) are fixed to be linear or squared functions:<disp-formula id="pcbi.1003143.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e003" xlink:type="simple"/><label>(3)</label></disp-formula>where <bold>k</bold><italic><sub>L</sub></italic> is a linear filter, and the <italic>M</italic> squared filters <bold>k</bold><italic><sub>i</sub></italic> generally provide a low-rank approximation to the quadratic component <bold>C</bold> <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref>. In this sense, the probabilistic framework described here is easily extended to encompass quadratic models, providing a means for direct comparison between different nonlinear structures.</p>
<p>For the ON-OFF RGC, the GQM finds one linear and two quadratic filters, all of which are contained in the two-dimensional subspace identified by STC analysis, meaning that the GQM filters are also linear combinations of the true ON and OFF filters (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3B</xref>). Note that while two filters are sufficient to span the relevant stimulus subspace, the third GQM filter provides an additional degree of freedom to capture the best quadratic approximation to the underlying ‘neural response function’ (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3E</xref>).</p>
<p>Although in this example the resulting quadratic function cannot completely capture the form of the response function constructed from rectified inputs, we note that it still provides a good approximation, as shown by only modest reductions in model performance compared to the NIM (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3F</xref>). However, as expected from a second-order Taylor series expansion, such an approximation breaks down further from the “origin” of the subspace. Thus, the quadratic approximation will typically be less robust for stimuli with heavy-tailed distributions such as those associated with natural stimuli <xref ref-type="bibr" rid="pcbi.1003143-Field1">[69]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Lewicki1">[71]</xref>. To illustrate this point we performed simulations of the same ON-OFF RGC presented with white noise stimuli having a Student's <italic>t</italic>-distribution, where the tail thickness was controlled by varying the number of degrees of freedom (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3G</xref>). The improved performance of the NIM over the GQM is indeed substantially enhanced for stimulus distributions with heavier tails (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3H</xref>). We also verified similar effects for a range of simulated neurons (data not shown).</p>
<p>We emphasize that one of the key advantages of the NIM over previously described methods is that it provides a more interpretable picture of stimulus processing as a sum of rectified neuronal inputs. As we demonstrate through several examples below in both the visual and auditory systems, it appears that sensory computation by neurons will often adhere to this general form, which is motivated primarily by physiological, rather than mathematical, considerations.</p>
</sec><sec id="s2d">
<title>Inferring the interplay of excitation and inhibition in visual and auditory neurons</title>
<p>One of the main advantages of the NIM structure is the ability to specifically model the effects of inhibitory inputs, which are increasingly being shown to have a large impact on neuronal processing in many sensory areas <xref ref-type="bibr" rid="pcbi.1003143-Gabernet1">[72]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Wu1">[74]</xref>. Indeed, the NIM generates predictions of the functional tuning of excitation and inhibition, and provides insight into their role in sensory processing. To demonstrate this, we apply the NIM to example neurons from visual and auditory areas.</p>
<p>We first consider an example cat LGN neuron during the presentation of natural movies <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lesica1">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts2">[76]</xref>. Accurate characterization of LGN processing poses substantial challenges for previous nonlinear approaches, due to the high temporal resolution of LGN responses in this context <xref ref-type="bibr" rid="pcbi.1003143-Butts3">[77]</xref> combined with the large number of spatial dimensions of the stimulus. As a result, previous nonlinear applications have either utilized lower temporal resolutions <xref ref-type="bibr" rid="pcbi.1003143-Sincich1">[78]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Wang1">[79]</xref> or parametric models of the spatial processing <xref ref-type="bibr" rid="pcbi.1003143-Mante1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Kaplan1">[80]</xref>. The methods described here allow for (appropriately regularized) spatiotemporal receptive fields (STRFs) of LGN neurons to be fit at sufficiently high resolution, using natural movies. We find that the response of the example LGN neuron consists of an excitatory receptive field that is delayed relative to the linear STRF (<xref ref-type="fig" rid="pcbi-1003143-g004">Fig. 4A</xref>), along with a second, more delayed ‘suppressive’ receptive field (<xref ref-type="fig" rid="pcbi-1003143-g004">Fig. 4B</xref>), corresponding to putative inhibitory input. Unlike in previous studies, the tractability of the fitting procedures used here allows for high spatial and temporal resolution of the putative inputs (<xref ref-type="fig" rid="pcbi-1003143-g004">Fig. 4B</xref>), as well as the application of sparseness and smoothness regularization (see <xref ref-type="sec" rid="s3">Methods</xref>). By comparison, the GQM identifies similar STRFs, but has worse performance (<xref ref-type="fig" rid="pcbi-1003143-g004">Fig. 4C</xref>), as well as a different nonlinear structure and resulting physiological interpretation (<xref ref-type="supplementary-material" rid="pcbi.1003143.s003">Fig. S3</xref>).</p>
<fig id="pcbi-1003143-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g004</object-id><label>Figure 4</label><caption>
<title>Spatiotemporal tuning of excitatory and suppressive inputs to an LGN neuron.</title>
<p>A) The linear receptive field can be represented as the sum of two space-time separable components, corresponding to the receptive field center (red) and surround (blue). B) The NIM with excitatory (top) and suppressive (i.e., putative inhibitory, bottom) inputs. The excitatory and suppressive components (solid) both have slower, and less biphasic, temporal responses (left) compared with the linear model (dashed). The suppressive input is also delayed relative to the excitatory input. Both excitatory and suppressive inputs have roughly the same spatial profiles (middle), and both provide rectified input through the corresponding upstream nonlinearities (right). C) The NIM has significantly better performance, as measured by cross-validated log-likelihood, compared to the linear model (<italic>p</italic> = 0.002; <italic>n</italic> = 10 cross-validation sets; Wilcoxon signed rank test) and the GQM (<italic>p</italic> = 0.002).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g004" position="float" xlink:type="simple"/></fig>
<p>Next we consider an example neuron from zebra finch area MLd, as the animal is presented with conspecific bird songs <xref ref-type="bibr" rid="pcbi.1003143-Hsu1">[81]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Teeters1">[83]</xref>. These neurons respond to specific frequencies of the song input, and hence their stimulus selectivity can be characterized by a linear spectrotemporal receptive field (STRF) <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>, which can be recovered in an unbiased manner using maximum-likelihood estimation (<xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5A</xref>) <xref ref-type="bibr" rid="pcbi.1003143-Calabrese1">[84]</xref> despite the presence of higher order correlations in the stimulus. Application of the NIM to this example neuron again recovers both an excitatory and a temporally delayed suppressive component (<xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5B</xref>). The description of the neuron's stimulus tuning provided by the NIM is closely related to that given by the linear model, but instead of identifying positive and negative domains of the linear STRF as excitatory and suppressive, these effects are segregated into different nonlinear processing subunits, each individually rectified. The separate excitatory and suppressive inputs provide a more accurate description of the underlying stimulus processing than a single linear STRF, as demonstrated by the significantly improved model performance of the NIM compared with the LN model (<xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5C</xref>). As with the LGN example, the GQM identifies similar excitatory and suppressive filters as the NIM, but again provides a less physiologically interpretable description of the underlying computation (<xref ref-type="supplementary-material" rid="pcbi.1003143.s004">Fig. S4</xref>), and has comparable, if slightly reduced, performance (<xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5C</xref>).</p>
<fig id="pcbi-1003143-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g005</object-id><label>Figure 5</label><caption>
<title>Spectrotemporal tuning of excitation and suppression in the songbird auditory midbrain.</title>
<p>A) The linear spectrotemporal receptive field (STRF; left) contains two subfields of opposite sign. B) The excitatory (top) and suppressive (bottom) spectrotemporal filters identified by the NIM are similar to the positive and negative subfields of the linear STRF respectively. However, these inputs are both rectified by the upstream nonlinearities (right), resulting in different stimulus processing (see <xref ref-type="supplementary-material" rid="pcbi.1003143.s004">Fig. S4</xref>). C) Comparison of log-likelihoods of the LN model, GQM, and NIM. Red lines show the performance across models for each cross-validation set. Note that the duration of the recording, and the neuron's relatively low firing rate, limit the statistical power of model comparisons.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Modeling complex neural response functions in terms of rectified inputs</title>
<p>Thus far we have only considered cases where the neuron's response is described by a NIM with a small number of inputs, consistent with simpler stimulus processing in sub-cortical areas. In contrast, in the visual cortex, even V1 ‘simple cells’ can exhibit selectivity to large numbers of stimulus dimensions <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>. Further, the dominant model of V1 ‘complex cells’ is the nonlinear “Energy Model” <xref ref-type="bibr" rid="pcbi.1003143-Carandini1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Adelson1">[85]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Mante2">[87]</xref>, which posits quadratic stimulus processing that results in the response representing the amount of local, oriented, band-pass “stimulus energy”. The Energy Model has been broadly tested <xref ref-type="bibr" rid="pcbi.1003143-Carandini1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Mante2">[87]</xref>, and is well supported by previous nonlinear modeling approaches <xref ref-type="bibr" rid="pcbi.1003143-Schwartz1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Fitzgerald1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Touryan1">[88]</xref>. While the Energy Model provides a functional description of stimulus processing for V1 complex cells, it is less clear how such stimulus selectivity is constructed, and how it is related to V1 simple cell processing. Here we demonstrate that the NIM can describe both simple and complex cell processing as a sum of rectified inputs, providing a basis for a unified description of visual cortical neuron computation <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>.</p>
<p>We first consider two simulated V1 neurons in order to demonstrate the capacity for such a unified description, before applying the NIM to experimental data. We generate simulated data using a one-dimensional white-noise bar stimulus aligned with the simulated neurons' preferred spatial orientation (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6A</xref>), which is a common, relatively low-dimensional, stimulus used in nonlinear characterizations of V1 neurons <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Touryan1">[88]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Tanabe1">[89]</xref>. The first simulated neuron's response is constructed as a sum of six rectified direction-selective inputs (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6B</xref>), consistent with the structure of the NIM, while the second neuron's response is constructed from four such inputs processed by a squaring nonlinearity, similar to the standard Energy Model of V1 complex cells <xref ref-type="bibr" rid="pcbi.1003143-Adelson1">[85]</xref>.</p>
<fig id="pcbi-1003143-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g006</object-id><label>Figure 6</label><caption>
<title>Modeling stimulus selectivity arising from many inputs.</title>
<p>A) Simulated V1 neurons are presented with one-dimensional spatiotemporal white noise stimuli (left). Their stimulus processing is constructed from a set of spatiotemporal filters (example shown at right), depicted with one spatial dimension (x-axis) and time lag (y-axis). B) The first simulated neuron is constructed from six spatially overlapping direction-selective filters (top), similar to those observed experimentally for V1 neurons. Below, the corresponding filtered stimulus distributions are shown along with the respective upstream nonlinearities (blue). C) The NIM identifies the correct spatiotemporal filters (top), as well as the form of the upstream nonlinearities (middle). The projections of the NIM filters onto the true filters (bottom) illustrate that the NIM identifies the true filters. D) The STA for the simulated neuron (left), along with the three significant STC filters (right) are largely contained in the subspace spanned by the true filters, but reflect non-trivial linear combinations of these filters (bottom). E) The GQM is composed of a linear input (left) and three excitatory squared inputs (right). While the GQM filters are more similar to the true filters, they also represent non-trivial linear combinations of them (bottom). F) The second simulated neuron consists of four similar, but spatially shifted, inputs that are squared. G) The NIM represents each true (squared) input by an opposing pair of rectified inputs. H) The STA (left) does not show any structure because the neuron's response is, by construction, symmetric in the stimulus. The four significant STC filters (right) represent distributed linear combinations of the four underlying filters. I) The GQM recovers the correct stimulus filters, given appropriate sparseness regularization.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g006" position="float" xlink:type="simple"/></fig>
<p>For the neuron with rectified inputs, the NIM fitting procedure is indeed able to identify the true underlying stimulus filters and the form of the rectifying upstream nonlinearities (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6C</xref>). Additionally, while the optimal number of filters can be determined using the cross-validated model performance, the identified stimulus filters, and the resulting model performance itself, are relatively insensitive to specification of the precise number of model subunits (<xref ref-type="supplementary-material" rid="pcbi.1003143.s005">Fig. S5</xref>). This demonstrates the ability of the NIM to robustly identify even relatively complex stimulus processing, in cases where such processing arises from a sum of rectified inputs.</p>
<p>Furthermore, as with the ON-OFF RGC example above (<xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1</xref> and <xref ref-type="fig" rid="pcbi-1003143-g003">3</xref>), STC analysis of this simulated V1 neuron can identify the appropriate stimulus subspace, although not the true underlying filters (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6D</xref>). Because of the high dimensionality of the resulting subspace, however, it is more difficult to estimate the mapping from the subspace to the neuronal response compared with the ON-OFF example. The lack of alignment between the STA/STC filters and the true filters further complicates a straightforward interpretation of the estimated function.</p>
<p>By comparison, the GQM identifies filters with characteristics that more closely resemble those of the true input filters (e.g., more localized, fewer lobes). The improved performance of the GQM compared with an STC-based model (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6E</xref>) highlights the greater power and flexibility of a probabilistic modeling framework, particularly the importance of regularization. Nevertheless, the GQM filters still reflect non-trivial linear combinations of the true filters, as with the STC filters (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6E</xref>, <italic>bottom</italic>).</p>
<p>Of course, one would expect the NIM to outperform other models when the generative model is composed as a sum of rectified inputs. In a second simulated example, however, we illustrate the flexibility of the NIM in capturing other neural response functions. The second simulated neuron is constructed from four direction-selective inputs that are squared and summed together to generate a quadratic response function (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6F</xref>). The NIM is still able to identify the true generative model using pairs of rectified inputs with equal but opposite input filters to represent each quadratic filter (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6G</xref>). This representation is certainly not the most efficient in this case, as the GQM is able to identify the correct filters (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6I</xref>) using fewer parameters and a more straightforward estimation procedure.</p>
<p>These two simulated V1 examples thus illustrate the potential tradeoffs between the NIM and GQM. On the one hand, the NIM provides a more flexible framework that can capture a broader range of nonlinear stimulus processing. In fact, any response function can in principle be represented with this structure <xref ref-type="bibr" rid="pcbi.1003143-Diaconis1">[90]</xref>. The NIM structure is also more appropriate for explicitly modeling neuronal inputs, and thus allows for more plausible physiological interpretation of its components. On the other hand, the GQM can capture the nonlinear mapping up to second order more efficiently, and identifies the relevant stimulus subspace robustly. This suggests the potential for combining these approaches when investigating complex neuronal processing, such as by using the GQM to identify the relevant stimulus subspace and provide initial estimates of the number and properties of NIM filters, followed by application of the NIM framework (see <xref ref-type="sec" rid="s3">Methods</xref>; <xref ref-type="supplementary-material" rid="pcbi.1003143.s001">Figs. S1</xref>, <xref ref-type="supplementary-material" rid="pcbi.1003143.s003">S3</xref>).</p>
</sec><sec id="s2f">
<title>Application of the NIM to recorded V1 neurons</title>
<p>While the simulated examples above allowed for model comparisons when the neurons' response functions were known, they also provide a foundation for understanding model fits to real V1 data. We first consider a V1 neuron recorded from an anesthetized macaque in the context of similar one-dimensional white noise stimuli <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>. While this neuron has a clear STA and is considered a simple cell by classical measures, STC analysis identifies two excitatory and six suppressive stimulus dimensions (based on inspection of the eigenvalue spectrum) in addition to the STA (<xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7A</xref>). In this case, the GQM identifies similar filters to STC analysis, although the application of smoothness and sparseness regularization allows it to resolve more realistic stimulus filters (<xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7B</xref>), and produce significantly improved model performance (<xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7D</xref>) compared to an STC-based model (see <xref ref-type="sec" rid="s3">Methods</xref>).</p>
<fig id="pcbi-1003143-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g007</object-id><label>Figure 7</label><caption>
<title>Models of multi-input stimulus processing in a V1 neuron.</title>
<p>A) Standard spike-triggered characterization for this neuron reveals a ‘complicated simple-cell’ response <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>, with a clear direction-selective STA (left), two excitatory STC filters (middle), and six suppressive STC filters (right). B) The GQM identifies a set of filters (one linear, two squared excitatory, and six squared suppressive) that are roughly similar to the STA/STC filters, but with smoother and sparser spatiotemporal structure (due to regularization). C) The NIM filters (top) and upstream nonlinearities (bottom) reveal a similar description of the stimulus processing, although with greater consistency among the (six) excitatory and (six) suppressive stimulus filters. D) Comparison of the cross-validated log-likelihood of the LN model (one linear filter), the ‘STC model’ given by fitting a GLM to the outputs of the STA/STC filters (see <xref ref-type="sec" rid="s3">Methods</xref>), the GQM, and the NIM. Given the neuron's simple-cell-like response (i.e., large weight of the STA), a large fraction of the response can be captured with the linear filter alone (the LN model). Nevertheless, all three multi-filter models provide substantial improvements compared to the LN model. E) In order to compare the performance of the nonlinear (multi-filter) models directly, their improvement relative to the LN model is depicted. This shows that the GQM significantly outperforms the ‘STC’ model (<italic>p</italic> = 0.002; <italic>n</italic> = 10; Wilcoxon signed rank test), and that the NIM similarly outperforms the GQM (<italic>p</italic> = 0.002).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g007" position="float" xlink:type="simple"/></fig>
<p>We also fit a NIM with six excitatory and six suppressive stimulus filters, where the number of filters was selected based on cross-validated model performance (<xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7C</xref>; see also <xref ref-type="supplementary-material" rid="pcbi.1003143.s005">Fig. S5</xref>). As expected, these 12 filters span a stimulus subspace that is largely overlapping with the subspace identified by the GQM. However, the additional stimulus filters, and the inferred upstream nonlinearities associated with each subunit, allow the NIM to capture additional aspects of the neural response function that significantly improve the cross-validated model performance relative to the quadratic models (<xref ref-type="fig" rid="pcbi-1003143-g007">Figs. 7D, E</xref>). We also note that the NIM appears to identify a more consistent set of stimulus filters than the quadratic models.</p>
<p>Similar comparisons also come to light in when applying the models to V1 complex cells, even in the most demanding stimulus contexts. To illustrate this, we consider an example V1 neuron recorded from an anesthetized cat presented with natural and naturalistic stimuli (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8A</xref>) <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Blanche1">[91]</xref>. Because the stimuli are sequences of two-dimensional images, the required spatiotemporal stimulus filters span two dimensions of space and one dimension of time (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8B</xref>), resulting in a very large number of parameters associated with each subunit. Nevertheless, the parameters of the GQM and NIM can be estimated directly utilizing appropriate regularization (see <xref ref-type="sec" rid="s3">Methods</xref>).</p>
<fig id="pcbi-1003143-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003143.g008</object-id><label>Figure 8</label><caption>
<title>Models of a V1 neuron in the context of natural stimuli.</title>
<p>A) The natural movie stimulus used here has two spatial and one temporal dimension. B) The neuron's response is characterized in terms of three-dimensional spatiotemporal filters. An example spatiotemporal filter is comprised of a spatial filter at each time step (at 20 ms resolution). To simplify the depiction of each filter, we take advantage of their stereotyped structure, and plot the spatial distribution at the best time slice (BTS, left), as well as the space-time projection (STP, right) along an axis orthogonal to the preferred orientation (red line; see <xref ref-type="sec" rid="s3">Methods</xref>). C) The GQM for this neuron consists of one linear (top) and two excitatory squared filters (bottom). The BTS and STP for each filter are shown at left, and the distributions of the filtered stimulus, and associated nonlinearities, are shown at right. Note that the two squared filters roughly form a ‘quadrature pair’ of direction-selective Gabor filters. There is also a linear filter (top), which has less clear spatial structure, and is not direction-selective. D) The NIM consists of four excitatory filters (left) that are qualitatively similar to the quadrature pair of GQM filters. However, by identifying four inputs with inferred upstream nonlinearities (right), the NIM has greater flexibility in describing the neuron's computation. E) Comparison of model performance for the LN and STC-based models, as well as the GQM and NIM, showing that the NIM substantially outperformed other models for this neuron.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003143.g008" position="float" xlink:type="simple"/></fig>
<p>The GQM estimated for this neuron is comprised of a pair of excitatory, direction-selective squared filters, as well as a weaker, non-direction-selective linear filter (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8C</xref>). This characterization reflects the neuron's spatial-phase invariance, and is thus consistent with an Energy Model description. While such selectivity suggests that this neuron would be ideally suited for a quadratic model, the NIM (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8D</xref>) significantly outperforms both the GQM and a whitened STC-based model <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref> (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8E</xref>).</p>
<p>The NIM identifies four rectified excitatory inputs that share similar spatial tuning and direction selectivity, but with different spatial phases (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8D</xref>). This description is similar to that provided by the quadratic terms of the GQM, but by identifying the nonlinearities associated with each of these inputs individually, the NIM has additional flexibility that results in improved performance (<xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8E</xref>). This suggests that a description of complex cells using physiologically plausible inputs (in the form of the NIM) may be a viable alternative to the Energy Model. The improved performance of the NIM is also likely due, at least in part, to the heavy-tailed distribution associated with the naturalistic movie stimuli (as described above, <xref ref-type="fig" rid="pcbi-1003143-g003">Figs. 3G, H</xref>).</p>
<p>Thus, the application of the NIM to V1 neurons further illustrates the generality of the method, and specifically emphasizes its ability to capture substantially more complex stimulus processing, with large numbers of inputs. We note that because cortical neurons are several synapses removed from receptor neurons, a cascade model with a longer chain of upstream LN components might be more appropriate, although existing methods could not be used for parameter estimation with such a model. The ability of the NIM to capture a given neuron's stimulus processing thus relates to the extent to which the upstream neurons themselves can be approximated by LN models. In cases where this assumption is not appropriate, one can apply a fixed nonlinear transformation to the stimulus resembling the response properties of upstream neurons <xref ref-type="bibr" rid="pcbi.1003143-Nishimoto1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Mineault2">[32]</xref>, thus allowing the problem to be cast into a more general NIM framework.</p>
</sec><sec id="s2g">
<title>Conclusions</title>
<p>We have presented a physiologically inspired modeling framework, the NIM, which extends several recently developed probabilistic modeling approaches. Specifically, the NIM assumes a form analogous to an integrate-and-fire neuron, whereby a neuron receives a set of rectified excitatory and inhibitory inputs, each of which is assumed to process the stimulus linearly. The parameters can be estimated robustly and efficiently, and the resulting model structure is able to capture a broader range of neural responses than previously proposed probabilistic methods. Importantly, the physiologically inspired model structure of the NIM also allows for greater interpretability of the model fits, as the components of the model take the form of stimulus-driven excitatory and inhibitory inputs. The NIM thus provides a framework for connecting nonlinear models of sensory processing directly with the underlying physiology that can be applied in a range of sensory areas and experimental conditions.</p>
</sec></sec><sec id="s3" sec-type="methods">
<title>Methods</title>
<sec id="s3a">
<title>Parameter estimation details</title>
<p>As described above, the key parameters in the NIM are the stimulus filters <bold>k</bold><sub>i</sub> and the set of coefficients <italic>a</italic><sub>ij</sub> representing the upstream nonlinearities <italic>f</italic><sub>i</sub>(.). While these parameters cannot generally be optimized simultaneously, a powerful approach is to use block coordinate ascent <xref ref-type="bibr" rid="pcbi.1003143-Bertsekas1">[66]</xref> and alternate between optimizing the filters <bold>k</bold><sub>i</sub>, and upstream nonlinearities <italic>f</italic><sub>i</sub>(.), holding the remaining parameters fixed in each iteration. The parameters of the spiking nonlinearity function <italic>F</italic>[<italic>x</italic>; <italic>α</italic>,<italic>β</italic>,<italic>θ</italic>] = <italic>α</italic>log[1+exp(<italic>β</italic>(<italic>x</italic>-<italic>θ</italic>))] can also be estimated iteratively, or as a final stage after convergence of the <bold>k</bold><sub>i</sub> and <italic>f</italic><sub>i</sub>(.) (which we find is typically sufficient). Note that the parameter <italic>β</italic> is not generally identifiable in the model (being degenerate with the coefficients <italic>a<sub>ij</sub></italic> of the upstream nonlinearities), but joint estimation of <italic>α</italic> and <italic>β</italic> after the other model parameters are fixed allows for a more precise final fit to the spiking nonlinearity function.</p>
<p>Thus, at each stage of the fitting procedure we have the problem of maximizing a (penalized) log-likelihood function with respect to some subset of parameters, while holding a remaining set of parameters fixed. In all cases, we use a standard line search strategy to locate an optimum of the likelihood function given some initial values for the parameters. Because we are often optimizing very high-dimensional parameter vectors (specifically when optimizing the <bold>k</bold><sub>i</sub>), we use a quasi-Newton method with a limited-memory BFGS approximation of the inverse Hessian matrix <xref ref-type="bibr" rid="pcbi.1003143-Nocedal1">[92]</xref> to determine the search direction. This code is implemented in the <italic>Matlab</italic> function “minFunc”, provided by Mark Schmidt (available at <ext-link ext-link-type="uri" xlink:href="http://www.di.ens.fr/~mschmidt/" xlink:type="simple">http://www.di.ens.fr/~mschmidt/</ext-link>). When using sparseness (L1) regularization we utilize the <italic>Matlab</italic> package “L1General”, also provided by Mark Schmidt. When optimizing the coefficients <italic>a</italic><sub>ij</sub> of the upstream nonlinearities we additionally enforce a set of linear constraints (described below), and in such cases we utilize <italic>Matlab</italic>'s constrained optimization routine “fmincon”. A <italic>Matlab</italic> implementation of the NIM parameter estimation routines described here is available from our website: (<ext-link ext-link-type="uri" xlink:href="http://www.clfs.umd.edu/biology/ntlab/NIM/" xlink:type="simple">http://www.clfs.umd.edu/biology/ntlab/NIM/</ext-link>)</p>
</sec><sec id="s3b">
<title>Optimizing the filters k<sub>i</sub></title>
<p>Optimization of the filters can be accomplished efficiently by analytic calculation of the log-likelihood gradient with respect to the <bold>k</bold><sub>i</sub>, which is given by:<disp-formula id="pcbi.1003143.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e004" xlink:type="simple"/><label>(4)</label></disp-formula>where the ‘internal generating function’ <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003143.e005" xlink:type="simple"/></inline-formula>, <italic>F</italic>'[.] and <italic>f<sub>i</sub></italic>'(.) are the derivatives of <italic>F</italic>[.] and <italic>f<sub>i</sub></italic>(.) with respect to their arguments, and <italic>s</italic><sub>m</sub>(t) is the <italic>m</italic><sup>th</sup> element of the stimulus at time <italic>t</italic>. While the likelihood surface is not generally convex with respect to the <bold>k</bold><sub>i</sub>, the optimization problem is well-behaved in practice. We note that while the derivatives of the <italic>f<sub>i</sub></italic>(.) are discontinuous (piece-wise constant) when using the tent-basis representation (<xref ref-type="disp-formula" rid="pcbi.1003143.e007">eq. 6</xref> below), gradient-based optimization methods still provide robust results, in particular because we use regularization to enforce smooth <italic>f<sub>i</sub></italic>(.) such that the contribution of the discontinuities to the overall log-likelihood gradient is negligible in practice.</p>
<p>To diagnose the presence of undesirable local maxima, and to identify the global optimum of the likelihood function, we use repeated random initializations of our optimization routine (<xref ref-type="supplementary-material" rid="pcbi.1003143.s001">Fig. S1</xref>). In some cases, such as the ON-OFF RGC example (<xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1</xref>, <xref ref-type="fig" rid="pcbi-1003143-g003">3</xref>), this approach reveals that the choice of initial values for <bold>k</bold><sub>i</sub> does not affect the identified local optimum. In other cases, the likelihood surface will contain more than one distinct local maximum, although usually only a small number. For example, when optimizing the filters for the example MLd neuron (<xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5</xref>) we found two distinct local optima of the likelihood function. For models with large numbers of subunits, the filter optimization remains well-behaved, generally identifying a relatively small number of local optima that correspond to similar models (<xref ref-type="supplementary-material" rid="pcbi.1003143.s001">Fig. S1</xref>).</p>
<p>This procedure can be greatly sped up by initially optimizing the filters in a low-dimensional stimulus subspace, rather than in the full stimulus space. Such subspace optimization has been previous used in conjunction with STC analysis to identify the relevant stimulus subspace <xref ref-type="bibr" rid="pcbi.1003143-Pillow1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Saleem1">[93]</xref>; however the GQM provides a means of generalizing the robust subspace identification properties of STC analysis to arbitrary non-Gaussian stimuli, and in cases where regularization is important. With a low-dimensional subspace identified the filters of a NIM can be rapidly optimized, and many filter initializations can be tested.</p>
</sec><sec id="s3c">
<title>Fitting the upstream nonlinearities <italic>f</italic><sub>i</sub>(.)</title>
<p>We begin the NIM fitting with its upstream nonlinearities <italic>f</italic><sub>i</sub>(.) initialized to be threshold-linear functions:<disp-formula id="pcbi.1003143.e006"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e006" xlink:type="simple"/><label>(5)</label></disp-formula>and perform initial estimation of the filters. While other rectifying functions can be used, the use of scale-invariant functions such as this one has the advantage that the effect of the upstream nonlinearity is independent of the scale of the filter.</p>
<p>After estimating the <bold>k</bold><sub>i</sub>, we then estimate the <italic>f</italic><sub>i</sub>(.) nonparametrically, as a linear combination of a set of piecewise linear basis functions <italic>f<sub>i</sub></italic>(<italic>g</italic>) = Σ<italic><sub>j</sub> a<sub>ij</sub>φ<sub>j</sub></italic>(<italic>g</italic>) <xref ref-type="bibr" rid="pcbi.1003143-Ahrens1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, while holding the <bold>k</bold><sub>i</sub> fixed. These basis functions are given by:<disp-formula id="pcbi.1003143.e007"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e007" xlink:type="simple"/><label>(6)</label></disp-formula>These piecewise linear functions are particularly useful as they provide a set of localized basis functions, requiring only that we choose a set of ‘grid points’ <italic>x</italic><sub>k</sub>. These points can be selected by referencing the distribution of the argument of <italic>f</italic><sub>i</sub>(.), i.e., <italic>p</italic>(<italic>g<sub>i</sub></italic>) where <italic>g</italic><sub>i</sub>(t) = <bold>k</bold><sub>i</sub>•<bold>s</bold>(<italic>t</italic>), either at <italic>n</italic>-quantiles of <italic>p</italic>(<italic>g<sub>i</sub></italic>), or at uniformly spaced points across the support of <italic>p</italic>(<italic>g<sub>i</sub></italic>). In order to encourage interpretability of the model subunits as ‘neural inputs’, we constrain the <italic>f</italic><sub>i</sub>(.) to be monotonically increasing functions by using a system of linear constraints on the <italic>a</italic><sub>ij</sub> during optimization. Because the model is invariant to shifts in the ‘y-offset’ of the <italic>f</italic><sub>i</sub>(.) (which can be absorbed into the spiking nonlinearity function), we add the additional set of constraints that <italic>f</italic><sub>i</sub>(0) = 0 to eliminate this degeneracy. Furthermore, changes in the upstream nonlinearities can influence the effective regularization of the <bold>k</bold><sub>i</sub>, by altering how each <bold>k</bold><sub>i</sub> contributes to the model prediction. As a result, the coefficients <italic>a</italic><sub>ij</sub> are rescaled after each iteration so that the standard deviation of each subunit's output is conserved. This ensures that the upstream nonlinearities do not absorb the scale of the <bold>k</bold><sub>i</sub>.</p>
</sec><sec id="s3d">
<title>Regularization</title>
<p>An important advantage of explicit probabilistic models such as the NIM is the ability to incorporate prior knowledge about the parameters via regularization. Because each of the filters <bold>k</bold><sub>i</sub> often contains a large number of parameters, regularization of the filters is of particular importance, as discussed elsewhere in the context of the GLM <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Calabrese1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Sahani1">[94]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Park3">[97]</xref>, as well as other nonlinear models <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref>. Such regularization can impose prior knowledge about the smoothness <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Sahani1">[94]</xref>, sparseness <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Calabrese1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Sahani1">[94]</xref>–<xref ref-type="bibr" rid="pcbi.1003143-Gerwinn2">[96]</xref>, and localization <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Park3">[97]</xref> of filters in space, frequency and time.</p>
<p>We consider several different forms of regularization in the examples shown, to encourage the detection of smooth filters with sparse coefficients. Specifically, we add a general penalty term of the form:<disp-formula id="pcbi.1003143.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003143.e008" xlink:type="simple"/><label>(7)</label></disp-formula>to the equation for the log-likelihood (<xref ref-type="disp-formula" rid="pcbi.1003143.e002">equation 2</xref>), where <bold>L<sup>s</sup></bold> and <bold>L<sup>t</sup></bold> are the discrete Laplacian operators with respect to spatial (or spectral) and temporal dimensions respectively, and <italic>λ<sub>i</sub><sup>Ls</sup></italic> , <italic>λ<sub>i</sub><sup>Lt</sup></italic> and <italic>λ<sub>i</sub><sup>s</sup></italic> are hyperparameters which determine the strength of spatial and temporal smoothness, and sparseness regularization, respectively. Other types of regularization, such as those that encourage localized filters <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Park3">[97]</xref>, as well as approximate Bayesian techniques for inferring hyperparameters <xref ref-type="bibr" rid="pcbi.1003143-Park2">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Sahani1">[94]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn2">[96]</xref> could be incorporated as well, although we do not do so here.</p>
<p>Because we also expect the upstream nonlinearities <italic>f</italic><sub>i</sub>(.) to be smooth functions, we incorporate penalty terms when estimating the parameters of the <italic>f</italic><sub>i</sub>(.). Because we represent the <italic>f</italic><sub>i</sub>(.) as linear combinations of localized tent basis functions: <italic>f</italic><sub>i</sub>(.) = <italic>a</italic><sub>ij</sub><italic>φ</italic><sub>j</sub>(.), we can encourage smooth <italic>f</italic><sub>i</sub>(.) by applying a penalty of the form: <italic>λ</italic><sub>i</sub><sup>L</sup>∥<bold>L</bold><italic>a</italic><sub>ij</sub>∥<sub>2</sub> to the set of coefficients <italic>a</italic><sub>ij</sub> corresponding to a given <italic>f</italic><sub>i</sub>(.), where <bold>L</bold> is again the one-dimensional discrete Laplacian operator.</p>
<p>In general, the hyperparameters can be inferred from the data using Bayesian techniques <xref ref-type="bibr" rid="pcbi.1003143-Sahani1">[94]</xref>, or estimated using a (separate) cross-validation data set. Both methods can be time-consuming, however, and in practice we find that similar results can be achieved by ‘manually’ tuning the hyperparameters to produce filters <bold>k</bold><sub>i</sub> and upstream nonlinearities <italic>f</italic><sub>i</sub>(.) with the expected degree of smoothness/sparseness. To demonstrate that our results were not overly sensitive to the selection of hyperparameters, we compare the NIM and GQM fit to the example V1 neuron from <xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8</xref> using a range of regularization strengths (<xref ref-type="supplementary-material" rid="pcbi.1003143.s006">Fig. S6</xref>).</p>
</sec><sec id="s3e">
<title>Evaluating model performance</title>
<p>To evaluate model performance, we use <italic>k</italic>-fold cross-validation, in general taking the log-likelihood as a performance metric. The likelihood has the advantage over related measures such as <italic>R<sup>2</sup></italic> in that it does not require repeated stimulus presentations to estimate, and thus can be applied to most data sets. It can also capture goodness-of-fit when spike history terms are incorporated <xref ref-type="bibr" rid="pcbi.1003143-Pillow2">[35]</xref>. Subtracting the log-likelihood of the null model (that predicts a constant firing rate, independent of the stimulus) provides a measure of the information carried by the spike train about the stimulus, in units of bits per spike <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Kouh1">[98]</xref>. This measure is also directly related to the more traditional measure of deviance, which compares the log-likelihood of the estimated model to that of the ‘saturated’ model. In order to provide a more direct connection to standard measures of model performance based on repeated presentations of a stimulus, we also computed the ‘predictive power’ of the models for the simulated ON-OFF RGC (<xref ref-type="fig" rid="pcbi-1003143-g003">Fig. 3F</xref>), which is defined as the fraction of ‘explainable’ variance accounted for by the model <xref ref-type="bibr" rid="pcbi.1003143-Sahani2">[99]</xref>. Due to the lack of sufficient repeat trial data for our recorded data examples we could not compute this measure in those cases, however qualitatively similar results would be expected.</p>
</sec><sec id="s3f">
<title>Model selection</title>
<p>While selection of the optimal number of excitatory and suppressive subunits can be performed using standard model selection techniques, such as nested cross-validation, this choice can also often be guided by the specific application. Importantly, we find that the subunits identified by the NIM, as well as its performance, are generally robust towards precise specification of the number of excitatory and suppressive subunits, with ‘nearby’ models typically providing a very similar characterization of the neurons' stimulus processing (<xref ref-type="supplementary-material" rid="pcbi.1003143.s005">Fig. S5</xref>). This robustness is further aided by the incorporation of sparseness regularization on the filters, where the filters of extraneous subunits tend to be driven to zero. The procedure of testing a series of NIMs with different subunit compositions can again be substantially facilitated by optimizing the filters in a low-dimensional stimulus subspace, such as identified by STC or GQM analysis (<xref ref-type="supplementary-material" rid="pcbi.1003143.s005">Fig. S5</xref>).</p>
</sec><sec id="s3g">
<title>RGC simulation details</title>
<p>In order to simulate the response of an ON-OFF RGC, we generated a Gaussian white noise process sampled at 15 Hz (such as a luminance-modulated spot stimulus), which was then filtered using separate ON- and OFF-like filters (<xref ref-type="fig" rid="pcbi-1003143-g001">Fig. 1A</xref>). These filter outputs were then rectified using functions of the form <italic>f</italic>(<italic>x</italic>) = log(1+exp(<italic>b<sub>1</sub></italic>x)), summed together and the resulting signal was passed through a spiking nonlinearity of the form <italic>F</italic>[<italic>x</italic>] = <italic>a</italic>log(1+exp(<italic>b<sub>2</sub></italic>(<italic>x</italic>-<italic>c</italic>))). This conditional intensity function was then used to generate a set of spike times. To generate heavy-tailed stimulus distributions (<xref ref-type="fig" rid="pcbi-1003143-g003">Figs. 3G, H</xref>), we sampled white noise from a Student's <italic>t</italic>-distribution with a range of values for the degrees of freedom to control the tail thickness.</p>
<p>The data were simulated at a temporal resolution of 8.3 ms, and model filters were represented at a lower resolution of 33 ms, with a length of 1 s. For the GQM and NIM we incorporated smoothness regularization on the filters, and for the NIM we also incorporated smoothness regularization on the upstream nonlinearity coefficients <italic>a<sub>ij</sub></italic>.</p>
<p>To identify the STA/STC subspace depicted in <xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1</xref> and <xref ref-type="fig" rid="pcbi-1003143-g003">3</xref>, we performed STC analysis after projecting out the STA. For comparison with the NIM, we also created a simple model based on the STA and STC filters, using a GLM-based optimization of linear coefficients on the outputs of the STA filter and the squared outputs of the STC filters, similar to previous work <xref ref-type="bibr" rid="pcbi.1003143-Gerwinn1">[58]</xref>. Note that in order to maximize performance when estimating STC-based models, we did not project out the STA before computing the STC filters.</p>
</sec><sec id="s3h">
<title>LGN neuron model fit details</title>
<p>Data for the LGN example were recorded extracellularly from an anaesthetized and paralyzed cat by the Alonso Lab <xref ref-type="bibr" rid="pcbi.1003143-Butts1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Lesica1">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Butts2">[76]</xref>. The stimulus consisted of 800 seconds of a 32×32 pixel natural movie, refreshed at 60 Hz, which was recorded from a camera mounted on top of a cat's head <xref ref-type="bibr" rid="pcbi.1003143-Kayser1">[100]</xref>. A 17×17 pixel patch of the movie was cropped around the receptive field, detected via STA at the optimal latency, and the movie was up-sampled by a factor of six, to produce a temporal resolution of 2.8 ms. Ten-fold cross-validation was used for evaluating model performance.</p>
<p>Each filter was represented by space-time separable center and surround components, and thus consisted of two sets of spatial and temporal filters <xref ref-type="bibr" rid="pcbi.1003143-Cai1">[101]</xref>. Temporal filters were represented with 30 equally spaced tent basis functions, with grid points ranging from 0 to −240 ms. For the LN model, the spatial filters were initialized as Gaussian functions with the same center as the STA and different widths (1 pixel for the center and 6 pixels for the surround). For the GQM and NIM, both excitatory and suppressive filters were initialized to be the same as the optimal linear filters. In the filter optimization stage, the spatial and temporal filters were optimized alternately until convergence of the log-likelihood. Both the GQM and the NIM were fit using smoothness regularization for the spatial and temporal kernels, and sparseness regularization for the spatial kernels. For the NIM, we also used smoothness regularization on the <italic>a<sub>ij</sub></italic> when estimating the upstream nonlinearities.</p>
</sec><sec id="s3i">
<title>Songbird auditory midbrain model fit details</title>
<p>Data for the songbird auditory midbrain example were provided by the Theunissen lab through the CRCNS database <xref ref-type="bibr" rid="pcbi.1003143-Teeters1">[83]</xref>, and details of experimental methods can be found in <xref ref-type="bibr" rid="pcbi.1003143-Hsu1">[81]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-Gill1">[82]</xref>. The example neuron was recorded extracellularly from the zebra finch mesencephalicus lateralis dorsalis (MLd). Stimuli consisted of 20 different conspecific bird songs, each lasting 2–4 sec, and each presented 10 times. These 20 songs were then divided into 5 equal groups for five-fold cross-validation. The raw sound waveforms were preprocessed by computing the spectrogram using a short-time Fourier transform to produce a stimulus matrix X(<italic>t</italic>,<italic>f</italic>), representing the power of the audio signal at frequency <italic>f</italic> and time <italic>t</italic>. We used a time resolution of 2 ms, and 20 uniformly spaced frequency bins, ranging from 250 Hz to 8 kHz. For estimating spectrotemporal filters, we used 20 time lags. Thus, each filter was represented by 400 parameters. Filter estimates were regularized using sparseness and smoothness penalties, where the smoothness penalty utilized the spectrotemporal Laplacian (with equal weighting in the frequency and time dimensions).</p>
</sec><sec id="s3j">
<title>V1 simulation details</title>
<p>The simulated V1 neurons shown in <xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6</xref> were constructed as LNLN models (<xref ref-type="fig" rid="pcbi-1003143-g002">Fig. 2C</xref>). The stimulus filters were spatial Gabor functions that were amplitude- and phase-modulated in time (i.e., direction-selective). Stimulus filters were identical up to a spatial translation, and were weighted by a spatial Gaussian envelope. The filter outputs were then passed through a set of static nonlinear functions (either <italic>x</italic><sup>2</sup>, or log(1+exp(<italic>b<sub>1</sub>x</italic>))), before being summed together, and passed through the spiking nonlinearity (again, of the form <italic>a</italic>log(1+exp(<italic>b<sub>2</sub></italic>(<italic>x</italic>-<italic>c</italic>))) to generate a conditional intensity function. Spike times were simulated in response to binary random bar stimuli <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>, using a time resolution of 10 ms, and 24 bar positions.</p>
<p>Both the GQM and NIM were fit using a sparseness penalty on the filters. For the NIM, we also used smoothness regularization on the <italic>a<sub>ij</sub></italic> when estimating the upstream nonlinearities. To measure how well the estimated model filters matched the true filters, we represented the model filters as linear combinations of the true filters.</p>
</sec><sec id="s3k">
<title>V1 neuron modeling details</title>
<p>The V1 neuron shown in <xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7</xref> was recorded from an anesthetized macaque <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>. The stimuli (refreshed at 100 Hz) consisted of random arrays of black and white bars covering the neuron's classical receptive field, and oriented along its preferred orientation. Full experimental details can be found in <xref ref-type="bibr" rid="pcbi.1003143-Rust2">[57]</xref>. Spatiotemporal filters were represented by 16 ‘pixels’ and 14 time lags. For model evaluation we used ten-fold cross-validation. Model fitting was analogous to that described for the V1 simulations (<xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6</xref>). STC-based models were constructed as described above for the simulated ON-OFF RGC.</p>
<p>The V1 neuron shown in <xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8</xref> was recorded from an anesthetized cat <xref ref-type="bibr" rid="pcbi.1003143-Blanche1">[91]</xref>. The stimuli consisted of natural and naturalistic movies at various contrasts, including noise processes with pink spatial and white temporal statistics, pink temporal and white spatial statistics, pink temporal and pink spatial statistics, and natural movies recorded with a ‘cat cam’ <xref ref-type="bibr" rid="pcbi.1003143-Kayser1">[100]</xref>. The mean luminance across all stimuli (15 different stimuli, each lasting 2 minutes) was the same. The raw movies were 64×64 pixels and were sampled at 50 Hz. These raw movies were spatially down-sampled and cropped to produce 20×20 pixel patches that were individually mean-subtracted. Model performance was evaluated using five-fold cross-validation, with cross-validation sets constructed by taking 20% of the data from each stimulus type.</p>
<p>For all analysis we used 8 time lags to construct spatiotemporal filters (each described by 8×20×20 = 3200 parameters). For STA/STC analysis we first whitened the stimulus by rotating into the principal component axes and normalizing each dimension to have unit standard deviation <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>. Because the stimulus covariance matrix for natural stimuli has many eigenvalues close to zero, we avoided amplifying noise associated with these low-variance dimensions by using a pseudoinverse of the covariance matrix, effectively discarding the <italic>n</italic> lowest variance dimensions of the stimulus <xref ref-type="bibr" rid="pcbi.1003143-Theunissen1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003143-David1">[95]</xref>. In addition to removing biases due to pairwise correlations in the stimulus, this method effectively imparts a prior favoring spatiotemporally smooth filters, since the lowest variance dimensions of natural stimuli have high spatial and temporal frequencies. We retained 500/3200 of the stimulus dimensions for STA/STC analysis.</p>
<p>To estimate filters of the LN model, GQM and NIM, we used sparseness regularization, as well as penalties on the (two-dimensional) spatial Laplacian at each time lag. To display the three-dimensional spatiotemporal filters we plot the time slice of each filter containing the most variance across pixels (‘best time slice’), as well as the projection of the filter onto a spatial axis orthogonal to the neuron's preferred orientation (‘space-time projection’) <xref ref-type="bibr" rid="pcbi.1003143-Lochmann1">[62]</xref>. The preferred orientation was determined by fitting a two-dimensional Gabor function to the best time slice for each filter, and taking the (circular) average of the individual Gabor orientations across all filters.</p>
</sec></sec><sec id="s4">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003143.s001" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Robustness of filter estimation.</bold> <bold>A</bold>) For the simulated ON-OFF RGC in <xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1</xref> and <xref ref-type="fig" rid="pcbi-1003143-g003">3</xref>, the likelihood function with respect to the NIM stimulus filters shows only a single global optimum (up to an interchange of the filters) over a broad range of parameter space. To illustrate this, 100 iterations of the optimization were performed with random initializations of the filters, and in all cases the correct filters were identified. The initial filters are projected onto the true ON and OFF filters (inset), and are plotted along with the resulting optimized filter projections. Each iteration of the optimization is thus represented by a pair of optimized filters (large blue and red circles), along with a pair of initial filters (small blue and red circles, color coded based on the resulting filter estimates). <bold>B</bold>) For the example MLd neuron in <xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5</xref>, we found two distinct local maxima when optimizing the NIM stimulus filters, corresponding to the two clusters of the maximized log-likelihood across many repetitions of optimizing the filters with random initial conditions. The global optimum (right) corresponds to the set of filters shown in <xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5</xref>, while a locally optimum solution (left) corresponds to the excitatory filter matching the STA. <bold>C</bold>) For the simulated V1 neuron shown in <xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6A</xref>, optimization of the NIM is again well-behaved. In this case there are potentially several spurious local maxima, illustrated by the distribution of maximized log-likelihood values. However, these local maxima correspond to models that are very similar to the identified global maximum, as shown by the similar log-likelihood values, as well as the similarity of the identified filters (example models shown at left and right).</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003143.s002" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>NIM parameter optimization scales approximately linearly.</bold> <bold>A</bold>) The time required to estimate the filters (black) and upstream nonlinearities (red) scales linearly as a function of data duration for the ON-OFF RGC simulation (with two subunits) shown in <xref ref-type="fig" rid="pcbi-1003143-g001">Figs. 1</xref> and <xref ref-type="fig" rid="pcbi-1003143-g003">3</xref>. The error bars show +/−1 standard deviation around the mean across multiple repetitions of the parameter estimation (with random initialization). Estimation was performed on a machine running Mac OS X 10.6 with two 2.26 GHz quad-core Intel Xeon processors and 16 GB of RAM. <bold>B</bold>) To measure parameter estimation time as a function of the number of stimulus dimensions, we simulated a V1 neuron (similar to that shown in <xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6A</xref>) receiving two rectified inputs (data duration of 10<sup>5</sup> time samples). We then varied the number of time lags used to represent the stimulus and measured the time required for parameter estimation. Estimation of the stimulus filters scales roughly linearly with the number of stimulus dimensions, while estimation of the upstream nonlinearities is largely independent of the number of stimulus dimensions. <bold>C</bold>) Parameter estimation time for the filters and upstream nonlinearities also scales approximately linearly as a function of the number of subunits. Here we again used a simulated V1 neuron similar to that shown in <xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6A</xref>, although with 10 rectified inputs (200 stimulus dimensions and data duration of 10<sup>5</sup> time samples). Note that the additional step of estimating the upstream nonlinearities adds relatively little to the overall parameter estimation time, especially for more complex models.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003143.s003" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Comparison of the NIM and GQM for the example LGN neuron.</bold> The linear model (<bold>A</bold>), NIM (<bold>B</bold>), and GQM (<bold>C</bold>) fit to the example LGN neuron from <xref ref-type="fig" rid="pcbi-1003143-g004">Fig. 4</xref> are shown for comparison. Here (A) and (B) are reproduced from <xref ref-type="fig" rid="pcbi-1003143-g004">Figs. 4A and B</xref> respectively. Note that the spatial and temporal profiles of the linear and squared (suppressive) GQM filters are largely similar to the (rectified) excitatory and suppressive filters identified by the NIM. Despite the similarity of the identified filters, however, the NIM and GQM imply a different picture of the neuron's stimulus processing, as illustrated in <xref ref-type="supplementary-material" rid="pcbi.1003143.s004">Fig. S4</xref>.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003143.s004" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Different predictions of the GQM and NIM with excitation and delayed suppression.</bold> The GLM (<bold>A</bold>), NIM (<bold>B</bold>), and GQM (<bold>C</bold>) fit to the example MLd neuron in <xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5</xref> (A and B here are reproduced from <xref ref-type="fig" rid="pcbi-1003143-g005">Figs. 5A and B</xref>). The NIM and GQM identify similar excitatory and suppressive filters, but the GQM assumes linear and squared upstream nonlinearities for these inputs respectively, while the NIM infers the rectified form of these functions. Despite the similarities in the identified filters, the different upstream nonlinearities in these models imply distinct interactions between the excitatory and suppressive inputs. To illustrate this, we consider how these different models process two stimuli in (D) and (E), which highlight these differences. <bold>D</bold>) First, we consider a negative impulse (left) presented at the preferred frequency (horizontal black lines in A–C). The outputs of the excitatory (blue) and suppressive (red) subunits are shown for the linear model (top), GQM (middle), and NIM (bottom). The combined outputs of these subunits are then transformed by the spiking nonlinearity into the corresponding predicted firing rates at right. In this case, only the linear model responds to the stimulus, since the GQM is strongly suppressed, and the NIM is largely unaffected due to the rectification of the negatively driven inputs. <bold>E</bold>) Similar to (D), we consider a biphasic stimulus (left), also presented at the neuron's preferred frequency. This stimulus drives different responses in all three models. The response predicted by the GQM is by far the weakest because the (squared) suppression driven by the initial negative phase of the stimulus coincides with the excitation driven by the positive phase of the stimulus, causing them to partially cancel each other out. For the NIM, the negative phase of the stimulus does not drive the suppression, due to rectification, and the excitation is able to elicit a much larger response. The response predicted by the linear model is even larger since this is essentially the optimal stimulus for driving the linear filter. This suggests targeted stimuli that might be able to distinguish the computations being performed by MLd neurons.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003143.s005" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Selecting the number of model subunits.</bold> <bold>A</bold>) To illustrate the robustness of NIM parameter estimation to specification of the precise number of subunits, we first consider the simulated V1 neuron from <xref ref-type="fig" rid="pcbi-1003143-g006">Fig. 6A</xref>, which was constructed from six rectified excitatory subunits. Fitting a sequence of NIMs (blue) and GQMs (red) with increasing numbers of (excitatory) subunits reveals that the log-likelihood (evaluated on a simulated cross-validation data set) initially improves dramatically, but becomes nearly saturated for models with four or more subunits. Here we plot log-likelihood relative to that of the best model, and error bars show one std. dev. about the mean. While it is possible in this case to identify the true number of underlying subunits (six) from the cross-validated model performance of the NIM, the model performance is relatively insensitive to specification of the precise number of subunits. <bold>B</bold>) Stimulus filters from example NIM fits from (A), with four, six, and eight filters. Note that the identified filters are nearly identical across these different models, and when more than the true number (six) of subunits are included in the model, sparseness regularization on the filters tends to drive the extra filters to zero, yielding effectively identical models. <bold>C</bold>) To illustrate the procedure of selecting the number of model subunits with real data, we consider fitting a series of models to the example MLd neuron from <xref ref-type="fig" rid="pcbi-1003143-g005">Fig. 5</xref>. In this case there are both excitatory and suppressive stimulus dimensions, so we independently vary the number of each. Average (+/−1 std. error) cross-validated model performance is depicted for each subunit composition for models with up to three subunits (the color indicates the number of suppressive subunits). While we do not have sufficient data to identify statistically significant differences, a two-filter model with one excitatory and one suppressive filter appears to achieve optimal performance. <bold>D</bold>) Three example NIM fits for one, two, and three-filter models corresponding to the Roman numerals in (C). (i) Model with one excitatory subunit. (ii) Model with one excitatory and one suppressive subunit. (iii) Model with two excitatory and one suppressive subunits. Note that the excitatory and suppressive filters from the two-filter model are also present in the three-filter model, and that the addition of a second excitatory subunit (resembling the linear filter) provides little, if any, additional predictive power. <bold>E</bold>) Similar to (C–D), we consider fitting models with different numbers of excitatory and suppressive subunits to the example macaque V1 neuron from <xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7</xref>. In this case, the neuron is selective to a large number of stimulus dimensions (<xref ref-type="fig" rid="pcbi-1003143-g007">Fig. 7A</xref>), and thus there are a large number of possible excitatory/suppressive subunit compositions to consider. To greatly speed this process (and illustrate a procedure for rapid model characterization), we fit the NIM filters in a reduced stimulus subspace (see <xref ref-type="sec" rid="s3">Methods</xref>) that is identified by a GQM with four excitatory and six suppressive dimensions. The number of subunits in the GQM was selected in order to ensure that all filters with discernible structure were included. The figure then shows the average (+/−1 std. error) cross-validated log-likelihood (relative to a model with two excitatory and one suppressive filters) for NIMs with varying numbers of excitatory and suppressive subunits. Note that the model performance increases initially, but tends to saturate for models with more than about four excitatory and four suppressive subunits. For comparison, the cross-validated log-likelihood of the GQM (green line) – which established the stimulus subspace – is below most of the NIM solutions. While fitting the stimulus filters in the full stimulus space provides slightly different (though qualitatively very similar) results, limiting the NIM to the subspace provides a tractable way to fully explore the nonlinear structure of computation, and can then serve as an initial guess for a more computationally-intensive search in the full stimulus space. <bold>F–G</bold>) Two example NIM fits from those depicted in (E). A NIM with four excitatory and four suppressive subunits (F) is compared to a NIM with six excitatory and six suppressive subunits (G), the latter providing only a slight improvement relative to the former. Both models provide a qualitatively similar depiction of the neuron's stimulus processing, identifying largely similar sets of excitatory and suppressive inputs.</p>
<p>(EPS)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003143.s006" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003143.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p><bold>Selection of regularization parameters.</bold> To illustrate how the performance of our models depends on selection of the regularization hyperparameters, we fit a series of models to the example V1 neuron from <xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8</xref>. For this example neuron regularization of the stimulus filters is particularly important, given the large number (3200) of parameters associated with each filter. As described in the <xref ref-type="sec" rid="s3">Methods</xref> section, we use both smoothness (L2 penalty on the spatial Laplacian) and sparseness regularization on the filters, each of which is governed by a hyperparameter. While in principle we could independently optimize these regularization parameters for each filter, we consider here only the case where all filters are subject to the same regularization penalties. Further, we consider optimizing the smoothness and sparseness penalties independently, which will not in general identify the optimal set of hyperparameters. <bold>A</bold>) We first set the sparseness regularization penalty to zero, and systematically vary the strength of the smoothness penalty. The cross-validated log-likelihood is plotted for the NIM (blue trace) and GQM (red trace), showing that the NIM outperforms the GQM over a range of smoothness regularization strengths. <bold>B–D</bold>) Representative filters are shown from model fits at several regularization strengths, as indicated by the black circles in (A). The filters are depicted as the ‘best-time slice’ (BTS) and the ‘space-time projection’ (STP), as in <xref ref-type="fig" rid="pcbi-1003143-g008">Fig. 8</xref>. <bold>E</bold>) Similar to (A), we next consider varying the strength of sparseness regularization given fixed values for the smoothness regularization (set to the value indicated by the vertical dashed line in A). Note that the performance of the NIM again remains significantly better than the GQM across a range of regularization strengths. <bold>F–H</bold>) Representative filters at several sparseness regularization strengths, as indicated in (E). Note that (F) is identical to (C), reproduced for ease of comparison.</p>
<p>(EPS)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors thank J.-M. Alonso for contributing the LGN data, the Theunissen lab and CRCNS database for providing the songbird auditory midbrain data, N. Rust and T. Movshon for contributing the macaque V1 data, and T. Blanche for contributing the cat V1 data. We also thank N. Lesica, and S. David for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003143-Riesenhuber1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Brincat1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brincat</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>CE</given-names></name> (<year>2004</year>) <article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>880</fpage>–<lpage>886</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Mineault1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mineault</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Khawaja</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Pack</surname><given-names>CC</given-names></name> (<year>2012</year>) <article-title>Hierarchical processing of complex motion along the primate dorsal visual pathway</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>: <fpage>E972</fpage>–<lpage>E980</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-DiCarlo1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name> (<year>2007</year>) <article-title>Untangling invariant object recognition</article-title>. <source>Trends Cogn Sci</source> <volume>11</volume>: <fpage>333</fpage>–<lpage>341</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Rust1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Stocker</surname><given-names>AA</given-names></name> (<year>2010</year>) <article-title>Ambiguity and invariance: two fundamental challenges for visual processing</article-title>. <source>Curr Opin Neurobiol</source> <volume>20</volume>: <fpage>382</fpage>–<lpage>388</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Brillinger1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brillinger</surname><given-names>DR</given-names></name> (<year>1988</year>) <article-title>Maximum likelihood analysis of spike trains of interacting nerve cells</article-title>. <source>Biol Cybern</source> <volume>59</volume>: <fpage>189</fpage>–<lpage>200</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Paninski1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2004</year>) <article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title>. <source>Network</source> <volume>15</volume>: <fpage>243</fpage>–<lpage>262</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Truccolo1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Eden</surname><given-names>UT</given-names></name>, <name name-style="western"><surname>Fellows</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>EN</given-names></name> (<year>2005</year>) <article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title>. <source>J Neurophysiol</source> <volume>93</volume>: <fpage>1074</fpage>–<lpage>1089</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Theunissen1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Vinje</surname><given-names>WE</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</article-title>. <source>Network</source> <volume>12</volume>: <fpage>289</fpage>–<lpage>316</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Carandini1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Demb</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Tolhurst</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Do we know what the early visual system does?</article-title> <source>J Neurosci</source> <volume>25</volume>: <fpage>10577</fpage>–<lpage>10597</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Shapley1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Linear and nonlinear systems analysis of the visual system: why does it seem so linear? A review dedicated to the memory of Henk Spekreijse</article-title>. <source>Vision Res</source> <volume>49</volume>: <fpage>907</fpage>–<lpage>921</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-deRuytervanSteveninck1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>1988</year>) <article-title>Coding and information transfer in short spike sequences</article-title>. <source>P Roy Soc Lond B Bio</source> <volume>234</volume>: <fpage>379</fpage>–<lpage>414</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Schwartz1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2006</year>) <article-title>Spike-triggered neural characterization</article-title>. <source>J Vis</source> <volume>6</volume>: <fpage>484</fpage>–<lpage>507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Sharpee1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharpee</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2004</year>) <article-title>Analyzing neural responses to natural signals: maximally informative dimensions</article-title>. <source>Neural Comput</source> <volume>16</volume>: <fpage>223</fpage>–<lpage>250</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Pillow1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2006</year>) <article-title>Dimensionality reduction in neural models: an information-theoretic generalization of spike-triggered average and covariance analysis</article-title>. <source>J Vis</source> <volume>6</volume>: <fpage>414</fpage>–<lpage>428</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Chichilnisky1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2001</year>) <article-title>A simple white noise analysis of neuronal light responses</article-title>. <source>Network</source> <volume>12</volume>: <fpage>199</fpage>–<lpage>213</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Ahrens1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahrens</surname><given-names>MB</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Inferring input nonlinearities in neural encoding models</article-title>. <source>Network</source> <volume>19</volume>: <fpage>35</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Park1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Horwitz</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name> (<year>2011</year>) <article-title>Active learning of neural response functions with Gaussian processes</article-title>. <source>Adv in Neural Inf Process Syst (NIPS)</source> <volume>24</volume>: <fpage>2043</fpage>–<lpage>2051</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Truccolo2"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Truccolo</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name> (<year>2007</year>) <article-title>Nonparametric modeling of neural point processes via stochastic gradient boosting regression</article-title>. <source>Neural Comput</source> <volume>19</volume>: <fpage>672</fpage>–<lpage>705</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Marmarelis1"><label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Marmarelis PZ, Marmarelis VZ (1978) Analysis of physiological systems: The white-noise approach. New York: Plenum Press.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Schetzen1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Schetzen M (1989) The Volterra and Wiener theories of nonlinear systems. Malabar, Florida: Krieger.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Eggermont1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eggermont</surname><given-names>JJ</given-names></name> (<year>1993</year>) <article-title>Wiener and Volterra analyses applied to the auditory system</article-title>. <source>Hear Res</source> <volume>66</volume>: <fpage>177</fpage>–<lpage>201</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-DeWeese1"><label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">DeWeese MR (1995) Optimization principles for the neural code: Princeton.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Marmarelis2"><label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Marmarelis VZ (2004) Nonlinear Dynamic Modeling of Physiological Systems. Hoboken, NJ: Wiley Interscience.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Ahrens2"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahrens</surname><given-names>MB</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Nonlinearities and contextual influences in auditory cortical responses modeled with multilinear spectrotemporal methods</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>1929</fpage>–<lpage>1942</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Park2"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name> (<year>2011</year>) <article-title>Bayesian spike-triggered covariance analysis</article-title>. <source>Adv Neural Inf Process Syst (NIPS)</source> <volume>24</volume>: <fpage>1692</fpage>–<lpage>1700</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Fitzgerald1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitzgerald</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Rowekamp</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name> (<year>2011</year>) <article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002249</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Rajan1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Rajan K, Bialek W (2012) Maximally informative “stimulus energies” in the analysis of neural responses to natural signals. arXiv:12010321 [q-bioNC].</mixed-citation>
</ref>
<ref id="pcbi.1003143-Lau1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lau</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stanley</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2002</year>) <article-title>Computational subunits of visual cortical neurons revealed by artificial neural networks</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>99</volume>: <fpage>8974</fpage>–<lpage>8979</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Prenger1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prenger</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2004</year>) <article-title>Nonlinear V1 responses to natural scenes revealed by neural network analysis</article-title>. <source>Neural Netw</source> <volume>17</volume>: <fpage>663</fpage>–<lpage>679</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Nishimoto1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nishimoto</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Nishimoto</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2011</year>) <article-title>A three-dimensional spatiotemporal receptive field model explains responses of area MT neurons to naturalistic movies</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>14551</fpage>–<lpage>14564</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Mineault2"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mineault</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Khawaja</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Pack</surname><given-names>CC</given-names></name> (<year>2012</year>) <article-title>Hierarchical processing of complex motion along the primate dorsal visual pathway</article-title>. <source>Proc Natl Acad Sci U S A</source></mixed-citation>
</ref>
<ref id="pcbi.1003143-Berry1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Refractoriness and neural precision</article-title>. <source>J Neurosci</source> <volume>18</volume>: <fpage>2200</fpage>–<lpage>2211</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Keat1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keat</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>Predicting every spike: a model for the responses of visual neurons</article-title>. <source>Neuron</source> <volume>30</volume>: <fpage>803</fpage>–<lpage>817</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Pillow2"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Uzzell</surname><given-names>VJ</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2005</year>) <article-title>Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model</article-title>. <source>J Neurosci</source> <volume>25</volume>: <fpage>11003</fpage>–<lpage>11013</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Shapley2"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shapley</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name> (<year>1978</year>) <article-title>The effect of contrast on the transfer properties of cat retinal ganglion cells</article-title>. <source>J Physiol</source> <volume>285</volume>: <fpage>275</fpage>–<lpage>298</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Meister1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meister</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name> (<year>1999</year>) <article-title>The neural code of the retina</article-title>. <source>Neuron</source> <volume>22</volume>: <fpage>435</fpage>–<lpage>450</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Mante1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Bonin</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Functional mechanisms shaping lateral geniculate responses to artificial and natural stimuli</article-title>. <source>Neuron</source> <volume>58</volume>: <fpage>625</fpage>–<lpage>638</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Wehr1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wehr</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zador</surname><given-names>AM</given-names></name> (<year>2003</year>) <article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title>. <source>Nature</source> <volume>426</volume>: <fpage>442</fpage>–<lpage>446</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Murphy1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murphy</surname><given-names>BK</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name> (<year>2009</year>) <article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title>. <source>Neuron</source> <volume>61</volume>: <fpage>635</fpage>–<lpage>648</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Vogels1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname><given-names>TP</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2009</year>) <article-title>Gating multiple signals through detailed balance of excitation and inhibition in spiking networks</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>483</fpage>–<lpage>491</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Sun1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname><given-names>YJ</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>GK</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Fine-tuning of pre-balanced excitation and inhibition during auditory cortical development</article-title>. <source>Nature</source> <volume>465</volume>: <fpage>927</fpage>–<lpage>931</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Dorrn1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dorrn</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Yuan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Barker</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name> (<year>2010</year>) <article-title>Developmental sensory experience balances cortical excitation and inhibition</article-title>. <source>Nature</source> <volume>465</volume>: <fpage>932</fpage>–<lpage>936</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Pillow3"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Butts1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Alonso</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>Temporal precision in the visual pathway through the interplay of excitation and stimulus-driven suppression</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>11313</fpage>–<lpage>11327</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Carcieri1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carcieri</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Jacobs</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Classification of retinal ganglion cells: a statistical approach</article-title>. <source>J Neurophysiol</source> <volume>90</volume>: <fpage>1704</fpage>–<lpage>1713</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Farrow1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farrow</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Masland</surname><given-names>RH</given-names></name> (<year>2011</year>) <article-title>Physiological clustering of visual channels in the mouse retina</article-title>. <source>J Neurophysiol</source> <volume>105</volume>: <fpage>1516</fpage>–<lpage>1530</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Zhang1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>IJ</given-names></name>, <name name-style="western"><surname>Sanes</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>109</volume>: <fpage>E2391</fpage>–<lpage>2398</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Cantrell1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cantrell</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Cang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Troy</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>X</given-names></name> (<year>2010</year>) <article-title>Non-centered spike-triggered covariance analysis reveals neurotrophin-3 as a developmental regulator of receptive field properties of ON-OFF retinal ganglion cells</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000967</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Demb1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Demb</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Zaghloul</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Haarsma</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sterling</surname><given-names>P</given-names></name> (<year>2001</year>) <article-title>Bipolar cells contribute to nonlinear spatial summation in the brisk-transient (Y) ganglion cell in mammalian retina</article-title>. <source>J Neurosci</source> <volume>21</volume>: <fpage>7447</fpage>–<lpage>7454</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Kim1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name> (<year>2001</year>) <article-title>Temporal contrast adaptation in the input and output signals of salamander retinal ganglion cells</article-title>. <source>J Neurosci</source> <volume>21</volume>: <fpage>287</fpage>–<lpage>299</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Geffen1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geffen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>de Vries</surname><given-names>SEJ</given-names></name>, <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Retinal ganglion cells can rapidly change polarity from Off to On</article-title>. <source>PLoS Biol</source> <volume>5</volume>: <fpage>e65</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Gollisch1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gollisch</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Modeling convergent ON and OFF pathways in the early visual system</article-title>. <source>Biol Cybern</source> <volume>99</volume>: <fpage>263</fpage>–<lpage>278</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Schwartz2"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>GW</given-names></name>, <name name-style="western"><surname>Okawa</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Dunn</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Morgan</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Kerschensteiner</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The spatial structure of a nonlinear receptive field</article-title>. <source>Nat Neurosci</source> <volume>15</volume>: <fpage>1572</fpage>–<lpage>1580</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Fairhall1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Burlingame</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Narasimhan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Selectivity for multiple stimulus features in retinal ganglion cells</article-title>. <source>J Neurophysiol</source> <volume>96</volume>: <fpage>2724</fpage>–<lpage>2738</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Rad1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rad</surname><given-names>KR</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2010</year>) <article-title>Efficient, adaptive estimation of two-dimensional firing rate surfaces via Gaussian process methods</article-title>. <source>Network</source> <volume>21</volume>: <fpage>142</fpage>–<lpage>168</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Rust2"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rust</surname><given-names>NC</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2005</year>) <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>945</fpage>–<lpage>956</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Gerwinn1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerwinn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Macke</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Seeger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Bayesian inference for spiking neuron models with a sparsity prior</article-title>. <source>Adv in Neural Inf Proces Syst (NIPS)</source> <volume>20</volume>: <fpage>529</fpage>–<lpage>536</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Narendra1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Narendra</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Gallman</surname><given-names>P</given-names></name> (<year>1966</year>) <article-title>An iterative method for the identification of nonlinear systems using a Hammerstein model</article-title>. <source>IEEE T Automat Contr</source> <volume>11</volume>: <fpage>546</fpage>–<lpage>550</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Hunter1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hunter</surname><given-names>IW</given-names></name>, <name name-style="western"><surname>Korenberg</surname><given-names>MJ</given-names></name> (<year>1986</year>) <article-title>The identification of nonlinear biological systems: Wiener and Hammerstein cascade models</article-title>. <source>Biol Cybern</source> <volume>55</volume>: <fpage>135</fpage>–<lpage>144</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-SchinkelBielefeld1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schinkel-Bielefeld</surname><given-names>N</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name> (<year>2012</year>) <article-title>Inferring the role of inhibition in auditory processing of complex natural stimuli</article-title>. <source>J Neurophysiol</source> <volume>107</volume>: <fpage>3296</fpage>–<lpage>3307</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Lochmann1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lochmann</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Blanche</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name> (<year>2013</year>) <article-title>Construction of direction selectivity through local energy computations in primary visual cortex</article-title>. <source>PLoS ONE</source> <volume>8</volume>: <fpage>e58666</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Friedman1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Stuetzle</surname><given-names>W</given-names></name> (<year>1981</year>) <article-title>Projection pursuit regression</article-title>. <source>J Am Stat Assoc</source> <volume>76</volume>: <fpage>817</fpage>–<lpage>823</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Lingjrde1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lingjærde</surname><given-names>OC</given-names></name>, <name name-style="western"><surname>Liestøl</surname><given-names>K</given-names></name> (<year>1998</year>) <article-title>Generalized projection pursuit regression</article-title>. <source>SIAM J Sci Comput</source> <volume>20</volume>: <fpage>844</fpage>–<lpage>857</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Vintch1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vintch</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Zaharia</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Movshon</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2012</year>) <article-title>Efficient and direct estimation of a neural subunit model for sensory coding</article-title>. <source>Adv in Neural Information Processing Systems (NIPS)</source> <volume>25</volume>: <fpage>3113</fpage>–<lpage>3121</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Bertsekas1"><label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Bertsekas DP (1999) Nonlinear Programming. Belmont, MA: Athena Scientific.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Paninski2"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2003</year>) <article-title>Convergence properties of three spike-triggered analysis techniques</article-title>. <source>Network</source> <volume>14</volume>: <fpage>437</fpage>–<lpage>464</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-J1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>J</surname><given-names>KM</given-names></name>, <name name-style="western"><surname>M</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>K</surname><given-names>N</given-names></name> (<year>1989</year>) <article-title>Dissection of the neuron network in the catfish inner retina. III. Interpretation of spike kernels</article-title>. <source>J Neurophysiol</source> <volume>61</volume>: <fpage>1110</fpage>–<lpage>1120</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Field1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1987</year>) <article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title>. <source>J Opt Soc Am</source> <volume>4</volume>: <fpage>2379</fpage>–<lpage>2394</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Olshausen1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>DJ</given-names></name> (<year>1996</year>) <article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>. <source>Nature</source> <volume>381</volume>: <fpage>607</fpage>–<lpage>609</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Lewicki1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name> (<year>2002</year>) <article-title>Efficient coding of natural sounds</article-title>. <source>Nat Neurosci</source> <volume>5</volume>: <fpage>356</fpage>–<lpage>363</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Gabernet1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gabernet</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Jadhav</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Scanziani</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Somatosensory integration controlled by dynamic thalamocortical feed-forward inhibition</article-title>. <source>Neuron</source> <volume>48</volume>: <fpage>315</fpage>–<lpage>327</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Okun1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lampl</surname><given-names>I</given-names></name> (<year>2008</year>) <article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>535</fpage>–<lpage>537</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Wu1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>GK</given-names></name>, <name name-style="western"><surname>Arbuckle</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>HW</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>LI</given-names></name> (<year>2008</year>) <article-title>Lateral sharpening of cortical frequency tuning by approximately balanced inhibition</article-title>. <source>Neuron</source> <volume>58</volume>: <fpage>132</fpage>–<lpage>143</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Lesica1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lesica</surname><given-names>NA</given-names></name>, <name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yeh</surname><given-names>CI</given-names></name>, <name name-style="western"><surname>Alonso</surname><given-names>JM</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Dynamic encoding of natural luminance sequences by LGN bursts</article-title>. <source>PLoS Biol</source> <volume>4</volume>: <fpage>e209</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Butts2"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Desbordes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Alonso</surname><given-names>JM</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>The episodic nature of spike trains in the early visual pathway</article-title>. <source>J Neurophysiol</source> <volume>104</volume>: <fpage>3371</fpage>–<lpage>3387</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Butts3"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Butts</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Weng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yeh</surname><given-names>CI</given-names></name>, <name name-style="western"><surname>Lesica</surname><given-names>NA</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Temporal precision in the neural code and the timescales of natural vision</article-title>. <source>Nature</source> <volume>449</volume>: <fpage>92</fpage>–<lpage>95</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Sincich1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sincich</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Horton</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name> (<year>2009</year>) <article-title>Preserving information in neural transmission</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>6207</fpage>–<lpage>6216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Wang1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Vaingankar</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Sanchez</surname><given-names>CS</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name>, <name name-style="western"><surname>Hirsch</surname><given-names>JA</given-names></name> (<year>2011</year>) <article-title>Thalamic interneurons and relay cells use complementary synaptic mechanisms for visual processing</article-title>. <source>Nat Neurosci</source> <volume>14</volume>: <fpage>224</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Kaplan1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaplan</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Marcus</surname><given-names>S</given-names></name>, <name name-style="western"><surname>So</surname><given-names>Y</given-names></name> (<year>1979</year>) <article-title>Effects of dark adaptation on spatial and temporal properties of receptive fields in cat lateral geniculate nucleus</article-title>. <source>J Physiol</source> <volume>294</volume>: <fpage>561</fpage>–<lpage>580</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Hsu1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hsu</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Woolley</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Fremouw</surname><given-names>TE</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2004</year>) <article-title>Modulation power and phase spectrum of natural sounds enhance neural encoding performed by single auditory neurons</article-title>. <source>J Neurosci</source> <volume>24</volume>: <fpage>9201</fpage>–<lpage>9211</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Gill1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gill</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wooley</surname><given-names>SMN</given-names></name>, <name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name> (<year>2006</year>) <article-title>Sound representation methods for spectro-temporal receptive field estimation</article-title>. <source>J Comput Neurosci</source> <volume>21</volume>: <fpage>5</fpage>–<lpage>20</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Teeters1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Teeters</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Millman</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Olshausen</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name> (<year>2008</year>) <article-title>Data sharing for computational neuroscience</article-title>. <source>Neuroinformatics</source> <volume>6</volume>: <fpage>47</fpage>–<lpage>55</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Calabrese1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Calabrese</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schumacher</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name> (<year>2011</year>) <article-title>A Generalized Linear Model for Estimating Spectrotemporal Receptive Fields from Responses to Natural Sounds</article-title>. <source>PLoS ONE</source> <volume>6</volume>: <fpage>e16104</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Adelson1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name>, <name name-style="western"><surname>Bergen</surname><given-names>JR</given-names></name> (<year>1985</year>) <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>J Opt Soc Am A</source> <volume>2</volume>: <fpage>284</fpage>–<lpage>299</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Emerson1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Emerson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Bergen</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Adelson</surname><given-names>EH</given-names></name> (<year>1992</year>) <article-title>Directionally selective complex cells and the computation of motion energy in cat visual cortex</article-title>. <source>Vision Res</source> <volume>32</volume>: <fpage>203</fpage>–<lpage>218</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Mante2"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Mapping of stimulus energy in primary visual cortex</article-title>. <source>J Neurophysiol</source> <volume>94</volume>: <fpage>788</fpage>–<lpage>798</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Touryan1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Touryan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lau</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2002</year>) <article-title>Isolation of relevant visual features from random stimuli for cortical complex cells</article-title>. <source>J Neurosci</source> <volume>22</volume>: <fpage>10811</fpage>–<lpage>10818</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Tanabe1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanabe</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Haefner</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Cumming</surname><given-names>BG</given-names></name> (<year>2011</year>) <article-title>Suppressive mechanisms in monkey V1 help to solve the stereo correspondence problem</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>8295</fpage>–<lpage>8305</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Diaconis1"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diaconis</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shahshahani</surname><given-names>M</given-names></name> (<year>1984</year>) <article-title>On nonlinear functions of linear combinations</article-title>. <source>SIAM J Sci and Stat Comput</source> <volume>5</volume>: <fpage>175</fpage>–<lpage>191</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Blanche1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blanche</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Spacek</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Hetke</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Swindale</surname><given-names>NV</given-names></name> (<year>2005</year>) <article-title>Polytrodes: high-density silicon electrode arrays for large-scale multiunit recording</article-title>. <source>J Neurophysiol</source> <volume>93</volume>: <fpage>2987</fpage>–<lpage>3000</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Nocedal1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nocedal</surname><given-names>J</given-names></name> (<year>1980</year>) <article-title>Updating quasi-Newton matrices with limited storage</article-title>. <source>Math Comput</source> <volume>35</volume>: <fpage>773</fpage>–<lpage>782</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Saleem1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saleem</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>Krapp</surname><given-names>HG</given-names></name>, <name name-style="western"><surname>Shultz</surname><given-names>SR</given-names></name> (<year>2008</year>) <article-title>Receptive field characterization by spike-triggered independent component analysis</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>2.1</fpage>–<lpage>2.16</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Sahani1"><label>94</label>
<mixed-citation publication-type="other" xlink:type="simple">Sahani M, Linden J (2003) Evidence optimization techniques for estimating stimulus-response functions. In: Becker S, Thrun S, Obermayer K, editors. Adv in Neural Information Processing Systems. Cambridge, MA: The MIT Press. pp. 317–324.</mixed-citation>
</ref>
<ref id="pcbi.1003143-David1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2007</year>) <article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title>. <source>Network</source> <volume>18</volume>: <fpage>191</fpage>–<lpage>212</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Gerwinn2"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gerwinn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Macke</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Bayesian inference for generalized linear models for spiking neurons</article-title>. <source>Front Comput Neurosci</source> <volume>4</volume>: <fpage>12</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Park3"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name> (<year>2011</year>) <article-title>Receptive field inference with localized priors</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1002219</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Kouh1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kouh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sharpee</surname><given-names>TO</given-names></name> (<year>2009</year>) <article-title>Estimating linear-nonlinear models using Renyi divergences</article-title>. <source>Network</source> <volume>20</volume>: <fpage>49</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Sahani2"><label>99</label>
<mixed-citation publication-type="other" xlink:type="simple">Sahani M, Linden J (2003) How linear are auditory cortical responses? In: Becker S, Thrun S, Obermayer K, editors. Advances in neural information processing systems. Cambridge: MIT.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Kayser1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Salazar</surname><given-names>RF</given-names></name>, <name name-style="western"><surname>Konig</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>Responses to natural scenes in cat V1</article-title>. <source>J Neurophysiol</source> <volume>90</volume>: <fpage>1910</fpage>–<lpage>1920</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003143-Cai1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Deangelis</surname><given-names>GC</given-names></name>, <name name-style="western"><surname>Freeman</surname><given-names>RD</given-names></name> (<year>1997</year>) <article-title>Spatiotemporal receptive field organization in the lateral geniculate nucleus of cats and kittens</article-title>. <source>J Neurophysiol</source> <volume>78</volume>: <fpage>1045</fpage>–<lpage>1061</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>