<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004194</article-id>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01753</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Automated Processing of Imaging Data through Multi-tiered Classification of Biological Structures Illustrated Using <italic>Caenorhabditis elegans</italic></article-title>
<alt-title alt-title-type="running-head">Multi-tiered Classification for Automated Image Processing</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zhan</surname>
<given-names>Mei</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Crane</surname>
<given-names>Matthew M.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="currentaff001" ref-type="fn"><sup>¤</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Entchev</surname>
<given-names>Eugeni V.</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Caballero</surname>
<given-names>Antonio</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Fernandes de Abreu</surname>
<given-names>Diana Andrea</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ch’ng</surname>
<given-names>QueeLim</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Lu</surname>
<given-names>Hang</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff004" ref-type="aff"><sup>4</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Interdisciplinary Program in Bioengineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Wallace H. Coulter Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>MRC Centre for Developmental Neurobiology, Kings College London, London, United Kingdom</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>School of Chemical &amp; Biomolecular Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Rao</surname>
<given-names>Christopher V.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Illinois at Urbana-Champaign, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MZ MMC HL. Performed the experiments: MZ. Analyzed the data: MZ. Contributed reagents/materials/analysis tools: EVE AC DAFdA QC. Wrote the paper: MZ MMC QC HL. Designed the software used in analysis: MZ MMC.</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label><p>Current Address: School of Biological Sciences, University of Edinburgh, Edinburgh, United Kingdom</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">hang.lu@gatech.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>24</day>
<month>4</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<month>4</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>4</issue>
<elocation-id>e1004194</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>9</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>2</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Zhan et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004194" xlink:type="simple"/>
<abstract>
<p>Quantitative imaging has become a vital technique in biological discovery and clinical diagnostics; a plethora of tools have recently been developed to enable new and accelerated forms of biological investigation. Increasingly, the capacity for high-throughput experimentation provided by new imaging modalities, contrast techniques, microscopy tools, microfluidics and computer controlled systems shifts the experimental bottleneck from the level of physical manipulation and raw data collection to automated recognition and data processing. Yet, despite their broad importance, image analysis solutions to address these needs have been narrowly tailored. Here, we present a generalizable formulation for autonomous identification of specific biological structures that is applicable for many problems. The process flow architecture we present here utilizes standard image processing techniques and the multi-tiered application of classification models such as support vector machines (SVM). These low-level functions are readily available in a large array of image processing software packages and programming languages. Our framework is thus both easy to implement at the modular level and provides specific high-level architecture to guide the solution of more complicated image-processing problems. We demonstrate the utility of the classification routine by developing two specific classifiers as a toolset for automation and cell identification in the model organism <italic>Caenorhabditis elegans</italic>. To serve a common need for automated high-resolution imaging and behavior applications in the <italic>C</italic>. <italic>elegans</italic> research community, we contribute a ready-to-use classifier for the identification of the head of the animal under bright field imaging. Furthermore, we extend our framework to address the pervasive problem of cell-specific identification under fluorescent imaging, which is critical for biological investigation in multicellular organisms or tissues. Using these examples as a guide, we envision the broad utility of the framework for diverse problems across different length scales and imaging methods.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>New technologies have increased the size and content-richness of biological imaging datasets. As a result, automated image processing is increasingly necessary to extract relevant data in an objective, consistent and time-efficient manner. While image processing tools have been developed for general problems that affect large communities of biologists, the diversity of biological research questions and experimental techniques have left many problems unaddressed. Moreover, there is no clear way in which non-computer scientists can immediately apply a large body of computer vision and image processing techniques to address their specific problems or adapt existing tools to their needs. Here, we address this need by demonstrating an adaptable framework for image processing that is capable of accommodating a large range of biological problems with both high accuracy and computational efficiency. Moreover, we demonstrate the utilization of this framework for disparate problems by solving two specific image processing challenges in the model organism <italic>Caenorhabditis elegans</italic>. In addition to contributions to the <italic>C</italic>. <italic>elegans</italic> community, the solutions developed here provide both useful concepts and adaptable image-processing modules for other biological problems.</p>
</abstract>
<funding-group>
<funding-statement>This work is partially supported by funding from the US National Institutes of Health (R01AG035317, R01GM088333, and R21EB012803 to HL), the US National Science Foundation (CBET 0954578 to HL, graduate research fellowships 0946809 to MZ and MMC), Wellcome Trust (Project Grant 087146 to QC), and European Research Council (NeuroAge 242666 to QC). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="21"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>The data are available at <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/1853/53154" xlink:type="simple">http://hdl.handle.net/1853/53154</ext-link>. The code package is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/meizhan/SVMelegans.git" xlink:type="simple">https://github.com/meizhan/SVMelegans.git</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec>
<title/>
<disp-quote>
<p>This is a <italic>PLOS Computational Biology</italic> Methods paper</p>
</disp-quote>
</sec>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Diverse imaging techniques exist to provide functional and structural information about biological specimens in clinical and experimental settings. On the clinical side, new and augmented imaging modalities and contrast techniques have increased the types of information that can be garnered from biological samples [<xref rid="pcbi.1004194.ref001" ref-type="bibr">1</xref>]. Similarly, many tools have recently been developed to enable new and accelerated forms of biological experimentation in both single cells and multicellular model organisms [<xref rid="pcbi.1004194.ref002" ref-type="bibr">2</xref>–<xref rid="pcbi.1004194.ref010" ref-type="bibr">10</xref>]. Increasingly, the capacity for high-throughput experimentation provided by new optical tools, microfluidics and computer controlled systems has eased the experimental bottleneck at the level of physical manipulation and raw data collection. Still, the power of many of these toolsets lies in facilitating the automation of experimental processes. The ability to perform real-time information extraction from images during the course of an experiment is therefore a crucial computational step to harnessing the potential of many of these physical systems (<xref rid="pcbi.1004194.g001" ref-type="fig">Fig 1A</xref>). Even when off-line data analysis is sufficient, the capability of these systems to generate large, high-content datasets places a large burden on the speed of the downstream analysis.</p>
<fig id="pcbi.1004194.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Overview of biological structure detection using multi-tiered classification.</title>
<p>a) Unsupervised image processing techniques are often necessary to harness the power of emerging imaging and experimental technologies. b) An overview of the proposed generalizable two layer classification architecture for the autonomous identification of specific biological structures. Intrinsic, computationally simple features and relational or computationally expensive features are partitioned into two layers to accommodate both structural complexity and efficiency.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g001" position="float" xlink:type="simple"/>
</fig>
<p>Automated image processing and the use of supervised learning techniques have the potential for bridging this gap between raw data availability and the limitations of manual analysis in terms of speed, objectivity and sensitivity to subtle changes [<xref rid="pcbi.1004194.ref011" ref-type="bibr">11</xref>]. In this area, many computer vision techniques, including some general object detection strategies, have been developed to address the detection and recognition of faces, vehicles, animals and household objects from standard camera images [<xref rid="pcbi.1004194.ref012" ref-type="bibr">12</xref>–<xref rid="pcbi.1004194.ref017" ref-type="bibr">17</xref>]. While this body of literature solves complex recognition problems within the domain of everyday objects and images, it is not clear how or whether they are generalizable to the imaging modalities and object detection problems that arise in biological image processing. While these techniques have garnered some important but limited adoption in biological applications[<xref rid="pcbi.1004194.ref018" ref-type="bibr">18</xref>–<xref rid="pcbi.1004194.ref028" ref-type="bibr">28</xref>], there is not a systematic methodology by which these computational approaches can be applied to solving common problems in mining biological images [<xref rid="pcbi.1004194.ref029" ref-type="bibr">29</xref>]. Thus, the development or adaptation of these tools for specific problems has thus far been relatively opaque to many potential end-users and require a high degree of expertise and intuition.</p>
<p>At the same time, there is a diverse array of specific object recognition problems that arise in biology. Specifically, extraction of meaningful information from biological images usually involves the identification of particular structures and calculation of their metrics, rather than the usage of global image metrics. Depending on the specimen and the experimental platform, this may range in scale from molecular or sub-cellular structure to individual cells or tissue structures within a heterogeneous specimen, or entire organisms. While toolsets have already been developed to address some common needs in biology [<xref rid="pcbi.1004194.ref019" ref-type="bibr">19</xref>–<xref rid="pcbi.1004194.ref022" ref-type="bibr">22</xref>, <xref rid="pcbi.1004194.ref024" ref-type="bibr">24</xref>, <xref rid="pcbi.1004194.ref025" ref-type="bibr">25</xref>, <xref rid="pcbi.1004194.ref030" ref-type="bibr">30</xref>–<xref rid="pcbi.1004194.ref032" ref-type="bibr">32</xref>] and while powerful algorithmic tools exist for pattern and feature discrimination and decision-making [<xref rid="pcbi.1004194.ref033" ref-type="bibr">33</xref>–<xref rid="pcbi.1004194.ref035" ref-type="bibr">35</xref>], there are still many unaddressed needs in biological image processing.</p>
<p>Here, we present a general scheme for the detection of specific biological structures applicable as a basis for solving a broad set of problems while using non-specific image processing modules. As opposed to finished, ready-to-use toolsets, which address a limited problem definition by design, the workflow we propose has the power to simultaneously address the need for accuracy, problem-specificity, and generalizability; end-users have the opportunity to choose platforms and customize as needed. We demonstrate the power of this approach for solving disparate biological image processing problems by developing two widely relevant toolsets for the multicellular model organism, <italic>Caenorhabditis elegans</italic>. To address the problems of extracting region-, tissue- and cell-specific information within a multicellular context, we developed an image processing algorithm to distinguish the head of the worm under bright-field imaging and a set of tools for specific cell identification under fluorescence imaging. These developments demonstrate the flexibility of our framework to accommodate different imaging modalities and disparate biological structures. The resulting toolsets contribute directing to addressing two fundamental needs for automated studies in the worm and contribute specific concepts and modules that may be applied to a broader range of biological problems.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<p>Our framework is a two-tiered classification scheme to identify specific biological structures within an image (<xref rid="pcbi.1004194.g001" ref-type="fig">Fig 1B</xref>). To identify biological structures of interest, images are first pre-processed to condition the data and generate candidates for the structure of interest. In general, candidates can either be individual pixels or discrete segmented regions generated via a thresholding algorithm applied during pre-processing. To accommodate different image acquisition setups and acquisition parameters, we propose the use of an image calibration factor, <italic>C</italic>, in preprocessing and in all subsequent feature calculation steps. This calibration factor characterizes the relationship between the digitized and real-world length scales for a specific experimental setup and can be used to normalize feature and parameter scaling in all image processing steps (Materials and Methods, <xref rid="pcbi.1004194.s009" ref-type="supplementary-material">S1 Table</xref>).</p>
<p>Subsequently we apply a two-layer classification scheme to identify whether the candidates generated are features of interest. The candidate particles are quantitatively described by two distinct sets of descriptive features. These features may be derived from intuitive metrics designed to mimic human recognition or abstractions that capture additional information [<xref rid="pcbi.1004194.ref033" ref-type="bibr">33</xref>, <xref rid="pcbi.1004194.ref036" ref-type="bibr">36</xref>]; they are mathematical descriptors that help delineate the structures of interest from other candidates and will form the basis for classification. Separation of features into two distinct layers of classification in our proposed scheme serves three purposes. First, it permits conceptual separation of intrinsic and extrinsic or relational properties of a biological structure. Second, it permits the inclusion of higher level descriptions of the relationships between structures identified from the first layer of classification. Finally, it allows computationally expensive features to only be associated with the second layer, which reduces the number of times these features must be calculated as low probability candidates have already been removed. Accordingly, the first layer of classification uses computationally inexpensive, intrinsic features of the candidates to generate a smaller set of candidates. The second layer addresses additional complexity, and uses computationally more expensive features or extrinsic features describing the relationship between candidates, but only on a smaller number of candidates. This two-tier scheme allows significant reduction in computational time. At each layer of classification, a trained classifier is used to make a decision about the candidate’s identity based on the features calculated. In this work, we chose to use support vector machines for all classification steps because of its insensitivity to specific conditioning of feature sets and therefore being more robust [<xref rid="pcbi.1004194.ref034" ref-type="bibr">34</xref>, <xref rid="pcbi.1004194.ref037" ref-type="bibr">37</xref>]. We note that when constraints of the feature sets are well known, other models including Bayesian discriminators and heuristic thresholds can also be used. In general, the workflow architecture presented in <xref rid="pcbi.1004194.g001" ref-type="fig">Fig 1B</xref> permits the identification of generic biological structures and balances the capability for complexity with computational speed. We describe here two distinct applications using this two-tier classification methodology.</p>
<sec id="sec003">
<title>Bright-Field Head Identification</title>
<p>Due to its relatively large size, only a limited portion of the adult worm body can be captured within the field of view under high-resolution imaging; yet it is necessary to target specific regions along the anterior-posterior axis of the worm to capture or apply experimental perturbations to specific cells or tissues of interest (<xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2A</xref>). Thus, image processing for orientation along the anterior-posterior axis of the worm is crucial to enabling the full potential of many of the toolsets for high-resolution imaging and physical, chemical and optical manipulation of the worm. To address this need, many <italic>ad hoc</italic> tactics such as the presence of fluorescent markers [<xref rid="pcbi.1004194.ref005" ref-type="bibr">5</xref>, <xref rid="pcbi.1004194.ref024" ref-type="bibr">24</xref>, <xref rid="pcbi.1004194.ref038" ref-type="bibr">38</xref>, <xref rid="pcbi.1004194.ref039" ref-type="bibr">39</xref>] or the assumption of forward locomotion in freely moving worms [<xref rid="pcbi.1004194.ref022" ref-type="bibr">22</xref>, <xref rid="pcbi.1004194.ref025" ref-type="bibr">25</xref>, <xref rid="pcbi.1004194.ref032" ref-type="bibr">32</xref>, <xref rid="pcbi.1004194.ref040" ref-type="bibr">40</xref>–<xref rid="pcbi.1004194.ref043" ref-type="bibr">43</xref>] are often used delineate between the head and tail and orient the anterior-posterior axis. However, reliance on exogenously introduced fluorescent markers can necessitate time-consuming treatment of the worms under study and can spatially interfere with other fluorescent readouts of interest. While the assumption of forward locomotion does not require additional treatments, it is only useful in experimental contexts where worms are freely mobile. Therefore, these tactics lack general applicability to many high resolution imaging experiments, where worms may lack appropriate fluorescent markers or are physically restrained or chemically immobilized. Additionally, not relying on fluorescent markers avoids unnecessary photobleaching of the sample before data acquisition and affords robustness against age and condition-specific autofluorescence in the worm body [<xref rid="pcbi.1004194.ref044" ref-type="bibr">44</xref>].</p>
<fig id="pcbi.1004194.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Preprocessing and feature selection for head versus tail discrimination in <italic>C</italic>. <italic>elegans</italic>.</title>
<p>a) The limited field of view of high resolution imaging systems creates a need for spatial positioning along the anterior-posterior axis of the worm. As a landmark for orienting the A-P axis, the head of the worm is distinguished by the presence of the pharynx and a grinder structure (inset below). b) Preprocessing for bright field structural detection consists of minimum intensity projection of a sparse z-stack (<italic>MP</italic>) followed by Niblack local thresholding (<italic>BW</italic><sub>0</sub>) and preliminary filtration of segmented particles to generate candidates for subsequent classification (<italic>BW</italic><sub>1</sub>). c) In layer 1 of classification, computationally inexpensive, intrinsic properties of the candidates (<italic>BW</italic><sub>1</sub>) are calculated for SVM classification and reduction of the candidate pool (<italic>BW</italic><sub>2</sub>). d) Two example image processing sequences showing that while the shape-intrinsic features used in layer 1 of classification significantly reduces the candidate pool, it is insufficient for robust, specific identification of the grinder particle. e) From the reduced candidate pool, layer 2 of classification utilizes regional properties of the remaining candidates to distinguish the grinder from other structural and textural elements of the worm body with high specificity, making identification of the head possible on the basis of the presence of the grinder particle.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g002" position="float" xlink:type="simple"/>
</fig>
<p>In order to approach this problem with minimal reliance on specific experimental conditions, we note several consistent morphological differences between the head and the tail of the worm that are observable in bright-field imaging. Bright-field is a commonly available imaging modality and often used for location and positioning of specimens prior to fluorescent imaging. While the shape of the head and the tail differs somewhat, these differences are difficult to detect due to low contrast and may be physically obscured by some experimental platforms [<xref rid="pcbi.1004194.ref038" ref-type="bibr">38</xref>]. Instead, the head of the worm is more clearly distinguished by the presence of the pharynx, which has a stereotypical morphology that includes a biological structure for masticating food called the grinder [<xref rid="pcbi.1004194.ref045" ref-type="bibr">45</xref>]. As shown in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2A</xref>, the grinder is a dark, uniquely shaped, high-contrast structure under bright field imaging. The grinder can also be easily resolved by most digital cameras at imaging magnifications above 20X and maintains its shape and integrity for several days of early adulthood [<xref rid="pcbi.1004194.ref046" ref-type="bibr">46</xref>, <xref rid="pcbi.1004194.ref047" ref-type="bibr">47</xref>]. This stereotypical feature of the head, which is relatively consistent in the worm post-developmentally, can thus serve as the target biological structure for our two-layer classification scheme.</p>
<p>To construct and validate our classification scheme, bright-field images of the worm head and tail were collected using a custom microfluidic device (Materials and Exp. Methods), although similar images on agar pad would also suffice (<xref rid="pcbi.1004194.s001" ref-type="supplementary-material">S1 Fig</xref>). Following our architecture in <xref rid="pcbi.1004194.g001" ref-type="fig">Fig 1B</xref> from left to right, application of the scheme involves three major steps: preprocessing of raw images to generate candidates for the structure of interest, selection and calculation features to describe these candidates at both layers of classification, and optimization and training of the two classifiers based on these feature sets.</p>
<p>First, in the preprocessing step, we apply a minimum intensity projection to consolidate dark structures of multi-plane bright-field images into a single image (<italic>MP</italic> in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2B</xref>) and use Niblack local thresholding to generate discrete binary particles as potential candidates for the grinder particle (<italic>BW</italic><sub>0</sub> in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2B</xref>). We employ the Niblack local thresholding procedure in both this and our subsequent cell identification application to robustly segment particles, despite the potential variability in local lighting, texture and background tissue intensity as there would be in different imaging setups (Materials and Exp. Methods). Following initial thresholding, preliminary filtering of the binary particles is then applied to remove segmented regions that are either too small (less than 37.5 μm<sup>2</sup>) or too large (greater than 100 μm<sup>2</sup>) to reduce downstream computation (<italic>BW</italic><sub>1</sub> in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2B</xref>). The remaining particles are processed through our two-layer classification scheme to detect the presence of the pharyngeal grinder.</p>
<p>Second, in the feature selection step, distinct mathematical descriptors that may help to describe and distinguish the structure of interest are calculated for each layer of classification. In the first layer of classification, intrinsic and computationally inexpensive metrics of the particles are computed and used as features (<xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2C</xref> and <xref rid="pcbi.1004194.s002" ref-type="supplementary-material">S2 Fig</xref>) in classification of the grinder shape. These features represent a combination of simple, intuitive geometric features, such as area and perimeter, in addition to higher level measures of the object geometry and invariant moments suitable for shape description and identification [<xref rid="pcbi.1004194.ref036" ref-type="bibr">36</xref>]. Training and application of a classifier with this feature set eliminates candidates on the basis of intrinsic shape (<italic>BW</italic><sub>2</sub> in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2C</xref>). However, the resulting false positives in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2D</xref> show that the information within these shape metrics is insufficient to distinguish the grinder with high specificity.</p>
<p>To refine the description of the biological structure in the second layer classification, we utilize features that describe the relationship of candidate particles to nearby particles and texture (<xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2E</xref> and <xref rid="pcbi.1004194.s003" ref-type="supplementary-material">S3 Fig</xref>). Specifically, we note that the grinder resides inside the terminal bulb of the pharynx, which is characterized by a distinct circular region of muscular tissue (<xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2A</xref>). Based on this observation, we define second layer features based on distributions of particle properties within a circular region around the centroid of the grinder candidate particle (<xref rid="pcbi.1004194.s003" ref-type="supplementary-material">S3 Fig</xref>). Noting that the pharyngeal tissue is characterized by textural ranges in the radial direction and relative uniformity in the angular direction, we build features sets describing both the radial and angular distributions the surrounding particles (<xref rid="pcbi.1004194.s003" ref-type="supplementary-material">S3 Fig</xref>).</p>
<p>Using the features outlined in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2</xref>, each classification step is a mathematical model that is trained to distinguish between structures of interest such as the pharyngeal grinder and irrelevant structures generated represented the textures and boundaries of other tissues in the worm. To allow for supervised training of both the layer 1 and layer 2 classifiers, we annotated a selection of images (n = 1,430) by manually identifying particles that represent the pharyngeal grinder. The classifiers can then be trained to associate properties of the feature sets with the manually specified identity of candidate particles. However, in addition to informative feature selection and the curation of a representative training set, the performance of SVM classification models is subject to several parameters associated with the model itself and its kernel function [<xref rid="pcbi.1004194.ref034" ref-type="bibr">34</xref>, <xref rid="pcbi.1004194.ref048" ref-type="bibr">48</xref>]. Thus, to ensure good performance of the final SVM model, we first optimize model parameters based on five-fold cross-validation on the training set (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3A and 3B</xref>, Materials and Methods).</p>
<fig id="pcbi.1004194.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Optimization and training of the two layers of SVM classification for pharyngeal grinder detection.</title>
<p>a) To construct the layer 1 classifier with the specified feature set, five-fold cross-validation with a manually annotated training set is first used to optimize SVM model parameters and ensure classification performance. b) Classification performance based on the false positive (FPR) and false negative (FNR) error rates observed in five-fold cross-validation allows selection of an optimal parameter set. c) The full training set and optimized parameters are used to construct the final layer 1 SVM model. Linear projections of the training set features onto two dimensions show that the layer 1 feature set and the optimized SVM model are insufficient for identifying the grinder particle with high specificity. d) The second layer of classification refines the final classification decision and is parameter-optimized using the candidates passed from layer 1 of classification. e) Classification performance based on five-fold cross-validation is used for parameter selection. f) The reduced layer 2 training set and optimized parameters are used to construct the final layer 2 SVM model. Linear projections of layer 2 features for the training set demonstrate the capability of a two layer scheme for the detection of the grinder with both high specificity and sensitivity.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g003" position="float" xlink:type="simple"/>
</fig>
<p>In the parameter selection process, the optimization metric can be designed to reflect the goals of classification in each layer (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3B</xref>). In our application, for the first layer of classification, the goal is to eliminate the large majority of background particles while retaining as many grinder particles in the candidate pool as possible for refined classification in the second layer. In other words, we aim to minimize false negatives while tolerating a moderate number of false positives. Therefore, we optimize the SVM parameters via the minimization of an adjusted error rate that penalizes false negatives more than false positives (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3B</xref>). We show that with an appropriate parameter selection, the first layer of classification can eliminate over 90% of background particles while retaining almost 99% of the true grinder particles for further analysis downstream (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3B</xref>).</p>
<p>To visualize feature and classifier performance, we use Fisher’s linear discriminant analysis to linearly project the 14 layer 1 features of the training set onto two dimensions that show maximum separation between grinder and background particles (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3C</xref>). A high degree of overlap between the distributions of the grinder and background particles and high error rates associated with the trained SVM in this visualization suggest that shape-intrinsic features are insufficient to fully describe the grinder structure. Nevertheless, the first layer of classification enriches the true grinder structure candidates in the training set from roughly 6.2% of the original particle set to 40% of the particle set entering into the second layer of classification (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3C</xref>). This enriched set of candidate particles is used to optimize and train the second layer of classification in a similar manner (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3D</xref>). With appropriate parameter selection, we show that the second layer of classification is capable of identifying the grinder with sensitivity and specificity above 95% (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3E</xref>). We train the final layer 2 classifier with the reduced training set and these optimized parameters to yield high classification performance in combination with layer 1 (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3F</xref>).</p>
<p>Changes in experimental conditions, the genetic background of the worms under study or changes to the imaging system, can cause significant variation in the features, and thus degrade the classifier performance due to overfitting that fails to take into account experimental variation (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3</xref>). To account for this potential variability, we include worms imaged at different ages and food conditions in the training set of images. To validate the utility and efficacy of the resulting classification scheme in a real-life laboratory setting, we analyze its performance on new data sets that were not used in training the classifier. First, in spite of morphological changes due to experimental conditions (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4A</xref>), we show the resulting classification scheme operates with consistently high performance in distinguishing the head and the tail of the worm in the new data sets (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4B</xref>). Second, while the training set only includes wildtype worms imaged under different conditions, the morphology and texture of the worm is also subject to genetic alteration (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4C</xref>). To see whether our classification scheme can accommodate some of this genetic variability, we validate the classification scheme against a mutant strain (<italic>dpy-4(-)</italic>) with large morphological changes in the body of the worm (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4C</xref>). Finally, changes in the imaging system can alter the digital resolution of biological structures of interest (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4E</xref>). We show that the inclusion of a calibration factor adjusting for the pixel to micron conversion of the imaging system is sufficient for maintain classifier operation across a two-fold change in the resolution of the imaging system (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4F</xref>). Thus, this calibrated classification scheme can be easily adapted to systems with different camera pixel formats via the calculation of a new calibration factor.</p>
<fig id="pcbi.1004194.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Head versus tail classification using grinder detection is robust to changes in experimental conditions and genetic background.</title>
<p>a) Changes in experimental conditions, such as food availability, can alter the bulk morphology and the appearance of worm body in bright field, with potential consequences for classification accuracy. b) Our head versus tail classification scheme maintains sensitivity and specificity at over 95% at different ages and feeding conditions despite these biological changes. c) Genetic changes can also induce changes in bulk morphology and texture of the worm. d) Despite not being represented within the training set, the performance of the classifier is maintained even for mutant worms (<italic>dpy-4 (-)</italic>) with major morphological changes. e) Changes in the optics, camera or acquisition parameters can alter the final resolution of images. f) The inclusion of the calibration metric within feature calculation (<xref rid="pcbi.1004194.s002" ref-type="supplementary-material">S2 Fig</xref> and <xref rid="pcbi.1004194.s003" ref-type="supplementary-material">S3 Fig</xref>) maintains classifier performance across a two-fold change in image resolution due to alternations in digital binning.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g004" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec004">
<title>Identification of Fluorescently Labeled Cells</title>
<p>An ever-expanding array of fluorescent markers and biosensors [<xref rid="pcbi.1004194.ref006" ref-type="bibr">6</xref>] has made the identification of specific fluorescent objects and patterns a common biological image processing problem. Although fluorescent staining or tagging techniques can be used to target structures or molecules of interest, they often cannot offer perfect specificity. Furthermore, biological specimens can also include autofluorescent elements that confound the analysis of fluorescent images. Thus, sifting relevant information from fluorescent images can pose non-trivial image processing problems where background fluorescent objects can have similar intensities or spatial locations.</p>
<p>The usage of fluorescent tools in <italic>C</italic>. <italic>elegans</italic> is no exception. Existing toolsets permit fluorescent labeling of different genetic outputs of subsets of cells and tissues. However, fluorescent tags also often label multiple cells, cellular processes or tissue structures that must be distinguished to address specific biological questions. Moreover, <italic>C</italic>. <italic>elegans</italic> exhibits significant gut autofluorescence that varies in intensity and can obscure the identification of fluorescent targets throughout the length of the worm [<xref rid="pcbi.1004194.ref044" ref-type="bibr">44</xref>]. Here, we demonstrate the use of our framework to address these common challenges in fluorescent image processing, using neuron identification in the worm as a broadly useful example.</p>
<p>We first focus on the identification of the ASI neurons as a stereotypical example of a bilaterally symmetric neuron pair in the worm. <xref rid="pcbi.1004194.g005" ref-type="fig">Fig 5B</xref> shows a corresponding set of bright field and fluorescent images illustrating the positioning of the neuron pair within the head region of the worm. In addition to the cell bodies of interest, the raw fluorescent image also shows cellular processes and autofluorescent granules in the gut of the worm that can confound cell-specific image analysis. Similar to our approach for pharyngeal grinder detection in <xref rid="pcbi.1004194.g002" ref-type="fig">Fig 2B</xref>, we begin building our cell identification toolset via preprocessing of the raw images by maximum intensity projection, Niblack thresholding and preliminary filtering of the resulting candidate particles (<xref rid="pcbi.1004194.g005" ref-type="fig">Fig 5C</xref>, Materials and Exp. Methods). In the selection of features for both layers of classification, we note that the layer 1 feature set we developed for the detection of the pharyngeal grinder can be generally applied to the description of particle shape within other contexts (<xref rid="pcbi.1004194.s002" ref-type="supplementary-material">S2 Fig</xref>). Using this feature set, we optimize and train a layer 1 SVM classifier using a manually annotated training set (n = 218) (<xref rid="pcbi.1004194.s004" ref-type="supplementary-material">S4A Fig</xref>, Materials and Methods) and show that it is sufficient for identifying cellular regions with relatively high sensitivity and specificity (<xref rid="pcbi.1004194.g005" ref-type="fig">Fig 5D</xref> and <xref rid="pcbi.1004194.s004" ref-type="supplementary-material">S4A Fig</xref>).</p>
<fig id="pcbi.1004194.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g005</object-id>
<label>Fig 5</label>
<caption>
<title>First layer classification for detection of fluorescently labelled neuronal cells demonstrates generalizability of first layer features for particle shape classification.</title>
<p>a) Stereotypical positioning of the ASI neuron pair in the head of the worm. Many neuronal cells in the worm are organized as similar pairs near the pharynx. b) Bright field and fluorescent maximum intensity projection showing the appearance and positioning of fluorescently labelled ASI cells in the head of the worm. c) Preprocessing of raw fluorescent images showing binary image after Niblack thresholding (<italic>BW</italic><sub>0</sub>) and initial filtration of the candidate set by size (<italic>BW</italic><sub>1</sub>). d) First layer classification of fluorescently labeled neurons shows good generalizability of the first layer feature set developed for pharyngeal grinder detection for classification based on binary particle shape.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g005" position="float" xlink:type="simple"/>
</fig>
<p>While the first layer of classification is effective at eliminating the large majority of background particles, variable background intensity within the tissues surrounding the neurons can generate confounding binary particles that pass layer 1 classification (<xref rid="pcbi.1004194.g006" ref-type="fig">Fig 6A</xref>). To make a final identification of a true cell pair, we apply a second layer of classification based on the relational properties of potential pairs of particles that pass layer 1 classification (<xref rid="pcbi.1004194.g006" ref-type="fig">Fig 6B</xref> and <xref rid="pcbi.1004194.s005" ref-type="supplementary-material">S5 Fig</xref>). To construct our layer 2 classifier, we optimize and train an SVM model based on these pairwise relational features (<xref rid="pcbi.1004194.s004" ref-type="supplementary-material">S4B Fig</xref>). We note that while the relational features we utilize are computationally simple, embedding relational features on the second layer of classification dramatically reduces the size of the paired candidate set. For example, for detection of cell pairs amongst n particles, there are <inline-formula id="pcbi.1004194.e001"><alternatives><graphic id="pcbi.1004194.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e001" xlink:type="simple"/>
<mml:math display="inline" id="M1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</alternatives></inline-formula> possible candidate pairs that require feature calculation.</p>
<fig id="pcbi.1004194.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Second layer classification for neuron pair detection.</title>
<p>a) The first layer of classification is insufficient for rejection of all background particles. b) The reduced candidate set from the first layer of classification is used to form candidate cell pairs with feature sets describing their relative positioning and intensities. c) Although classification based on these features is sufficient for accurate cell pair detection in the majority of cases (left), multiple potential cell pairs are sometimes classified within the same image (right). d) Incorporating probability estimates (shown in panel c) into the SVM model and selecting the most likely cell pair eliminates these false positives and increases the specificity of the classifier.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g006" position="float" xlink:type="simple"/>
</fig>
<p>Validating the resulting cell pair classifier against new test images, we find robust single cell-pair detection in the majority of cases (<xref rid="pcbi.1004194.g006" ref-type="fig">Fig 6C</xref>, left). However, in a minority of cases, multiple candidate pairs are identified as potential neuron pairs in each image (<xref rid="pcbi.1004194.g006" ref-type="fig">Fig 6C</xref>, right). This is a common scenario as many promoters used in transgene markers are not necessarily specific to a single class of cells. In this case, the probability estimates from the SVM classifier [<xref rid="pcbi.1004194.ref037" ref-type="bibr">37</xref>, <xref rid="pcbi.1004194.ref049" ref-type="bibr">49</xref>] along with the selection of the most likely candidate in images with multiple positive classification results is used to eliminate these false positives. This boosts the specificity of the classifier without compromising the high sensitivity (<xref rid="pcbi.1004194.g006" ref-type="fig">Fig 6D</xref>). This additional step incorporates the real-world constraint that, at most, one cell pair exists in each valid image and resolves any conflicts that may arise in direct classification.</p>
<p>To demonstrate the ability of our framework to detect more complex cellular arrangements, we use the expression pattern of a worm insulin-like peptide gene (<italic>ins-6</italic>) in two bilaterally symmetric neuron pairs (<xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7A</xref>) [<xref rid="pcbi.1004194.ref050" ref-type="bibr">50</xref>]. In this case, the specificity offered by the <italic>ins-6</italic> promoter is insufficient to offer full cell specificity, requiring the identification of different cells from the raw fluorescent image. Taking advantage of our modular two-layer architecture, we reuse the preprocessing and first layer classification tools that we have already constructed to identify a small number of cell-shaped objects shown in <xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7B</xref>. To detect the tetrad of cells with specificity for the ASI and ASJ neurons, we construct a relational feature set based on combinations of neuron pairs (<xref rid="pcbi.1004194.s006" ref-type="supplementary-material">S6 Fig</xref>). As shown in <xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7C</xref>, accounting for both correct cell pair identification and non-repetition of individual cells within the tetrad set, there are <inline-formula id="pcbi.1004194.e002"><alternatives><graphic id="pcbi.1004194.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e002" xlink:type="simple"/>
<mml:math display="inline" id="M2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</alternatives></inline-formula> tetrad sets that require feature calculation. Our two-layer architecture is therefore essential for the construction of such relational feature sets with larger numbers of targets. Without layer 1 classification, description of such complex sets quickly becomes intractable: even 10 candidate particles generates 1,260 different possible tetrad sets for feature calculation.</p>
<fig id="pcbi.1004194.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004194.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Second layer classifier for cell pattern recognition and identification.</title>
<p>a) Representative maximum intensity projection and schematic representation of the two neuron pairs in which an insulin-like peptide is expressed. b) The modularity of our scheme permits the preprocessing and layer 1 classification components from neuron pair detection to be re-used for the recognition and identification of these neuron pairs. c) To identify the pattern with the appropriate cell identifications, properties for all possible combinations and arrangements of the layer 1 candidates are calculated. Here, all six such candidate sets for 4 candidate particles are shown. d) Validation of the SVM classifier trained with these features shows high specificity but only moderate sensitivity. e) The lower sensitivity observed for this classification scheme is mainly due to the limit ability to accommodate biological deviations from the stereotypical arrangement of the neurons while still maintaining high specificity.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.g007" position="float" xlink:type="simple"/>
</fig>
<p>To construct a new problem-specific layer 2 classifier based on relationships within these tetrad candidates, we optimize and train a SVM model based on a manually annotated training set (n = 324) (<xref rid="pcbi.1004194.s004" ref-type="supplementary-material">S4C Fig</xref>). Subsequent validation of our two-layer classifier against new test images shows that the two-layer classification scheme operates with higher specificity but lower sensitivity in comparison to our single cell-pair classification problem (<xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7D</xref>). Further analysis of the classifier performance within the test set of images shows that this lower sensitivity is mainly due to more degrees of freedom for variability associated with this particular image processing problem. As shown in <xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7E</xref>, while the second-layer classifier accommodates some deviation from the stereotypical arrangement of the neurons shown in <xref rid="pcbi.1004194.g007" ref-type="fig">Fig 7A</xref> (positive identification on the left), there is a trade-off between maintaining specificity and sensitivity (rejecting larger deviations as illustrated by the negative identification on the right). If the stringency is important, i.e. maintaining specificity and reducing misidentification rate, the users would have to tolerate a small amounts of false negatives. Users would need to determine a comfortable level of rejection rate for each specific problem to tune the classifier.</p>
</sec>
</sec>
<sec id="sec005" sec-type="conclusions">
<title>Discussion</title>
<p>We have demonstrated the flexibility and computational benefits of our two-layer architecture in handling two disparate image recognition problems. Using our pipeline, we have developed two specific solutions addressing common image processing problems for the <italic>C</italic>. <italic>elegans</italic> community. Our contribution of a ready-to-use head-versus-tail classification scheme under bright-field imaging enables automated high-resolution imaging and stimulus application in a large range of biological experiments in the worm. Our neuronal cell pair identification application forms the basis for approaching the general problem of cell-specific information extraction within a multicellular context such as the worm. Together, these specific tools permit automated visual dissection of the multicellular worm at different resolutions that range from the targeting of rough anatomical regions to cell-specificity.</p>
<p>In addition to the immediate utility of the two examples provided in this work, they are also representative of two classes of problems that are commonly found in biological image processing. The detection of the pharyngeal grinder demonstrates a general class of problems where discrete structures are distinguished by both their intrinsic shape and the characteristics of their local environment. The entire framework, including the feature sets, developed and documented for this problem can be applied to the recognition of other discrete structures including subcellular organelles such as nuclei, specific cell types and tissue structures. The detection of single and multiple cell pairs extends the analysis to stereotypical formations of objects. The feature sets documented here for analyzing paired objects is directly applicable to the analysis of many symmetrical structures that arise in biology, such as in the nervous system. However, with some modification, similar features can be applied to the analysis of different patterns that may arise in specific biological processes such as development. Finally, the preprocessing modules developed for these two applications demonstrate the ability to segment out objects of different intensities from both bright-field and fluorescent imaging and are applicable to many other problem sets.</p>
<p>Our two specific applications also highlight the effectiveness of our algorithm in segregating complex image recognition problems in both a computationally effective and conceptually convenient manner. In the detection of the pharyngeal grinder, two-layer construction eliminated the need to compute a large set of regional descriptors by associating them with the second layer of classification and therefore a smaller candidate set. In comparison with direct calculation of all features in a single layer of classification, the two-layer architecture employed in this work reduced average total computational time by a factor of two (<xref rid="pcbi.1004194.s007" ref-type="supplementary-material">S7 Fig</xref>). In cell identification, reserving relational properties for the second-layer of classifications dramatically reduced the number of pairs or sets for which relationships must be described. In this case, the computational time savings associated with the two-layer architecture increases with the complexity of the second-layer relationships and can result in large, roughly six-fold speed improvements in the case of two cell pair identification (<xref rid="pcbi.1004194.s008" ref-type="supplementary-material">S8 Fig</xref>). In addition to these computational benefits, our two applications also demonstrate that the segregation of intrinsic and secondary or extrinsic properties of a structure onto two layers of classification reserves many problem-specific features for the second layer and renders the first layer feature set generalizable. In addition, we have demonstrated that by incorporating a calibration factor to normalize feature calculation, these classifiers can be adapted to different optical systems and sensor configurations with only the modification of the calibration factor itself (<xref rid="pcbi.1004194.g004" ref-type="fig">Fig 4E and 4F</xref> and <xref rid="pcbi.1004194.s009" ref-type="supplementary-material">S1</xref> Table).</p>
<p>In both layers of classification, we adopt a supervised learning approach that depends upon human annotation of training sets of data. This approach imposes user-defined structure onto the data-extraction problem and promotes familiarization with the condition and fundamental limitations on the information content of imaging datasets [<xref rid="pcbi.1004194.ref029" ref-type="bibr">29</xref>, <xref rid="pcbi.1004194.ref051" ref-type="bibr">51</xref>]. Moreover, having a small set of manually annotated images allows for the assessment of the reliability of the final analysis [<xref rid="pcbi.1004194.ref029" ref-type="bibr">29</xref>]. Thus, the user exercises control over higher level problem structure including the formulation of the overall classification question, the choice of the type of candidates and the features used. However, to constrain the construction of the solution, we present a specific workflow with integrated computational techniques that bypass much of the manual guesswork. Annotation and calculation of quantitative descriptors about particle or pixel candidates captures multivariate information about different structures. The use of this multivariate information with a classification model such as SVM obviates the need for manually assessing rectilinear thresholds for classification. Moreover, the performance of our classifiers demonstrate that the potentially nonlinear, multi-dimensional classification provided by SVM prove more powerful than rectilinear thresholding of individual features or dimensionality reduction techniques (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3C and 3F</xref>). Overall, our proposed methodology provides a pipeline that streamlines and formalizes the image processing steps after the annotation of a training set.</p>
<p>Finally, while utility of our framework will require feature selection and training for each particular application, the modularity and architecture of our framework permits aspects of the specific tools we have developed here to be reused. In general, the construction of our classification scheme affords layer 1 classifiers more general applicability. For example, we have demonstrated the generalizability of our layer 1 feature set for binary particle classification with re-training for the identification of different shapes. The layer 1 classifier constructed in our cell identification scheme can also be reused for the classification of different downstream cellular arrangements. Even for the second layer of classification, where feature sets are problem-specific, we have provided examples of both regional and relational feature set constructions that can form the basis of feature sets for other problems.</p>
<sec id="sec006">
<title>Conclusion</title>
<p>Beyond the specific applications we discuss here, we envision that our methodology can be a powerful way to tackle a broad range of biological image processing problems. For instance, we consider our scheme to be a generalization of the previously reported application of SVMs towards the understanding of synaptic morphology in <italic>C</italic>. <italic>elegans</italic> [<xref rid="pcbi.1004194.ref024" ref-type="bibr">24</xref>]. In this application, individual pixels within the image form the pool of candidates for potential synaptic pixels in the first layer classification. The second layer of classification then refines this decision on the basis of relational characteristics between candidates. Here, we formalize this classification approach and demonstrate that it can be adapted towards detection of disparate structures imaged under different imaging modalities. The imaging processing approach we present here has inherent structural advantages in terms of conceptual division, modularization and computational efficiency and demonstrates the application of a powerful supervised learning model to streamline biological image processing. We thus envision that our methodology can form the basis for detection algorithms for structures ranging from the molecular to the tissue or organismal level under different experimental methodologies.</p>
</sec>
</sec>
<sec id="sec007" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec008">
<title>Worm Maintenance and Culture</title>
<p><italic>C</italic>. <italic>elegans</italic> worms used in this study were maintained and cultured according to standard techniques [<xref rid="pcbi.1004194.ref052" ref-type="bibr">52</xref>]. Briefly, populations of worms were allowed to reach reproductive maturity and lay eggs on NGM agar media overnight. Age-synchronized worms were then obtained by washing free-moving worms off of the agar plate, allowing the remaining eggs to hatch for one hour and then washing the resulting L1 stage larvae off of the plate. Age-synchronized L1 worms were then transferred onto new NGM plates seeded with OP50 <italic>E</italic>. <italic>coli</italic> bacteria as a standard food source and grown until the desired age for imaging. To avoid over-crowding and food depletion, adult worms were transferred onto new plates daily. For starvation experiments, worms were transferred onto fresh NGM plates lacking a bacterial food source the day before imaging.</p>
<p><italic>C</italic>. <italic>elegans</italic> strains used in these studies were wild-type N2 worms, QH3833 <italic>dpy-4</italic>(<italic>e1166</italic>), QL296 <italic>drcSi89[pdaf-7</italic>::<italic>GFP; unc-119(+)]</italic> and QL617 <italic>drcSi68[unc-119(+); Pins-6</italic>::<italic>mCherry]II; gjIs140[dpy-20(+); gpa-4</italic>::<italic>GFP]</italic>.</p>
</sec>
<sec id="sec009">
<title>Microfluidics and Image Acquisition</title>
<p>We use standard soft lithographic techniques to produce polydimethylsiloxane (PDMS) imaging devices similar to those previously described [<xref rid="pcbi.1004194.ref024" ref-type="bibr">24</xref>, <xref rid="pcbi.1004194.ref038" ref-type="bibr">38</xref>]. For automated imaging, worms are washed off of NGM plates using S Basal buffer and introduced via pressure injection into the microfluidic device. Sequential activation of pressure sources driving liquid delivery and on-chip pneumatic valves is then used to drive individual worms within the device for imaging.</p>
<p>Images were collected either on a Leica DMI 6000B microscope with a Hamamatsu Orca D2 camera and a 40X oil objective or on an Olympus IX-73 microscope with a Hamamatsu Flash 4.0 camera and a 40X oil objective. Relevant specifications and calibration metrics for these set-ups can be found in <xref rid="pcbi.1004194.s009" ref-type="supplementary-material">S1 Table</xref>. Although not strictly necessary, for generalizability in cases where the center of focus is adjusted to specific fluorescent targets and do not capture the pharynx well, a sparse three plane z-stack with a 15μm step size is used for bright field image acquisition. To fully capture neuronal cells, a dense z-stack was collected through the body of the worm. For fluorescence imaging of the single neuron pair in QL296, a 0.4μm step size was used over a 60μm thick volume. For fluorescence imaging of multiple neurons pairs in QL617, a 1μm step size was used over a 100μm thick volume.</p>
</sec>
<sec id="sec010">
<title>Image Analysis and Computational Tools</title>
<p>We use custom MATLAB code to perform all image preprocessing and feature extraction steps and enable the construction and testing of our classification schemes. In preprocessing, the three dimensional information in the acquired z-stacks were either maximum or minimum projected onto a single two-dimensional image for further processing. For bright-field images, a minimum projection with respect to z was utilized to accentuate the appearance of dark objects throughout the stack. Conversely, for fluorescence images, a maximum projection was utilized to accentuate the appearance of bright objects throughout the stack:</p>
<disp-formula id="pcbi.1004194.e003">
<alternatives>
<graphic id="pcbi.1004194.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>In order to generate binary particles for classification, we use a local thresholding algorithm that uses information about the mean and variability of pixel intensities within a local region around a pixel:</p>
<disp-formula id="pcbi.1004194.e004">
<alternatives>
<graphic id="pcbi.1004194.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e004" xlink:type="simple"/>
<mml:math display="block" id="M4" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
<p><italic>μ</italic><sub><italic>local</italic></sub> and <italic>σ</italic><sub><italic>local</italic></sub> are the means and standard deviations of all pixel values that fall within a square region of width 2<italic>R</italic> + 1 centered around the pixel of interest <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub> and <italic>k</italic> is a parameter specifying the stringency of the threshold. <italic>μ</italic><sub><italic>local</italic></sub> and <italic>σ</italic><sub><italic>local</italic></sub> can be derived using standard image filtering with a binary square filter <italic>h</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>) of width 2<italic>R</italic> + 1:</p>
<disp-formula id="pcbi.1004194.e005">
<alternatives>
<graphic id="pcbi.1004194.e005g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e005" xlink:type="simple"/>
<mml:math display="block" id="M5" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
<p>Using local mean and standard deviation information in the binary decision affords robustness against local background intensity and texture changes.</p>
<p>The width of the local region, <italic>R</italic>, can be roughly selected on the basis of the size scale of the structure of interest. In accordance with the size scales of the pharyngeal structure and individual neurons, we use <italic>R</italic> = 15<italic>μm</italic> for detection of the pharyngeal grinder and <italic>R</italic> = 5<italic>μm</italic> for fluorescent cell segmentation.</p>
<p>The parameter <italic>k</italic> can be roughly selected by visual inspection of segmentation results. We use <italic>k</italic> = 0.75 for our bright field application and <italic>k</italic> = 0.85 for our fluorescence application. Individual candidate particles in the resulting binary image are defined as groups of nonzero pixels that are connected to each other via any adjacent of diagonal pixel (8-connected). We note that changes in <italic>k</italic> can alter the size of segmented particles and the connectivity of segmented particles. Particularly in bright field, where the contrast mechanism lacks specificity, decreases in <italic>k</italic> can cause particles to merge via small bridges of dark texture. In order to build in some robustness against changes in <italic>k</italic> and background texture in these scenarios, we perform a form of a morphological opening operation after thresholding to remove small bridges that may arise between otherwise distinct particles. To do this, we perform a morphological erosion with a small circular structuring element followed by a morphological dilation with a smaller structuring element [<xref rid="pcbi.1004194.ref053" ref-type="bibr">53</xref>].</p>
<p>In order to fully capture both intrinsic and secondary characteristics of biological structures, we calculate distinct sets of features for two layers of classification. The first layer, which delineates structures of interest from other structures on the basis on its intrinsic geometric properties, is generally applicable to particle classification problems and is used for both the bright field and fluorescent structure detection outlined here. Details and equations for the calculation of the 14 features for layer 1 classification can be found in <xref rid="pcbi.1004194.s002" ref-type="supplementary-material">S2 Fig</xref>.</p>
<p>Secondary characteristics of biological structures describe the context in which structures exist and their relationship to other structures. Due to the large variability in the secondary characteristics of biological structures, a generic set of features is not necessarily attainable or desirable due to concerns for computational efficiency. Rather, secondary features can be derived via a mathematical description of empirical observations of important structural properties. In the case of pharyngeal grinder detection, the secondary features are regional, forming a description of the image context in which the grinder structure resides. The form of the features is based on an empirical understanding of this structural context and full details and equations for the calculation of the 34 features in layer 2 of the bright field classifier can be found in <xref rid="pcbi.1004194.s003" ref-type="supplementary-material">S3 Fig</xref>. In the case of cell pair detection, the secondary features are mostly relational, describing how particles from layer 1 of classification may or may not exist as pairs on the basis of both positioning and intensity. Second layer features for cell pair detection can be found in <xref rid="pcbi.1004194.s005" ref-type="supplementary-material">S5 Fig</xref>.</p>
<p>We do briefly note that we scale all calculated features using a calibration factor, <italic>C</italic>, derived from specifications of both the optics and sensors that form the imaging system:</p>
<disp-formula id="pcbi.1004194.e006">
<alternatives>
<graphic id="pcbi.1004194.e006g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e006" xlink:type="simple"/>
<mml:math display="block" id="M6" overflow="scroll">
<mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>O</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>The use of this calibration system renders the trained classifier relatively invariable to small changes in the imaging set-up via conversion of all features into real units. Calibration factors for all imaging systems and configurations used here can be found in <xref rid="pcbi.1004194.s009" ref-type="supplementary-material">S1 Table</xref>.</p>
<p>To implement discrete classification steps using support vector machines, we use the LIBSVM library, which is freely available for multiple platforms including MATLAB [<xref rid="pcbi.1004194.ref037" ref-type="bibr">37</xref>]. For general performance, we train use a Gaussian radial basis function kernel for all of our trained classifiers [<xref rid="pcbi.1004194.ref048" ref-type="bibr">48</xref>]. To ensure performance of the SVM model for our datasets, we optimize the penalty or margin parameter, <italic>C</italic><sub><italic>SVM</italic></sub>, and the kernel parameter, <italic>γ</italic>, for each training set using the five-fold cross-validation performance of the classifier as the output metric. For efficient parameter optimization, we start with a rough exponential grid search (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3B and 3D</xref> and <xref rid="pcbi.1004194.s004" ref-type="supplementary-material">S4 Fig</xref>) and refine parameter selection with a finer grid search based on these results. To adjust for the relative proportions of positive and negative candidates in unbalanced training sets (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3C</xref>), we also adjust the relative weight, <italic>W</italic>, of the classes according to their representation in the training set while training [<xref rid="pcbi.1004194.ref037" ref-type="bibr">37</xref>]. Additionally, we perform a small grid search for the optimal weighting factor to fully optimize the following performance metric. Probability estimates for single and multiple neuron pair identification are derived according to the native LIBSVM algorithm [<xref rid="pcbi.1004194.ref037" ref-type="bibr">37</xref>].</p>
<p>For visualization of the high dimensionality feature sets (<xref rid="pcbi.1004194.g003" ref-type="fig">Fig 3C and 3F</xref>), we apply Fisher’s linear discriminant analysis [<xref rid="pcbi.1004194.ref054" ref-type="bibr">54</xref>]. The two projection directions are chosen to be the first two eigen vectors of:</p>
<disp-formula id="pcbi.1004194.e007">
<alternatives>
<graphic id="pcbi.1004194.e007g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1004194.e007" xlink:type="simple"/>
<mml:math display="block" id="M7" overflow="scroll">
<mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>S</mml:mi><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>S</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable>
</mml:math>
</alternatives>
</disp-formula>
<p><italic>S</italic><sub><italic>B</italic></sub> is a measure of inter-class separation and <italic>S</italic><sub><italic>W</italic></sub> is a measure of intra-class scatter.</p>
</sec>
</sec>
<sec id="sec011">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004194.s001" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s001" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Images Collected with Standard Agar Pad Techniques Can Also Be Subjected to the same Analysis for Identification of the Grinder.</title>
<p>a, b and c show three representative images of day 2, well-fed adult worms acquired using standard agar pad imaging techniques. The intermediate outputs of grinder detection (<italic>MP</italic>, <italic>BW</italic><sub>0</sub>, <italic>BW</italic><sub>1</sub>, <italic>BW</italic><sub>2</sub>, <italic>BW</italic><sub>3</sub>) show the minimally projected image, the binary image after thresholding, the initial particle candidate set, the candidate set after the first layer of classification and the final particle set after the second layer of classification, respectively. The same process developed for head versus tail analysis on microfluidic chip robustly identifies the grinder structure in these conventionally acquired images.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s002" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s002" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Robust Descriptors for Binary Particle Shape for Layer 1 of Classification Scheme.</title>
<p>a) Table of 14 features for binary shape description including low-level geometric descriptors, more complex derived measures of geometry and invariant moments. b) Diagram of binary particle indicating variables used for feature definition. c) Illustration and example of defining and calculating the perimeter of an irregular particle based on pixel connectivity. d) Illustration and example of the convex hull of a binary particle.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s003" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s003" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Regional Descriptors for Structural Detection of the Pharyngeal Grinder.</title>
<p>a) Diagram of the region of interest around a grinder particle showing changes in texture and particle density along radial partitions. b) Diagram of the region of interest around a grinder particle distinguishing individual particles using different colors and showing particle distributions along angular partitions. c) Table of 34 features used to describe regional characteristics of the grinder particle for the second layer of classification.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s004" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s004" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Parameter selection for the first and second layer classifiers in neuron pair identification.</title>
<p>Optimized parameters for the first layer classifier (a), the second layer single pair classifier (b) and the second layer two pair classifier (c) show considerable variability, reinforcing the need for case-specific parameter optimization.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s005" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s005" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Relational features for pairs of neurons.</title>
<p>a) Maximum intensity projection (<italic>MP</italic>) and binary image (<italic>BW</italic><sub>2</sub>) showing candidate particles after the first layer of classification with relevant axes and regions labeled. b) Identification of possible pairs for feature calculation and schematic of an example feature set for one pair. c) Table of the four relational features used to describe cell pair patterns.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s006" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s006" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Relational features for multiple cell pair detection and identification.</title>
<p>a) Maximum intensity projection (<italic>MP</italic>) and binary image showing candidate particles after layer 1 classification (<italic>BW</italic><sub>2</sub>) with relevant axes and regions labeled. b) Enumeration of the possible neuron pairs and the possible sets of neuron pairs with correct distinction between the ASI and ASJ pairs. c) Schematic showing the frame of reference (<italic>X</italic><sub><italic>C</italic></sub>, <italic>Y</italic><sub><italic>C</italic></sub>) for the calculation of the relative location of each neuron and the intensities of the neurons within two particular sets. d) Table showing that 6 properties are calculated for each neuron pair, resulting in a total of 12 relational features to identify the tetrad of neurons.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s007" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s007" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Computational time savings associated with two-layer classification architecture for head versus tail detection.</title>
<p>a) Schematic comparisons of the two-layer, serial classification architecture employed in this work and an equivalent single-layer, parallel classification architecture used for time comparisons. b) Comparison of process-specific and total time requirements for the two-layer and equivalent one-layer architectures. Reducing second-layer feature calculations using the two-layer scheme results in over a two-fold reduction in total classification time. All times are based on performance on MATLAB 2013b running on a quad core processor at 3.50 GHz.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s008" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s008" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title>Computational time savings associated with two-layer classification architecture for cell identification.</title>
<p>a) Comparison of process-specific and total time requirements for the two-layer and equivalent one-layer architectures when applied to single neuron pair detection. b) Comparison of process-specific and total time requirements for the two-layer and equivalent one-layer architectures when applied to the identification of two distinct neuron pairs. All times are based on performance on MATLAB 2013b running on a quad core processor at 3.50 GHz.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004194.s009" xlink:href="info:doi/10.1371/journal.pcbi.1004194.s009" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Calculation of the calibration metric for common changes in the imaging system and acquisition parameters.</title>
<p>Setups used for this study are highlighted.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors would like to gratefully acknowledge Brad Parker and Jeffrey Andrews for machining hardware necessary for this work, and Dhaval S. Patel for critical commentary on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004194.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Brant</surname> <given-names>WE</given-names></name>, <name name-style="western"><surname>Helms</surname> <given-names>CA</given-names></name>. <chapter-title>Fundamentals of diagnostic radiology</chapter-title>: <publisher-name>Lippincott Williams &amp; Wilkins</publisher-name>; <year>2012</year>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref002"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">San-Miguel A, Lu H. Microfluidics as a tool for C. elegans research. WormBook: the online review of C elegans biology. 2013:1–19.</mixed-citation></ref>
<ref id="pcbi.1004194.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhan</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chingozha</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>H</given-names></name>. <article-title>Enabling Systems Biology Approaches Through Microfabricated Systems</article-title>. <source>Analytical Chemistry</source>. <year>2013</year>;<volume>85</volume>(<issue>19</issue>):<fpage>8882</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1021/ac401472y" xlink:type="simple">10.1021/ac401472y</ext-link></comment> <object-id pub-id-type="pmid">23984862</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fenno</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Yizhar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Deisseroth</surname> <given-names>K</given-names></name>. <article-title>The development and application of optogenetics</article-title>. <source>Annual review of neuroscience</source>. <year>2011</year>;<volume>34</volume>:<fpage>389</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-neuro-061010-113817" xlink:type="simple">10.1146/annurev-neuro-061010-113817</ext-link></comment> <object-id pub-id-type="pmid">21692661</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larsch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ventimiglia</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Bargmann</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Albrecht</surname> <given-names>DR</given-names></name>. <article-title>High-throughput imaging of neuronal activity in Caenorhabditis elegans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2013</year>;<volume>110</volume>(<issue>45</issue>):<fpage>E4266</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1318325110" xlink:type="simple">10.1073/pnas.1318325110</ext-link></comment> <object-id pub-id-type="pmid">24145415</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Qin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>McCombs</surname> <given-names>JE</given-names></name>. <article-title>Design and application of genetically encoded biosensors</article-title>. <source>Trends in biotechnology</source>. <year>2011</year>;<volume>29</volume>(<issue>3</issue>):<fpage>144</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tibtech.2010.12.004" xlink:type="simple">10.1016/j.tibtech.2010.12.004</ext-link></comment> <object-id pub-id-type="pmid">21251723</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kocabas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>C-H</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>ZV</given-names></name>, <name name-style="western"><surname>Ramanathan</surname> <given-names>S</given-names></name>. <article-title>Controlling interneuron activity in Caenorhabditis elegans to evoke chemotactic behaviour</article-title>. <source>Nature</source>. <year>2012</year>;<volume>490</volume>(<issue>7419</issue>):<fpage>273</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature11431" xlink:type="simple">10.1038/nature11431</ext-link></comment> <object-id pub-id-type="pmid">23000898</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shaffer</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>M-T</given-names></name>, <name name-style="western"><surname>Levesque</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Raj</surname> <given-names>A</given-names></name>. <article-title>Turbo FISH: A Method for Rapid Single Molecule RNA FISH</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>9</issue>):<fpage>e75120</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0075120" xlink:type="simple">10.1371/journal.pone.0075120</ext-link></comment> <object-id pub-id-type="pmid">24066168</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raj</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Van Den Bogaard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rifkin</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Van Oudenaarden</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tyagi</surname> <given-names>S</given-names></name>. <article-title>Imaging individual mRNA molecules using multiple singly labeled probes</article-title>. <source>Nature methods</source>. <year>2008</year>;<volume>5</volume>(<issue>10</issue>):<fpage>877</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1253" xlink:type="simple">10.1038/nmeth.1253</ext-link></comment> <object-id pub-id-type="pmid">18806792</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Schafer</surname> <given-names>WR</given-names></name>. <article-title>Unrestrained worms bridled by the light</article-title>. <source>nature methods</source>. <year>2011</year>;<volume>8</volume>(<issue>2</issue>):<fpage>129</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth0211-129" xlink:type="simple">10.1038/nmeth0211-129</ext-link></comment> <object-id pub-id-type="pmid">21278723</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eliceiri</surname> <given-names>KW</given-names></name>, <name name-style="western"><surname>Berthold</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Ibanez</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Manjunath</surname> <given-names>BS</given-names></name>, <name name-style="western"><surname>Martone</surname> <given-names>ME</given-names></name>, <etal>et al</etal>. <article-title>Biological imaging software tools</article-title>. <source>Nat Meth</source>. <year>2012</year>;<volume>9</volume>(<issue>7</issue>):<fpage>697</fpage>–<lpage>710</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Everingham</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eslami</surname> <given-names>SMA</given-names></name>, <name name-style="western"><surname>Van Gool</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Winn</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zisserman</surname> <given-names>A</given-names></name>. <article-title>The Pascal Visual Object Classes Challenge: A Retrospective</article-title>. <source>Int J Comput Vis</source>. <volume>2014</volume>:<fpage>1</fpage>–<lpage>39</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Papageorgiou CP, Oren M, Poggio T, editors. A general framework for object detection. Computer Vision, 1998 Sixth International Conference on; 1998 4–7 Jan 1998.</mixed-citation></ref>
<ref id="pcbi.1004194.ref014"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Viola P, Jones M, editors. Rapid object detection using a boosted cascade of simple features. Computer Vision and Pattern Recognition, 2001 CVPR 2001 Proceedings of the 2001 IEEE Computer Society Conference on; 2001 2001.</mixed-citation></ref>
<ref id="pcbi.1004194.ref015"><label>15</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lienhart</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Kuranov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pisarevsky</surname> <given-names>V</given-names></name>. <chapter-title>Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection</chapter-title>. In: <name name-style="western"><surname>Michaelis</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Krell</surname> <given-names>G</given-names></name>, editors. <source>Pattern Recognition</source>. Lecture Notes in Computer Science. 2781: <publisher-name>Springer Berlin</publisher-name> <publisher-loc>Heidelberg</publisher-loc>; <year>2003</year>. p. <fpage>297</fpage>–<lpage>304</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Papageorgiou</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Example-based object detection in images by components</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2001</year>;<volume>23</volume>(<issue>4</issue>):<fpage>349</fpage>–<lpage>61</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Papageorgiou</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>A Trainable System for Object Detection</article-title>. <source>Int J Comput Vis</source>. <year>2000</year>;<volume>38</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>33</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boland</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Markey</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Murphy</surname> <given-names>RF</given-names></name>. <article-title>Automated recognition of patterns characteristic of subcellular structures in fluorescence microscopy images</article-title>. <source>Cytometry</source>. <year>1998</year>;<volume>33</volume>(<issue>3</issue>):<fpage>366</fpage>–<lpage>75</lpage>. <object-id pub-id-type="pmid">9822349</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Murray</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Boyle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ooi</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Sandel</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Waterston</surname> <given-names>RH</given-names></name>. <article-title>Automated cell lineage tracing in Caenorhabditis elegans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2006</year>;<volume>103</volume>(<issue>8</issue>):<fpage>2707</fpage>–<lpage>12</lpage>. <object-id pub-id-type="pmid">16477039</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Murray</surname> <given-names>JI</given-names></name>, <name name-style="western"><surname>Bao</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Boyle</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Boeck</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Mericle</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Nicholas</surname> <given-names>TJ</given-names></name>, <etal>et al</etal>. <article-title>Automated analysis of embryonic gene expression with cellular resolution in C. elegans</article-title>. <source>Nat Methods</source>. <year>2008</year>;<volume>5</volume>(<issue>8</issue>):<fpage>703</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1228" xlink:type="simple">10.1038/nmeth.1228</ext-link></comment> <object-id pub-id-type="pmid">18587405</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Santella</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Du</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Nowotschin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hadjantonakis</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Bao</surname> <given-names>Z</given-names></name>. <article-title>A hybrid blob-slice model for accurate and efficient detection of fluorescence labeled nuclei in 3D</article-title>. <source>BMC bioinformatics</source>. <year>2010</year>;<volume>11</volume>:<fpage>580</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2105-11-580" xlink:type="simple">10.1186/1471-2105-11-580</ext-link></comment> <object-id pub-id-type="pmid">21114815</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>K-M</given-names></name>, <name name-style="western"><surname>Cosman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schafer</surname> <given-names>WR</given-names></name>. <article-title>Machine vision based detection of omega bends and reversals in C. elegans</article-title>. <source>Journal of neuroscience methods</source>. <year>2006</year>;<volume>158</volume>(<issue>2</issue>):<fpage>323</fpage>–<lpage>36</lpage>. <object-id pub-id-type="pmid">16839609</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yemini</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jucikas</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Grundy</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Schafer</surname> <given-names>WR</given-names></name>. <article-title>A database of Caenorhabditis elegans behavioral phenotypes</article-title>. <source>Nature methods</source>. <year>2013</year>;<volume>10</volume>(<issue>9</issue>):<fpage>877</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2560" xlink:type="simple">10.1038/nmeth.2560</ext-link></comment> <object-id pub-id-type="pmid">23852451</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crane</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Stirman</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Ou</surname> <given-names>C-Y</given-names></name>, <name name-style="western"><surname>Kurshan</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Rehg</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Autonomous screening of C. elegans identifies genes implicated in synaptogenesis</article-title>. <source>Nature methods</source>. <year>2012</year>;<volume>9</volume>(<issue>10</issue>):<fpage>977</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.2141" xlink:type="simple">10.1038/nmeth.2141</ext-link></comment> <object-id pub-id-type="pmid">22902935</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Restif</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Ibáñez-Ventoso</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vora</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Metaxas</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Driscoll</surname> <given-names>M</given-names></name>. <article-title>CeleST: Computer Vision Software for Quantitative Analysis of C. elegans Swim Behavior Reveals Novel Features of Locomotion</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>7</issue>):<fpage>e1003702</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003702" xlink:type="simple">10.1371/journal.pcbi.1003702</ext-link></comment> <object-id pub-id-type="pmid">25033081</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ranzato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>House</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Flagan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Automatic recognition of biological particles in microscopic images</article-title>. <source>Pattern Recognition Letters</source>. <year>2007</year>;<volume>28</volume>(<issue>1</issue>):<fpage>31</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dankert</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hoopfer</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Automated monitoring and analysis of social behavior in Drosophila</article-title>. <source>Nat Meth</source>. <year>2009</year>;<volume>6</volume>(<issue>4</issue>):<fpage>297</fpage>–<lpage>303</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yin</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Sadok</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sailem</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Xia</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>A screen for morphological complexity identifies regulators of switch-like transitions between discrete cell shapes</article-title>. <source>Nature cell biology</source>. <year>2013</year>;<volume>15</volume>(<issue>7</issue>):<fpage>860</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncb2764" xlink:type="simple">10.1038/ncb2764</ext-link></comment> <object-id pub-id-type="pmid">23748611</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ljosa</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Sokolnicki</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>. <article-title>Annotated high-throughput microscopy image sets for validation</article-title>. <source>Nat Meth</source>. <year>2012</year>;<volume>9</volume>(<issue>7</issue>):<fpage>637</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Lamprecht</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kang</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Friman</surname> <given-names>O</given-names></name>, <etal>et al</etal>. <article-title>CellProfiler: image analysis software for identifying and quantifying cell phenotypes</article-title>. <source>Genome biology</source>. <year>2006</year>;<volume>7</volume>(<issue>10</issue>):<fpage>R100</fpage>. <object-id pub-id-type="pmid">17076895</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wählby</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kamentsky</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>ZH</given-names></name>, <name name-style="western"><surname>Riklin-Raviv</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Conery</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>O’Rourke</surname> <given-names>EJ</given-names></name>, <etal>et al</etal>. <article-title>An image analysis toolbox for high-throughput C. elegans assays</article-title>. <source>Nature methods</source>.<volume>9</volume>(<issue>7</issue>):<fpage>714</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1984" xlink:type="simple">10.1038/nmeth.1984</ext-link></comment> <object-id pub-id-type="pmid">22522656</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Albrecht</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Bargmann</surname> <given-names>CI</given-names></name>. <article-title>High-content behavioral analysis of Caenorhabditis elegans in precise spatiotemporal chemical environments</article-title>. <source>Nature methods</source>. <year>2011</year>;<volume>8</volume>(<issue>7</issue>):<fpage>599</fpage>–<lpage>605</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1630" xlink:type="simple">10.1038/nmeth.1630</ext-link></comment> <object-id pub-id-type="pmid">21666667</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref033"><label>33</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sonka</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hlavac</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Boyle</surname> <given-names>R</given-names></name>. <chapter-title>Image processing, analysis, and machine vision</chapter-title>: <publisher-name>Cengage Learning</publisher-name>; <year>2014</year>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ben-Hur</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ong</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Sonnenburg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Rätsch</surname> <given-names>G</given-names></name>. <article-title>Support vector machines and kernels for computational biology</article-title>. <source>PLoS computational biology</source>. <year>2008</year>;<volume>4</volume>(<issue>10</issue>):<fpage>e1000173</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000173" xlink:type="simple">10.1371/journal.pcbi.1000173</ext-link></comment> <object-id pub-id-type="pmid">18974822</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamir</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Delaney</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Orlov</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Eckley</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>IG</given-names></name>. <article-title>Pattern recognition software and techniques for biological image analysis</article-title>. <source>PLoS computational biology</source>. <year>2010</year>;<volume>6</volume>(<issue>11</issue>):<fpage>e1000974</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000974" xlink:type="simple">10.1371/journal.pcbi.1000974</ext-link></comment> <object-id pub-id-type="pmid">21124870</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>G</given-names></name>. <article-title>Review of shape representation and description techniques</article-title>. <source>Pattern recognition</source>. <year>2004</year>;<volume>37</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>C-J</given-names></name>. <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Transactions on Intelligent Systems and Technology (TIST)</source>. <year>2011</year>;<volume>2</volume>(<issue>3</issue>):<fpage>27</fpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chung</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Crane</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>H</given-names></name>. <article-title>Automated on-chip rapid microscopy, phenotyping and sorting of C. elegans</article-title>. <source>Nature methods</source>. <year>2008</year>;<volume>5</volume>(<issue>7</issue>):<fpage>637</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1227" xlink:type="simple">10.1038/nmeth.1227</ext-link></comment> <object-id pub-id-type="pmid">18568029</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohde</surname> <given-names>CB</given-names></name>, <name name-style="western"><surname>Zeng</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gonzalez-Rubio</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Angel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Yanik</surname> <given-names>MF</given-names></name>. <article-title>Microfluidic system for on-chip high-throughput whole-animal sorting and screening at subcellular resolution</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>35</issue>):<fpage>13891</fpage>–<lpage>5</lpage>. <object-id pub-id-type="pmid">17715055</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swierczek</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Giles</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Rankin</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Kerr</surname> <given-names>RA</given-names></name>. <article-title>High-throughput behavioral analysis in C. elegans</article-title>. <source>Nature methods</source>. <year>2011</year>;<volume>8</volume>(<issue>7</issue>):<fpage>592</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1625" xlink:type="simple">10.1038/nmeth.1625</ext-link></comment> <object-id pub-id-type="pmid">21642964</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stirman</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Crane</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Husson</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Wabnig</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schultheis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gottschalk</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Real-time multimodal optical control of neurons and muscles in freely behaving Caenorhabditis elegans</article-title>. <source>Nature methods</source>. <year>2011</year>;<volume>8</volume>(<issue>2</issue>):<fpage>153</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1555" xlink:type="simple">10.1038/nmeth.1555</ext-link></comment> <object-id pub-id-type="pmid">21240278</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leifer</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Fang-Yen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gershow</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Alkema</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Samuel</surname> <given-names>AD</given-names></name>. <article-title>Optogenetic manipulation of neural activity in freely moving Caenorhabditis elegans</article-title>. <source>Nature methods</source>. <year>2011</year>;<volume>8</volume>(<issue>2</issue>):<fpage>147</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1554" xlink:type="simple">10.1038/nmeth.1554</ext-link></comment> <object-id pub-id-type="pmid">21240279</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramot</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Berry</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Carnell</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>MB</given-names></name>. <article-title>The Parallel Worm Tracker: a platform for measuring average speed and drug-induced paralysis in nematodes</article-title>. <source>PloS one</source>. <year>2008</year>;<volume>3</volume>(<issue>5</issue>):<fpage>e2208</fpage>–<lpage>e</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0002208" xlink:type="simple">10.1371/journal.pone.0002208</ext-link></comment> <object-id pub-id-type="pmid">18493300</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunk</surname> <given-names>UT</given-names></name>, <name name-style="western"><surname>Terman</surname> <given-names>A</given-names></name>. <article-title>Lipofuscin: mechanisms of age-related accumulation and influence on cell function</article-title>. <source>Free radical biology &amp; medicine</source>. <year>2002</year>;<volume>33</volume>(<issue>5</issue>):<fpage>611</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref045"><label>45</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Avery</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Thomas</surname> <given-names>JH</given-names></name>. <chapter-title>Feeding and defecation</chapter-title>. In: <name name-style="western"><surname>Riddle</surname> <given-names>DL BT</given-names></name>, <name name-style="western"><surname>Meyer</surname> <given-names>BJ</given-names></name>, <etal>et al</etal>., editor. <source>C elegans II</source>. <edition>2nd ed.</edition> <publisher-loc>Cold Spring Harbor, NY</publisher-loc>: <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>; <year>1997</year>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chow</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Glenn</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Johnston</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Wolkow</surname> <given-names>CA</given-names></name>. <article-title>Sarcopenia in the Caenorhabditis elegans pharynx correlates with muscle contraction rate over lifespan</article-title>. <source>Experimental gerontology</source>. <year>2006</year>;<volume>41</volume>(<issue>3</issue>):<fpage>252</fpage>–<lpage>60</lpage>. <object-id pub-id-type="pmid">16446070</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnston</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Iser</surname> <given-names>WB</given-names></name>, <name name-style="western"><surname>Chow</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>IG</given-names></name>, <name name-style="western"><surname>Wolkow</surname> <given-names>CA</given-names></name>. <article-title>Quantitative Image Analysis Reveals Distinct Structural Transitions during Aging in Caenorhabditis elegans Tissues</article-title>. <source>PLoS ONE</source>. <year>2008</year>;<volume>3</volume>(<issue>7</issue>):<fpage>e2821</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0002821" xlink:type="simple">10.1371/journal.pone.0002821</ext-link></comment> <object-id pub-id-type="pmid">18665238</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref048"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Hsu C-W, Chang C-C, Lin C-J. A practical guide to support vector classification. 2003.</mixed-citation></ref>
<ref id="pcbi.1004194.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname> <given-names>T-F</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>C-J</given-names></name>, <name name-style="western"><surname>Weng</surname> <given-names>RC</given-names></name>. <article-title>Probability estimates for multi-class classification by pairwise coupling</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2004</year>;<volume>5</volume>:<fpage>975</fpage>–<lpage>1005</lpage>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cornils</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gloeck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Alcedo</surname> <given-names>J</given-names></name>. <article-title>Specific insulin-like peptides encode sensory information to regulate distinct developmental processes</article-title>. <source>Development</source>. <year>2011</year>;<volume>138</volume>(<issue>6</issue>):<fpage>1183</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1242/dev.060905" xlink:type="simple">10.1242/dev.060905</ext-link></comment> <object-id pub-id-type="pmid">21343369</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bray</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Fraser</surname> <given-names>AN</given-names></name>, <name name-style="western"><surname>Hasaka</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Carpenter</surname> <given-names>AE</given-names></name>. <article-title>Workflow and metrics for image quality control in large-scale high-content screens</article-title>. <source>Journal of biomolecular screening</source>. <year>2012</year>;<volume>17</volume>(<issue>2</issue>):<fpage>266</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1087057111420292" xlink:type="simple">10.1177/1087057111420292</ext-link></comment> <object-id pub-id-type="pmid">21956170</object-id></mixed-citation></ref>
<ref id="pcbi.1004194.ref052"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">Stiernagle T. Maintenance of C. elegans. WormBook: the online review of C elegans biology. 2006:1–11.</mixed-citation></ref>
<ref id="pcbi.1004194.ref053"><label>53</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dougherty</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Lotufo</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>SPIE</surname> <given-names>TISfOE</given-names></name>. <chapter-title>Hands-on morphological image processing</chapter-title>: <publisher-name>SPIE press Bellingham</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="pcbi.1004194.ref054"><label>54</label><mixed-citation publication-type="other" xlink:type="simple">Scholkopft B, Mullert K-R. Fisher discriminant analysis with kernels. Neural networks for signal processing IX. 1999.</mixed-citation></ref>
</ref-list>
</back>
</article>